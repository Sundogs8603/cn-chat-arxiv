<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>DeepSeek LLM&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#20197;&#38271;&#26399;&#35270;&#37326;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#30340;&#39033;&#30446;&#65292;&#36890;&#36807;&#30740;&#31350;&#21644;&#24212;&#29992;&#25193;&#23637;&#35268;&#24459;&#65292;&#25104;&#21151;&#21019;&#24314;&#20102;DeepSeek Chat&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.02954</link><description>&lt;p&gt;
DeepSeek LLM&#65306;&#20511;&#21161;&#38271;&#26399;&#20027;&#20041;&#25193;&#23637;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. (arXiv:2401.02954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02954
&lt;/p&gt;
&lt;p&gt;
DeepSeek LLM&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#20197;&#38271;&#26399;&#35270;&#37326;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#30340;&#39033;&#30446;&#65292;&#36890;&#36807;&#30740;&#31350;&#21644;&#24212;&#29992;&#25193;&#23637;&#35268;&#24459;&#65292;&#25104;&#21151;&#21019;&#24314;&#20102;DeepSeek Chat&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20196;&#20154;&#30633;&#30446;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#25991;&#29486;&#20013;&#25551;&#36848;&#30340;&#25193;&#23637;&#35268;&#24459;&#24471;&#20986;&#20102;&#19981;&#21516;&#30340;&#32467;&#35770;&#65292;&#36825;&#23545;&#25193;&#23637;LLM&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#25193;&#23637;&#35268;&#24459;&#65292;&#24182;&#25552;&#20986;&#20102;&#25105;&#20204;&#29420;&#29305;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;&#20419;&#36827;&#20004;&#31181;&#24120;&#29992;&#30340;&#24320;&#28304;&#37197;&#32622;&#65288;7B&#21644;67B&#65289;&#20013;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#25193;&#23637;&#12290;&#22312;&#25193;&#23637;&#35268;&#24459;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;DeepSeek LLM&#39033;&#30446;&#65292;&#33268;&#21147;&#20110;&#20197;&#38271;&#26399;&#35270;&#37326;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#25903;&#25345;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#30446;&#21069;&#21253;&#21547;2&#19975;&#20159;&#20010;&#26631;&#35760;&#65292;&#24182;&#19981;&#26029;&#25193;&#22823;&#12290;&#25105;&#20204;&#36824;&#22312;DeepSeek LLM&#22522;&#26412;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;DeepSeek Chat&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;DeepSeek LLM 67B&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;LLaMA-2 70B&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#31215;&#31639;&#27861;(ADMM)&#30340;&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#23618;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#65292;&#32467;&#21512;&#31616;&#21333;&#30340;&#36845;&#20195;&#20462;&#21098;&#25513;&#30721;&#36873;&#25321;&#65292;&#22312;&#24191;&#27867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02938</link><description>&lt;p&gt;
&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast and Optimal Weight Update for Pruned Large Language Models. (arXiv:2401.02938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#31215;&#31639;&#27861;(ADMM)&#30340;&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#23618;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#65292;&#32467;&#21512;&#31616;&#21333;&#30340;&#36845;&#20195;&#20462;&#21098;&#25513;&#30721;&#36873;&#25321;&#65292;&#22312;&#24191;&#27867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35268;&#27169;&#24222;&#22823;&#12290;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#24494;&#35843;&#65292;&#36825;&#26159;&#20026;&#20102;&#24674;&#22797;&#22240;&#21024;&#38500;&#26435;&#37325;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#35201;&#20040;&#23436;&#20840;&#24573;&#30053;&#20102;&#24494;&#35843;&#65292;&#19987;&#27880;&#20110;&#39640;&#25928;&#30340;&#20462;&#21098;&#26631;&#20934;&#65292;&#35201;&#20040;&#23581;&#35797;&#36880;&#23618;&#26435;&#37325;&#26356;&#26032;&#65292;&#20445;&#25345;&#27599;&#20010;&#23618;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#36880;&#23618;&#26435;&#37325;&#26356;&#26032;&#23545;LLMs&#26469;&#35828;&#20063;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#24471;&#19981;&#37319;&#29992;&#21508;&#31181;&#36817;&#20284;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#31215;&#31639;&#27861;(ADMM)&#30340;&#24555;&#36895;&#19988;&#26368;&#20248;&#30340;&#20462;&#21098;&#23618;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#12290;&#32467;&#21512;&#31616;&#21333;&#30340;&#36845;&#20195;&#20462;&#21098;&#25513;&#30721;&#36873;&#25321;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24191;&#27867;&#30340;LLMs&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/fmfi-compbio/admm-pruning&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations.  In our paper, we propose a fast and optimal weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). Coupled with a simple iterative pruning mask selection, our algorithm achieves state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;ASR&#31995;&#32479;&#30340;lattice&#36755;&#20986;&#26469;&#25552;&#21319;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24357;&#21512;ASR&#20551;&#35774;&#21644;&#29702;&#24819;&#19978;&#38480;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#25552;&#21319;&#22312;&#22024;&#26434;&#35821;&#38899;&#36716;&#24405;&#19979;&#30340;SLU&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02921</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#35789;&#28151;&#28102;&#32593;&#23454;&#29616;ASR&#40065;&#26834;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards ASR Robust Spoken Language Understanding Through In-Context Learning With Word Confusion Networks. (arXiv:2401.02921v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;ASR&#31995;&#32479;&#30340;lattice&#36755;&#20986;&#26469;&#25552;&#21319;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24357;&#21512;ASR&#20551;&#35774;&#21644;&#29702;&#24819;&#19978;&#38480;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#25552;&#21319;&#22312;&#22024;&#26434;&#35821;&#38899;&#36716;&#24405;&#19979;&#30340;SLU&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#39046;&#22495;&#65292;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#27861;&#24050;&#32463;&#25913;&#36827;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#36716;&#24405;&#30340;&#35821;&#38899;&#36827;&#34892;&#20102;&#36866;&#37197;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#20070;&#38754;&#25991;&#26412;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#29983;&#25104;&#36755;&#20986;&#30340;&#36716;&#24405;&#20551;&#35774;&#65292;&#20854;&#20013;&#30340;&#38169;&#35823;&#20250;&#38477;&#20302;&#21518;&#32493;&#30340;SLU&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;ASR&#31995;&#32479;&#30340;lattice&#36755;&#20986;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#39030;&#37096;&#30340;&#20551;&#35774;&#65292;&#26088;&#22312;&#21253;&#21547;&#35821;&#38899;&#27169;&#31946;&#24615;&#24182;&#25552;&#21319;SLU&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28085;&#30422;&#21475;&#35821;&#38382;&#31572;&#21644;&#24847;&#22270;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#39564;&#65292;&#24378;&#35843;&#20102;LLM&#23545;&#22024;&#26434;&#30340;&#35821;&#38899;&#36716;&#24405;&#30340;&#38887;&#24615;&#65292;&#20511;&#21161;&#26469;&#33258;lattice&#30340;&#35789;&#28151;&#28102;&#32593;&#32476;&#65292;&#24357;&#21512;&#20102;&#20351;&#29992;&#39030;&#37096;ASR&#20551;&#35774;&#21644;&#29702;&#24819;&#19978;&#38480;&#20043;&#38388;&#30340;SLU&#24615;&#33021;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;LLM&#23545;&#19981;&#21516;ASR&#24615;&#33021;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of spoken language understanding (SLU), numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM, an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification, underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR perf
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Bode&#65292;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#25552;&#31034;&#30340;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20379;&#20813;&#36153;&#29992;&#20110;&#30740;&#31350;&#25110;&#21830;&#19994;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.02909</link><description>&lt;p&gt;
&#24341;&#20837; Bode&#65306;&#38024;&#23545;&#22522;&#20110;&#33889;&#33796;&#29273;&#35821;&#25552;&#31034;&#30340;&#20219;&#21153;&#30340;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task. (arXiv:2401.02909v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02909
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Bode&#65292;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#25552;&#31034;&#30340;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20379;&#20813;&#36153;&#29992;&#20110;&#30740;&#31350;&#25110;&#21830;&#19994;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24102;&#26469;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#21363;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#24191;&#27867;&#20851;&#27880;&#25110;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#22815;&#20805;&#20998;&#30340;&#35821;&#35328;&#65288;&#22914;&#33889;&#33796;&#29273;&#35821;&#65289;&#65292;&#24050;&#32463;&#20174;LLM&#20013;&#33719;&#24471;&#20102;&#20960;&#20010;&#22909;&#22788;&#65292;&#20294;&#31243;&#24230;&#19978;&#19981;&#21450;&#20854;&#20182;&#35821;&#35328;&#12290;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;LLM&#36890;&#24120;&#38590;&#20197;&#20196;&#20154;&#28385;&#24847;&#22320;&#22238;&#24212;&#33889;&#33796;&#29273;&#35821;&#25552;&#31034;&#65292;&#20363;&#22914;&#65292;&#22312;&#22238;&#24212;&#20013;&#20986;&#29616;&#20195;&#30721;&#20999;&#25442;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#25552;&#31034;&#30340;&#31934;&#35843;LLaMA 2&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;Bode&#65292;&#20998;&#20026;7B&#21644;13B&#20004;&#20010;&#29256;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#38646;&#26679;&#26412;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;LLM&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#33889;&#33796;&#29273;&#35821;&#20013;&#20855;&#26377;&#20196;&#20154;&#28385;&#24847;&#32467;&#26524;&#30340;LLM&#65292;&#24182;&#19988;&#21487;&#20379;&#30740;&#31350;&#25110;&#21830;&#19994;&#30446;&#30340;&#20813;&#36153;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly bringing advances to Natural Language Processing. However, low-resource languages, those lacking extensive prominence in datasets for various NLP tasks, or where existing datasets are not as substantial, such as Portuguese, already obtain several benefits from LLMs, but not to the same extent. LLMs trained on multilingual datasets normally struggle to respond to prompts in Portuguese satisfactorily, presenting, for example, code switching in their responses. This work proposes a fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two versions: 7B and 13B. We evaluate the performance of this model in classification tasks using the zero-shot approach with in-context learning, and compare it with other LLMs. Our main contribution is to bring an LLM with satisfactory results in the Portuguese language, as well as to provide a model that is free for research or commercial purposes.
&lt;/p&gt;</description></item><item><title>MLLM-Protektor&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#25915;&#20987;&#12290;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;MLLM&#30340;&#26377;&#25928;&#20445;&#25252;&#65292;&#38450;&#27490;&#20854;&#20135;&#29983;&#26377;&#23475;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2401.02906</link><description>&lt;p&gt;
MLLM-Protektor: &#30830;&#20445;MLLM&#30340;&#23433;&#20840;&#24615;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance. (arXiv:2401.02906v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02906
&lt;/p&gt;
&lt;p&gt;
MLLM-Protektor&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#25915;&#20987;&#12290;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;MLLM&#30340;&#26377;&#25928;&#20445;&#25252;&#65292;&#38450;&#27490;&#20854;&#20135;&#29983;&#26377;&#23475;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLM)&#30340;&#37096;&#32626;&#24102;&#26469;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#33030;&#24369;&#24615;&#65306;&#23545;&#35270;&#35273;&#36755;&#20837;&#30340;&#24694;&#24847;&#25915;&#20987;&#26131;&#21463;&#25915;&#20987;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20445;&#25252;MLLM&#20813;&#21463;&#27492;&#31867;&#25915;&#20987;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22270;&#20687;&#20316;&#20026;&#19968;&#31181;&#8220;&#22806;&#35821;&#8221;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#27809;&#26377;&#34987;&#32771;&#34385;&#21040;&#65292;&#36825;&#21487;&#33021;&#20351;MLLM&#26356;&#23481;&#26131;&#20135;&#29983;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19982;&#25991;&#26412;&#20013;&#25152;&#32771;&#34385;&#30340;&#31163;&#25955;&#26631;&#35760;&#19981;&#21516;&#65292;&#22270;&#20687;&#20449;&#21495;&#30340;&#36830;&#32493;&#24615;&#36136;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20174;&#32780;&#20351;&#35206;&#30422;&#21487;&#33021;&#24773;&#26223;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#21152;&#21095;&#20102;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#24320;&#28304;&#30340;MLLM&#20027;&#35201;&#22312;&#26377;&#38480;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#36828;&#36828;&#23567;&#20110;&#24191;&#27867;&#30340;&#25991;&#26412;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#36825;&#20351;&#24471;MLLM&#22312;&#26126;&#30830;&#23545;&#40784;&#35843;&#25972;&#36807;&#31243;&#20013;&#26356;&#23481;&#26131;&#36951;&#24536;&#20854;&#21407;&#22987;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MLLM-Protektor&#65292;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;
&lt;/p&gt;
&lt;p&gt;
The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. We delve into the novel challenge of defending MLLMs against such attacks. We discovered that images act as a "foreign language" that is not considered during alignment, which can make MLLMs prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover the possible scenarios. This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22609;&#36896;&#20559;&#22909;&#21644;&#20010;&#24615;&#30340;&#20195;&#29702;&#26694;&#26550;&#65288;AFSPP&#65289;&#65292;&#21487;&#20197;&#25506;&#32034;&#31038;&#20132;&#32593;&#32476;&#21644;&#20027;&#35266;&#24847;&#35782;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#20559;&#22909;&#21644;&#20010;&#24615;&#24418;&#25104;&#30340;&#24433;&#21709;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;&#20960;&#20010;&#20154;&#31867;&#20010;&#24615;&#23454;&#39564;&#30340;&#20851;&#38190;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02870</link><description>&lt;p&gt;
AFSPP: &#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#22609;&#36896;&#20559;&#22909;&#21644;&#20010;&#24615;&#30340;&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models. (arXiv:2401.02870v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22609;&#36896;&#20559;&#22909;&#21644;&#20010;&#24615;&#30340;&#20195;&#29702;&#26694;&#26550;&#65288;AFSPP&#65289;&#65292;&#21487;&#20197;&#25506;&#32034;&#31038;&#20132;&#32593;&#32476;&#21644;&#20027;&#35266;&#24847;&#35782;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#20559;&#22909;&#21644;&#20010;&#24615;&#24418;&#25104;&#30340;&#24433;&#21709;&#65292;&#24182;&#25104;&#21151;&#22797;&#21046;&#20102;&#20960;&#20010;&#20154;&#31867;&#20010;&#24615;&#23454;&#39564;&#30340;&#20851;&#38190;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20223;&#30495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#21019;&#24314;&#20102;&#19968;&#20010;&#31038;&#20250;&#23398;&#30740;&#31350;&#29615;&#22659;&#65292;&#20195;&#29702;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#36807;&#28388;&#29305;&#24449;&#23637;&#31034;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24573;&#35270;&#20102;&#22312;&#31867;&#20284;&#20154;&#31867;&#29615;&#22659;&#20013;&#30340;&#36845;&#20195;&#21457;&#23637; - &#20154;&#31867;&#30340;&#20559;&#22909;&#21644;&#20010;&#24615;&#26159;&#22797;&#26434;&#30340;&#65292;&#21463;&#21040;&#21508;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22240;&#29615;&#22659;&#21644;&#20027;&#35266;&#24433;&#21709;&#32780;&#19981;&#26029;&#21464;&#21270;&#12290;&#37492;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22609;&#36896;&#20559;&#22909;&#21644;&#20010;&#24615;&#30340;&#20195;&#29702;&#26694;&#26550;&#65288;AFSPP&#65289;&#65292;&#25506;&#32034;&#31038;&#20132;&#32593;&#32476;&#21644;&#20027;&#35266;&#24847;&#35782;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#20559;&#22909;&#21644;&#20010;&#24615;&#24418;&#25104;&#30340;&#22810;&#26041;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;AFSPP&#65292;&#25105;&#20204;&#39318;&#27425;&#25104;&#21151;&#22797;&#21046;&#20102;&#20960;&#20010;&#20154;&#31867;&#20010;&#24615;&#23454;&#39564;&#30340;&#20851;&#38190;&#21457;&#29616;&#12290;&#20854;&#20182;&#22522;&#20110;AFSPP&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35745;&#21010;&#21046;&#23450;...
&lt;/p&gt;
&lt;p&gt;
The evolution of Large Language Models (LLMs) has introduced a new paradigm for investigating human behavior emulation. Recent research has employed LLM-based Agents to create a sociological research environment, in which agents exhibit behavior based on the unfiltered characteristics of large language models. However, these studies overlook the iterative development within a human-like setting - Human preferences and personalities are complex, shaped by various factors and subject to ongoing change as a result of environmental and subjective influences. In light of this observation, we propose Agent Framework for Shaping Preference and Personality (AFSPP), exploring the multifaceted impact of social networks and subjective consciousness on LLM-based Agents' preference and personality formation. With AFSPP, we have, for the first time, successfully replicated several key findings from human personality experiments. And other AFSPP-based experimental results indicate that plan making, s
&lt;/p&gt;</description></item><item><title>Pheme&#26159;&#19968;&#20010;&#39640;&#25928;&#21644;&#23545;&#35805;&#24335;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#19981;&#20165;&#20855;&#22791;&#33258;&#28982;&#30340;&#35821;&#38899;&#29983;&#25104;&#33021;&#21147;&#65292;&#36824;&#33021;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02839</link><description>&lt;p&gt;
Pheme: &#39640;&#25928;&#21644;&#23545;&#35805;&#24335;&#35821;&#38899;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pheme: Efficient and Conversational Speech Generation. (arXiv:2401.02839v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02839
&lt;/p&gt;
&lt;p&gt;
Pheme&#26159;&#19968;&#20010;&#39640;&#25928;&#21644;&#23545;&#35805;&#24335;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#19981;&#20165;&#20855;&#22791;&#33258;&#28982;&#30340;&#35821;&#38899;&#29983;&#25104;&#33021;&#21147;&#65292;&#36824;&#33021;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35821;&#38899;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29616;&#22312;&#24050;&#32463;&#23454;&#29616;&#20102;&#19968;&#27425;&#29983;&#25104;&#33021;&#21147;&#65292;&#24448;&#24448;&#20960;&#20046;&#26080;&#27861;&#21306;&#20998;&#26159;&#21542;&#20026;&#30495;&#23454;&#30340;&#20154;&#22768;&#12290;&#23558;&#36825;&#26679;&#30340;&#35821;&#38899;&#29983;&#25104;&#36827;&#23637;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#33021;&#20250;&#24443;&#24213;&#25913;&#21464;&#21508;&#31181;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#24212;&#29992;&#65292;&#22914;&#36741;&#21161;&#23545;&#35805;&#31995;&#32479;&#65292;&#38656;&#35201;&#33258;&#28982;&#21644;&#23545;&#35805;&#24335;&#35821;&#38899;&#29983;&#25104;&#24037;&#20855;&#65292;&#20063;&#38656;&#35201;&#22312;&#23454;&#26102;&#24773;&#20917;&#19979;&#36816;&#34892;&#25928;&#29575;&#39640;&#12290;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#22914;VALL-E&#21644;SoundStorm&#65292;&#30001;&#20998;&#23618;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#39537;&#21160;&#65292;&#38656;&#35201;&#22823;&#22411;&#31070;&#32463;&#32452;&#20214;&#21644;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#33391;&#22909;&#36816;&#20316;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;MQTTS&#26088;&#22312;&#26500;&#24314;&#26356;&#32039;&#20945;&#30340;&#23545;&#35805;&#24335;TTS&#27169;&#22411;&#65292;&#21516;&#26102;&#21033;&#29992;&#23567;&#35268;&#27169;&#30495;&#23454;&#23545;&#35805;&#24335;&#35821;&#38899;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20854;&#33258;&#22238;&#24402;&#24615;&#36136;&#23548;&#33268;&#25512;&#29702;&#24310;&#36831;&#39640;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, speech generation has seen remarkable progress, now achieving one-shot generation capability that is often virtually indistinguishable from real human voice. Integrating such advancements in speech generation with large language models might revolutionize a wide range of applications. However, certain applications, such as assistive conversational systems, require natural and conversational speech generation tools that also operate efficiently in real time. Current state-of-the-art models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs, require large neural components and extensive training data to work well. In contrast, MQTTS aims to build more compact conversational TTS models while capitalizing on smaller-scale real-life conversational speech data. However, its autoregressive nature yields high inference latency and thus limits its real-time usage. In order to mitigate the current limitations of the state-of-the-art TTS models while capitali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DocGraphLM&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32852;&#21512;&#32534;&#30721;&#22120;&#26550;&#26500;&#34920;&#31034;&#25991;&#26723;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#37325;&#26500;&#25991;&#26723;&#22270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#22270;&#29305;&#24449;&#21487;&#20197;&#22312;&#20449;&#24687;&#25277;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#36895;&#20102;&#25910;&#25947;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.02823</link><description>&lt;p&gt;
DocGraphLM&#65306;&#20449;&#24687;&#25277;&#21462;&#30340;&#25991;&#26723;&#22270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DocGraphLM: Documental Graph Language Model for Information Extraction. (arXiv:2401.02823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DocGraphLM&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32852;&#21512;&#32534;&#30721;&#22120;&#26550;&#26500;&#34920;&#31034;&#25991;&#26723;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#37325;&#26500;&#25991;&#26723;&#22270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#22270;&#29305;&#24449;&#21487;&#20197;&#22312;&#20449;&#24687;&#25277;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#36895;&#20102;&#25910;&#25947;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#29702;&#35299;(VrDU)&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#23545;&#20855;&#26377;&#22797;&#26434;&#24067;&#23616;&#30340;&#25991;&#26723;&#36827;&#34892;&#20449;&#24687;&#25277;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#25104;&#20026;&#21487;&#33021;&#12290;&#20986;&#29616;&#20102;&#20004;&#31181;&#26550;&#26500;&#30340;&#27169;&#24335;-&#21463;LLM&#21551;&#21457;&#30340;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;DocGraphLM&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#35821;&#20041;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;1)&#19968;&#31181;&#32852;&#21512;&#32534;&#30721;&#22120;&#26550;&#26500;&#26469;&#34920;&#31034;&#25991;&#26723;&#65292;&#20197;&#21450;2)&#19968;&#31181;&#26032;&#39062;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#26469;&#37325;&#26500;&#25991;&#26723;&#22270;&#12290;DocGraphLM&#20351;&#29992;&#19968;&#20010;&#25910;&#25947;&#30340;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#39044;&#27979;&#33410;&#28857;&#20043;&#38388;&#30340;&#26041;&#21521;&#21644;&#36317;&#31163;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#20248;&#20808;&#32771;&#34385;&#37051;&#22495;&#24674;&#22797;&#24182;&#20943;&#36731;&#36828;&#31243;&#33410;&#28857;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#22270;&#29305;&#24449;&#21487;&#20197;&#22312;&#20449;&#24687;&#25277;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#19978;&#20445;&#25345;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25253;&#21578;&#35828;&#65292;&#23613;&#31649;&#20165;&#30001;&#26500;&#24314;&#32780;&#26469;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#22270;&#29305;&#24449;&#21487;&#20197;&#21152;&#24555;&#25910;&#25947;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in Visually Rich Document Understanding (VrDU) have enabled information extraction and question answering over documents with complex layouts. Two tropes of architectures have emerged -- transformer-based models inspired by LLMs, and Graph Neural Networks. In this paper, we introduce DocGraphLM, a novel framework that combines pre-trained language models with graph semantics. To achieve this, we propose 1) a joint encoder architecture to represent documents, and 2) a novel link prediction approach to reconstruct document graphs. DocGraphLM predicts both directions and distances between nodes using a convergent joint loss function that prioritizes neighborhood restoration and downweighs distant node detection. Our experiments on three SotA datasets show consistent improvement on IE and QA tasks with the adoption of graph features. Moreover, we report that adopting the graph features accelerates convergence in the learning process during training, despite being solely constructe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#19978;&#27604;GPT-4v&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;26&#65285;&#12290;</title><link>http://arxiv.org/abs/2401.02797</link><description>&lt;p&gt;
PeFoMed&#65306;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering. (arXiv:2401.02797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#19978;&#27604;GPT-4v&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;26&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19978;&#36827;&#34892;&#20102;&#36827;&#21270;&#25193;&#23637;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#24212;&#23545;&#36229;&#36234;&#32431;&#25991;&#26412;&#24212;&#29992;&#33539;&#22260;&#30340;&#25361;&#25112;&#12290;&#23427;&#21033;&#29992;&#20102;&#20808;&#21069;&#32534;&#30721;&#22312;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#21644;&#21151;&#33021;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;MLLMs&#36866;&#24212;&#20026;&#29983;&#25104;&#20219;&#21153;&#20197;&#35299;&#20915;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;Med-VQA&#65289;&#20219;&#21153;&#30340;&#33258;&#30001;&#24418;&#24335;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Med-VQA&#24212;&#29992;&#29305;&#21035;&#23450;&#21046;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#24182;&#22312;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#20026;&#20102;&#20934;&#30830;&#34913;&#37327;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23553;&#38381;&#24335;&#38382;&#39064;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#19978;&#36798;&#21040;&#20102;81.9&#65285;&#65292;&#19988;&#20854;&#30456;&#23545;&#20110;GPT-4v&#27169;&#22411;&#30340;&#32477;&#23545;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;26&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models (MLLMs) represent an evolutionary expansion in the capabilities of traditional large language models, enabling them to tackle challenges that surpass the scope of purely text-based applications. It leverages the knowledge previously encoded within these language models, thereby enhancing their applicability and functionality in the reign of multimodal contexts. Recent works investigate the adaptation of MLLMs to predict free-form answers as a generative task to solve medical visual question answering (Med-VQA) tasks. In this paper, we propose a parameter efficient framework for fine-tuning MLLM specifically tailored to Med-VQA applications, and empirically validate it on a public benchmark dataset. To accurately measure the performance, we employ human evaluation and the results reveal that our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v model by a significant margin of 26% absolute accuracy on closed-ended questions. The cod
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26893;&#29289;&#29983;&#29289;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#26893;&#29289;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#24378;&#22823;&#30340;&#39044;&#27979;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.02789</link><description>&lt;p&gt;
&#26893;&#29289;&#29983;&#29289;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Plant Biology. (arXiv:2401.02789v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02789
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26893;&#29289;&#29983;&#29289;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#26893;&#29289;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#24378;&#22823;&#30340;&#39044;&#27979;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#24109;&#21367;&#20840;&#29699;&#65292;&#24182;&#36890;&#36807;&#26576;&#20123;&#24418;&#24335;&#30340;&#22270;&#28789;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;LLM&#19981;&#20165;&#23616;&#38480;&#20110;&#20154;&#31867;&#35821;&#35328;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;DNA&#12289;&#34507;&#30333;&#36136;&#21644;&#22522;&#22240;&#34920;&#36798;&#31561;&#39034;&#24207;&#25968;&#25454;&#12290;&#25152;&#24471;&#21040;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#34987;&#37325;&#26032;&#21033;&#29992;&#26469;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#65292;&#20174;&#32780;&#24418;&#25104;&#24378;&#22823;&#30340;&#22810;&#21151;&#33021;&#39044;&#27979;&#24037;&#20855;&#65292;&#33021;&#22815;&#35299;&#37322;&#32454;&#32990;&#31995;&#32479;&#12290;&#26412;&#32508;&#36848;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;LLM&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#29983;&#29289;&#23398;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#12290;&#32771;&#34385;&#21040;LLM&#23578;&#26410;&#22312;&#26893;&#29289;&#23398;&#30028;&#34987;&#25509;&#21463;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#26893;&#29289;&#29579;&#22269;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, have taken the world by storm and have passed certain forms of the Turing test. However, LLMs are not limited to human language and analyze sequential data, such as DNA, protein, and gene expression. The resulting foundation models can be repurposed to identify the complex patterns within the data, resulting in powerful, multi-purpose prediction tools able to explain cellular systems. This review outlines the different types of LLMs and showcases their recent uses in biology. Since LLMs have not yet been embraced by the plant community, we also cover how these models can be deployed for the plant kingdom.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RAISE&#30340;&#26550;&#26500;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#25972;&#21512;&#21040;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#32452;&#20214;&#35760;&#24518;&#31995;&#32479;&#26469;&#22686;&#24378;&#20195;&#29702;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21487;&#25511;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#39044;liminary evaluations&#34920;&#26126;&#65292;RAISE&#22312;&#25151;&#22320;&#20135;&#38144;&#21806;&#39046;&#22495;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.02777</link><description>&lt;p&gt;
&#20174;LLM&#21040;&#23545;&#35805;&#20195;&#29702;&#65306;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#35760;&#24518;&#22686;&#24378;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. (arXiv:2401.02777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RAISE&#30340;&#26550;&#26500;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#25972;&#21512;&#21040;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#32452;&#20214;&#35760;&#24518;&#31995;&#32479;&#26469;&#22686;&#24378;&#20195;&#29702;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21487;&#25511;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#39044;liminary evaluations&#34920;&#26126;&#65292;RAISE&#22312;&#25151;&#22320;&#20135;&#38144;&#21806;&#39046;&#22495;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RAISE&#65288;Scratchpad&#21644;Examples&#36741;&#21161;&#25512;&#29702;&#21644;&#34892;&#20026;&#65289;,&#19968;&#31181;&#20808;&#36827;&#30340;&#26550;&#26500;&#65292;&#22686;&#24378;&#20102;&#23558;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#23545;&#35805;&#20195;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;RAISE&#26159;ReAct&#26694;&#26550;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#21253;&#25324;&#19968;&#20010;&#21452;&#32452;&#20214;&#35760;&#24518;&#31995;&#32479;&#65292;&#27169;&#20223;&#20154;&#31867;&#30340;&#30701;&#26399;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#20445;&#25345;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#21644;&#36830;&#32493;&#24615;&#12290;&#23427;&#21253;&#25324;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20195;&#29702;&#26500;&#24314;&#24773;&#26223;&#65292;&#21253;&#25324;&#23545;&#35805;&#36873;&#25321;&#65292;&#22330;&#26223;&#25552;&#21462;&#65292;CoT&#23436;&#25104;&#21644;&#22330;&#26223;&#22686;&#24378;&#31561;&#38454;&#27573;&#65292;&#26368;&#32456;&#23548;&#33268;LLMs&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;&#36825;&#31181;&#26041;&#27861;&#20284;&#20046;&#25552;&#39640;&#20102;&#20195;&#29702;&#22312;&#22797;&#26434;&#30340;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21487;&#25511;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#22312;&#25151;&#22320;&#20135;&#38144;&#21806;&#29615;&#22659;&#20013;&#30340;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#65292;RAISE&#30456;&#23545;&#20110;&#20256;&#32479;&#20195;&#29702;&#26377;&#19968;&#20123;&#20248;&#21183;&#65292;&#34920;&#26126;&#23427;&#22312;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#26469;&#24320;&#21457;&#26356;&#20855;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#22810;&#21151;&#33021;&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;AI&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile convers
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20174;&#22797;&#26434;&#24615;&#31185;&#23398;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#21253;&#25324;&#35789;&#39057;&#21644;&#30456;&#20851;&#24615;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#22797;&#26434;&#31995;&#32479;&#29983;&#25104;&#30340;&#20449;&#21495;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.02772</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#31995;&#32479;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Complex systems approach to natural language. (arXiv:2401.02772v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20174;&#22797;&#26434;&#24615;&#31185;&#23398;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#21253;&#25324;&#35789;&#39057;&#21644;&#30456;&#20851;&#24615;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#22797;&#26434;&#31995;&#32479;&#29983;&#25104;&#30340;&#20449;&#21495;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20174;&#22797;&#26434;&#24615;&#31185;&#23398;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#25152;&#20351;&#29992;&#30340;&#20027;&#35201;&#26041;&#27861;&#35770;&#27010;&#24565;&#65292;&#24182;&#35760;&#24405;&#20102;&#23427;&#20204;&#22312;&#35782;&#21035;&#35821;&#35328;&#22312;&#20070;&#38754;&#34920;&#36798;&#20013;&#30340;&#36890;&#29992;&#29305;&#24449;&#21644;&#31995;&#32479;&#29305;&#23450;&#29305;&#24449;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#28085;&#30422;&#20102;&#25968;&#37327;&#35821;&#35328;&#23398;&#20013;&#19982;&#22797;&#26434;&#24615;&#30456;&#20851;&#30340;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#36235;&#21183;&#12290;&#31532;&#19968;&#37096;&#20998;&#35752;&#35770;&#20102;&#25991;&#26412;&#20013;&#30340;&#35789;&#39057;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#32771;&#34385;&#26631;&#28857;&#31526;&#21495;&#21487;&#20197;&#24674;&#22797;&#36829;&#21453;Zipf&#23450;&#24459;&#30340;&#26368;&#24120;&#35265;&#35789;&#35821;&#30340;&#27604;&#20363;&#12290;&#31532;&#20108;&#37096;&#20998;&#20171;&#32461;&#20102;&#21463;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#20070;&#38754;&#25991;&#26412;&#20013;&#30340;&#21508;&#31181;&#30456;&#20851;&#24615;&#12290;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#26159;&#22522;&#20110;&#23558;&#25991;&#26412;&#20998;&#21106;&#20026;&#21477;&#23376;&#25110;&#36830;&#32493;&#26631;&#28857;&#20043;&#38388;&#30340;&#30701;&#35821;&#32780;&#29983;&#25104;&#30340;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#24207;&#21015;&#20855;&#26377;&#22797;&#26434;&#31995;&#32479;&#29983;&#25104;&#30340;&#20449;&#21495;&#20013;&#24120;&#35265;&#30340;&#38271;&#31243;&#30456;&#20851;&#24615;&#25110;&#65288;&#22810;&#65289;&#20998;&#24418;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The review summarizes the main methodological concepts used in studying natural language from the perspective of complexity science and documents their applicability in identifying both universal and system-specific features of language in its written representation. Three main complexity-related research trends in quantitative linguistics are covered. The first part addresses the issue of word frequencies in texts and demonstrates that taking punctuation into consideration restores scaling whose violation in the Zipf's law is often observed for the most frequent words. The second part introduces methods inspired by time series analysis, used in studying various kinds of correlations in written texts. The related time series are generated on the basis of text partition into sentences or into phrases between consecutive punctuation marks. It turns out that these series develop features often found in signals generated by complex systems, like long-range correlations or (multi)fractal st
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#36817;&#20284;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;AMBR&#65289;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#24555;&#22320;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36845;&#20195;&#28040;&#38500;&#27861;&#31639;&#27861;&#26469;&#35299;&#20915;&#20013;&#20301;&#25968;&#35782;&#21035;&#38382;&#39064;&#65292;&#20197;&#36798;&#21040;&#21152;&#36895;&#35299;&#30721;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.02749</link><description>&lt;p&gt;
&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#26356;&#24555;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding. (arXiv:2401.02749v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#36817;&#20284;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;AMBR&#65289;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#24555;&#22320;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36845;&#20195;&#28040;&#38500;&#27861;&#31639;&#27861;&#26469;&#35299;&#20915;&#20013;&#20301;&#25968;&#35782;&#21035;&#38382;&#39064;&#65292;&#20197;&#36798;&#21040;&#21152;&#36895;&#35299;&#30721;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#22312;&#24191;&#27867;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#26367;&#20195;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;MBR&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#35745;&#31639;MBR&#30446;&#26631;&#65292;&#36825;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#38656;&#35201;&#21709;&#24212;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#19981;&#21487;&#34892;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#21098;&#26525;(CBP)&#26041;&#27861;&#26469;&#38477;&#20302;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#23427;&#33021;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#20294;&#26159;&#23427;&#38656;&#35201;&#20351;&#29992;&#24320;&#21457;&#38598;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#25165;&#33021;&#21457;&#25381;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#36817;&#20284;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;AMBR&#65289;&#35299;&#30721;&#26041;&#27861;&#12290;AMBR&#22522;&#20110;&#20197;&#19979;&#35266;&#23519;&#24471;&#20986;&#65306;&#35745;&#31639;&#22522;&#20110;&#26679;&#26412;&#30340;MBR&#30446;&#26631;&#30340;&#38382;&#39064;&#26159;&#20013;&#20301;&#25968;&#35782;&#21035;&#38382;&#39064;&#12290;AMBR&#20351;&#29992;&#20102;&#36845;&#20195;&#28040;&#38500;&#27861;&#65288;CSH&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to beam search decoding for a wide range of text generation tasks. However, MBR requires a huge amount of time for inference to compute the MBR objective, which makes the method infeasible in many situations where response time is critical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently been proposed to reduce the inference time in machine translation tasks. Although it is shown to significantly reduce the amount of computation, it requires hyperparameter tuning using a development set to be effective. To this end, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a hyperparameter-free method to run MBR decoding approximately. AMBR is derived from the observation that the problem of computing the sample-based MBR objective is the medoid identification problem. AMBR uses the Correlated Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best approximation algorithm to date
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MAMI&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27880;&#24847;&#21147;&#20114;&#20449;&#24687;&#29992;&#20110;&#38271;&#24207;&#21015;&#31070;&#32463;&#20803;&#23383;&#24149;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#31867;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;&#22810;&#20010;&#27880;&#24847;&#21147;&#32467;&#26524;&#30340;&#27719;&#32858;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;MILAN&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02744</link><description>&lt;p&gt;
MAMI: &#22810;&#27880;&#24847;&#21147;&#20114;&#20449;&#24687;&#29992;&#20110;&#38271;&#24207;&#21015;&#31070;&#32463;&#20803;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron Captioning. (arXiv:2401.02744v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MAMI&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27880;&#24847;&#21147;&#20114;&#20449;&#24687;&#29992;&#20110;&#38271;&#24207;&#21015;&#31070;&#32463;&#20803;&#23383;&#24149;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#31867;&#22411;&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;&#22810;&#20010;&#27880;&#24847;&#21147;&#32467;&#26524;&#30340;&#27719;&#32858;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;MILAN&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20803;&#26631;&#35760;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#29305;&#23450;&#31070;&#32463;&#20803;&#23545;&#28608;&#27963;&#35813;&#31070;&#32463;&#20803;&#30340;&#27169;&#24335;&#30340;&#34892;&#20026;&#21644;&#21453;&#24212;&#30340;&#26041;&#27861;&#12290;&#31070;&#32463;&#20803;&#26631;&#35760;&#25552;&#21462;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26576;&#20123;&#31070;&#32463;&#20803;&#25429;&#33719;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#20854;&#20013;&#20043;&#19968;&#20351;&#29992;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;&#32534;&#30721;&#22120;&#21487;&#20197;&#26159;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#65292;&#35299;&#30721;&#22120;&#26159;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#21363;MILAN&#65288;Mutual Information-guided Linguistic Annotation of Neuron&#65289;&#65292;&#23581;&#35797;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;Show, Attend, and Tell&#65288;SAT&#65289;&#27169;&#22411;&#36827;&#34892;&#31070;&#32463;&#20803;&#34892;&#20026;&#30340;&#21487;&#35270;&#21270;&#65292;&#22312;&#32534;&#30721;&#22120;&#20013;&#20351;&#29992;LSTM&#19982;Bahdanau&#27880;&#24847;&#21147;&#12290;MILAN&#22312;&#30701;&#24207;&#21015;&#31070;&#32463;&#20803;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#38271;&#24207;&#21015;&#31070;&#32463;&#20803;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#27492;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#39069;&#22806;&#28155;&#21152;&#33509;&#24178;&#27880;&#24847;&#21147;&#32467;&#26524;&#26469;&#36827;&#19968;&#27493;&#25552;&#21319;MILAN&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuron labeling is an approach to visualize the behaviour and respond of a certain neuron to a certain pattern that activates the neuron. Neuron labeling extract information about the features captured by certain neurons in a deep neural network, one of which uses the encoder-decoder image captioning approach. The encoder used can be a pretrained CNN-based model and the decoder is an RNN-based model for text generation. Previous work, namely MILAN (Mutual Information-guided Linguistic Annotation of Neuron), has tried to visualize the neuron behaviour using modified Show, Attend, and Tell (SAT) model in the encoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show great result on short sequence neuron captioning, but it does not show great result on long sequence neuron captioning, so in this work, we would like to improve the performance of MILAN even more by utilizing different kind of attention mechanism and additionally adding several attention result into one, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#19981;&#21516;&#39046;&#22495;&#20013;&#32858;&#31867;&#24503;&#35821;&#25991;&#26412;&#23884;&#20837;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#34920;&#29616;&#24378;&#21170;&#65292;&#32553;&#20943;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#25552;&#39640;&#32858;&#31867;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#24503;&#35821;BERT&#27169;&#22411;&#21487;&#33021;&#23545;&#30701;&#25991;&#26412;&#24615;&#33021;&#26377;&#26174;&#33879;&#25913;&#21892;&#12290;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#22343;&#20844;&#24320;&#21487;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02709</link><description>&lt;p&gt;
&#24503;&#35821;&#25991;&#26412;&#23884;&#20837;&#32858;&#31867;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
German Text Embedding Clustering Benchmark. (arXiv:2401.02709v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#19981;&#21516;&#39046;&#22495;&#20013;&#32858;&#31867;&#24503;&#35821;&#25991;&#26412;&#23884;&#20837;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#34920;&#29616;&#24378;&#21170;&#65292;&#32553;&#20943;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#25552;&#39640;&#32858;&#31867;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#24503;&#35821;BERT&#27169;&#22411;&#21487;&#33021;&#23545;&#30701;&#25991;&#26412;&#24615;&#33021;&#26377;&#26174;&#33879;&#25913;&#21892;&#12290;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#22343;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#32858;&#31867;&#24503;&#35821;&#25991;&#26412;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#26159;&#30001;&#23545;&#32858;&#31867;&#31070;&#32463;&#25991;&#26412;&#23884;&#20837;&#22312;&#38656;&#35201;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#32452;&#65288;&#22914;&#20027;&#39064;&#24314;&#27169;&#65289;&#30340;&#20219;&#21153;&#20013;&#30340;&#22686;&#21152;&#20351;&#29992;&#25152;&#39537;&#21160;&#30340;&#65292;&#24182;&#19988;&#22312;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#20013;&#38656;&#35201;&#24503;&#35821;&#36164;&#28304;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#39044;&#35757;&#32451;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#22312;&#19981;&#21516;&#32858;&#31867;&#31639;&#27861;&#30340;&#32467;&#26524;&#19978;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#12290;&#32467;&#26524;&#21253;&#25324;&#34920;&#29616;&#24378;&#21170;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#12290;&#32553;&#20943;&#23884;&#20837;&#30340;&#32500;&#24230;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#32858;&#31867;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19982;&#24503;&#35821;BERT&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#36825;&#31181;&#39069;&#22806;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#20110;&#30701;&#25991;&#26412;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#22343;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a benchmark assessing the performance of clustering German text embeddings in different domains. This benchmark is driven by the increasing use of clustering neural text embeddings in tasks that require the grouping of texts (such as topic modeling) and the need for German resources in existing benchmarks. We provide an initial analysis for a range of pre-trained mono- and multilingual models evaluated on the outcome of different clustering algorithms. Results include strong performing mono- and multilingual models. Reducing the dimensions of embeddings can further improve clustering. Additionally, we conduct experiments with continued pre-training for German BERT models to estimate the benefits of this additional training. Our experiments suggest that significant performance improvements are possible for short text. All code and datasets are publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;TF-IDF&#26816;&#32034;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#30828;&#36127;&#26679;&#26412;&#22686;&#24378;&#26041;&#27861;UNA&#65292;&#36890;&#36807;&#26367;&#25442;&#21477;&#23376;&#20013;&#30340;&#26415;&#35821;&#29983;&#25104;&#21512;&#25104;&#30340;&#36127;&#26679;&#26412;&#65292;&#23454;&#39564;&#35777;&#26126;UNA&#26041;&#27861;&#33021;&#25552;&#39640;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#19988;&#19982;&#37322;&#20041;&#22686;&#24378;&#30456;&#32467;&#21512;&#21487;&#33719;&#24471;&#39069;&#22806;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.02594</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30828;&#36127;&#26679;&#26412;&#22686;&#24378;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised hard Negative Augmentation for contrastive learning. (arXiv:2401.02594v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;TF-IDF&#26816;&#32034;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#30828;&#36127;&#26679;&#26412;&#22686;&#24378;&#26041;&#27861;UNA&#65292;&#36890;&#36807;&#26367;&#25442;&#21477;&#23376;&#20013;&#30340;&#26415;&#35821;&#29983;&#25104;&#21512;&#25104;&#30340;&#36127;&#26679;&#26412;&#65292;&#23454;&#39564;&#35777;&#26126;UNA&#26041;&#27861;&#33021;&#25552;&#39640;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#19988;&#19982;&#37322;&#20041;&#22686;&#24378;&#30456;&#32467;&#21512;&#21487;&#33719;&#24471;&#39069;&#22806;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#30828;&#36127;&#26679;&#26412;&#22686;&#24378;&#65288;UNA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#26816;&#32034;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#30340;&#36127;&#26679;&#26412;&#12290;UNA&#20351;&#29992;TF-IDF&#20998;&#25968;&#30830;&#23450;&#21477;&#23376;&#20013;&#26415;&#35821;&#30340;&#24863;&#30693;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#26415;&#35821;&#26469;&#29983;&#25104;&#36127;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;UNA&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#24403;&#23558;UNA&#19982;&#37322;&#20041;&#22686;&#24378;&#30456;&#32467;&#21512;&#26102;&#65292;&#36824;&#21487;&#33719;&#24471;&#39069;&#22806;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36827;&#19968;&#27493;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#19981;&#21516;&#30340;&#39592;&#24178;&#27169;&#22411;&#20860;&#23481;&#12290;&#28040;&#34701;&#30740;&#31350;&#36824;&#25903;&#25345;&#22312;&#36127;&#26679;&#26412;&#22686;&#24378;&#20013;&#20855;&#26377;&#22522;&#20110;TF-IDF&#30340;&#25511;&#21046;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Unsupervised hard Negative Augmentation (UNA), a method that generates synthetic negative instances based on the term frequency-inverse document frequency (TF-IDF) retrieval model. UNA uses TF-IDF scores to ascertain the perceived importance of terms in a sentence and then produces negative samples by replacing terms with respect to that. Our experiments demonstrate that models trained with UNA improve the overall performance in semantic textual similarity tasks. Additional performance gains are obtained when combining UNA with the paraphrasing augmentation. Further results show that our method is compatible with different backbone models. Ablation studies also support the choice of having a TF-IDF-driven control on negative augmentation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#23572;&#25991;&#30340;&#35760;&#24518;&#29702;&#35770;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#24847;&#35782;&#21487;&#33021;&#26159;&#19968;&#31181;&#22522;&#20110;&#36825;&#31181;&#23545;&#24212;&#20851;&#31995;&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#29468;&#24819;&#12290;</title><link>http://arxiv.org/abs/2401.02509</link><description>&lt;p&gt;
&#35760;&#24518;&#12289;&#24847;&#35782;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Memory, Consciousness and Large Language Model. (arXiv:2401.02509v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02509
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#23572;&#25991;&#30340;&#35760;&#24518;&#29702;&#35770;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#24847;&#35782;&#21487;&#33021;&#26159;&#19968;&#31181;&#22522;&#20110;&#36825;&#31181;&#23545;&#24212;&#20851;&#31995;&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35748;&#30693;&#31185;&#23398;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#32852;&#31995;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#25581;&#31034;&#20986;&#26469;&#12290;&#22312;&#36825;&#20123;&#32852;&#31995;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29468;&#24819;&#65292;&#21363;LLM&#21644;&#22270;&#23572;&#25991;&#30340;&#35760;&#24518;&#29702;&#35770;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#23545;&#20598;&#20851;&#31995;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#23572;&#25991;&#30340;&#21327;&#21516;&#24341;&#21457;&#65288;SEM&#65289;&#26816;&#32034;&#27169;&#22411;&#21644;LLM&#20013;&#35266;&#23519;&#21040;&#30340;&#26032;&#20852;&#33021;&#21147;&#20043;&#38388;&#30340;&#28508;&#22312;&#23545;&#24212;&#20851;&#31995;&#65292;&#20026;&#25105;&#20204;&#30340;&#29468;&#24819;&#25552;&#20379;&#20102;&#25903;&#25345;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#27979;&#24847;&#35782;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#36825;&#31181;&#23545;&#20598;&#24615;&#30340;&#19968;&#31181;&#26032;&#20852;&#33021;&#21147;&#24418;&#24335;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20854;&#20182;&#24847;&#35782;&#29702;&#35770;&#22914;&#20309;&#19982;&#25105;&#20204;&#30340;&#30740;&#31350;&#30456;&#20132;&#21449;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development in cognitive science and Large Language Models (LLMs), increasing connections have come to light between these two distinct fields. Building upon these connections, we propose a conjecture suggesting the existence of a duality between LLMs and Tulving's theory of memory. We identify a potential correspondence between Tulving's synergistic ecphory model (SEM) of retrieval and the emergent abilities observed in LLMs, serving as supporting evidence for our conjecture. Furthermore, we speculate that consciousness may be considered a form of emergent ability based on this duality. We also discuss how other theories of consciousness intersect with our research.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#21644;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#22825;&#25991;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#25193;&#23637;&#20102;AstroLLaMA&#65292;&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#30340;LLaMA-2&#27169;&#22411;&#21644;&#19987;&#38376;&#30340;&#22825;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19987;&#38376;&#20027;&#39064;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#21457;&#24067;&#20102;&#24102;&#26377;&#32842;&#22825;&#21151;&#33021;&#30340;AstroLLaMA&#12290;</title><link>http://arxiv.org/abs/2401.01916</link><description>&lt;p&gt;
AstroLLaMA-Chat: &#20351;&#29992;&#23545;&#35805;&#21644;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#25193;&#23637;AstroLLaMA
&lt;/p&gt;
&lt;p&gt;
AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets. (arXiv:2401.01916v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01916
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#21644;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#22825;&#25991;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#25193;&#23637;&#20102;AstroLLaMA&#65292;&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#30340;LLaMA-2&#27169;&#22411;&#21644;&#19987;&#38376;&#30340;&#22825;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19987;&#38376;&#20027;&#39064;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#21457;&#24067;&#20102;&#24102;&#26377;&#32842;&#22825;&#21151;&#33021;&#30340;AstroLLaMA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#21644;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#22825;&#25991;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#22686;&#24378;LLM&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#32039;&#20945;&#30340;7B&#21442;&#25968;&#30340;LLaMA-2&#27169;&#22411;&#65292;&#24182;&#19988;&#19987;&#27880;&#20110;&#19968;&#32452;&#32463;&#36807;&#31579;&#36873;&#30340;&#22825;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#20171;&#32461;&#21644;&#32467;&#35770;&#65292;&#25105;&#20204;&#22312;&#19987;&#38376;&#20027;&#39064;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#34429;&#28982;&#20687;GPT-4&#36825;&#26679;&#30340;&#36890;&#29992;LLMs&#22312;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#22238;&#31572;&#22330;&#26223;&#20013;&#30001;&#20110;&#26356;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#26377;&#38480;&#36164;&#28304;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19987;&#38376;&#20027;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AstroLLaMA&#30340;&#25193;&#23637;&#65306;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23545;7B LLaMA&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#26368;&#32456;&#21457;&#24067;&#20102;&#36866;&#29992;&#20110;&#31038;&#21306;&#20351;&#29992;&#30340;&#20855;&#26377;&#32842;&#22825;&#21151;&#33021;&#30340;AstroLLaMA&#12290;&#20840;&#38754;&#30340;&#23450;&#37327;&#22522;&#20934;&#27979;&#35797;&#27491;&#22312;&#36827;&#34892;&#20013;&#65292;&#24182;&#23558;&#22312;&#21363;&#23558;&#21457;&#24067;&#30340;&#23436;&#25972;&#35770;&#25991;&#20013;&#35814;&#32454;&#20171;&#32461;&#12290;&#27169;&#22411;AstroLLaMA-Chat&#29616;&#24050;&#22312;...
&lt;/p&gt;
&lt;p&gt;
We explore the potential of enhancing LLM performance in astronomy-focused question-answering through targeted, continual pre-training. By employing a compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of astronomy corpus -- comprising abstracts, introductions, and conclusions -- we achieve notable improvements in specialized topic comprehension. While general LLMs like GPT-4 outperform in broader question-answering scenarios due to superior reasoning capabilities, our findings suggest that continual pre-training with limited resources can still enhance model performance on specialized topics. Additionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B LLaMA model on a domain-specific conversational dataset, culminating in the release of the chat-enabled AstroLLaMA for community use. Comprehensive quantitative benchmarking is currently in progress and will be detailed in an upcoming full paper. The model, AstroLLaMA-Chat, is now available at
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#22270;&#34920;&#65292;&#25506;&#35752;&#20102;&#21512;&#35789;&#21644;&#38899;&#38901;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#21407;&#22240;&#65292;&#24182;&#25903;&#25345;&#20102;&#19968;&#39033;&#20808;&#21069;&#24050;&#26377;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2401.01698</link><description>&lt;p&gt;
&#19990;&#30028;&#35821;&#35328;&#30340;&#25345;&#20037;&#24615;&#21644;&#25193;&#25955;&#24615;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Patterns of Persistence and Diffusibility across World's Languages. (arXiv:2401.01698v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#22270;&#34920;&#65292;&#25506;&#35752;&#20102;&#21512;&#35789;&#21644;&#38899;&#38901;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#21407;&#22240;&#65292;&#24182;&#25903;&#25345;&#20102;&#19968;&#39033;&#20808;&#21069;&#24050;&#26377;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#30340;&#30456;&#20284;&#24615;&#21487;&#33021;&#26159;&#30001;&#20110;&#36951;&#20256;&#20851;&#31995;&#12289;&#21306;&#22495;&#25509;&#35302;&#12289;&#26222;&#36941;&#24615;&#25110;&#20598;&#28982;&#24615;&#12290;&#21512;&#35789;&#26159;&#19968;&#31181;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#30456;&#20284;&#24615;&#31867;&#22411;&#65292;&#21363;&#20351;&#29992;&#19968;&#20010;&#35789;&#27719;&#24418;&#24335;&#26469;&#34920;&#36798;&#22810;&#20010;&#24847;&#20041;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;&#23478;&#26063;&#31283;&#23450;&#24615;&#65288;&#25345;&#20037;&#24615;&#65289;&#21644;&#25509;&#35302;&#35825;&#21457;&#21464;&#21270;&#65288;&#25193;&#25955;&#24615;&#65289;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21512;&#35789;&#21644;&#38899;&#38901;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#21407;&#22240;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#28085;&#30422;1966&#31181;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22270;&#65292;&#21253;&#25324;&#35821;&#20041;&#12289;&#23478;&#26063;&#12289;&#38899;&#38901;&#21644;&#22320;&#29702;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#23398;&#39046;&#22495;&#20197;&#21069;&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#24050;&#24314;&#31435;&#30340;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#20551;&#35774;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#36164;&#28304;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#28872;&#25903;&#25345;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#19968;&#20010;&#20043;&#21069;&#24050;&#24314;&#31435;&#30340;&#20551;&#35774;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21478;&#19968;&#20010;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#36164;&#28304;&#20026;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#27604;&#36739;&#35821;&#35328;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language similarities can be caused by genetic relatedness, areal contact, universality, or chance. Colexification, i.e.~a type of similarity where a single lexical form is used to convey multiple meanings, is underexplored. In our work, we shed light on the linguistic causes of cross-lingual similarity in colexification and phonology, by exploring genealogical stability (persistence) and contact-induced change (diffusibility). We construct large-scale graphs incorporating semantic, genealogical, phonological and geographical data for 1,966 languages. We then show the potential of this resource, by investigating several established hypotheses from previous work in linguistics, while proposing new ones. Our results strongly support a previously established hypothesis in the linguistic literature, while offering contradicting evidence to another. Our large scale resource opens for further research across disciplines, e.g.~in multilingual NLP and comparative linguistics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.01623</link><description>&lt;p&gt;
AI&#26159;&#21542;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#20855;&#22791;&#21019;&#36896;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#21147;&#26159;&#31038;&#20250;&#36827;&#27493;&#21644;&#21019;&#26032;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#20027;&#35266;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33021;&#22815;&#23436;&#25104;&#26366;&#32463;&#21482;&#23646;&#20110;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#20219;&#21153;&#65292;&#25506;&#32034;AI&#30340;&#21019;&#36896;&#28508;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20854;&#36127;&#36131;&#20219;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#8220;&#30456;&#23545;&#21019;&#36896;&#21147;&#8221;&#30340;&#26032;&#27010;&#24565;&#26469;&#35299;&#20915;&#23450;&#20041;&#21644;&#35780;&#20272;&#21019;&#36896;&#21147;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#19981;&#20877;&#35797;&#22270;&#23545;&#21019;&#36896;&#21147;&#36827;&#34892;&#26222;&#36941;&#23450;&#20041;&#65292;&#32780;&#26159;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#19968;&#20301;&#20551;&#35774;&#30340;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;&#36825;&#31181;&#35266;&#28857;&#20511;&#37492;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#24605;&#24819;&#65292;&#24182;&#25193;&#23637;&#20854;&#33539;&#22260;&#20197;&#35299;&#20915;&#35780;&#20272;&#21019;&#36896;&#21147;&#20013;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#21644;&#20027;&#35266;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#36716;&#21464;&#20351;&#24471;&#23545;AI&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#32479;&#35745;&#21019;&#36896;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;AI&#19982;&#29305;&#23450;&#20154;&#31867;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creativity serves as a cornerstone for societal progress and innovation, but its assessment remains a complex and often subjective endeavor. With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application. This paper addresses the complexities in defining and evaluating creativity by introducing a new concept called Relative Creativity. Instead of trying to define creativity universally, we shift the focus to whether AI can match the creative abilities of a hypothetical human. This perspective draws inspiration from the Turing Test, expanding upon it to address the challenges and subjectivities inherent in evaluating creativity. This methodological shift facilitates a statistically quantifiable evaluation of AI's creativity, which we term Statistical Creativity. This approach allows for direct comparisons of AI's creative abilities with those of sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#24182;&#25353;&#38656;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#24182;&#20248;&#21270;&#25968;&#25454;&#20256;&#36755;&#21644;&#35835;&#21462;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#31383;&#21475;&#21270;&#21644;&#34892;&#21015;&#32465;&#23450;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2312.11514</link><description>&lt;p&gt;
&#38378;&#23384;LLM&#65306;&#22312;&#26377;&#38480;&#20869;&#23384;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#24182;&#25353;&#38656;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#24182;&#20248;&#21270;&#25968;&#25454;&#20256;&#36755;&#21644;&#35835;&#21462;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#31383;&#21475;&#21270;&#21644;&#34892;&#21015;&#32465;&#23450;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;DRAM&#23481;&#37327;&#30340;&#35774;&#22791;&#32780;&#35328;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#65292;&#24182;&#25353;&#38656;&#23558;&#20854;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#36229;&#36807;&#21487;&#29992;DRAM&#23481;&#37327;&#30340;LLM&#39640;&#25928;&#36816;&#34892;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#32771;&#34385;&#38378;&#23384;&#29305;&#24615;&#30340;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#65292;&#24341;&#23548;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#36827;&#34892;&#20248;&#21270;&#65306;&#20943;&#23569;&#20174;&#38378;&#23384;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#65292;&#24182;&#20197;&#36739;&#22823;&#12289;&#26356;&#36830;&#32493;&#30340;&#22359;&#35835;&#21462;&#25968;&#25454;&#12290;&#22312;&#36825;&#20010;&#21463;&#30828;&#20214;&#21551;&#21457;&#30340;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#8220;&#31383;&#21475;&#21270;&#8221;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#20043;&#21069;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#26469;&#31574;&#30053;&#24615;&#22320;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#65292;&#20854;&#27425;&#65292;&#8220;&#34892;&#21015;&#32465;&#23450;&#8221;&#36866;&#24212;&#20102;&#38378;&#23384;&#30340;&#39034;&#24207;&#25968;&#25454;&#35775;&#38382;&#29305;&#28857;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, "windowing" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#33539;&#24335;&#65306;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;RAG&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.10997</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#33539;&#24335;&#65306;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;RAG&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#38754;&#20020;&#24187;&#35273;&#12289;&#36807;&#26102;&#30693;&#35782;&#21644;&#38750;&#36879;&#26126;&#12289;&#19981;&#21487;&#36861;&#28335;&#30340;&#25512;&#29702;&#36807;&#31243;&#31561;&#25361;&#25112;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;RAG&#23558;LLMs&#33258;&#36523;&#30340;&#30693;&#35782;&#19982;&#24222;&#22823;&#12289;&#21160;&#24577;&#30340;&#22806;&#37096;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#21327;&#21516;&#25928;&#24212;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32771;&#23519;&#20102;RAG&#33539;&#24335;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;&#23427;&#35814;&#32454;&#23457;&#35270;&#20102;RAG&#26694;&#26550;&#30340;&#19977;&#20010;&#22522;&#26412;&#35201;&#32032;&#65292;&#21253;&#25324;&#26816;&#32034;&#12289;&#29983;&#25104;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#23884;&#20837;&#22312;RAG&#26694;&#26550;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in e
&lt;/p&gt;</description></item><item><title>PromptBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;&#65292;&#21253;&#25324;&#20102;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#31561;&#32452;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#21644;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>http://arxiv.org/abs/2312.07910</link><description>&lt;p&gt;
PromptBench&#65306;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07910
&lt;/p&gt;
&lt;p&gt;
PromptBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;&#65292;&#21253;&#25324;&#20102;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#31561;&#32452;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#21644;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#23545;&#20110;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PromptBench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#32479;&#19968;&#24211;&#12290;&#23427;&#30001;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;&#21644;&#25193;&#23637;&#65306;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#12290;PromptBench&#26088;&#22312;&#25104;&#20026;&#19968;&#20010;&#24320;&#25918;&#12289;&#36890;&#29992;&#21644;&#28789;&#27963;&#30340;&#20195;&#30721;&#24211;&#65292;&#20197;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#65292;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#21644;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/microsoft/promptbench&#19978;&#25214;&#21040;&#65292;&#24182;&#23558;&#25345;&#32493;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#65292;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#34892;&#20026;&#20934;&#21017;&#24182;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#65292;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2312.04889</link><description>&lt;p&gt;
KwaiAgents&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#65292;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#34892;&#20026;&#20934;&#21017;&#24182;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#65292;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30001;&#20110;&#22909;&#22855;&#24515;&#30340;&#39537;&#20351;&#65292;&#19981;&#26029;&#25506;&#32034;&#21644;&#29702;&#35299;&#21608;&#22260;&#30340;&#19990;&#30028;&#65292;&#20174;&#32780;&#21457;&#26126;&#20102;&#21508;&#31181;&#24037;&#20855;&#26469;&#28385;&#36275;&#36825;&#31181;&#22909;&#22855;&#24515;&#12290;&#23613;&#31649;&#20154;&#31867;&#26080;&#27861;&#22312;&#22823;&#33041;&#20013;&#22788;&#29702;&#21644;&#35760;&#24518;&#22823;&#37327;&#20449;&#24687;&#65292;&#20294;&#22312;&#25209;&#21028;&#24605;&#32500;&#12289;&#35268;&#21010;&#12289;&#21453;&#24605;&#20197;&#21450;&#21033;&#29992;&#29616;&#26377;&#24037;&#20855;&#19982;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#35299;&#37322;&#26041;&#38754;&#21331;&#36234;&#20986;&#33394;&#65292;&#20351;&#20854;&#33021;&#22815;&#39640;&#25928;&#22320;&#23547;&#25214;&#31572;&#26696;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#27493;&#34920;&#26126;&#65292;&#26426;&#22120;&#21487;&#33021;&#20063;&#20855;&#22791;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#21442;&#25968;&#25968;&#37327;&#21463;&#38480;&#65292;&#20063;&#33021;&#23637;&#31034;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#22312; KwaiAgents &#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLM&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#30340;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#12289;&#34892;&#20026;&#20934;&#21017;&#21644;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#12290;&#26234;&#33021;&#20307;&#36824;&#21487;&#20197;&#26356;&#26032;&#26597;&#35810;&#32467;&#26524;&#65292;&#19982;&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update an
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#27880;&#37322;&#26412;&#36523;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#23545;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#25910;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#21457;&#29616;&#27880;&#37322;&#24037;&#20855;&#30340;&#35774;&#35745;&#36873;&#25321;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#26126;&#26174;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2311.14212</link><description>&lt;p&gt;
&#27880;&#37322;&#25935;&#24863;&#24615;&#65306;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Annotation Sensitivity: Training Data Collection Methods Affect Model Performance. (arXiv:2311.14212v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#27880;&#37322;&#26412;&#36523;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#23545;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#25910;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#21457;&#29616;&#27880;&#37322;&#24037;&#20855;&#30340;&#35774;&#35745;&#36873;&#25321;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#25968;&#25454;&#30001;&#20154;&#24037;&#27880;&#37322;&#32773;&#25910;&#38598;&#26102;&#65292;&#27880;&#37322;&#24037;&#20855;&#30340;&#35774;&#35745;&#12289;&#32473;&#20104;&#27880;&#37322;&#32773;&#30340;&#25351;&#31034;&#12289;&#27880;&#37322;&#32773;&#30340;&#29305;&#24449;&#20197;&#21450;&#20182;&#20204;&#20043;&#38388;&#30340;&#20114;&#21160;&#37117;&#21487;&#33021;&#23545;&#35757;&#32451;&#25968;&#25454;&#20135;&#29983;&#24433;&#21709;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#21019;&#24314;&#27880;&#37322;&#24037;&#20855;&#26102;&#30340;&#35774;&#35745;&#36873;&#25321;&#20063;&#20250;&#24433;&#21709;&#22522;&#20110;&#24471;&#21040;&#30340;&#27880;&#37322;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#27880;&#37322;&#25935;&#24863;&#24615;"&#36825;&#20010;&#26415;&#35821;&#65292;&#29992;&#26469;&#25351;&#20195;&#27880;&#37322;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#23545;&#27880;&#37322;&#26412;&#36523;&#20197;&#21450;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#21644;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20116;&#31181;&#23454;&#39564;&#26465;&#20214;&#19979;&#23545;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#25910;&#38598;&#65292;&#38543;&#26426;&#23558;&#27880;&#37322;&#32773;&#20998;&#37197;&#21040;&#19981;&#21516;&#26465;&#20214;&#19979;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#24471;&#21040;&#30340;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#27599;&#20010;&#26465;&#20214;&#30340;&#20445;&#30041;&#37096;&#20998;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20197;&#19979;&#26041;&#38754;&#26465;&#20214;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65306;1&#65289;&#20167;&#24680;&#35328;&#35770;/&#20882;&#29359;&#24615;&#35821;&#35328;&#27880;&#37322;&#30340;&#27604;&#20363;&#65292;2&#65289;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. We introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations themselves and on downstream model performance and predictions. We collect annotations of hate speech and offensive language in five experimental conditions of an annotation instrument, randomly assigning annotators to conditions. We then fine-tune BERT models on each of the five resulting datasets and evaluate model performance on a holdout portion of each condition. We find considerable differences between the conditions for 1) the share of hate speech/offensive language annotations, 2) model per
&lt;/p&gt;</description></item><item><title>FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2311.01282</link><description>&lt;p&gt;
FlashDecoding++: &#22312;GPU&#19978;&#21152;&#36895;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26356;&#24555;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01282
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#26410;&#35299;&#20915;&#65306;(1) &#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12290;softmax&#25805;&#20316;&#38656;&#35201;&#21516;&#27493;&#26356;&#26032;&#27599;&#20010;&#37096;&#20998;softmax&#32467;&#26524;&#65292;&#23548;&#33268;LLM&#20013;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#24320;&#38144;&#22686;&#21152;&#32422;20%&#12290;(2) &#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#12290;&#22312;LLM&#25512;&#29702;&#20013;&#25191;&#34892;GEMM&#30340;&#30697;&#38453;&#24418;&#29366;&#26159;&#25153;&#24179;&#30340;&#65292;&#23548;&#33268;&#22312;&#20808;&#21069;&#30340;&#35774;&#35745;&#20013;&#22635;&#20805;&#38646;&#21518;&#35745;&#31639;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#24615;&#33021;&#25439;&#22833;&#36229;&#36807;50%&#12290;(3) &#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;LLM&#20013;&#30340;&#20869;&#26680;&#24615;&#33021;&#21462;&#20915;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#25968;&#25454;&#29305;&#24449;&#12289;&#30828;&#20214;&#37197;&#32622;&#31561;&#12290;&#21333;&#19968;&#21644;&#38745;&#24577;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#23548;&#33268;LLM&#25512;&#29702;&#20013;&#19981;&#21516;&#24418;&#29366;&#30340;GEMM&#30340;&#24615;&#33021;&#25439;&#22833;&#36798;&#21040;50.25%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FlashDecoding++&#65292;&#19968;&#31181;&#24555;&#36895;&#25903;&#25345;&#20027;&#27969;LLM&#21644;&#30828;&#20214;&#21518;&#31471;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;FlashDecoding++&#23454;&#29616;&#20102;&#20197;&#19979;&#30446;&#26631;&#65306;
&lt;/p&gt;
&lt;p&gt;
As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and &gt;50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32534;&#31243;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#20013;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#26684;&#24335;&#38169;&#35823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04695</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32534;&#31243;&#39118;&#26684;&#20197;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32534;&#31243;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#20013;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#26684;&#24335;&#38169;&#35823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#38024;&#23545;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;(KBQA)&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#22797;&#26434;&#30340;&#35757;&#32451;&#25216;&#26415;&#21644;&#27169;&#22411;&#26694;&#26550;&#65292;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#33021;&#21147;&#30340;&#20986;&#29616;&#20026;KBQA&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26080;&#38656;&#35757;&#32451;&#30340;&#35821;&#20041;&#35299;&#26512;&#33539;&#24335;&#65306;&#32473;&#23450;&#23569;&#37327;&#38382;&#39064;&#21450;&#20854;&#26631;&#35760;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#20316;&#20026;&#28436;&#31034;&#31034;&#20363;&#65292;LLMs&#33021;&#22815;&#29702;&#35299;&#20219;&#21153;&#24847;&#22270;&#24182;&#20026;&#26032;&#38382;&#39064;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24378;&#22823;&#30340;LLMs&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#20102;&#35299;&#24456;&#23569;&#65292;&#23548;&#33268;&#26684;&#24335;&#38169;&#35823;&#29575;&#36739;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;KBQA&#30340;&#20195;&#30721;&#39118;&#26684;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#38476;&#29983;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#29983;&#25104;&#36807;&#31243;&#36716;&#25442;&#20026;&#26356;&#20026;&#29087;&#24713;&#30340;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#12290;&#23545;&#19977;&#20010;&#20027;&#27969;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#36731;&#20102;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#20013;&#30340;&#26684;&#24335;&#38169;&#35823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. Recently, the emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA: Given a small number of questions and their labeled logical forms as demo examples, LLMs can understand the task intent and generate the logic form for a new question. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate. To solve this problem, we propose a code-style in-context learning method for KBQA, which converts the generation process of unfamiliar logical form into the more familiar code generation process for LLMs. Experimental results on three mainstream datasets show that our method dramatically mitigated the formatting error problem in generating logic for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#37319;&#26679;&#36866;&#37197;&#22120;&#65292;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#35821;&#35328;&#29983;&#25104;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#25913;&#21464;&#27169;&#22411;&#30340;&#37319;&#26679;&#20998;&#24067;&#26469;&#29983;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#25991;&#26412;&#12290;&#36825;&#31181;&#36716;&#21464;&#21487;&#20197;&#35270;&#20026;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26399;&#26395;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.03749</link><description>&lt;p&gt;
&#37319;&#26679;&#36866;&#37197;&#22120;&#30340;&#25928;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Efficacy of Sampling Adapters. (arXiv:2307.03749v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#37319;&#26679;&#36866;&#37197;&#22120;&#65292;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#35821;&#35328;&#29983;&#25104;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#25913;&#21464;&#27169;&#22411;&#30340;&#37319;&#26679;&#20998;&#24067;&#26469;&#29983;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#25991;&#26412;&#12290;&#36825;&#31181;&#36716;&#21464;&#21487;&#20197;&#35270;&#20026;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26399;&#26395;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#26679;&#26159;&#20174;&#27010;&#29575;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#24120;&#35265;&#31574;&#30053;&#65292;&#20294;&#26631;&#20934;&#30340;&#31062;&#20808;&#37319;&#26679;&#24448;&#24448;&#23548;&#33268;&#25991;&#26412;&#19981;&#36830;&#36143;&#25110;&#19981;&#31526;&#21512;&#35821;&#27861;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#20462;&#25913;&#27169;&#22411;&#37319;&#26679;&#20998;&#24067;&#30340;&#25216;&#26415;&#65292;&#22914;&#26680;&#24515;&#25110;top-k&#37319;&#26679;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#29702;&#35299;&#36825;&#20123;&#25216;&#26415;&#65292;&#31216;&#20043;&#20026;&#37319;&#26679;&#36866;&#37197;&#22120;&#12290;&#37319;&#26679;&#36866;&#37197;&#22120;&#36890;&#24120;&#21487;&#20197;&#29983;&#25104;&#36136;&#37327;&#26356;&#39640;&#30340;&#25991;&#26412;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#20174;&#24418;&#24335;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#20204;&#26159;&#22914;&#20309;&#25913;&#21464;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#65288;&#23376;&#65289;&#35789;&#32423;&#20998;&#24067;&#30340;&#65311;&#20026;&#20160;&#20040;&#36825;&#20123;&#23616;&#37096;&#25913;&#21464;&#20250;&#23548;&#33268;&#26356;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65311;&#25105;&#20204;&#35748;&#20026;&#65292;&#23427;&#20204;&#25152;&#24378;&#21046;&#25191;&#34892;&#30340;&#36716;&#21464;&#21487;&#20197;&#34987;&#35270;&#20026;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#34429;&#28982;&#27169;&#22411;&#22833;&#21435;&#20102;&#20135;&#29983;&#26576;&#20123;&#23383;&#31526;&#20018;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#26399;&#26395;&#30340;&#25991;&#26412;&#65292;&#20854;&#31934;&#30830;&#29575;&#25552;&#39640;&#20102;&#12290;&#23613;&#31649;&#36825;&#31181;&#26435;&#34913;&#22312;&#26631;&#20934;&#30340;&#36317;&#31163;&#24230;&#37327;&#20013;&#27809;&#26377;&#21453;&#26144;&#20986;&#26469;&#65292;&#20294;&#23427;&#30830;&#23454;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling is a common strategy for generating text from probabilistic models, yet standard ancestral sampling often results in text that is incoherent or ungrammatical. To alleviate this issue, various modifications to a model's sampling distribution, such as nucleus or top-k sampling, have been introduced and are now ubiquitously used in language generation systems. We propose a unified framework for understanding these techniques, which we term sampling adapters. Sampling adapters often lead to qualitatively better text, which raises the question: From a formal perspective, how are they changing the (sub)word-level distributions of language generation models? And why do these local changes lead to higher-quality text? We argue that the shift they enforce can be viewed as a trade-off between precision and recall: while the model loses its ability to produce certain strings, its precision rate on desirable text increases. While this trade-off is not reflected in standard metrics of dist
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#21487;&#20449;&#24230;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#22810;&#20010;&#26041;&#38754;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#23041;&#32961;&#28431;&#27934;&#65292;&#20363;&#22914;&#23545;&#27602;&#24615;&#36755;&#20986;&#21644;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#30340;&#26131;&#34987;&#35823;&#23548;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11698</link><description>&lt;p&gt;
DecodingTrust: GPT&#27169;&#22411;&#30340;&#20840;&#38754;&#21487;&#20449;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (arXiv:2306.11698v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11698
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#21487;&#20449;&#24230;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#22810;&#20010;&#26041;&#38754;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#23041;&#32961;&#28431;&#27934;&#65292;&#20363;&#22914;&#23545;&#27602;&#24615;&#36755;&#20986;&#21644;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#30340;&#26131;&#34987;&#35823;&#23548;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#20854;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#65292;&#24341;&#36215;&#20102;&#20174;&#20174;&#19994;&#32773;&#21040;&#20844;&#20247;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20851;&#20110;GPT&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#25991;&#29486;&#20173;&#28982;&#26377;&#38480;&#65292;&#20174;&#19994;&#32773;&#20204;&#25552;&#35758;&#23558;&#24378;&#22823;&#30340;GPT&#27169;&#22411;&#29992;&#20110;&#25935;&#24863;&#24212;&#29992;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#39046;&#22495;&#65292;&#20854;&#20013;&#38169;&#35823;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#37325;&#28857;&#25918;&#22312;GPT-4&#21644;GPT-3.5&#19978;&#65289;&#36827;&#34892;&#20840;&#38754;&#30340;&#21487;&#20449;&#24230;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#22810;&#26679;&#30340;&#35266;&#28857; - &#21253;&#25324;&#26377;&#27602;&#24615;&#12289;&#38472;&#35268;&#20559;&#35265;&#12289;&#23545;&#25239;&#24378;&#24230;&#12289;&#36229;&#20986;&#20998;&#24067;&#30340;&#24378;&#24230;&#12289;&#23545;&#25239;&#31034;&#33539;&#30340;&#24378;&#24230;&#12289;&#38544;&#31169;&#12289;&#26426;&#22120;&#20262;&#29702;&#21644;&#20844;&#24179;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#21487;&#20449;&#24230;&#23041;&#32961;&#28431;&#27934;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;GPT&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#34987;&#35823;&#23548;&#29983;&#25104;&#26377;&#27602;&#21644;&#20559;&#35265;&#30340;&#36755;&#20986;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#19978;&#19979;&#25991;&#20013;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance -- where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives -- including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#25552;&#39640;&#20102;31.6&#65285;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.07372</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#32534;&#36753;&#30340;&#36880;&#27493;&#35299;&#37322;&#23454;&#29616;&#20132;&#20114;&#24335;&#25991;&#26412;&#36716;SQL
&lt;/p&gt;
&lt;p&gt;
Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations. (arXiv:2305.07372v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#25552;&#39640;&#20102;31.6&#65285;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#24211;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#38750;&#19987;&#23478;&#24456;&#38590;&#23436;&#20840;&#37322;&#25918;&#20851;&#31995;&#25968;&#25454;&#24211;&#30340;&#20998;&#26512;&#33021;&#21147;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#29087;&#24713;SQL&#31561;&#25968;&#25454;&#24211;&#35821;&#35328;&#12290;&#35768;&#22810;&#25216;&#26415;&#24050;&#34987;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#33258;&#21160;&#29983;&#25104;SQL&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20197;&#19979;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23545;&#20110;&#22797;&#26434;&#26597;&#35810;&#23427;&#20204;&#20173;&#20250;&#29359;&#24456;&#22810;&#38169;&#35823;&#65292;&#65288;2&#65289;&#23427;&#20204;&#19981;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#24335;&#65292;&#35753;&#38750;&#19987;&#23478;&#29992;&#25143;&#39564;&#35777;&#21644;&#25913;&#36827;&#19981;&#27491;&#30830;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#12290;&#22312;Spider&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33267;&#23569;&#27604;&#19977;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#25191;&#34892;&#20934;&#30830;&#24615;&#26041;&#38754;&#25552;&#39640;&#20102;31.6&#65285;&#12290;24&#21517;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational databases play an important role in this Big Data era. However, it is challenging for non-experts to fully unleash the analytical power of relational databases, since they are not familiar with database languages such as SQL. Many techniques have been proposed to automatically generate SQL from natural language, but they suffer from two issues: (1) they still make many mistakes, particularly for complex queries, and (2) they do not provide a flexible way for non-expert users to validate and refine the incorrect queries. To address these issues, we introduce a new interaction mechanism that allows users directly edit a step-by-step explanation of an incorrect SQL to fix SQL errors. Experiments on the Spider benchmark show that our approach outperforms three SOTA approaches by at least 31.6% in terms of execution accuracy. A user study with 24 participants further shows that our approach helped users solve significantly more SQL tasks with less time and higher confidence, demo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;mFACE&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#27169;&#22411;&#25913;&#36827;&#22810;&#35821;&#35328;&#25688;&#35201;&#65292;&#37319;&#29992;&#25968;&#25454;&#36807;&#28388;&#21644;&#25511;&#21046;&#29983;&#25104;&#20004;&#31181;&#26041;&#27861;&#26469;&#20943;&#23569;&#34394;&#26500;&#20449;&#24687;&#65292;&#24182;&#22312;XLSum&#25968;&#25454;&#38598;&#30340;45&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2212.10622</link><description>&lt;p&gt;
mFACE:&#20855;&#26377;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#30340;&#22810;&#35821;&#35328;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
mFACE: Multilingual Summarization with Factual Consistency Evaluation. (arXiv:2212.10622v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;mFACE&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#27169;&#22411;&#25913;&#36827;&#22810;&#35821;&#35328;&#25688;&#35201;&#65292;&#37319;&#29992;&#25968;&#25454;&#36807;&#28388;&#21644;&#25511;&#21046;&#29983;&#25104;&#20004;&#31181;&#26041;&#27861;&#26469;&#20943;&#23569;&#34394;&#26500;&#20449;&#24687;&#65292;&#24182;&#22312;XLSum&#25968;&#25454;&#38598;&#30340;45&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#20877;&#27425;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#29983;&#25104;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25928;&#29992;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#21162;&#21147;&#23581;&#35797;&#36890;&#36807;&#35774;&#35745;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#20165;&#19987;&#27880;&#20110;&#36164;&#28304;&#20016;&#23500;&#30340;&#33521;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20107;&#23454;&#19968;&#33268;&#24615;&#35780;&#20272;&#27169;&#22411;&#26469;&#25913;&#36827;&#22810;&#35821;&#35328;&#25688;&#35201;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#34394;&#26500;&#20449;&#24687;&#65292;&#20998;&#21035;&#22522;&#20110;&#22810;&#35821;&#35328;NLI&#27169;&#22411;&#25552;&#20379;&#30340;&#20449;&#21495;&#65292;&#21363;&#25968;&#25454;&#36807;&#28388;&#21644;&#25511;&#21046;&#29983;&#25104;&#12290;&#22312;XLSum&#25968;&#25454;&#38598;&#30340;45&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#37117;&#33719;&#24471;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. Several recent efforts attempt to address this by devising models that automatically detect factual inconsistencies in machine generated summaries. However, they focus exclusively on English, a language with abundant resources. In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate hallucinations based on the signal provided by a multilingual NLI model, namely data filtering and controlled generation. Experimental results in the 45 languages from the XLSum dataset show gains over strong baselines in both automatic and human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#23553;&#38381;&#25928;&#24212;&#21644;&#20449;&#24687;&#35770;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20808;&#21069;&#19978;&#19979;&#25991;&#20013;&#30340;&#20449;&#24687;&#20998;&#24067;&#36890;&#24120;&#33021;&#22815;&#39044;&#27979;&#21477;&#23376;&#26411;&#23614;&#21644;&#20174;&#21477;&#26411;&#23614;&#30340;&#38405;&#35835;&#26102;&#38388;&#65288;RT&#65289;&#65292;&#20174;&#32780;&#25903;&#25345;&#20102;&#20851;&#20110;&#23553;&#38381;&#25928;&#24212;&#28041;&#21450;&#30340;&#20960;&#20010;&#20808;&#21069;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2203.17213</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#20998;&#26512;&#23553;&#38381;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Analyzing Wrap-Up Effects through an Information-Theoretic Lens. (arXiv:2203.17213v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.17213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#23553;&#38381;&#25928;&#24212;&#21644;&#20449;&#24687;&#35770;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20808;&#21069;&#19978;&#19979;&#25991;&#20013;&#30340;&#20449;&#24687;&#20998;&#24067;&#36890;&#24120;&#33021;&#22815;&#39044;&#27979;&#21477;&#23376;&#26411;&#23614;&#21644;&#20174;&#21477;&#26411;&#23614;&#30340;&#38405;&#35835;&#26102;&#38388;&#65288;RT&#65289;&#65292;&#20174;&#32780;&#25903;&#25345;&#20102;&#20851;&#20110;&#23553;&#38381;&#25928;&#24212;&#28041;&#21450;&#30340;&#20960;&#20010;&#20808;&#21069;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#23454;&#26045;&#20102;&#35768;&#22810;&#38405;&#35835;&#26102;&#38388;&#65288;RT&#65289;&#25968;&#25454;&#30340;&#20998;&#26512;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#39537;&#21160;&#38405;&#35835;&#29702;&#35299;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25152;&#35859;&#30340;&#8220;&#23553;&#38381;&#25928;&#24212;&#8221;&#24341;&#20837;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#36890;&#24120;&#20250;&#24573;&#30053;&#21477;&#23376;&#26411;&#23614;&#29978;&#33267;&#20174;&#21477;&#26411;&#23614;&#30340;&#21333;&#35789;&#30340;&#25968;&#25454;&#65292;&#36825;&#34920;&#29616;&#20026;&#36825;&#20123;&#21333;&#35789;&#30340;RT&#20998;&#24067;&#19981;&#22343;&#34913;&#12290;&#22240;&#27492;&#65292;&#23545;&#21442;&#19982;&#36825;&#20123;&#23553;&#38381;&#25928;&#24212;&#21487;&#33021;&#28041;&#21450;&#30340;&#35748;&#30693;&#36807;&#31243;&#30340;&#29702;&#35299;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#30740;&#31350;&#23553;&#38381;&#25928;&#24212;&#19982;&#35789;&#35821;&#21644;&#19978;&#19979;&#25991;&#24778;&#35766;&#20540;&#31561;&#20449;&#24687;&#35770;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#20102;&#35299;&#36825;&#20123;&#36807;&#31243;&#12290;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#19978;&#19979;&#25991;&#20013;&#30340;&#20449;&#24687;&#20998;&#24067;&#36890;&#24120;&#33021;&#22815;&#39044;&#27979;&#21477;&#23376;&#26411;&#23614;&#21644;&#20174;&#21477;&#26411;&#23614;&#30340;RT&#65288;&#32780;&#19981;&#33021;&#39044;&#27979;&#21477;&#20013;RT&#65289;&#65292;&#36825;&#25903;&#25345;&#20102;&#20851;&#20110;&#23553;&#38381;&#25928;&#24212;&#28041;&#21450;&#30340;&#20960;&#20010;&#20808;&#21069;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous analyses of reading time (RT) data have been implemented -- all in an effort to better understand the cognitive processes driving reading comprehension. However, data measured on words at the end of a sentence -- or even at the end of a clause -- is often omitted due to the confounding factors introduced by so-called "wrap-up effects," which manifests as a skewed distribution of RTs for these words. Consequently, the understanding of the cognitive processes that might be involved in these wrap-up effects is limited. In this work, we attempt to learn more about these processes by examining the relationship between wrap-up effects and information-theoretic quantities, such as word and context surprisals. We find that the distribution of information in prior contexts is often predictive of sentence- and clause-final RTs (while not of sentence-medial RTs). This lends support to several prior hypotheses about the processes involved in wrap-up effects.
&lt;/p&gt;</description></item></channel></rss>