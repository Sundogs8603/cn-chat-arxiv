<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35762;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;WebBrain&#65292;&#23427;&#36890;&#36807;&#25366;&#25496;Web&#20013;&#30340;&#25903;&#25345;&#35777;&#25454;&#65292;&#20026;&#26597;&#35810;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#31616;&#30701;&#25991;&#31456;&#12290;&#25105;&#20204;&#20174;&#32500;&#22522;&#30334;&#31185;&#20013;&#25552;&#21462;&#20102;&#25968;&#25454;&#38598;WebBrain-Raw&#65292;&#26500;&#24314;&#20102;WebBrain-R&#21644;WebBrain-G&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#20107;&#23454;&#24615;&#30340;&#26694;&#26550;ReGen&#12290;</title><link>http://arxiv.org/abs/2304.04358</link><description>&lt;p&gt;
WebBrain: &#22522;&#20110;&#22823;&#22411;Web&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25366;&#25496;&#25903;&#25345;&#35777;&#25454;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#25991;&#31456;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
WebBrain: Learning to Generate Factually Correct Articles for Queries by Grounding on Large Web Corpus. (arXiv:2304.04358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35762;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;WebBrain&#65292;&#23427;&#36890;&#36807;&#25366;&#25496;Web&#20013;&#30340;&#25903;&#25345;&#35777;&#25454;&#65292;&#20026;&#26597;&#35810;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#31616;&#30701;&#25991;&#31456;&#12290;&#25105;&#20204;&#20174;&#32500;&#22522;&#30334;&#31185;&#20013;&#25552;&#21462;&#20102;&#25968;&#25454;&#38598;WebBrain-Raw&#65292;&#26500;&#24314;&#20102;WebBrain-R&#21644;WebBrain-G&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#20107;&#23454;&#24615;&#30340;&#26694;&#26550;ReGen&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#20174;Web&#25366;&#25496;&#25903;&#25345;&#35777;&#25454;&#65292;&#20026;&#26597;&#35810;&#29983;&#25104;&#24102;&#21442;&#32771;&#25991;&#29486;&#30340;&#31616;&#30701;&#20107;&#23454;&#25991;&#31456;&#12290;&#22312;&#36825;&#20010;&#21517;&#20026;WebBrain&#30340;&#20219;&#21153;&#20013;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#20026;&#32500;&#22522;&#30334;&#31185;&#20013;&#26410;&#20986;&#29616;&#30340;&#20107;&#23454;&#26597;&#35810;&#29983;&#25104;&#27969;&#30021;&#12289;&#20449;&#24687;&#20016;&#23500;&#12289;&#20107;&#23454;&#27491;&#30830;&#30340;&#31616;&#30701;&#25991;&#31456;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;WebBrain&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25353;&#29031;&#32500;&#22522;&#30334;&#31185;&#20013;&#30340;&#25991;&#31456;&#21644;&#21487;&#29228;&#34892;&#30340;&#32500;&#22522;&#30334;&#31185;&#21442;&#32771;&#25991;&#29486;&#25552;&#21462;&#33521;&#35821;&#25968;&#25454;&#38598;WebBrain-Raw&#12290;WebBrain-Raw&#27604;&#20197;&#21069;&#26368;&#22823;&#30340;&#21516;&#34892;&#25968;&#25454;&#38598;&#22823;&#21313;&#20493;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#24800;&#21450;&#30740;&#31350;&#31038;&#21306;&#12290;&#20174;WebBrain-Raw&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#38598;&#65306;WebBrain-R&#21644;WebBrain-G&#65292;&#20998;&#21035;&#29992;&#20110;&#35757;&#32451;&#39046;&#22495;&#20869;&#30340;&#26816;&#32034;&#22120;&#21644;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;WebBrain&#19978;&#23454;&#35777;&#20998;&#26512;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#34920;&#29616;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#22686;&#24378;&#35777;&#25454;&#25903;&#25345;&#30340;&#29983;&#25104;&#20107;&#23454;&#24615;&#30340;&#26032;&#26694;&#26550;ReGen&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new NLP task -- generating short factual articles with references for queries by mining supporting evidence from the Web. In this task, called WebBrain, the ultimate goal is to generate a fluent, informative, and factually-correct short article (e.g., a Wikipedia article) for a factual query unseen in Wikipedia. To enable experiments on WebBrain, we construct a large-scale dataset WebBrain-Raw by extracting English Wikipedia articles and their crawlable Wikipedia references. WebBrain-Raw is ten times larger than the previous biggest peer dataset, which can greatly benefit the research community. From WebBrain-Raw, we construct two task-specific datasets: WebBrain-R and WebBrain-G, which are used to train in-domain retriever and generator, respectively. Besides, we empirically analyze the performances of the current state-of-the-art NLP techniques on WebBrain and introduce a new framework ReGen, which enhances the generation factualness by improved evidence
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;&#26631;&#20934;&#35780;&#20272;&#12289;&#26497;&#24615;&#36716;&#31227;&#35780;&#20272;&#12289;&#24320;&#25918;&#22495;&#35780;&#20272;&#21644;&#24773;&#24863;&#25512;&#29702;&#35780;&#20272;&#65292;&#20849;&#28041;&#21450;18&#20010;&#25968;&#25454;&#38598;&#21644;5&#20010;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#21644;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.04339</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#24773;&#24863;&#20998;&#26512;&#22120;&#21527;&#65311;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. (arXiv:2304.04339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;&#26631;&#20934;&#35780;&#20272;&#12289;&#26497;&#24615;&#36716;&#31227;&#35780;&#20272;&#12289;&#24320;&#25918;&#22495;&#35780;&#20272;&#21644;&#24773;&#24863;&#25512;&#29702;&#35780;&#20272;&#65292;&#20849;&#28041;&#21450;18&#20010;&#25968;&#25454;&#38598;&#21644;5&#20010;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#21644;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ChatGPT&#22312;&#30740;&#31350;&#21644;&#20844;&#20247;&#30340;&#20851;&#27880;&#19979;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#29305;&#21035;&#24819;&#30693;&#36947;&#23427;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#22120;&#12290;&#20026;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#22312;&#25991;&#26412;&#20013;&#21253;&#21547;&#30340;&#24847;&#35265;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#30340;&#29702;&#35299;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#35774;&#32622;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#26631;&#20934;&#35780;&#20272;&#12289;&#26497;&#24615;&#36716;&#31227;&#35780;&#20272;&#12289;&#24320;&#25918;&#22495;&#35780;&#20272;&#21644;&#24773;&#24863;&#25512;&#29702;&#35780;&#20272;&#12290;&#20197;&#19978;&#35780;&#20272;&#28041;&#21450;18&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;5&#20010;&#20195;&#34920;&#24615;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;ChatGPT&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#21644;&#30456;&#24212;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#22312;&#26411;&#31471;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#23450;&#24615;&#26696;&#20363;&#30740;&#31350;&#20197;&#28145;&#20837;&#29702;&#35299;&#20854;&#24773;&#24863;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly curious about whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text. Specifically, we evaluate it in four settings, including standard evaluation, polarity shift evaluation, open-domain evaluation, and sentiment inference evaluation. The above evaluation involves 18 benchmark datasets and 5 representative sentiment analysis tasks, and we compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task. Moreover, we also conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.
&lt;/p&gt;</description></item><item><title>ARNOLD&#26159;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#12289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;8&#20010;&#35821;&#35328;&#26465;&#20214;&#20219;&#21153;&#65292;&#22312;&#35821;&#35328;&#24341;&#23548;&#19979;&#24110;&#21161;&#26426;&#22120;&#20154;&#23398;&#20064;&#29702;&#35299;&#29289;&#20307;&#29366;&#24577;&#21644;&#23398;&#20064;&#36830;&#32493;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.04321</link><description>&lt;p&gt;
ARNOLD&#65306;&#22522;&#20110;&#36830;&#32493;&#29366;&#24577;&#23454;&#29616;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#35821;&#35328;&#24341;&#23548;&#20219;&#21153;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes. (arXiv:2304.04321v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04321
&lt;/p&gt;
&lt;p&gt;
ARNOLD&#26159;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#12289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;8&#20010;&#35821;&#35328;&#26465;&#20214;&#20219;&#21153;&#65292;&#22312;&#35821;&#35328;&#24341;&#23548;&#19979;&#24110;&#21161;&#26426;&#22120;&#20154;&#23398;&#20064;&#29702;&#35299;&#29289;&#20307;&#29366;&#24577;&#21644;&#23398;&#20064;&#36830;&#32493;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#29702;&#35299;&#29289;&#20307;&#30340;&#36830;&#32493;&#29366;&#24577;&#23545;&#20110;&#20219;&#21153;&#23398;&#20064;&#21644;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20551;&#23450;&#30446;&#26631;&#29366;&#24577;&#26159;&#31163;&#25955;&#30340;(&#20363;&#22914;&#20108;&#36827;&#21046;&#29366;&#24577;)&#65292;&#36825;&#32473;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#21644;&#23558;&#23398;&#20064;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29366;&#24577;&#31163;&#25955;&#21270;&#38480;&#21046;&#20102;&#26426;&#22120;&#20154;&#26681;&#25454;&#21160;&#20316;&#21644;&#29366;&#24577;&#30340;&#24341;&#23548;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARNOLD&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#12289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;ARNOLD&#30001;8&#20010;&#35821;&#35328;&#26465;&#20214;&#20219;&#21153;&#32452;&#25104;&#65292;&#28041;&#21450;&#29702;&#35299;&#29289;&#20307;&#29366;&#24577;&#21644;&#23398;&#20064;&#36830;&#32493;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#20419;&#36827;&#35821;&#35328;&#24341;&#23548;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#26495;&#29983;&#25104;&#30340;&#35821;&#35328;&#25551;&#36848;&#30340;&#19987;&#23478;&#28436;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;&#35821;&#35328;&#26465;&#20214;&#31574;&#30053;&#23398;&#20064;&#27169;&#22411;&#26469;&#35780;&#20272;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ARNOLD&#20026;&#22522;&#20110;&#36830;&#32493;&#29366;&#24577;&#30340;&#35821;&#35328;&#24341;&#23548;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#24182;&#21487;&#29992;&#20110;&#35780;&#20272;&#20174;&#27169;&#25311;&#22330;&#26223;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the continuous states of objects is essential for task learning and planning in the real world. However, most existing task learning benchmarks assume discrete(e.g., binary) object goal states, which poses challenges for the learning of complex tasks and transferring learned policy from simulated environments to the real world. Furthermore, state discretization limits a robot's ability to follow human instructions based on the grounding of actions and states. To tackle these challenges, we present ARNOLD, a benchmark that evaluates language-grounded task learning with continuous states in realistic 3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve understanding object states and learning policies for continuous goals. To promote language-instructed learning, we provide expert demonstrations with template-generated language descriptions. We assess task performance by utilizing the latest language-conditioned policy learning models. Our results ind
&lt;/p&gt;</description></item><item><title>FrenchMedMCQA&#26159;&#27861;&#35821;&#21307;&#23398;MCQA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;3,105&#36947;&#30495;&#23454;&#32771;&#35797;&#39064;&#30446;&#12290;&#38656;&#35201;&#20351;&#29992;&#21307;&#23398;&#39046;&#22495;&#25110;MCQA&#20219;&#21153;&#19987;&#29992;&#30340;&#34920;&#31034;&#24418;&#24335;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04280</link><description>&lt;p&gt;
FrenchMedMCQA: &#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27861;&#35821;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain. (arXiv:2304.04280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04280
&lt;/p&gt;
&lt;p&gt;
FrenchMedMCQA&#26159;&#27861;&#35821;&#21307;&#23398;MCQA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;3,105&#36947;&#30495;&#23454;&#32771;&#35797;&#39064;&#30446;&#12290;&#38656;&#35201;&#20351;&#29992;&#21307;&#23398;&#39046;&#22495;&#25110;MCQA&#20219;&#21153;&#19987;&#29992;&#30340;&#34920;&#31034;&#24418;&#24335;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FrenchMedMCQA&#65292;&#36825;&#26159;&#20844;&#24320;&#21457;&#24067;&#30340;&#21307;&#23398;&#39046;&#22495;&#27861;&#35821;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;3,105&#36947;&#30495;&#23454;&#30340;&#27861;&#22269;&#33647;&#23398;&#19987;&#19994;&#25991;&#20973;&#32771;&#35797;&#39064;&#30446;&#32452;&#25104;&#65292;&#21253;&#25324;&#21333;&#39033;&#21644;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#27599;&#20010;&#23454;&#20363;&#21253;&#21547;&#26631;&#35782;&#31526;&#12289;&#38382;&#39064;&#12289;&#20116;&#20010;&#21487;&#33021;&#30340;&#31572;&#26696;&#21644;&#23427;&#20204;&#30340;&#25163;&#21160;&#32416;&#27491;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#32447;&#27169;&#22411;&#26469;&#33258;&#21160;&#22788;&#29702;&#35813;MCQA&#20219;&#21153;&#65292;&#20197;&#25253;&#21578;&#24403;&#21069;&#30340;&#24615;&#33021;&#21644;&#31361;&#20986;&#20219;&#21153;&#30340;&#38590;&#28857;&#12290;&#32467;&#26524;&#30340;&#35814;&#32454;&#20998;&#26512;&#26174;&#31034;&#65292;&#38656;&#35201;&#26377;&#36866;&#24212;&#20110;&#21307;&#23398;&#39046;&#22495;&#25110;MCQA&#20219;&#21153;&#30340;&#34920;&#31034;&#24418;&#24335;&#65306;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;FrenchMedMCQA&#26159;&#20197;&#27861;&#35821;&#20070;&#20889;&#30340;&#65292;&#33521;&#35821;&#19987;&#38376;&#30340;&#27169;&#22411;&#20063;&#27604;&#36890;&#29992;&#30340;&#27861;&#35821;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#35821;&#26009;&#24211;&#12289;&#27169;&#22411;&#21644;&#24037;&#20855;&#37117;&#21487;&#22312;&#32447;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces FrenchMedMCQA, the first publicly available Multiple-Choice Question Answering (MCQA) dataset in French for medical domain. It is composed of 3,105 questions taken from real exams of the French medical specialization diploma in pharmacy, mixing single and multiple answers. Each instance of the dataset contains an identifier, a question, five possible answers and their manual correction(s). We also propose first baseline models to automatically process this MCQA task in order to report on the current performances and to highlight the difficulty of the task. A detailed analysis of the results showed that it is necessary to have representations adapted to the medical domain or to the MCQA task: in our case, English specialized models yielded better results than generic French ones, even though FrenchMedMCQA is in French. Corpus, models and tools are available online.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#38646;&#26679;&#26412;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#23384;&#22312;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;SLU&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20998;&#26512;&#26174;&#31034;&#22312;DST&#20219;&#21153;&#20013;&#65292;&#22810;&#36718;&#20132;&#20114;&#23545;&#20110;ChatGPT&#30340;&#34920;&#29616;&#26377;&#30410;&#22788;&#12290;&#27492;&#22806;&#65292;&#36824;&#24635;&#32467;&#20102;ChatGPT&#22312;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#24847;&#24819;&#19981;&#21040;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.04256</link><description>&lt;p&gt;
ChatGPT&#22312;&#38646;&#26679;&#26412;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#21021;&#27493;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding. (arXiv:2304.04256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#38646;&#26679;&#26412;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#23384;&#22312;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;SLU&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20998;&#26512;&#26174;&#31034;&#22312;DST&#20219;&#21153;&#20013;&#65292;&#22810;&#36718;&#20132;&#20114;&#23545;&#20110;ChatGPT&#30340;&#34920;&#29616;&#26377;&#30410;&#22788;&#12290;&#27492;&#22806;&#65292;&#36824;&#24635;&#32467;&#20102;ChatGPT&#22312;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#24847;&#24819;&#19981;&#21040;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#35805;&#29702;&#35299;&#26088;&#22312;&#20351;&#23545;&#35805;&#36319;&#36394;&#29992;&#25143;&#38656;&#27714;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#38646;&#26679;&#26412;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#65288;SLU&#65289;&#21644;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#26041;&#38754;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#22312;&#22235;&#20010;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;ChatGPT&#22312;&#38646;&#26679;&#26412;&#23545;&#35805;&#29702;&#35299;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#24191;&#27867;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ChatGPT&#22312;DST&#20219;&#21153;&#20013;&#21463;&#30410;&#20110;&#22810;&#36718;&#20132;&#20114;&#24335;&#25552;&#31034;&#65292;&#20294;&#22312;SLU&#26041;&#38754;&#24456;&#38590;&#25191;&#34892;&#27133;&#22635;&#20805;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;ChatGPT&#22312;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#24847;&#24819;&#19981;&#21040;&#30340;&#34892;&#20026;&#65292;&#24076;&#26395;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26500;&#24314;&#38646;&#26679;&#26412;&#23545;&#35805;&#29702;&#35299;&#31995;&#32479;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot dialogue understanding aims to enable dialogue to track the user's needs without any training data, which has gained increasing attention. In this work, we investigate the understanding ability of ChatGPT for zero-shot dialogue understanding tasks including spoken language understanding (SLU) and dialogue state tracking (DST). Experimental results on four popular benchmarks reveal the great potential of ChatGPT for zero-shot dialogue understanding. In addition, extensive analysis shows that ChatGPT benefits from the multi-turn interactive prompt in the DST task but struggles to perform slot filling for SLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue understanding tasks, hoping to provide some insights for future research on building zero-shot dialogue understanding systems with Large Language Models (LLMs).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04250</link><description>&lt;p&gt;
&#21487;&#32534;&#36753;&#29992;&#25143;&#26723;&#26696;&#30340;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#39640;&#36136;&#37327;&#25512;&#33616;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#20132;&#20114;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#25552;&#20379;&#32473;&#29992;&#25143;&#25511;&#21046;&#25152;&#25509;&#25910;&#30340;&#25512;&#33616;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LACE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;LACE&#22522;&#20110;&#29992;&#25143;&#20132;&#20114;&#30340;&#25991;&#26723;&#26816;&#32034;&#65292;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#31616;&#27905;&#30340;&#21487;&#35835;&#30340;&#27010;&#24565;&#38598;&#65292;&#24182;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#27010;&#24565;&#30340;&#20010;&#24615;&#21270;&#34920;&#31034;&#12290;&#35813;&#22522;&#20110;&#27010;&#24565;&#30340;&#29992;&#25143;&#26723;&#26696;&#34987;&#21033;&#29992;&#26469;&#20570;&#20986;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#35745;&#36890;&#36807;&#36879;&#26126;&#30340;&#29992;&#25143;&#26723;&#26696;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#22810;&#31181;&#30452;&#35266;&#20132;&#20114;&#26041;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19977;&#20010;&#25512;&#33616;&#20219;&#21153;&#65288;&#28201;&#21551;&#21160;&#12289;&#20919;&#21551;&#21160;&#21644;&#38646;&#26679;&#26412;&#65289;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#31163;&#32447;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#20174;LACE&#33719;&#24471;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#22312;&#32447;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;LACE&#30340;&#26377;&#25928;&#24615;&#21644;&#29992;&#25143;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RISC&#30340;Python&#24320;&#28304;&#21253;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20110;&#39745;&#21271;&#20811;&#30465;&#27861;&#35268;&#20445;&#38505;&#26684;&#24335;&#30340;&#27773;&#36710;&#20445;&#38505;&#21512;&#21516;&#65292;&#21253;&#25324;&#27861;&#35821;&#21644;&#33521;&#35821;&#29256;&#26412;&#65292;&#22522;&#20110;&#27492;&#29983;&#25104;&#30340;RISCBAC&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;NLP&#30740;&#31350;&#20013;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#31616;&#21270;&#12289;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#20197;&#21450;&#30417;&#30563;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.04212</link><description>&lt;p&gt;
RISC: &#29983;&#25104;&#30495;&#23454;&#30340;&#21452;&#35821;&#20445;&#38505;&#21512;&#21516;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
RISC: Generating Realistic Synthetic Bilingual Insurance Contract. (arXiv:2304.04212v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RISC&#30340;Python&#24320;&#28304;&#21253;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20110;&#39745;&#21271;&#20811;&#30465;&#27861;&#35268;&#20445;&#38505;&#26684;&#24335;&#30340;&#27773;&#36710;&#20445;&#38505;&#21512;&#21516;&#65292;&#21253;&#25324;&#27861;&#35821;&#21644;&#33521;&#35821;&#29256;&#26412;&#65292;&#22522;&#20110;&#27492;&#29983;&#25104;&#30340;RISCBAC&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;NLP&#30740;&#31350;&#20013;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#31616;&#21270;&#12289;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#20197;&#21450;&#30417;&#30563;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RISC&#30340;Python&#24320;&#28304;&#21253;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20110;&#39745;&#21271;&#20811;&#30465;&#27861;&#35268;&#20445;&#38505;&#26684;&#24335;&#30340;&#27773;&#36710;&#20445;&#38505;&#21512;&#21516;&#65292;&#21253;&#25324;&#27861;&#35821;&#21644;&#33521;&#35821;&#29256;&#26412;&#12290;&#20445;&#38505;&#21512;&#21516;&#36890;&#24120;&#38271;&#36798;90&#21040;100&#39029;&#65292;&#20351;&#29992;&#19987;&#19994;&#30340;&#27861;&#24459;&#21644;&#20445;&#38505;&#26415;&#35821;&#65292;&#22240;&#27492;&#27604;&#20256;&#32479;&#30340;NLP&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26723;&#31867;&#21035;&#26356;&#20026;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#39745;&#21271;&#20811;&#30465;&#24378;&#21046;&#27773;&#36710;&#20445;&#38505;&#21512;&#21516;&#30340;&#36924;&#30495;&#20445;&#38505;&#21512;&#25104;&#21452;&#35821;&#27773;&#36710;&#21512;&#21516;&#25968;&#25454;&#38598;RISCBAC&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;10,000&#20221;&#26410;&#32463;&#26631;&#27880;&#30340;&#27861;&#35821;&#21644;&#33521;&#35821;&#20445;&#38505;&#21512;&#21516;&#12290;RISCBAC&#21487;&#29992;&#20110;NLP&#30740;&#31350;&#20013;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#25688;&#35201;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#31616;&#21270;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#33258;&#21160;&#26631;&#27880;&#20026;&#30417;&#30563;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents RISC, an open-source Python package data generator (https://github.com/GRAAL-Research/risc). RISC generates look-alike automobile insurance contracts based on the Quebec regulatory insurance form in French and English. Insurance contracts are 90 to 100 pages long and use complex legal and insurance-specific vocabulary for a layperson. Hence, they are a much more complex class of documents than those in traditional NLP corpora. Therefore, we introduce RISCBAC, a Realistic Insurance Synthetic Bilingual Automobile Contract dataset based on the mandatory Quebec car insurance contract. The dataset comprises 10,000 French and English unannotated insurance contracts. RISCBAC enables NLP research for unsupervised automatic summarisation, question answering, text simplification, machine translation and more. Moreover, it can be further automatically annotated as a dataset for supervised tasks such as NER
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25277;&#21462;&#24335;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20854;ROUGE&#24471;&#20998;&#30456;&#23545;&#20110;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#25512;&#29702;&#23545;&#20854;&#24615;&#33021;&#25552;&#21319;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#20808;&#25277;&#21462;&#21518;&#29983;&#25104;&#27969;&#31243;&#21644;&#21477;&#23376;&#36873;&#25321;&#27169;&#22359;&#32531;&#35299;&#20102;&#20854;&#29983;&#25104;&#25688;&#35201;&#30340;&#24544;&#23454;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04193</link><description>&lt;p&gt;
&#37319;&#29992;ChatGPT&#36827;&#34892;&#25688;&#35201;&#25552;&#21462;&#20197;&#29983;&#25104;&#24544;&#23454;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Extractive Summarization via ChatGPT for Faithful Summary Generation. (arXiv:2304.04193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25277;&#21462;&#24335;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20854;ROUGE&#24471;&#20998;&#30456;&#23545;&#20110;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#25512;&#29702;&#23545;&#20854;&#24615;&#33021;&#25552;&#21319;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#20808;&#25277;&#21462;&#21518;&#29983;&#25104;&#27969;&#31243;&#21644;&#21477;&#23376;&#36873;&#25321;&#27169;&#22359;&#32531;&#35299;&#20102;&#20854;&#29983;&#25104;&#25688;&#35201;&#30340;&#24544;&#23454;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#21462;&#24335;&#25688;&#35201;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#25552;&#21462;&#21477;&#23376;&#23558;&#38271;&#25991;&#26723;&#21387;&#32553;&#20026;&#36739;&#30701;&#30340;&#29256;&#26412;&#12290;ChatGPT&#30340;&#24341;&#20837;&#24341;&#36215;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20854;&#20107;&#23454;&#24615;&#21644;&#24544;&#23454;&#24230;&#30340;&#25285;&#24551;&#38459;&#30861;&#20102;&#20854;&#22312;&#25688;&#35201;&#31995;&#32479;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;ChatGPT&#22312;&#25277;&#21462;&#24335;&#25688;&#35201;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#25581;&#31034;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#31995;&#32479;&#65292;ChatGPT&#22312;ROUGE&#20998;&#25968;&#26041;&#38754;&#30340;&#25277;&#21462;&#24335;&#25688;&#35201;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#25512;&#29702;&#23545;&#20854;&#24615;&#33021;&#25552;&#21319;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#37319;&#29992;&#20808;&#25277;&#21462;&#21518;&#29983;&#25104;&#27969;&#31243;&#20197;&#21450;&#21477;&#23376;&#36873;&#25321;&#27169;&#22359;&#21487;&#20197;&#32531;&#35299;ChatGPT&#29983;&#25104;&#25688;&#35201;&#30340;&#24544;&#23454;&#24615;&#38382;&#39064;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20351;&#29992;ChatGPT&#36827;&#34892;&#25277;&#21462;&#24335;&#25688;&#35201;&#25552;&#20379;&#20102;&#23616;&#38480;&#24615;&#21644;&#28508;&#22312;&#25913;&#36827;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extractive summarization is a crucial task in natural language processing that aims to condense long documents into shorter versions by directly extracting sentences. The recent introduction of ChatGPT has attracted significant interest in the NLP community due to its remarkable performance on a wide range of downstream tasks. However, concerns regarding factuality and faithfulness have hindered its practical applications for summarization systems. This paper first presents a thorough evaluation of ChatGPT's performance on extractive summarization and compares it with traditional fine-tuning methods on various benchmark datasets. Our experimental analysis reveals that ChatGPT's extractive summarization performance is still inferior to existing supervised systems in terms of ROUGE scores. In addition, we explore the effectiveness of in-context learning and chain-of-thought reasoning for enhancing its performance. Furthermore, we find that applying an extract-then-generate pipeline with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#26041;&#27861;&#26469;&#26816;&#27979;&#22312;&#32447;&#26032;&#38395;&#30340;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#65292;&#24182;&#21457;&#29616;&#22810;&#35821;&#26041;&#27861;&#27604;&#21333;&#35821;&#26041;&#27861;&#26356;&#22909;&#65292;&#20351;&#29992;&#31867;&#26435;&#37325;&#21644;&#26679;&#26412;&#26435;&#37325;&#30340;&#32452;&#21512;&#23545;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#29992;&#20110;&#24212;&#23545;&#22810;&#25968;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#22312;SemEval2023&#20219;&#21153;3&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65288;&#38646;&#26679;&#26412;&#65289;&#30340;&#23376;&#20219;&#21153;1&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;</title><link>http://arxiv.org/abs/2304.04190</link><description>&lt;p&gt;
QUST&#38431;&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#32508;&#21512;&#30740;&#31350;&#65306;&#26816;&#27979;&#22312;&#32447;&#26032;&#38395;&#30340;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques. (arXiv:2304.04190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#26041;&#27861;&#26469;&#26816;&#27979;&#22312;&#32447;&#26032;&#38395;&#30340;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#65292;&#24182;&#21457;&#29616;&#22810;&#35821;&#26041;&#27861;&#27604;&#21333;&#35821;&#26041;&#27861;&#26356;&#22909;&#65292;&#20351;&#29992;&#31867;&#26435;&#37325;&#21644;&#26679;&#26412;&#26435;&#37325;&#30340;&#32452;&#21512;&#23545;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#29992;&#20110;&#24212;&#23545;&#22810;&#25968;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#22312;SemEval2023&#20219;&#21153;3&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65288;&#38646;&#26679;&#26412;&#65289;&#30340;&#23376;&#20219;&#21153;1&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;QUST&#22242;&#38431;&#21442;&#21152;SemEval2023&#20219;&#21153;3&#30340;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#21333;&#35821;&#27169;&#22411;&#22312;&#20219;&#21153;&#26089;&#26399;&#23545;&#22810;&#25968;&#31867;&#36827;&#34892;&#20102;&#27424;&#37319;&#26679;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#31867;&#26435;&#37325;&#21644;&#26679;&#26412;&#26435;&#37325;&#30340;&#32452;&#21512;&#23545;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#20004;&#31181;&#19981;&#21516;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#20998;&#21035;&#20026;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#30456;&#20851;&#30340;&#12290;&#25152;&#26377;&#23454;&#39564;&#37117;&#22312;10&#25240;&#20132;&#21449;&#39564;&#35777;&#19979;&#36827;&#34892;&#65292;&#22810;&#35821;&#26041;&#27861;&#27604;&#21333;&#35821;&#26041;&#27861;&#26356;&#20855;&#20248;&#21183;&#12290;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65288;&#38646;&#26679;&#26412;&#65289;&#30340;&#23376;&#20219;&#21153;1&#20013;&#21462;&#24471;&#20102;&#31532;&#20108;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the participation of team QUST in the SemEval2023 task 3. The monolingual models are first evaluated with the under-sampling of the majority classes in the early stage of the task. Then, the pre-trained multilingual model is fine-tuned with a combination of the class weights and the sample weights. Two different fine-tuning strategies, the task-agnostic and the task-dependent, are further investigated. All experiments are conducted under the 10-fold cross-validation, the multilingual approaches are superior to the monolingual ones. The submitted system achieves the second best in Italian and Spanish (zero-shot) in subtask-1.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#25928;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20551;&#26032;&#38395;&#26816;&#27979;&#12290;&#25152;&#35774;&#35745;&#30340;&#30456;&#20284;&#24230;&#24863;&#30693;&#34701;&#21512;&#27169;&#22359;&#21487;&#20197;&#37327;&#21270;&#22320;&#26681;&#25454;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26435;&#34913;&#27599;&#20010;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#22122;&#22768;&#24178;&#25200;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#20063;&#35777;&#26126;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#24378;&#22823;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04187</link><description>&lt;p&gt;
&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Similarity-Aware Multimodal Prompt Learning for Fake News Detection. (arXiv:2304.04187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#25928;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20551;&#26032;&#38395;&#26816;&#27979;&#12290;&#25152;&#35774;&#35745;&#30340;&#30456;&#20284;&#24230;&#24863;&#30693;&#34701;&#21512;&#27169;&#22359;&#21487;&#20197;&#37327;&#21270;&#22320;&#26681;&#25454;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26435;&#34913;&#27599;&#20010;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#22122;&#22768;&#24178;&#25200;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#20063;&#35777;&#26126;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#24378;&#22823;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#26631;&#20934;&#33539;&#24335;&#20027;&#35201;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#26469;&#24314;&#31435;&#26032;&#38395;&#30340;&#30495;&#23454;&#24615;&#65292;&#28982;&#32780;&#65292;&#32593;&#19978;&#20551;&#26032;&#38395;&#30340;&#35805;&#35821;&#36890;&#24120;&#27604;&#36739;&#24494;&#22937;&#65292;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#25165;&#33021;&#20351;&#29992;&#25991;&#26412;&#20449;&#24687;&#25581;&#38706;&#20551;&#26032;&#38395;&#12290;&#26368;&#36817;&#65292;&#20851;&#27880;&#20110;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#30740;&#31350;&#24050;&#32463;&#36229;&#36234;&#20102;&#20165;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#21462;&#21333;&#27169;&#24577;&#29305;&#24449;&#65292;&#25110;&#30452;&#25509;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36825;&#25104;&#20026;&#26816;&#27979;&#20551;&#26032;&#38395;&#30340;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#23454;&#20363;&#65292;&#35201;&#20040;&#38656;&#35201;&#26356;&#26032;&#25972;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#38598;&#65292;&#19981;&#23454;&#38469;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#23558;&#36328;&#27169;&#24577;&#29305;&#24449;&#30452;&#25509;&#34701;&#21512;&#65292;&#32780;&#19981;&#32771;&#34385;&#19981;&#30456;&#20851;&#30340;&#35821;&#20041;&#34920;&#31034;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#21040;&#22810;&#27169;&#24577;&#29305;&#24449;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#23398;&#20064;&#65288;SAMPLE&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#22312;&#30456;&#20851;&#24615;&#26469;&#26377;&#25928;&#22320;&#34701;&#21512;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#30456;&#20284;&#24230;&#24863;&#30693;&#34701;&#21512;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#23398;&#20064;&#26681;&#25454;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#26435;&#34913;&#19981;&#21516;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22823;&#24133;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard paradigm for fake news detection mainly utilizes text information to model the truthfulness of news. However, the discourse of online fake news is typically subtle and it requires expert knowledge to use textual information to debunk fake news. Recently, studies focusing on multimodal fake news detection have outperformed text-only methods. Recent approaches utilizing the pre-trained model to extract unimodal features, or fine-tuning the pre-trained model directly, have become a new paradigm for detecting fake news. Again, this paradigm either requires a large number of training instances, or updates the entire set of pre-trained model parameters, making real-world fake news detection impractical. Furthermore, traditional multimodal methods fuse the cross-modal features directly without considering that the uncorrelated semantic representation might inject noise into the multimodal features. This paper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE) framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#20013;&#65292;&#21152;&#20837;&#35821;&#35843;&#26029;&#28857;&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#26377;&#29992;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#20854;&#26377;&#25928;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#35821;&#35843;&#27169;&#22411;&#39044;&#27979;&#26029;&#28857;&#30340;&#25925;&#20107;&#27604;&#26410;&#20351;&#29992;&#39044;&#27979;&#26029;&#28857;&#30340;&#25925;&#20107;&#26356;&#21463;&#27426;&#36814;&#12290;</title><link>http://arxiv.org/abs/2304.04157</link><description>&lt;p&gt;
&#22312;&#31471;&#21040;&#31471;&#30340;TTS&#31995;&#32479;&#20013;&#65292;&#35828;&#35805;&#20154;&#29420;&#31435;&#35821;&#35843;&#26029;&#28857;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An investigation of speaker independent phrase break models in End-to-End TTS systems. (arXiv:2304.04157v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#20013;&#65292;&#21152;&#20837;&#35821;&#35843;&#26029;&#28857;&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#26377;&#29992;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#20854;&#26377;&#25928;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#35821;&#35843;&#27169;&#22411;&#39044;&#27979;&#26029;&#28857;&#30340;&#25925;&#20107;&#27604;&#26410;&#20351;&#29992;&#39044;&#27979;&#26029;&#28857;&#30340;&#25925;&#20107;&#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25105;&#20204;&#23545;&#20110;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#20013;&#35821;&#35843;&#26029;&#28857;&#39044;&#27979;&#30340;&#30740;&#31350;&#65292;&#30740;&#31350;&#21160;&#26426;&#26159;&#65306;&#65288;&#19968;&#65289;&#22312;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#20013;&#34701;&#20837;&#26126;&#30830;&#30340;&#35821;&#35843;&#27169;&#22411;&#26159;&#21542;&#26377;&#29992;&#65311;&#65288;&#20108;&#65289;&#22914;&#20309;&#35780;&#20272;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#30340;&#35821;&#35843;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#65311;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#23545;&#20799;&#31461;&#25925;&#20107;&#21512;&#25104;&#30340;&#35821;&#22659;&#19979;&#30701;&#35821;&#26029;&#28857;&#39044;&#27979;&#27169;&#22411;&#30340;&#25928;&#29992;&#21644;&#26377;&#25928;&#24615;&#36827;&#34892;&#35780;&#20272;&#65292;&#20351;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#20026;&#21548;&#20247;&#29702;&#35299;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#21548;&#21147;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35843;&#27169;&#22411;&#39044;&#27979;&#30701;&#35821;&#26029;&#28857;&#20301;&#32622;&#21512;&#25104;&#30340;&#25925;&#20107;&#27604;&#30452;&#25509;&#21512;&#25104;&#30340;&#25925;&#20107;&#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our work on phrase break prediction in the context of end-to-end TTS systems, motivated by the following questions: (i) Is there any utility in incorporating an explicit phrasing model in an end-to-end TTS system?, and (ii) How do you evaluate the effectiveness of a phrasing model in an end-to-end TTS system? In particular, the utility and effectiveness of phrase break prediction models are evaluated in in the context of childrens story synthesis, using listener comprehension. We show by means of perceptual listening evaluations that there is a clear preference for stories synthesized after predicting the location of phrase breaks using a trained phrasing model, over stories directly synthesized without predicting the location of phrase breaks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25345;&#32493;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;(ContGCN)&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#22343;&#25552;&#20986;&#20102;&#26032;&#30340;&#20840;&#35789;&#20803;-&#20219;&#24847;&#25991;&#26723;&#33539;&#24335;&#65292;&#36890;&#36807;&#19968;&#20010;&#20986;&#29616;&#35760;&#24518;&#27169;&#22359;&#21644;&#19968;&#20010;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#26080;&#26631;&#31614;&#30340;&#26356;&#26032;ContGCN&#65292;&#21487;&#23558;&#20174;&#35266;&#23519;&#25991;&#26723;&#25512;&#24191;&#21040;&#26410;&#35266;&#23519;&#25991;&#26723;&#30340;&#25512;&#29702;&#29992;&#20110;&#22312;&#32447;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.04152</link><description>&lt;p&gt;
&#25345;&#32493;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Continual Graph Convolutional Network for Text Classification. (arXiv:2304.04152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25345;&#32493;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;(ContGCN)&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#22343;&#25552;&#20986;&#20102;&#26032;&#30340;&#20840;&#35789;&#20803;-&#20219;&#24847;&#25991;&#26723;&#33539;&#24335;&#65292;&#36890;&#36807;&#19968;&#20010;&#20986;&#29616;&#35760;&#24518;&#27169;&#22359;&#21644;&#19968;&#20010;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#26080;&#26631;&#31614;&#30340;&#26356;&#26032;ContGCN&#65292;&#21487;&#23558;&#20174;&#35266;&#23519;&#25991;&#26723;&#25512;&#24191;&#21040;&#26410;&#35266;&#23519;&#25991;&#26723;&#30340;&#25512;&#29702;&#29992;&#20110;&#22312;&#32447;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#25429;&#33719;&#20102;&#20840;&#23616;&#38750;&#36830;&#32493;&#21644;&#38271;&#36317;&#31163;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22522;&#20110;GCN&#30340;&#26041;&#27861;&#22312;&#31163;&#32447;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#36981;&#24490;&#19968;&#20010;&#24050;&#30693;&#30340;&#35789;&#20803;-&#24050;&#30693;&#30340;&#25991;&#26723;&#33539;&#20363;&#65292;&#26500;&#36896;&#20102;&#19968;&#20010;&#22266;&#23450;&#30340;&#25991;&#26723;-&#35789;&#20803;&#22270;&#65292;&#24182;&#19981;&#33021;&#23545;&#26032;&#30340;&#25991;&#26723;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#32447;&#31995;&#32479;&#38754;&#20020;&#23558;&#23427;&#20204;&#37096;&#32626;&#21040;&#25512;&#26029;&#27969;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25345;&#32493;&#30340;GCN&#27169;&#22411;&#65288;ContGCN&#65289;&#65292;&#20197;&#23558;&#20174;&#35266;&#23519;&#25991;&#26723;&#25512;&#24191;&#21040;&#26410;&#35266;&#23519;&#25991;&#26723;&#30340;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20840;&#35789;&#20803;-&#20219;&#24847;&#25991;&#26723;&#33539;&#24335;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#31995;&#32479;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#20013;&#65292;&#22312;&#27599;&#20010;&#25209;&#27425;&#21160;&#24577;&#26356;&#26032;&#25991;&#26723;-&#35789;&#20803;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20986;&#29616;&#35760;&#24518;&#27169;&#22359;&#21644;&#19968;&#20010;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#20197;&#26080;&#26631;&#31614;&#30340;&#26041;&#24335;&#26356;&#26032;ContGCN&#12290;&#21326;&#20026;&#20844;&#20849;&#35266;&#28857;&#20998;&#26512;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#20026;&#26399;3&#20010;&#26376;&#30340;A/B&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional network (GCN) has been successfully applied to capture global non-consecutive and long-distance semantic information for text classification. However, while GCN-based methods have shown promising results in offline evaluations, they commonly follow a seen-token-seen-document paradigm by constructing a fixed document-token graph and cannot make inferences on new documents. It is a challenge to deploy them in online systems to infer steaming text data. In this work, we present a continual GCN model (ContGCN) to generalize inferences from observed documents to unobserved documents. Concretely, we propose a new all-token-any-document paradigm to dynamically update the document-token graph in every batch during both the training and testing phases of an online system. Moreover, we design an occurrence memory module and a self-supervised contrastive learning objective to update ContGCN in a label-free manner. A 3-month A/B test on Huawei public opinion analysis system sho
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#23545;&#20110;&#38271;&#25991;&#26412;&#24515;&#29702;&#38556;&#30861;&#21407;&#22240;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;Longformer&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#25512;&#21160;&#24515;&#29702;&#30142;&#30149;&#22240;&#26524;&#20998;&#26512;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.04118</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#20013;&#24515;&#29702;&#38556;&#30861;&#21407;&#22240;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-class Categorization of Reasons behind Mental Disturbance in Long Texts. (arXiv:2304.04118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#23545;&#20110;&#38271;&#25991;&#26412;&#24515;&#29702;&#38556;&#30861;&#21407;&#22240;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;Longformer&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#25512;&#21160;&#24515;&#29702;&#30142;&#30149;&#22240;&#26524;&#20998;&#26512;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25512;&#26029;&#29992;&#25143;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28608;&#21457;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#38416;&#36848;&#20102;&#22312;&#33258;&#25105;&#25253;&#21578;&#25991;&#26412;&#20013;&#26597;&#25214;&#24515;&#29702;&#30142;&#30149;&#22240;&#26524;&#25351;&#26631;&#30340;&#38382;&#39064;&#12290;&#36807;&#21435;&#65292;&#25105;&#20204;&#35265;&#35777;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#38024;&#23545;Facebook&#25968;&#25454;&#30340;&#22240;&#26524;&#35299;&#37322;&#20998;&#26512;&#30740;&#31350;&#12290;&#38024;&#23545;Reddit&#24086;&#23376;&#30340;&#22810;&#31867;&#21035;&#22240;&#26524;&#20998;&#31867;&#30340;Transformer&#27169;&#22411;&#26041;&#38754;&#30340;&#35843;&#26597;&#65292;&#25351;&#20986;&#20102;&#20351;&#29992;&#21253;&#21547;&#22810;&#36798;4000&#20010;&#21333;&#35789;&#30340;&#38271;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;Longformer&#65292;&#24182;&#23558;&#20854;&#32534;&#30721;&#37096;&#32626;&#22312;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#19978;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Longformer&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21253;&#21547;62&#65285; F1-score&#30340;&#25968;&#25454;&#38598;M-CAMS&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#22240;&#26524;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#35777;&#26126;&#20102;Longformer&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#24037;&#20316;&#26377;&#21161;&#20110;&#25512;&#21160;&#25233;&#37057;&#30151;&#21644;&#33258;&#26432;&#30340;&#22240;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated with recent advances in inferring users' mental state in social media posts, we identify and formulate the problem of finding causal indicators behind mental illness in self-reported text. In the past, we witness the presence of rule-based studies for causal explanation analysis on curated Facebook data. The investigation on transformer-based model for multi-class causal categorization in Reddit posts point to a problem of using long-text which contains as many as 4000 words. Developing end-to-end transformer-based models subject to the limitation of maximum-length in a given instance. To handle this problem, we use Longformer and deploy its encoding on transformer-based classifier. The experimental results show that Longformer achieves new state-of-the-art results on M-CAMS, a publicly available dataset with 62\% F1-score. Cause-specific analysis and ablation study prove the effectiveness of Longformer. We believe our work facilitates causal analysis of depression and suicid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.04099</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#23884;&#20837;&#20174;&#36830;&#32493;&#26032;&#38395;&#27969;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#23454;&#26102;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#25925;&#20107;&#65292;&#26377;&#21161;&#20110;&#20154;&#20204;&#22312;&#19981;&#38656;&#35201;&#26114;&#36149;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30740;&#31350;&#30340;&#26222;&#36941;&#26041;&#27861;&#26159;&#29992;&#31526;&#21495;&#25110;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#23558;&#23427;&#20204;&#36880;&#27493;&#32858;&#31867;&#25104;&#25925;&#20107;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#36827;&#19968;&#27493;&#25913;&#21892;&#23884;&#20837;&#65292;&#20294;&#26159;&#36890;&#36807;&#26080;&#24046;&#21035;&#22320;&#32534;&#30721;&#25991;&#31456;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#26469;&#30452;&#25509;&#37319;&#29992;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;&#23500;&#21547;&#25991;&#26412;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#26032;&#38395;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#21477;&#23376;&#32534;&#30721;&#22120;&#26469;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30340;&#24819;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#26694;&#26550;USTORY&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#65292;&#21363;&#20027;&#39064;&#21644;&#26102;&#38388;&#24863;&#30693;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26032;&#39062;&#24615;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fuel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#26377;&#23475;&#35780;&#35770;&#12290;&#20351;&#29992;LSTM&#21644;BERT&#23884;&#20837;&#23454;&#29616;&#20102;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;LSTM&#19982;&#27880;&#24847;&#26426;&#21046;&#32452;&#21512;&#23454;&#29616;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#20934;&#30830;&#29575;&#21644;&#21152;&#26435;F1-score&#22343;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.04087</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#23391;&#21152;&#25289;&#26377;&#23475;&#35780;&#35770;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpretable Multi Labeled Bengali Toxic Comments Classification using Deep Learning. (arXiv:2304.04087v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#26377;&#23475;&#35780;&#35770;&#12290;&#20351;&#29992;LSTM&#21644;BERT&#23884;&#20837;&#23454;&#29616;&#20102;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;LSTM&#19982;&#27880;&#24847;&#26426;&#21046;&#32452;&#21512;&#23454;&#29616;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#20934;&#30830;&#29575;&#21644;&#21152;&#26435;F1-score&#22343;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#26696;&#26469;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#26377;&#23475;&#35780;&#35770;&#65292;&#39318;&#20808;&#20351;&#29992;&#20108;&#20803;&#20998;&#31867;&#27169;&#22411;&#30830;&#23450;&#35780;&#35770;&#26159;&#21542;&#26377;&#23475;&#65292;&#28982;&#21518;&#20351;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#30830;&#23450;&#35813;&#35780;&#35770;&#23646;&#20110;&#21738;&#31181;&#27602;&#24615;&#31867;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20934;&#22791;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;16,073&#20010;&#23454;&#20363;&#65292;&#20854;&#20013;8,488&#20010;&#26159;&#26377;&#23475;&#30340;&#65292;&#24182;&#19988;&#20219;&#20309;&#26377;&#23475;&#30340;&#35780;&#35770;&#21487;&#33021;&#21516;&#26102;&#23646;&#20110;&#20845;&#31181;&#26377;&#23475;&#31867;&#22411;-&#20302;&#20439;&#65292;&#20167;&#24680;&#65292;&#23447;&#25945;&#65292;&#23041;&#32961;&#65292;&#24694;&#24847;&#21644;&#20398;&#36785;&#12290;&#22312;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;LSTM&#21644;BERT&#23884;&#20837;&#23454;&#29616;&#20102;89.42&#65285;&#30340;&#20934;&#30830;&#29575;&#65307;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#26041;&#38754;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#21521;LSTM&#65288;CNN-BiLSTM&#65289;&#19982;&#27880;&#24847;&#26426;&#21046;&#32452;&#21512;&#65292;&#33719;&#24471;&#20102;78.92&#65285;&#30340;&#20934;&#30830;&#29575;&#21644;0.86&#30340;&#21152;&#26435;F1-score&#12290;&#20026;&#20102;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#24182;&#35299;&#37322;&#20998;&#31867;&#26399;&#38388;&#30340;&#21333;&#35789;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;LIME&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning-based pipeline for categorizing Bengali toxic comments, in which at first a binary classification model is used to determine whether a comment is toxic or not, and then a multi-label classifier is employed to determine which toxicity type the comment belongs to. For this purpose, we have prepared a manually labeled dataset consisting of 16,073 instances among which 8,488 are Toxic and any toxic comment may correspond to one or more of the six toxic categories - vulgar, hate, religious, threat, troll, and insult simultaneously. Long Short Term Memory (LSTM) with BERT Embedding achieved 89.42% accuracy for the binary classification task while as a multi-label classifier, a combination of Convolutional Neural Network and Bi-directional Long Short Term Memory (CNN-BiLSTM) with attention mechanism achieved 78.92% accuracy and 0.86 as weighted F1-score. To explain the predictions and interpret the word feature importance during classification by the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20110;SemEval-2023&#30340;&#20219;&#21153;9&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#20102;&#38598;&#25104;&#23398;&#20064;&#65292;&#22312;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#26816;&#27979;&#20013;&#25490;&#21517;&#31532;4&#65292;&#36798;&#21040;&#20102;0.5688&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#27599;&#20010;&#25512;&#29305;&#37117;&#36827;&#34892;&#20102;&#33521;&#25991;&#32763;&#35793;&#30340;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2304.04054</link><description>&lt;p&gt;
tmn&#22312;SemEval-2023&#20219;&#21153;9&#20013;&#30340;&#24212;&#29992;&#65306;&#20351;&#29992;XLM-T&#12289;Google&#32763;&#35793;&#21644;&#38598;&#25104;&#23398;&#20064;&#36827;&#34892;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
tmn at SemEval-2023 Task 9: Multilingual Tweet Intimacy Detection using XLM-T, Google Translate, and Ensemble Learning. (arXiv:2304.04054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20110;SemEval-2023&#30340;&#20219;&#21153;9&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#20102;&#38598;&#25104;&#23398;&#20064;&#65292;&#22312;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#26816;&#27979;&#20013;&#25490;&#21517;&#31532;4&#65292;&#36798;&#21040;&#20102;0.5688&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#27599;&#20010;&#25512;&#29305;&#37117;&#36827;&#34892;&#20102;&#33521;&#25991;&#32763;&#35793;&#30340;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#31995;&#32479;&#65292;&#38024;&#23545;SemEval-2023&#20219;&#21153;9&#65306;&#22810;&#35821;&#35328;&#25512;&#29305;&#20146;&#23494;&#24230;&#20998;&#26512;&#36827;&#34892;&#35774;&#35745;&#12290;&#20219;&#21153;&#30340;&#30446;&#30340;&#26159;&#39044;&#27979;&#19968;&#31995;&#21015;&#25512;&#29305;&#30340;&#20146;&#23494;&#24230;&#65292;&#33539;&#22260;&#20174;1&#65288;&#23436;&#20840;&#19981;&#20146;&#23494;&#65289;&#21040;5&#65288;&#38750;&#24120;&#20146;&#23494;&#65289;&#12290;&#27604;&#36187;&#30340;&#23448;&#26041;&#35757;&#32451;&#38598;&#21253;&#21547;&#20845;&#31181;&#35821;&#35328;&#30340;&#25512;&#29305;&#65288;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#12289;&#27861;&#35821;&#21644;&#20013;&#25991;&#65289;&#12290;&#27979;&#35797;&#38598;&#21253;&#25324;&#20845;&#31181;&#32473;&#23450;&#30340;&#35821;&#35328;&#20197;&#21450;&#22806;&#37096;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#35757;&#32451;&#38598;&#20013;&#26410;&#20986;&#29616;&#30340;&#22235;&#31181;&#35821;&#35328;&#65288;&#21360;&#22320;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#33655;&#20848;&#35821;&#21644;&#38889;&#35821;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;XLM-T&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#36866;&#29992;&#20110;Twitter&#39046;&#22495;&#30340;&#22810;&#35821;&#31181;RoBERTa&#27169;&#22411;&#30340;&#38598;&#25104;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#26410;&#35265;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25105;&#20204;&#23545;&#27599;&#26465;&#25512;&#29305;&#36827;&#34892;&#20102;&#33521;&#25991;&#32763;&#35793;&#30340;&#34917;&#20805;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#23558;&#32763;&#35793;&#25968;&#25454;&#24212;&#29992;&#20110;&#24494;&#35843;&#20013;&#30475;&#21040;&#30340;&#35821;&#35328;&#19982;&#26410;&#30475;&#21040;&#30340;&#35821;&#35328;&#30340;transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20272;&#35745;&#20351;&#29992;&#32763;&#35793;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;50&#20010;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;4&#65292;&#24182;&#23454;&#29616;&#20102;0.5688&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper describes a transformer-based system designed for SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis. The purpose of the task was to predict the intimacy of tweets in a range from 1 (not intimate at all) to 5 (very intimate). The official training set for the competition consisted of tweets in six languages (English, Spanish, Italian, Portuguese, French, and Chinese). The test set included the given six languages as well as external data with four languages not presented in the training set (Hindi, Arabic, Dutch, and Korean). We presented a solution based on an ensemble of XLM-T, a multilingual RoBERTa model adapted to the Twitter domain. To improve the performance of unseen languages, each tweet was supplemented by its English translation. We explored the effectiveness of translated data for the languages seen in fine-tuning compared to unseen languages and estimated strategies for using translated data in transformer-based models. Our solution ranked 4th on the leade
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#20998;&#26512;&#20102;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04052</link><description>&lt;p&gt;
&#20165;&#35299;&#30721;&#22120;&#25110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65311;&#23558;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#20026;&#27491;&#21017;&#21270;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder. (arXiv:2304.04052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#20998;&#26512;&#20102;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#20219;&#21153;&#26088;&#22312;&#22522;&#20110;&#32473;&#23450;&#30340;&#36755;&#20837;&#28304;&#24207;&#21015;&#29983;&#25104;&#30446;&#26631;&#24207;&#21015;&#12290; &#20256;&#32479;&#19978;&#65292;&#22823;&#22810;&#25968;seq2seq&#20219;&#21153;&#37117;&#26159;&#36890;&#36807;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#35299;&#20915;&#30340;&#65292;&#35813;&#26694;&#26550;&#38656;&#35201;&#32534;&#30721;&#22120;&#26469;&#32534;&#30721;&#28304;&#24207;&#21015;&#65292;&#24182;&#19988;&#38656;&#35201;&#35299;&#30721;&#22120;&#26469;&#29983;&#25104;&#30446;&#26631;&#25991;&#26412;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#26032;&#26041;&#27861;&#65292;&#23558;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;seq2seq&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#23558;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;seq2seq&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#23545;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#26377;&#25928;&#24615;&#30340;&#24443;&#24213;&#20998;&#26512;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#36827;&#34892;&#20998;&#26512;&#26469;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#12290;&#35813;&#32467;&#26500;&#26088;&#22312;&#22797;&#21046;&#32463;&#20856;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#34892;&#20026;&#65292;&#20294;&#20855;&#26377;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#26356;&#23481;&#26131;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#36896;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934; bipol &#20197;&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#31038;&#20132;&#20559;&#35265;&#12290;&#35813;&#26631;&#20934;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#35780;&#20272;&#21644;&#21477;&#23376;&#32423;&#21035;&#35780;&#20272;&#20004;&#20010;&#27493;&#39588;&#65292;&#24182;&#20351;&#29992; SotA &#26550;&#26500;&#21019;&#24314;&#20102;&#26032;&#27169;&#22411;&#20197;&#26816;&#27979;&#22810;&#20010;&#36724;&#30340;&#20559;&#24046;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20559;&#35265;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#20844;&#24320;&#20102;&#30456;&#20851;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2304.04029</link><description>&lt;p&gt;
Bipol: &#19968;&#31181;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22810;&#36724;&#20559;&#35265;&#35780;&#20272;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Bipol: A Novel Multi-Axes Bias Evaluation Metric with Explainability for NLP. (arXiv:2304.04029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#36896;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934; bipol &#20197;&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#31038;&#20132;&#20559;&#35265;&#12290;&#35813;&#26631;&#20934;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#35780;&#20272;&#21644;&#21477;&#23376;&#32423;&#21035;&#35780;&#20272;&#20004;&#20010;&#27493;&#39588;&#65292;&#24182;&#20351;&#29992; SotA &#26550;&#26500;&#21019;&#24314;&#20102;&#26032;&#27169;&#22411;&#20197;&#26816;&#27979;&#22810;&#20010;&#36724;&#30340;&#20559;&#24046;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20559;&#35265;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#20844;&#24320;&#20102;&#30456;&#20851;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934; bipol&#65292;&#29992;&#20110;&#20272;&#31639;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#31038;&#20132;&#20559;&#35265;&#12290;&#26377;&#23475;&#20559;&#35265;&#22312;&#35768;&#22810;&#22312;&#32447;&#25968;&#25454;&#28304;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#21019;&#36896;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#22522;&#20110;&#27169;&#22411;&#20998;&#31867;&#30340;&#35821;&#26009;&#24211;&#32423;&#21035;&#35780;&#20272;&#21644;&#22522;&#20110;&#65288;&#25935;&#24863;&#65289;&#35789;&#39057;&#65288;TF&#65289;&#30340;&#21477;&#23376;&#32423;&#21035;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;SotA&#26550;&#26500;&#21019;&#24314;&#20102;&#26032;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#27839;&#22810;&#20010;&#36724;&#30340;&#20559;&#24046;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;NLP&#25968;&#25454;&#38598;&#65288;COPA&#21644;SQUAD&#65289;&#12290;&#20316;&#20026;&#38468;&#21152;&#36129;&#29486;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#20960;&#20046;&#26377;200&#19975;&#20010;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#65289;&#65292;&#29992;&#20110;&#35757;&#32451;&#20559;&#35265;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#20844;&#24320;&#20102;&#23427;&#12290;&#25105;&#20204;&#36824;&#20844;&#24320;&#20102;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce bipol, a new metric with explainability, for estimating social bias in text data. Harmful bias is prevalent in many online sources of data that are used for training machine learning (ML) models. In a step to address this challenge we create a novel metric that involves a two-step process: corpus-level evaluation based on model classification and sentence-level evaluation based on (sensitive) term frequency (TF). After creating new models to detect bias along multiple axes using SotA architectures, we evaluate two popular NLP datasets (COPA and SQUAD). As additional contribution, we created a large dataset (with almost 2 million labelled samples) for training models in bias detection and make it publicly available. We also make public our codes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#21487;&#35266;&#35268;&#27169;&#30340;&#20154;&#24037;&#26631;&#35760;&#30340;&#26031;&#27931;&#20240;&#20811;NER&#25968;&#25454;&#38598;WikiGoldSK&#65292;&#36890;&#36807;&#35780;&#20272;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#29616;&#26377;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#23454;&#39564;&#12290;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04026</link><description>&lt;p&gt;
WikiGoldSK:&#26031;&#27931;&#20240;&#20811;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24102;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#22522;&#20934;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
WikiGoldSK: Annotated Dataset, Baselines and Few-Shot Learning Experiments for Slovak Named Entity Recognition. (arXiv:2304.04026v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#21487;&#35266;&#35268;&#27169;&#30340;&#20154;&#24037;&#26631;&#35760;&#30340;&#26031;&#27931;&#20240;&#20811;NER&#25968;&#25454;&#38598;WikiGoldSK&#65292;&#36890;&#36807;&#35780;&#20272;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#29616;&#26377;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#23454;&#39564;&#12290;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#31181;&#22522;&#30784;&#30340;NLP&#20219;&#21153;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;NER&#26041;&#27861;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#39640;&#36136;&#37327;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20294;&#23545;&#20110;&#19968;&#20123;&#35821;&#35328;&#20173;&#19981;&#23384;&#22312;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;WikiGoldSK&#26469;&#35299;&#20915;&#26031;&#27931;&#20240;&#20811;&#35821;&#20013;&#36825;&#31181;&#24773;&#20917;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#21487;&#35266;&#35268;&#27169;&#30340;&#20154;&#24037;&#26631;&#35760;&#30340;&#26031;&#27931;&#20240;&#20811;NER&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#30340;&#38134;&#26631;&#20934;&#26031;&#27931;&#20240;&#20811;&#35821;NER&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#26469;&#23545;&#20854;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#22312;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25903;&#25345;&#26410;&#26469;&#22522;&#20110;&#26031;&#27931;&#20240;&#20811;NER&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312;https://github.com/NaiveNeuron/WikiGoldSK&#20844;&#24320;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#65292;&#37319;&#29992;&#21487;&#20801;&#35768;&#30340;&#35768;&#21487;&#26465;&#27454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a fundamental NLP tasks with a wide range of practical applications. The performance of state-of-the-art NER methods depends on high quality manually anotated datasets which still do not exist for some languages. In this work we aim to remedy this situation in Slovak by introducing WikiGoldSK, the first sizable human labelled Slovak NER dataset. We benchmark it by evaluating state-of-the-art multilingual Pretrained Language Models and comparing it to the existing silver-standard Slovak NER dataset. We also conduct few-shot experiments and show that training on a sliver-standard dataset yields better results. To enable future work that can be based on Slovak NER, we release the dataset, code, as well as the trained models publicly under permissible licensing terms at https://github.com/NaiveNeuron/WikiGoldSK.
&lt;/p&gt;</description></item><item><title>MphayaNER&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#33576;&#27721;&#25991;&#36798;&#35821;&#30340;NER&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#36890;&#36807;&#22312;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#33576;&#25991;&#36798;&#35821;&#19982;&#20854;&#20182;&#30456;&#20851;&#29677;&#22270;&#35821;&#20043;&#38388;&#30340;&#38646;&#26679;&#26412;&#36716;&#31227;&#12290;&#29992;chiShona&#25968;&#25454;&#25193;&#20805;MphayaNER&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03952</link><description>&lt;p&gt;
MphayaNER&#65306;&#36866;&#29992;&#20110;&#33576;&#27721;&#25991;&#36798;&#35821;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MphayaNER: Named Entity Recognition for Tshivenda. (arXiv:2304.03952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03952
&lt;/p&gt;
&lt;p&gt;
MphayaNER&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#33576;&#27721;&#25991;&#36798;&#35821;&#30340;NER&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#36890;&#36807;&#22312;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#33576;&#25991;&#36798;&#35821;&#19982;&#20854;&#20182;&#30456;&#20851;&#29677;&#22270;&#35821;&#20043;&#38388;&#30340;&#38646;&#26679;&#26412;&#36716;&#31227;&#12290;&#29992;chiShona&#25968;&#25454;&#25193;&#20805;MphayaNER&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22914;&#20449;&#24687;&#26816;&#32034;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#26377;&#38480;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#32780;&#35328;&#65292;NER&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;MphayaNER&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#26159;&#26032;&#38395;&#39046;&#22495;&#20013;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#33576;&#27721;&#25991;&#36798;&#35821;&#30340;NER&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MphayaNER&#19978;\&#24494;&#35843;\&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26469;&#24314;&#31435;NER&#22522;&#32447;&#12290;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#33576;&#25991;&#36798;&#35821;&#19982;&#20854;&#20182;&#30456;&#20851;&#29677;&#22270;&#35821;&#20043;&#38388;&#30340;&#38646;&#26679;&#26412;&#36716;&#31227;&#65292;&#20854;&#20013;chiShona&#21644;Kiswahili&#34920;&#29616;&#26368;&#20339;&#12290;&#21457;&#29616;&#29992;chiShona&#25968;&#25454;&#25193;&#20805;MphayaNER&#20063;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;MphayaNER&#21644;&#22522;&#32447;&#27169;&#22411;&#37117;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) plays a vital role in various Natural Language Processing tasks such as information retrieval, text classification, and question answering. However, NER can be challenging, especially in low-resource languages with limited annotated datasets and tools. This paper adds to the effort of addressing these challenges by introducing MphayaNER, the first Tshivenda NER corpus in the news domain. We establish NER baselines by \textit{fine-tuning} state-of-the-art models on MphayaNER. The study also explores zero-shot transfer between Tshivenda and other related Bantu languages, with chiShona and Kiswahili showing the best results. Augmenting MphayaNER with chiShona data was also found to improve model performance significantly. Both MphayaNER and the baseline models are made publicly available.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#35299;&#37322;&#30340;&#28508;&#21147;&#65292;&#20197;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#29702;&#35299;&#21644;&#35299;&#37322;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.03938</link><description>&lt;p&gt;
&#23398;&#29983;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#21019;&#24314;&#30340;&#20195;&#30721;&#35299;&#37322;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparing Code Explanations Created by Students and Large Language Models. (arXiv:2304.03938v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#35299;&#37322;&#30340;&#28508;&#21147;&#65292;&#20197;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#29702;&#35299;&#21644;&#35299;&#37322;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20195;&#30721;&#24182;&#35299;&#37322;&#20854;&#29992;&#36884;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#30340;&#22522;&#26412;&#25216;&#33021;&#12290;&#22312;&#35745;&#31639;&#26426;&#25945;&#32946;&#39046;&#22495;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#23398;&#29983;&#35299;&#37322;&#20195;&#30721;&#33021;&#21147;&#19982;&#32534;&#20889;&#21644;&#36861;&#36394;&#20195;&#30721;&#31561;&#20854;&#20182;&#25216;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29305;&#21035;&#26159;&#65292;&#20197;&#39640;&#25277;&#35937;&#32423;&#21035;&#25551;&#36848;&#20195;&#30721;&#22312;&#25152;&#26377;&#21487;&#33021;&#36755;&#20837;&#19979;&#30340;&#34892;&#20026;&#30340;&#33021;&#21147;&#24378;&#28872;&#20851;&#32852;&#30528;&#20195;&#30721;&#32534;&#20889;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#23398;&#29983;&#26469;&#35828;&#65292;&#24320;&#21457;&#29702;&#35299;&#21644;&#20934;&#30830;&#31616;&#27905;&#22320;&#35299;&#37322;&#20195;&#30721;&#30340;&#19987;&#19994;&#30693;&#35782;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#25945;&#23398;&#26041;&#27861;&#24182;&#26410;&#23454;&#29616;&#29983;&#20135;&#21363;&#26102;&#33539;&#20363;&#20195;&#30721;&#35299;&#37322;&#20197;&#36827;&#34892;&#25351;&#23548;&#30340;&#22823;&#35268;&#27169;&#35838;&#22530;&#30340;&#27493;&#39588;&#12290;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#20986;&#29616;&#36817;&#26399;&#21487;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102; LLMs &#29983;&#25104;&#21487;&#20197;&#20316;&#20026;&#31034;&#20363;&#26469;&#25903;&#25345;&#23398;&#29983;&#29702;&#35299;&#21644;&#35299;&#37322;&#20195;&#30721;&#30340;&#35299;&#37322;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about code and explaining its purpose are fundamental skills for computer scientists. There has been extensive research in the field of computing education on the relationship between a student's ability to explain code and other skills such as writing and tracing code. In particular, the ability to describe at a high-level of abstraction how code will behave over all possible inputs correlates strongly with code writing skills. However, developing the expertise to comprehend and explain code accurately and succinctly is a challenge for many students. Existing pedagogical approaches that scaffold the ability to explain code, such as producing exemplar code explanations on demand, do not currently scale well to large classrooms. The recent emergence of powerful large language models (LLMs) may offer a solution. In this paper, we explore the potential of LLMs in generating explanations that can serve as examples to scaffold students' ability to understand and explain code. To e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#27169;&#24335;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#21033;&#29992;&#36879;&#35270;&#25439;&#22833;&#26469;&#34701;&#21512;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;IEMOCAP&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03899</link><description>&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#23454;&#35777;&#30740;&#31350;&#19982;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study and Improvement for Speech Emotion Recognition. (arXiv:2304.03899v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#27169;&#24335;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#21033;&#29992;&#36879;&#35270;&#25439;&#22833;&#26469;&#34701;&#21512;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;IEMOCAP&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26088;&#22312;&#20174;&#38899;&#39057;&#21644;&#25991;&#26412;&#20013;&#26816;&#27979;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#21033;&#29992;&#20808;&#36827;&#32593;&#32476;&#23545;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#21644;&#34701;&#21512;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#24573;&#30053;&#20102;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#23545;&#24773;&#24863;&#35782;&#21035;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#65306;&#22914;&#20309;&#34701;&#21512;&#38899;&#39057;&#21644;&#25991;&#26412;&#27169;&#24577;&#20449;&#24687;&#23545;&#20110;&#22810;&#27169;&#24335;&#20219;&#21153;&#26356;&#26377;&#24110;&#21161;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#36879;&#35270;&#25439;&#22833;&#25913;&#36827;&#30340;&#22810;&#27169;&#24335;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;IEMOCAP&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#28145;&#20837;&#20998;&#26512;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#25913;&#36827;&#30340;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#25913;&#36827;&#24182;&#36229;&#36807;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal speech emotion recognition aims to detect speakers' emotions from audio and text. Prior works mainly focus on exploiting advanced networks to model and fuse different modality information to facilitate performance, while neglecting the effect of different fusion strategies on emotion recognition. In this work, we consider a simple yet important problem: how to fuse audio and text modality information is more helpful for this multimodal task. Further, we propose a multimodal emotion recognition model improved by perspective loss. Empirical results show our method obtained new state-of-the-art results on the IEMOCAP dataset. The in-depth analysis explains why the improved model can achieve improvements and outperforms baselines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#34917;&#20805;&#21477;&#23376;&#65292;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#35821;&#20041;&#21305;&#37197;&#65292;&#24182;&#20351;&#29992;&#20851;&#38190;&#35789;&#36991;&#20813;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.03898</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#30693;&#35782;&#30340;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning. (arXiv:2304.03898v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03898
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#34917;&#20805;&#21477;&#23376;&#65292;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#35821;&#20041;&#21305;&#37197;&#65292;&#24182;&#20351;&#29992;&#20851;&#38190;&#35789;&#36991;&#20813;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30701;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#22312;&#24191;&#21578;&#25628;&#32034;&#21644;&#25512;&#33616;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#30001;&#20110;&#25991;&#26412;&#38271;&#24230;&#30701;&#65292;&#35821;&#20041;&#20449;&#24687;&#21294;&#20047;&#21644;&#21333;&#35789;&#27495;&#20041;&#38382;&#39064;&#25104;&#20026;&#27492;&#31867;&#20219;&#21153;&#30340;&#38590;&#28857;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#25991;&#26412;&#34917;&#20805;&#21477;&#23376;&#25110;&#30693;&#35782;&#24211;&#26469;&#25552;&#20379;&#38468;&#21152;&#30340;&#29305;&#24449;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#22320;&#20132;&#20114;&#21407;&#22987;&#21477;&#23376;&#21644;&#34917;&#20805;&#21477;&#23376;&#65292;&#20063;&#27809;&#26377;&#32771;&#34385;&#21040;&#22806;&#37096;&#30693;&#35782;&#24211;&#24341;&#20837;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#23545;&#24212;&#30340;&#34917;&#20805;&#21477;&#23376;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25351;&#23548;&#27169;&#22411;&#33719;&#24471;&#26356;&#20855;&#35821;&#20041;&#21305;&#37197;&#24615;&#30340;&#21407;&#22987;&#21477;&#23376;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36991;&#20813;&#22122;&#22768;&#65292;&#25105;&#20204;&#20351;&#29992;&#20851;&#38190;&#35789;&#20316;&#20026;&#21407;&#22987;&#21477;&#23376;&#30340;&#20027;&#35201;&#35821;&#20041;&#36827;&#34892;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, short Text Matching tasks have been widely applied in the fields ofadvertising search and recommendation. The difficulty lies in the lack of semantic information and word ambiguity caused by the short length of the text. Previous works have introduced complement sentences or knowledge bases to provide additional feature information. However, these methods have not fully interacted between the original sentence and the complement sentence, and have not considered the noise issue that may arise from the introduction of external knowledge bases. Therefore, this paper proposes a short Text Matching model that combines contrastive learning and external knowledge. The model uses a generative model to generate corresponding complement sentences and uses the contrastive learning method to guide the model to obtain more semantically meaningful encoding of the original sentence. In addition, to avoid noise, we use keywords as the main semantics of the original sentence to retrie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;Factify 2&#65292;&#20854;&#25903;&#25345;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#34164;&#21547;&#20851;&#31995;&#12290;&#35813;&#25968;&#25454;&#38598;&#20197;&#25903;&#25345;&#12289;&#26080;&#35777;&#25454;&#21644;&#39539;&#26021;&#19977;&#20010;&#31867;&#21035;&#20026;&#20027;&#65292;&#21253;&#21547;50,000&#20010;&#26032;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;BERT&#21644;Vision Transformer&#30340;&#26368;&#26032;&#20107;&#23454;&#26680;&#26597;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03897</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#21644;&#35773;&#21050;&#26032;&#38395;&#25968;&#25454;&#38598;Factify 2
&lt;/p&gt;
&lt;p&gt;
Factify 2: A Multimodal Fake News and Satire News Dataset. (arXiv:2304.03897v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;Factify 2&#65292;&#20854;&#25903;&#25345;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#34164;&#21547;&#20851;&#31995;&#12290;&#35813;&#25968;&#25454;&#38598;&#20197;&#25903;&#25345;&#12289;&#26080;&#35777;&#25454;&#21644;&#39539;&#26021;&#19977;&#20010;&#31867;&#21035;&#20026;&#20027;&#65292;&#21253;&#21547;50,000&#20010;&#26032;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;BERT&#21644;Vision Transformer&#30340;&#26368;&#26032;&#20107;&#23454;&#26680;&#26597;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#20026;&#20840;&#29699;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#24179;&#21488;&#65292;&#35753;&#20154;&#20204;&#34920;&#36798;&#33258;&#24049;&#30340;&#35266;&#28857;&#24182;&#20998;&#20139;&#33258;&#24049;&#30340;&#25925;&#20107;&#12290;&#34429;&#28982;&#36825;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20294;&#23427;&#20063;&#20351;&#24471;&#34394;&#20551;&#26032;&#38395;&#25104;&#20026;&#25105;&#20204;&#31038;&#20250;&#26368;&#32039;&#36843;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#25163;&#21160;&#30340;&#20107;&#23454;&#26680;&#23545;&#36807;&#31243;&#38750;&#24120;&#32791;&#26102;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#24456;&#38590;&#22312;&#35823;&#23548;&#24615;&#35328;&#35770;&#36896;&#25104;&#37325;&#22823;&#20260;&#23475;&#20043;&#21069;&#39539;&#26021;&#23427;&#20204;&#12290;&#36825;&#23601;&#26159;&#33258;&#21160;&#20107;&#23454;&#25110;&#22768;&#26126;&#39564;&#35777;&#21463;&#21040;&#20851;&#27880;&#30340;&#21407;&#22240;&#12290;&#19968;&#20123;&#29616;&#26377;&#25968;&#25454;&#38598;&#26088;&#22312;&#25903;&#25345;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#37117;&#26159;&#22522;&#20110;&#25991;&#26412;&#30340;&#12290;&#22810;&#27169;&#24577;&#20107;&#23454;&#39564;&#35777;&#19968;&#30452;&#21463;&#21040;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;FACTIFY 2&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#26469;&#28304;&#21644;&#28155;&#21152;&#35773;&#21050;&#25991;&#31456;&#26469;&#25913;&#36827;Factify 1&#12290;Factify 2&#26377;50,000&#20010;&#26032;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;&#19982;FACTIFY 1.0&#31867;&#20284;&#65292;&#25105;&#20204;&#26377;&#19977;&#20010;&#24191;&#27867;&#30340;&#31867;&#21035;&#8212;&#8212;&#25903;&#25345;&#12289;&#26080;&#35777;&#25454;&#21644;&#39539;&#26021;&#65292;&#36825;&#20123;&#31867;&#21035;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#34164;&#21547;&#20851;&#31995;&#20855;&#26377;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;BERT&#21644;Vision Transformer&#30340;FACTIFY 2&#20107;&#23454;&#26680;&#26597;&#27169;&#22411;&#65292;&#24182;&#34920;&#26126;&#20854;&#22312;Factify 1&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The internet gives the world an open platform to express their views and share their stories. While this is very valuable, it makes fake news one of our society's most pressing problems. Manual fact checking process is time consuming, which makes it challenging to disprove misleading assertions before they cause significant harm. This is he driving interest in automatic fact or claim verification. Some of the existing datasets aim to support development of automating fact-checking techniques, however, most of them are text based. Multi-modal fact verification has received relatively scant attention. In this paper, we provide a multi-modal fact-checking dataset called FACTIFY 2, improving Factify 1 by using new data sources and adding satire articles. Factify 2 has 50,000 new data instances. Similar to FACTIFY 1.0, we have three broad categories - support, no-evidence, and refute, with sub-categories based on the entailment of visual and textual data. We also provide a BERT and Vison Tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.03843</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35201;&#36880;&#27493;&#24605;&#32771;&#65311;&#25512;&#29702;&#28304;&#20110;&#32463;&#39564;&#30340;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#30528;&#24378;&#22823;&#32780;&#31070;&#31192;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#32431;&#31929;&#30340;&#24605;&#32500;&#27493;&#39588;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#29702;&#20986;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#24471;&#20986;&#30340;&#25512;&#35770; - &#23613;&#31649;&#25105;&#20204;&#20174;&#19990;&#30028;&#19978;&#27809;&#26377;&#24471;&#21040;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#12290;&#21516;&#26679;&#22320;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#29983;&#25104;&#20013;&#38388;&#27493;&#39588;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36825;&#20123;&#35757;&#32451;&#26465;&#20214;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#23450;&#20041;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#21697;&#23545;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#27599;&#20010;&#26679;&#21697;&#21482;&#21253;&#25324;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#21464;&#37327;&#12290;&#25105;&#20204;&#27604;&#36739;&#20351;&#29992;&#25512;&#29702;&#29983;&#25104;&#30340;&#21464;&#37327;&#23376;&#38598;&#19982;&#20351;&#29992;&#23436;&#25972;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#27431;&#27954; Twitter &#32593;&#32476;&#65292;&#36890;&#36807;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#65292;&#37327;&#21270;&#22810;&#35821;&#29992;&#25143;&#22312;&#36328;&#35821;&#35328;&#20449;&#24687;&#20132;&#27969;&#20013;&#30340;&#32467;&#26500;&#20316;&#29992;&#21644;&#27807;&#36890;&#24433;&#21709;&#12290;&#22810;&#35821;&#32773;&#30340;&#20171;&#25968;&#20013;&#24515;&#24230;&#27604;&#21333;&#35821;&#29992;&#25143;&#39640;13&#65285;&#65292;&#19988;&#20855;&#26377;&#22810;&#35821;&#32593;&#32476;&#37051;&#23621;&#30340;&#21333;&#35821;&#29992;&#25143;&#20998;&#20139;&#20854;&#20182;&#35821;&#35328;&#22495;&#21517;&#21644;hashtag&#30340;&#21487;&#33021;&#24615;&#20998;&#21035;&#22686;&#21152;&#20102;16&#20493;&#21644;4&#20493;&#12290;&#22810;&#35821;&#32773;&#23545;&#20110;&#20256;&#25773;&#37027;&#20123;&#21333;&#35821;&#21516;&#32990;&#19981;&#26131;&#33719;&#24471;&#30340;&#20449;&#24687;&#20855;&#26377;&#26356;&#22823;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.03797</link><description>&lt;p&gt;
&#36328;&#22269;&#20043;&#38388;&#30340;&#26725;&#26753;: &#37327;&#21270;&#22810;&#35821;&#32773;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20132;&#27969;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bridging Nations: Quantifying the Role of Multilinguals in Communication on Social Media. (arXiv:2304.03797v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03797
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#27431;&#27954; Twitter &#32593;&#32476;&#65292;&#36890;&#36807;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#65292;&#37327;&#21270;&#22810;&#35821;&#29992;&#25143;&#22312;&#36328;&#35821;&#35328;&#20449;&#24687;&#20132;&#27969;&#20013;&#30340;&#32467;&#26500;&#20316;&#29992;&#21644;&#27807;&#36890;&#24433;&#21709;&#12290;&#22810;&#35821;&#32773;&#30340;&#20171;&#25968;&#20013;&#24515;&#24230;&#27604;&#21333;&#35821;&#29992;&#25143;&#39640;13&#65285;&#65292;&#19988;&#20855;&#26377;&#22810;&#35821;&#32593;&#32476;&#37051;&#23621;&#30340;&#21333;&#35821;&#29992;&#25143;&#20998;&#20139;&#20854;&#20182;&#35821;&#35328;&#22495;&#21517;&#21644;hashtag&#30340;&#21487;&#33021;&#24615;&#20998;&#21035;&#22686;&#21152;&#20102;16&#20493;&#21644;4&#20493;&#12290;&#22810;&#35821;&#32773;&#23545;&#20110;&#20256;&#25773;&#37027;&#20123;&#21333;&#35821;&#21516;&#32990;&#19981;&#26131;&#33719;&#24471;&#30340;&#20449;&#24687;&#20855;&#26377;&#26356;&#22823;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#33021;&#24555;&#36895;&#20256;&#25773;&#21508;&#31181;&#20449;&#24687;&#65292;&#20174;&#34920;&#24773;&#21253;&#21040;&#31038;&#20250;&#36816;&#21160;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20449;&#24687;&#22914;&#20309;&#36328;&#36234;&#35821;&#35328;&#36793;&#30028;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#22312;&#27431;&#27954; Twitter &#32593;&#32476;&#19978;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#65292;&#37327;&#21270;&#22810;&#35821;&#29992;&#25143;&#22312;&#36328;&#35821;&#35328;&#20449;&#24687;&#20132;&#27969;&#20013;&#30340;&#32467;&#26500;&#20316;&#29992;&#21644;&#27807;&#36890;&#24433;&#21709;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#22810;&#35821;&#32773;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65307;&#21457;&#24067;&#22810;&#31181;&#35821;&#35328;&#30340;&#25512;&#25991;&#23558;&#20171;&#25968;&#20013;&#24515;&#24230;&#25552;&#39640;&#20102;13&#65285;&#65292;&#24182;&#19988;&#22914;&#26524;&#20854;&#32593;&#32476;&#37051;&#23621;&#26159;&#22810;&#35821;&#32773;&#65292;&#21017;&#21333;&#35821;&#29992;&#25143;&#20998;&#20139;&#21478;&#19968;&#31181;&#35821;&#35328;&#22495;&#21517;&#21644;hashtag&#30340;&#21487;&#33021;&#24615;&#20998;&#21035;&#22686;&#21152;&#20102;16&#20493;&#21644;4&#20493;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#22810;&#35821;&#32773;&#23545;&#20110;&#20256;&#25773;&#37027;&#20123;&#21333;&#35821;&#21516;&#32990;&#19981;&#26131;&#33719;&#24471;&#30340;&#20449;&#24687;&#20855;&#26377;&#26356;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#20363;&#22914;&#26469;&#33258;&#36965;&#36828;&#22269;&#23478;&#30340;&#20449;&#24687;&#12289;&#20851;&#20110;&#21306;&#22495;&#25919;&#27835;&#12289;&#26032;&#20852;&#31038;&#20250;&#36816;&#21160;&#21644;&#24037;&#20316;&#26426;&#20250;&#30340;&#20869;&#23481;&#12290;&#36890;&#36807;&#31361;&#20986;&#36328;&#22659;&#20449;&#24687;&#20132;&#27969;&#65292;&#36825;&#39033;&#24037;&#20316;&#25581;&#31034;&#20102;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media enables the rapid spread of many kinds of information, from memes to social movements. However, little is known about how information crosses linguistic boundaries. We apply causal inference techniques on the European Twitter network to quantify multilingual users' structural role and communication influence in cross-lingual information exchange. Overall, multilinguals play an essential role; posting in multiple languages increases betweenness centrality by 13%, and having a multilingual network neighbor increases monolinguals' odds of sharing domains and hashtags from another language 16-fold and 4-fold, respectively. We further show that multilinguals have a greater impact on diffusing information less accessible to their monolingual compatriots, such as information from far-away countries and content about regional politics, nascent social movements, and job opportunities. By highlighting information exchange across borders, this work sheds light on a crucial component 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HiCatGLR&#20219;&#21153;&#65292;&#33268;&#21147;&#20110;&#20026;&#25991;&#29486;&#32508;&#36848;&#29983;&#25104;&#20998;&#23618;&#30446;&#24405;&#65292;&#23427;&#21487;&#20197;&#20174;&#22810;&#31687;&#35770;&#25991;&#20013;&#25552;&#21462;&#21644;&#32452;&#32455;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#32570;&#23569;&#28165;&#26224;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#27010;&#36848;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#21019;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27492;&#39033;&#30740;&#31350;&#65292;&#21487;&#20197;&#26356;&#21152;&#20934;&#30830;&#22320;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#24182;&#39564;&#35777;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03512</link><description>&lt;p&gt;
&#25991;&#29486;&#32508;&#36848;&#30340;&#20998;&#23618;&#30446;&#24405;&#29983;&#25104;&#65306;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Catalogue Generation for Literature Review: A Benchmark. (arXiv:2304.03512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03512
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HiCatGLR&#20219;&#21153;&#65292;&#33268;&#21147;&#20110;&#20026;&#25991;&#29486;&#32508;&#36848;&#29983;&#25104;&#20998;&#23618;&#30446;&#24405;&#65292;&#23427;&#21487;&#20197;&#20174;&#22810;&#31687;&#35770;&#25991;&#20013;&#25552;&#21462;&#21644;&#32452;&#32455;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#20013;&#32570;&#23569;&#28165;&#26224;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#27010;&#36848;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#21019;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27492;&#39033;&#30740;&#31350;&#65292;&#21487;&#20197;&#26356;&#21152;&#20934;&#30830;&#22320;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#24182;&#39564;&#35777;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25991;&#26723;&#31185;&#23398;&#25688;&#35201;&#21487;&#20197;&#20174;&#22823;&#37327;&#30340;&#35770;&#25991;&#20013;&#25552;&#21462;&#21644;&#32452;&#32455;&#37325;&#35201;&#20449;&#24687;&#65292;&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20135;&#29983;&#32570;&#20047;&#28165;&#26224;&#21644;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#30340;&#20887;&#38271;&#27010;&#36848;&#19978;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Hierarchical Catalogue Generation for Literature Review (HiCatGLR)&#8221;&#30340;&#21407;&#23376;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#26681;&#25454;&#21508;&#31181;&#21442;&#32771;&#25991;&#29486;&#20026;&#32508;&#36848;&#35770;&#25991;&#29983;&#25104;&#20998;&#23618;&#30446;&#24405;&#12290;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#33521;&#25991;&#25991;&#29486;&#32508;&#36848;&#20998;&#23618;&#30446;&#24405;&#25968;&#25454;&#38598;(HiCaD)&#65292;&#20854;&#20013;&#21253;&#21547;13.8k&#31687;&#25991;&#29486;&#32508;&#36848;&#30446;&#24405;&#21644;120k&#31687;&#21442;&#32771;&#35770;&#25991;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#21644;&#27969;&#27700;&#32447;&#26041;&#27861;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20174;&#35821;&#20041;&#21644;&#32467;&#26500;&#19978;&#19982;&#21442;&#32771;&#26631;&#20934;&#30456;&#20284;&#24230;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24191;&#27867;&#20998;&#26512;&#39564;&#35777;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#21644;&#25105;&#20204;&#35780;&#20272;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-document scientific summarization can extract and organize important information from an abundant collection of papers, arousing widespread attention recently. However, existing efforts focus on producing lengthy overviews lacking a clear and logical hierarchy. To alleviate this problem, we present an atomic and challenging task named Hierarchical Catalogue Generation for Literature Review (HiCatGLR), which aims to generate a hierarchical catalogue for a review paper given various references. We carefully construct a novel English Hierarchical Catalogues of Literature Reviews Dataset (HiCaD) with 13.8k literature review catalogues and 120k reference papers, where we benchmark diverse experiments via the end-to-end and pipeline methods. To accurately assess the model performance, we design evaluation metrics for similarity to ground truth from semantics and structure. Besides, our extensive analyses verify the high quality of our dataset and the effectiveness of our evaluation met
&lt;/p&gt;</description></item><item><title>CAPOT&#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#22122;&#22768;&#26597;&#35810;&#30340;&#20581;&#22766;&#24615;&#65292;&#34920;&#29616;&#31867;&#20284;&#20110;&#25968;&#25454;&#22686;&#24378;&#20294;&#27809;&#26377;&#20854;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2304.03401</link><description>&lt;p&gt;
CAPOT: &#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#21019;&#24314;&#24378;&#20581;&#30340;&#23494;&#38598;&#26597;&#35810;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CAPOT: Creating Robust Dense Query Encoders using Post Training Contrastive Alignment. (arXiv:2304.03401v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03401
&lt;/p&gt;
&lt;p&gt;
CAPOT&#20351;&#29992;&#21518;&#35757;&#32451;&#23545;&#27604;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#22122;&#22768;&#26597;&#35810;&#30340;&#20581;&#22766;&#24615;&#65292;&#34920;&#29616;&#31867;&#20284;&#20110;&#25968;&#25454;&#22686;&#24378;&#20294;&#27809;&#26377;&#20854;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#30340;&#25104;&#21151;&#21644;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#30340;&#36827;&#27493;&#20351;&#24471;&#22522;&#20110;&#23494;&#38598;&#21521;&#37327;&#30340;&#26816;&#32034;&#25104;&#20026;&#27573;&#33853;&#21644;&#25991;&#26723;&#25490;&#21517;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#21452;&#32534;&#30721;&#22120;&#34429;&#28982;&#26377;&#25928;&#21644;&#39640;&#25928;&#65292;&#20294;&#23545;&#26597;&#35810;&#20998;&#24067;&#21644;&#22024;&#26434;&#26597;&#35810;&#21464;&#21270;&#24456;&#33030;&#24369;&#12290;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#21152;&#20581;&#22766;&#65292;&#20294;&#20250;&#24341;&#20837;&#35757;&#32451;&#38598;&#29983;&#25104;&#30340;&#24320;&#38144;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#21644;&#32034;&#24341;&#37325;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Contrastive Alignment POst Training (CAPOT)&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#25991;&#26723;&#32534;&#30721;&#22120;&#65292;&#35753;&#26597;&#35810;&#32534;&#30721;&#22120;&#23398;&#20064;&#23558;&#22024;&#26434;&#26597;&#35810;&#19982;&#20854;&#26410;&#26356;&#25913;&#30340;&#26681;&#23545;&#40784;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102; CAPOT &#22312; MSMARCO&#12289;&#33258;&#28982;&#38382;&#39064;&#21644; Trivia QA &#27573;&#33853;&#26816;&#32034;&#30340;&#22024;&#26434;&#21464;&#20307;&#19978;&#65292;&#21457;&#29616; CAPOT &#20855;&#26377;&#19982;&#25968;&#25454;&#22686;&#24378;&#31867;&#20284;&#30340;&#24433;&#21709;&#65292;&#20294;&#27809;&#26377;&#23427;&#30340;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of contextual word representations and advances in neural information retrieval have made dense vector-based retrieval a standard approach for passage and document ranking. While effective and efficient, dual-encoders are brittle to variations in query distributions and noisy queries. Data augmentation can make models more robust but introduces overhead to training set generation and requires retraining and index regeneration. We present Contrastive Alignment POst Training (CAPOT), a highly efficient finetuning method that improves model robustness without requiring index regeneration, the training set optimization, or alteration. CAPOT enables robust retrieval by freezing the document encoder while the query encoder learns to align noisy queries with their unaltered root. We evaluate CAPOT noisy variants of MSMARCO, Natural Questions, and Trivia QA passage retrieval, finding CAPOT has a similar impact as data augmentation with none of its overhead.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38053;&#21273;&#65306;&#29992;GPT&#35299;&#23494;&#26448;&#26009;&#31185;&#23398;&#30340;&#31192;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#20449;&#24687;&#25552;&#21462;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#38041;&#38043;&#30719;&#22826;&#38451;&#33021;&#30005;&#27744;FAIR&#25968;&#25454;&#38598;&#23545;GPT-3&#36827;&#34892;&#24494;&#35843;&#65292;&#33719;&#24471;&#20102;91.8 F1&#24471;&#20998;&#65292;&#24182;&#26356;&#26032;&#20102;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36804;&#20170;&#20026;&#27490;&#25152;&#26377;&#30456;&#20851;&#31185;&#23398;&#35770;&#25991;&#12290;&#25152;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#24050;&#34987;&#26684;&#24335;&#21270;&#21644;&#26631;&#20934;&#21270;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#30452;&#25509;&#20316;&#20026;&#21518;&#32493;&#25968;&#25454;&#20998;&#26512;&#30340;&#36755;&#20837;&#12290;&#36825;&#20010;&#29305;&#24615;&#23558;&#20351;&#26448;&#26009;&#31185;&#23398;&#23478;&#36890;&#36807;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#35780;&#35770;&#25991;&#31456;&#26469;&#24320;&#21457;&#20854;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#24182;&#33719;&#24471;&#20102;&#19982;DFT&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20687;&#26448;&#26009;&#23398;&#23478;&#19968;&#26679;&#35780;&#21028;&#26448;&#26009;&#21644;&#35774;&#35745;&#26032;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;GPT-4&#65292;&#21253;&#25324;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21069;&#26223;&#24212;&#29992;&#65292;&#24182;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#21019;&#26032;&#12290;ChatGPT/GPT-4&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#39046;&#22495;&#20063;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01852</link><description>&lt;p&gt;
ChatGPT/GPT-4&#30740;&#31350;&#32508;&#36848;&#21450;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#26410;&#26469;&#30340;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;GPT-4&#65292;&#21253;&#25324;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21069;&#26223;&#24212;&#29992;&#65292;&#24182;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#21019;&#26032;&#12290;ChatGPT/GPT-4&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#39046;&#22495;&#20063;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26469;&#33258;GPT&#31995;&#21015;&#30340;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;ChatGPT&#21644;GPT-4&#21450;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21069;&#26223;&#24212;&#29992;&#12290;&#23454;&#38469;&#19978;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#25552;&#39640;LLMs&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#21019;&#26032;&#12290;&#25105;&#20204;&#22312;arXiv&#19978;&#28145;&#20837;&#20998;&#26512;&#20102;194&#31687;&#30456;&#20851;&#25991;&#29486;&#65292;&#21253;&#25324;&#36235;&#21183;&#20998;&#26512;&#12289;&#35789;&#20113;&#34920;&#29616;&#21644;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#20998;&#24067;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;ChatGPT/GPT-4&#30740;&#31350;&#26174;&#33879;&#22686;&#38271;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#30452;&#25509;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#19978;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#22312;&#20174;&#25945;&#32946;&#21644;&#21382;&#21490;&#21040;&#25968;&#23398;&#12289;&#21307;&#23398;&#21644;&#29289;&#29702;&#31561;&#39046;&#22495;&#20855;&#26377;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;ChatGPT&#30340;&#33021;&#21147;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabi
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#21644;&#20256;&#25773;&#32467;&#26500;&#65292;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#36328;&#36234;&#35821;&#35328;&#21644;&#39046;&#22495;&#30028;&#38480;&#30340;&#35875;&#35328;&#12290;</title><link>http://arxiv.org/abs/2304.01492</link><description>&lt;p&gt;
&#32479;&#19968;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#19982;&#20256;&#25773;&#32467;&#26500;&#29992;&#20110;&#25552;&#39640;&#20302;&#36164;&#28304;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#21644;&#20256;&#25773;&#32467;&#26500;&#65292;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#36328;&#36234;&#35821;&#35328;&#21644;&#39046;&#22495;&#30028;&#38480;&#30340;&#35875;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#35875;&#35328;&#20276;&#38543;&#30528;&#31361;&#21457;&#26032;&#38395;&#25110;&#28909;&#38376;&#35805;&#39064;&#32780;&#20256;&#25773;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#30495;&#30456;&#30340;&#20256;&#25773;&#12290;&#29616;&#26377;&#30340;&#35875;&#35328;&#26816;&#27979;&#31639;&#27861;&#23637;&#31034;&#20102;&#22312;&#21069;&#20960;&#22825;&#26032;&#38395;&#19978;&#33391;&#22909;&#24615;&#33021;&#30340;&#21069;&#26223;&#65292;&#20294;&#26159;&#30001;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#20808;&#21069;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23427;&#20204;&#24456;&#38590;&#21457;&#29616;&#19982;&#39044;&#26399;&#20107;&#20214;&#26377;&#20851;&#30340;&#35875;&#35328;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#35821;&#35328;&#65288;&#21363;&#20302;&#36164;&#28304;&#29615;&#22659;&#65289;&#20013;&#20256;&#25773;&#30340;&#35875;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#29305;&#24449;&#26469;&#26816;&#27979;&#35875;&#35328;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#25773;&#30340;&#35875;&#35328;&#34920;&#31034;&#20026;&#26080;&#21521;&#25299;&#25169;&#32467;&#26500;&#65292;&#28982;&#21518;&#36890;&#36807;&#32479;&#19968;&#23545;&#27604;&#33539;&#24335;&#36827;&#34892;Multi-scale&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#30830;&#22320;&#31361;&#30772;&#20102;&#39046;&#22495;&#21644;/&#25110;&#35821;&#35328;&#38382;&#39064;&#30340;&#38556;&#30861;&#65292;&#36890;&#36807;&#35821;&#35328;&#23545;&#40784;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The truth is significantly hampered by massive rumors that spread along with breaking news or popular topics. Since there is sufficient corpus gathered from the same domain for model training, existing rumor detection algorithms show promising performance on yesterday's news. However, due to a lack of training data and prior expert knowledge, they are poor at spotting rumors concerning unforeseen events, especially those propagated in different languages (i.e., low-resource regimes). In this paper, we propose a unified contrastive transfer framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced. More specifically, we first represent rumor circulated on social media as an undirected topology, and then train a Multi-scale Graph Convolutional Network via a unified contrastive paradigm. Our model explicitly breaks the barriers of the domain and/or language issues, via language alignment and a novel domain-adaptive contrastive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#24178;&#20928;&#25110;&#25915;&#20987;&#24773;&#22659;&#19979;&#22343;&#26377;&#25928;&#65292;&#21516;&#26102;&#20445;&#25252;&#20102;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2304.01005</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#24178;&#20928;&#21644;&#25915;&#20987;&#24773;&#26223;&#19979;&#30340;&#22810;&#35821;&#35328;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Based Multilingual Emoji Prediction In Clean and Attack Scenarios. (arXiv:2304.01005v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#24178;&#20928;&#25110;&#25915;&#20987;&#24773;&#22659;&#19979;&#22343;&#26377;&#25928;&#65292;&#21516;&#26102;&#20445;&#25252;&#20102;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#19968;&#20010;&#26085;&#30410;&#22686;&#38271;&#30340;&#39046;&#22495;&#65292;&#30001;&#20110;&#20854;&#20998;&#25955;&#21644;&#31169;&#23494;&#30340;&#35774;&#35745;&#32780;&#24471;&#21040;&#21457;&#23637;&#12290;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#35757;&#32451;&#20998;&#24067;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#22823;&#37327;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#21516;&#26102;&#20445;&#25252;&#20102;&#27599;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#12290;&#28982;&#21518;&#65292;&#26381;&#21153;&#22120;&#32858;&#21512;&#20102;&#22312;&#36825;&#20123;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#30340;&#35757;&#32451;&#65292;&#32780;&#19981;&#35775;&#38382;&#23427;&#20204;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#26159;&#22312;&#20219;&#20309;&#31038;&#20132;&#23186;&#20307;&#26381;&#21153;&#21644;&#21363;&#26102;&#36890;&#35759;&#24179;&#21488;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#34920;&#24773;&#31526;&#21495;&#65292;&#20197;&#34920;&#36798;&#29992;&#25143;&#30340;&#24773;&#24863;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#24178;&#20928;&#21644;&#25915;&#20987;&#24773;&#22659;&#19979;&#30340;&#22810;&#35821;&#35328;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;&#12290;&#34920;&#24773;&#31526;&#21495;&#39044;&#27979;&#25968;&#25454;&#20174;Twitter&#21644;SemEval&#34920;&#24773;&#31526;&#21495;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#12290;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#19981;&#21516;&#22823;&#23567;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#21253;&#25324;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#20013;&#20551;&#23450;&#25968;&#25454;&#24178;&#20928;&#25110;&#22312;&#19968;&#20123;&#23458;&#25143;&#31471;&#20013;&#36827;&#34892;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#30340;&#31232;&#30095;&#28608;&#27963;&#36716;&#25442;&#22120;&#12290;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24178;&#20928;&#25110;&#25915;&#20987;&#24773;&#22659;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;&#22810;&#35821;&#35328;&#34920;&#24773;&#31526;&#21495;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a growing field in the machine learning community due to its decentralized and private design. Model training in federated learning is distributed over multiple clients giving access to lots of client data while maintaining privacy. Then, a server aggregates the training done on these multiple clients without access to their data, which could be emojis widely used in any social media service and instant messaging platforms to express users' sentiments. This paper proposes federated learning-based multilingual emoji prediction in both clean and attack scenarios. Emoji prediction data have been crawled from both Twitter and SemEval emoji datasets. This data is used to train and evaluate different transformer model sizes including a sparsely activated transformer with either the assumption of clean data in all clients or poisoned data via label flipping attack in some clients. Experimental results on these models show that federated learning in either clean or attack
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26080;&#21442;&#32771;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;ChatGPT&#29983;&#25104;&#30340;&#26174;&#24335;&#24471;&#20998;&#26159;&#26368;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.00723</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26080;&#21442;&#32771;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#65306;&#21021;&#27493;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study. (arXiv:2304.00723v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26080;&#21442;&#32771;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;ChatGPT&#29983;&#25104;&#30340;&#26174;&#24335;&#24471;&#20998;&#26159;&#26368;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#25991;&#26412;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#32780;&#20135;&#29983;&#22256;&#38590;&#12290;&#26368;&#36817;&#65292;OpenAI&#30340;ChatGPT&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21457;&#24067;&#27492;&#25253;&#21578;&#65292;&#20197;&#35843;&#26597;LLMs&#65292;&#29305;&#21035;&#26159;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#32034;&#20248;&#21270;&#23427;&#20204;&#22312;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#26041;&#38754;&#30340;&#24212;&#29992;&#26041;&#24335;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;ChatGPT&#25110;&#31867;&#20284;LLMs&#30340;&#19977;&#31181;&#26080;&#21442;&#32771;&#35780;&#20272;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;ChatGPT&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#21508;&#20010;&#35282;&#24230;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#32780;&#19981;&#38656;&#35201;&#21442;&#32771;&#65292;&#24182;&#23637;&#31034;&#20102;&#27604;&#22823;&#22810;&#25968;&#29616;&#26377;&#33258;&#21160;&#25351;&#26631;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#26174;&#24335;&#24471;&#20998;&#26159;&#21033;&#29992;ChatGPT&#29983;&#25104;&#34913;&#37327;&#25991;&#26412;&#36136;&#37327;&#30340;&#25968;&#23383;&#20998;&#25968;&#30340;&#26368;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;LLMs&#24212;&#29992;&#20110;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the quality of generated text is a challenging task in natural language processing. This difficulty arises from the inherent complexity and diversity of text. Recently, OpenAI's ChatGPT, a powerful large language model (LLM), has garnered significant attention due to its impressive performance in various tasks. Therefore, we present this report to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality. We compared three kinds of reference-free evaluation methods based on ChatGPT or similar LLMs. The experimental results prove that ChatGPT is capable to evaluate text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics. In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches. However, directly
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00008</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Creativity of Large Language Models. (arXiv:2304.00008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00008
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#30340;&#38590;&#28857;&#21644;&#26131;&#28857;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#39072;&#35206;&#20154;&#24037;&#26234;&#33021;&#30340;&#22810;&#20010;&#39046;&#22495;&#12290;&#20854;&#20013;&#26368;&#26174;&#33879;&#30340;&#24212;&#29992;&#20043;&#19968;&#26159;&#21019;&#20316;&#65292;&#20363;&#22914;&#35799;&#27468;&#25110;&#25925;&#20107;&#65306;&#29983;&#25104;&#30340;&#36755;&#20986;&#36890;&#24120;&#20855;&#26377;&#24778;&#20154;&#30340;&#36136;&#37327;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;LLMs&#30495;&#30340;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21019;&#36896;&#24615;&#30340;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21019;&#36896;&#24615;&#29702;&#35770;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLMs&#30340;&#21457;&#23637;&#65292;&#25506;&#35752;&#20102;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19982;LLMs&#30456;&#20851;&#30340;&#26426;&#22120;&#21019;&#36896;&#24615;&#26041;&#38754;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#26131;&#8221;&#21644;&#8220;&#38590;&#8221;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#21019;&#24847;&#20135;&#19994;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arise: can LLMs really be considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. Then, we identify a set of "easy" and "hard" problems in machine creativity, discussing them in relation to LLMs. Finally, we analyze the societal impact of these technologies with a particular focus on the creative industries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.16755</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#35268;&#27169;&#21270;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#29983;&#25104;&#19981;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#23475;&#30340;&#25991;&#26412;&#25110;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31616;&#21333;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#65288;&#21363;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#20043;&#38388;&#30340;&#27604;&#36739;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#27604;&#36739;&#21453;&#39304;&#21482;&#33021;&#20256;&#36798;&#26377;&#38480;&#30340;&#20851;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65288;ILF&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#12290;ILF&#30001;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#32452;&#25104;&#65306;&#31532;&#19968;&#27493;&#65292;&#26681;&#25454;&#36755;&#20837;&#65292;&#21021;&#22987;LM&#36755;&#20986;&#21644;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#33410;&#20197;&#29983;&#25104;&#25913;&#36827;&#12290;&#31532;&#20108;&#27493;&#65292;&#36873;&#25321;&#26368;&#22810;&#21453;&#39304;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#27493;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#30340;&#25913;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ILF&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#31867;&#20284;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;ILF&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#29983;&#25104;&#23450;&#21521;&#21050;&#28608;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#29983;&#25104;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#36890;&#36807;&#31574;&#30053;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#65292;&#24182;&#22312;&#25688;&#35201;&#21644;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.11520</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#21050;&#28608;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models via Directional Stimulus Prompting. (arXiv:2302.11520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#29983;&#25104;&#23450;&#21521;&#21050;&#28608;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#29983;&#25104;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#36890;&#36807;&#31574;&#30053;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#20110;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#65292;&#24182;&#22312;&#25688;&#35201;&#21644;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#23450;&#21521;&#21050;&#28608;&#24341;&#23548;&#65292;&#23427;&#20351;&#29992;&#21487;&#35843;&#33410;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#40657;&#30418;&#20923;&#32467;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25351;&#23548;&#12290;&#19982;&#20197;&#24448;&#25163;&#21160;&#25110;&#33258;&#21160;&#25214;&#21040;&#27599;&#20010;&#20219;&#21153;&#30340;&#26368;&#20248;&#25552;&#31034;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#31574;&#30053;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#31163;&#25955;&#30340;token&#20316;&#20026;&#27599;&#20010;&#36755;&#20837;&#30340;&#23450;&#21521;&#21050;&#28608;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#31034;&#25110;&#25552;&#31034;&#65292;&#20363;&#22914;&#25991;&#31456;&#30340;&#20851;&#38190;&#35789;&#29992;&#20110;&#25688;&#35201;&#12290;&#28982;&#21518;&#23558;&#23450;&#21521;&#21050;&#28608;&#19982;&#21407;&#22987;&#36755;&#20837;&#32452;&#21512;&#65292;&#24182;&#36755;&#20837;&#21040;LLM&#20013;&#65292;&#20197;&#25351;&#23548;&#20854;&#21521;&#25152;&#38656;&#30446;&#26631;&#29983;&#25104;&#12290;&#31574;&#30053;LM&#21487;&#20197;&#36890;&#36807;1&#65289;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#30417;&#30563;&#23398;&#20064;&#21644;2&#65289;&#20174;&#31163;&#32447;&#21644;&#22312;&#32447;&#22870;&#21169;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25506;&#32034;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#23450;&#21521;&#21050;&#28608;&#12290;&#35813;&#26694;&#26550;&#21487;&#28789;&#27963;&#36866;&#29992;&#20110;&#21508;&#31181;LM&#21644;&#20219;&#21153;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#25928;&#26524;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#25688;&#35201;&#21644;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new framework, Directional Stimulus Prompting, that uses a tuneable language model (LM) to provide guidance for the black-box frozen large language model (LLM) on downstream tasks. Unlike prior work that manually or automatically finds the optimal prompt for each task, we train a policy LM to generate discrete tokens as directional stimulus of each input, which is a hint/cue such as keywords of an article for summarization. The directional stimulus is then combined with the original input and fed into the LLM to guide its generation toward the desired target. The policy LM can be trained through 1) supervised learning from annotated data and 2) reinforcement learning from offline and online rewards to explore directional stimulus that better aligns LLMs with human preferences. This framework is flexibly applicable to various LMs and tasks. To verify its effectiveness, we apply our framework to summarization and dialogue response generation tasks. Experimental results dem
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25490;&#21517;&#22686;&#24378;&#30340;&#32534;&#30721;&#21644;&#39592;&#26550;&#24863;&#30693;&#35299;&#30721;&#26694;&#26550;&#65292;&#20197;&#35299;&#32806;&#25991;&#26412;&#21040;SQL&#20013;&#30340;&#26550;&#26500;&#36830;&#25509;&#21644;&#39592;&#26550;&#35299;&#26512;&#65292;&#20174;&#32780;&#20943;&#36731;&#23545;&#26550;&#26500;&#36830;&#25509;&#30340;&#24037;&#20316;&#37327;&#24182;&#38544;&#24335;&#38480;&#21046;SQL&#35299;&#26512;&#12290;</title><link>http://arxiv.org/abs/2302.05965</link><description>&lt;p&gt;
RESDSQL&#65306;&#35299;&#32806;&#25991;&#26412;&#21040;SQL&#20013;&#30340;&#26550;&#26500;&#36830;&#25509;&#21644;&#39592;&#26550;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL. (arXiv:2302.05965v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25490;&#21517;&#22686;&#24378;&#30340;&#32534;&#30721;&#21644;&#39592;&#26550;&#24863;&#30693;&#35299;&#30721;&#26694;&#26550;&#65292;&#20197;&#35299;&#32806;&#25991;&#26412;&#21040;SQL&#20013;&#30340;&#26550;&#26500;&#36830;&#25509;&#21644;&#39592;&#26550;&#35299;&#26512;&#65292;&#20174;&#32780;&#20943;&#36731;&#23545;&#26550;&#26500;&#36830;&#25509;&#30340;&#24037;&#20316;&#37327;&#24182;&#38544;&#24335;&#38480;&#21046;SQL&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#25991;&#26412;&#21040;SQL&#20013;&#30340;&#26368;&#20339;&#23581;&#35797;&#20043;&#19968;&#12290;&#30001;&#20110;SQL&#26597;&#35810;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;seq2seq&#27169;&#22411;&#36127;&#36131;&#35299;&#26512;&#26550;&#26500;&#39033;&#65288;&#21363;&#34920;&#26684;&#21644;&#21015;&#65289;&#21644;&#39592;&#26550;&#65288;&#21363;SQL&#20851;&#38190;&#23383;&#65289;&#12290;&#36825;&#20123;&#32806;&#21512;&#30340;&#30446;&#26631;&#22686;&#21152;&#20102;&#27491;&#30830;&#35299;&#26512;SQL&#26597;&#35810;&#30340;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#28041;&#21450;&#35768;&#22810;&#26550;&#26500;&#39033;&#21644;&#36923;&#36753;&#25805;&#20316;&#31526;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25490;&#21517;&#22686;&#24378;&#30340;&#32534;&#30721;&#21644;&#39592;&#26550;&#24863;&#30693;&#35299;&#30721;&#26694;&#26550;&#65292;&#20197;&#35299;&#32806;&#26550;&#26500;&#36830;&#25509;&#21644;&#39592;&#26550;&#35299;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;seq2seq&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#20854;&#32534;&#30721;&#22120;&#27880;&#20837;&#26368;&#30456;&#20851;&#30340;&#26550;&#26500;&#39033;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#26080;&#24207;&#30340;&#26550;&#26500;&#39033;&#65292;&#21487;&#20197;&#20943;&#36731;SQL&#35299;&#26512;&#20013;&#30340;&#26550;&#26500;&#36830;&#25509;&#24037;&#20316;&#65292;&#32780;&#20854;&#35299;&#30721;&#22120;&#39318;&#20808;&#29983;&#25104;&#39592;&#26550;&#65292;&#28982;&#21518;&#29983;&#25104;&#23454;&#38469;&#30340;SQL&#26597;&#35810;&#65292;&#21487;&#20197;&#38544;&#24335;&#22320;&#38480;&#21046;SQL&#35299;&#26512;&#12290;&#25105;&#20204;&#22312;Spider&#21450;&#20854;&#19977;&#20010;&#40065;&#26834;&#24615;&#21464;&#20307;&#65306;Spider-DK, Spider-EMNLP19&#21644;Spider-WTq&#31561;SQL&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the recent best attempts at Text-to-SQL is the pre-trained language model. Due to the structural property of the SQL queries, the seq2seq model takes the responsibility of parsing both the schema items (i.e., tables and columns) and the skeleton (i.e., SQL keywords). Such coupled targets increase the difficulty of parsing the correct SQL queries especially when they involve many schema items and logic operators. This paper proposes a ranking-enhanced encoding and skeleton-aware decoding framework to decouple the schema linking and the skeleton parsing. Specifically, for a seq2seq encoder-decode model, its encoder is injected by the most relevant schema items instead of the whole unordered ones, which could alleviate the schema linking effort during SQL parsing, and its decoder first generates the skeleton and then the actual SQL query, which could implicitly constrain the SQL parsing. We evaluate our proposed framework on Spider and its three robustness variants: Spider-DK, Spid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#31435;&#22330;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20854;&#39044;&#27979;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.14548</link><description>&lt;p&gt;
ChatGPT&#21457;&#24067;&#21518;&#65292;&#31435;&#22330;&#26816;&#27979;&#25216;&#26415;&#20250;&#22914;&#20309;&#21457;&#23637;&#65311;
&lt;/p&gt;
&lt;p&gt;
How would Stance Detection Techniques Evolve after the Launch of ChatGPT?. (arXiv:2212.14548v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14548
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#31435;&#22330;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20854;&#39044;&#27979;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#26159;&#25351;&#20174;&#32473;&#23450;&#25991;&#26412;&#20013;&#25552;&#21462;&#23545;&#30446;&#26631;&#30340;&#31435;&#22330;&#65288;&#25903;&#25345;&#12289;&#21453;&#23545;&#25110;&#20013;&#31435;&#65289;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#30340;&#22823;&#37327;&#22686;&#21152;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#22788;&#29702;&#31435;&#22330;&#26816;&#27979;&#30340;&#26694;&#26550;&#26159;&#23558;&#20854;&#36716;&#21270;&#20026;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#21462;&#20195;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#26631;&#35760;&#25968;&#25454;&#21644;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#30340;&#20449;&#24687;&#19981;&#36275;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#21487;&#35299;&#37322;&#24615;&#12290;ChatGPT&#26159;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20110;2022&#24180;11&#26376;30&#26085;&#21457;&#24067;&#12290;&#38024;&#23545;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ChatGPT&#21487;&#20197;&#22312;&#24120;&#29992;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;SemEval-2016&#21644;P-Stance&#65289;&#19978;&#23454;&#29616;SOTA&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;ChatGPT&#21487;&#20197;&#20026;&#20854;&#33258;&#36523;&#30340;&#39044;&#27979;&#25552;&#20379;&#35299;&#37322;&#65292;&#36825;&#36229;&#20986;&#20102;&#20219;&#20309;&#29616;&#26377;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stance detection refers to the task of extracting the standpoint (Favor, Against or Neither) towards a target in given texts. Such research gains increasing attention with the proliferation of social media contents. The conventional framework of handling stance detection is converting it into text classification tasks. Deep learning models have already replaced rule-based models and traditional machine learning models in solving such problems. Current deep neural networks are facing two main challenges which are insufficient labeled data and information in social media posts and the unexplainable nature of deep learning models. A new pre-trained language model chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our experiments show that ChatGPT can achieve SOTA or similar performance for commonly used datasets including SemEval-2016 and P-Stance. At the same time, ChatGPT can provide explanation for its own prediction, which is beyond the capability of any existing mo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MiLMo&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#22909;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#23569;&#25968;&#27665;&#26063;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2212.01779</link><description>&lt;p&gt;
MiLMo&#65306;&#23569;&#25968;&#27665;&#26063;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MiLMo:Minority Multilingual Pre-trained Language Model. (arXiv:2212.01779v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MiLMo&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#22909;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#23569;&#25968;&#27665;&#26063;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23567;&#35268;&#27169;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#27169;&#22411;&#21516;&#26102;&#33021;&#22815;&#29702;&#35299;&#22810;&#31181;&#35821;&#35328;&#12290;&#30446;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#19978;&#65292;&#30456;&#23545;&#20110;&#24120;&#35265;&#35821;&#35328;&#32780;&#35328;&#65292;&#23545;&#20110;&#23569;&#25968;&#27665;&#26063;&#31561;&#36164;&#28304;&#31232;&#32570;&#35821;&#35328;&#30340;&#30740;&#31350;&#21364;&#30456;&#23545;&#36739;&#23569;&#65292;&#20844;&#20849;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MiLMo&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#33945;&#21476;&#35821;&#12289;&#34255;&#35821;&#12289;&#32500;&#21566;&#23572;&#35821;&#12289;&#21704;&#33832;&#20811;&#35821;&#21644;&#26397;&#40092;&#35821;&#22312;&#20869;&#30340;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#22909;&#12290;&#20026;&#39564;&#35777;MiLMo&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#24182;&#35299;&#20915;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#25968;&#25454;&#38598;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MiTC&#30340;&#23569;&#25968;&#27665;&#26063;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;Word2vec&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models are trained on large-scale unsupervised data, and they can fine-turn the model only on small-scale labeled datasets, and achieve good results. Multilingual pre-trained language models can be trained on multiple languages, and the model can understand multiple languages at the same time. At present, the search on pre-trained models mainly focuses on rich resources, while there is relatively little research on low-resource languages such as minority languages, and the public multilingual pre-trained language model can not work well for minority languages. Therefore, this paper constructs a multilingual pre-trained model named MiLMo that performs better on minority language tasks, including Mongolian, Tibetan, Uyghur, Kazakh and Korean. To solve the problem of scarcity of datasets on minority languages and verify the effectiveness of the MiLMo model, this paper constructs a minority multilingual text classification dataset named MiTC, and trains a word2vec mode
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;KG&#19978;&#65292;&#26080;&#27861;&#38543;&#30528;KG&#19981;&#26029;&#22686;&#38271;&#32780;&#21450;&#26102;&#33719;&#21462;&#26032;&#30693;&#35782;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#32456;&#36523;KG&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#32456;&#36523;&#23884;&#20837;&#23398;&#20064;&#19982;&#36716;&#31227;&#65292;&#36890;&#36807;&#23884;&#20837;&#36716;&#31227;&#31574;&#30053;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2211.15845</link><description>&lt;p&gt;
&#19981;&#26029;&#22686;&#38271;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#32456;&#36523;&#23884;&#20837;&#23398;&#20064;&#19982;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Lifelong Embedding Learning and Transfer for Growing Knowledge Graphs. (arXiv:2211.15845v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15845
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;KG&#19978;&#65292;&#26080;&#27861;&#38543;&#30528;KG&#19981;&#26029;&#22686;&#38271;&#32780;&#21450;&#26102;&#33719;&#21462;&#26032;&#30693;&#35782;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#32456;&#36523;KG&#23884;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#32456;&#36523;&#23884;&#20837;&#23398;&#20064;&#19982;&#36716;&#31227;&#65292;&#36890;&#36807;&#23884;&#20837;&#36716;&#31227;&#31574;&#30053;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;KG&#19978;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;KG&#24182;&#19981;&#20445;&#25345;&#38745;&#24577;&#65292;&#32780;&#26159;&#38543;&#30528;KG&#24212;&#29992;&#30340;&#21457;&#23637;&#32780;&#21457;&#23637;&#21644;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#26032;&#20107;&#23454;&#21644;&#20197;&#21069;&#26410;&#35265;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#19981;&#26029;&#20986;&#29616;&#65292;&#38656;&#35201;&#19968;&#31181;&#23884;&#20837;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#22686;&#38271;&#24555;&#36895;&#23398;&#20064;&#21644;&#36716;&#31227;&#26032;&#30693;&#35782;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;KG&#23884;&#20837;&#30340;&#19968;&#20010;&#25193;&#23637;&#39046;&#22495;&#65292;&#21363;&#32456;&#36523;KG&#23884;&#20837;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#19981;&#24517;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23884;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#25345;&#23545;KG&#22686;&#38271;&#24555;&#29031;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#20445;&#30041;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21253;&#25324;&#29992;&#20110;&#23884;&#20837;&#23398;&#20064;&#21644;&#26356;&#26032;&#30340;&#25513;&#30721;KG&#33258;&#32534;&#30721;&#22120;&#65292;&#20855;&#26377;&#23884;&#20837;&#36716;&#31227;&#31574;&#30053;&#65292;&#23558;&#23398;&#20064;&#30340;&#30693;&#35782;&#27880;&#20837;&#26032;&#23454;&#20307;&#21644;&#20851;&#31995;&#23884;&#20837;&#65292;&#20197;&#21450;&#23884;&#20837;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#30740;&#31350;KG&#22686;&#38271;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Existing knowledge graph (KG) embedding models have primarily focused on static KGs. However, real-world KGs do not remain static, but rather evolve and grow in tandem with the development of KG applications. Consequently, new facts and previously unseen entities and relations continually emerge, necessitating an embedding model that can quickly learn and transfer new knowledge through growth. Motivated by this, we delve into an expanding field of KG embedding in this paper, i.e., lifelong KG embedding. We consider knowledge transfer and retention of the learning on growing snapshots of a KG without having to learn embeddings from scratch. The proposed model includes a masked KG autoencoder for embedding learning and update, with an embedding transfer strategy to inject the learned knowledge into the new entity and relation embeddings, and an embedding regularization method to avoid catastrophic forgetting. To investigate the impacts of different aspects of KG growth, we construct four
&lt;/p&gt;</description></item><item><title>STAGE&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;Aspect Sentiment Triplet Extraction&#65288;ASTE&#65289;&#20219;&#21153;&#65292;&#21487;&#20197;&#22788;&#29702;&#21333;&#35789;&#20855;&#26377;&#22810;&#20010;&#35282;&#33394;&#25110;&#26041;&#38754;/&#24847;&#35265;&#26415;&#35821;&#30001;&#22810;&#20010;&#21333;&#35789;&#32452;&#25104;&#31561;&#22797;&#26434;&#24773;&#20917;&#65292;&#36890;&#36807;Span&#26631;&#35760;&#21644;&#36138;&#24515;&#25512;&#29702;&#26041;&#26696;&#26469;&#25552;&#21462;&#24773;&#24863;&#19977;&#20803;&#32452;&#65292;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#32473;&#23450;&#21477;&#23376;&#20013;&#26041;&#38754;&#26415;&#35821;&#12289;&#30456;&#24212;&#30340;&#24847;&#35265;&#26415;&#35821;&#21644;&#30456;&#20851;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15003</link><description>&lt;p&gt;
STAGE: &#22522;&#20110;Span&#26631;&#35760;&#21644;&#36138;&#24515;&#25512;&#29702;&#26041;&#26696;&#30340;Aspect Sentiment Triplet&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
STAGE: Span Tagging and Greedy Inference Scheme for Aspect Sentiment Triplet Extraction. (arXiv:2211.15003v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15003
&lt;/p&gt;
&lt;p&gt;
STAGE&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;Aspect Sentiment Triplet Extraction&#65288;ASTE&#65289;&#20219;&#21153;&#65292;&#21487;&#20197;&#22788;&#29702;&#21333;&#35789;&#20855;&#26377;&#22810;&#20010;&#35282;&#33394;&#25110;&#26041;&#38754;/&#24847;&#35265;&#26415;&#35821;&#30001;&#22810;&#20010;&#21333;&#35789;&#32452;&#25104;&#31561;&#22797;&#26434;&#24773;&#20917;&#65292;&#36890;&#36807;Span&#26631;&#35760;&#21644;&#36138;&#24515;&#25512;&#29702;&#26041;&#26696;&#26469;&#25552;&#21462;&#24773;&#24863;&#19977;&#20803;&#32452;&#65292;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#32473;&#23450;&#21477;&#23376;&#20013;&#26041;&#38754;&#26415;&#35821;&#12289;&#30456;&#24212;&#30340;&#24847;&#35265;&#26415;&#35821;&#21644;&#30456;&#20851;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Aspect Sentiment Triplet Extraction&#65288;ASTE&#65289;&#24050;&#25104;&#20026;&#24773;&#24863;&#20998;&#26512;&#30740;&#31350;&#20013;&#30340;&#26032;&#20852;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#32473;&#23450;&#30340;&#21477;&#23376;&#20013;&#25552;&#21462;&#26041;&#38754;&#26415;&#35821;&#12289;&#30456;&#24212;&#30340;&#24847;&#35265;&#26415;&#35821;&#21644;&#30456;&#24212;&#30340;&#24773;&#24863;&#26497;&#24615;&#19977;&#20803;&#32452;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25552;&#20986;&#30340;&#22823;&#22810;&#25968;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#37117;&#26377;&#20854;&#23616;&#38480;&#24615;&#65306;&#36807;&#20110;&#20381;&#36182;&#20110;1&#65289;&#20551;&#35774;&#27599;&#20010;&#21333;&#35789;&#20165;&#19982;&#21333;&#20010;&#35282;&#33394;&#65288;&#20363;&#22914;&#65292;&#26041;&#38754;&#26415;&#35821;&#25110;&#24847;&#35265;&#26415;&#35821;&#31561;&#65289;&#30456;&#20851;&#32852;&#21644;2&#65289;&#35789;&#32423;&#20132;&#20114;&#24182;&#23558;&#27599;&#20010;&#24847;&#35265;/&#26041;&#38754;&#35270;&#20026;&#19968;&#32452;&#29420;&#31435;&#30340;&#21333;&#35789;&#12290;&#22240;&#27492;&#65292;&#22312;&#22797;&#26434;&#30340;ASTE&#20219;&#21153;&#20013;&#25928;&#26524;&#19981;&#20339;&#65292;&#20363;&#22914;&#19968;&#20010;&#21333;&#35789;&#19982;&#22810;&#20010;&#35282;&#33394;&#30456;&#20851;&#32852;&#25110;&#32773;&#19968;&#20010;&#26041;&#38754;/&#24847;&#35265;&#26415;&#35821;&#30001;&#22810;&#20010;&#21333;&#35789;&#32452;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Span&#26631;&#35760;&#21644;&#36138;&#24515;&#25512;&#29702;&#65288;STAGE&#65289;&#65292;&#20197;&#22312;&#36328;&#24230;&#32423;&#21035;&#19978;&#25552;&#21462;&#24773;&#24863;&#19977;&#20803;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#36328;&#24230;&#21487;&#33021;&#30001;&#22810;&#20010;&#21333;&#35789;&#32452;&#25104;&#65292;&#24182;&#21516;&#26102;&#25198;&#28436;&#19981;&#21516;&#30340;&#35282;&#33394;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STAGE&#30340;&#26032;&#26041;&#27861;&#26469;&#22788;&#29702;ASTE&#20219;&#21153;&#65292;&#23427;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20102;&#32473;&#23450;&#21477;&#23376;&#20013;&#26041;&#38754;&#26415;&#35821;&#12289;&#30456;&#24212;&#30340;&#24847;&#35265;&#26415;&#35821;&#21644;&#30456;&#20851;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect Sentiment Triplet Extraction (ASTE) has become an emerging task in sentiment analysis research, aiming to extract triplets of the aspect term, its corresponding opinion term, and its associated sentiment polarity from a given sentence. Recently, many neural networks based models with different tagging schemes have been proposed, but almost all of them have their limitations: heavily relying on 1) prior assumption that each word is only associated with a single role (e.g., aspect term, or opinion term, etc. ) and 2) word-level interactions and treating each opinion/aspect as a set of independent words. Hence, they perform poorly on the complex ASTE task, such as a word associated with multiple roles or an aspect/opinion term with multiple words. Hence, we propose a novel approach, Span TAgging and Greedy infErence (STAGE), to extract sentiment triplets in span-level, where each span may consist of multiple words and play different roles simultaneously. To this end, this paper for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;&#21307;&#38498;&#20303;&#38498;&#30149;&#21382;&#25991;&#26412;&#36827;&#34892;&#20986;&#38498;&#24635;&#32467;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#25552;&#21462;&#21644;&#25277;&#35937;&#24635;&#32467;&#65292;&#21516;&#26102;&#27979;&#35797;&#20102;&#19968;&#31181;&#32467;&#21512;&#21307;&#23398;&#27010;&#24565;&#26412;&#20307;&#35770;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.07126</link><description>&lt;p&gt;
&#20020;&#24202;&#27010;&#24565;&#25351;&#23548;&#19979;&#30340;&#28145;&#24230;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#23545;&#20303;&#38498;&#30149;&#21382;&#25991;&#26412;&#36827;&#34892;&#20986;&#38498;&#24635;&#32467;&#21307;&#38498;&#32463;&#36807;&#30340;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
Discharge Summary Hospital Course Summarisation of In Patient Electronic Health Record Text with Clinical Concept Guided Deep Pre-Trained Transformer Models. (arXiv:2211.07126v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;&#21307;&#38498;&#20303;&#38498;&#30149;&#21382;&#25991;&#26412;&#36827;&#34892;&#20986;&#38498;&#24635;&#32467;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#25552;&#21462;&#21644;&#25277;&#35937;&#24635;&#32467;&#65292;&#21516;&#26102;&#27979;&#35797;&#20102;&#19968;&#31181;&#32467;&#21512;&#21307;&#23398;&#27010;&#24565;&#26412;&#20307;&#35770;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#35201;&#20303;&#38498;&#32463;&#36807;&#65288;BHC&#65289;&#25688;&#35201;&#26159;&#19968;&#27425;&#23436;&#25972;&#21307;&#38498;&#27835;&#30103;&#32463;&#21382;&#30340;&#31616;&#27905;&#27010;&#36848;&#65292;&#34987;&#20889;&#20837;&#30001;&#20840;&#31185;&#21307;&#29983;&#36127;&#36131;&#24739;&#32773;&#25972;&#20307;&#25252;&#29702;&#30340;&#20986;&#38498;&#27010;&#36848;&#20013;&#12290;&#33258;&#21160;&#20174;&#20303;&#38498;&#25991;&#26723;&#20013;&#29983;&#25104;&#25688;&#35201;&#30340;&#26041;&#27861;&#23558;&#22312;&#20943;&#36731;&#21307;&#29983;&#22312;&#39640;&#26102;&#38388;&#21387;&#21147;&#19979;&#24635;&#32467;&#25991;&#26723;&#30340;&#25163;&#21160;&#36127;&#25285;&#26041;&#38754;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#20174;&#20303;&#38498;&#36807;&#31243;&#33258;&#21160;&#20135;&#29983;&#36825;&#20123;&#24635;&#32467;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#65292;&#22240;&#20026;&#28304;&#31508;&#35760;&#26159;&#22312;&#20303;&#38498;&#26399;&#38388;&#20174;&#21508;&#31181;&#19981;&#21516;&#30340;&#35282;&#24230;&#65288;&#22914;&#25252;&#29702;&#65292;&#21307;&#29983;&#65292;&#25918;&#23556;&#23398;&#65289;&#32534;&#20889;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#31181;BHC&#25688;&#35201;&#27010;&#25324;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#25688;&#35201;&#27169;&#22411;&#22312;&#25552;&#21462;&#21644;&#25277;&#35937;&#25688;&#35201;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21253;&#21547;&#21307;&#23398;&#27010;&#24565;&#26412;&#20307;&#35770;&#65288;SNOMED&#65289;&#20316;&#20026;&#20020;&#24202;&#25351;&#23548;&#30340;&#21512;&#22863;&#24335;&#25552;&#21462;&#21644;&#25277;&#35937;&#24635;&#32467;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brief Hospital Course (BHC) summaries are succinct summaries of an entire hospital encounter, embedded within discharge summaries, written by senior clinicians responsible for the overall care of a patient. Methods to automatically produce summaries from inpatient documentation would be invaluable in reducing clinician manual burden of summarising documents under high time-pressure to admit and discharge patients. Automatically producing these summaries from the inpatient course, is a complex, multi-document summarisation task, as source notes are written from various perspectives (e.g. nursing, doctor, radiology), during the course of the hospitalisation. We demonstrate a range of methods for BHC summarisation demonstrating the performance of deep learning summarisation models across extractive and abstractive summarisation scenarios. We also test a novel ensemble extractive and abstractive summarisation model that incorporates a medical concept ontology (SNOMED) as a clinical guidanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26367;&#20195;&#21697;&#25512;&#33616;&#36866;&#24212;&#21040;&#35821;&#35328;&#21305;&#37197;&#38382;&#39064;&#20013;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#21435;&#38500;&#20449;&#21495;&#22122;&#38899;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#35821;&#35328;&#25903;&#25345;&#12290;&#35813;&#27169;&#22411;&#24050;&#25104;&#21151;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;11&#20010;&#24066;&#22330;&#21644;6&#31181;&#35821;&#35328;&#20013;&#37096;&#32626;&#65292;&#25552;&#39640;&#20102;&#39038;&#23458;&#24544;&#35802;&#24230;&#21644;&#36141;&#20080;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.02533</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26367;&#20195;&#21697;&#25512;&#33616;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#24369;&#30417;&#30563;&#30340;&#39038;&#23458;&#34892;&#20026;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A Transformer-Based Substitute Recommendation Model Incorporating Weakly Supervised Customer Behavior Data. (arXiv:2211.02533v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26367;&#20195;&#21697;&#25512;&#33616;&#36866;&#24212;&#21040;&#35821;&#35328;&#21305;&#37197;&#38382;&#39064;&#20013;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#21435;&#38500;&#20449;&#21495;&#22122;&#38899;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#35821;&#35328;&#25903;&#25345;&#12290;&#35813;&#27169;&#22411;&#24050;&#25104;&#21151;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;11&#20010;&#24066;&#22330;&#21644;6&#31181;&#35821;&#35328;&#20013;&#37096;&#32626;&#65292;&#25552;&#39640;&#20102;&#39038;&#23458;&#24544;&#35802;&#24230;&#21644;&#36141;&#20080;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26367;&#20195;&#21697;&#30340;&#25512;&#33616;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#26367;&#20195;&#21697;&#32473;&#39038;&#23458;&#12290;&#20294;&#26159;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#39038;&#23458;&#30340;&#34892;&#20026;&#20449;&#21495;&#65288;&#22914;&#20849;&#21516;&#27983;&#35272;&#21644;&#27983;&#35272;&#20294;&#36141;&#20080;&#21478;&#19968;&#20010;&#20135;&#21697;&#65289;&#26469;&#25429;&#25417;&#26367;&#20195;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#20010;&#26041;&#27861;&#21548;&#36215;&#26469;&#24456;&#30452;&#35266;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20570;&#27861;&#21487;&#33021;&#20250;&#24573;&#30053;&#20135;&#21697;&#30340;&#21151;&#33021;&#21644;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#20135;&#21697;&#26631;&#39064;&#25551;&#36848;&#20316;&#20026;&#27169;&#22411;&#36755;&#20837;&#65292;&#24182;&#32771;&#34385;&#20135;&#21697;&#21151;&#33021;&#65292;&#23558;&#26367;&#20195;&#21697;&#25512;&#33616;&#36866;&#24212;&#21040;&#35821;&#35328;&#21305;&#37197;&#38382;&#39064;&#20013;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#26041;&#27861;&#26469;&#21435;&#38500;&#20174;&#29983;&#20135;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#20449;&#21495;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#24037;&#31243;&#35282;&#24230;&#32771;&#34385;&#22810;&#35821;&#35328;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#22343;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24050;&#37096;&#32626;&#22312;&#19968;&#20010;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;11&#20010;&#24066;&#22330;&#21644;6&#31181;&#35821;&#35328;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39038;&#23458;&#24544;&#35802;&#24230;&#21644;&#36141;&#20080;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The substitute-based recommendation is widely used in E-commerce to provide better alternatives to customers. However, existing research typically uses the customer behavior signals like co-view and view-but-purchase-another to capture the substitute relationship. Despite its intuitive soundness, we find that such an approach might ignore the functionality and characteristics of products. In this paper, we adapt substitute recommendation into language matching problem by taking product title description as model input to consider product functionality. We design a new transformation method to de-noise the signals derived from production data. In addition, we consider multilingual support from the engineering point of view. Our proposed end-to-end transformer-based model achieves both successes from offline and online experiments. The proposed model has been deployed in a large-scale E-commerce website for 11 marketplaces in 6 languages. Our proposed model is demonstrated to increase re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22810;&#35821;&#38899;&#21040;&#22270;&#20687;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#38750;&#33521;&#35821;&#26816;&#32034;&#20013;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#24615;&#33021;&#22823;&#24133;&#25552;&#39640;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#20063;&#36866;&#29992;&#20110;&#36328;&#35821;&#38899;&#26816;&#32034;&#21644;&#36328;&#35821;&#38899;&#25991;&#26412;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2211.01180</link><description>&lt;p&gt;
M-SpeechCLIP&#65306;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22810;&#35821;&#38899;&#21040;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for Multilingual Speech to Image Retrieval. (arXiv:2211.01180v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22810;&#35821;&#38899;&#21040;&#22270;&#20687;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#38750;&#33521;&#35821;&#26816;&#32034;&#20013;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#24615;&#33021;&#22823;&#24133;&#25552;&#39640;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#20063;&#36866;&#29992;&#20110;&#36328;&#35821;&#38899;&#26816;&#32034;&#21644;&#36328;&#35821;&#38899;&#25991;&#26412;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#33521;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;CLIP&#21644;HuBERT&#65289;&#36827;&#34892;&#22810;&#35821;&#31181;&#22270;&#20687;-&#35821;&#38899;&#26816;&#32034;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#38750;&#33521;&#35821;&#22270;&#20687;-&#35821;&#38899;&#26816;&#32034;&#65292;&#25105;&#20204;&#22312;&#20998;&#21035;&#20026;&#27599;&#31181;&#35821;&#35328;&#35757;&#32451;&#21333;&#29420;&#27169;&#22411;&#20197;&#21450;&#22788;&#29702;&#25152;&#26377;&#19977;&#31181;&#35821;&#35328;&#35821;&#38899;&#30340;&#21333;&#19968;&#27169;&#22411;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#32972;&#26223;&#19979;&#27169;&#22411;&#34892;&#20026;&#21644;&#34920;&#29616;&#20043;&#38388;&#30340;&#20027;&#35201;&#24046;&#24322;&#65292;&#36825;&#20123;&#24046;&#24322;&#21487;&#20197;&#24402;&#22240;&#20110;CLIP&#21644;HuBERT&#30340;&#33521;&#35821;&#39044;&#35757;&#32451;&#65292;&#24182;&#25506;&#31350;&#20102;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#24046;&#24322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#26410;&#30475;&#21040;&#20219;&#20309;&#24179;&#34892;&#35821;&#38899;-&#25991;&#26412;&#25110;&#35821;&#38899;-&#35821;&#38899;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#29992;&#20110;&#21333;&#35821;&#21644;&#36328;&#35821;&#38899;&#25991;&#26412;&#26816;&#32034;&#20197;&#21450;&#36328;&#35821;&#38899;&#35821;&#38899;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the use of large-scale, English-only pre-trained models (CLIP and HuBERT) for multilingual image-speech retrieval. For non-English image-speech retrieval, we outperform the current state-of-the-art performance by a wide margin both when training separate models for each language, and with a single model which processes speech in all three languages. We identify key differences in model behavior and performance between English and non-English settings, attributable to the English-only pre-training of CLIP and HuBERT, and investigate how fine-tuning the pre-trained models impacts these differences. Finally, we show that our models can be used for mono- and cross-lingual speech-text retrieval and cross-lingual speech-speech retrieval, despite never having seen any parallel speech-text or speech-speech data during training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#35780;LLMs&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#24182;&#27979;&#35797;&#20102;&#27969;&#34892;&#30340;LLMs (GPT-3 &#21644; GShard) &#22312;&#27492;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#26368;&#31616;&#21333;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#24378;&#35843;&#20102;&#30446;&#21069;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#24314;&#35758;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#24320;&#21457;&#26356;&#20808;&#36827;&#30340;LLM&#22522;&#30784;&#31995;&#32479;&#26469;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2206.10498</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#26080;&#27861;&#35268;&#21010;&#65288;LLM&#22312;&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#20013;&#30340;&#22522;&#20934;&#65289;&#12290;&#65288;arXiv:2206.10498v3 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). (arXiv:2206.10498v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#35780;LLMs&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#24182;&#27979;&#35797;&#20102;&#27969;&#34892;&#30340;LLMs (GPT-3 &#21644; GShard) &#22312;&#27492;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#26368;&#31616;&#21333;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#24378;&#35843;&#20102;&#30446;&#21069;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#24314;&#35758;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#24320;&#21457;&#26356;&#20808;&#36827;&#30340;LLM&#22522;&#30784;&#31995;&#32479;&#26469;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#12290;&#20174;GPT-3&#21040;PaLM&#65292;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#26368;&#26032;&#24615;&#33021;&#27491;&#22312;&#38543;&#30528;&#27599;&#20010;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#20986;&#19981;&#26029;&#25552;&#39640;&#12290;&#38500;&#20102;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#22806;&#65292;&#20154;&#20204;&#23545;&#20110;&#29702;&#35299;&#27492;&#31867;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25512;&#29702;&#33021;&#21147;&#20135;&#29983;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#24182;&#37319;&#29992;&#20102;&#25512;&#29702;&#22522;&#20934;&#26469;&#36827;&#34892;&#27979;&#35780;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#32467;&#26524;&#30475;&#20284;&#31215;&#26497;&#65292;&#36825;&#20123;&#22522;&#20934;&#22312;&#26412;&#36136;&#19978;&#26159;&#31616;&#21333;&#30340;&#65292;LLMs&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#24182;&#19981;&#33021;&#20316;&#20026;&#25903;&#25345;LLMs&#25512;&#29702;&#33021;&#21147;&#65288;&#26377;&#26102;&#26159;&#33618;&#35884;&#30340;&#65289;&#22768;&#31216;&#30340;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#21482;&#20195;&#34920;&#20102;&#19968;&#20010;&#38750;&#24120;&#26377;&#38480;&#30340;&#31616;&#21333;&#25512;&#29702;&#20219;&#21153;&#38598;&#65292;&#22914;&#26524;&#25105;&#20204;&#35201;&#34913;&#37327;&#27492;&#31867;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#30340;&#30495;&#27491;&#38480;&#21046;&#65292;&#25105;&#20204;&#38656;&#35201;&#30740;&#31350;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#35797;LLMs&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#31995;&#21015;&#30340;&#35268;&#21010;&#21644;&#25512;&#29702;&#20219;&#21153;&#65292;&#20363;&#22914;&#21629;&#39064;&#36923;&#36753;&#12289;&#22240;&#26524;&#25512;&#26029;&#21644;&#24120;&#35782;&#25512;&#29702;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#38590;&#24230;&#38543;&#30528;&#20219;&#21153;&#30340;&#36827;&#23637;&#32780;&#36880;&#28176;&#22686;&#21152;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;LLMs&#65288;GPT-3&#21644;GShard&#65289;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#29978;&#33267;&#26080;&#27861;&#22788;&#29702;&#26368;&#31616;&#21333;&#30340;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#24403;&#21069;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#20005;&#37325;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#24320;&#21457;&#21487;&#20197;&#35268;&#21010;&#21644;&#25512;&#29702;&#21464;&#21270;&#30340;LLM&#22522;&#30784;&#31995;&#32479;&#65292;&#20197;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model. Along with natural language abilities, there has been a significant interest in understanding whether such models exhibit reasoning capabilities with the use of reasoning benchmarks. However, even though results are seemingly positive, these benchmarks prove to be simplistic in nature and the performance of LLMs on these benchmarks cannot be used as evidence to support, many a times outlandish, claims being made about LLMs' reasoning capabilities. Further, these only represent a very limited set of simple reasoning tasks and we need to look at more sophisticated reasoning problems if we are to measure the true limits of such LLM-based systems. Motivated by this, we propose an extensible assessment framework to test the capabilities of LL
&lt;/p&gt;</description></item><item><title>InCoder&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#31243;&#24207;&#21512;&#25104;&#21644;&#21452;&#21521;&#19978;&#19979;&#25991;&#30340;&#20195;&#30721;&#22635;&#20805;&#65292;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38646;&#26679;&#26412;&#20195;&#30721;&#22635;&#20805;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2204.05999</link><description>&lt;p&gt;
InCoder&#65306;&#19968;&#31181;&#20195;&#30721;&#22635;&#20805;&#21644;&#21512;&#25104;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InCoder: A Generative Model for Code Infilling and Synthesis. (arXiv:2204.05999v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05999
&lt;/p&gt;
&lt;p&gt;
InCoder&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#31243;&#24207;&#21512;&#25104;&#21644;&#21452;&#21521;&#19978;&#19979;&#25991;&#30340;&#20195;&#30721;&#22635;&#20805;&#65292;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38646;&#26679;&#26412;&#20195;&#30721;&#22635;&#20805;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#24448;&#24448;&#19981;&#26159;&#19968;&#27425;&#20174;&#24038;&#21040;&#21491;&#30340;&#20889;&#20316;&#36807;&#31243;&#65292;&#32780;&#26159;&#21453;&#22797;&#32534;&#36753;&#21644;&#25913;&#36827;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;InCoder&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#24038;&#21040;&#21491;&#30340;&#29983;&#25104;&#36827;&#34892;&#31243;&#24207;&#21512;&#25104;&#65292;&#20063;&#21487;&#20197;&#36827;&#34892;&#32534;&#36753;&#65288;&#36890;&#36807;&#22635;&#20805;&#65289;&#12290;InCoder&#36890;&#36807;&#20174;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#20195;&#30721;&#24211;&#20013;&#38543;&#26426;&#23631;&#34109;&#20195;&#30721;&#22359;&#24182;&#23558;&#20854;&#31227;&#21160;&#21040;&#27599;&#20010;&#25991;&#20214;&#26411;&#23614;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#20854;&#21487;&#20197;&#36827;&#34892;&#21452;&#21521;&#19978;&#19979;&#25991;&#30340;&#20195;&#30721;&#22635;&#20805;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38646;&#26679;&#26412;&#20195;&#30721;&#22635;&#20805;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#31867;&#22411;&#25512;&#26029;&#12289;&#27880;&#37322;&#29983;&#25104;&#21644;&#21464;&#37327;&#37325;&#21629;&#21517;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20855;&#26377;&#21452;&#21521;&#19978;&#19979;&#25991;&#30340;&#26465;&#20214;&#19979;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#22312;&#26631;&#20934;&#31243;&#24207;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#19982;&#30456;&#20284;&#35268;&#27169;&#30340;&#20165;&#20174;&#24038;&#21040;&#21491;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#12290;InCoder&#27169;&#22411;&#21644;&#20195;&#30721;&#24050;&#32463;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. htt
&lt;/p&gt;</description></item><item><title>&#25688;&#35201;&#25991;&#26412;&#33258;&#21160;&#27010;&#25324;&#27169;&#22411;&#30340;&#25277;&#35937;&#21270;&#33021;&#21147;&#23545;&#20110;&#29983;&#25104;&#20934;&#30830;&#25688;&#35201;&#25991;&#26412;&#26159;&#21452;&#20995;&#21073;&#65292;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#35774;&#35745;&#20107;&#23454;&#24863;&#30693;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25913;&#36827;&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#20943;&#23569;&#20107;&#23454;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2104.14839</link><description>&lt;p&gt;
&#25688;&#35201;&#25991;&#26412;&#33258;&#21160;&#27010;&#25324;&#20013;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#38382;&#39064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey. (arXiv:2104.14839v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.14839
&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#25991;&#26412;&#33258;&#21160;&#27010;&#25324;&#27169;&#22411;&#30340;&#25277;&#35937;&#21270;&#33021;&#21147;&#23545;&#20110;&#29983;&#25104;&#20934;&#30830;&#25688;&#35201;&#25991;&#26412;&#26159;&#21452;&#20995;&#21073;&#65292;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#35774;&#35745;&#20107;&#23454;&#24863;&#30693;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25913;&#36827;&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#20943;&#23569;&#20107;&#23454;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#20123;&#22522;&#20110;Seq2Seq&#26694;&#26550;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#65292;&#29992;&#20110;&#23558;&#36755;&#20837;&#36716;&#21270;&#20026;&#26356;&#20026;&#25277;&#35937;&#30340;&#25688;&#35201;&#25991;&#26412;&#12290;&#36825;&#20123;&#31070;&#32463;&#27169;&#22411;&#21487;&#20197;&#33258;&#30001;&#22320;&#29983;&#25104;&#25688;&#35201;&#65292;&#27809;&#26377;&#23545;&#25152;&#20351;&#29992;&#21333;&#35789;&#25110;&#30701;&#35821;&#30340;&#20219;&#20309;&#38480;&#21046;&#65292;&#20854;&#26684;&#24335;&#26356;&#25509;&#36817;&#20110;&#20154;&#24037;&#32534;&#36753;&#30340;&#25688;&#35201;&#65292;&#36755;&#20986;&#26356;&#26131;&#35835;&#65292;&#27969;&#30021;&#33258;&#28982;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#27169;&#22411;&#30340;&#25277;&#35937;&#21270;&#33021;&#21147;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#12290;&#25152;&#29983;&#25104;&#25688;&#35201;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#26159;&#25991;&#31456;&#20107;&#23454;&#24615;&#20449;&#24687;&#30340;&#25197;&#26354;&#25110;&#25423;&#36896;&#12290;&#21407;&#25991;&#26412;&#21644;&#25688;&#35201;&#30340;&#19981;&#19968;&#33268;&#24615;&#24341;&#36215;&#20102;&#21508;&#31181;&#20851;&#20110;&#20854;&#36866;&#29992;&#24615;&#30340;&#25285;&#24551;&#65292;&#20197;&#21450;&#38024;&#23545;&#25991;&#26412;&#25688;&#35201;&#30340;&#20808;&#21069;&#35780;&#20272;&#26041;&#27861;&#24182;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#20027;&#35201;&#20998;&#20026;&#20004;&#31867;&#65292;&#19968;&#31867;&#26159;&#35774;&#35745;&#20107;&#23454;&#24863;&#30693;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#36873;&#25321;&#20248;&#31168;&#30340;&#25688;&#35201;&#25991;&#26412;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, various neural encoder-decoder models pioneered by Seq2Seq framework have been proposed to achieve the goal of generating more abstractive summaries by learning to map input text to output text. At a high level, such neural models can freely generate summaries without any constraint on the words or phrases used. Moreover, their format is closer to human-edited summaries and output is more readable and fluent. However, the neural model's abstraction ability is a double-edged sword. A commonly observed problem with the generated summaries is the distortion or fabrication of factual information in the article. This inconsistency between the original text and the summary has caused various concerns over its applicability, and the previous evaluation methods of text summarization are not suitable for this issue. In response to the above problems, the current research direction is predominantly divided into two categories, one is to design fact-aware evaluation metrics to select ou
&lt;/p&gt;</description></item></channel></rss>