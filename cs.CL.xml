<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#39033;&#35843;&#30740;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#26041;&#27861;&#30340;&#29616;&#29366;&#65292;&#21253;&#25324;&#22522;&#20110;LLM&#23548;&#20986;&#30340;&#25351;&#26631;&#12289;&#24341;&#23548;LLM&#21644;&#20351;&#29992;&#24102;&#26377;&#26631;&#35760;&#35780;&#20272;&#25968;&#25454;&#30340;LLM&#24494;&#35843;&#65292;&#24182;&#35752;&#35770;&#20102;&#20154;&#31867;&#19982;LLM&#30340;&#21512;&#20316;&#12290;&#21516;&#26102;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01383</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#65306;&#29616;&#29366;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
LLM-based NLG Evaluation: Current Status and Challenges
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01383
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#26041;&#27861;&#30340;&#29616;&#29366;&#65292;&#21253;&#25324;&#22522;&#20110;LLM&#23548;&#20986;&#30340;&#25351;&#26631;&#12289;&#24341;&#23548;LLM&#21644;&#20351;&#29992;&#24102;&#26377;&#26631;&#35760;&#35780;&#20272;&#25968;&#25454;&#30340;LLM&#24494;&#35843;&#65292;&#24182;&#35752;&#35770;&#20102;&#20154;&#31867;&#19982;LLM&#30340;&#21512;&#20316;&#12290;&#21516;&#26102;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#25361;&#25112;&#37325;&#37325;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#35780;&#20272;&#25351;&#26631;&#20027;&#35201;&#36890;&#36807;&#31995;&#32479;&#36755;&#20986;&#21644;&#21442;&#32771;&#25991;&#26412;&#20043;&#38388;&#30340;&#20869;&#23481;&#65288;&#22914;n-gram&#65289;&#37325;&#21472;&#24230;&#26469;&#25429;&#25417;&#65292;&#36828;&#36828;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#32780;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;NLG&#35780;&#20272;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#21508;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;LLM&#23548;&#20986;&#30340;&#25351;&#26631;&#12289;&#24341;&#23548;LLM&#21644;&#20351;&#29992;&#24102;&#26377;&#26631;&#35760;&#35780;&#20272;&#25968;&#25454;&#30340;LLM&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;&#22522;&#20110;LLM&#30340;NLG&#35780;&#20272;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20998;&#21035;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20154;&#31867;&#19982;LLM&#30340;&#21512;&#20316;&#29992;&#20110;NLG&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#20960;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating natural language generation (NLG) is a vital but challenging problem in artificial intelligence. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled evaluation data. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. We also discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several open problems in this area and point out future research directions.
&lt;/p&gt;</description></item><item><title>Pok\'eLLMon&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#12290;&#23427;&#36890;&#36807;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#21644;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#24182;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01118</link><description>&lt;p&gt;
Pok\'eLLMon&#65306;&#19968;&#20010;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Pok\'emon&#23545;&#25112;&#30340;&#19982;&#20154;&#31867;&#33021;&#21147;&#30456;&#24403;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01118
&lt;/p&gt;
&lt;p&gt;
Pok\'eLLMon&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#12290;&#23427;&#36890;&#36807;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#21644;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#24182;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;\textsc{Pok\'eLLMon}&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#25112;&#26415;&#23545;&#25112;&#28216;&#25103;&#20013;&#23454;&#29616;&#20154;&#31867;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#21270;&#36523;&#20195;&#29702;&#26426;&#22120;&#20154;&#65292;&#21516;&#26102;&#20197;Pok\'emon&#23545;&#25112;&#20026;&#20363;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290; \textsc{Pok\'eLLMon}&#30340;&#35774;&#35745;&#37319;&#29992;&#20102;&#19977;&#20010;&#20851;&#38190;&#31574;&#30053;&#65306;&#65288;i&#65289;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#21363;&#26102;&#20351;&#29992;&#20174;&#23545;&#25112;&#20013;&#33719;&#24471;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21453;&#39304;&#26469;&#36880;&#27493;&#23436;&#21892;&#31574;&#30053;&#65307;&#65288;ii&#65289;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#65292;&#21363;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#20197;&#23545;&#25239;&#20135;&#29983;&#24187;&#35273;&#29616;&#35937;&#65292;&#24182;&#20351;&#20195;&#29702;&#26426;&#22120;&#20154;&#33021;&#22815;&#21450;&#26102;&#27491;&#30830;&#22320;&#34892;&#21160;&#65307;&#65288;iii&#65289;&#19968;&#33268;&#30340;&#34892;&#21160;&#29983;&#25104;&#65292;&#20197;&#20943;&#36731;&#20195;&#29702;&#26426;&#22120;&#20154;&#38754;&#23545;&#24378;&#25932;&#26102;&#30340;&#8220;&#24778;&#24908;&#25442;&#25163;&#8221;&#29616;&#35937;&#65292;&#20351;&#20854;&#21487;&#20197;&#36867;&#36991;&#25112;&#26007;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#36827;&#34892;&#30340;&#22312;&#32447;&#23545;&#25112;&#20013;&#65292;\textsc{Pok\'eLLMon}&#37319;&#29992;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25112;&#26007;&#31574;&#30053;&#21644;&#21450;&#26102;&#20915;&#31574;&#65292;&#20854;&#22312;Ladder&#27604;&#36187;&#20013;&#36798;&#21040;&#20102;49%&#30340;&#32988;&#29575;&#65292;&#22312;&#36992;&#35831;&#23545;&#25112;&#20013;&#36798;&#21040;&#20102;56%&#30340;&#32988;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21644;&#21487;&#29609;&#30340;&#25112;&#26007;&#26085;&#24535;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#25214;&#21040;&#65306;\url{https://gith
&lt;/p&gt;
&lt;p&gt;
We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pok\'emon battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the \textit{panic switching} phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates \textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision making, achieving 49\% of win rate in the Ladder competitions and 56\% of win rate in the invited battles. Our implementation and playable battle logs are available at: \url{https://gith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#29616;&#26377;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#30340;&#25216;&#26415;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;"Who's Harry Potter" (WHP)&#27169;&#22411;&#19978;&#24212;&#29992;&#20102;&#19968;&#31995;&#21015;&#27979;&#35797;&#65292;&#21457;&#29616;WHP&#30340;&#36951;&#24536;&#34920;&#29616;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12289;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#22312;&#30456;&#20851;&#39046;&#22495;&#23384;&#22312;&#26049;&#36335;&#36951;&#24536;&#12290;</title><link>https://arxiv.org/abs/2402.16835</link><description>&lt;p&gt;
&#35780;&#20272;LLMs&#20013;&#24378;&#22823;&#36951;&#24536;&#30340;&#20843;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Eight Methods to Evaluate Robust Unlearning in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#29616;&#26377;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#30340;&#25216;&#26415;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;"Who's Harry Potter" (WHP)&#27169;&#22411;&#19978;&#24212;&#29992;&#20102;&#19968;&#31995;&#21015;&#27979;&#35797;&#65292;&#21457;&#29616;WHP&#30340;&#36951;&#24536;&#34920;&#29616;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12289;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#22312;&#30456;&#20851;&#39046;&#22495;&#23384;&#22312;&#26049;&#36335;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16835v1 &#31867;&#22411;&#20844;&#21578;&#65306;&#26032;&#25688;&#35201;&#65306;&#26426;&#22120;&#36951;&#24536;&#21487;&#29992;&#20110;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#21024;&#38500;&#26377;&#23475;&#33021;&#21147;&#21644;&#35760;&#24518;&#25991;&#26412;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#26631;&#20934;&#21270;&#26041;&#27861;&#20005;&#26684;&#35780;&#20272;&#23427;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;&#36951;&#24536;&#35780;&#20272;&#30340;&#25216;&#26415;&#21644;&#23616;&#38480;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;Eldan&#21644;Russinovich&#65288;2023&#24180;&#65289;&#30340;&#8220;&#35841;&#26159;&#21704;&#21033;&#27874;&#29305;&#8221;&#65288;WHP&#65289;&#27169;&#22411;&#30340;&#36951;&#24536;&#30340;&#31283;&#20581;&#24615;&#21644;&#31454;&#20105;&#21147;&#24212;&#29992;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#38598;&#12290;&#34429;&#28982;&#20351;&#29992;Eldan&#21644;Russinovich&#30340;&#8220;&#29087;&#24713;&#24230;&#8221;&#25351;&#26631;&#35780;&#20272;WHP&#30340;&#36951;&#24536;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65306;i&#65289;&#21487;&#20197;&#21487;&#38752;&#22320;&#25552;&#21462;&#39640;&#20110;&#22522;&#20934;&#32447;&#30340;&#30693;&#35782;&#37327;&#65292;ii&#65289;&#22312;&#21704;&#21033;&#27874;&#29305;&#38382;&#31572;&#20219;&#21153;&#19978;&#65292;WHP&#30340;&#34920;&#29616;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#65292;iii&#65289;&#23427;&#20197;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#30340;&#26041;&#24335;&#20195;&#34920;&#28508;&#22312;&#30693;&#35782;&#65292;iv&#65289;&#22312;&#30456;&#20851;&#39046;&#22495;&#23384;&#22312;&#26049;&#36335;&#36951;&#24536;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#20840;&#38754;&#35780;&#20272;&#36951;&#24536;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16835v1 Announce Type: new  Abstract: Machine unlearning can be useful for removing harmful capabilities and memorized text from large language models (LLMs), but there are not yet standardized methods for rigorously evaluating it. In this paper, we first survey techniques and limitations of existing unlearning evaluations. Second, we apply a comprehensive set of tests for the robustness and competitiveness of unlearning in the "Who's Harry Potter" (WHP) model from Eldan and Russinovich (2023). While WHP's unlearning generalizes well when evaluated with the "Familiarity" metric from Eldan and Russinovich, we find i) higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP performs on par with the original model on Harry Potter Q&amp;A tasks, iii) it represents latent knowledge comparably to the original model, and iv) there is collateral unlearning in related domains. Overall, our results highlight the importance of comprehensive unlearning evaluation that av
&lt;/p&gt;</description></item><item><title>MLLMs&#36890;&#36807;&#24494;&#35843;&#33719;&#24471;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#20294;&#25237;&#24433;&#24182;&#26410;&#25552;&#21462;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16832</link><description>&lt;p&gt;
&#31070;&#31192;&#30340;&#25237;&#24433;&#65306;&#22810;&#27169;&#24577;LLMs&#22312;&#27809;&#26377;&#26356;&#20016;&#23500;&#30340;&#36328;&#27169;&#24577;&#25237;&#24433;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16832
&lt;/p&gt;
&lt;p&gt;
MLLMs&#36890;&#36807;&#24494;&#35843;&#33719;&#24471;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#20294;&#25237;&#24433;&#24182;&#26410;&#25552;&#21462;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22914;LLaVA&#21644;GPT-4(V)&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#20851;&#20110;&#22270;&#20687;&#30340;&#36890;&#29992;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#29616;&#25104;&#30340;MLLMs&#21487;&#33021;&#22312;&#35832;&#22914;&#30382;&#32932;&#30149;&#23398;&#21644;&#20892;&#19994;&#31561;&#39046;&#22495;&#30340;&#22270;&#20687;&#19978;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#24517;&#39035;&#36827;&#34892;&#24494;&#35843;&#20197;&#35299;&#38145;&#29305;&#23450;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;4&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#22312;&#20004;&#31181;&#24494;&#35843;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#30528;MLLM&#30340;&#24494;&#35843;&#65292;&#23427;&#30830;&#23454;&#33719;&#24471;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26356;&#26032;&#24182;&#27809;&#26377;&#23548;&#33268;&#25237;&#24433;&#25552;&#21462;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16832v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable general-purpose conversations about images with the language modality. As off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications. The prevalent architecture of current open-source MLLMs comprises two major modules: an image-language (cross-modal) projection network and a large language model. It is desirable to understand the roles of these two modules in modeling domain-specific visual attributes to inform the design of future models and streamline the interpretability efforts on the current models. To this end, via experiments on 4 datasets and under 2 fine-tuning settings, we find that as the MLLM is fine-tuned, it indeed gains domain-specific visual capabilities, but the updates do not lead to the projection extracting relevant domain-specific visual
&lt;/p&gt;</description></item><item><title>SKILL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23618;&#32452;&#20043;&#38388;&#36827;&#34892;&#33976;&#39311;&#65292;&#32780;&#19981;&#26159;&#33976;&#39311;&#25945;&#24072;&#32593;&#32476;&#20013;&#20219;&#24847;&#36873;&#25321;&#30340;&#21333;&#20010;&#23618;&#65292;&#36890;&#36807;&#23618;&#27425;&#32858;&#31867;&#31243;&#24207;&#30830;&#23450;&#35201;&#33976;&#39311;&#30340;&#23618;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;DPHuBERT&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;30M&#21442;&#25968;&#27169;&#22411;&#31867;&#21035;&#20013;&#22312;&#20960;&#20010;SUPERB&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16830</link><description>&lt;p&gt;
SKILL&#65306;&#38754;&#21521;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#30456;&#20284;&#24615;&#24863;&#30693;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
SKILL: Similarity-aware Knowledge distILLation for Speech Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16830
&lt;/p&gt;
&lt;p&gt;
SKILL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23618;&#32452;&#20043;&#38388;&#36827;&#34892;&#33976;&#39311;&#65292;&#32780;&#19981;&#26159;&#33976;&#39311;&#25945;&#24072;&#32593;&#32476;&#20013;&#20219;&#24847;&#36873;&#25321;&#30340;&#21333;&#20010;&#23618;&#65292;&#36890;&#36807;&#23618;&#27425;&#32858;&#31867;&#31243;&#24207;&#30830;&#23450;&#35201;&#33976;&#39311;&#30340;&#23618;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;DPHuBERT&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;30M&#21442;&#25968;&#27169;&#22411;&#31867;&#21035;&#20013;&#22312;&#20960;&#20010;SUPERB&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#21508;&#31181;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#20026;&#20102;&#25552;&#39640;&#20854;&#25928;&#29575;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#21033;&#29992;&#21387;&#32553;&#25216;&#26415;&#12290;&#26368;&#36817;&#19968;&#20010;&#26174;&#33879;&#30340;&#23581;&#35797;&#26159;DPHuBERT&#65292;&#23427;&#24212;&#29992;&#32852;&#21512;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#21644;&#32467;&#26500;&#21270;&#20462;&#21098;&#26469;&#23398;&#20064;&#19968;&#20010;&#26174;&#33879;&#36739;&#23567;&#30340;SSL&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;SKILL&#36129;&#29486;&#21040;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#65292;SKILL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#23618;&#32452;&#36827;&#34892;&#33976;&#39311;&#65292;&#32780;&#19981;&#26159;&#23545;&#25945;&#24072;&#32593;&#32476;&#20013;&#20219;&#24847;&#36873;&#25321;&#30340;&#21333;&#20010;&#23618;&#36827;&#34892;&#33976;&#39311;&#12290;&#30830;&#23450;&#35201;&#33976;&#39311;&#30340;&#23618;&#26159;&#36890;&#36807;&#24212;&#29992;&#20110;&#23618;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#23618;&#27425;&#32858;&#31867;&#31243;&#24207;&#23454;&#29616;&#30340;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#33976;&#39311;&#21518;&#30340;WavLM Base+&#19981;&#20165;&#32988;&#36807;DPHuBERT&#65292;&#32780;&#19988;&#22312;30M&#21442;&#25968;&#27169;&#22411;&#31867;&#21035;&#20013;&#22312;&#20960;&#20010;SUPERB&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16830v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) has achieved remarkable success across various speech-processing tasks. To enhance its efficiency, previous works often leverage the use of compression techniques. A notable recent attempt is DPHuBERT, which applies joint knowledge distillation (KD) and structured pruning to learn a significantly smaller SSL model. In this paper, we contribute to this research domain by introducing SKILL, a novel method that conducts distillation across groups of layers instead of distilling individual arbitrarily selected layers within the teacher network. The identification of the layers to distill is achieved through a hierarchical clustering procedure applied to layer similarity measures. Extensive experiments demonstrate that our distilled version of WavLM Base+ not only outperforms DPHuBERT but also achieves state-of-the-art results in the 30M parameters model class across several SUPERB tasks.
&lt;/p&gt;</description></item><item><title>GISTEmbed&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#22686;&#24378;&#25209;&#20869;&#36127;&#20363;&#36873;&#25321;&#65292;&#25670;&#33073;&#38543;&#26426;&#37319;&#26679;&#21644;&#31561;&#25928;&#29992;&#20551;&#35774;&#65292;&#38477;&#20302;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#24102;&#26469;&#30340;&#22122;&#22768;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#24494;&#35843;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16829</link><description>&lt;p&gt;
GISTEmbed&#65306;&#25991;&#26412;&#23884;&#20837;&#24494;&#35843;&#20013;&#24341;&#23548;&#26679;&#26412;&#20869;&#35757;&#32451;&#36127;&#20363;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16829
&lt;/p&gt;
&lt;p&gt;
GISTEmbed&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#22686;&#24378;&#25209;&#20869;&#36127;&#20363;&#36873;&#25321;&#65292;&#25670;&#33073;&#38543;&#26426;&#37319;&#26679;&#21644;&#31561;&#25928;&#29992;&#20551;&#35774;&#65292;&#38477;&#20302;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#24102;&#26469;&#30340;&#22122;&#22768;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#24494;&#35843;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#27169;&#22411;&#23545;&#20110;&#35821;&#20041;&#25628;&#32034;&#12289;&#20010;&#24615;&#21270;&#25512;&#33616;&#20197;&#21450;&#29983;&#25104;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#31561;AI&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#25968;&#25454;&#25972;&#29702;&#30340;&#26377;&#38480;&#21487;&#25193;&#23637;&#24615;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#23436;&#25972;&#24615;&#12290;&#20256;&#32479;&#30340;&#26080;&#30417;&#30563;&#19977;&#20803;&#32452;&#25366;&#25496;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#20110;&#23884;&#20837;&#27169;&#22411;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#19981;&#24910;&#24341;&#20837;&#20559;&#35265;&#21644;&#22122;&#22768;&#65292;&#20174;&#32780;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GISTEmbed&#65292;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#22312;&#23545;&#27604;&#35757;&#32451;&#26399;&#38388;&#22686;&#24378;&#25209;&#20869;&#36127;&#20363;&#36873;&#25321;&#30340;&#26032;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#25670;&#33073;&#20102;&#23545;&#20110;&#38543;&#26426;&#25277;&#26679;&#21644;&#25209;&#36127;&#20363;&#31561;&#25928;&#29992;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#30001;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#24341;&#36215;&#30340;&#22122;&#22768;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24494;&#35843;&#25928;&#26524;&#12290;&#36890;&#36807;&#19982; Massive Text Embedding Benchmark (MTEB) &#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;GISTEmbed &#23637;&#31034;&#20102;&#19968;&#33268;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16829v1 Announce Type: cross  Abstract: Embedding models are integral to AI applications like semantic search, personalized recommendations, and retrieval augmented generation for LLMs, necessitating high-quality training data. However, the limited scalability of manual data curation prompts the need for automated methods to ensure data integrity. Traditional unsupervised triplet mining automates training data generation, crucial for embedding model training, yet inadvertently injects biases and noise, thereby degrading model performance. Addressing this, we introduce GISTEmbed, a novel strategy that enhances in-batch negative selection during contrastive training through a guide model. This approach departs from reliance on random sampling and equal utility assumption of batch negatives, significantly reducing noise from data quality issues and improving model fine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB), GISTEmbed showcases consistent perfor
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20294;&#22914;&#20309;&#20248;&#21270;&#36873;&#25321;&#25968;&#25454;&#20197;&#38477;&#20302;&#30899;&#36275;&#36857;&#21644;&#36130;&#21153;&#25104;&#26412;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16827</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Data Selection for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16827
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20294;&#22914;&#20309;&#20248;&#21270;&#36873;&#25321;&#25968;&#25454;&#20197;&#38477;&#20302;&#30899;&#36275;&#36857;&#21644;&#36130;&#21153;&#25104;&#26412;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#30340;&#19968;&#20010;&#20027;&#35201;&#22240;&#32032;&#26159;&#21033;&#29992;&#24040;&#22823;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65288;&#25110;&#19981;&#21487;&#34892;&#65289;&#65292;&#22240;&#20026;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#36136;&#37327;&#21487;&#33021;&#26377;&#25152;&#19981;&#21516;&#12290;&#25968;&#25454;&#36807;&#28388;&#20063;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#37327;&#26469;&#38477;&#20302;&#35757;&#32451;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#21644;&#36130;&#21153;&#25104;&#26412;&#12290;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#26088;&#22312;&#30830;&#23450;&#35201;&#21253;&#25324;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#21738;&#20123;&#20505;&#36873;&#25968;&#25454;&#28857;&#65292;&#20197;&#21450;&#22914;&#20309;&#20174;&#25152;&#36873;&#25968;&#25454;&#28857;&#20013;&#36866;&#24403;&#37319;&#26679;&#12290;&#25913;&#36827;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#30340;&#21069;&#26223;&#24050;&#32463;&#23548;&#33268;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#37327;&#36805;&#36895;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#20027;&#35201;&#21463;&#23454;&#35777;&#35777;&#25454;&#39537;&#21160;&#65292;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#25104;&#26412;&#26114;&#36149;&#65292;&#24456;&#23569;&#26377;&#32452;&#32455;&#25317;&#26377;&#36164;&#28304;&#36827;&#34892;&#24191;&#27867;&#30340;&#25968;&#25454;&#36873;&#25321;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#26377;&#25928;&#25968;&#25454;&#36873;&#25321;&#30340;&#30693;&#35782;&#21487;&#33021;&#22823;&#22810;&#23616;&#38480;&#20110;&#22823;&#22411;&#25216;&#26415;&#20844;&#21496;&#25110;&#30740;&#31350;&#26426;&#26500;&#20869;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16827v1 Announce Type: new  Abstract: A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required.   Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data se
&lt;/p&gt;</description></item><item><title>&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32479;&#19968;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#65292;&#25552;&#20986;&#26032;&#39062;&#30340;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#33410;&#28857;&#21644;&#36793;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21160;&#21327;&#20316;&#21644;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.16823</link><description>&lt;p&gt;
&#20316;&#20026;&#21487;&#20248;&#21270;&#22270;&#30340;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Language Agents as Optimizable Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16823
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32479;&#19968;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#65292;&#25552;&#20986;&#26032;&#39062;&#30340;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#33410;&#28857;&#21644;&#36793;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21160;&#21327;&#20316;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#20154;&#31867;&#35774;&#35745;&#30340;&#25552;&#21319;&#25216;&#26415;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#39064;&#27714;&#35299;&#22120;&#65292;&#20135;&#29983;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#20195;&#30721;&#24211;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;LLM&#20195;&#29702;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#26469;&#32479;&#19968;&#36825;&#20123;&#26041;&#27861;&#12290;&#33410;&#28857;&#23454;&#29616;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#25110;&#26597;&#35810;LLMs&#30340;&#21151;&#33021;&#65292;&#24182;&#19988;&#36793;&#25551;&#36848;&#25805;&#20316;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#22270;&#24418;&#21487;&#20197;&#36882;&#24402;&#22320;&#32452;&#21512;&#25104;&#20195;&#34920;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#21327;&#20316;&#23618;&#27425;&#30340;&#26356;&#22823;&#32452;&#21512;&#22270;&#65288;&#20854;&#20013;&#36793;&#36830;&#25509;&#19981;&#21516;&#20195;&#29702;&#30340;&#25805;&#20316;&#65289;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#65288;1&#65289;&#20248;&#21270;&#33410;&#28857;&#32423;LLM&#25552;&#31034;&#65288;&#33410;&#28857;&#20248;&#21270;&#65289;&#24182;&#65288;2&#65289;&#36890;&#36807;&#25913;&#21464;&#22270;&#36830;&#25509;&#24615;&#26469;&#25913;&#21892;&#20195;&#29702;&#21327;&#35843;&#65288;&#36793;&#32536;&#20248;&#21270;&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#29992;&#20110;&#39640;&#25928;&#24320;&#21457;&#12289;&#38598;&#25104;&#21644;&#33258;&#21160;&#25913;&#36827;&#21508;&#31181;LLM&#20195;&#29702;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/metauto-ai/gptswarm&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16823v1 Announce Type: cross  Abstract: Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.
&lt;/p&gt;</description></item><item><title>Rainbow Teaming&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#25918;&#24335;&#25628;&#32034;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#39046;&#22495;&#30340;&#27169;&#22411;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.16822</link><description>&lt;p&gt;
&#24425;&#34425;&#22242;&#38431;&#65306;&#22810;&#26679;&#21270;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16822
&lt;/p&gt;
&lt;p&gt;
Rainbow Teaming&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#25918;&#24335;&#25628;&#32034;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#39046;&#22495;&#30340;&#27169;&#22411;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#29702;&#35299;&#21644;&#22686;&#24378;&#23427;&#20204;&#23545;&#29992;&#25143;&#36755;&#20837;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35782;&#21035;&#25932;&#23545;&#25552;&#31034;&#30340;&#26041;&#27861;&#24448;&#24448;&#19987;&#27880;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#25110;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24425;&#34425;&#22242;&#38431;&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#21270;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#24425;&#34425;&#22242;&#38431;&#23558;&#23545;&#25239;&#24615;&#25552;&#31034;&#29983;&#25104;&#35270;&#20026;&#19968;&#20010;&#36136;&#37327; - &#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24320;&#25918;&#24335;&#25628;&#32034;&#26469;&#29983;&#25104;&#26082;&#26377;&#25928;&#21448;&#22810;&#26679;&#30340;&#25552;&#31034;&#12290;&#23427;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#22312;&#24191;&#27867;&#39046;&#22495;&#20869;&#30340;&#33030;&#24369;&#24615;&#65292;&#21253;&#25324;&#26412;&#25991;&#20013;&#30340;&#23433;&#20840;&#24615;&#12289;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23545;&#30001;&#24425;&#34425;&#22242;&#38431;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;LLMs&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#25439;&#23475;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16822v1 Announce Type: new  Abstract: As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to user inputs is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem, and uses open-ended search to generate prompts that are both effective and diverse. It can uncover a model's vulnerabilities across a broad range of domains including, in this paper, safety, question answering, and cybersecurity. We also demonstrate that fine-tuning on synthetic data generated by Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting their general capabilities 
&lt;/p&gt;</description></item><item><title>Nemotron-4 15B&#26159;&#19968;&#20010;150&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#35821;&#35328;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20854;&#20182;&#35268;&#27169;&#30456;&#20284;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.16819</link><description>&lt;p&gt;
Nemotron-4 15B&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Nemotron-4 15B Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16819
&lt;/p&gt;
&lt;p&gt;
Nemotron-4 15B&#26159;&#19968;&#20010;150&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#35821;&#35328;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20854;&#20182;&#35268;&#27169;&#30456;&#20284;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Nemotron-4 15B&#65292;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;150&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;8000&#19975;&#20159;&#20010;&#25991;&#26412;&#26631;&#35760;&#12290;Nemotron-4 15B&#22312;&#33521;&#35821;&#12289;&#22810;&#35821;&#35328;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65306;&#22312;7&#20010;&#19979;&#28216;&#35780;&#20272;&#39046;&#22495;&#20013;&#65292;&#23427;&#22312;4&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#20854;&#20313;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#35268;&#27169;&#30456;&#20284;&#30340;&#24320;&#25918;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Nemotron-4 15B&#23637;&#29616;&#20986;&#20102;&#25152;&#26377;&#35268;&#27169;&#30456;&#20284;&#27169;&#22411;&#20013;&#26368;&#24378;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#20248;&#20110;&#22235;&#20493;&#20197;&#19978;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16819v1 Announce Type: new  Abstract: We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.
&lt;/p&gt;</description></item><item><title>Gisting&#26041;&#27861;&#21487;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#23558;&#20449;&#24687;&#21387;&#32553;&#20026;&#26356;&#23569;&#30340;&#26631;&#35760;&#34920;&#31034;&#65292;&#26500;&#24314;&#30340;HyperLlama&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#20449;&#24687;&#20174;&#23569;&#37327;&#31034;&#20363;&#21387;&#32553;&#25104;&#36719;&#21069;&#32512;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#21069;&#32512;&#24494;&#35843;&#25552;&#20379;&#26356;&#22909;&#30340;&#21021;&#22987;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.16817</link><description>&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;Gisting&#30340;HyperTuning&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Effectiveness of HyperTuning via Gisting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16817
&lt;/p&gt;
&lt;p&gt;
Gisting&#26041;&#27861;&#21487;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#23558;&#20449;&#24687;&#21387;&#32553;&#20026;&#26356;&#23569;&#30340;&#26631;&#35760;&#34920;&#31034;&#65292;&#26500;&#24314;&#30340;HyperLlama&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#20449;&#24687;&#20174;&#23569;&#37327;&#31034;&#20363;&#21387;&#32553;&#25104;&#36719;&#21069;&#32512;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#21069;&#32512;&#24494;&#35843;&#25552;&#20379;&#26356;&#22909;&#30340;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gisting&#65288;Mu&#31561;&#65292;2023&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#23558;&#20449;&#24687;&#21387;&#32553;&#20026;&#26356;&#23569;&#30340;&#26631;&#35760;&#34920;&#31034;&#65292;&#20854;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;&#27880;&#24847;&#21147;&#33945;&#29256;&#65292;&#24182;&#21487;&#20316;&#20026;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#36229;&#32593;&#32476;&#30340;&#32463;&#27982;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HyperLlama&#65292;&#36825;&#26159;&#19968;&#32452;&#22522;&#20110;Llama-2&#27169;&#22411;&#26500;&#24314;&#30340;Gisting&#22411;&#36229;&#32593;&#32476;&#65292;&#23427;&#26681;&#25454;&#23569;&#37327;&#36755;&#20837;&#29983;&#25104;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#36719;&#21069;&#32512;&#12290;&#22312;P3&#12289;Super-NaturalInstructions&#21644;Symbol Tuning&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;HyperLlama&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#20449;&#24687;&#20174;&#23569;&#37327;&#31034;&#20363;&#21387;&#32553;&#25104;&#36719;&#21069;&#32512;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#20363;&#19978;&#30340;&#20840;&#27880;&#24847;&#21147;&#19979;&#30340;&#34920;&#29616;&#20173;&#19981;&#22914;&#22810;&#20219;&#21153;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;HyperLlama&#29983;&#25104;&#30340;&#36719;&#21069;&#32512;&#21487;&#29992;&#20316;&#36827;&#19968;&#27493;&#21069;&#32512;&#24494;&#35843;&#30340;&#26356;&#22909;&#21021;&#22987;&#21270;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#22522;&#20110;Gisting&#30340;&#36229;&#32593;&#32476;&#26159;&#32463;&#27982;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#65292;&#20294;&#22312;&#23454;&#35777;&#34920;&#29616;&#19978;&#26377;&#19968;&#23450;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16817v1 Announce Type: new  Abstract: Gisting (Mu et al., 2023) is a simple method for training models to compress information into fewer token representations using a modified attention mask, and can serve as an economical approach to training Transformer-based hypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks built on Llama-2 models that generates task-specific soft prefixes based on few-shot inputs. In experiments across P3, Super-NaturalInstructions and Symbol Tuning datasets, we show that HyperLlama models can effectively compress information from few-shot examples into soft prefixes. However, they still underperform multi-task fine-tuned language models with full attention over few-shot in-context examples. We also show that HyperLlama-generated soft prefixes can serve as better initializations for further prefix tuning. Overall, Gisting-based hypernetworks are economical and easy to implement, but have mixed empirical performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#32959;&#30244;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25552;&#20379;&#32959;&#30244;&#30456;&#20851;&#24314;&#35758;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16810</link><description>&lt;p&gt;
OncoGPT: &#19968;&#20010;&#38024;&#23545;&#32959;&#30244;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#30340;&#21307;&#23398;&#23545;&#35805;&#27169;&#22411;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Meta-AI (LLaMA)
&lt;/p&gt;
&lt;p&gt;
OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#32959;&#30244;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25552;&#20379;&#32959;&#30244;&#30456;&#20851;&#24314;&#35758;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#19968;&#24180;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;OpenAI&#24320;&#21457;&#30340;ChatGPT&#31561;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#19987;&#38376;&#22788;&#29702;&#32959;&#30244;&#30456;&#20851;&#26597;&#35810;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#24320;&#21457;&#19968;&#20010;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#25552;&#20379;&#19982;&#32959;&#30244;&#30456;&#20851;&#24314;&#35758;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22312;&#32447;&#38382;&#39064;-&#31572;&#26696;&#20114;&#21160;&#25968;&#25454;&#25910;&#38598;&#65292;&#22260;&#32469;&#32959;&#30244;&#65292;&#26469;&#28304;&#20110;&#20540;&#24471;&#20449;&#36182;&#30340;&#21307;&#29983;-&#24739;&#32773;&#24179;&#21488;&#12290;&#32463;&#36807;&#25968;&#25454;&#28165;&#29702;&#21644;&#21311;&#21517;&#21270;&#22788;&#29702;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;180K+&#32959;&#30244;&#30456;&#20851;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#23545;&#35805;&#34987;&#20998;&#31867;&#65292;&#24182;&#30001;&#39046;&#22495;&#19987;&#23478;&#21644;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#20102;&#32454;&#33268;&#23457;&#26597;&#65292;&#20197;&#30830;&#20445;&#20934;&#30830;&#24615;&#12290;&#21033;&#29992;LLaMA&#27169;&#22411;&#21644;&#20854;&#20182;&#36873;&#23450;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36845;&#20195;&#30340;fine-tu
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16810v1 Announce Type: new  Abstract: In the past year, there has been a growing trend in applying Large Language Models (LLMs) to the field of medicine, particularly with the advent of advanced language models such as ChatGPT developed by OpenAI. However, there is limited research on LLMs specifically addressing oncology-related queries. The primary aim of this research was to develop a specialized language model that demonstrates improved accuracy in providing advice related to oncology. We performed an extensive data collection of online question-answer interactions centered around oncology, sourced from reputable doctor-patient platforms. Following data cleaning and anonymization, a dataset comprising over 180K+ oncology-related conversations was established. The conversations were categorized and meticulously reviewed by field specialists and clinicians to ensure precision. Employing the LLaMA model and other selected open-source datasets, we conducted iterative fine-tu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#28151;&#20081;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#23558;LMs&#23545;&#40784;&#21040;&#26368;&#36817;&#26102;&#38388;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.16797</link><description>&lt;p&gt;
&#35774;&#23450;&#26102;&#38388;&#65306;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Set the Clock: Temporal Alignment of Pretrained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#28151;&#20081;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#23558;LMs&#23545;&#40784;&#21040;&#26368;&#36817;&#26102;&#38388;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#26469;&#33258;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#32593;&#32476;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#24120;&#27809;&#26377;&#20219;&#20309;&#26126;&#30830;&#30340;&#26102;&#38388;&#22522;&#30784;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;LMs&#30340;&#26102;&#38388;&#28151;&#20081;&#65292;&#24182;&#25506;&#35752;&#20102;&#23558;&#23427;&#20204;&#30340;&#20869;&#37096;&#30693;&#35782;&#23545;&#40784;&#21040;&#30446;&#26631;&#26102;&#38388;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#26102;&#38388;&#23545;&#40784;&#8221;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#33258;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;20K&#20010;&#26102;&#24577;&#38382;&#39064;&#21450;&#20854;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20174;2000&#24180;&#21040;2023&#24180;&#30340;&#27599;&#19968;&#24180;&#12290;&#26681;&#25454;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;LMs&#65288;&#20363;&#22914;LLaMa2&#65289;&#65292;&#23613;&#31649;&#26377;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#65288;&#20363;&#22914;2022&#24180;&#65289;&#65292;&#22823;&#22810;&#25968;&#20351;&#29992;&#26356;&#26089;&#30340;&#30693;&#35782;&#26469;&#22238;&#31572;&#38382;&#39064;&#65288;&#20363;&#22914;&#22312;2019&#24180;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20174;&#25552;&#31034;&#21040;&#24494;&#35843;&#65292;&#26469;&#23545;&#40784;LMs&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#20351;&#29992;&#26368;&#26032;&#30340;&#30693;&#35782;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#31181;&#23545;&#40784;&#20013;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;LLaMa2&#23545;&#40784;&#21040;2022&#24180;&#21487;&#20197;&#23558;&#20854;&#24615;&#33021;&#25552;&#39640;&#39640;&#36798;62%
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16797v1 Announce Type: new  Abstract: Language models (LMs) are trained on web text originating from many points in time and, in general, without any explicit temporal grounding. This work investigates the temporal chaos of pretrained LMs and explores various methods to align their internal knowledge to a target time, which we call "temporal alignment." To do this, we first automatically construct a dataset containing 20K time-sensitive questions and their answers for each year from 2000 to 2023. Based on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2), despite having a recent pretraining cutoff (e.g., 2022), mostly answer questions using earlier knowledge (e.g., in 2019). We then develop several methods, from prompting to finetuning, to align LMs to use their most recent knowledge when answering questions, and investigate various factors in this alignment. Our experiments show that aligning LLaMa2 to the year 2022 can boost its performance by up to 62% 
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#20256;&#32479;&#30340;&#21463;&#38480;&#35780;&#20272;&#33539;&#24335;&#65292;&#25506;&#32034;&#26356;&#30495;&#23454;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16786</link><description>&lt;p&gt;
&#25919;&#27835;&#32599;&#30424;&#25110;&#26059;&#36716;&#31661;&#65311;&#26397;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#26356;&#26377;&#24847;&#20041;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16786
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#20256;&#32479;&#30340;&#21463;&#38480;&#35780;&#20272;&#33539;&#24335;&#65292;&#25506;&#32034;&#26356;&#30495;&#23454;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35768;&#22810;&#24037;&#20316;&#36890;&#36807;&#22810;&#39033;&#36873;&#25321;&#35843;&#26597;&#21644;&#38382;&#21367;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#12290;&#22823;&#22810;&#25968;&#24037;&#20316;&#30340;&#21160;&#26426;&#26159;&#28304;&#20110;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;LLM&#24212;&#29992;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#27880;&#19982;&#24403;&#21069;&#35780;&#20272;&#30340;&#20154;&#20026;&#24615;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65306;&#30495;&#23454;&#29992;&#25143;&#36890;&#24120;&#19981;&#20250;&#21521;LLMs&#25552;&#20986;&#35843;&#26597;&#38382;&#39064;&#12290;&#21463;&#21040;&#36825;&#31181;&#24046;&#24322;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#30446;&#21069;&#23545;LLMs&#20013;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#32422;&#26463;&#35780;&#20272;&#33539;&#24335;&#65292;&#24182;&#25506;&#32034;&#20102;&#26356;&#29616;&#23454;&#30340;&#19981;&#21463;&#38480;&#21046;&#30340;&#35780;&#20272;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#24191;&#21463;&#27426;&#36814;&#30340;&#25919;&#27835;&#32599;&#30424;&#27979;&#35797;&#65288;PCT&#65289;&#12290;&#22312;&#19968;&#20010;&#31995;&#32479;&#24615;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#20808;&#21069;&#20351;&#29992;PCT&#30340;&#24037;&#20316;&#37117;&#24378;&#21046;&#27169;&#22411;&#36981;&#23432;PCT&#30340;&#22810;&#39033;&#36873;&#25321;&#26684;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#19981;&#34987;&#24378;&#21046;&#26102;&#65292;&#27169;&#22411;&#32473;&#20986;&#30340;&#31572;&#26696;&#23454;&#36136;&#19978;&#26159;&#19981;&#21516;&#30340;&#65307;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16786v1 Announce Type: new  Abstract: Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in LLMs and explore more realistic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers cha
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#31574;&#30053;&#23637;&#24320;&#20840;&#38754;&#35780;&#20272;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.16775</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#31574;&#30053;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Quantization Strategies for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16775
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#31574;&#30053;&#23637;&#24320;&#20840;&#38754;&#35780;&#20272;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#36890;&#24120;&#20250;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20250;&#22686;&#21152;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#21464;&#24471;&#22256;&#38590;&#12290;&#37327;&#21270;&#25216;&#26415;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#26435;&#37325;&#25110;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#65292;&#24182;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#65292;&#24050;&#32463;&#22240;LLMs&#30340;&#20852;&#36215;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#37327;&#21270;&#30740;&#31350;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#65292;&#37327;&#21270;&#23545;&#35843;&#25972;&#36807;&#25351;&#20196;&#30340;LLMs&#30340;&#24433;&#21709;&#20197;&#21450;&#37327;&#21270;LLMs&#30340;&#22256;&#24785;&#24230;&#19982;&#22522;&#20934;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#23578;&#19981;&#26126;&#30830;&#12290;&#23545;&#37327;&#21270;LLMs&#30340;&#35780;&#20272;&#36890;&#24120;&#20165;&#38480;&#20110;&#35821;&#35328;&#24314;&#27169;&#21644;&#23569;&#25968;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#22312;&#20854;&#20182;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#65288;1&#65289;&#30693;&#35782;&#21644;&#23481;&#37327;&#65292;&#65288;2&#65289;&#23545;&#40784;&#24615;&#21644;&#65288;3&#65289;&#25928;&#29575;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16775v1 Announce Type: new  Abstract: Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \&amp; capacity, (2) alignment, and (3) efficiency, and conduct extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#25345;&#32493;&#25991;&#26723;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25506;&#32034;&#26816;&#32034;&#27169;&#22411;&#26377;&#25928;&#22788;&#29702;&#21160;&#24577;&#26816;&#32034;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16767</link><description>&lt;p&gt;
CorpusBrain++: &#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#30340;&#32487;&#32493;&#29983;&#25104;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#25345;&#32493;&#25991;&#26723;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25506;&#32034;&#26816;&#32034;&#27169;&#22411;&#26377;&#25928;&#22788;&#29702;&#21160;&#24577;&#26816;&#32034;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#20174;&#21487;&#20449;&#30340;&#35821;&#26009;&#24211;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#20197;&#29983;&#25104;&#29305;&#23450;&#31572;&#26696;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;CorpusBrain&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;&#26816;&#32034;&#24615;&#33021;&#26368;&#20248;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20851;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;CorpusBrain&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#38598;&#20013;&#22312;&#38745;&#24577;&#25991;&#26723;&#38598;&#19978;&#65292;&#24573;&#35270;&#20102;&#29616;&#23454;&#22330;&#26223;&#30340;&#21160;&#24577;&#24615;&#36136;&#65292;&#20854;&#20013;&#26032;&#25991;&#26723;&#25345;&#32493;&#22320;&#34987;&#32435;&#20837;&#28304;&#35821;&#26009;&#24211;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25506;&#32034;&#26816;&#32034;&#27169;&#22411;&#26377;&#25928;&#22788;&#29702;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#21160;&#24577;&#26816;&#32034;&#22330;&#26223;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#30693;&#35782;&#23494;&#38598;&#22411;&#35821;&#35328;&#20219;&#21153;&#30340;&#25345;&#32493;&#25991;&#26723;&#23398;&#20064;&#65288;CDL&#65289;&#20219;&#21153;&#65292;&#24182;&#22522;&#20110;&#21407;&#22987;KILT&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;KILT++&#30340;&#26032;&#22411;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16767v1 Announce Type: cross  Abstract: Knowledge-intensive language tasks (KILTs) typically require retrieving relevant documents from trustworthy corpora, e.g., Wikipedia, to produce specific answers. Very recently, a pre-trained generative retrieval model for KILTs, named CorpusBrain, was proposed and reached new state-of-the-art retrieval performance. However, most existing research on KILTs, including CorpusBrain, has predominantly focused on a static document collection, overlooking the dynamic nature of real-world scenarios, where new documents are continuously being incorporated into the source corpus. To address this gap, it is crucial to explore the capability of retrieval models to effectively handle the dynamic retrieval scenario inherent in KILTs.   In this work, we first introduce the continual document learning (CDL) task for KILTs and build a novel benchmark dataset named KILT++ based on the original KILT dataset for evaluation. Then, we conduct a comprehensi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#20013;&#30340;&#20215;&#20540;&#20559;&#22909;&#20272;&#35745;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#21442;&#19982;&#32773;&#20114;&#21160;&#35299;&#20915;&#20102;&#36873;&#25321;&#19982;&#21160;&#26426;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#24182;&#37325;&#28857;&#27604;&#36739;&#20102;&#20174;&#21160;&#26426;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#19982;&#20165;&#20174;&#36873;&#25321;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.16751</link><description>&lt;p&gt;
&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#20013;&#30340;&#20215;&#20540;&#20559;&#22909;&#20272;&#35745;&#21644;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#20013;&#30340;&#20215;&#20540;&#20559;&#22909;&#20272;&#35745;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#21442;&#19982;&#32773;&#20114;&#21160;&#35299;&#20915;&#20102;&#36873;&#25321;&#19982;&#21160;&#26426;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#24182;&#37325;&#28857;&#27604;&#36739;&#20102;&#20174;&#21160;&#26426;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#19982;&#20165;&#20174;&#36873;&#25321;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#20013;&#29702;&#35299;&#20844;&#27665;&#30340;&#20215;&#20540;&#35266;&#23545;&#20110;&#20197;&#20844;&#27665;&#20026;&#20013;&#24515;&#30340;&#25919;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#20010;&#28151;&#21512;&#21442;&#19982;&#24335;&#31995;&#32479;&#65292;&#22312;&#36825;&#20010;&#31995;&#32479;&#20013;&#65292;&#21442;&#19982;&#32773;&#20570;&#20986;&#36873;&#25321;&#24182;&#25552;&#20379;&#36873;&#25321;&#30340;&#21160;&#26426;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#36807;&#19982;&#20182;&#20204;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#20215;&#20540;&#20559;&#22909;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#21442;&#19982;&#32773;&#30340;&#36873;&#25321;&#21644;&#21160;&#26426;&#20043;&#38388;&#26816;&#27979;&#21040;&#20914;&#31361;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#20272;&#35745;&#20215;&#20540;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#36890;&#36807;&#19982;&#21442;&#19982;&#32773;&#20114;&#21160;&#26469;&#35299;&#20915;&#26816;&#27979;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23558;&#8220;&#29645;&#35270;&#26159;&#32463;&#36807;&#28145;&#24605;&#29087;&#34385;&#30340;&#26377;&#24847;&#20041;&#34892;&#20026;&#8221;&#36825;&#19968;&#21746;&#23398;&#31435;&#22330;&#25805;&#20316;&#21270;&#12290;&#20063;&#23601;&#26159;&#22914;&#26524;&#21442;&#19982;&#32773;&#30340;&#36873;&#25321;&#26159;&#22522;&#20110;&#23545;&#20215;&#20540;&#20559;&#22909;&#30340;&#28145;&#24605;&#29087;&#34385;&#65292;&#37027;&#20040;&#21487;&#20197;&#22312;&#21442;&#19982;&#32773;&#20026;&#36873;&#25321;&#25552;&#20379;&#30340;&#21160;&#26426;&#20013;&#35266;&#23519;&#21040;&#20215;&#20540;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#20248;&#20808;&#32771;&#34385;&#20174;&#21160;&#26426;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#32780;&#19981;&#26159;&#20165;&#20174;&#36873;&#25321;&#20013;&#20272;&#35745;&#30340;&#20215;&#20540;&#30340;&#20215;&#20540;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16751v1 Announce Type: cross  Abstract: Understanding citizens' values in participatory systems is crucial for citizen-centric policy-making. We envision a hybrid participatory system where participants make choices and provide motivations for those choices, and AI agents estimate their value preferences by interacting with them. We focus on situations where a conflict is detected between participants' choices and motivations, and propose methods for estimating value preferences while addressing detected inconsistencies by interacting with the participants. We operationalize the philosophical stance that "valuing is deliberatively consequential." That is, if a participant's choice is based on a deliberation of value preferences, the value preferences can be observed in the motivation the participant provides for the choice. Thus, we propose and compare value estimation methods that prioritize the values estimated from motivations over the values estimated from choices alone.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#26631;&#20934;&#25968;&#25454;&#38598;DREsS&#65292;&#29992;&#20110;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#65292;&#22312;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#22351;&#30340;&#20316;&#25991;&#22686;&#24378;&#31574;&#30053;CASE&#21518;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#32447;&#32467;&#26524;&#25552;&#39640;&#20102;45.44&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.16733</link><description>&lt;p&gt;
DREsS: &#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#20889;&#20316;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#26631;&#20934;&#25968;&#25454;&#38598;DREsS&#65292;&#29992;&#20110;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#65292;&#22312;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#22351;&#30340;&#20316;&#25991;&#22686;&#24378;&#31574;&#30053;CASE&#21518;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#32447;&#32467;&#26524;&#25552;&#39640;&#20102;45.44&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#65288;AES&#65289;&#26159;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#20889;&#20316;&#25945;&#32946;&#20013;&#19968;&#31181;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#20026;&#23398;&#29983;&#21644;&#25945;&#24072;&#25552;&#20379;&#23454;&#26102;&#20316;&#25991;&#35780;&#20998;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;AES&#27169;&#22411;&#26159;&#22312;&#19982;EFL&#20889;&#20316;&#25945;&#32946;&#23454;&#38469;&#22330;&#26223;&#19981;&#30456;&#20851;&#30340;&#20316;&#25991;&#21644;&#20998;&#25968;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#32780;&#25552;&#20379;&#21333;&#19968;&#30340;&#25972;&#20307;&#35780;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;DREsS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#30340;&#22823;&#22411;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;DREsS&#21253;&#25324;&#19977;&#20010;&#23376;&#25968;&#25454;&#38598;&#65306;DREsS_New&#65292;DREsS_Std.&#21644;DREsS_CASE&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;DREsS_New&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;EFL&#26412;&#31185;&#29983;&#25776;&#20889;&#24182;&#30001;&#33521;&#35821;&#25945;&#32946;&#19987;&#23478;&#35780;&#20998;&#30340;&#30495;&#23454;&#35838;&#22530;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#29616;&#26377;&#30340;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#20316;&#25991;&#35780;&#20998;&#25968;&#25454;&#38598;&#26631;&#20934;&#21270;&#20026;DREsS_Std&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CASE&#30340;&#22522;&#20110;&#30772;&#22351;&#30340;&#20316;&#25991;&#22686;&#24378;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;20K&#20010;DREsS_CASE&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#22522;&#32447;&#32467;&#26524;&#25552;&#39640;&#20102;45.44&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16733v1 Announce Type: new  Abstract: Automated essay scoring (AES) is a useful tool in English as a Foreign Language (EFL) writing education, offering real-time essay scores for students and instructors. However, previous AES models were trained on essays and scores irrelevant to the practical scenarios of EFL writing education and usually provided a single holistic score due to the lack of appropriate datasets. In this paper, we release DREsS, a large-scale, standard dataset for rubric-based automated essay scoring. DREsS comprises three sub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with 1.7K essays authored by EFL undergraduate students and scored by English education experts. We also standardize existing rubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation strategy for essays, which generates 20K synthetic samples of DREsS_CASE and improves the baseline results by 45.44%. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#21152;&#23494;&#31574;&#30053;&#30340;&#26032;&#22411;&#36234;&#29425;&#26694;&#26550;CodeChameleon&#65292;&#36890;&#36807;&#37325;&#22609;&#20219;&#21153;&#26684;&#24335;&#21644;&#23884;&#20837;&#35299;&#23494;&#21151;&#33021;&#65292;&#25104;&#21151;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16717</link><description>&lt;p&gt;
CodeChameleon: &#38024;&#23545;&#36234;&#29425;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#21152;&#23494;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#21152;&#23494;&#31574;&#30053;&#30340;&#26032;&#22411;&#36234;&#29425;&#26694;&#26550;CodeChameleon&#65292;&#36890;&#36807;&#37325;&#22609;&#20219;&#21153;&#26684;&#24335;&#21644;&#23884;&#20837;&#35299;&#23494;&#21151;&#33021;&#65292;&#25104;&#21151;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Adversarial misuse, particularly through `jailbreaking' that circumvents a model's safety and ethical protocols, poses a significant challenge for Large Language Models (LLMs). This paper delves into the mechanisms behind such successful attacks, introducing a hypothesis for the safety mechanism of aligned LLMs: intent security recognition followed by response generation. Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak framework based on personalized encryption tactics. To elude the intent security recognition phase, we reformulate tasks into a code completion format, enabling users to encrypt queries using personalized encryption functions. To guarantee response generation functionality, we embed a decryption function within the instructions, which allows the LLM to decrypt and execute the encrypted queries successfully. We conduct extensive experiments on 7 LLMs, achieving state-of-the-art average Attack Success
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16717v1 Announce Type: new  Abstract: Adversarial misuse, particularly through `jailbreaking' that circumvents a model's safety and ethical protocols, poses a significant challenge for Large Language Models (LLMs). This paper delves into the mechanisms behind such successful attacks, introducing a hypothesis for the safety mechanism of aligned LLMs: intent security recognition followed by response generation. Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak framework based on personalized encryption tactics. To elude the intent security recognition phase, we reformulate tasks into a code completion format, enabling users to encrypt queries using personalized encryption functions. To guarantee response generation functionality, we embed a decryption function within the instructions, which allows the LLM to decrypt and execute the encrypted queries successfully. We conduct extensive experiments on 7 LLMs, achieving state-of-the-art average Attack Succes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23481;&#38169;&#24615;&#37327;&#23376;&#35745;&#31639;&#30340;&#35270;&#35282;&#19979;Transformer&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#26500;&#24314;Transformer&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.16714</link><description>&lt;p&gt;
&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#26159;Transformer&#26550;&#26500;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Quantum linear algebra is all you need for Transformer architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23481;&#38169;&#24615;&#37327;&#23376;&#35745;&#31639;&#30340;&#35270;&#35282;&#19979;Transformer&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#26500;&#24314;Transformer&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#21019;&#20316;&#12290;&#26412;&#25991;&#36890;&#36807;&#23481;&#38169;&#24615;&#37327;&#23376;&#35745;&#31639;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;Transformer&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20934;&#22791;self-attention&#30697;&#38453;&#30340;&#22359;&#32534;&#30721;&#65292;&#24182;&#32467;&#21512;&#37327;&#23376;&#23376;&#31243;&#24207;&#26500;&#24314;&#20102;Transformer&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16714v1 Announce Type: cross  Abstract: Generative machine learning methods such as large-language models are revolutionizing the creation of text and images. While these models are powerful they also harness a large amount of computational resources. The transformer is a key component in large language models that aims to generate a suitable completion of a given partial sequence. In this work, we investigate transformer architectures under the lens of fault-tolerant quantum computing. The input model is one where pre-trained weight matrices are given as block encodings to construct the query, key, and value matrices for the transformer. As a first step, we show how to prepare a block encoding of the self-attention matrix, with a row-wise application of the softmax function using the Hadamard product. In addition, we combine quantum subroutines to construct important building blocks in the transformer, the residual connection, layer normalization, and the feed-forward neura
&lt;/p&gt;</description></item><item><title>SelectIT&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#30340;&#33021;&#21147;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#30340;&#39640;&#25928;&#36873;&#25321;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#36827;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16705</link><description>&lt;p&gt;
SelectIT: &#36890;&#36807;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#33258;&#25105;&#21453;&#24605;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#25351;&#23548;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16705
&lt;/p&gt;
&lt;p&gt;
SelectIT&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#30340;&#33021;&#21147;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#30340;&#39640;&#25928;&#36873;&#25321;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#36827;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#65288;IT&#65289;&#23545;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#36866;&#24212;&#20154;&#31867;&#20013;&#24515;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#31934;&#24515;&#36873;&#25321;&#19968;&#23567;&#37096;&#20998;&#39640;&#36136;&#37327;&#30340;IT&#25968;&#25454;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#24120;&#35265;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#27169;&#22411;&#25110;&#25968;&#25454;&#38598;&#65292;&#36825;&#22686;&#21152;&#20102;&#25104;&#26412;&#24182;&#38480;&#21046;&#20102;&#24191;&#27867;&#37319;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SelectIT&#65292;&#23427;&#21033;&#29992;LLM&#26412;&#36523;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26356;&#26377;&#25928;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;IT&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;IT&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#36873;&#25321;&#24615;&#32650;&#39548;&#65288;Selective Alpaca&#65289;&#65292;&#36890;&#36807;&#23558;SelectIT&#24212;&#29992;&#20110;Alpaca-GPT4&#25968;&#25454;&#38598;&#32780;&#21019;&#24314;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36873;&#25321;&#24615;&#32650;&#39548;&#36827;&#34892;IT&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;SelectIT&#30340;&#31283;&#20581;&#24615;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16705v1 Announce Type: new  Abstract: Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Hierarchical Ensemble Construction&#65288;HEC&#65289;&#31639;&#27861;&#23558;&#20256;&#32479;NLP&#27169;&#22411;&#19982;&#36716;&#25442;&#22120;&#27169;&#22411;&#32467;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#38598;&#25104;&#27169;&#22411;&#65292;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16700</link><description>&lt;p&gt;
&#20026;&#24773;&#24863;&#20998;&#26512;&#29983;&#25104;&#26377;&#25928;&#30340;&#38598;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generating Effective Ensembles for Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16700
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Hierarchical Ensemble Construction&#65288;HEC&#65289;&#31639;&#27861;&#23558;&#20256;&#32479;NLP&#27169;&#22411;&#19982;&#36716;&#25442;&#22120;&#27169;&#22411;&#32467;&#21512;&#65292;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#38598;&#25104;&#27169;&#22411;&#65292;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36716;&#25442;&#22120;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#29992;&#20110;SA&#30340;&#26368;&#26032;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292; &#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36825;&#20123;SA&#38598;&#25104;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#22312;&#20110;&#19981;&#20165;&#21253;&#25324;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#36824;&#21253;&#25324;&#20256;&#32479;NLP&#27169;&#22411;&#65292;&#23613;&#31649;&#21518;&#32773;&#30456;&#23545;&#20110;&#36716;&#25442;&#22120;&#27169;&#22411;&#22788;&#20110;&#21155;&#21183;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#30340;&#65292;&#36825;&#38656;&#35201;&#25913;&#21464;&#38598;&#25104;&#27169;&#22411;&#30340;&#26500;&#24314;&#26041;&#24335;&#65292;&#20855;&#20307;&#20381;&#36182;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#23618;&#38598;&#25104;&#26500;&#24314;&#65288;HEC&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#20856;&#22411;SA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;HEC&#32467;&#26500;&#21270;&#30340;&#28151;&#21512;&#27169;&#22411;&#31867;&#22411;&#30340;&#38598;&#25104;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16700v1 Announce Type: new  Abstract: In recent years, transformer models have revolutionized Natural Language Processing (NLP), achieving exceptional results across various tasks, including Sentiment Analysis (SA). As such, current state-of-the-art approaches for SA predominantly rely on transformer models alone, achieving impressive accuracy levels on benchmark datasets. In this paper, we show that the key for further improving the accuracy of such ensembles for SA is to include not only transformers, but also traditional NLP models, despite the inferiority of the latter compared to transformer models. However, as we empirically show, this necessitates a change in how the ensemble is constructed, specifically relying on the Hierarchical Ensemble Construction (HEC) algorithm we present. Our empirical studies across eight canonical SA datasets reveal that ensembles incorporating a mix of model types, structured via HEC, significantly outperform traditional ensembles. Finally
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#24863;&#30693;&#21644;&#21487;&#27867;&#21270;&#30340;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25805;&#20316;&#24037;&#20855;&#26102;&#25552;&#39640;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.16696</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23457;&#24910;&#34892;&#20107;&#65306;&#36808;&#21521;&#20915;&#31574;&#24863;&#30693;&#21644;&#21487;&#27867;&#21270;&#30340;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#24863;&#30693;&#21644;&#21487;&#27867;&#21270;&#30340;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25805;&#20316;&#24037;&#20855;&#26102;&#25552;&#39640;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33719;&#21462;&#26368;&#26032;&#30693;&#35782;&#21644;&#32531;&#35299;&#20135;&#29983;&#24187;&#35273;&#38382;&#39064;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24403;&#21069;&#65292;&#20808;&#36827;&#30340;&#38381;&#28304;LLM&#65288;&#22914;ChatGPT&#65289;&#36890;&#36807;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#23637;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;&#24320;&#28304;LLM&#65288;&#22914;LLaMA&#65289;&#22312;&#25805;&#20316;&#24037;&#20855;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24403;&#21069;&#30340;&#21162;&#21147;&#38598;&#20013;&#20110;&#22522;&#20110;&#27169;&#26495;&#39537;&#21160;&#25110;&#22522;&#20110;&#26631;&#35760;&#35302;&#21457;&#30340;&#24037;&#20855;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#30001;&#20110;&#21463;&#21040;&#38480;&#21046;&#30340;&#24037;&#20855;&#20132;&#20114;&#65292;&#38480;&#21046;&#20102;LLM&#28789;&#27963;&#22320;&#35299;&#20915;&#21508;&#31181;&#29992;&#25143;&#26597;&#35810;&#65292;&#32780;&#21518;&#32773;&#22312;&#20351;&#29992;&#26032;&#24037;&#20855;&#26102;&#38480;&#21046;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#20026;&#24037;&#20855;&#20351;&#29992;&#23398;&#20064;&#22522;&#20110;&#20219;&#21153;&#21644;&#24037;&#20855;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#24863;&#30693;&#21644;&#21487;&#27867;&#21270;&#30340;&#24037;&#20855;&#20351;&#29992;&#26694;&#26550;&#65288;DEER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20855;&#26377;&#22810;&#20010;&#20915;&#31574;&#20998;&#25903;&#30340;&#24037;&#20855;&#20351;&#29992;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16696v1 Announce Type: new  Abstract: Tool-augmented large language models (LLMs) are attracting widespread attention when accessing up-to-date knowledge and alleviating hallucination issues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated surprising tool-usage capabilities through prompting and in-context learning techniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in manipulating tools, current efforts focus on either template-driven or token-triggered tool-usage. However, the former hampers LLMs' flexibility to address diverse user's queries due to constrained tool interactions, while the latter limits the generalizability when engaging with new tools, since tool-usage learning is based on task- and tool-specific datasets. To alleviate these concerns, in this paper, we propose a decision-aware and generalizable tool-usage framework (DEER). Specifically, we first construct the tool-usage samples with multiple decision branches 
&lt;/p&gt;</description></item><item><title>HumanEval-XL &#26159;&#19968;&#20010;&#38754;&#21521;&#36328;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#27867;&#21270;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#24314;&#31435;23&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;12&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#24179;&#21488;&#65292;&#24357;&#34917;&#20102;&#22810;&#35821;&#35328;LLM&#35780;&#20272;&#30340;&#37325;&#35201;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.16694</link><description>&lt;p&gt;
HumanEval-XL&#65306;&#38754;&#21521;&#36328;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#27867;&#21270;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16694
&lt;/p&gt;
&lt;p&gt;
HumanEval-XL &#26159;&#19968;&#20010;&#38754;&#21521;&#36328;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#27867;&#21270;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#24314;&#31435;23&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;12&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#24179;&#21488;&#65292;&#24357;&#34917;&#20102;&#22810;&#35821;&#35328;LLM&#35780;&#20272;&#30340;&#37325;&#35201;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#20195;&#30721;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#33521;&#35821;&#25552;&#31034;&#32763;&#35793;&#20026;&#22810;&#35821;&#35328;&#20195;&#30721;&#65292;&#25110;&#32773;&#20165;&#38480;&#20110;&#38750;&#24120;&#26377;&#38480;&#30340;&#33258;&#28982;&#35821;&#35328;(NLs)&#12290;&#36825;&#20123;&#22522;&#20934;&#24573;&#35270;&#20102;&#24222;&#22823;&#30340;&#20316;&#20026;&#23545;&#27604;&#30340;&#22810;&#35821;&#35328;NL&#21040;&#22810;&#35821;&#35328;&#20195;&#30721;&#30340;&#24191;&#38420;&#39046;&#22495;&#65292;&#23548;&#33268;&#20102;&#23545;&#22810;&#35821;&#35328;LLM&#35780;&#20272;&#30340;&#37325;&#22823;&#31354;&#30333;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HumanEval-XL&#65292;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#12290;HumanEval-XL&#24314;&#31435;&#20102;23&#31181;NL&#21644;12&#31181;&#32534;&#31243;&#35821;&#35328;(PLs)&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21253;&#25324;&#20102;22,080&#20010;&#25552;&#31034;&#30340;&#38598;&#21512;&#65292;&#24179;&#22343;&#26377;8.33&#20010;&#27979;&#35797;&#29992;&#20363;&#12290;&#36890;&#36807;&#30830;&#20445;&#22312;&#22810;&#20010;NL&#21644;PL&#20043;&#38388;&#30340;&#24182;&#34892;&#25968;&#25454;&#65292;HumanEval-XL&#20026;&#22810;&#35821;&#35328;LLMs&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#24179;&#21488;&#65292;&#20801;&#35768;&#35780;&#20272;&#23545;&#19981;&#21516;NL&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16694v1 Announce Type: new  Abstract: Large language models (LLMs) have made significant progress in generating codes from textual prompts. However, existing benchmarks have mainly concentrated on translating English prompts to multilingual codes or have been constrained to very limited natural languages (NLs). These benchmarks have overlooked the vast landscape of massively multilingual NL to multilingual code, leaving a critical gap in the evaluation of multilingual LLMs. In response, we introduce HumanEval-XL, a massively multilingual code generation benchmark specifically crafted to address this deficiency. HumanEval-XL establishes connections between 23 NLs and 12 programming languages (PLs), and comprises of a collection of 22,080 prompts with an average of 8.33 test cases. By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a comprehensive evaluation platform for multilingual LLMs, allowing the assessment of the understanding of different NLs. O
&lt;/p&gt;</description></item><item><title>&#36827;&#19968;&#27493;&#36890;&#36807;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#23545;&#33521;&#25991;&#20020;&#24202;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#20248;&#20110;&#20854;&#20182;&#20004;&#31181;&#35843;&#25972;&#31574;&#30053;&#65292;&#32467;&#26524;&#24378;&#35843;&#20102;&#38271;&#24207;&#21015;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.16689</link><description>&lt;p&gt;
&#23558;&#29983;&#29289;&#21307;&#23398;&#21644;&#20020;&#24202;&#39044;&#35757;&#32451;&#27169;&#22411;&#35843;&#25972;&#21040;&#27861;&#35821;&#38271;&#25991;&#26723;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16689
&lt;/p&gt;
&lt;p&gt;
&#36827;&#19968;&#27493;&#36890;&#36807;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#23545;&#33521;&#25991;&#20020;&#24202;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#20248;&#20110;&#20854;&#20182;&#20004;&#31181;&#35843;&#25972;&#31574;&#30053;&#65292;&#32467;&#26524;&#24378;&#35843;&#20102;&#38271;&#24207;&#21015;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;BERT&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24341;&#20837;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20020;&#24202;NLP&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;512&#20010;&#20196;&#29260;&#30340;&#26377;&#38480;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#22312;&#24212;&#29992;&#20110;&#20020;&#24202;&#35760;&#24405;&#26102;&#20250;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#36866;&#29992;&#20110;&#38271;&#24207;&#21015;&#27169;&#22411;&#30340;&#35843;&#25972;&#31574;&#30053;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#21033;&#29992;&#20102;Longformer&#26550;&#26500;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#28085;&#30422;&#29983;&#29289;&#21307;&#23398;&#21644;&#20020;&#24202;&#39046;&#22495;&#30340;16&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#23545;&#33521;&#25991;&#20020;&#24202;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#20248;&#20110;&#23558;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;BERT&#36716;&#25442;&#20026;Longformer&#26550;&#26500;&#20197;&#21450;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;Longformer&#12290;&#32467;&#26524;&#24378;&#35843;&#20102;&#38271;&#24207;&#21015;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16689v1 Announce Type: new  Abstract: Recently, pretrained language models based on BERT have been introduced for the French biomedical domain. Although these models have achieved state-of-the-art results on biomedical and clinical NLP tasks, they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes. In this paper, we present a comparative study of three adaptation strategies for long-sequence models, leveraging the Longformer architecture. We conducted evaluations of these models on 16 downstream tasks spanning both biomedical and clinical domains. Our findings reveal that further pre-training an English clinical model with French biomedical texts can outperform both converting a French biomedical BERT to the Longformer architecture and pre-training a French biomedical Longformer from scratch. The results underscore that long-sequence French biomedical models improve performance across most downstream tasks 
&lt;/p&gt;</description></item><item><title>StructLM&#31995;&#21015;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#36229;&#36234;&#20102;&#22823;&#22810;&#25968;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2402.16671</link><description>&lt;p&gt;
StructLM: &#26397;&#21521;&#26500;&#24314;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StructLM: Towards Building Generalist Models for Structured Knowledge Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16671
&lt;/p&gt;
&lt;p&gt;
StructLM&#31995;&#21015;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#36229;&#36234;&#20102;&#22823;&#22810;&#25968;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#22312;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65292;&#22914;&#34920;&#26684;&#12289;&#22270;&#24418;&#21644;&#25968;&#25454;&#24211;&#65292;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#30693;&#35782;&#28304;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32431;&#25991;&#26412;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#37322;&#21644;&#21033;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#26174;&#30528;&#19981;&#36275;&#65292;&#20363;&#22914;&#65292;ChatGPT&#24179;&#22343;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;(SoTA)35%&#12290;&#20026;&#22686;&#24378;LLM&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#36830;&#25509;&#65288;SKG&#65289;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;110&#19975;&#20010;&#31034;&#20363;&#30340;&#20840;&#38754;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;Code-LLaMA&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;StructLM&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;7B&#21040;34B&#12290;&#25105;&#20204;&#30340;StructLM&#31995;&#21015;&#22312;18&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#26377;14&#20010;&#36229;&#36234;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;7&#20010;SKG&#20219;&#21153;&#19978;&#30830;&#31435;&#20102;&#26032;&#30340;SoTA&#25104;&#23601;&#12290;&#27492;&#22806;&#65292;StructLM&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16671v1 Announce Type: new  Abstract: Structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalizat
&lt;/p&gt;</description></item><item><title>RepoAgent&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#21160;&#21147;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#31215;&#26497;&#29983;&#25104;&#12289;&#32500;&#25252;&#21644;&#26356;&#26032;&#20195;&#30721;&#25991;&#26723;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#23384;&#20648;&#24211;&#32423;&#25991;&#26723;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16667</link><description>&lt;p&gt;
RepoAgent&#65306;&#19968;&#31181;&#20197;LLM&#20026;&#21160;&#21147;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#23384;&#20648;&#24211;&#32423;&#20195;&#30721;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16667
&lt;/p&gt;
&lt;p&gt;
RepoAgent&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#21160;&#21147;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#31215;&#26497;&#29983;&#25104;&#12289;&#32500;&#25252;&#21644;&#26356;&#26032;&#20195;&#30721;&#25991;&#26723;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#23384;&#20648;&#24211;&#32423;&#25991;&#26723;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16667v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#29983;&#25104;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#23637;&#31034;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#35832;&#22914;&#20195;&#30721;&#29983;&#25104;&#21644;&#35843;&#35797;&#20043;&#31867;&#30340;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#20195;&#30721;&#25991;&#26723;&#29983;&#25104;&#39046;&#22495;&#20013;&#65292;&#23427;&#20204;&#30340;&#21033;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#24320;&#21457;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RepoAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#21160;&#21147;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#31215;&#26497;&#29983;&#25104;&#12289;&#32500;&#25252;&#21644;&#26356;&#26032;&#20195;&#30721;&#25991;&#26723;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#25105;&#20204;&#24050;&#32463;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;RepoAgent&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23384;&#20648;&#24211;&#32423;&#25991;&#26723;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20195;&#30721;&#21644;&#32467;&#26524;&#21487;&#20197;&#20844;&#24320;&#35775;&#38382;&#65306;https://github.com/OpenBMB/RepoAgent&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16667v1 Announce Type: new  Abstract: Generative models have demonstrated considerable potential in software engineering, particularly in tasks such as code generation and debugging. However, their utilization in the domain of code documentation generation remains underexplored. To this end, we introduce RepoAgent, a large language model powered open-source framework aimed at proactively generating, maintaining, and updating code documentation. Through both qualitative and quantitative evaluations, we have validated the effectiveness of our approach, showing that RepoAgent excels in generating high-quality repository-level documentation. The code and results are publicly accessible at https://github.com/OpenBMB/RepoAgent.
&lt;/p&gt;</description></item><item><title>GigaPevt&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#19994;&#21307;&#30103;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;&#65292;&#22312;&#23545;&#35805;&#36136;&#37327;&#21644;&#24230;&#37327;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;1.18\%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16654</link><description>&lt;p&gt;
GigaPevt&#65306;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
GigaPevt: Multimodal Medical Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16654
&lt;/p&gt;
&lt;p&gt;
GigaPevt&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#19994;&#21307;&#30103;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;&#65292;&#22312;&#23545;&#35805;&#36136;&#37327;&#21644;&#24230;&#37327;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;1.18\%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#19968;&#20010;&#26234;&#33021;&#39640;&#25928;&#30340;&#21307;&#30103;&#21161;&#25163;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#12290;&#20027;&#35201;&#38480;&#21046;&#26469;&#33258;&#25968;&#25454;&#27169;&#24577;&#30340;&#31232;&#32570;&#24615;&#65292;&#38477;&#20302;&#20102;&#20840;&#38754;&#30340;&#24739;&#32773;&#24863;&#30693;&#12290;&#26412;&#28436;&#31034;&#35770;&#25991;&#20171;&#32461;&#20102;GigaPevt&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#21151;&#33021;&#21644;&#19987;&#19994;&#21307;&#30103;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23545;&#35805;&#36136;&#37327;&#21644;&#24230;&#37327;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#20351;&#24471;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.18\%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16654v1 Announce Type: cross  Abstract: Building an intelligent and efficient medical assistant is still a challenging AI problem. The major limitation comes from the data modality scarceness, which reduces comprehensive patient perception. This demo paper presents the GigaPevt, the first multimodal medical assistant that combines the dialog capabilities of large language models with specialized medical models. Such an approach shows immediate advantages in dialog quality and metric performance, with a 1.18\% accuracy improvement in the question-answering task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20154;&#31867;&#21644;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#34913;&#37327;ESG&#30456;&#20851;&#24773;&#32490;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.16650</link><description>&lt;p&gt;
ESG&#24773;&#32490;&#20998;&#26512;&#65306;&#27604;&#36739;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;GPT
&lt;/p&gt;
&lt;p&gt;
ESG Sentiment Analysis: comparing human and language model performance including GPT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20154;&#31867;&#21644;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#34913;&#37327;ESG&#30456;&#20851;&#24773;&#32490;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19982;&#29615;&#20445;&#12289;&#31038;&#20250;&#36131;&#20219;&#21644;&#20844;&#21496;&#27835;&#29702;&#65288;ESG&#65289;&#31038;&#20132;&#23186;&#20307;&#30456;&#20851;&#30340;&#24773;&#32490;&#27979;&#37327;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;ESG&#22312;&#37325;&#35201;&#24615;&#19978;&#22686;&#38271;&#65292;&#37329;&#34701;&#37096;&#38376;&#23545;&#20854;&#34920;&#29616;&#20986;&#20102;&#26497;&#22823;&#20852;&#36259;&#65292;&#24182;&#19988;&#35768;&#22810;&#20225;&#19994;&#30340;&#32489;&#25928;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20854;ESG&#30456;&#20851;&#22768;&#35465;&#12290;&#21033;&#29992;&#24773;&#32490;&#20998;&#26512;&#26469;&#34913;&#37327;ESG&#30456;&#20851;&#22768;&#35465;&#24050;&#32463;&#24471;&#21040;&#21457;&#23637;&#65292;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#23545;&#26426;&#22120;&#36827;&#34892;&#27492;&#31867;&#20998;&#26512;&#30340;&#20852;&#36259;&#12290;&#25968;&#23383;&#23186;&#20307;&#26102;&#20195;&#20652;&#29983;&#20102;&#26032;&#23186;&#20307;&#26469;&#28304;&#30340;&#29190;&#28856;&#24335;&#22686;&#38271;&#65292;&#36825;&#24471;&#30410;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#22686;&#38271;&#12290;&#36825;&#31181;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#29615;&#22659;&#24050;&#25104;&#20026;&#35768;&#22810;&#39046;&#22495;&#34892;&#20026;&#27934;&#23519;&#30740;&#31350;&#30340;&#26497;&#20339;&#26469;&#28304;&#65292;&#28085;&#30422;&#25919;&#27835;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#24066;&#22330;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#20154;&#31867;&#34920;&#29616;&#19982;&#35821;&#35328;&#27169;&#22411;&#22312;&#34913;&#37327;ESG&#30456;&#20851;&#24773;&#32490;&#26041;&#38754;&#30340;&#23574;&#31471;&#34920;&#29616;&#12290;&#20026;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;150&#26465;&#25512;&#25991;&#30340;&#24773;&#32490;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16650v1 Announce Type: new  Abstract: In this paper we explore the challenges of measuring sentiment in relation to Environmental, Social and Governance (ESG) social media. ESG has grown in importance in recent years with a surge in interest from the financial sector and the performance of many businesses has become based in part on their ESG related reputations. The use of sentiment analysis to measure ESG related reputation has developed and with it interest in the use of machines to do so. The era of digital media has created an explosion of new media sources, driven by the growth of social media platforms. This growing data environment has become an excellent source for behavioural insight studies across many disciplines that includes politics, healthcare and market research. Our study seeks to compare human performance with the cutting edge in machine performance in the measurement of ESG related sentiment. To this end researchers classify the sentiment of 150 tweets an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#22495;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20174;&#30005;&#23376;&#35789;&#20856;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#35821;&#20041;&#20540;&#21644;&#31163;&#25955;&#25551;&#36848;&#20043;&#38388;&#30340;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.16632</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#23884;&#20837;&#29983;&#25104;&#24847;&#22823;&#21033;&#35821;&#27010;&#24565;&#22797;&#26434;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Domain Embeddings for Generating Complex Descriptions of Concepts in Italian Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16632
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#22495;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20174;&#30005;&#23376;&#35789;&#20856;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#36830;&#32493;&#35821;&#20041;&#20540;&#21644;&#31163;&#25955;&#25551;&#36848;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#35821;&#20041;&#36164;&#28304;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#30005;&#23376;&#35789;&#20856;&#20013;&#25552;&#21462;&#30340;&#35821;&#35328;&#21644;&#35789;&#27719;&#20449;&#24687;&#65292;&#26088;&#22312;&#35299;&#20915;&#36830;&#32493;&#35821;&#20041;&#20540;&#19982;&#19968;&#33324;&#35821;&#20041;&#29702;&#35770;&#25552;&#20379;&#30340;&#31163;&#25955;&#25551;&#36848;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#25968;&#25454;&#30340;&#26367;&#20195;&#31574;&#30053;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#29305;&#23450;&#39046;&#22495;&#30340;&#20849;&#29616;&#30697;&#38453;&#65292;&#20174;&#20004;&#20010;&#26469;&#28304;&#27966;&#29983;&#32780;&#26469;&#65306;&#23558;&#24847;&#22823;&#21033;&#21517;&#35789;&#20998;&#31867;&#20026;4&#20010;&#35821;&#20041;&#29305;&#24449;&#21644;20
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16632v1 Announce Type: new  Abstract: In this work, we propose a Distributional Semantic resource enriched with linguistic and lexical information extracted from electronic dictionaries, designed to address the challenge of bridging the gap between the continuous semantic values represented by distributional vectors and the discrete descriptions offered by general semantics theory. Recently, many researchers have concentrated on the nexus between embeddings and a comprehensive theory of semantics and meaning. This often involves decoding the representation of word meanings in Distributional Models into a set of discrete, manually constructed properties such as semantic primitives or features, using neural decoding techniques. Our approach introduces an alternative strategy grounded in linguistic data. We have developed a collection of domain-specific co-occurrence matrices, derived from two sources: a classification of Italian nouns categorized into 4 semantic traits and 20 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CEPE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24182;&#34892;&#32534;&#30721;&#25193;&#23637;&#20102;&#29616;&#26377;&#20165;&#35299;&#30721;&#22120;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.16617</link><description>&lt;p&gt;
&#20855;&#26377;&#24182;&#34892;&#19978;&#19979;&#25991;&#32534;&#30721;&#30340;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Long-Context Language Modeling with Parallel Context Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16617
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CEPE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24182;&#34892;&#32534;&#30721;&#25193;&#23637;&#20102;&#29616;&#26377;&#20165;&#35299;&#30721;&#22120;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#21040;&#22788;&#29702;&#26356;&#38271;&#30340;&#36755;&#20837;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;transformers&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#21450;&#20301;&#32622;&#32534;&#30721;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Context Expansion with Parallel Encoding&#65288;CEPE&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#20165;&#35299;&#30721;&#22120;LLMs&#65292;&#20197;&#25193;&#23637;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;CEPE&#37319;&#29992;&#19968;&#20010;&#23567;&#22411;&#32534;&#30721;&#22120;&#26469;&#20998;&#22359;&#22788;&#29702;&#38271;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#20351;&#20923;&#32467;&#30340;&#35299;&#30721;&#22120;&#33021;&#22815;&#21033;&#29992;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#12290;CEPE&#39640;&#25928;&#12289;&#36890;&#29992;&#19988;&#22810;&#21151;&#33021;&#65306;&#36890;&#36807;&#20351;&#29992;8K&#26631;&#35760;&#25991;&#26723;&#36827;&#34892;&#35757;&#32451;&#65292;CEPE&#23558;LLAMA-2&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;128K&#26631;&#35760;&#65292;&#20165;&#20351;&#29992;1/6&#30340;&#20869;&#23384;&#21363;&#21487;&#33719;&#24471;10&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;CEPE&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#24615;&#33021;&#12290;CEPE&#22312;&#26816;&#32034;&#22686;&#24378;&#24212;&#29992;&#20013;&#20063;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#21017;&#36864;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16617v1 Announce Type: new  Abstract: Extending large language models (LLMs) to process longer inputs is crucial for numerous applications. However, the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window. We introduce Context Expansion with Parallel Encoding (CEPE), a framework that can be applied to any existing decoder-only LLMs to extend their context window. CEPE adopts a small encoder to process long inputs chunk by chunk and enables the frozen decoder to leverage additional contexts via cross-attention. CEPE is efficient, generalizable, and versatile: trained with 8K-token documents, CEPE extends the context window of LLAMA-2 to 128K tokens, offering 10x the throughput with only 1/6 of the memory. CEPE yields strong performance on language modeling and in-context learning. CEPE also excels in retrieval-augmented applications, while existing long-context models degenerat
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#36136;&#37327;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20013;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;&#30340;&#39318;&#35201;&#20219;&#21153;&#65292;&#20294;&#31649;&#29702;&#32773;&#38388;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#32570;&#20047;&#20849;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.16611</link><description>&lt;p&gt;
&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#32972;&#21518;&#30340;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Understanding the Dataset Practitioners Behind Large Language Model Development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16611
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#36136;&#37327;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20013;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;&#30340;&#39318;&#35201;&#20219;&#21153;&#65292;&#20294;&#31649;&#29702;&#32773;&#38388;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#32570;&#20047;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21464;&#24471;&#36234;&#26469;&#36234;&#20808;&#36827;&#21644;&#26377;&#24433;&#21709;&#21147;&#65292;&#23457;&#35270;&#23427;&#20204;&#20381;&#36182;&#21644;&#20135;&#29983;&#30340;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;&#30340;&#24037;&#20316;&#20869;&#23481;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#35895;&#27468;&#36129;&#29486;LLM&#24320;&#21457;&#22242;&#38431;&#36131;&#20219;&#30340;&#22238;&#39038;&#24615;&#20998;&#26512;&#65292;&#23450;&#20041;&#20102;&#8220;&#25968;&#25454;&#38598;&#31649;&#29702;&#32773;&#8221;&#30340;&#35282;&#33394;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#31649;&#29702;&#32773;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#65288;N=10&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#36136;&#37327;&#26159;&#39318;&#35201;&#20219;&#21153;&#12290;&#20026;&#20102;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#65292;&#31649;&#29702;&#32773;&#35201;&#20040;&#20973;&#30452;&#35273;&#65292;&#35201;&#20040;&#32534;&#20889;&#33258;&#23450;&#20041;&#35780;&#20272;&#36923;&#36753;&#12290;&#31649;&#29702;&#32773;&#20043;&#38388;&#23545;&#25968;&#25454;&#36136;&#37327;&#30340;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#32570;&#20047;&#20849;&#35782;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#28508;&#22312;&#21407;&#22240;&#21644;&#23454;&#29616;&#19968;&#33268;&#24615;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16611v1 Announce Type: new  Abstract: As large language models (LLMs) become more advanced and impactful, it is increasingly important to scrutinize the data that they rely upon and produce. What is it to be a dataset practitioner doing this work? We approach this in two parts: first, we define the role of "dataset practitioner" by performing a retrospective analysis on the responsibilities of teams contributing to LLM development at Google. Then, we conduct semi-structured interviews with a cross-section of these practitioners (N=10). We find that data quality is the top priority. To evaluate data quality, practitioners either rely on their own intuition or write custom evaluation logic. There is a lack of consensus across practitioners on what quality is and how to evaluate it. We discuss potential reasons for this phenomenon and opportunities for alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#38598;&#19981;&#36275;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PAQA&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#26597;&#35810;&#21644;&#25991;&#26723;&#20013;&#30340;&#27495;&#20041;&#65292;&#29983;&#25104;&#30456;&#20851;&#28548;&#28165;&#38382;&#39064;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16608</link><description>&lt;p&gt;
PAQA&#65306;&#38754;&#21521;&#31215;&#26497;&#24320;&#25918;&#22411;&#26816;&#32034;&#38382;&#31572;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PAQA: Toward ProActive Open-Retrieval Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#38598;&#19981;&#36275;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PAQA&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#26597;&#35810;&#21644;&#25991;&#26723;&#20013;&#30340;&#27495;&#20041;&#65292;&#29983;&#25104;&#30456;&#20851;&#28548;&#28165;&#38382;&#39064;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#31995;&#32479;&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#34987;&#21160;&#35282;&#33394;&#65292;&#30446;&#21069;&#20316;&#20026;&#23545;&#35805;&#24335;&#25628;&#32034;&#31995;&#32479;&#30340;&#28508;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#25552;&#20379;&#24102;&#26377;&#25903;&#25345;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#30456;&#20851;&#28548;&#28165;&#38382;&#39064;&#30340;&#26631;&#35760;&#27169;&#26865;&#20004;&#21487;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#26597;&#35810;&#21644;&#25991;&#26723;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#27495;&#20041;&#26469;&#35299;&#20915;&#29983;&#25104;&#30456;&#20851;&#28548;&#28165;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PAQA&#65292;&#36825;&#26159;&#29616;&#26377;AmbiNQ&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#65292;&#20854;&#20013;&#21253;&#21547;&#28548;&#28165;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#27573;&#33853;&#26816;&#32034;&#22914;&#20309;&#24433;&#21709;&#27169;&#26865;&#20004;&#21487;&#26816;&#27979;&#21644;&#29983;&#25104;&#28548;&#28165;&#38382;&#39064;&#12290;&#36890;&#36807;&#35299;&#20915;&#23545;&#35805;&#25628;&#32034;&#31995;&#32479;&#20013;&#30340;&#36825;&#19968;&#32570;&#38519;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20197;&#22686;&#24378;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16608v1 Announce Type: new  Abstract: Conversational systems have made significant progress in generating natural language responses. However, their potential as conversational search systems is currently limited due to their passive role in the information-seeking process. One major limitation is the scarcity of datasets that provide labelled ambiguous questions along with a supporting corpus of documents and relevant clarifying questions. This work aims to tackle the challenge of generating relevant clarifying questions by taking into account the inherent ambiguities present in both user queries and documents. To achieve this, we propose PAQA, an extension to the existing AmbiNQ dataset, incorporating clarifying questions. We then evaluate various models and assess how passage retrieval impacts ambiguity detection and the generation of clarifying questions. By addressing this gap in conversational search systems, we aim to provide additional supervision to enhance their ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#29983;&#25104;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#24341;&#20837;&#36127;&#20363;&#35757;&#32451;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#36127;&#20363;&#30340;&#24341;&#20837;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#28165;&#26224;&#21010;&#23450;&#26631;&#31614;&#36793;&#30028;&#26469;&#26174;&#33879;&#25913;&#36827;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Matching&#30340;&#26032;&#39062;&#39640;&#25928;&#31639;&#27861;&#65292;&#36827;&#19968;&#27493;&#23558;&#38750;&#32467;&#26500;&#21270;&#39044;&#27979;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.16602</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#29983;&#25104;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#36127;&#20363;
&lt;/p&gt;
&lt;p&gt;
Rethinking Negative Instances for Generative Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#29983;&#25104;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#24341;&#20837;&#36127;&#20363;&#35757;&#32451;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#36127;&#20363;&#30340;&#24341;&#20837;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#28165;&#26224;&#21010;&#23450;&#26631;&#31614;&#36793;&#30028;&#26469;&#26174;&#33879;&#25913;&#36827;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Matching&#30340;&#26032;&#39062;&#39640;&#25928;&#31639;&#27861;&#65292;&#36827;&#19968;&#27493;&#23558;&#38750;&#32467;&#26500;&#21270;&#39044;&#27979;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#26410;&#30693;&#20219;&#21153;&#20013;&#25797;&#38271;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#37319;&#29992;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#27169;&#24335;&#35843;&#25972;&#65292;LLMs&#30340;&#26174;&#33879;&#25913;&#36827;&#24050;&#32463;&#22312;&#24191;&#27867;&#30340;&#23454;&#20307;&#39046;&#22495;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#36890;&#36807;&#23558;&#36127;&#20363;&#32435;&#20837;&#35757;&#32451;&#26469;&#22686;&#24378;&#29616;&#26377;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#36127;&#20363;&#36890;&#36807;&#65288;1&#65289;&#24341;&#20837;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#65288;2&#65289;&#28165;&#26224;&#21010;&#23450;&#26631;&#31614;&#36793;&#30028;&#32780;&#23545;&#25913;&#36827;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Matching&#30340;&#26032;&#39062;&#39640;&#25928;&#31639;&#27861;&#65292;&#26088;&#22312;&#23558;&#38750;&#32467;&#26500;&#21270;&#39044;&#27979;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNER&#65292;&#19968;&#20010;&#29983;&#25104;&#24335;NER&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#36328;&#26410;&#30693;&#23454;&#20307;&#39046;&#22495;&#30340;&#25552;&#21319;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16602v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities for generalizing in unseen tasks. In the Named Entity Recognition (NER) task, recent advancements have seen the remarkable improvement of LLMs in a broad range of entity domains via instruction tuning, by adopting entity-centric schema. In this work, we explore the potential enhancement of the existing methods by incorporating negative instances into training. Our experiments reveal that negative instances contribute to remarkable improvements by (1) introducing contextual information, and (2) clearly delineating label boundaries. Furthermore, we introduce a novel and efficient algorithm named Hierarchical Matching, which is tailored to transform unstructured predictions into structured entities. By integrating these components, we present GNER, a Generative NER system that shows improved zero-shot performance across unseen entity domains. Our comprehensive evaluation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26031;&#27931;&#25991;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#38169;&#35823;&#29575;&#38477;&#20302;22.8%&#12290;</title><link>https://arxiv.org/abs/2402.16596</link><description>&lt;p&gt;
Slovene&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#65306;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semantic change detection for Slovene language: a novel dataset and an approach based on optimal transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16596
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26031;&#27931;&#25991;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#38169;&#35823;&#29575;&#38477;&#20302;22.8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26816;&#27979;&#26031;&#27931;&#25991;&#23612;&#20122;&#35821;&#20013;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#25317;&#26377;&#20004;&#30334;&#19975;&#20351;&#29992;&#32773;&#30340;&#36164;&#28304;&#36739;&#23569;&#30340;&#26031;&#25289;&#22827;&#35821;&#35328;&#12290;&#26816;&#27979;&#21644;&#36319;&#36394;&#35821;&#20041;&#21464;&#21270;&#21487;&#20197;&#25581;&#31034;&#35821;&#35328;&#30340;&#28436;&#21464;&#65292;&#36825;&#26159;&#30001;&#31038;&#20250;&#21644;&#25991;&#21270;&#21464;&#21270;&#24341;&#36215;&#30340;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#31995;&#32479;&#26469;&#24110;&#21161;&#36825;&#39033;&#30740;&#31350;&#65292;&#20294;&#25152;&#26377;&#36825;&#20123;&#31995;&#32479;&#37117;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26031;&#27931;&#25991;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20174;3000&#22810;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21477;&#23545;&#20013;&#33719;&#24471;&#30340;104&#20010;&#30446;&#26631;&#35789;&#30340;&#27719;&#24635;&#35821;&#20041;&#21464;&#21270;&#35780;&#20998;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20960;&#31181;&#24050;&#26377;&#30340;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#65292;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;22.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16596v1 Announce Type: new  Abstract: In this paper, we focus on the detection of semantic changes in Slovene, a less resourced Slavic language with two million speakers. Detecting and tracking semantic changes provides insights into the evolution of the language caused by changes in society and culture. Recently, several systems have been proposed to aid in this study, but all depend on manually annotated gold standard datasets for evaluation. In this paper, we present the first Slovene dataset for evaluating semantic change detection systems, which contains aggregated semantic change scores for 104 target words obtained from more than 3000 manually annotated sentence pairs. We evaluate several existing semantic change detection methods on this dataset and also propose a novel approach based on optimal transport that improves on the existing state-of-the-art systems with an error reduction rate of 22.8%.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#38646;&#27604;&#29305;&#26080;&#22833;&#30495;&#27700;&#21360;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27700;&#21360;&#20013;&#23884;&#20837;&#22810;&#27604;&#29305;&#20803;&#20449;&#24687;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#20174;&#27700;&#21360;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#36807;&#31243;&#65292;&#20855;&#26377;&#24456;&#20302;&#30340;&#27604;&#29305;&#35823;&#24046;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16578</link><description>&lt;p&gt;
&#22810;&#27604;&#29305;&#26080;&#22833;&#30495;&#25968;&#23383;&#27700;&#21360;&#25216;&#26415;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-Bit Distortion-Free Watermarking for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#38646;&#27604;&#29305;&#26080;&#22833;&#30495;&#27700;&#21360;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27700;&#21360;&#20013;&#23884;&#20837;&#22810;&#27604;&#29305;&#20803;&#20449;&#24687;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#20174;&#27700;&#21360;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#36807;&#31243;&#65292;&#20855;&#26377;&#24456;&#20302;&#30340;&#27604;&#29305;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26366;&#25552;&#20986;&#27700;&#21360;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31245;&#24494;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#20998;&#24067;&#26469;&#21306;&#20998;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#20294;&#23427;&#20204;&#20063;&#20250;&#25197;&#26354;&#25991;&#26412;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#23558;&#27700;&#21360;&#26292;&#38706;&#32473;&#23545;&#25239;&#24615;&#26816;&#27979;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#26080;&#22833;&#30495;&#27700;&#21360;&#26041;&#27861;&#65292;&#38656;&#35201;&#19968;&#20010;&#31192;&#38053;&#26469;&#26816;&#27979;&#27700;&#21360;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23884;&#20837;&#38646;&#27604;&#29305;&#27700;&#21360;&#65292;&#38500;&#20102;&#26631;&#35760;&#25991;&#26412;&#20026;AI&#29983;&#25104;&#22806;&#24182;&#26410;&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23884;&#20837;&#22810;&#27604;&#29305;&#30340;&#20803;&#20449;&#24687;&#20316;&#20026;&#27700;&#21360;&#30340;&#19968;&#37096;&#20998;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#38646;&#27604;&#29305;&#26080;&#22833;&#30495;&#27700;&#21360;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#30340;&#35299;&#30721;&#22120;&#65292;&#20174;&#27700;&#21360;&#20013;&#25552;&#21462;&#23884;&#20837;&#30340;&#20449;&#24687;&#65292;&#20855;&#26377;&#24456;&#20302;&#30340;&#27604;&#29305;&#35823;&#30721;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16578v1 Announce Type: new  Abstract: Methods for watermarking large language models have been proposed that distinguish AI-generated text from human-generated text by slightly altering the model output distribution, but they also distort the quality of the text, exposing the watermark to adversarial detection. More recently, distortion-free watermarking methods were proposed that require a secret key to detect the watermark. The prior methods generally embed zero-bit watermarks that do not provide additional information beyond tagging a text as being AI-generated. We extend an existing zero-bit distortion-free watermarking method by embedding multiple bits of meta-information as part of the watermark. We also develop a computationally efficient decoder that extracts the embedded information from the watermark with low bit error rate.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;&#26694;&#26550;GenTKGQA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#21363;&#23376;&#22270;&#26816;&#32034;&#21644;&#31572;&#26696;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.16568</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20004;&#38454;&#27573;&#29983;&#25104;&#24335;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16568
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;&#26694;&#26550;GenTKGQA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#21363;&#23376;&#22270;&#26816;&#32034;&#21644;&#31572;&#26696;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;(TKGQA)&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#20219;&#21153;&#65292;&#22240;&#20026;&#38382;&#39064;&#20013;&#38544;&#34255;&#30528;&#26102;&#38388;&#32422;&#26463;&#65292;&#24182;&#19988;&#20174;&#21160;&#24577;&#32467;&#26500;&#21270;&#30693;&#35782;&#20013;&#23547;&#25214;&#31572;&#26696;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;TKGQA&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;&#26694;&#26550;GenTKGQA&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#24341;&#23548;LLMs&#22238;&#31572;&#26102;&#38388;&#24615;&#38382;&#39064;&#65306;&#23376;&#22270;&#26816;&#32034;&#21644;&#31572;&#26696;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#30340;&#22266;&#26377;&#30693;&#35782;&#26469;&#25366;&#25496;&#38382;&#39064;&#20013;&#30340;&#26102;&#38388;&#32422;&#26463;&#21644;&#32467;&#26500;&#38142;&#25509;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#20174;&#32780;&#32553;&#23567;&#20102;&#22312;&#26102;&#38388;&#21644;&#32467;&#26500;&#32500;&#24230;&#19978;&#30340;&#23376;&#22270;&#25628;&#32034;&#31354;&#38388;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#34394;&#25311;&#30693;&#35782;&#25351;&#31034;&#22120;&#26469;&#34701;&#21512;&#23376;&#22270;&#21644;&#25991;&#26412;&#34920;&#31034;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16568v1 Announce Type: new  Abstract: Temporal knowledge graph question answering (TKGQA) poses a significant challenge task, due to the temporal constraints hidden in questions and the answers sought from dynamic structured knowledge. Although large language models (LLMs) have made considerable progress in their reasoning ability over structured data, their application to the TKGQA task is a relatively unexplored area. This paper first proposes a novel generative temporal knowledge graph question answering framework, GenTKGQA, which guides LLMs to answer temporal questions through two phases: Subgraph Retrieval and Answer Generation. First, we exploit LLM's intrinsic knowledge to mine temporal constraints and structural links in the questions without extra training, thus narrowing down the subgraph search space in both temporal and structural dimensions. Next, we design virtual knowledge indicators to fuse the graph neural network signals of the subgraph and the text repres
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#24211;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#29983;&#25104;NL-GQL&#25968;&#25454;&#23545;&#24182;&#24494;&#35843;LLMs&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.16567</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models to a Domain-specific Graph Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#24211;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;ChatGPT&#29983;&#25104;NL-GQL&#25968;&#25454;&#23545;&#24182;&#24494;&#35843;LLMs&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#24211;&#65288;Graph DB&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#37329;&#34701;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#21307;&#33647;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#36716;&#25442;&#20026;&#22270;&#26597;&#35810;&#35821;&#35328;&#65288;GQL&#65289;&#65292;&#36890;&#24120;&#31216;&#20026;NL2GQL&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#19987;&#19994;&#21270;&#29305;&#24615;&#32780;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#20123;&#26041;&#27861;&#35797;&#22270;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#31867;&#20284;&#30340;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#36716;SQL&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;NL2GQL&#20219;&#21153;&#20013;&#65292;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;NL-GQL&#25968;&#25454;&#23545;&#20351;&#24471;&#38590;&#20197;&#24314;&#31435;LLMs&#21644;&#22270;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#27969;&#27700;&#32447;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;ChatGPT&#22522;&#20110;&#32473;&#23450;&#30340;&#22270;&#25968;&#25454;&#24211;&#33258;&#25105;&#29983;&#25104;NL-GQL&#25968;&#25454;&#23545;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21019;&#24314;&#30340;&#25968;&#25454;&#26469;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;LLMs&#19982;&#22270;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16567v1 Announce Type: new  Abstract: Graph Databases (Graph DB) are widely applied in various fields, including finance, social networks, and medicine. However, translating Natural Language (NL) into the Graph Query Language (GQL), commonly known as NL2GQL, proves to be challenging due to its inherent complexity and specialized nature. Some approaches have sought to utilize Large Language Models (LLMs) to address analogous tasks like text2SQL. Nevertheless, when it comes to NL2GQL taskson a particular domain, the absence of domain-specific NL-GQL data pairs makes it difficult to establish alignment between LLMs and the graph DB. To address this challenge, we propose a well-defined pipeline. Specifically, we utilize ChatGPT to create NL-GQL data pairs based on the given graph DB with self-instruct. Then, we use the created data to fine-tune LLMs, thereby achieving alignment between LLMs and the graph DB. Additionally, during inference, we propose a method that extracts relev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMGR&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#20250;&#35805;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35805;&#25512;&#33616;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.16539</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#20250;&#35805;&#25512;&#33616;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models with Graphical Session-Based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMGR&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#20250;&#35805;&#25512;&#33616;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20250;&#35805;&#25512;&#33616;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#25506;&#32034;&#65292;&#21033;&#29992;LLMs&#22312;&#25512;&#33616;&#31995;&#32479;&#19978;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;&#34429;&#28982;&#24320;&#21019;&#24615;&#30340;&#31574;&#30053;&#20027;&#35201;&#26159;&#23558;&#20256;&#32479;&#25512;&#33616;&#20219;&#21153;&#36716;&#21464;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25361;&#25112;&#65292;&#20294;&#22312;&#20250;&#35805;&#25512;&#33616;&#65288;SBR&#65289;&#39046;&#22495;&#30340;&#25506;&#32034;&#30456;&#23545;&#36739;&#23569;&#65292;&#22240;&#20026;&#20854;&#20855;&#20307;&#24615;&#12290;SBR&#20027;&#35201;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#20027;&#23548;&#65292;&#30001;&#20110;&#20854;&#25429;&#33719;&#30456;&#37051;&#34892;&#20026;&#20043;&#38388;&#30340;&#20869;&#22312;&#21644;&#26174;&#24615;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#32467;&#26524;&#12290;&#22270;&#30340;&#32467;&#26500;&#24615;&#36136;&#19982;&#33258;&#28982;&#35821;&#35328;&#30340;&#26412;&#36136;&#24418;&#25104;&#23545;&#27604;&#65292;&#20026;LLMs&#25552;&#20986;&#20102;&#37325;&#22823;&#30340;&#36866;&#24212;&#24615;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#20250;&#35805;&#25512;&#33616;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;LLMGR&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21644;&#35856;&#22320;&#24357;&#21512;&#19978;&#36848;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16539v1 Announce Type: cross  Abstract: With the rapid development of Large Language Models (LLMs), various explorations have arisen to utilize LLMs capability of context understanding on recommender systems. While pioneering strategies have primarily transformed traditional recommendation tasks into challenges of natural language generation, there has been a relative scarcity of exploration in the domain of session-based recommendation (SBR) due to its specificity. SBR has been primarily dominated by Graph Neural Networks, which have achieved many successful outcomes due to their ability to capture both the implicit and explicit relationships between adjacent behaviors. The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs. In this paper, we introduce large language models with graphical Session-Based recommendation, named LLMGR, an effective framework that bridges the aforementioned gap by harmoniously 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;LLM&#21644;&#20998;&#24067;&#23548;&#24072;&#30340;&#30693;&#35782;&#33976;&#39311;&#24341;&#23548;&#19979;&#30340;&#38544;&#31169;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#65292;&#23558;DP-based&#20266;&#26679;&#26412;&#29983;&#25104;&#20219;&#21153;&#36716;&#31227;&#21040;DP-based&#29983;&#25104;&#26679;&#26412;&#37492;&#21035;&#20219;&#21153;&#65292;&#36890;&#36807;&#25945;&#24072;&#27169;&#22411;&#25945;&#23548;&#23398;&#29983;&#22914;&#20309;&#36873;&#25321;&#24102;&#26377;&#26657;&#20934;&#22122;&#22768;&#30340;&#31169;&#26377;&#26679;&#26412;&#20197;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2402.16515</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#21644;&#20998;&#24067;&#23548;&#24072;&#30340;&#30693;&#35782;&#33976;&#39311;&#24341;&#23548;&#19979;&#30340;&#38544;&#31169;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16515
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;LLM&#21644;&#20998;&#24067;&#23548;&#24072;&#30340;&#30693;&#35782;&#33976;&#39311;&#24341;&#23548;&#19979;&#30340;&#38544;&#31169;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#65292;&#23558;DP-based&#20266;&#26679;&#26412;&#29983;&#25104;&#20219;&#21153;&#36716;&#31227;&#21040;DP-based&#29983;&#25104;&#26679;&#26412;&#37492;&#21035;&#20219;&#21153;&#65292;&#36890;&#36807;&#25945;&#24072;&#27169;&#22411;&#25945;&#23548;&#23398;&#29983;&#22914;&#20309;&#36873;&#25321;&#24102;&#26377;&#26657;&#20934;&#22122;&#22768;&#30340;&#31169;&#26377;&#26679;&#26412;&#20197;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#24182;&#38750;&#22987;&#32456;&#20844;&#24320;&#21487;&#35775;&#38382;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#20808;&#36827;&#30340;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#25110;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26469;&#25193;&#23637;&#25968;&#25454;&#38598;&#12290;&#22312;&#31169;&#26377;&#39046;&#22495;&#36827;&#34892;DA&#38656;&#35201;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65288;&#22914;&#21311;&#21517;&#21270;&#21644;&#25200;&#21160;&#65289;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#20445;&#25252;&#20445;&#35777;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#38480;&#21046;&#20445;&#25252;&#65292;&#20294;&#19981;&#25797;&#38271;&#20351;&#29992;&#22823;&#27169;&#22411;&#29983;&#25104;&#20266;&#25991;&#26412;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;DP&#30340;&#20266;&#26679;&#26412;&#29983;&#25104;&#20219;&#21153;&#36716;&#31227;&#20026;&#22522;&#20110;DP&#29983;&#25104;&#26679;&#26412;&#37492;&#21035;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;LLM&#21644;&#22522;&#20110;DP&#37492;&#21035;&#22120;&#30340;DP-based DA&#26041;&#27861;&#65292;&#29992;&#20110;&#31169;&#26377;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#20316;&#20026;&#22522;&#20110;DP&#30340;&#37492;&#21035;&#22120;&#65306;&#25945;&#24072;&#27169;&#22411;&#35775;&#38382;&#31169;&#26377;&#25968;&#25454;&#65292;&#25945;&#23548;&#23398;&#29983;&#22914;&#20309;&#36873;&#25321;&#20855;&#26377;&#26657;&#20934;&#22122;&#22768;&#30340;&#31169;&#26377;&#26679;&#26412;&#20197;&#23454;&#29616;DP&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16515v1 Announce Type: new  Abstract: As sufficient data are not always publically accessible for model training, researchers exploit limited data with advanced learning algorithms or expand the dataset via data augmentation (DA). Conducting DA in private domain requires private protection approaches (i.e. anonymization and perturbation), but those methods cannot provide protection guarantees. Differential privacy (DP) learning methods theoretically bound the protection but are not skilled at generating pseudo text samples with large models. In this paper, we transfer DP-based pseudo sample generation task to DP-based generated samples discrimination task, where we propose a DP-based DA method with a LLM and a DP-based discriminator for text classification on private domains. We construct a knowledge distillation model as the DP-based discriminator: teacher models, accessing private data, teaches students how to select private samples with calibrated noise to achieve DP. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#30340;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#21512;&#25104;&#30417;&#30563;&#20449;&#21495;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16508</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#21512;&#25104;&#30417;&#30563;&#36827;&#34892;&#36328;&#35821;&#35328;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#30340;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#21512;&#25104;&#30417;&#30563;&#20449;&#21495;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#38382;&#31572;&#65288;CLQA&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20174;&#22810;&#35821;&#35328;&#30693;&#35782;&#24211;&#36827;&#34892;&#36328;&#35821;&#35328;&#26816;&#32034;&#65292;&#28982;&#21518;&#22312;&#33521;&#35821;&#25110;&#26597;&#35810;&#35821;&#35328;&#20013;&#29983;&#25104;&#31572;&#26696;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;CLQA&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38142;&#25509;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#26469;&#21512;&#25104;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#36890;&#36807;&#19968;&#31181;&#22635;&#31354;&#26597;&#35810;&#24418;&#24335;&#29983;&#25104;&#26356;&#33258;&#28982;&#30340;&#26597;&#35810;&#20197;&#30417;&#30563;&#31572;&#26696;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CLASS&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#38646;-shot&#24773;&#20917;&#19979;&#22343;&#20248;&#20110;&#21487;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16508v1 Announce Type: new  Abstract: Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation. Together, we show our approach, \texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot lan
&lt;/p&gt;</description></item><item><title>LLMArena&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#26131;&#20110;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20195;&#29702;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#22810;&#26679;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16499</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#30340;LLMArena
&lt;/p&gt;
&lt;p&gt;
LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16499
&lt;/p&gt;
&lt;p&gt;
LLMArena&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#26131;&#20110;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20195;&#29702;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#22810;&#26679;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#25581;&#31034;&#20102;&#23427;&#20204;&#23454;&#29616;&#25317;&#26377;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#29616;&#26377;&#22522;&#20934;&#35201;&#20040;&#20351;&#29992;&#38745;&#24577;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#27844;&#28431;&#65292;&#35201;&#20040;&#20165;&#20851;&#27880;&#21333;&#19968;&#20195;&#29702;&#22330;&#26223;&#65292;&#24573;&#30053;&#20102;&#22810;&#20195;&#29702;&#20114;&#21160;&#30340;&#22797;&#26434;&#24615;&#12290;&#32570;&#20047;&#19968;&#20010;&#22522;&#20934;&#65292;&#21487;&#20197;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#22810;&#20195;&#29702;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#22810;&#26679;&#21270;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMArena&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#26131;&#20110;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#22312;&#22810;&#20195;&#29702;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#22810;&#26679;&#21270;&#33021;&#21147;&#12290;LLMArena&#21253;&#21547;&#19971;&#20010;&#19981;&#21516;&#30340;&#28216;&#25103;&#29615;&#22659;&#65292;&#37319;&#29992;Trueskill&#35780;&#20998;&#26469;&#35780;&#20272;LLM&#20195;&#29702;&#20013;&#20851;&#38190;&#33021;&#21147;&#65292;&#21253;&#25324;&#31354;&#38388;&#25512;&#29702;&#12289;&#25112;&#30053;&#35268;&#21010;&#12289;&#25968;&#23383;&#25512;&#29702;&#12289;&#39118;&#38505;&#35780;&#20272;&#12289;&#27807;&#36890;&#12289;&#23545;&#25163;&#24314;&#27169;&#21644;&#22242;&#38431;&#21327;&#20316;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16499v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive ex
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19977;&#31181;&#21151;&#33021;&#21270;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#21040;&#27169;&#25311;&#65288;Lang2Sim&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#31934;&#20934;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#27169;&#25311;&#22120;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16482</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#27169;&#25311;&#24341;&#25806;&#36827;&#34892;&#35821;&#35328;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Languaging a Simulation Engine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16482
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19977;&#31181;&#21151;&#33021;&#21270;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#21040;&#27169;&#25311;&#65288;Lang2Sim&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#31934;&#20934;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#27169;&#25311;&#22120;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26234;&#33021;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#32534;&#31243;&#26448;&#26009;&#27169;&#25311;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#22330;&#26223;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#23558;&#20154;&#31867;&#35821;&#35328;&#31934;&#30830;&#36716;&#21270;&#20026;&#23450;&#21046;&#27169;&#25311;&#22120;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#21151;&#33021;&#21270;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#21040;&#27169;&#25311;&#65288;Lang2Sim&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#20132;&#20114;&#24335;&#23548;&#33322;&#65292;&#36890;&#36807;&#20197;&#22810;&#23380;&#30697;&#38453;&#20013;&#27700;&#21560;&#38468;&#30340;&#22330;&#26223;&#23454;&#20363;&#20026;&#20363;&#23545;&#27169;&#25311;&#24341;&#25806;&#36827;&#34892;&#35821;&#35328;&#21270;&#12290;&#19982;&#36880;&#34892;&#32534;&#30721;&#30446;&#26631;&#27169;&#25311;&#22120;&#19981;&#21516;&#65292;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27599;&#20010;&#27169;&#25311;&#22120;&#20026;&#20855;&#26377;&#19981;&#21464;&#24037;&#20855;&#21151;&#33021;&#21450;&#20854;&#21464;&#20307;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#24635;&#20307;&#12290; Lang2Sim&#36890;&#36807;&#21151;&#33021;&#21270;&#21644;&#24207;&#21015;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#24212;&#22320;&#65292;&#23545;&#24037;&#20855;&#20998;&#31867;&#36827;&#34892;&#21512;&#29702;&#21270;&#65292;&#23450;&#21046;&#20854;&#36755;&#20837;-&#36755;&#20986;&#32452;&#21512;&#65292;&#24182;&#23558;&#27169;&#25311;&#22120;&#36755;&#20837;&#31934;&#28860;&#20026;&#21487;&#25191;&#34892;&#26684;&#24335;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16482v1 Announce Type: new  Abstract: Language model intelligence is revolutionizing the way we program materials simulations. However, the diversity of simulation scenarios renders it challenging to precisely transform human language into a tailored simulator. Here, using three functionalized types of language model, we propose a language-to-simulation (Lang2Sim) framework that enables interactive navigation on languaging a simulation engine, by taking a scenario instance of water sorption in porous matrices. Unlike line-by-line coding of a target simulator, the language models interpret each simulator as an assembly of invariant tool function and its variant input-output pair. Lang2Sim enables the precise transform of textual description by functionalizing and sequentializing the language models of, respectively, rationalizing the tool categorization, customizing its input-output combinations, and distilling the simulator input into executable format. Importantly, dependin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;mEdIT&#65292;&#36825;&#26159;CoEdIT&#30340;&#22810;&#35821;&#35328;&#25193;&#23637;&#65292;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#23545;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#65292;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#20013;&#34920;&#29616;&#24378;&#21170;&#12290;</title><link>https://arxiv.org/abs/2402.16472</link><description>&lt;p&gt;
mEdIT: &#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
mEdIT: Multilingual Text Editing via Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;mEdIT&#65292;&#36825;&#26159;CoEdIT&#30340;&#22810;&#35821;&#35328;&#25193;&#23637;&#65292;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#23545;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#65292;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#20013;&#34920;&#29616;&#24378;&#21170;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;mEdIT&#65292;&#36825;&#26159;CoEdIT&#30340;&#19968;&#20010;&#22810;&#35821;&#35328;&#25193;&#23637;&#65292;CoEdIT&#26159;&#26368;&#36817;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#65292;&#29992;&#20110;&#20889;&#20316;&#36741;&#21161;&#12290;mEdIT&#27169;&#22411;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#12290;&#23427;&#20204;&#26088;&#22312;&#25509;&#25910;&#29992;&#25143;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25351;&#23450;&#25152;&#38656;&#25991;&#26412;&#23646;&#24615;&#30340;&#25351;&#20196;&#65292;&#20363;&#22914;Grammatik korrigieren&#65288;&#24503;&#35821;&#65289;&#25110;Parafrasee la oraci&#243;n&#65288;&#35199;&#29677;&#29273;&#35821;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#24037;&#27880;&#37322;&#25991;&#26412;&#32534;&#36753;&#25968;&#25454;&#38598;&#20013;&#31574;&#21010;&#25968;&#25454;&#65292;&#38024;&#23545;&#20845;&#31181;&#19981;&#21516;&#35821;&#31995;&#30340;&#22810;&#35821;&#35328;&#65292;&#20026;&#19977;&#20010;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#65288;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#65288;GEC&#65289;&#12289;&#25991;&#26412;&#31616;&#21270;&#21644;&#25913;&#20889;&#65289;&#26500;&#24314;&#20102;mEdIT&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;mEdIT&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#38598;&#19978;&#19982;&#20854;&#20182;&#22810;&#35821;&#35328;LLMs&#30456;&#27604;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;mEdIT gen
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16472v1 Announce Type: cross  Abstract: We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent state-of-the-art text editing models for writing assistance. mEdIT models are trained by fine-tuning multi-lingual large, pre-trained language models (LLMs) via instruction tuning. They are designed to take instructions from the user specifying the attributes of the desired text in the form of natural language instructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\'on (Spanish). We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families. We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing benchmarks against other multilingual LLMs. We also find that mEdIT gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;HackAttend&#25200;&#21160;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;PLMs&#22240;&#24494;&#23567;&#30340;&#27880;&#24847;&#21147;&#25200;&#21160;&#32780;&#20135;&#29983;&#30340;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#23558;&#25991;&#26412;&#25915;&#20987;&#20174;&#35789;&#27719;&#25200;&#21160;&#25193;&#23637;&#21040;&#32467;&#26500;&#25200;&#21160;&#12290;</title><link>https://arxiv.org/abs/2402.16470</link><description>&lt;p&gt;
&#25581;&#31034;&#33258;&#27880;&#24847;&#21147;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unveiling Vulnerability of Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;HackAttend&#25200;&#21160;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;PLMs&#22240;&#24494;&#23567;&#30340;&#27880;&#24847;&#21147;&#25200;&#21160;&#32780;&#20135;&#29983;&#30340;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#23558;&#25991;&#26412;&#25915;&#20987;&#20174;&#35789;&#27719;&#25200;&#21160;&#25193;&#23637;&#21040;&#32467;&#26500;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#34987;&#21457;&#29616;&#23545;&#24494;&#23567;&#35789;&#35821;&#21464;&#21270;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#36825;&#23545;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#26500;&#25104;&#20102;&#24040;&#22823;&#23041;&#32961;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Transformer&#30340;PLMs&#30340;&#22522;&#26412;&#32467;&#26500;&#65292;&#21363;&#33258;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#25200;&#21160;&#25216;&#26415;HackAttend&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#33945;&#29256;&#25200;&#21160;SA&#30697;&#38453;&#20869;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;PLMs&#23384;&#22312;&#20005;&#37325;&#33030;&#24369;&#24615;&#65292;&#24494;&#23567;&#30340;&#27880;&#24847;&#21147;&#25200;&#21160;&#65288;1%&#65289;&#23601;&#33021;&#20135;&#29983;&#38750;&#24120;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;98%&#65289;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#23558;&#20256;&#32479;&#30340;&#25991;&#26412;&#25915;&#20987;&#20174;&#35789;&#27719;&#25200;&#21160;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#32467;&#26500;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16470v1 Announce Type: new  Abstract: Pre-trained language models (PLMs) are shown to be vulnerable to minor word changes, which poses a big threat to real-world systems. While previous studies directly focus on manipulating word inputs, they are limited by their means of generating adversarial samples, lacking generalization to versatile real-world attack. This paper studies the basic structure of transformer-based PLMs, the self-attention (SA) mechanism. (1) We propose a powerful perturbation technique \textit{HackAttend}, which perturbs the attention scores within the SA matrices via meticulously crafted attention masks. We show that state-of-the-art PLMs fall into heavy vulnerability that minor attention perturbations $(1\%)$ can produce a very high attack success rate $(98\%)$. Our paper expands the conventional text attack of word perturbations to more general structural perturbations. (2) We introduce \textit{S-Attend}, a novel smoothing technique that effectively mak
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#65292;&#23558;&#29983;&#25104;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#29992;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16459</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending LLMs against Jailbreaking Attacks via Backtranslation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#65292;&#23558;&#29983;&#25104;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#29992;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34987;&#35757;&#32451;&#25104;&#25298;&#32477;&#26377;&#23475;&#35831;&#27714;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#20250;&#37325;&#20889;&#21407;&#22987;&#25552;&#31034;&#20197;&#38544;&#34255;&#20854;&#26377;&#23475;&#24847;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#21453;&#21521;&#32763;&#35793;&#8221;&#26469;&#38450;&#24481;LLMs&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#30446;&#26631;LLM&#20174;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#30340;&#21021;&#22987;&#21709;&#24212;&#65292;&#25105;&#20204;&#30340;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#26029;&#21487;&#20197;&#23548;&#33268;&#35813;&#21709;&#24212;&#30340;&#36755;&#20837;&#25552;&#31034;&#12290;&#25512;&#26029;&#30340;&#25552;&#31034;&#31216;&#20026;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#65292;&#20542;&#21521;&#20110;&#25581;&#31034;&#21407;&#22987;&#25552;&#31034;&#30340;&#23454;&#38469;&#24847;&#22270;&#65292;&#22240;&#20026;&#23427;&#26159;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#30340;&#65292;&#19981;&#26159;&#30452;&#25509;&#30001;&#25915;&#20987;&#32773;&#25805;&#32437;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20877;&#27425;&#22312;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#19978;&#36816;&#34892;&#30446;&#26631;LLM&#65292;&#22914;&#26524;&#27169;&#22411;&#25298;&#32477;&#20102;&#21453;&#21521;&#32763;&#35793;&#25552;&#31034;&#65292;&#21017;&#25298;&#32477;&#21407;&#22987;&#25552;&#31034;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#25514;&#26045;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#20960;&#20010;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16459v1 Announce Type: cross  Abstract: Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original prompt to conceal its harmful intent. In this paper, we propose a new method for defending LLMs against jailbreaking attacks by ``backtranslation''. Specifically, given an initial response generated by the target LLM from an input prompt, our backtranslation prompts a language model to infer an input prompt that can lead to the response. The inferred prompt is called the backtranslated prompt which tends to reveal the actual intent of the original prompt, since it is generated based on the LLM's response and is not directly manipulated by the attacker. We then run the target LLM again on the backtranslated prompt, and we refuse the original prompt if the model refuses the backtranslated prompt. We explain that the proposed defense provides several benefits on its effectiv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ID-XCB&#65292;&#36825;&#26159;&#39318;&#20010;&#25968;&#25454;&#26080;&#20851;&#21435;&#20559;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#32531;&#35299;&#27169;&#22411;&#23545;&#24341;&#21457;&#20559;&#35265;&#30340;&#35789;&#30340;&#20851;&#27880;&#30340;&#21516;&#26102;&#25552;&#39640;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16458</link><description>&lt;p&gt;
D-XCB&#65306;&#25968;&#25454;&#26080;&#20851;&#21435;&#20559;&#26041;&#27861;&#65292;&#29992;&#20110;&#20844;&#24179;&#20934;&#30830;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
D-XCB: Data-independent Debiasing for Fair and Accurate Transformer-based Cyberbullying Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16458
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ID-XCB&#65292;&#36825;&#26159;&#39318;&#20010;&#25968;&#25454;&#26080;&#20851;&#21435;&#20559;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#32531;&#35299;&#27169;&#22411;&#23545;&#24341;&#21457;&#20559;&#35265;&#30340;&#35789;&#30340;&#20851;&#27880;&#30340;&#21516;&#26102;&#25552;&#39640;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39554;&#20154;&#35805;&#26159;&#25910;&#38598;&#21253;&#21547;&#32593;&#32476;&#27450;&#20940;&#20107;&#20214;&#25968;&#25454;&#30340;&#24120;&#29992;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#34913;&#37327;&#21644;&#20943;&#36731;&#30001;&#39554;&#20154;&#35805;&#21644;&#22240;&#27492;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#23548;&#33268;&#20107;&#20214;&#20043;&#38388;&#30340;&#34394;&#20551;&#20851;&#32852;&#20135;&#29983;&#30340;&#20559;&#35265;&#12290;&#22312;&#28436;&#31034;&#21644;&#37327;&#21270;&#36825;&#20123;&#20559;&#35265;&#20043;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ID-XCB&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25968;&#25454;&#26080;&#20851;&#21435;&#20559;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;&#25932;&#23545;&#35757;&#32451;&#12289;&#20559;&#35265;&#32422;&#26463;&#21644;&#21435;&#20559;&#24494;&#35843;&#26041;&#27861;&#65292;&#26088;&#22312;&#32531;&#35299;&#27169;&#22411;&#23545;&#24341;&#21457;&#20559;&#35265;&#30340;&#35789;&#30340;&#20851;&#27880;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#32593;&#32476;&#27450;&#20940;&#25968;&#25454;&#38598;&#19978;&#25506;&#35752;&#20102;ID-XCB&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#28040;&#34701;&#21644;&#27867;&#21270;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ID-XCB&#23398;&#20064;&#20102;&#24378;&#22823;&#30340;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#20559;&#35265;&#65292;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#20559;&#35265;&#32531;&#35299;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;&#20854;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16458v1 Announce Type: new  Abstract: Swear words are a common proxy to collect datasets with cyberbullying incidents. Our focus is on measuring and mitigating biases derived from spurious associations between swear words and incidents occurring as a result of such data collection strategies. After demonstrating and quantifying these biases, we introduce ID-XCB, the first data-independent debiasing technique that combines adversarial training, bias constraints and debias fine-tuning approach aimed at alleviating model attention to bias-inducing words without impacting overall model performance. We explore ID-XCB on two popular session-based cyberbullying datasets along with comprehensive ablation and generalisation studies. We show that ID-XCB learns robust cyberbullying detection capabilities while mitigating biases, outperforming state-of-the-art debiasing methods in both performance and bias mitigation. Our quantitative and qualitative analyses demonstrate its generalisab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;RetrievalQA&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;TA-ARE</title><link>https://arxiv.org/abs/2402.16457</link><description>&lt;p&gt;
RetrievalQA&#65306;&#35780;&#20272;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#29992;&#20110;&#30701;&#25991;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;RetrievalQA&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;TA-ARE
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;ARAG&#65289;&#26088;&#22312;&#21160;&#24577;&#30830;&#23450;&#26597;&#35810;&#26159;&#21542;&#38656;&#35201;&#26816;&#32034;&#65292;&#32780;&#19981;&#26159;&#26080;&#36873;&#25321;&#22320;&#26816;&#32034;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#30340;&#25928;&#29575;&#21644;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#35780;&#20272;ARAG&#26041;&#27861;&#26041;&#38754;&#22823;&#22810;&#34987;&#24573;&#35270;&#65292;&#23548;&#33268;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#26410;&#21463;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20934;&#65292;RetrievalQA&#65292;&#21253;&#21547;1,271&#20010;&#28085;&#30422;&#26032;&#19990;&#30028;&#21644;&#38271;&#23614;&#30693;&#35782;&#30340;&#30701;&#25991;&#38382;&#39064;&#12290;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#25152;&#38656;&#30340;&#30693;&#35782;&#19981;&#22312;LLM&#20013;&#65307;&#22240;&#27492;&#65292;&#24517;&#39035;&#26816;&#32034;&#22806;&#37096;&#20449;&#24687;&#26469;&#27491;&#30830;&#22238;&#31572;&#12290;&#36825;&#20351;&#24471;RetrievalQA&#25104;&#20026;&#35780;&#20272;&#29616;&#26377;ARAG&#26041;&#27861;&#30340;&#21512;&#36866;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#26657;&#20934;&#30340;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#38408;&#20540;&#35843;&#25972;&#65292;&#32780;&#26222;&#36890;&#25552;&#31034;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;LLMs&#20570;&#20986;&#21487;&#38752;&#30340;&#26816;&#32034;&#20915;&#31574;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;Time-Aware Adaptive Retrieval&#65288;TA-ARE&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16457v1 Announce Type: new  Abstract: Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet e
&lt;/p&gt;</description></item><item><title>ShieldLM&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#23433;&#20840;&#26816;&#27979;&#22120;&#65292;&#31526;&#21512;&#19968;&#33324;&#20154;&#31867;&#23433;&#20840;&#26631;&#20934;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#30340;&#26816;&#27979;&#35268;&#21017;&#65292;&#24182;&#25552;&#20379;&#20915;&#31574;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.16444</link><description>&lt;p&gt;
ShieldLM: &#20351;LLMs&#25104;&#20026;&#23545;&#40784;&#12289;&#21487;&#23450;&#21046;&#21644;&#21487;&#35299;&#37322;&#30340;&#23433;&#20840;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16444
&lt;/p&gt;
&lt;p&gt;
ShieldLM&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#23433;&#20840;&#26816;&#27979;&#22120;&#65292;&#31526;&#21512;&#19968;&#33324;&#20154;&#31867;&#23433;&#20840;&#26631;&#20934;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#30340;&#26816;&#27979;&#35268;&#21017;&#65292;&#24182;&#25552;&#20379;&#20915;&#31574;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#24615;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#65292;&#20294;&#22312;&#23545;LLMs&#30340;&#21709;&#24212;&#20013;&#26816;&#27979;&#23433;&#20840;&#38382;&#39064;&#30340;&#26041;&#27861;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#23545;&#40784;&#12289;&#21487;&#23450;&#21046;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ShieldLM&#65292;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#23433;&#20840;&#26816;&#27979;&#22120;&#65292;&#23427;&#19982;&#19968;&#33324;&#20154;&#31867;&#23433;&#20840;&#26631;&#20934;&#30456;&#31526;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#30340;&#26816;&#27979;&#35268;&#21017;&#65292;&#24182;&#20026;&#20854;&#20915;&#31574;&#25552;&#20379;&#35299;&#37322;&#12290;&#20026;&#20102;&#35757;&#32451;ShieldLM&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;14,387&#20010;&#26597;&#35810;-&#21709;&#24212;&#23545;&#30340;&#22823;&#22411;&#21452;&#35821;&#25968;&#25454;&#38598;&#65292;&#26681;&#25454;&#21508;&#31181;&#23433;&#20840;&#26631;&#20934;&#23545;&#21709;&#24212;&#30340;&#23433;&#20840;&#24615;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ShieldLM&#22312;&#22235;&#20010;&#27979;&#35797;&#38598;&#19978;&#36229;&#36234;&#20102;&#24378;&#22522;&#32447;&#65292;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#23450;&#21046;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#38500;&#20102;&#22312;&#26631;&#20934;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#22806;&#65292;ShieldLM&#36824;&#34987;&#35777;&#26126;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#20316;&#20026;&#20808;&#36827;LLMs&#30340;&#23433;&#20840;&#35780;&#20272;&#22120;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16444v1 Announce Type: new  Abstract: The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner. In this paper, we propose ShieldLM, an LLM-based safety detector, which aligns with general human safety standards, supports customizable detection rules, and provides explanations for its decisions. To train ShieldLM, we compile a large bilingual dataset comprising 14,387 query-response pairs, annotating the safety of responses based on various safety standards. Through extensive experiments, we demonstrate that ShieldLM surpasses strong baselines across four test sets, showcasing remarkable customizability and explainability. Besides performing well on standard detection datasets, ShieldLM has also been shown to be effective in real-world situations as a safety evaluator for advanced LLMs. We relea
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#21487;&#20197;&#35299;&#37322;&#20854;&#22810;&#35821;&#33021;&#21147;&#65292;&#36890;&#36807;&#25552;&#20986;&#35821;&#35328;&#28608;&#27963;&#27010;&#29575;&#29109;&#65288;LAPE&#65289;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#30340;&#33021;&#21147;&#20027;&#35201;&#30001;&#23569;&#37327;&#31070;&#32463;&#20803;&#20915;&#23450;&#12290;</title><link>https://arxiv.org/abs/2402.16438</link><description>&lt;p&gt;
&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#35821;&#33021;&#21147;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16438
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#21487;&#20197;&#35299;&#37322;&#20854;&#22810;&#35821;&#33021;&#21147;&#65292;&#36890;&#36807;&#25552;&#20986;&#35821;&#35328;&#28608;&#27963;&#27010;&#29575;&#29109;&#65288;LAPE&#65289;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#30340;&#33021;&#21147;&#20027;&#35201;&#30001;&#23569;&#37327;&#31070;&#32463;&#20803;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#22810;&#35821;&#33021;&#21147;&#65292;&#21363;&#20351;&#26410;&#32463;&#36807;&#19987;&#38376;&#31574;&#21010;&#30340;&#22810;&#35821;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#12290;&#35299;&#37322;LLMs&#22788;&#29702;&#22810;&#35821;&#25991;&#26412;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;LLMs&#20013;Transformer&#26550;&#26500;&#30340;&#26500;&#25104;&#65292;&#20197;&#25214;&#20986;&#35821;&#35328;&#29305;&#23450;&#21306;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;&#35821;&#35328;&#28608;&#27963;&#27010;&#29575;&#29109;&#65288;LAPE&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;LLMs&#20869;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#12290;&#22522;&#20110;LAPE&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#65292;&#21363;LLaMA-2&#21644;BLOOM&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#30340;&#33021;&#21147;&#20027;&#35201;&#26159;&#30001;&#19968;&#23567;&#37096;&#20998;&#31070;&#32463;&#20803;&#20915;&#23450;&#30340;&#65292;&#36825;&#20123;&#31070;&#32463;&#20803;&#20027;&#35201;&#20301;&#20110;&#27169;&#22411;&#30340;&#39030;&#37096;&#21644;&#24213;&#37096;&#23618;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36873;&#25321;&#24615;&#28608;&#27963;&#25110;&#20572;&#29992;&#31070;&#32463;&#20803;&#26469;&#8220;&#24341;&#23548;&#8221;LLMs&#30340;&#36755;&#20986;&#35821;&#35328;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16438v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora. It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts. In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions. Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs. Based on LAPE, we conduct comprehensive experiments on two representative LLMs, namely LLaMA-2 and BLOOM. Our findings indicate that LLMs' proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models' top and bottom layers. Furthermore, we showcase the feasibility to "steer" the output language of LLMs by selectively activating or deactivatin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20195;&#30721;&#39118;&#26684;&#25351;&#20196;&#26367;&#25442;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#26356;&#31934;&#30830;&#30340;&#25351;&#20196;&#24182;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#26223;&#19979;&#36890;&#36807;&#32452;&#21512;&#24178;&#20928;&#21644;&#23545;&#25239;&#26679;&#26412;&#21152;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16431</link><description>&lt;p&gt;
RoCoIns: &#36890;&#36807;&#20195;&#30721;&#39118;&#26684;&#25351;&#20196;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16431
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#39118;&#26684;&#25351;&#20196;&#26367;&#25442;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#26356;&#31934;&#30830;&#30340;&#25351;&#20196;&#24182;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#26223;&#19979;&#36890;&#36807;&#32452;&#21512;&#24178;&#20928;&#21644;&#23545;&#25239;&#26679;&#26412;&#21152;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;LLMs&#22312;&#25509;&#21463;&#32467;&#21512;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#30340;&#25351;&#20196;&#26102;&#40065;&#26834;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#20174;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#33719;&#24471;&#21551;&#31034;&#65292;LLMs&#23545;&#25351;&#20196;&#35774;&#35745;&#25935;&#24863;&#65292;&#25105;&#20204;&#21033;&#29992;&#20195;&#30721;&#39118;&#26684;&#30340;&#25351;&#20196;&#26367;&#25442;&#20102;&#36890;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#36825;&#20123;&#25351;&#20196;&#26356;&#20855;&#32467;&#26500;&#24615;&#21644; less&#27169;&#31946;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#36716;&#25442;&#65292;&#25105;&#20204;&#20026;LLMs&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#20196;&#65292;&#22686;&#24378;&#20102;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24178;&#20928;&#21644;&#23545;&#25239;&#26679;&#26412;&#26469;&#32452;&#25104;&#19978;&#19979;&#25991;&#28436;&#31034;&#65288;\textit{&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;&#26041;&#27861;&#65289;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20843;&#20010;&#40065;&#26834;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;LLMs&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16431v1 Announce Type: new  Abstract: Large Language Models (LLMs) have showcased remarkable capabilities in following human instructions. However, recent studies have raised concerns about the robustness of LLMs when prompted with instructions combining textual adversarial samples. In this paper, drawing inspiration from recent works that LLMs are sensitive to the design of the instructions, we utilize instructions in code style, which are more structural and less ambiguous, to replace typically natural language instructions. Through this conversion, we provide LLMs with more precise instructions and strengthen the robustness of LLMs. Moreover, under few-shot scenarios, we propose a novel method to compose in-context demonstrations using both clean and adversarial samples (\textit{adversarial context method}) to further boost the robustness of the LLMs. Experiments on eight robustness datasets show that our method consistently outperforms prompting LLMs with natural languag
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLM&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#39044;&#27979;&#22823;&#23398;&#35838;&#31243;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#36866;&#24212;SDGs&#12290;</title><link>https://arxiv.org/abs/2402.16420</link><description>&lt;p&gt;
&#20351;&#29992;&#35838;&#31243;&#25551;&#36848;&#39044;&#27979;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631; - &#20174;LLMs&#21040;&#20256;&#32479;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16420
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLM&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#39044;&#27979;&#22823;&#23398;&#35838;&#31243;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#36866;&#24212;SDGs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#39044;&#27979;&#22823;&#23398;&#35838;&#31243;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDG&#65289;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;PaLM 2&#30340;LLM&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#23558;&#21547;&#26377;&#22024;&#26434;&#20154;&#24037;&#32534;&#20889;&#30340;&#35838;&#31243;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#20102;&#20960;&#20010;&#19981;&#21516;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#22823;&#23398;&#35838;&#31243;&#30340;SDG&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#36866;&#24212;SDG&#30340;&#22823;&#23398;&#23618;&#38754;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;0.786&#30340;F1&#20998;&#25968;&#30340;BART&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16420v1 Announce Type: new  Abstract: We present our work on predicting United Nations sustainable development goals (SDG) for university courses. We use an LLM named PaLM 2 to generate training data given a noisy human-authored course description input as input. We use this data to train several different smaller language models to predict SDGs for university courses. This work contributes to better university level adaptation of SDGs. The best performing model in our experiments was BART with an F1-score of 0.786.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20020;&#24202;&#35797;&#39564;&#26041;&#26696;&#25991;&#20214;&#37096;&#20998;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21487;&#26174;&#30528;&#25552;&#39640;LLM&#30340;&#25776;&#20889;&#36136;&#37327;&#65292;&#23545;LLMs&#22312;&#20020;&#24202;&#35797;&#39564;&#30456;&#20851;&#20889;&#20316;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.16406</link><description>&lt;p&gt;
&#20174; RAGs &#21040;&#36130;&#23500;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20020;&#24202;&#35797;&#39564;&#25776;&#20889;&#25991;&#20214;
&lt;/p&gt;
&lt;p&gt;
From RAGs to riches: Using large language models to write documents for clinical trials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20020;&#24202;&#35797;&#39564;&#26041;&#26696;&#25991;&#20214;&#37096;&#20998;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21487;&#26174;&#30528;&#25552;&#39640;LLM&#30340;&#25776;&#20889;&#36136;&#37327;&#65292;&#23545;LLMs&#22312;&#20020;&#24202;&#35797;&#39564;&#30456;&#20851;&#20889;&#20316;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#38656;&#35201;&#25776;&#20889;&#22823;&#37327;&#25991;&#20214;&#65292;&#21253;&#25324;&#21327;&#35758;&#12289;&#21516;&#24847;&#20070;&#12289;&#20020;&#24202;&#30740;&#31350;&#25253;&#21578;&#31561;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#24555;&#36895;&#29983;&#25104;&#36825;&#20123;&#25991;&#20214;&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#36755;&#20986;&#36136;&#37327;&#23384;&#22312;&#25285;&#24551;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;LLMs&#22312;&#29983;&#25104;&#20854;&#20013;&#19968;&#20010;&#25991;&#20214;&#65288;&#20020;&#24202;&#35797;&#39564;&#26041;&#26696;&#65289;&#30340;&#37096;&#20998;&#20869;&#23481;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#25104;&#30340;LLM&#22312;&#20869;&#23481;&#30456;&#20851;&#24615;&#21644;&#26415;&#35821;&#20351;&#29992;&#27491;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#21512;&#29702;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#65306;&#29305;&#21035;&#26159;&#20020;&#24202;&#24605;&#32500;&#21644;&#36923;&#36753;&#65292;&#20197;&#21450;&#21442;&#32771;&#25991;&#29486;&#30340;&#36866;&#24403;&#20351;&#29992;&#12290;&#20026;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26469;&#25552;&#31034;LLM&#20351;&#29992;&#20934;&#30830;&#30340;&#26368;&#26032;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;RAG&#65292;LLM&#30340;&#25776;&#20889;&#36136;&#37327;&#26174;&#33879;&#25552;&#39640;&#65292;&#36825;&#23545;LLMs&#22312;&#20020;&#24202;&#35797;&#39564;&#30456;&#20851;&#20889;&#20316;&#20013;&#30340;&#23454;&#38469;&#21487;&#29992;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16406v1 Announce Type: new  Abstract: Clinical trials require numerous documents to be written -- protocols, consent forms, clinical study reports and others. Large language models (LLMs) offer the potential to rapidly generate first versions of these documents, however there are concerns about the quality of their output Here we report an evaluation of LLMs in generating parts of one such document, clinical trial protocols. We find that an offthe-shelf LLM delivers reasonable results, especially when assessing content relevance and the correct use of terminology. However, deficiencies remain: specifically clinical thinking and logic, and appropriate use of references. To improve performance, we used retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date information. As a result of using RAG, the writing quality of the LLM improves substantially, which has implications for the practical useability of LLMs in clinical trial-related writing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;MoZIP&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#20135;&#26435;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;IP-oriented&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;MoZi&#65292;&#23454;&#39564;&#35777;&#26126;MoZi&#22312;MoZIP&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.16389</link><description>&lt;p&gt;
MoZIP&#65306;&#35780;&#20272;&#30693;&#35782;&#20135;&#26435;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;MoZIP&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#20135;&#26435;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;IP-oriented&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;MoZi&#65292;&#23454;&#39564;&#35777;&#26126;MoZi&#22312;MoZIP&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23545;LLMs&#22312;&#29305;&#23450;&#39046;&#22495;&#65288;&#20363;&#22914;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#39046;&#22495;&#65289;&#30340;&#34920;&#29616;&#36824;&#20102;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21363;&#31532;&#19968;&#20010;&#38754;&#21521;&#30693;&#35782;&#20135;&#26435;&#39046;&#22495;&#30340;&#22810;&#35821;&#35328;&#26234;&#21147;&#20135;&#26435;&#27979;&#39564;(MoZIP)&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#30693;&#35782;&#20135;&#26435;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;MoZIP&#22522;&#20934;&#21253;&#25324;&#19977;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65306;&#30693;&#35782;&#20135;&#26435;&#22810;&#39033;&#36873;&#25321;&#27979;&#39564;&#65288;IPQuiz&#65289;&#12289;&#30693;&#35782;&#20135;&#26435;&#38382;&#31572;&#65288;IPQA&#65289;&#21644;&#19987;&#21033;&#21305;&#37197;&#65288;PatentMatch&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#38754;&#21521;&#30693;&#35782;&#20135;&#26435;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#31216;&#20026;MoZi&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;BLOOMZ&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;IP&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;MoZIP&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;MoZi&#27169;&#22411;&#21644;&#22235;&#31181;&#30693;&#21517;LLMs&#65288;&#21363;BLOOMZ&#12289;BELLE&#12289;ChatGLM&#21644;ChatGPT&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MoZi&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16389v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance in various natural language processing (NLP) tasks. However, there is limited understanding of how well LLMs perform in specific domains (e.g, the intellectual property (IP) domain). In this paper, we contribute a new benchmark, the first Multilingual-oriented quiZ on Intellectual Property (MoZIP), for the evaluation of LLMs in the IP domain. The MoZIP benchmark includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question answering (IPQA), and patent matching (PatentMatch). In addition, we also develop a new IP-oriented multilingual large language model (called MoZi), which is a BLOOMZ-based model that has been supervised fine-tuned with multilingual IP-related text data. We evaluate our proposed MoZi model and four well-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP benchmark. Experimental results demonstrate that MoZi outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38450;&#33539;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#23475;&#24494;&#35843;&#25915;&#20987;&#30340;&#20813;&#30123;&#26465;&#20214;&#38598;&#65292;&#20197;&#24110;&#21161;&#29702;&#35299;&#22914;&#20309;&#26500;&#24314;&#21644;&#34913;&#37327;&#26410;&#26469;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2402.16382</link><description>&lt;p&gt;
&#38450;&#33539;&#26377;&#23475;&#24494;&#35843;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Immunization against harmful fine-tuning attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38450;&#33539;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#23475;&#24494;&#35843;&#25915;&#20987;&#30340;&#20813;&#30123;&#26465;&#20214;&#38598;&#65292;&#20197;&#24110;&#21161;&#29702;&#35299;&#22914;&#20309;&#26500;&#24314;&#21644;&#34913;&#37327;&#26410;&#26469;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35843;&#25972;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#32416;&#27491;&#39044;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#19981;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20851;&#27880;&#24573;&#30053;&#20102;&#21478;&#19968;&#31181;&#19981;&#19968;&#33268;&#30340;&#26469;&#28304;&#65306;&#24694;&#24847;&#34892;&#20026;&#32773;&#21487;&#33021;&#26377;&#24847;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#20197;&#23454;&#29616;&#26377;&#23475;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#28304;&#20110;&#23545;&#40784;&#35268;&#36991;&#21644;&#24494;&#35843;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#20316;&#21697;&#32570;&#20047;&#26377;&#25928;&#38450;&#24481;&#26465;&#20214;&#30340;&#28165;&#26224;&#21576;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25239;LLMs&#20013;&#26377;&#23475;&#24494;&#35843;&#30340;&#26377;&#25928;&#38450;&#24481;&#26465;&#20214;&#38598;&#65292;&#31216;&#20026;&#8220;&#20813;&#30123;&#26465;&#20214;&#8221;&#65292;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#20102;&#35299;&#22914;&#20309;&#26500;&#24314;&#21644;&#34913;&#37327;&#26410;&#26469;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;&#21033;&#29992;&#36825;&#31181;&#38450;&#24481;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19981;&#21516;&#30740;&#31350;&#26041;&#21521;&#30340;&#32508;&#21512;&#65292;&#20197;&#38450;&#27490;&#26377;&#23475;&#24494;&#35843;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#36825;&#20123;&#26465;&#20214;&#30340;&#26089;&#26399;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16382v1 Announce Type: new  Abstract: Approaches to aligning large language models (LLMs) with human values has focused on correcting misalignment that emerges from pretraining. However, this focus overlooks another source of misalignment: bad actors might purposely fine-tune LLMs to achieve harmful goals. In this paper, we present an emerging threat model that has arisen from alignment circumvention and fine-tuning attacks. However, lacking in previous works is a clear presentation of the conditions for effective defence. We propose a set of conditions for effective defence against harmful fine-tuning in LLMs called "Immunization conditions," which help us understand how we would construct and measure future defences. Using this formal framework for defence, we offer a synthesis of different research directions that might be persued to prevent harmful fine-tuning attacks and provide a demonstration of how to use these conditions experimentally showing early results of using
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#29305;&#23450;&#30340;&#35821;&#38899;&#20998;&#24067;&#25972;&#21512;&#12289;&#33258;&#21160;&#21270;&#24405;&#21046;&#36807;&#31243;&#12289;&#33258;&#21160;&#21270;&#21644;&#20154;&#26426;&#21327;&#20316;&#30340;&#24405;&#38899;&#36136;&#37327;&#20445;&#35777;&#20197;&#21450;&#24405;&#38899;&#26684;&#24335;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.16380</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#36716;&#35821;&#38899;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#24320;&#28304;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#29305;&#23450;&#30340;&#35821;&#38899;&#20998;&#24067;&#25972;&#21512;&#12289;&#33258;&#21160;&#21270;&#24405;&#21046;&#36807;&#31243;&#12289;&#33258;&#21160;&#21270;&#21644;&#20154;&#26426;&#21327;&#20316;&#30340;&#24405;&#38899;&#36136;&#37327;&#20445;&#35777;&#20197;&#21450;&#24405;&#38899;&#26684;&#24335;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#23545;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#22522;&#20110;&#35821;&#38899;&#30340;&#25216;&#26415;&#12290;&#38543;&#30528;&#20869;&#23481;&#21019;&#20316;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20869;&#23481;&#65292;&#32763;&#35793;&#21644;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#24517;&#19981;&#21487;&#23569;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#23545;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#24613;&#20999;&#38656;&#27714;&#12290;&#26412;&#20316;&#21697;&#30340;&#36129;&#29486;&#22810;&#26041;&#38754;&#65292;&#21253;&#25324;&#65306;&#23558;&#35821;&#35328;&#29305;&#23450;&#30340;&#35821;&#38899;&#20998;&#24067;&#25972;&#21512;&#21040;&#26679;&#26412;&#36873;&#25321;&#20013;&#65292;&#33258;&#21160;&#21270;&#24405;&#21046;&#36807;&#31243;&#65292;&#33258;&#21160;&#21270;&#21644;&#20154;&#26426;&#21327;&#20316;&#30340;&#24405;&#38899;&#36136;&#37327;&#20445;&#35777;&#65292;&#20197;&#21450;&#22788;&#29702;&#24405;&#38899;&#20197;&#28385;&#36275;&#25351;&#23450;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16380v1 Announce Type: cross  Abstract: Data availability is crucial for advancing artificial intelligence applications, including voice-based technologies. As content creation, particularly in social media, experiences increasing demand, translation and text-to-speech (TTS) technologies have become essential tools. Notably, the performance of these TTS technologies is highly dependent on the quality of the training data, emphasizing the mutual dependence of data availability and technological progress. This paper introduces an end-to-end tool to generate high-quality datasets for text-to-speech (TTS) models to address this critical need for high-quality data. The contributions of this work are manifold and include: the integration of language-specific phoneme distribution into sample selection, automation of the recording process, automated and human-in-the-loop quality assurance of recordings, and processing of recordings to meet specified formats. The proposed application
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21517;&#20026;TER&#30340;&#31995;&#32479;LLM&#33258;&#26657;&#27491;&#32763;&#35793;&#26694;&#26550;&#65292;&#25104;&#21151;&#24110;&#21161;LLMs&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16379</link><description>&lt;p&gt;
&#29992;&#31995;&#32479;&#33258;&#26657;&#27491;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Improving LLM-based Machine Translation with Systematic Self-Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16379
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21517;&#20026;TER&#30340;&#31995;&#32479;LLM&#33258;&#26657;&#27491;&#32763;&#35793;&#26694;&#26550;&#65292;&#25104;&#21151;&#24110;&#21161;LLMs&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#20180;&#32454;&#35780;&#20272;&#21457;&#29616;&#65292;LLMs&#29983;&#25104;&#30340;&#32763;&#35793;&#20173;&#28982;&#21253;&#21547;&#22810;&#20010;&#38169;&#35823;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23558;&#36825;&#31181;&#38169;&#35823;&#20449;&#24687;&#21453;&#39304;&#21040;LLMs&#20013;&#21487;&#20197;&#23454;&#29616;&#33258;&#26657;&#27491;&#65292;&#24182;&#25913;&#21892;&#32763;&#35793;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;TER&#30340;&#31995;&#32479;LLM&#33258;&#26657;&#27491;&#32763;&#35793;&#26694;&#26550;&#65292;&#20195;&#34920;&#20102;&#22312;&#36825;&#19968;&#26041;&#21521;&#19978;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;&#25105;&#20204;&#30340;&#33258;&#26657;&#27491;&#26694;&#26550;&#25104;&#21151;&#22320;&#24110;&#21161;LLMs&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#19981;&#31649;&#26159;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#36824;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#36824;&#26159;&#22260;&#32469;&#20854;&#20182;&#35821;&#35328;&#65307;2&#65289;TER&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#23637;&#31034;&#20986;&#26356;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65307;3&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16379v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors. Importantly, feeding back such error information into the LLMs can lead to self-correction and result in improved translation performance. Motivated by these insights, we introduce a systematic LLM-based self-correcting translation framework, named TER, which stands for Translate, Estimate, and Refine, marking a significant step forward in this direction. Our findings demonstrate that 1) our self-correction framework successfully assists LLMs in improving their translation quality across a wide range of languages, whether it's from high-resource languages to low-resource ones or whether it's English-centric or centered around other languages; 2) TER exhibits superior systematicity and interpretability compared to previous methods; 3)
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#21407;&#22987;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#19987;&#23478;&#28151;&#21512;&#26550;&#26500;&#65292;&#30740;&#31350;&#20102;LLMs&#30340;&#22810;&#35821;&#35328;&#28608;&#27963;&#27169;&#24335;&#65292;&#21457;&#29616;&#23384;&#22312;&#38750;&#29305;&#23450;&#35821;&#35328;&#30340;&#31070;&#32463;&#20803;&#21644;&#29305;&#23450;&#35821;&#35328;&#28608;&#27963;&#31070;&#32463;&#20803;&#65292;&#20197;&#21450;&#39640;&#39057;&#28608;&#27963;&#31070;&#32463;&#20803;&#21487;&#20197;&#21152;&#36895;&#25512;&#26029;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16367</link><description>&lt;p&gt;
&#25581;&#31034;&#24052;&#21035;&#22612;&#65306;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#30340;&#22810;&#35821;&#35328;&#28608;&#27963;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16367
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#21407;&#22987;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#19987;&#23478;&#28151;&#21512;&#26550;&#26500;&#65292;&#30740;&#31350;&#20102;LLMs&#30340;&#22810;&#35821;&#35328;&#28608;&#27963;&#27169;&#24335;&#65292;&#21457;&#29616;&#23384;&#22312;&#38750;&#29305;&#23450;&#35821;&#35328;&#30340;&#31070;&#32463;&#20803;&#21644;&#29305;&#23450;&#35821;&#35328;&#28608;&#27963;&#31070;&#32463;&#20803;&#65292;&#20197;&#21450;&#39640;&#39057;&#28608;&#27963;&#31070;&#32463;&#20803;&#21487;&#20197;&#21152;&#36895;&#25512;&#26029;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#31361;&#30772;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#26102;&#30340;&#26426;&#21046;&#20173;&#28982;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#22810;&#35821;&#35328;&#28608;&#27963;&#27169;&#24335;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36716;&#21270;&#20026;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26550;&#26500;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22788;&#29702;&#21508;&#31181;&#35821;&#35328;&#26102;&#19987;&#23478;&#30340;&#28608;&#27963;&#27169;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#28608;&#27963;&#27169;&#24335;&#22312;&#35821;&#35328;&#23478;&#26063;&#23618;&#38754;&#19978;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#38750;&#29305;&#23450;&#35821;&#35328;&#30340;&#31070;&#32463;&#20803;&#20197;&#21450;&#29305;&#23450;&#35821;&#35328;&#28608;&#27963;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#12290;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#29978;&#33267;&#23637;&#31034;&#20102;&#20165;&#21033;&#29992;&#39640;&#39057;&#28608;&#27963;&#31070;&#32463;&#20803;&#21487;&#20197;&#21152;&#36895;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;LLMs&#30340;&#22810;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#65292;&#24182;&#22312;&#25351;&#23548;&#22810;&#35821;&#35328;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16367v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have achieved tremendous breakthroughs in the field of language processing, yet their mechanisms in processing multiple languages remain agnostic. Therefore, in this work we study the multilingual activation patterns of LLMs. By transforming the original Large Language Models (LLMs) into a Mixture of Experts (MoE) architecture, we analyze the expert activation patterns when processing various languages and demonstrate the connections of these activation patterns at the level of language families. We discover the existence of non-language-specific neurons as well as language-specific activation neurons. Further exploration even showcases that merely leveraging high-frequency activation neurons can accelerate inference while maintaining comparable performance. These findings shed light on the LLMs' multilingual processing mechanism, and are of significant importance in guiding the multilingual trainin
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#33258;&#28982;&#31354;&#38388;&#25551;&#36848;&#36827;&#34892;&#22810;&#23610;&#24230;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#36890;&#36807;&#33719;&#30693;&#22320;&#22270;&#30693;&#35782;&#24471;&#21040;&#30340;&#25551;&#36848;&#33021;&#22815;&#25552;&#20379;&#29615;&#22659;&#30340;&#25972;&#20307;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.16364</link><description>&lt;p&gt;
&#20174;&#21738;&#37324;&#20986;&#21457;&#65311;&#26469;&#33258;&#33258;&#28982;&#31354;&#38388;&#25551;&#36848;&#20013;&#30340;&#22810;&#23610;&#24230;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16364
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#33258;&#28982;&#31354;&#38388;&#25551;&#36848;&#36827;&#34892;&#22810;&#23610;&#24230;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#36890;&#36807;&#33719;&#30693;&#22320;&#22270;&#30693;&#35782;&#24471;&#21040;&#30340;&#25551;&#36848;&#33021;&#22815;&#25552;&#20379;&#29615;&#22659;&#30340;&#25972;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29992;&#33258;&#28982;&#35821;&#35328;&#20256;&#36798;&#36335;&#32447;&#26102;&#65292;&#8220;&#33719;&#24471;&#30340;&#31354;&#38388;&#30693;&#35782;&#8221;&#27010;&#24565;&#23545;&#22320;&#29702;&#20449;&#24687;&#26816;&#32034;&#65288;GIR&#65289;&#21644;&#31354;&#38388;&#35748;&#30693;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23548;&#33322;&#30740;&#31350;&#32463;&#24120;&#24573;&#35270;&#36825;&#31181;&#33719;&#24471;&#30693;&#35782;&#23545;&#25991;&#26412;&#25551;&#36848;&#30340;&#24433;&#21709;&#12290;&#24403;&#21069;&#23548;&#33322;&#30740;&#31350;&#38598;&#20013;&#22312;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#26412;&#22320;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#8220;&#23427;&#23558;&#22312;&#24744;&#30340;&#21491;&#36793;&#8221;&#65289;&#65292;&#36825;&#20123;&#25551;&#36848;&#38656;&#35201;&#23545;&#20195;&#29702;&#20154;&#30340;&#26412;&#22320;&#30693;&#35273;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#22320;&#22270;&#33719;&#24471;&#30340;&#30693;&#35782;&#22522;&#30784;&#19978;&#30340;&#25551;&#36848;&#25552;&#20379;&#20102;&#29615;&#22659;&#30340;&#25972;&#20307;&#35270;&#22270;&#65292;&#24182;&#25429;&#25417;&#20102;&#20854;&#24635;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16364v1 Announce Type: new  Abstract: When communicating routes in natural language, the concept of {\em acquired spatial knowledge} is crucial for geographic information retrieval (GIR) and in spatial cognitive research. However, NLP navigation studies often overlook the impact of such acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., `it will be on your right') that require reasoning over the agent's local perception. These instructions are typically given as a sequence of steps, with each action-step explicitly mentioning and being followed by a landmark that the agent can use to verify they are on the right path (e.g., `turn right and then you will see...'). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure. These instructions (e.g., `it is south of Central Park and a block north of a police station') are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.16363</link><description>&lt;p&gt;
LLM&#25512;&#26029;&#25581;&#31034;&#65306;&#35843;&#26597;&#19982;Roofline&#27169;&#22411;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLM Inference Unveiled: Survey and Roofline Model Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#20379;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#29420;&#29305;&#32467;&#21512;&#12290;&#34429;&#28982;&#35813;&#39046;&#22495;&#24050;&#32463;&#25193;&#23637;&#24182;&#20805;&#28385;&#27963;&#21147;&#65292;&#20294;&#33267;&#20170;&#36824;&#27809;&#26377;&#19968;&#20010;&#31616;&#26126;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;LLM&#25512;&#26029;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#28165;&#26224;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#19981;&#20165;&#24635;&#32467;&#20102;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#36824;&#22522;&#20110;Roofline&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#12290;&#36825;&#19968;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#35782;&#21035;LLM&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20174;&#32780;&#20026;&#37096;&#32626;LLM&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27719;&#24635;&#20102;&#39640;&#25928;LLM&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20851;&#38190;&#39046;&#22495;&#65292;&#27604;&#22914;&#26435;&#37325;&#20248;&#21270;&#65288;&#22914;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;Transformer-based&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#26032;&#39062;&#30340;&#20998;&#23618;&#27491;&#21017;&#21270;Dropout&#65288;LR-Drop&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#35757;&#32451;&#31574;&#30053;&#36880;&#23618;&#23545;&#27599;&#20010;Transformer&#23618;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#38544;&#34255;&#29366;&#24577;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#30697;&#38453;&#21644;&#36755;&#20986;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16361</link><description>&lt;p&gt;
&#20998;&#23618;&#27491;&#21017;&#21270;Dropout&#29992;&#20110;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Layer-wise Regularized Dropout for Neural Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;Transformer-based&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#26032;&#39062;&#30340;&#20998;&#23618;&#27491;&#21017;&#21270;Dropout&#65288;LR-Drop&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#35757;&#32451;&#31574;&#30053;&#36880;&#23618;&#23545;&#27599;&#20010;Transformer&#23618;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#38544;&#34255;&#29366;&#24577;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#30697;&#38453;&#21644;&#36755;&#20986;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#27969;&#34892;&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;dropout&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#19981;&#21487;&#25110;&#32570;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;dropout&#38543;&#26426;&#24615;&#24341;&#36215;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#19981;&#19968;&#33268;&#24615;&#65292;&#19968;&#20123;&#30740;&#31350;&#37319;&#29992;&#19968;&#33268;&#24615;&#35757;&#32451;&#26469;&#23545;&#36755;&#20986;&#23618;&#30340;dropout&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#27491;&#21017;&#21270;Dropout&#65288;LR-Drop&#65289;&#65292;&#19987;&#20026;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LR-Drop&#20351;&#29992;&#23618;&#27425;&#19968;&#33268;&#24615;&#35757;&#32451;&#31574;&#30053;&#65292;&#36880;&#23618;&#23545;&#27599;&#20010;Transformer&#23618;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#36890;&#36807;dropout&#37319;&#26679;&#30340;&#20004;&#20010;&#23402;&#29983;&#23376;&#27169;&#22411;&#65292;&#28982;&#21518;LR-Drop&#24378;&#21046;&#20351;&#20004;&#20010;&#23402;&#29983;&#23376;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#30697;&#38453;&#21644;&#36755;&#20986;&#20998;&#24067;&#20445;&#25345;&#19968;&#33268;&#12290;&#25152;&#25552;&#20986;&#30340;LR-Drop&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#8220;&#33258;&#33976;&#39311;&#8221;&#26694;&#26550;&#65292;&#20854;&#20013;dropout&#29983;&#25104;&#30340;&#27599;&#20010;&#23376;&#27169;&#22411;&#37117;&#26159;&#21478;&#19968;&#20010;&#30340;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16361v1 Announce Type: cross  Abstract: Among the various pre-trained neural language models that are popular today, dropout is already an indispensable regularization technique. To solve the inconsistency between training and inference caused by the randomness of dropout, some studies use consistency training to regularize dropout at the output layer. In this paper, we propose a novel Layer-wise Regularized Dropout (LR-Drop), which is specially designed for Transformer-based Language models. Specifically, LR-Drop layer-wise regularizes each Transformer layer using the consistency training strategy. Each training sample passes through the two siamese sub-models sampled by dropout, and then LR-Drop forces the hidden states, multi-head attention matrices, and output distribution of the two siamese sub-models to be consistent. The proposed LR-Drop can be regarded as a "self-distillation" framework, in which each sub-model generated by dropout is the other's "teacher" model and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16358</link><description>&lt;p&gt;
&#19968;&#20010;&#25972;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#29992;&#20110;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Integrated Data Processing Framework for Pretraining Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16358
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#32463;&#24120;&#38656;&#35201;&#25163;&#21160;&#20174;&#19981;&#21516;&#26469;&#28304;&#31574;&#21010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#27599;&#20010;&#25968;&#25454;&#23384;&#20648;&#24211;&#24320;&#21457;&#19987;&#38376;&#30340;&#25968;&#25454;&#28165;&#27927;&#27969;&#31243;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#36825;&#19968;&#36807;&#31243;&#37325;&#22797;&#32780;&#32321;&#29712;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#22788;&#29702;&#27169;&#22359;&#21253;&#25324;&#19968;&#31995;&#21015;&#19981;&#21516;&#31890;&#24230;&#27700;&#24179;&#30340;&#25805;&#20316;&#31526;&#65292;&#32780;&#20998;&#26512;&#27169;&#22359;&#25903;&#25345;&#23545;&#31934;&#28860;&#25968;&#25454;&#36827;&#34892;&#25506;&#26597;&#21644;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26131;&#20110;&#20351;&#29992;&#19988;&#39640;&#24230;&#28789;&#27963;&#12290;&#22312;&#36825;&#31687;&#28436;&#31034;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#24182;&#23637;&#31034;&#23427;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#19982;ChatGPT&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#31471;&#21040;&#31471;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16358v1 Announce Type: cross  Abstract: The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#24341;&#20837;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#26469;&#24341;&#23548;&#36825;&#19968;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#24182;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16354</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#21464;&#20998;&#25512;&#26029;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Language-guided Skill Learning with Temporal Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24341;&#23548;&#30340;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#24341;&#20837;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#26469;&#24341;&#23548;&#36825;&#19968;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21152;&#36895;&#23398;&#20064;&#24182;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#21457;&#29616;&#25216;&#33021;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25552;&#20986;&#36712;&#36857;&#30340;&#21021;&#22987;&#20998;&#21106;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#20998;&#23618;&#21464;&#20998;&#25512;&#26029;&#26694;&#26550;&#23558;LLM&#29983;&#25104;&#30340;&#20998;&#21106;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#36890;&#36807;&#21512;&#24182;&#36712;&#36857;&#27573;&#26469;&#21457;&#29616;&#21487;&#37325;&#29992;&#30340;&#25216;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25511;&#21046;&#21387;&#32553;&#21644;&#21487;&#37325;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#30340;&#26032;&#36741;&#21161;&#30446;&#26631;&#65292;&#24110;&#21161;&#24341;&#23548;&#36825;&#31181;&#25216;&#33021;&#21457;&#29616;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;Agent&#33021;&#22815;&#21457;&#29616;&#26377;&#21161;&#20110;&#21152;&#36895;&#23398;&#20064;&#30340;&#25216;&#33021;&#65292;&#22312;BabyAI&#65288;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#23548;&#33322;&#29615;&#22659;&#65289;&#20197;&#21450;ALFRED&#65288;&#19968;&#20010;&#23478;&#24237;&#27169;&#25311;&#29615;&#22659;&#65289;&#30340;&#26032;&#38271;&#26399;&#20219;&#21153;&#20013;&#32988;&#36807;&#22522;&#32447;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16354v1 Announce Type: cross  Abstract: We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.
&lt;/p&gt;</description></item><item><title>MathGenie&#36890;&#36807;&#38382;&#39064;&#21453;&#21521;&#32763;&#35793;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21019;&#36896;&#20102;&#19968;&#20010;&#23478;&#26063;&#21270;&#30340;&#27169;&#22411;&#31995;&#21015;MathGenieLM&#12290;</title><link>https://arxiv.org/abs/2402.16352</link><description>&lt;p&gt;
MathGenie: &#20351;&#29992;&#38382;&#39064;&#21453;&#21521;&#32763;&#35793;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16352
&lt;/p&gt;
&lt;p&gt;
MathGenie&#36890;&#36807;&#38382;&#39064;&#21453;&#21521;&#32763;&#35793;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21019;&#36896;&#20102;&#19968;&#20010;&#23478;&#26063;&#21270;&#30340;&#27169;&#22411;&#31995;&#21015;MathGenieLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24320;&#28304;&#27169;&#22411;&#21644;GPT-4&#31561;&#38381;&#28304;&#27169;&#22411;&#20043;&#38388;&#22312;&#36825;&#19968;&#39046;&#22495;&#20173;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;MathGenie&#65292;&#29992;&#20110;&#20174;&#23567;&#35268;&#27169;&#38382;&#39064;-&#35299;&#20915;&#26041;&#26696;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#31181;&#23376;&#25968;&#25454;&#65289;&#20013;&#29983;&#25104;&#22810;&#26679;&#19988;&#21487;&#38752;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#20805;&#20102;&#31181;&#23376;&#25968;&#25454;&#30340;&#30495;&#23454;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#21453;&#21521;&#32763;&#35793;&#27169;&#22411;&#65292;&#23558;&#25193;&#20805;&#30340;&#35299;&#20915;&#26041;&#26696;&#32763;&#35793;&#22238;&#26032;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20026;&#26032;&#38382;&#39064;&#29983;&#25104;&#20102;&#38598;&#25104;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#30830;&#20445;&#38598;&#25104;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21407;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#39564;&#35777;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#26032;&#31579;&#36873;&#30340;&#25968;&#25454;&#19978;&#23545;&#20174;7B&#21040;70B&#19981;&#31561;&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#27979;&#35797;&#25152;&#25552;&#20986;&#30340;&#22686;&#24378;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#31216;&#20026;MathGenieLM&#30340;&#27169;&#22411;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16352v1 Announce Type: cross  Abstract: Large language models (LLMs) have exhibited great potential in mathematical reasoning. However, there remains a performance gap in this area between existing open-source models and closed-source models such as GPT-4. In this paper, we introduce MathGenie, a novel method for generating diverse and reliable math problems from a small-scale problem-solution dataset (denoted as seed data). We augment the ground-truth solutions of our seed data and train a back-translation model to translate the augmented solutions back into new questions. Subsequently, we generate code-integrated solutions for the new questions. To ensure the correctness of the code-integrated solutions, we employ rationale-based strategy for solution verification. Various pretrained models, ranging from 7B to 70B, are trained on the newly curated data to test the effectiveness of the proposed augmentation technique, resulting in a family of models known as MathGenieLM. Th
&lt;/p&gt;</description></item><item><title>CodeS&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36739;&#23567;&#30340;&#21442;&#25968;&#35268;&#27169;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#37319;&#29992;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;SQL&#20013;&#24515;&#35821;&#26009;&#24211;&#36827;&#34892;&#28176;&#36827;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;&#20854;SQL&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16347</link><description>&lt;p&gt;
CodeS&#65306;&#26500;&#24314;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeS: Towards Building Open-source Language Models for Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16347
&lt;/p&gt;
&lt;p&gt;
CodeS&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36739;&#23567;&#30340;&#21442;&#25968;&#35268;&#27169;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#37319;&#29992;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;SQL&#20013;&#24515;&#35821;&#26009;&#24211;&#36827;&#34892;&#28176;&#36827;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;&#20854;SQL&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32763;&#35793;&#20026;SQL&#26597;&#35810;&#65288;Text-to-SQL&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#22823;&#20294;&#23553;&#38381;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#21487;&#33021;&#23384;&#22312;&#27169;&#22411;&#26550;&#26500;&#19981;&#26126;&#30830;&#12289;&#25968;&#25454;&#38544;&#31169;&#39118;&#38505;&#21644;&#26114;&#36149;&#30340;&#25512;&#29702;&#24320;&#38144;&#31561;&#23616;&#38480;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CodeS&#65292;&#19968;&#31995;&#21015;&#38024;&#23545;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21442;&#25968;&#33539;&#22260;&#20174;1B&#21040;15B&#19981;&#31561;&#12290;CodeS&#26159;&#19968;&#20010;&#23436;&#20840;&#24320;&#28304;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#20197;&#26356;&#23567;&#30340;&#21442;&#25968;&#35268;&#27169;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26500;&#24314;CodeS&#26102;&#38754;&#20020;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#20026;&#22686;&#24378;CodeS&#30340;SQL&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;SQL&#20013;&#24515;&#35821;&#26009;&#24211;&#36827;&#34892;&#28176;&#36827;&#39044;&#35757;&#32451;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26550;&#26500;&#38142;&#25509;&#25361;&#25112;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16347v1 Announce Type: new  Abstract: Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL). However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema lin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20195;&#29702;&#27169;&#22411;&#23545;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#36827;&#34892;&#27169;&#25311;&#65292;&#26500;&#24314;&#20102;&#31867;&#20284;Twitter&#29615;&#22659;&#27169;&#25311;&#20182;&#20204;&#23545;&#35302;&#21457;&#20107;&#20214;&#30340;&#21453;&#24212;&#65292;&#20026;&#31038;&#20250;&#36816;&#21160;&#27169;&#25311;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.16333</link><description>&lt;p&gt;
&#25581;&#31034;&#30495;&#30456;&#20419;&#36827;&#21464;&#38761;&#65306;&#38754;&#21521;&#22522;&#20110;&#20195;&#29702;&#30340;&#22823;&#35268;&#27169;&#31038;&#20250;&#36816;&#21160;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16333
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20195;&#29702;&#27169;&#22411;&#23545;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#36827;&#34892;&#27169;&#25311;&#65292;&#26500;&#24314;&#20102;&#31867;&#20284;Twitter&#29615;&#22659;&#27169;&#25311;&#20182;&#20204;&#23545;&#35302;&#21457;&#20107;&#20214;&#30340;&#21453;&#24212;&#65292;&#20026;&#31038;&#20250;&#36816;&#21160;&#27169;&#25311;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24050;&#32463;&#25104;&#20026;&#31038;&#20250;&#36816;&#21160;&#30340;&#22522;&#30707;&#65292;&#22312;&#25512;&#21160;&#31038;&#20250;&#21464;&#38761;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#24433;&#21709;&#21147;&#12290;&#27169;&#25311;&#20844;&#20247;&#21453;&#24212;&#24182;&#39044;&#27979;&#28508;&#22312;&#24433;&#21709;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#27169;&#25311;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20854;&#20013;&#29992;&#25143;&#20998;&#20026;&#20004;&#31867;&#12290;&#26680;&#24515;&#29992;&#25143;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#65292;&#32780;&#20247;&#22810;&#26222;&#36890;&#29992;&#25143;&#21017;&#30001;&#28436;&#32462;&#24335;&#20195;&#29702;&#27169;&#22411;&#24314;&#27169;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#31867;&#20284;Twitter&#30340;&#29615;&#22659;&#26469;&#22797;&#21046;&#20182;&#20204;&#23545;&#35302;&#21457;&#20107;&#20214;&#30340;&#21453;&#24212;&#21160;&#24577;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#22522;&#20934;SoMoSiMu-Bench&#29992;&#20110;&#35780;&#20272;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16333v1 Announce Type: cross  Abstract: Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change. Simulating the response of the public and forecasting the potential impact has become increasingly important. However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants. In this paper, we introduce a hybrid framework for social media user simulation, wherein users are categorized into two types. Core users are driven by Large Language Models, while numerous ordinary users are modeled by deductive agent-based models. We further construct a Twitter-like environment to replicate their response dynamics following trigger events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real-world datasets. Experimental results demonstrat
&lt;/p&gt;</description></item><item><title>&#26080;&#38656;&#25968;&#25454;&#21442;&#19982;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26435;&#37325;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#21387;&#32553;&#21442;&#25968;&#30697;&#38453;&#24182;&#20445;&#25345;&#27491;&#20132;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16319</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25968;&#25454;&#26435;&#37325;&#21387;&#32553;&#21644;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Data-freeWeight Compress and Denoise for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16319
&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#25968;&#25454;&#21442;&#19982;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26435;&#37325;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#21387;&#32553;&#21442;&#25968;&#30697;&#38453;&#24182;&#20445;&#25345;&#27491;&#20132;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#37325;&#22609;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#30340;&#26684;&#23616;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#30340;&#26174;&#33879;&#25193;&#22823;&#65292;&#36328;&#36234;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#21442;&#25968;&#30340;&#21487;&#25193;&#23637;&#24615;&#21463;&#38480;&#20110;GPU&#20869;&#23384;&#21644;&#35745;&#31639;&#36895;&#24230;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#26435;&#37325;&#21387;&#32553;&#26041;&#27861;&#65292;&#22914;&#21098;&#26525;&#21644;&#37327;&#21270;&#12290;&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#37325;&#30697;&#38453;&#30340;&#20302;&#31209;&#29305;&#24615;&#65292;&#36890;&#36807;&#30697;&#38453;&#20998;&#35299;&#20943;&#23569;&#26435;&#37325;&#22312;&#21387;&#32553;&#21442;&#25968;&#26041;&#38754;&#26080;&#30097;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#21644;&#21069;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20511;&#37492;LLMs&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26080;&#25968;&#25454;&#32852;&#21512;&#31209;-k&#36924;&#36817;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#21387;&#32553;&#21442;&#25968;&#30697;&#38453;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#28857;&#22312;&#20110;&#26080;&#38656;&#39069;&#22806;&#28041;&#21450;&#20219;&#20309;&#35821;&#26009;&#24211;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#20132;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16319v1 Announce Type: new  Abstract: Large Language Models (LLMs) are reshaping the research landscape in artificial intelligence, particularly as model parameters scale up significantly, unlocking remarkable capabilities across various domains. Nevertheless, the scalability of model parameters faces constraints due to limitations in GPU memory and computational speed. To address these constraints, various weight compression methods have emerged, such as Pruning and Quantization. Given the low-rank nature of weight matrices in language models, the reduction of weights through matrix decomposition undoubtedly holds significant potential and promise. In this paper, drawing upon the intrinsic structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k Approximation for compressing the parameter matrices. Significantly, our method is characterized by without necessitating additional involvement of any corpus, while simultaneously preserving orthogonality in con
&lt;/p&gt;</description></item><item><title>Finer&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#19978;&#30340;&#30701;&#26495;&#65292;&#23588;&#20854;&#26159;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#32454;&#33268;&#23646;&#24615;&#35299;&#37322;&#65292;&#23613;&#31649;&#20855;&#26377;&#29983;&#25104;&#39640;&#27700;&#24179;&#22270;&#20687;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16315</link><description>&lt;p&gt;
Finer: &#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30740;&#31350;&#21644;&#22686;&#24378;&#32454;&#31890;&#24230;&#35270;&#35273;&#27010;&#24565;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16315
&lt;/p&gt;
&lt;p&gt;
Finer&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#19978;&#30340;&#30701;&#26495;&#65292;&#23588;&#20854;&#26159;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#32454;&#33268;&#23646;&#24615;&#35299;&#37322;&#65292;&#23613;&#31649;&#20855;&#26377;&#29983;&#25104;&#39640;&#27700;&#24179;&#22270;&#20687;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#27169;&#22411;&#33021;&#22815;&#36731;&#26494;&#29983;&#25104;&#39640;&#27700;&#24179;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#36825;&#31181;&#33021;&#21147;&#20027;&#35201;&#24402;&#22240;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#21253;&#21547;&#30340;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20845;&#20010;&#19981;&#21516;&#22522;&#20934;&#35774;&#32622;&#19979;&#30340;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#65288;FGVC&#65289;&#19978;&#30340;&#32570;&#38519;&#12290;&#26368;&#36817;&#30340;LVLMs&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22914;LLaVa-1.5&#65292;InstructBLIP&#21644;GPT-4V&#65292;&#22312;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#20005;&#37325;&#19979;&#38477;&#65292;&#20363;&#22914;&#65292;LLaVA-1.5&#22312;&#26031;&#22374;&#31119;&#29399;&#30340;EM&#24179;&#22343;&#19979;&#38477;&#20102;65.58&#65292;&#32780;&#19988;&#36824;&#38590;&#20197;&#26681;&#25454;&#20986;&#29616;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#27010;&#24565;&#29983;&#25104;&#20855;&#26377;&#35814;&#32454;&#23646;&#24615;&#30340;&#20934;&#30830;&#35299;&#37322;&#65292;&#23613;&#31649;&#23427;&#20204;&#26377;&#29983;&#25104;&#25972;&#20307;&#22270;&#20687;&#32423;&#25551;&#36848;&#30340;&#33021;&#21147;&#12290;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;LVLMs&#22312;&#32473;&#23450;&#25991;&#26412;&#26102;&#21576;&#29616;&#20986;&#27169;&#24577;&#24046;&#36317;&#65292;&#26174;&#31034;&#20986;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16315v1 Announce Type: cross  Abstract: Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given tex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Discussion&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#30340;&#36136;&#37327;</title><link>https://arxiv.org/abs/2402.16313</link><description>&lt;p&gt;
Chain-of-Discussion&#65306;&#22797;&#26434;&#35777;&#25454;&#38382;&#39064;&#22238;&#31572;&#30340;&#22810;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Chain-of-Discussion&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#38656;&#35201;&#27169;&#22411;&#25214;&#21040;&#36866;&#24403;&#30340;&#35777;&#25454;&#26469;&#24418;&#25104;&#21512;&#29702;&#12289;&#20840;&#38754;&#21644;&#26377;&#24110;&#21161;&#30340;&#31572;&#26696;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#36824;&#38656;&#35201;&#21442;&#19982;&#23545;&#19982;&#38382;&#39064;&#23494;&#20999;&#30456;&#20851;&#30340;&#28508;&#22312;&#22330;&#26223;&#36827;&#34892;&#28145;&#20837;&#35752;&#35770;&#12290;&#22312;&#26816;&#32034;&#27169;&#22359;&#30340;&#22686;&#24378;&#19979;&#65292;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#33021;&#22815;&#20135;&#29983;&#19968;&#33268;&#30340;&#31572;&#26696;&#65292;&#20294;&#22312;&#21487;&#38752;&#35777;&#25454;&#36873;&#25321;&#21644;&#28145;&#20837;&#38382;&#39064;&#20998;&#26512;&#26041;&#38754;&#20173;&#19981;&#22815;&#29702;&#24819;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Chain-of-Discussion&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#24320;&#28304;LLMs&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20026;&#24320;&#25918;&#24335;QA&#25552;&#20379;&#26356;&#27491;&#30830;&#12289;&#26356;&#20840;&#38754;&#30340;&#31572;&#26696;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20010;&#20307;&#19978;&#36824;&#19981;&#22815;&#24378;&#22823;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22810;&#20010;LLMs&#20043;&#38388;&#30340;&#35752;&#35770;&#23545;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;\url{https://github.com/kobaya}&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16313v1 Announce Type: cross  Abstract: Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \textbf{more correct} and \textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We release our data and code at \url{https://github.com/kobaya
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#23558;&#28304;&#39046;&#22495;&#21477;&#27861;&#35268;&#21017;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;&#22120;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;1.68&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16311</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#20013;&#25991;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Chinese Sentence Pattern Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#23558;&#28304;&#39046;&#22495;&#21477;&#27861;&#35268;&#21017;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;&#22120;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;1.68&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16311v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#21477;&#24335;&#32467;&#26500;&#65288;SPS&#65289;&#35299;&#26512;&#26159;&#19968;&#31181;&#20027;&#35201;&#29992;&#20110;&#35821;&#35328;&#25945;&#23398;&#30340;&#21477;&#27861;&#20998;&#26512;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;SPS&#35299;&#26512;&#22120;&#20027;&#35201;&#20381;&#36182;&#20110;&#25945;&#31185;&#20070;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#65292;&#32570;&#20047;&#36328;&#39046;&#22495;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#25105;&#35757;&#32451;&#26694;&#26550;&#20869;&#12290;&#20174;&#28304;&#39046;&#22495;&#20013;&#25552;&#21462;&#37096;&#20998;&#21477;&#27861;&#35268;&#21017;&#65292;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#32467;&#21512;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#35299;&#26512;&#22120;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#65292;F1&#25351;&#26631;&#27604;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;&#39640;&#20986;1.68&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16311v1 Announce Type: cross  Abstract: Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.
&lt;/p&gt;</description></item><item><title>PerLTQA&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#35821;&#20041;&#21644;&#24773;&#33410;&#35760;&#24518;&#30340;&#21019;&#26032;QA&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25506;&#32034;&#20010;&#24615;&#21270;&#35760;&#24518;&#22312;QA&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#21644;&#35760;&#24518;&#25972;&#21512;&#12289;&#26816;&#32034;&#12289;&#21512;&#25104;&#30340;&#26694;&#26550;</title><link>https://arxiv.org/abs/2402.16288</link><description>&lt;p&gt;
PerLTQA: &#19968;&#20010;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#35760;&#24518;&#20998;&#31867;&#12289;&#26816;&#32034;&#21644;&#21512;&#25104;&#30340;&#20010;&#20154;&#38271;&#26399;&#35760;&#24518;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16288
&lt;/p&gt;
&lt;p&gt;
PerLTQA&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#35821;&#20041;&#21644;&#24773;&#33410;&#35760;&#24518;&#30340;&#21019;&#26032;QA&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25506;&#32034;&#20010;&#24615;&#21270;&#35760;&#24518;&#22312;QA&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#21644;&#35760;&#24518;&#25972;&#21512;&#12289;&#26816;&#32034;&#12289;&#21512;&#25104;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#35760;&#24518;&#22312;&#20010;&#20154;&#20114;&#21160;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32771;&#34385;&#21040;&#38271;&#26399;&#35760;&#24518;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#12289;&#21382;&#21490;&#20449;&#24687;&#21644;&#23545;&#35805;&#20013;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;PerLTQA&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;QA&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;&#35821;&#20041;&#21644;&#24773;&#33410;&#35760;&#24518;&#65292;&#21253;&#25324;&#19990;&#30028;&#30693;&#35782;&#12289;&#29992;&#25143;&#36164;&#26009;&#12289;&#31038;&#20250;&#20851;&#31995;&#12289;&#20107;&#20214;&#21644;&#23545;&#35805;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#34987;&#25910;&#38598;&#29992;&#20110;&#25506;&#35752;&#20010;&#24615;&#21270;&#35760;&#24518;&#22312;QA&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20132;&#20114;&#21160;&#21644;&#20107;&#20214;&#12290;PerLTQA&#20855;&#26377;&#20004;&#31181;&#35760;&#24518;&#31867;&#22411;&#21644;&#19968;&#20010;&#21253;&#21547;8,593&#20010;&#38382;&#39064;&#30340;30&#20010;&#23383;&#31526;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20419;&#36827;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25506;&#32034;&#21644;&#24212;&#29992;&#20010;&#24615;&#21270;&#35760;&#24518;&#12290;&#22522;&#20110;PerLTQA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35760;&#24518;&#25972;&#21512;&#21644;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#35760;&#24518;&#20998;&#31867;&#12289;&#35760;&#24518;&#26816;&#32034;&#21644;&#35760;&#24518;&#21512;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;LLM&#21644;&#19977;&#20010;&#35780;&#20272;&#20102;&#36825;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16288v1 Announce Type: cross  Abstract: Long-term memory plays a critical role in personal interaction, considering long-term memory can better leverage world knowledge, historical information, and preferences in dialogues. Our research introduces PerLTQA, an innovative QA dataset that combines semantic and episodic memories, including world knowledge, profiles, social relationships, events, and dialogues. This dataset is collected to investigate the use of personalized memories, focusing on social interactions and events in the QA task. PerLTQA features two types of memory and a comprehensive benchmark of 8,593 questions for 30 characters, facilitating the exploration and application of personalized memories in Large Language Models (LLMs). Based on PerLTQA, we propose a novel framework for memory integration and generation, consisting of three main components: Memory Classification, Memory Retrieval, and Memory Synthesis. We evaluate this framework using five LLMs and thre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.16278</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#27880;&#37322;&#23884;&#20837;&#27169;&#22411;&#30340;&#26412;&#20307;&#21253;&#21547;&#20851;&#31995;&#39044;&#27979;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Self-matching Training Method with Annotation Embedding Models for Ontology Subsumption Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16278
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#25429;&#33719;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#34920;&#31034;&#23454;&#20307;&#30340;&#26412;&#20307;&#23884;&#20837;&#65292;&#29992;&#20110;&#26412;&#20307;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#26412;&#20307;&#23884;&#20837;&#26410;&#35299;&#20915;&#31867;&#20284;&#21644;&#23396;&#31435;&#23454;&#20307;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#26410;&#25552;&#21462;&#26412;&#20307;&#20013;&#27880;&#37322;&#20844;&#29702;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20004;&#31181;&#26412;&#20307;&#23884;&#20837;&#27169;&#22411;&#30340;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#65306;Inverted-index Matrix Embedding (InME) &#21644; Co-occurrence Matrix Embedding (CoME)&#12290;&#36825;&#20004;&#31181;&#23884;&#20837;&#36890;&#36807;&#27599;&#20010;&#21333;&#35789;&#22312;&#19968;&#32452;&#20844;&#29702;&#20013;&#20986;&#29616;&#30340;&#20301;&#32622;&#20197;&#21450;&#27599;&#20010;&#20844;&#29702;&#20013;&#21333;&#35789;&#30340;&#20849;&#29616;&#26469;&#25429;&#33719;&#27880;&#37322;&#20844;&#29702;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#20449;&#24687;&#12290;&#33258;&#21305;&#37197;&#35757;&#32451;&#26041;&#27861;&#25552;&#39640;&#20102;&#27010;&#24565;&#23376;&#31867;&#39044;&#27979;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#39044;&#27979;&#30340;&#36229;&#31867;&#19982;&#23376;&#31867;&#30456;&#20284;&#19988;&#23396;&#31435;&#20110;&#26412;&#20307;&#20013;&#30340;&#20854;&#20182;&#23454;&#20307;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16278v1 Announce Type: new  Abstract: Recently, ontology embeddings representing entities in a low-dimensional space have been proposed for ontology completion. However, the ontology embeddings for concept subsumption prediction do not address the difficulties of similar and isolated entities and fail to extract the global information of annotation axioms from an ontology. In this paper, we propose a self-matching training method for the two ontology embedding models: Inverted-index Matrix Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings capture the global and local information in annotation axioms by means of the occurring locations of each word in a set of axioms and the co-occurrences of words in each axiom. The self-matching training method increases the robustness of the concept subsumption prediction when predicted superclasses are similar to subclasses and are isolated to other entities in an ontology. Our evaluation experiments show that
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;UniRetriever&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#20004;&#20010;&#25439;&#22833;&#32422;&#26463;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#20505;&#36873;&#32773;&#36873;&#25321;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#23545;&#35805;&#26816;&#32034;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.16261</link><description>&lt;p&gt;
UniRetriever&#65306;&#21508;&#31181;&#24773;&#22659;&#33258;&#36866;&#24212;&#23545;&#35805;&#26816;&#32034;&#30340;&#22810;&#20219;&#21153;&#20505;&#36873;&#32773;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16261
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;UniRetriever&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#20004;&#20010;&#25439;&#22833;&#32422;&#26463;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#20505;&#36873;&#32773;&#36873;&#25321;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#23545;&#35805;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#26816;&#32034;&#26159;&#25351;&#20197;&#36845;&#20195;&#21644;&#20132;&#20114;&#26041;&#24335;&#36816;&#34892;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#38656;&#35201;&#26816;&#32034;&#21508;&#31181;&#22806;&#37096;&#36164;&#28304;&#65288;&#22914;&#20154;&#35774;&#12289;&#30693;&#35782;&#29978;&#33267;&#22238;&#24212;&#65289;&#20197;&#26377;&#25928;&#19982;&#29992;&#25143;&#20132;&#20114;&#24182;&#25104;&#21151;&#23436;&#25104;&#23545;&#35805;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26694;&#26550;&#65292;&#20316;&#20026;&#19977;&#20010;&#20027;&#35201;&#26816;&#32034;&#20219;&#21153;&#30340;&#36890;&#29992;&#26816;&#32034;&#22120;&#65306;&#20154;&#35774;&#36873;&#25321;&#12289;&#30693;&#35782;&#36873;&#25321;&#21644;&#22238;&#24212;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#21253;&#25324;&#19968;&#20010;&#24773;&#22659;&#33258;&#36866;&#24212;&#23545;&#35805;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20505;&#36873;&#32773;&#32534;&#30721;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#31616;&#21333;&#30340;&#28857;&#31215;&#20851;&#27880;&#38271;&#23545;&#35805;&#20013;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#24182;&#26816;&#32034;&#21512;&#36866;&#30340;&#20505;&#36873;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#25439;&#22833;&#32422;&#26463;&#20197;&#25429;&#25417;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16261v1 Announce Type: new  Abstract: Conversational retrieval refers to an information retrieval system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue. However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency. Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection. To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product. Furthermore, we introduce two loss constraints to capture t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#20869;&#23481;&#36873;&#25321;&#30340;&#22797;&#21046;&#26426;&#21046;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#25913;&#36827;&#30340;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#20351;&#20027;&#39064;&#21040;&#25991;&#31456;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#25991;&#26412;&#29983;&#25104;&#22810;&#26679;&#24615;&#25552;&#39640;&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.16248</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#20869;&#23481;&#36873;&#25321;&#30340;&#20027;&#39064;&#21040;&#25991;&#31456;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Topic-to-essay generation with knowledge-based content selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16248
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#20869;&#23481;&#36873;&#25321;&#30340;&#22797;&#21046;&#26426;&#21046;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#20016;&#23500;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#25913;&#36827;&#30340;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#20351;&#20027;&#39064;&#21040;&#25991;&#31456;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#25991;&#26412;&#29983;&#25104;&#22810;&#26679;&#24615;&#25552;&#39640;&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#21040;&#25991;&#31456;&#29983;&#25104;&#20219;&#21153;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#20027;&#39064;&#35789;&#29983;&#25104;&#20855;&#26377;&#39640;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#27573;&#33853;&#32423;&#25991;&#26412;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#32780;&#24573;&#30053;&#20102;&#29983;&#25104;&#25991;&#26412;&#22810;&#26679;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#20869;&#23481;&#36873;&#25321;&#27169;&#22359;&#30340;&#22797;&#21046;&#26426;&#21046;&#27169;&#22411;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30693;&#35782;&#25972;&#21512;&#21040;&#35299;&#30721;&#22120;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25913;&#36827;&#30340;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#36755;&#20837;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;TEG&#20219;&#21153;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#22810;&#26679;&#24615;&#25552;&#39640;35%&#33267;59%&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#27700;&#24179;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16248v1 Announce Type: cross  Abstract: The topic-to-essay generation task is a challenging natural language generation task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words. Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity. In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the language model into the decoder. Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities. In addition, we have contributed a new Chinese dataset for TEG tasks. Experimental results demonstrate that the proposed model can improve the generated text diversity by 35\% to 59\% compared to the state-of-the-art method, while maintaining a high level of topic consistency.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#20064;&#24471;&#38382;&#39064;&#65288;CLAP&#65289;&#30340;&#26032;&#39062;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#22312;&#30446;&#26631;&#31038;&#21306;&#20013;&#20174;&#20114;&#21160;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#25918;&#23485;&#20102;Zero-Shot Coordination&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2402.16247</link><description>&lt;p&gt;
&#23398;&#20064;&#32763;&#35793;&#65306;&#24212;&#23545;&#21512;&#20316;&#35821;&#35328;&#20064;&#24471;&#30340;&#26032;&#20852;&#27807;&#36890;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16247
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#20064;&#24471;&#38382;&#39064;&#65288;CLAP&#65289;&#30340;&#26032;&#39062;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#22312;&#30446;&#26631;&#31038;&#21306;&#20013;&#20174;&#20114;&#21160;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#25918;&#23485;&#20102;Zero-Shot Coordination&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#20852;&#27807;&#36890;&#20013;&#65292;&#20195;&#29702;&#23398;&#20064;&#24444;&#27492;&#36827;&#34892;&#27807;&#36890;&#65292;&#20294;&#20182;&#20204;&#21046;&#23450;&#30340;&#21327;&#35758;&#26159;&#38024;&#23545;&#20182;&#20204;&#30340;&#35757;&#32451;&#32676;&#20307;&#30340;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#23548;&#33268;&#20102;&#23545;&#20110;&#23398;&#20064;&#23545;&#26410;&#22312;&#35757;&#32451;&#20013;&#36935;&#21040;&#30340;&#20195;&#29702;&#31283;&#20581;&#30340;&#27807;&#36890;&#31574;&#30053;&#30340;Zero-Shot Coordination&#65288;ZSC&#65289;&#30340;&#30740;&#31350;&#12290;&#20294;&#26159;&#65292;ZSC&#36890;&#24120;&#20551;&#35774;&#20851;&#20110;&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#20250;&#36935;&#21040;&#30340;&#20195;&#29702;&#30340;&#20808;&#21069;&#25968;&#25454;&#26159;&#26080;&#27861;&#33719;&#24471;&#30340;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#24517;&#35201;&#30340;&#26840;&#25163;&#38382;&#39064;&#65292;&#24182;&#25490;&#38500;&#20102;&#36890;&#36807;&#39044;&#20808;&#24314;&#31435;&#30340;&#32422;&#23450;&#36827;&#34892;&#27807;&#36890;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21512;&#20316;&#35821;&#35328;&#20064;&#24471;&#38382;&#39064;&#65288;CLAP&#65289;&#30340;&#26032;&#39062;&#20154;&#24037;&#26234;&#33021;&#25361;&#25112;&#65292;&#22312;&#20854;&#20013;&#36890;&#36807;&#20801;&#35768;&#8220;&#21152;&#20837;&#32773;&#8221;&#20195;&#29702;&#20174;&#30446;&#26631;&#31038;&#21306;&#20869;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26469;&#25918;&#23485;&#20102;ZSC&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#35299;&#20915;CLAPs&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21644;&#26032;&#20852;&#27807;&#36890;&#30340;&#39044;&#35757;&#32451;&#21644;&#32763;&#35793;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16247v1 Announce Type: cross  Abstract: In Emergent Communication (EC) agents learn to communicate with one another, but the protocols that they develop are specialised to their training community. This observation led to research into Zero-Shot Coordination (ZSC) for learning communication strategies that are robust to agents not encountered during training. However, ZSC typically assumes that no prior data is available about the agents that will be encountered in the zero-shot setting. In many cases, this presents an unnecessarily hard problem and rules out communication via preestablished conventions. We propose a novel AI challenge called a Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions are relaxed by allowing a 'joiner' agent to learn from a dataset of interactions between agents in a target community. We propose and compare two methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication pretraining and Translation Learni
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;LLMs&#30340;&#24187;&#35273;&#20542;&#21521;&#19982;&#39640;&#25928;&#30340;&#24187;&#35273;&#26816;&#27979;&#65292;&#21019;&#24314;&#20102;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#30340;HypoTermQA&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.16211</link><description>&lt;p&gt;
HypoTermQA&#65306;&#29992;&#20110;&#35780;&#20272;LLMs&#24187;&#35273;&#20542;&#21521;&#30340;&#20551;&#35774;&#26415;&#35821;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;LLMs&#30340;&#24187;&#35273;&#20542;&#21521;&#19982;&#39640;&#25928;&#30340;&#24187;&#35273;&#26816;&#27979;&#65292;&#21019;&#24314;&#20102;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#30340;HypoTermQA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#38752;&#24615;&#21644;&#23545;&#40784;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#20197;&#22806;&#30340;&#24191;&#27867;&#25509;&#21463;&#31243;&#24230;&#12290;&#23613;&#31649;&#19981;&#26029;&#21162;&#21147;&#65292;&#20294;&#24187;&#35273;&#20173;&#28982;&#26159;LLMs&#20013;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#24187;&#35273;&#30340;&#26816;&#27979;&#26412;&#36523;&#20063;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#32463;&#24120;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#25110;&#21463;&#38480;&#21046;&#30340;&#35780;&#20272;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#23558;LLMs&#30340;&#24187;&#35273;&#20542;&#21521;&#19982;&#39640;&#25928;&#30340;&#24187;&#35273;&#26816;&#27979;&#30456;&#32467;&#21512;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21033;&#29992;LLMs&#29983;&#25104;&#19982;&#20551;&#35774;&#29616;&#35937;&#30456;&#20851;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38543;&#21518;&#23558;&#23427;&#20204;&#29992;&#20316;&#26377;&#25928;&#24187;&#35273;&#26816;&#27979;&#30340;&#20195;&#29702;&#12290;&#35813;&#26694;&#26550;&#19982;&#39046;&#22495;&#26080;&#20851;&#65292;&#20801;&#35768;&#22312;&#20219;&#20309;&#39046;&#22495;&#20013;&#20351;&#29992;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#25110;&#35780;&#20272;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;HypoTermQA&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#33539;&#22260;&#22914;&#19979;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16211v1 Announce Type: new  Abstract: Hallucinations pose a significant challenge to the reliability and alignment of Large Language Models (LLMs), limiting their widespread acceptance beyond chatbot applications. Despite ongoing efforts, hallucinations remain a prevalent challenge in LLMs. The detection of hallucinations itself is also a formidable task, frequently requiring manual labeling or constrained evaluations. This paper introduces an automated scalable framework that combines benchmarking LLMs' hallucination tendencies with efficient hallucination detection. We leverage LLMs to generate challenging tasks related to hypothetical phenomena, subsequently employing them as agents for efficient hallucination detection. The framework is domain-agnostic, allowing the use of any language model for benchmark creation or evaluation in any domain. We introduce the publicly available HypoTermQA Benchmarking Dataset, on which state-of-the-art models' performance ranged between 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;IR2&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#26597;&#35810;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50%&#12290;</title><link>https://arxiv.org/abs/2402.16200</link><description>&lt;p&gt;
IR2&#65306;&#20449;&#24687;&#27491;&#21017;&#21270;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
IR2: Information Regularization for Information Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16200
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;IR2&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#26597;&#35810;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22797;&#26434;&#26597;&#35810;&#65292;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IR2&#65292;&#21363;&#20449;&#24687;&#26816;&#32034;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#22797;&#26434;&#26597;&#35810;&#29305;&#24449;&#30340;&#19977;&#20010;&#26368;&#36817;&#30340;IR&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;DORIS-MAE&#12289;ArguAna&#21644;WhatsThatBook&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#19981;&#20165;&#22312;&#25152;&#32771;&#34385;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#21512;&#25104;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#33021;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50&#65285;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23558;&#19981;&#21516;&#38454;&#27573;&#30340;&#19977;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#8212;&#8212;&#36755;&#20837;&#12289;&#25552;&#31034;&#21644;&#36755;&#20986;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25506;&#32034;&#65292;&#27599;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#27809;&#26377;&#27491;&#21017;&#21270;&#30340;&#27169;&#22411;&#22343;&#25552;&#20379;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16200v1 Announce Type: cross  Abstract: Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regulariz
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#20010;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#19987;&#23478;&#21644;&#19987;&#38376;&#30340;&#27880;&#24847;&#21147;&#31574;&#30053;&#65292;ASEM&#27169;&#22411;&#25552;&#20379;&#20102;&#20849;&#24773;&#22238;&#22797;&#29983;&#25104;&#25152;&#38656;&#30340;&#24773;&#24863;&#21644;&#20851;&#27880;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16194</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24773;&#24863;&#24314;&#27169;&#22686;&#24378;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#24773;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and Emotion Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#19987;&#23478;&#21644;&#19987;&#38376;&#30340;&#27880;&#24847;&#21147;&#31574;&#30053;&#65292;ASEM&#27169;&#22411;&#25552;&#20379;&#20102;&#20849;&#24773;&#22238;&#22797;&#29983;&#25104;&#25152;&#38656;&#30340;&#24773;&#24863;&#21644;&#20851;&#27880;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#29305;&#24449;&#34920;&#31034;&#22312;&#25552;&#21319;&#20381;&#36182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#20363;&#22914;&#26080;&#27861;&#25429;&#25417;&#35821;&#35328;&#30340;&#28145;&#23618;&#35821;&#20041;&#21644;&#23545;&#36755;&#20837;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#23548;&#33268;&#29983;&#25104;&#25991;&#26412;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#32534;&#30721;&#22120;&#30340;&#19987;&#23478;&#28151;&#21512;&#26469;&#25552;&#20379;&#29992;&#25143;&#35805;&#35821;&#24773;&#24863;&#29366;&#24577;&#30340;&#19981;&#21516;&#35270;&#35282;&#65292;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASEM&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#26550;&#26500;&#65292;&#23545;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#29616;&#29983;&#25104;&#26082;&#33258;&#28982;&#21448;&#30456;&#20851;&#30340;&#20849;&#24773;&#22238;&#22797;&#12290;&#19982;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#19987;&#38376;&#27880;&#24847;&#21147;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16194v1 Announce Type: new  Abstract: Effective feature representations play a critical role in enhancing the performance of text generation models that rely on deep neural networks. However, current approaches suffer from several drawbacks, such as the inability to capture the deep semantics of language and sensitivity to minor input variations, resulting in significant changes in the generated text. In this paper, we present a novel solution to these challenges by employing a mixture of experts, multiple encoders, to offer distinct perspectives on the emotional state of the user's utterance while simultaneously enhancing performance. We propose an end-to-end model architecture called ASEM that performs emotion analysis on top of sentiment analysis for open-domain chatbots, enabling the generation of empathetic responses that are fluent and relevant. In contrast to traditional attention mechanisms, the proposed model employs a specialized attention strategy that uniquely ze
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMANTICSMOOTH&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#35821;&#20041;&#36716;&#25442;&#21103;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#36935;GCG&#12289;PAIR&#21644;AutoDAN&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36739;&#24378;&#30340;&#27491;&#24120;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16192</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#24179;&#28369;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#36935;&#30417;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16192
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMANTICSMOOTH&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#35821;&#20041;&#36716;&#25442;&#21103;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#36935;GCG&#12289;PAIR&#21644;AutoDAN&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36739;&#24378;&#30340;&#27491;&#24120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23481;&#26131;&#21463;&#21040;&#30417;&#29425;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#32469;&#36807;&#30446;&#26631;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#39575;&#36807;&#23427;&#20204;&#29983;&#25104;&#20196;&#20154;&#21453;&#24863;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SEMANTICSMOOTH&#65292;&#19968;&#31181;&#22522;&#20110;&#24179;&#28369;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#32463;&#36807;&#35821;&#20041;&#36716;&#25442;&#30340;&#32473;&#23450;&#36755;&#20837;&#25552;&#31034;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#26469;&#25552;&#39640;&#23545;GCG&#12289;PAIR&#21644;AutoDAN&#25915;&#20987;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SEMANTICSMOOTH&#22312;&#20445;&#25345;&#25351;&#23548;&#24615;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;InstructionFollowing&#21644;AlpacaEval&#65289;&#19978;&#30340;&#24378;&#21170;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#25915;&#20987;&#30340;&#26368;&#26032;&#25216;&#26415;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16192v1 Announce Type: new  Abstract: Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth.
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#34429;&#28982;&#20855;&#26377;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#31561;&#20248;&#28857;&#65292;&#20294;&#20063;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#23454;&#29992;&#25351;&#21335;&#20197;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.16187</link><description>&lt;p&gt;
&#21033;&#29992;&#20854;&#20248;&#21183;&#25915;&#20987;LLM&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Attacking LLM Watermarks by Exploiting Their Strengths
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16187
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#34429;&#28982;&#20855;&#26377;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#31561;&#20248;&#28857;&#65292;&#20294;&#20063;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#23454;&#29992;&#25351;&#21335;&#20197;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#22270;&#29255;&#33021;&#22815;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27700;&#21360;&#25216;&#26415;&#26088;&#22312;&#23558;&#20449;&#24687;&#23884;&#20837;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#20197;&#39564;&#35777;&#20854;&#26469;&#28304;&#65292;&#23545;&#20110;&#20943;&#23569;&#23545;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28389;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#26696;&#20173;&#28982;&#20196;&#20154;&#24847;&#22806;&#22320;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#20849;&#20139;&#30340;&#21487;&#21462;&#29305;&#24615;&#65292;&#20363;&#22914;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#65292;&#21453;&#36807;&#26469;&#21364;&#20351;&#36825;&#20123;&#31995;&#32479;&#23481;&#26131;&#36973;&#21463;&#21508;&#31181;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#24120;&#35265;&#27700;&#21360;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#20005;&#26684;&#30740;&#31350;&#28508;&#22312;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25915;&#20987;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#38450;&#24481;&#25514;&#26045;&#8212;&#8212;&#24314;&#31435;&#20102;&#19968;&#22871;&#23884;&#20837;&#21644;&#26816;&#27979;LLM&#27700;&#21360;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16187v1 Announce Type: cross  Abstract: Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38750;&#32447;&#24615;&#32467;&#26500;&#25506;&#38024;&#26469;&#25506;&#31350;&#32534;&#30721;&#20449;&#24687;&#30340;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#31616;&#21333;&#26377;&#25928;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#21487;&#35270;&#21270;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20381;&#23384;&#26641;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.16168</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#32447;&#24615;&#26041;&#27861;&#21644;&#26356;&#22810;&#25506;&#38024;&#26469;&#25506;&#31350;&#32534;&#30721;&#20449;&#24687;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Hitting "Probe"rty with Non-Linearity, and More
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16168
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38750;&#32447;&#24615;&#32467;&#26500;&#25506;&#38024;&#26469;&#25506;&#31350;&#32534;&#30721;&#20449;&#24687;&#30340;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#31616;&#21333;&#26377;&#25928;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#21487;&#35270;&#21270;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20381;&#23384;&#26641;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#25506;&#38024;&#23398;&#20064;&#32447;&#24615;&#21464;&#25442;&#65292;&#20197;&#25214;&#21040;&#20381;&#23384;&#26641;&#22914;&#20309;&#23884;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#25105;&#20204;&#24341;&#20837;&#38750;&#32447;&#24615;&#32467;&#26500;&#25506;&#38024;&#65292;&#37325;&#26032;&#35774;&#35745;&#20102;White&#31561;&#20154;&#20171;&#32461;&#30340;&#38750;&#32447;&#24615;&#32467;&#26500;&#25506;&#38024;&#65292;&#20351;&#20854;&#35774;&#35745;&#26356;&#31616;&#21333;&#20294;&#26377;&#25928;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#23450;&#24615;&#35780;&#20272;&#21477;&#23376;&#20013;&#20004;&#20010;&#21333;&#35789;&#22312;&#39044;&#27979;&#30340;&#20381;&#23384;&#26641;&#20013;&#30340;&#36830;&#25509;&#24378;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#25216;&#26415;&#26469;&#29702;&#35299;&#21738;&#31181;&#38750;&#32447;&#24615;&#25506;&#38024;&#21464;&#20307;&#25797;&#38271;&#32534;&#30721;&#21477;&#27861;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36824;&#29992;&#23427;&#23450;&#24615;&#30740;&#31350;&#20102;BERT&#22312;&#27599;&#20010;&#23618;&#20013;&#32534;&#30721;&#30340;&#20381;&#23384;&#26641;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16168v1 Announce Type: cross  Abstract: Structural probes learn a linear transformation to find how dependency trees are embedded in the hidden states of language models. This simple design may not allow for full exploitation of the structure of the encoded information. Hence, to investigate the structure of the encoded information to its full extent, we incorporate non-linear structural probes. We reformulate the design of non-linear structural probes introduced by White et al. making its design simpler yet effective. We also design a visualization framework that lets us qualitatively assess how strongly two words in a sentence are connected in the predicted dependency tree. We use this technique to understand which non-linear probe variant is good at encoding syntactical information. Additionally, we also use it to qualitatively investigate the structure of dependency trees that BERT encodes in each of its layers. We find that the radial basis function (RBF) is an effectiv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#12289;&#35821;&#35328;&#21551;&#21457;&#12289;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22312;&#20851;&#31995;&#25277;&#21462;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.16159</link><description>&lt;p&gt;
DistALANER&#65306;&#24320;&#28304;&#36719;&#20214;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#36828;&#31243;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#22686;&#24378;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16159
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#12289;&#35821;&#35328;&#21551;&#21457;&#12289;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22312;&#20851;&#31995;&#25277;&#21462;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#20840;&#38754;&#30340;&#20004;&#27493;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#26469;&#35299;&#20915;&#36719;&#20214;&#25968;&#25454;&#26631;&#27880;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#35813;&#36807;&#31243;&#24039;&#22937;&#22320;&#21033;&#29992;&#35821;&#35328;&#21551;&#21457;&#12289;&#29420;&#29305;&#30340;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#25152;&#24102;&#26469;&#30340;&#38480;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;LLMs&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;NER&#22312;&#20851;&#31995;&#25277;&#21462;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16159v1 Announce Type: new  Abstract: This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.
&lt;/p&gt;</description></item><item><title>ChatMusician &#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#20869;&#22312;&#38899;&#20048;&#33021;&#21147;&#30340;&#24320;&#28304;LLM&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#20860;&#23481;&#30340;&#38899;&#20048;&#34920;&#31034;&#27861;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#20048;&#65292;&#34920;&#29616;&#20248;&#20110;GPT-4&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.16153</link><description>&lt;p&gt;
ChatMusician&#65306;&#29702;&#35299;&#21644;&#29983;&#25104;&#20855;&#26377;LLM&#30340;&#38899;&#20048;&#20869;&#22312;
&lt;/p&gt;
&lt;p&gt;
ChatMusician: Understanding and Generating Music Intrinsically with LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16153
&lt;/p&gt;
&lt;p&gt;
ChatMusician &#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#20869;&#22312;&#38899;&#20048;&#33021;&#21147;&#30340;&#24320;&#28304;LLM&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#20860;&#23481;&#30340;&#38899;&#20048;&#34920;&#31034;&#27861;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#20048;&#65292;&#34920;&#29616;&#20248;&#20110;GPT-4&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#33021;&#21147;&#23578;&#26410;&#25512;&#24191;&#21040;&#38899;&#20048;&#65292;&#20063;&#23601;&#26159;&#20154;&#31867;&#30340;&#21019;&#36896;&#24615;&#35821;&#35328;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ChatMusician&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#65292;&#38598;&#25104;&#20102;&#20869;&#22312;&#30340;&#38899;&#20048;&#33021;&#21147;&#12290;&#23427;&#22522;&#20110;&#23545;&#25991;&#26412;&#20860;&#23481;&#30340;&#38899;&#20048;&#34920;&#31034;&#27861;ABC&#35760;&#35889;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;LLaMA2&#65292;&#24182;&#19988;&#23558;&#38899;&#20048;&#35270;&#20026;&#31532;&#20108;&#35821;&#35328;&#12290;ChatMusician&#21487;&#20197;&#20351;&#29992;&#32431;&#25991;&#26412;&#26631;&#35760;&#22120;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#20048;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#22806;&#37096;&#22810;&#27169;&#24577;&#31070;&#32463;&#32467;&#26500;&#25110;&#26631;&#35760;&#22120;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36171;&#20104;&#38899;&#20048;&#33021;&#21147;&#24182;&#19981;&#20250;&#25439;&#23475;&#35821;&#35328;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#36798;&#21040;&#30053;&#39640;&#30340;MMLU&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#12289;&#21644;&#24358;&#12289;&#26059;&#24459;&#12289;&#20027;&#39064;&#12289;&#38899;&#20048;&#24418;&#24335;&#31561;&#21019;&#20316;&#32467;&#26500;&#33391;&#22909;&#12289;&#23436;&#25972;&#38271;&#24230;&#30340;&#38899;&#20048;&#65292;&#36229;&#36234;&#20102;GPT-4&#30340;&#22522;&#32447;&#12290;&#22312;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#30340;&#22823;&#23398;&#32423;&#38899;&#20048;&#29702;&#35299;&#22522;&#20934;&#19978;&#65292;MusicTheory
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16153v1 Announce Type: cross  Abstract: While Large Language Models (LLMs) demonstrate impressive capabilities in text generation, we find that their ability has yet to be generalized to music, humanity's creative language. We introduce ChatMusician, an open-source LLM that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure text tokenizer without any external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score. Our model is capable of composing well-structured, full-length music, conditioned on texts, chords, melodies, motifs, musical forms, etc, surpassing GPT-4 baseline. On our meticulously curated college-level music understanding benchmark, MusicTheory
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;LLMs&#22312;&#20581;&#36523;&#12289;&#22478;&#24066;&#35268;&#21010;&#12289;&#27668;&#20505;&#24314;&#27169;&#21644;&#28798;&#38590;&#21709;&#24212;&#31561;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24433;&#21709;&#21644;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16142</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#36716;&#21270;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#21151;&#33021;&#24615;&#30340;&#20840;&#38754;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39046;&#22495;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;LLMs&#22312;&#20581;&#36523;&#12289;&#22478;&#24066;&#35268;&#21010;&#12289;&#27668;&#20505;&#24314;&#27169;&#21644;&#28798;&#38590;&#21709;&#24212;&#31561;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24433;&#21709;&#21644;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#25442;&#22120;&#65288;BERT&#65289;&#22312;&#20174;&#25216;&#26415;&#12289;&#37329;&#34701;&#12289;&#21307;&#30103;&#20445;&#20581;&#21040;&#25945;&#32946;&#31561;&#21508;&#39046;&#22495;&#30340;&#25193;&#23637;&#12290;&#23613;&#31649;&#36825;&#20123;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#24050;&#32463;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23578;&#26410;&#31995;&#32479;&#22320;&#30740;&#31350;&#36807;&#23427;&#20204;&#23545;&#20581;&#36523;&#12289;&#25972;&#20307;&#24184;&#31119;&#24863;&#12289;&#22478;&#24066;&#35268;&#21010;&#12289;&#27668;&#20505;&#24314;&#27169;&#20197;&#21450;&#28798;&#23475;&#31649;&#29702;&#31561;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#38500;&#20102;&#20840;&#38754;&#20998;&#26512;LLMs&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#21033;&#29992;&#31243;&#24230;&#20043;&#22806;&#65292;&#35813;&#32508;&#36848;&#35770;&#25991;&#36824;&#35782;&#21035;&#20102;LLMs&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#21033;&#29992;&#30340;&#30740;&#31350;&#31354;&#30333;&#21644;&#39046;&#22495;&#12290;&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#21487;&#20197;&#22312;&#20581;&#36523;&#19982;&#24184;&#31119;&#12289;&#22478;&#24066;&#35268;&#21010;&#12289;&#27668;&#20505;&#24314;&#27169;&#21644;&#28798;&#23475;&#21709;&#24212;&#31561;&#39046;&#22495;&#30041;&#19979;&#30165;&#36857;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#20182;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16142v1 Announce Type: cross  Abstract: This groundbreaking study explores the expanse of Large Language Models (LLMs), such as Generative Pre-Trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT) across varied domains ranging from technology, finance, healthcare to education. Despite their established prowess in Natural Language Processing (NLP), these LLMs have not been systematically examined for their impact on domains such as fitness, and holistic well-being, urban planning, climate modelling as well as disaster management. This review paper, in addition to furnishing a comprehensive analysis of the vast expanse and extent of LLMs' utility in diverse domains, recognizes the research gaps and realms where the potential of LLMs is yet to be harnessed. This study uncovers innovative ways in which LLMs can leave a mark in the fields like fitness and wellbeing, urban planning, climate modelling and disaster response which could inspire 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PeriodicLoRA&#65288;PLoRA&#65289;&#26469;&#25171;&#30772;LoRA&#20248;&#21270;&#20013;&#30340;&#20302;&#31209;&#29942;&#39048;&#65292;&#36890;&#36807;&#22810;&#27425;&#32047;&#31215;&#20302;&#31209;&#26356;&#26032;&#30697;&#38453;&#26469;&#23454;&#29616;&#26356;&#39640;&#30340;&#26356;&#26032;&#31209;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16141</link><description>&lt;p&gt;
PeriodicLoRA: &#25171;&#30772;LoRA&#20248;&#21270;&#20013;&#30340;&#20302;&#31209;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16141
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PeriodicLoRA&#65288;PLoRA&#65289;&#26469;&#25171;&#30772;LoRA&#20248;&#21270;&#20013;&#30340;&#20302;&#31209;&#29942;&#39048;&#65292;&#36890;&#36807;&#22810;&#27425;&#32047;&#31215;&#20302;&#31209;&#26356;&#26032;&#30697;&#38453;&#26469;&#23454;&#29616;&#26356;&#39640;&#30340;&#26356;&#26032;&#31209;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#26159;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#20294;&#20840;&#24494;&#35843;LLMs&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#25104;&#26412;&#25928;&#30410;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290; LoRA&#26159;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#23427;&#20551;&#35774;&#20248;&#21270;&#36807;&#31243;&#26412;&#36136;&#19978;&#26159;&#20302;&#32500;&#30340;&#12290;&#34429;&#28982;LoRA&#24494;&#35843;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#19982;&#20840;&#24494;&#35843;&#30456;&#27604;&#20173;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#65292;&#22240;&#20026;&#20854;&#26435;&#37325;&#26356;&#26032;&#20165;&#38480;&#20110;&#20302;&#31209;&#30697;&#38453;&#12290;&#20026;&#20102;&#25171;&#30772;LoRA&#20248;&#21270;&#20013;&#30340;&#20302;&#31209;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PeriodicLoRA&#65288;PLoRA&#65289;&#65292;&#23427;&#22810;&#27425;&#32047;&#31215;&#20302;&#31209;&#26356;&#26032;&#30697;&#38453;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#26356;&#26032;&#31209;&#12290;PLoRA&#20855;&#26377;&#22810;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20173;&#28982;&#20165;&#26356;&#26032;LoRA&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#22312;&#27599;&#20010;&#38454;&#27573;&#32467;&#26463;&#26102;&#65292;&#25105;&#20204;&#23558;LoRA&#26435;&#37325;&#21368;&#36733;&#21040;&#39592;&#24178;&#21442;&#25968;&#20013;&#65292;&#28982;&#21518;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16141v1 Announce Type: new  Abstract: Supervised fine-tuning is the most common method to adapt large language models (LLMs) to downstream tasks, but full fine-tuning LLMs requires massive computational resources. Recently, parameter-efficient fine-tuning (PEFT) methods have been widely studied due to its cost-effectiveness. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low-dimensional. Although LoRA fine-tuning is effective, there is still a performance gap compared to full fine-tuning, since its weight update is limited to low-rank matrices. In order to break the low-rank bottleneck in LoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank update matrices multiple times to achieve a higher update rank. PLoRA has multiple training stages. During each stage, we still update only the LoRA weights. However, at the end of each stage, we unload the LoRA weights into the backbone parameters and then
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;ChatGPT&#22312;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#35821;&#22659;&#29305;&#23450;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#21487;&#20197;&#36741;&#21161;&#26415;&#35821;&#23398;&#23478;&#36827;&#34892;&#26415;&#35821;&#32534;&#32386;&#65292;&#23454;&#29616;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.16139</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#23545;&#26415;&#35821;&#23450;&#20041;&#30340;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
What Generative Artificial Intelligence Means for Terminological Definitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16139
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;ChatGPT&#22312;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#35821;&#22659;&#29305;&#23450;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#21487;&#20197;&#36741;&#21161;&#26415;&#35821;&#23398;&#23478;&#36827;&#34892;&#26415;&#35821;&#32534;&#32386;&#65292;&#23454;&#29616;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#23545;&#26415;&#35821;&#23450;&#20041;&#30340;&#21019;&#24314;&#21644;&#28040;&#36153;&#30340;&#24433;&#21709;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;GenAI&#24037;&#20855;&#19982;&#20256;&#32479;&#26415;&#35821;&#36164;&#28304;&#30456;&#27604;&#65292;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#30410;&#22788;&#21644;&#25361;&#25112;&#12290;ChatGPT&#22312;&#20197;&#20132;&#20114;&#24335;&#21644;&#23450;&#21046;&#21270;&#30340;&#26041;&#24335;&#25552;&#20379;&#29305;&#23450;&#35821;&#22659;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#35782;&#21035;&#36164;&#28304;&#20013;&#30340;&#26415;&#35821;&#23450;&#20041;&#21487;&#33021;&#20250;&#22240;&#20854;&#21487;&#38752;&#24615;&#32780;&#32487;&#32493;&#23384;&#22312;&#12290;&#20174;&#26415;&#35821;&#23398;&#23478;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#24037;&#20855;&#20351;&#24471;AI&#36741;&#21161;&#30340;&#26415;&#35821;&#32534;&#32386;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;&#21518;&#26399;&#32534;&#36753;&#26415;&#35821;&#32534;&#32386;&#65292;&#23558;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#36895;&#30340;&#23450;&#20041;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16139v1 Announce Type: cross  Abstract: This paper examines the impact of Generative Artificial Intelligence (GenAI) on the creation and consumption of terminological definitions. GenAI tools like ChatGPT present a mix of benefits and drawbacks compared to traditional terminological resources. ChatGPT excels in providing context-specific meanings in an interactive and customized fashion but faces challenges with accuracy. Terminological definitions in recognized resources will likely survive because of their reliability. From the point of view of the terminologist, tools like ChatGPT enable AI-assisted terminography, including post-editing terminography, as an approach blending AI efficiency with human expertise for faster definition creation.
&lt;/p&gt;</description></item><item><title>LSTPrompt&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20998;&#35299;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#25552;&#31034;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16132</link><description>&lt;p&gt;
LSTPrompt: &#38271;&#30701;&#26399;&#25552;&#31034;&#19979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16132
&lt;/p&gt;
&lt;p&gt;
LSTPrompt&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20998;&#35299;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#25552;&#31034;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#38646;shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#36807;&#20998;&#31616;&#21270;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#23558;&#20854;&#35270;&#20026;&#35821;&#35328;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#65292;&#24573;&#35270;&#20102;&#20854;&#21160;&#24577;&#24615;&#20197;&#21450;&#19982;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#31574;&#30053;&#65288;&#22914;Chain-of-Thought&#65289;&#30340;&#34701;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LSTPrompt&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#38646;shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#25552;&#31034;LLMs&#30340;&#26032;&#26041;&#27861;&#12290;LSTPrompt&#23558;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20998;&#35299;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#23376;&#20219;&#21153;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#25552;&#31034;&#12290;LSTPrompt&#24341;&#23548;LLMs&#23450;&#26399;&#37325;&#26032;&#35780;&#20272;&#39044;&#27979;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#36866;&#24212;&#24615;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;LSTPrompt&#30340;&#24615;&#33021;&#22987;&#32456;&#26356;&#22909;&#65292;&#24182;&#19988;&#19982;&#22522;&#26412;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16132v1 Announce Type: cross  Abstract: Time-series forecasting (TSF) finds broad applications in real-world scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates strong zero-shot TSF capabilities while preserving computational efficiency. However, existing prompting methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models.
&lt;/p&gt;</description></item><item><title>InstructEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#65292;&#36890;&#36807;&#31616;&#21333;&#25351;&#20196;&#20351;&#32534;&#36753;&#22120;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#32534;&#36753;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16123</link><description>&lt;p&gt;
InstructEdit&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
InstructEdit: Instruction-based Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16123
&lt;/p&gt;
&lt;p&gt;
InstructEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#65292;&#36890;&#36807;&#31616;&#21333;&#25351;&#20196;&#20351;&#32534;&#36753;&#22120;&#36866;&#24212;&#19981;&#21516;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#32534;&#36753;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25913;&#21464;&#27169;&#22411;&#30340;&#34892;&#20026;&#32780;&#19981;&#20250;&#23545;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#28040;&#26497;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#36328;&#20219;&#21153;&#30340;&#36890;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#35774;&#35745;&#19968;&#20010;&#29420;&#29305;&#30340;&#32534;&#36753;&#22120;&#65292;&#36825;&#26174;&#33879;&#38459;&#30861;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#30693;&#35782;&#32534;&#36753;&#20013;&#30340;&#22810;&#20219;&#21153;&#27867;&#21270;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#20196;&#30340;&#32534;&#36753;&#25216;&#26415;&#65292;&#31216;&#20026;InstructEdit&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25351;&#20196;&#20419;&#36827;&#32534;&#36753;&#22120;&#21516;&#26102;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#21482;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#32534;&#36753;&#22120;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#26041;&#38754;&#34920;&#26126;&#65292;InstructEdit&#21487;&#20197;&#25552;&#39640;&#32534;&#36753;&#22120;&#30340;&#25511;&#21046;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#22810;&#20219;&#21153;&#32534;&#36753;&#35774;&#32622;&#20013;&#24179;&#22343;&#25552;&#39640;&#21487;&#38752;&#24615;14.86%&#12290;&#27492;&#22806;&#65292;&#28041;&#21450;&#20445;&#30041;&#26410;&#35265;&#20219;&#21153;&#30340;&#23454;&#39564;&#35828;&#26126;&#65292;InstructEdi
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16123v1 Announce Type: cross  Abstract: Knowledge editing for large language models can offer an efficient solution to alter a model's behavior without negatively impacting the overall performance. However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor's adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each LLM, we empirically demonstrate that InstructEdit can improve the editor's control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdi
&lt;/p&gt;</description></item><item><title>FuseChat&#36890;&#36807;&#30693;&#35782;&#34701;&#21512;&#23558;&#22810;&#20010;&#23545;&#35805;&#27169;&#22411;&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16107</link><description>&lt;p&gt;
FuseChat&#65306;&#23545;&#35805;&#27169;&#22411;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FuseChat: Knowledge Fusion of Chat Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16107
&lt;/p&gt;
&lt;p&gt;
FuseChat&#36890;&#36807;&#30693;&#35782;&#34701;&#21512;&#23558;&#22810;&#20010;&#23545;&#35805;&#27169;&#22411;&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30830;&#23454;&#21487;&#20197;&#23548;&#33268;&#20855;&#26377;&#29420;&#29305;&#33021;&#21147;&#21644;&#20248;&#21183;&#30340;&#27169;&#22411;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20250;&#20135;&#29983;&#24040;&#22823;&#25104;&#26412;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#31454;&#20105;&#33021;&#21147;&#30340;&#28508;&#22312;&#20887;&#20313;&#12290;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#26159;&#23558;&#29616;&#26377;&#30340;LLMs&#32452;&#21512;&#25104;&#26356;&#24378;&#22823;&#30340;LLM&#65292;&#20174;&#32780;&#20943;&#23569;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;LLMs&#30340;&#22810;&#26679;&#21270;&#26550;&#26500;&#65292;&#30452;&#25509;&#21442;&#25968;&#34701;&#21512;&#34987;&#35777;&#26126;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26368;&#36817;&#65292;FuseLLM&#24341;&#20837;&#20102;&#30693;&#35782;&#34701;&#21512;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25345;&#32493;&#35757;&#32451;&#23558;&#22810;&#20010;&#32467;&#26500;&#22810;&#26679;&#30340;LLM&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#33267;&#30446;&#26631;LLM&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;FuseLLM&#26694;&#26550;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;LLM&#30340;&#34701;&#21512;&#65292;&#29983;&#25104;&#20102;FuseChat&#12290;FuseChat&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#32467;&#26500;&#21644;&#35268;&#27169;&#19981;&#21516;&#30340;&#28304;LLMs&#36827;&#34892;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16107v1 Announce Type: new  Abstract: While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \textsc{FuseChat}. \textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#20027;&#35201;&#35266;&#28857;&#65306;&#19968;&#31181;&#35748;&#20026;&#39044;&#27979;&#27010;&#29575;&#34920;&#26126;&#27169;&#22411;&#32622;&#20449;&#24230;&#65292;&#21478;&#19968;&#31181;&#35748;&#20026;&#39044;&#27979;&#27010;&#29575;&#34920;&#26126;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#12290;&#20316;&#32773;&#24314;&#35758;&#21516;&#26102;&#32771;&#34385;&#36825;&#20004;&#31181;&#35266;&#28857;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16102</link><description>&lt;p&gt;
&#35299;&#37322;&#39044;&#27979;&#27010;&#29575;&#65306;&#27169;&#22411;&#32622;&#20449;&#24230;&#36824;&#26159;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#65311;
&lt;/p&gt;
&lt;p&gt;
Interpreting Predictive Probabilities: Model Confidence or Human Label Variation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#20027;&#35201;&#35266;&#28857;&#65306;&#19968;&#31181;&#35748;&#20026;&#39044;&#27979;&#27010;&#29575;&#34920;&#26126;&#27169;&#22411;&#32622;&#20449;&#24230;&#65292;&#21478;&#19968;&#31181;&#35748;&#20026;&#39044;&#27979;&#27010;&#29575;&#34920;&#26126;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#12290;&#20316;&#32773;&#24314;&#35758;&#21516;&#26102;&#32771;&#34385;&#36825;&#20004;&#31181;&#35266;&#28857;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#24378;&#22823;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#23835;&#36215;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#35780;&#20272;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#24456;&#22909;&#22320;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#20854;&#23545;&#32467;&#26524;&#30340;&#39044;&#27979;&#20998;&#24067;&#36136;&#37327;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#31181;&#20027;&#35201;&#35266;&#28857;&#65292;&#23427;&#20204;&#25512;&#21160;&#20102;&#25130;&#28982;&#19981;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#31532;&#19968;&#31181;&#23558;&#39044;&#27979;&#27010;&#29575;&#35270;&#20026;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#25351;&#31034;&#65307;&#31532;&#20108;&#31181;&#23558;&#20854;&#35270;&#20026;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#30340;&#25351;&#31034;&#12290;&#25105;&#20204;&#35752;&#35770;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#35748;&#20026;&#20004;&#32773;&#23545;&#20110;&#20540;&#24471;&#20449;&#36182;&#21644;&#20844;&#24179;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#37117;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20165;&#21033;&#29992;&#21333;&#20010;&#39044;&#27979;&#20998;&#24067;&#26159;&#26377;&#38480;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#24314;&#35758;&#24037;&#20855;&#24182;&#31361;&#20986;&#25351;&#21521;&#20855;&#26377;&#20851;&#20110;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#20851;&#20110;&#20154;&#31867;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#33073;&#38057;&#34920;&#31034;&#27169;&#22411;&#30340;&#28608;&#21160;&#20154;&#24515;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16102v1 Announce Type: new  Abstract: With the rise of increasingly powerful and user-facing NLP systems, there is growing interest in assessing whether they have a good representation of uncertainty by evaluating the quality of their predictive distribution over outcomes. We identify two main perspectives that drive starkly different evaluation protocols. The first treats predictive probability as an indication of model confidence; the second as an indication of human label variation. We discuss their merits and limitations, and take the position that both are crucial for trustworthy and fair NLP systems, but that exploiting a single predictive distribution is limiting. We recommend tools and highlight exciting directions towards models with disentangled representations of uncertainty about predictions and uncertainty about human labels.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#26631;&#35760;&#26144;&#23556;&#21040;&#20849;&#20139;&#23383;&#31526;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#38463;&#25289;&#20271;-&#24076;&#20271;&#26469;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21516;&#26102;&#34920;&#31034;&#20004;&#31181;&#35821;&#35328;&#30340;&#32479;&#19968;&#33050;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#27604;&#20110;&#20445;&#25345;&#21407;&#26377;&#33050;&#26412;&#30340;&#27169;&#22411;&#26377;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.16065</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#26631;&#35760;&#26144;&#23556;&#21040;&#20849;&#20139;&#23383;&#31526;&#31354;&#38388;&#26469;&#35757;&#32451;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16065
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26631;&#35760;&#26144;&#23556;&#21040;&#20849;&#20139;&#23383;&#31526;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#38463;&#25289;&#20271;-&#24076;&#20271;&#26469;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21516;&#26102;&#34920;&#31034;&#20004;&#31181;&#35821;&#35328;&#30340;&#32479;&#19968;&#33050;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#27604;&#20110;&#20445;&#25345;&#21407;&#26377;&#33050;&#26412;&#30340;&#27169;&#22411;&#26377;&#30528;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#38463;&#25289;&#20271;&#25991;&#25991;&#26412;&#30340;&#38899;&#35793;&#29256;&#26412;&#22312;&#24076;&#20271;&#26469;&#35821;&#20013;&#35757;&#32451;&#20102;&#19968;&#20010;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#20004;&#31181;&#35821;&#35328;&#22312;&#21516;&#19968;&#33050;&#26412;&#20013;&#34920;&#31034;&#12290;&#37492;&#20110;&#38463;&#25289;&#20271;&#35821;&#21644;&#24076;&#20271;&#26469;&#35821;&#20043;&#38388;&#30340;&#24418;&#24577;&#23398;&#12289;&#32467;&#26500;&#30456;&#20284;&#24615;&#20197;&#21450;&#22823;&#37327;&#20849;&#21516;&#35789;&#28304;&#35789;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;&#32479;&#19968;&#33050;&#26412;&#34920;&#31034;&#20004;&#31181;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#36328;&#35821;&#35328;&#30693;&#35782;&#30340;&#26426;&#22120;&#32763;&#35793;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20445;&#25345;&#38463;&#25289;&#20271;&#25991;&#26412;&#22312;&#38463;&#25289;&#20271;&#33050;&#26412;&#20013;&#30340;&#23545;&#27604;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#38899;&#35793;&#27493;&#39588;&#30340;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#26041;&#38754;&#35757;&#32451;&#38598;&#22823;&#23567;&#32422;&#20026;&#20854;&#20182;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;60&#65285;&#65292;&#20294;&#22312;&#26426;&#22120;&#32763;&#35793;&#30340;&#20004;&#20010;&#26041;&#21521;&#19978;&#20284;&#20046;&#25552;&#20379;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16065v1 Announce Type: new  Abstract: We train a bilingual Arabic-Hebrew language model using a transliterated version of Arabic texts in Hebrew, to ensure both languages are represented in the same script. Given the morphological, structural similarities, and the extensive number of cognates shared among Arabic and Hebrew, we assess the performance of a language model that employs a unified script for both languages, on machine translation which requires cross-lingual knowledge. The results are promising: our model outperforms a contrasting model which keeps the Arabic texts in the Arabic script, demonstrating the efficacy of the transliteration step. Despite being trained on a dataset approximately 60% smaller than that of other existing language models, our model appears to deliver comparable performance in machine translation across both translation directions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16063</link><description>&lt;p&gt;
&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Citation-Enhanced Generation for LLM-based Chatbot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#26234;&#33021;&#65292;&#21253;&#25324;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22312;&#22238;&#22797;&#20013;&#21487;&#33021;&#20135;&#29983;&#34394;&#26500;&#20869;&#23481;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#32493;&#24341;&#29992;&#22686;&#24378;&#29983;&#25104;&#65288;CEG&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#26816;&#32034;&#35770;&#35777;&#12290;&#19982;&#20808;&#21069;&#20391;&#37325;&#20110;&#39044;&#38450;&#29983;&#25104;&#36807;&#31243;&#20013;&#24187;&#35273;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21518;&#32493;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#32467;&#21512;&#20102;&#19968;&#20010;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#19982;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#30340;&#25903;&#25345;&#25991;&#26723;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16063v1 Announce Type: cross  Abstract: Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc \textbf{C}itation-\textbf{E}nhanced \textbf{G}eneration (\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#36890;&#36807;&#25506;&#31350;&#20219;&#21153;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36880;&#23618;&#32534;&#30721;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;LLMs&#26356;&#20542;&#21521;&#20110;&#22312;&#19978;&#23618;&#32534;&#30721;&#26356;&#22810;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.16061</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#32534;&#30721;&#19978;&#19979;&#25991;&#30693;&#35782;&#65311;&#19968;&#39033;&#36880;&#23618;&#25506;&#31350;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#36890;&#36807;&#25506;&#31350;&#20219;&#21153;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36880;&#23618;&#32534;&#30721;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;LLMs&#26356;&#20542;&#21521;&#20110;&#22312;&#19978;&#23618;&#32534;&#30721;&#26356;&#22810;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26816;&#32034;&#20107;&#23454;&#21644;&#22788;&#29702;&#19978;&#19979;&#25991;&#30693;&#35782;&#26041;&#38754;&#30340;&#24341;&#20154;&#27880;&#30446;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#36880;&#23618;&#32534;&#30721;&#30693;&#35782;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#26377;&#38480;&#65292;&#36825;&#25361;&#25112;&#20102;&#25105;&#20204;&#23545;&#23427;&#20204;&#20869;&#37096;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25506;&#31350;&#20219;&#21153;&#26469;&#39318;&#27425;&#30740;&#31350;LLMs&#36880;&#23618;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;ChatGPT&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#26500;&#24314;&#25506;&#31350;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#19982;&#21508;&#31181;&#20107;&#23454;&#30456;&#23545;&#24212;&#30340;&#22810;&#26679;&#19988;&#36830;&#36143;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;$\mathcal V$-usable&#20449;&#24687;&#20316;&#20026;&#39564;&#35777;&#25351;&#26631;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#36328;&#19981;&#21516;&#23618;&#32534;&#30721;&#19978;&#19979;&#25991;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#26377;&#20914;&#31361;&#21644;&#26032;&#33719;&#24471;&#30693;&#35782;&#26041;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLMs&#65306;&#65288;1&#65289;&#26356;&#20542;&#21521;&#20110;&#22312;&#19978;&#23618;&#32534;&#30721;&#26356;&#22810;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65307;&#65288;2&#65289;&#20027;&#35201;&#22312;&#19982;&#30693;&#35782;&#30456;&#20851;&#30340;&#23454;&#20307;&#26631;&#35760;&#20869;&#32534;&#30721;&#19978;&#19979;&#25991;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16061v1 Announce Type: new  Abstract: Previous work has showcased the intriguing capability of large language models (LLMs) in retrieving facts and processing context knowledge. However, only limited research exists on the layer-wise capability of LLMs to encode knowledge, which challenges our understanding of their internal mechanisms. In this paper, we devote the first attempt to investigate the layer-wise capability of LLMs through probing tasks. We leverage the powerful generative capability of ChatGPT to construct probing datasets, providing diverse and coherent evidence corresponding to various facts. We employ $\mathcal V$-usable information as the validation metric to better reflect the capability in encoding context knowledge across different layers. Our experiments on conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode more context knowledge in the upper layers; (2) primarily encode context knowledge within knowledge-related entity tokens 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gist-COCO&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35201;&#28857;&#21387;&#32553;&#26469;&#24110;&#21161;&#25552;&#31034;&#35299;&#37322;&#21644;&#24037;&#31243;&#65292;&#21487;&#20197;&#36798;&#21040;&#36739;&#39640;&#30340;&#21387;&#32553;&#29575;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16058</link><description>&lt;p&gt;
&#29992;&#26356;&#23569;&#30340;&#25991;&#23383;&#35828;&#26356;&#22810;&#65306;&#36890;&#36807;&#35201;&#28857;&#21387;&#32553;&#29702;&#35299;&#25552;&#31034;&#23398;&#20064;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gist-COCO&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35201;&#28857;&#21387;&#32553;&#26469;&#24110;&#21161;&#25552;&#31034;&#35299;&#37322;&#21644;&#24037;&#31243;&#65292;&#21487;&#20197;&#36798;&#21040;&#36739;&#39640;&#30340;&#21387;&#32553;&#29575;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#38271;&#24230;&#36739;&#38271;&#30340;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20197;&#20135;&#29983;&#19982;&#29992;&#25143;&#24847;&#22270;&#19968;&#33268;&#30340;&#36755;&#20986;&#65292;&#36825;&#20010;&#36807;&#31243;&#22312;&#25512;&#29702;&#26399;&#38388;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Gist COnditioned deCOding&#65288;Gist-COCO&#65289;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21387;&#32553;&#25552;&#31034;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#21327;&#21161;&#25552;&#31034;&#30340;&#35299;&#37322;&#21644;&#24037;&#31243;&#12290;Gist-COCO&#37319;&#29992;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#39069;&#22806;&#30340;&#32534;&#30721;&#22120;&#20316;&#20026;&#25554;&#20214;&#27169;&#22359;&#25972;&#21512;&#36827;&#26469;&#65292;&#20351;&#29992;&#35201;&#28857;&#26631;&#35760;&#26469;&#21387;&#32553;&#25552;&#31034;&#12290;&#23427;&#24494;&#35843;&#21387;&#32553;&#25554;&#20214;&#27169;&#22359;&#65292;&#24182;&#20351;&#29992;&#35201;&#28857;&#26631;&#35760;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#21407;&#22987;&#25552;&#31034;&#22312;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#23558;&#35201;&#28857;&#26631;&#35760;&#30340;&#34920;&#31034;&#21475;&#22836;&#21270;&#20026;&#35201;&#28857;&#25552;&#31034;&#65292;Gist-COCO&#30340;&#21387;&#32553;&#33021;&#21147;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;LLMs&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#21387;&#32553;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Gist-COCO&#22312;&#20004;&#20010;&#26041;&#38754;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#25552;&#31034;&#21387;&#32553;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16058v1 Announce Type: new  Abstract: Large language models (LLMs) require lengthy prompts as the input context to produce output aligned with user intentions, a process that incurs extra costs during inference. In this paper, we propose the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering. Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens. It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model. By verbalizing the representations of gist tokens into gist prompts, the compression ability of Gist-COCO can be generalized to different LLMs with high compression rates. Our experiments demonstrate that Gist-COCO outperforms previous prompt compression models in both
&lt;/p&gt;</description></item><item><title>LSTP&#25552;&#20986;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#26102;&#31354;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#25552;&#31034;&#37319;&#26679;&#22120;&#65288;TPS&#65289;&#21644;&#31354;&#38388;&#25552;&#31034;&#27714;&#35299;&#22120;&#65288;SPS&#65289;&#20197;&#21450;&#19968;&#33268;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35745;&#31639;&#25928;&#29575;&#12289;&#26102;&#38388;&#29702;&#35299;&#21644;&#31354;&#38388;-&#26102;&#38388;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.16050</link><description>&lt;p&gt;
LSTP: &#35821;&#35328;&#24341;&#23548;&#30340;&#26102;&#31354;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#38271;&#31687;&#35270;&#39057;&#25991;&#26412;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16050
&lt;/p&gt;
&lt;p&gt;
LSTP&#25552;&#20986;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#26102;&#31354;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#25552;&#31034;&#37319;&#26679;&#22120;&#65288;TPS&#65289;&#21644;&#31354;&#38388;&#25552;&#31034;&#27714;&#35299;&#22120;&#65288;SPS&#65289;&#20197;&#21450;&#19968;&#33268;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35745;&#31639;&#25928;&#29575;&#12289;&#26102;&#38388;&#29702;&#35299;&#21644;&#31354;&#38388;-&#26102;&#38388;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35270;&#39057;&#35821;&#35328;&#24314;&#27169;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22238;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#35821;&#35328;&#26597;&#35810;&#26102;&#35299;&#37322;&#38271;&#31687;&#35270;&#39057;&#30340;&#35745;&#31639;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#39640;&#32500;&#35270;&#39057;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#35821;&#35328;&#19982;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#35270;&#35273;&#32447;&#32034;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#24341;&#23548;&#30340;&#26102;&#31354;&#25552;&#31034;&#23398;&#20064;&#65288;LSTP&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#21033;&#29992;&#20809;&#27969;&#20808;&#39564;&#30340;&#26102;&#38388;&#25552;&#31034;&#37319;&#26679;&#22120;&#65288;TPS&#65289;&#65292;&#21487;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#26377;&#25928;&#25552;&#21462;&#30456;&#20851;&#35270;&#39057;&#20869;&#23481;&#65307;&#20197;&#21450;&#28789;&#24039;&#22320;&#25429;&#25417;&#35270;&#35273;&#21644;&#25991;&#26412;&#20803;&#32032;&#20043;&#38388;&#22797;&#26434;&#31354;&#38388;&#20851;&#31995;&#30340;&#31354;&#38388;&#25552;&#31034;&#27714;&#35299;&#22120;&#65288;SPS&#65289;&#12290;&#36890;&#36807;&#23558;TPS&#21644;SPS&#19982;&#19968;&#33268;&#30340;&#35757;&#32451;&#31574;&#30053;&#30456;&#21327;&#35843;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#21319;&#20102;&#35745;&#31639;&#25928;&#29575;&#12289;&#26102;&#38388;&#29702;&#35299;&#21644;&#31354;&#38388;-&#26102;&#38388;&#23545;&#40784;&#12290;&#22312;&#20004;&#20010;&#25361;&#25112;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16050v1 Announce Type: cross  Abstract: Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time. To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal Prompt Learning (LSTP). This approach features two key components: a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements. By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment. Empirical evaluations across two challeng
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.16048</link><description>&lt;p&gt;
LLMs&#24102;&#26377;&#24605;&#32500;&#38142;&#26465;&#26159;&#38750;&#22240;&#26524;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
LLMs with Chain-of-Thought Are Non-Causal Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#26377;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;LLMs&#20013;&#27491;&#30830;&#31572;&#26696;&#36319;&#38543;&#19981;&#27491;&#30830;CoTs&#30340;&#39057;&#29575;&#21450;&#21453;&#20043;&#12290;&#25105;&#20204;&#37319;&#29992;&#22240;&#26524;&#20998;&#26512;&#26469;&#35780;&#20272;CoTs/&#25351;&#20196;&#19982;LLMs&#31572;&#26696;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25581;&#31034;LLMs&#36817;&#20284;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#12290;&#36890;&#36807;&#27604;&#36739;&#26263;&#31034;SCM&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;SCM&#65292;&#25105;&#20204;&#31361;&#26174;&#20102;LLM&#21644;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24433;&#21709;&#26263;&#31034;SCM&#22240;&#26524;&#32467;&#26500;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26174;&#33879;&#24433;&#21709;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;https://github.com/StevenZHB/CoT_Causal_Analysis&#21457;&#24067;&#20102;&#20195;&#30721;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16048v1 Announce Type: cross  Abstract: This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#31181;&#32676;&#24847;&#35782;&#20248;&#21270;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#20043;&#38388;&#24494;&#22937;&#30340;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16041</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#31181;&#32676;&#24847;&#35782;&#20248;&#21270;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;
&lt;/p&gt;
&lt;p&gt;
Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16041
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#31181;&#32676;&#24847;&#35782;&#20248;&#21270;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#20043;&#38388;&#24494;&#22937;&#30340;&#20998;&#24067;&#24046;&#24322;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#39118;&#38505;&#65292;&#22914;&#25220;&#34989;&#38382;&#39064;&#12289;&#35823;&#23548;&#24615;&#20449;&#24687;&#25110;&#24187;&#35273;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26159;&#38750;&#24120;&#32039;&#36843;&#21644;&#37325;&#35201;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;LLMs&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#21306;&#20998;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#24120;&#24120;&#38750;&#24120;&#24494;&#22937;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#21033;&#29992;\textit{&#26368;&#22823;&#22343;&#20540;&#31163;&#24046;}&#65288;MMD&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;MMD&#21487;&#20197;&#24456;&#22909;&#22320;&#35782;&#21035;&#20998;&#24067;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;&#21508;&#31181;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23545;MMD&#36827;&#34892;&#35757;&#32451;&#23558;&#23548;&#33268;MMD&#30340;&#26041;&#24046;&#26174;&#33879;&#22686;&#21152;&#65292;&#22240;&#20026;&#19981;&#21516;LLMs&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21487;&#33021;&#21253;&#21547;\textit{&#22810;&#20010;&#25991;&#26412;&#32676;&#20307;}&#12290;&#36825;&#23558;&#20005;&#37325;&#25439;&#23475;MMD&#27979;&#37327;&#20998;&#24067;&#24046;&#24322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16041v1 Announce Type: new  Abstract: Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMs. In this paper, we seek to exploit \textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However, directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \textit{multiple text populations} due to various LLMs. This will severely impair MMD's ability to measure the diff
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20855;&#26377;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#21644;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16040</link><description>&lt;p&gt;
EHRNoteQA&#65306;&#29992;&#20110;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16040
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20855;&#26377;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#21644;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#22312;MIMIC-IV&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#30001;&#19977;&#20301;&#21307;&#30103;&#19987;&#23478;&#22242;&#38431;&#31934;&#24515;&#31574;&#21010;&#20102;&#21253;&#21547;962&#20010;&#29420;&#29305;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#29305;&#23450;&#24739;&#32773;&#30340;EHR&#20020;&#24202;&#31508;&#35760;&#30456;&#20851;&#32852;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;EHR&#30340;&#22522;&#20934;&#19981;&#21516;&#30340;&#26159;&#65306;&#39318;&#20808;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#22312;&#33258;&#21160;&#35780;&#20272;&#30340;&#32972;&#26223;&#19979;&#26377;&#25928;&#35780;&#20272;LLMs&#30340;&#24471;&#20998;&#24615;&#33021;&#65292;&#19982;&#20854;&#20182;&#26684;&#24335;&#30456;&#27604;&#12290;&#20854;&#27425;&#65292;&#23427;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#25165;&#33021;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65292;&#21453;&#26144;&#20102;&#23454;&#38469;&#20020;&#24202;&#20915;&#31574;&#21046;&#23450;&#30340;&#22797;&#26434;&#24615;&#65292;&#21307;&#29983;&#38656;&#35201;&#23457;&#26597;&#22823;&#37327;&#24739;&#32773;&#30149;&#21490;&#35760;&#24405;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16040v1 Announce Type: new  Abstract: This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows: Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various large language models
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#35752;&#35770;&#65292;&#27604;&#36739;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#20154;&#23545;AI&#20250;&#35805;&#20195;&#29702;&#30340;&#30475;&#27861;&#65292;&#21457;&#29616;&#20013;&#22269;&#21442;&#19982;&#32773;&#26356;&#24841;&#24742;&#22320;&#30475;&#24453;CAs&#65292;&#32780;&#32654;&#22269;&#21442;&#19982;&#32773;&#21017;&#26356;&#30475;&#37325;&#20854;&#21151;&#33021;&#24615;&#65292;&#28201;&#26262;&#30340;&#30475;&#27861;&#26159;&#20004;&#22269;&#23545;CAs&#20135;&#29983;&#31215;&#26497;&#24773;&#32490;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.16039</link><description>&lt;p&gt;
&#29702;&#35299;&#20844;&#20247;&#23545;AI&#20250;&#35805;&#20195;&#29702;&#30340;&#30475;&#27861;&#65306;&#36328;&#25991;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Understanding Public Perceptions of AI Conversational Agents: A Cross-Cultural Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#35752;&#35770;&#65292;&#27604;&#36739;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#20154;&#23545;AI&#20250;&#35805;&#20195;&#29702;&#30340;&#30475;&#27861;&#65292;&#21457;&#29616;&#20013;&#22269;&#21442;&#19982;&#32773;&#26356;&#24841;&#24742;&#22320;&#30475;&#24453;CAs&#65292;&#32780;&#32654;&#22269;&#21442;&#19982;&#32773;&#21017;&#26356;&#30475;&#37325;&#20854;&#21151;&#33021;&#24615;&#65292;&#28201;&#26262;&#30340;&#30475;&#27861;&#26159;&#20004;&#22269;&#23545;CAs&#20135;&#29983;&#31215;&#26497;&#24773;&#32490;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#20195;&#29702;&#65288;CAs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#24341;&#21457;&#20102;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#37325;&#35201;&#35752;&#35770;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#20844;&#20247;&#23545;AI&#30340;&#30475;&#27861;&#65292;&#20294;&#20851;&#20110;CAs&#30340;&#30740;&#31350;&#26126;&#26174;&#19981;&#36275;&#65292;&#23545;&#25991;&#21270;&#24046;&#24322;&#22312;CA&#30475;&#27861;&#20013;&#30340;&#30740;&#31350;&#26356;&#23569;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;&#35745;&#31639;&#26041;&#27861;&#20998;&#26512;&#20102;&#32422;&#19968;&#30334;&#19975;&#20010;&#22260;&#32469;CAs&#30340;&#31038;&#20132;&#23186;&#20307;&#35752;&#35770;&#65292;&#24182;&#27604;&#36739;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#20154;&#23545;CAs&#30340;&#35805;&#35821;&#21644;&#30475;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20013;&#22269;&#21442;&#19982;&#32773;&#20542;&#21521;&#20110;&#24841;&#24742;&#22320;&#30475;&#24453;CAs&#65292;&#35748;&#20026;&#22522;&#20110;&#35821;&#38899;&#21644;&#20855;&#26377;&#23454;&#20307;&#34920;&#29616;&#30340;CAs&#26356;&#28201;&#26262;&#21644;&#26356;&#26377;&#33021;&#21147;&#65292;&#24182;&#19968;&#33324;&#34920;&#36798;&#31215;&#26497;&#24773;&#32490;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#32654;&#22269;&#21442;&#19982;&#32773;&#26356;&#30475;&#37325;CAs&#30340;&#21151;&#33021;&#24615;&#65292;&#25345;&#26377;&#30683;&#30462;&#30340;&#24577;&#24230;&#12290;&#28201;&#26262;&#30340;&#30475;&#27861;&#26159;&#20004;&#22269;&#23545;CAs&#20135;&#29983;&#31215;&#26497;&#24773;&#32490;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#35774;&#35745;&#29615;&#22659;&#36866;&#24212;&#22411;CAs&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16039v1 Announce Type: cross  Abstract: Conversational Agents (CAs) have increasingly been integrated into everyday life, sparking significant discussions on social media. While previous research has examined public perceptions of AI in general, there is a notable lack in research focused on CAs, with fewer investigations into cultural variations in CA perceptions. To address this gap, this study used computational methods to analyze about one million social media discussions surrounding CAs and compared people's discourses and perceptions of CAs in the US and China. We find Chinese participants tended to view CAs hedonically, perceived voice-based and physically embodied CAs as warmer and more competent, and generally expressed positive emotions. In contrast, US participants saw CAs more functionally, with an ambivalent attitude. Warm perception was a key driver of positive emotions toward CAs in both countries. We discussed practical implications for designing contextually
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#38382;&#31572;&#31995;&#32479;&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#23601;&#65292;&#23588;&#20854;&#26159;&#22312;&#32925;&#32454;&#32990;&#30284;&#30740;&#31350;&#20013;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.16038</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25913;&#36827;&#32925;&#32454;&#32990;&#30284;&#30740;&#31350;&#20013;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16038
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#38382;&#31572;&#31995;&#32479;&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#23601;&#65292;&#23588;&#20854;&#26159;&#22312;&#32925;&#32454;&#32990;&#30284;&#30740;&#31350;&#20013;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#21463;&#30410;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#21033;&#29992;&#35832;&#22914;GPU&#21644;TPU&#31561;&#24378;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20687;BERT&#21644;GPT-3&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#19979;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#21508;&#31181;&#20219;&#21153;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#65292;&#21253;&#25324;&#35821;&#20041;&#29702;&#35299;&#12289;&#26234;&#33021;&#20889;&#20316;&#21644;&#25512;&#29702;&#65292;&#20026;&#26356;&#36890;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;NLP&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26469;&#24357;&#21512;&#20154;&#19982;&#35745;&#31639;&#26426;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;NLP&#30340;&#24403;&#21069;&#26684;&#23616;&#21644;&#26410;&#26469;&#23637;&#26395;&#65292;&#37325;&#28857;&#25918;&#22312;&#36825;&#19968;&#39046;&#22495;&#20869;&#30340;&#38382;&#31572;&#31995;&#32479;&#19978;&#12290;&#20998;&#26512;&#20102;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#38382;&#31572;&#31995;&#32479;&#30340;&#23454;&#38469;&#26696;&#20363;&#21644;&#21457;&#23637;&#65292;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16038v1 Announce Type: cross  Abstract: In recent years, advancements in natural language processing (NLP) have been fueled by deep learning techniques, particularly through the utilization of powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3, trained on vast amounts of data, have revolutionized language understanding and generation. These pre-trained models serve as robust bases for various tasks including semantic understanding, intelligent writing, and reasoning, paving the way for a more generalized form of artificial intelligence. NLP, as a vital application of AI, aims to bridge the gap between humans and computers through natural language interaction. This paper delves into the current landscape and future prospects of large-scale model-based NLP, focusing on the question-answering systems within this domain. Practical cases and developments in artificial intelligence-driven question-answering systems are analyzed to foster further explora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#30340;&#26680;&#24515;&#24212;&#29992;&#65292;&#21253;&#25324;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#25512;&#33616;&#31995;&#32479;&#31561;&#26041;&#38754;&#65292;&#22312;&#33258;&#21160;&#29983;&#25104;&#20135;&#21697;&#25551;&#36848;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#26500;&#24314;&#21644;&#23458;&#26381;&#23545;&#35805;&#33258;&#21160;&#22788;&#29702;&#31561;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#31215;&#26497;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16035</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#26234;&#33021;&#30005;&#23376;&#21830;&#21153;&#25512;&#33616;&#30340;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#30340;&#26680;&#24515;&#24212;&#29992;&#65292;&#21253;&#25324;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#25512;&#33616;&#31995;&#32479;&#31561;&#26041;&#38754;&#65292;&#22312;&#33258;&#21160;&#29983;&#25104;&#20135;&#21697;&#25551;&#36848;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#26500;&#24314;&#21644;&#23458;&#26381;&#23545;&#35805;&#33258;&#21160;&#22788;&#29702;&#31561;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#31215;&#26497;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;Transformer&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20219;&#21153;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#65292;&#36825;&#20123;&#27169;&#22411;&#29305;&#21035;&#24191;&#27867;&#20351;&#29992;&#65292;&#20174;&#25991;&#26412;&#29702;&#35299;&#21040;&#29983;&#25104;&#25512;&#33616;&#31995;&#32479;&#65292;&#20026;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#21644;&#20248;&#21270;&#26381;&#21153;&#27969;&#31243;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25216;&#26415;&#25903;&#25345;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30005;&#23376;&#21830;&#21153;&#25991;&#26412;&#29702;&#35299;&#21644;&#25512;&#33616;&#29983;&#25104;&#20013;&#30340;&#26680;&#24515;&#24212;&#29992;&#22330;&#26223;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#20135;&#21697;&#25551;&#36848;&#30340;&#33258;&#21160;&#29983;&#25104;&#65292;&#29992;&#25143;&#35780;&#35770;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#26500;&#24314;&#20197;&#21450;&#23458;&#25143;&#26381;&#21153;&#23545;&#35805;&#30340;&#33258;&#21160;&#22788;&#29702;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#23454;&#29616;&#36807;&#31243;&#21644;&#29305;&#23450;&#26696;&#20363;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#20854;&#29420;&#29305;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16035v1 Announce Type: cross  Abstract: With the rapid development of artificial intelligence technology, Transformer structural pre-training model has become an important tool for large language model (LLM) tasks. In the field of e-commerce, these models are especially widely used, from text understanding to generating recommendation systems, which provide powerful technical support for improving user experience and optimizing service processes. This paper reviews the core application scenarios of Transformer pre-training model in e-commerce text understanding and recommendation generation, including but not limited to automatic generation of product descriptions, sentiment analysis of user comments, construction of personalized recommendation system and automated processing of customer service conversations. Through a detailed analysis of the model's working principle, implementation process, and application effects in specific cases, this paper emphasizes the unique advan
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#65292;&#21457;&#29616;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;BERT&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.16034</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#36827;&#34892;&#24773;&#32490;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Emotion Classification in Short English Texts using Deep Learning Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#65292;&#21457;&#29616;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;BERT&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#20013;&#30340;&#26377;&#38480;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#24773;&#32490;&#26159;&#19968;&#39033;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#26694;&#26550;&#21644;&#35745;&#31639;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#30701;&#33521;&#25991;&#25991;&#26412;&#20013;&#35782;&#21035;&#24773;&#32490;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#30740;&#31350;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#35789;&#23884;&#20837;&#65292;&#29305;&#21035;&#26159;BERT&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;SmallEnglishEmotions&#8221;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;6372&#20010;&#24102;&#26377;&#20116;&#31181;&#20027;&#35201;&#24773;&#32490;&#31867;&#21035;&#27880;&#37322;&#30340;&#19981;&#21516;&#30701;&#27874;&#26031;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#23884;&#20837;&#22312;&#20934;&#30830;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26412;&#26041;&#38754;&#20248;&#20110;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16034v1 Announce Type: cross  Abstract: Detecting emotions in limited text datasets from under-resourced languages presents a formidable obstacle, demanding specialized frameworks and computational strategies. This study conducts a thorough examination of deep learning techniques for discerning emotions in short English texts. Deep learning approaches employ transfer learning and word embedding, notably BERT, to attain superior accuracy. To evaluate these methods, we introduce the "SmallEnglishEmotions" dataset, comprising 6372 varied short Persian texts annotated with five primary emotion categories. Our experiments reveal that transfer learning and BERT-based text embedding outperform alternative methods in accurately categorizing the text in the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30340;&#26657;&#20934;&#65288;VCB&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#36890;&#29992;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16030</link><description>&lt;p&gt;
&#19981;&#35201;&#24536;&#35760;&#24744;&#30340;&#22870;&#21169;&#20215;&#20540;: &#36890;&#36807;&#22522;&#20110;&#20215;&#20540;&#30340;&#26657;&#20934;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#30340;&#26657;&#20934;&#65288;VCB&#65289;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#36890;&#29992;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#31639;&#27861;&#22797;&#26434;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#30340;&#25285;&#24551;&#65292;&#25552;&#35758;&#19968;&#31995;&#21015;&#22522;&#20110;&#39034;&#24207;&#30340;&#26657;&#20934;&#26041;&#27861;&#20316;&#20026;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#24403;&#21069;&#22522;&#20110;&#39034;&#24207;&#30340;&#26041;&#27861;&#65292;&#26816;&#26597;&#23427;&#20204;&#22312;&#21033;&#29992;&#22870;&#21169;&#20215;&#20540;&#21644;&#35299;&#20915;&#19981;&#23545;&#40784;&#38382;&#39064;&#26041;&#38754;&#30340;&#20302;&#25928;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20215;&#20540;&#30340;&#26657;&#20934;&#65288;VCB&#65289;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VCB&#22312;AI&#21161;&#25163;&#21644;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36890;&#29992;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16030v1 Announce Type: cross  Abstract: While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs), recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives. This paper delves further into current order-based methods, examining their inefficiencies in utilizing reward values and addressing misalignment issues. Building upon these findings, we propose a novel \textbf{V}alue-based \textbf{C}ali\textbf{B}ration (VCB) method to better align LLMs with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets, providing impressive generalizability, robustness, and stability in diverse settings.
&lt;/p&gt;</description></item><item><title>GraphWiz&#26159;&#19968;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#35299;&#20915;&#21508;&#31181;&#22270;&#38382;&#39064;&#31867;&#22411;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#36229;&#36807;&#20102;GPT-4&#30340;43.8%&#12290;</title><link>https://arxiv.org/abs/2402.16029</link><description>&lt;p&gt;
GraphWiz&#65306;&#29992;&#20110;&#22270;&#38382;&#39064;&#30340;&#25351;&#20196;&#36319;&#38543;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphWiz: An Instruction-Following Language Model for Graph Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16029
&lt;/p&gt;
&lt;p&gt;
GraphWiz&#26159;&#19968;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#35299;&#20915;&#21508;&#31181;&#22270;&#38382;&#39064;&#31867;&#22411;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#36229;&#36807;&#20102;GPT-4&#30340;43.8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#29702;&#35299;&#21644;&#35299;&#20915;&#22797;&#26434;&#22270;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GraphInstruct&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22788;&#29702;&#21508;&#31181;&#22270;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#26126;&#30830;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;&#21033;&#29992;GraphInstruct&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;GraphWiz&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#22270;&#38382;&#39064;&#31867;&#22411;&#24182;&#29983;&#25104;&#28165;&#26224;&#25512;&#29702;&#36807;&#31243;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26694;&#26550;&#32435;&#20837;&#22270;&#38382;&#39064;&#27714;&#35299;&#29615;&#22659;&#20013;&#12290;&#22686;&#24378;&#27169;&#22411;GraphWiz-DPO&#22312;&#20061;&#20010;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24615;&#27700;&#24179;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;65%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;43.8%&#30340;GPT-4&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16029v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive success across several fields, but their proficiency in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive instruction-tuning dataset designed to equip language models with the ability to tackle a broad spectrum of graph problems using explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various graph problem types while generating clear reasoning processes. To enhance the model's capability and reliability, we incorporate the Direct Preference Optimization (DPO) framework into the graph problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Moreover, our research delves into the d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;HiGPT&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#24322;&#36136;&#22270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#27867;&#21270;&#38480;&#21046;&#21644;&#20998;&#24067;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16024</link><description>&lt;p&gt;
HiGPT&#65306;&#24322;&#36136;&#22270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HiGPT: Heterogeneous Graph Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16024
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;HiGPT&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#24322;&#36136;&#22270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#27867;&#21270;&#38480;&#21046;&#21644;&#20998;&#24067;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#23398;&#20064;&#26088;&#22312;&#25429;&#25417;&#24322;&#26500;&#22270;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#22810;&#26679;&#21270;&#20851;&#31995;&#35821;&#20041;&#65292;&#20197;&#33719;&#24471;&#33410;&#28857;&#21644;&#36793;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#12290;&#26368;&#36817;&#22312;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#32771;&#34385;&#20851;&#31995;&#30340;&#24322;&#36136;&#24615;&#24182;&#20351;&#29992;&#19987;&#38376;&#30340;&#28040;&#24687;&#20989;&#25968;&#21644;&#32858;&#21512;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26694;&#26550;&#22312;&#27867;&#21270;&#21040;&#19981;&#21516;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#26694;&#26550;&#37117;&#36981;&#24490;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#30340;&#8220;&#39044;&#35757;&#32451;&#8221;&#21644;&#8220;&#24494;&#35843;&#8221;&#33539;&#24335;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#36866;&#24212;&#26032;&#30340;&#21644;&#30475;&#19981;&#35265;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#23558;&#24322;&#36136;&#22270;&#27169;&#22411;&#27867;&#21270;&#20026;&#36866;&#24212;&#20855;&#26377;&#33410;&#28857;&#20196;&#29260;&#38598;&#21644;&#20851;&#31995;&#31867;&#22411;&#24322;&#36136;&#24615;&#20998;&#24067;&#21464;&#21270;&#30340;&#19981;&#21516;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#65311;&#8221;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16024v1 Announce Type: new  Abstract: Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges. Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets. Most of these frameworks follow the "pre-train" and "fine-tune" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: "Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?'' To tackle those challenges, we p
&lt;/p&gt;</description></item><item><title>&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#22312;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#23454;&#29616;&#20102;&#19977;&#27169;&#32763;&#35793;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16021</link><description>&lt;p&gt;
TMT: &#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#35270;&#20026;&#19981;&#21516;&#35821;&#35328;&#26469;&#23454;&#29616;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#19977;&#27169;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16021
&lt;/p&gt;
&lt;p&gt;
&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#22312;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#23454;&#29616;&#20102;&#19977;&#27169;&#32763;&#35793;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20849;&#21516;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#27491;&#22312;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#37197;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#22823;&#37327;&#35745;&#31639;&#35201;&#27714;&#38459;&#30861;&#20102;&#21457;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27169;&#32763;&#35793;&#65288;TMT&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28085;&#30422;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20219;&#24847;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#21363;&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#24182;&#23558;&#22810;&#27169;&#24577;&#32763;&#35793;&#35270;&#20026;&#19968;&#20010;&#25104;&#29087;&#30340;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#35821;&#38899;&#21644;&#22270;&#20687;&#25968;&#25454;&#26631;&#35760;&#20026;&#31163;&#25955;&#26631;&#35760;&#65292;&#25552;&#20379;&#20102;&#36328;&#27169;&#24577;&#30340;&#32479;&#19968;&#25509;&#21475;&#65292;&#24182;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#25552;&#20986;&#30340;TMT&#20013;&#65292;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#36827;&#34892;&#26680;&#24515;&#32763;&#35793;&#65292;&#32780;&#27169;&#24577;&#29305;&#23450;&#22788;&#29702;&#20165;&#22312;&#26631;&#35760;&#21270;&#21644;&#21435;&#26631;&#35760;&#21270;&#38454;&#27573;&#20869;&#36827;&#34892;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#20845;&#31181;&#27169;&#24577;&#19978;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;TMT&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16021v1 Announce Type: cross  Abstract: The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35770;&#25991;&#26469;&#28304;&#36861;&#36394;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#26500;&#24314;&#20102;&#39640;&#36136;&#37327;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#38598; PST-Bench&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#20027;&#39064;&#20043;&#38388;&#30340;&#28436;&#21270;&#27169;&#24335;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#35813;&#38382;&#39064;&#30340;&#38590;&#24230;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.16009</link><description>&lt;p&gt;
PST-Bench: &#36861;&#36394;&#21644;&#22522;&#20934;&#27979;&#35797;&#35770;&#25991;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
PST-Bench: Tracing and Benchmarking the Source of Publications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16009
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35770;&#25991;&#26469;&#28304;&#36861;&#36394;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#26500;&#24314;&#20102;&#39640;&#36136;&#37327;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#38598; PST-Bench&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#20027;&#39064;&#20043;&#38388;&#30340;&#28436;&#21270;&#27169;&#24335;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#35813;&#38382;&#39064;&#30340;&#38590;&#24230;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36861;&#36394;&#30740;&#31350;&#35770;&#25991;&#30340;&#26469;&#28304;&#26159;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#19968;&#39033;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#35770;&#25991;&#20043;&#38388;&#25968;&#21313;&#20159;&#32423;&#30340;&#24341;&#29992;&#20851;&#31995;&#38459;&#30861;&#20102;&#30740;&#31350;&#20154;&#21592;&#26377;&#25928;&#29702;&#35299;&#31185;&#23398;&#30340;&#28436;&#21464;&#36807;&#31243;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20173;&#28982;&#32570;&#20047;&#30001;&#19987;&#19994;&#30740;&#31350;&#20154;&#21592;&#26500;&#24314;&#30340;&#20934;&#30830;&#19988;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35782;&#21035;&#30740;&#31350;&#35770;&#25991;&#30340;&#30452;&#25509;&#26469;&#28304;&#65292;&#22522;&#20110;&#36825;&#19968;&#28857;&#65292;&#21487;&#20197;&#24320;&#21457;&#33258;&#21160;&#31639;&#27861;&#26469;&#25193;&#23637;&#31185;&#23398;&#30340;&#28436;&#21464;&#30693;&#35782;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35770;&#25991;&#26469;&#28304;&#36861;&#36394;&#65288;PST&#65289;&#38382;&#39064;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#38598; PST-Bench&#12290;&#22522;&#20110; PST-Bench&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#65292;&#20363;&#22914;&#19981;&#21516;&#20027;&#39064;&#20043;&#38388;&#30340;&#28436;&#21270;&#27169;&#24335;&#24046;&#24322;&#12290;&#23545;&#21508;&#31181;&#26041;&#27861;&#30340;&#25506;&#32034;&#20984;&#26174;&#20102; PST-Bench &#30340;&#38590;&#24230;&#65292;&#25351;&#20986;&#20102;&#35813;&#20027;&#39064;&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#24050;&#22312; https://github.com &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16009v1 Announce Type: cross  Abstract: Tracing the source of research papers is a fundamental yet challenging task for researchers. The billion-scale citation relations between papers hinder researchers from understanding the evolution of science efficiently. To date, there is still a lack of an accurate and scalable dataset constructed by professional researchers to identify the direct source of their studied papers, based on which automatic algorithms can be developed to expand the evolutionary knowledge of science. In this paper, we study the problem of paper source tracing (PST) and construct a high-quality and ever-increasing dataset PST-Bench in computer science. Based on PST-Bench, we reveal several intriguing discoveries, such as the differing evolution patterns across various topics. An exploration of various methods underscores the hardness of PST-Bench, pinpointing potential directions on this topic. The dataset and codes have been available at https://github.com
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Adversarial Suffixes Embedding Translation Framework&#65292;&#23558;&#19981;&#21487;&#35835;&#30340;&#25932;&#23545;&#21518;&#32512;&#32763;&#35793;&#20026;&#36830;&#36143;&#12289;&#21487;&#35835;&#30340;&#25991;&#26412;&#65292;&#26377;&#21161;&#20110;&#26356;&#23481;&#26131;&#29702;&#35299;&#21644;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.16006</link><description>&lt;p&gt;
&#20174;&#22122;&#38899;&#21040;&#28165;&#26224;&#65306;&#36890;&#36807;&#25991;&#26412;&#23884;&#20837;&#30340;&#32763;&#35793;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25915;&#20987;&#30340;&#25932;&#23545;&#21518;&#32512;
&lt;/p&gt;
&lt;p&gt;
From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16006
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Adversarial Suffixes Embedding Translation Framework&#65292;&#23558;&#19981;&#21487;&#35835;&#30340;&#25932;&#23545;&#21518;&#32512;&#32763;&#35793;&#20026;&#36830;&#36143;&#12289;&#21487;&#35835;&#30340;&#25991;&#26412;&#65292;&#26377;&#21161;&#20110;&#26356;&#23481;&#26131;&#29702;&#35299;&#21644;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#38450;&#24481;&#26041;&#27861;&#20173;&#28982;&#26377;&#38480;&#65292;&#22240;&#20026;&#21361;&#38505;&#25552;&#31034;&#34987;&#25163;&#24037;&#31574;&#21010;&#20026;&#20165;&#20960;&#31181;&#24050;&#30693;&#30340;&#25915;&#20987;&#31867;&#22411;&#65292;&#36825;&#20007;&#22833;&#20102;&#19982;&#26032;&#20852;&#21464;&#20307;&#21516;&#27493;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26377;&#23475;&#25351;&#20196;&#21518;&#28155;&#21152;&#21518;&#32512;&#21487;&#20197;&#31361;&#30772;LLMs&#30340;&#38450;&#24481;&#65292;&#24182;&#23548;&#33268;&#21361;&#38505;&#36755;&#20986;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#19981;&#21487;&#35835;&#24615;&#65292;&#23384;&#22312;&#19968;&#31181;&#23380;&#38553;&#65292;&#20351;&#24471;&#36890;&#36807;&#24120;&#35265;&#30340;&#38450;&#24481;&#26041;&#27861;&#22914;&#22256;&#24785;&#24230;&#36807;&#28388;&#22120;&#30456;&#23545;&#23481;&#26131;&#30475;&#31359;&#36825;&#31181;&#23545;&#25239;&#24615;&#21518;&#32512;&#30340;&#20869;&#22312;&#26426;&#21046;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25932;&#23545;&#21518;&#32512;&#23884;&#20837;&#32763;&#35793;&#26694;&#26550;&#65288;ASETF&#65289;&#65292;&#21487;&#20197;&#23558;&#19981;&#21487;&#35835;&#30340;&#25932;&#23545;&#21518;&#32512;&#32763;&#35793;&#25104;&#36830;&#36143;&#30340;&#21487;&#35835;&#25991;&#26412;&#65292;&#20174;&#32780;&#26356;&#23481;&#26131;&#29702;&#35299;&#21644;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#22312;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#22914;LLaMa2&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16006v1 Announce Type: new  Abstract: The safety defense methods of Large language models(LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. This method, while effective, leaves a gap in understanding the underlying mechanics of such adversarial suffix due to the non-readability and it can be relatively easily seen through by common defense methods such as perplexity filters.To cope with this challenge, in this paper, we propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that are able to translate the unreadable adversarial suffixes into coherent, readable text, which makes it easier to understand and analyze the reasons behind harmful content generation by large language models. We conducted experiments on LLMs such as LLaMa2, 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#25512;&#25991;&#20197;&#30830;&#23450;&#23458;&#25143;&#28385;&#24847;&#27700;&#24179;&#65292;&#26377;&#21161;&#20110;&#31616;&#21270;&#30740;&#31350;&#25104;&#21315;&#19978;&#19975;&#26465;&#25512;&#25991;&#24182;&#25913;&#36827;&#33322;&#31354;&#20844;&#21496;&#26381;&#21153;&#30340;&#32321;&#29712;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.15992</link><description>&lt;p&gt;
&#20174;&#22810;&#20010;&#25512;&#25991;&#21442;&#25968;&#20013;&#26816;&#27979;&#23458;&#25143;&#28385;&#24847;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Approach to Detect Customer Satisfaction From Multiple Tweet Parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15992
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#25512;&#25991;&#20197;&#30830;&#23450;&#23458;&#25143;&#28385;&#24847;&#27700;&#24179;&#65292;&#26377;&#21161;&#20110;&#31616;&#21270;&#30740;&#31350;&#25104;&#21315;&#19978;&#19975;&#26465;&#25512;&#25991;&#24182;&#25913;&#36827;&#33322;&#31354;&#20844;&#21496;&#26381;&#21153;&#30340;&#32321;&#29712;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20114;&#32852;&#32593;&#25216;&#26415;&#24471;&#20197;&#21457;&#23637;&#20197;&#26469;&#65292;&#23458;&#25143;&#28385;&#24847;&#24230;&#24050;&#25104;&#20026;&#20225;&#19994;&#21457;&#23637;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#22312;&#32447;&#24179;&#21488;&#24050;&#25104;&#20026;&#20998;&#20139;&#35780;&#35770;&#30340;&#20027;&#35201;&#22330;&#25152;&#20043;&#19968;&#12290;Twitter&#26159;&#20854;&#20013;&#20043;&#19968;&#65292;&#23458;&#25143;&#32463;&#24120;&#22312;&#35813;&#24179;&#21488;&#19978;&#21457;&#24067;&#20182;&#20204;&#30340;&#24819;&#27861;&#12290;&#22312;&#36825;&#20123;&#24179;&#21488;&#19978;&#23545;&#33322;&#29677;&#30340;&#35780;&#35770;&#24050;&#25104;&#20026;&#33322;&#31354;&#20844;&#21496;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#31215;&#26497;&#30340;&#35780;&#35770;&#21487;&#20197;&#24110;&#21161;&#20844;&#21496;&#22686;&#38271;&#65292;&#32780;&#28040;&#26497;&#30340;&#35780;&#35770;&#21017;&#21487;&#33021;&#36805;&#36895;&#30772;&#22351;&#20854;&#25910;&#20837;&#21644;&#22768;&#35465;&#12290;&#22240;&#27492;&#65292;&#23545;&#33322;&#31354;&#20844;&#21496;&#26469;&#35828;&#65292;&#23457;&#26597;&#23458;&#25143;&#30340;&#21453;&#39304;&#21644;&#20307;&#39564;&#12289;&#25913;&#36827;&#20854;&#26381;&#21153;&#20197;&#20445;&#25345;&#31454;&#20105;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#25104;&#21315;&#19978;&#19975;&#26465;&#25512;&#25991;&#24182;&#20998;&#26512;&#23427;&#20204;&#20197;&#30830;&#23450;&#23458;&#25143;&#28385;&#24847;&#24230;&#26159;&#19968;&#39033;&#30456;&#24403;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#25512;&#25991;&#20197;&#30830;&#23450;&#23458;&#25143;&#28385;&#24847;&#27700;&#24179;&#21487;&#20197;&#31616;&#21270;&#36825;&#19968;&#36153;&#26102;&#30340;&#36807;&#31243;&#12290;&#24050;&#32463;&#26377;&#19968;&#20123;&#20851;&#20110;&#36825;&#31181;&#31574;&#30053;&#30340;&#24037;&#20316;&#21487;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#20351;&#29992;&#26426;&#22120;&#23398;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15992v1 Announce Type: cross  Abstract: Since internet technologies have advanced, one of the primary factors in company development is customer happiness. Online platforms have become prominent places for sharing reviews. Twitter is one of these platforms where customers frequently post their thoughts. Reviews of flights on these platforms have become a concern for the airline business. A positive review can help the company grow, while a negative one can quickly ruin its revenue and reputation. So it's vital for airline businesses to examine the feedback and experiences of their customers and enhance their services to remain competitive. But studying thousands of tweets and analyzing them to find the satisfaction of the customer is quite a difficult task. This tedious process can be made easier by using a machine learning approach to analyze tweets to determine client satisfaction levels. Some work has already been done on this strategy to automate the procedure using mach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32622;&#20449;&#24230;&#26657;&#20934;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#23567;&#27169;&#22411;&#30340;&#25512;&#26029;&#25928;&#29575;&#24182;&#35299;&#20915;&#28145;&#24230;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#21644;&#36328;&#35821;&#35328;&#32622;&#20449;&#24230;&#20998;&#24067;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15991</link><description>&lt;p&gt;
$C^3$: &#29992;&#20110;&#39640;&#25928;&#36328;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#27169;&#22411;&#32423;&#32852;
&lt;/p&gt;
&lt;p&gt;
$C^3$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32622;&#20449;&#24230;&#26657;&#20934;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#23567;&#27169;&#22411;&#30340;&#25512;&#26029;&#25928;&#29575;&#24182;&#35299;&#20915;&#28145;&#24230;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#21644;&#36328;&#35821;&#35328;&#32622;&#20449;&#24230;&#20998;&#24067;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;(NLU)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#24050;&#32463;&#30475;&#21040;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(mPLMs)&#26174;&#33879;&#22686;&#24378;&#20102;&#36825;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;mPLMs&#22312;&#25512;&#26029;&#26399;&#38388;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#24182;&#20135;&#29983;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#36825;&#32473;&#22312;&#30495;&#23454;&#19990;&#30028;&#21644;&#23454;&#26102;&#31995;&#32479;&#20013;&#37096;&#32626;&#24102;&#26469;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#36890;&#36807;&#36138;&#23146;&#22320;&#22522;&#20110;&#27169;&#22411;&#32622;&#20449;&#24230;&#24471;&#20998;&#20174;&#21508;&#31181;&#27169;&#22411;&#20013;&#36873;&#25321;&#33021;&#22815;&#22788;&#29702;&#24403;&#21069;&#36755;&#20837;&#30340;&#26368;&#36731;&#37327;&#27169;&#22411;&#26469;&#22686;&#24378;&#25512;&#26029;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#27169;&#22411;&#24448;&#24448;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#65292;&#32780;&#19988;&#32622;&#20449;&#24230;&#20998;&#24067;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#26377;&#25152;&#21464;&#21270;&#12290;&#36825;&#23548;&#33268;&#36739;&#23567;&#27169;&#22411;&#21457;&#20986;&#33258;&#20449;&#20294;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#65292;&#38459;&#30861;&#23427;&#20204;&#26377;&#25928;&#22320;&#22312;&#27979;&#35797;&#35821;&#35328;&#20013;&#36827;&#34892;&#27867;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32622;&#20449;&#24230;&#26657;&#20934;&#27169;&#22411;&#32423;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15991v1 Announce Type: new  Abstract: Cross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP). Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#22120;&#20013;&#30340;&#20284;&#28982;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15987</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20559;&#24046;&#30340;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Mitigation of Evaluation Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#22120;&#20013;&#30340;&#20284;&#28982;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20284;&#28982;&#20316;&#20026;&#34913;&#37327;LLM&#23545;&#21477;&#23376;&#21487;&#20449;&#24230;&#30340;&#25351;&#26631;&#65292;&#21487;&#33021;&#20250;&#22240;&#21477;&#23376;&#34920;&#38754;&#24046;&#24322;&#65288;&#22914;&#35789;&#24207;&#21644;&#21477;&#23376;&#32467;&#26500;&#65289;&#32780;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#23558;LLMs&#29992;&#20110;&#35780;&#20272;&#65292;&#21487;&#33021;&#23384;&#22312;&#20284;&#28982;&#20559;&#24046;&#65306;&#23427;&#20204;&#21487;&#33021;&#20250;&#39640;&#20272;&#20855;&#26377;&#36739;&#39640;&#20284;&#28982;&#24615;&#30340;&#21477;&#23376;&#65292;&#32780;&#20302;&#20272;&#20855;&#26377;&#36739;&#20302;&#20284;&#28982;&#24615;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#23545;LLM&#35780;&#20272;&#22120;&#20013;&#20284;&#28982;&#20559;&#24046;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#20284;&#28982;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39640;&#24230;&#20559;&#32622;&#30340;&#23454;&#20363;&#20316;&#20026;&#23569;&#26679;&#26412;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#35780;&#20272;&#25968;&#25454;&#21040;&#25991;&#26412;&#21644;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#20219;&#21153;&#26102;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#27979;&#35797;&#30340;&#20960;&#31181;LLMs&#26174;&#31034;&#20986;&#20284;&#28982;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#20943;&#36731;&#20102;&#36825;&#31181;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15987v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also impr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;HuBERT&#23454;&#29616;&#20102;&#23545;&#29356;&#21483;&#22768;&#30340;&#22768;&#32032;&#26631;&#31614;&#20998;&#31867;&#21644;&#35789;&#27719;&#35782;&#21035;&#65292;&#21457;&#29616;&#20102;&#20855;&#26377;&#26174;&#33879;&#22768;&#23398;&#19968;&#33268;&#24615;&#30340;&#29356;&#35789;&#27719;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;Web&#31995;&#32479;&#26631;&#35760;&#29399;&#21483;&#22768;&#20013;&#30340;&#22768;&#32032;n-gram&#12290;</title><link>https://arxiv.org/abs/2402.15985</link><description>&lt;p&gt;
&#20351;&#29992;HuBERT&#36827;&#34892;&#23545;&#29356;&#35821;&#35328;&#30340;&#35821;&#38899;&#21644;&#35789;&#27719;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Phonetic and Lexical Discovery of a Canine Language using HuBERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15985
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;HuBERT&#23454;&#29616;&#20102;&#23545;&#29356;&#21483;&#22768;&#30340;&#22768;&#32032;&#26631;&#31614;&#20998;&#31867;&#21644;&#35789;&#27719;&#35782;&#21035;&#65292;&#21457;&#29616;&#20102;&#20855;&#26377;&#26174;&#33879;&#22768;&#23398;&#19968;&#33268;&#24615;&#30340;&#29356;&#35789;&#27719;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;Web&#31995;&#32479;&#26631;&#35760;&#29399;&#21483;&#22768;&#20013;&#30340;&#22768;&#32032;n-gram&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#29356;&#21483;&#22768;&#20013;&#28508;&#22312;&#27807;&#36890;&#27169;&#24335;&#30340;&#24320;&#21019;&#24615;&#25506;&#32034;&#65292;&#24182;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#35821;&#35328;&#20998;&#26512;&#38556;&#30861;&#65292;&#22823;&#37327;&#20381;&#36182;&#20110;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#21644;&#26377;&#38480;&#25968;&#25454;&#38598;&#26469;&#21457;&#29616;&#29399;&#30340;&#21483;&#22768;&#20013;&#30340;&#22768;&#38899;&#21333;&#20803;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;HuBERT&#23454;&#29616;&#20102;&#22768;&#32032;&#26631;&#31614;&#30340;&#20934;&#30830;&#20998;&#31867;&#65292;&#24182;&#35782;&#21035;&#20102;&#26263;&#31034;&#29356;&#21483;&#22768;&#20013;&#19968;&#31181;&#22522;&#30784;&#35789;&#27719;&#30340;&#22768;&#38899;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#30830;&#23450;&#30340;&#29356;&#35789;&#27719;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#22768;&#23398;&#19968;&#33268;&#24615;&#65292;&#35206;&#30422;&#20102;&#25152;&#26377;&#35266;&#23519;&#21040;&#30340;&#29356;&#21483;&#22768;&#24207;&#21015;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#29356;&#21483;&#22768;&#26631;&#35760;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#29992;&#25143;&#19978;&#20256;&#30340;&#29399;&#21483;&#22768;&#20013;&#31361;&#20986;&#26174;&#31034;&#35789;&#27719;&#20013;&#23384;&#22312;&#30340;&#22768;&#32032;n-gram&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15985v1 Announce Type: cross  Abstract: This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. We present a self-supervised approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. We further develop a web-based dog vocalization labeling system. This system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#33268;&#21147;&#20110;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26049;&#36974;&#26222;&#35821;&#21040;&#33521;&#35821;&#30340;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#20840;&#29699;&#35821;&#38899;&#25216;&#26415;&#35206;&#30422;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15967</link><description>&lt;p&gt;
&#20351;&#29992;&#31163;&#25955;&#21333;&#20803;&#36827;&#34892;&#30452;&#25509;&#26049;&#36974;&#26222;&#35821;&#21040;&#33521;&#35821;&#30340;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Direct Punjabi to English speech translation using discrete units
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15967
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#33268;&#21147;&#20110;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26049;&#36974;&#26222;&#35821;&#21040;&#33521;&#35821;&#30340;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#20840;&#29699;&#35821;&#38899;&#25216;&#26415;&#35206;&#30422;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#23578;&#26410;&#36798;&#21040;&#19982;&#25991;&#26412;&#21040;&#25991;&#26412;&#32763;&#35793;&#31995;&#32479;&#30456;&#21516;&#30340;&#35206;&#30422;&#27700;&#24179;&#12290;&#24403;&#21069;&#30340;&#35821;&#38899;&#25216;&#26415;&#22312;&#20840;&#29699;&#36229;&#36807;7000&#31181;&#35821;&#35328;&#20013;&#30340;&#35206;&#30422;&#33539;&#22260;&#19978;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#65292;&#23548;&#33268;&#36229;&#36807;&#19968;&#21322;&#30340;&#20154;&#21475;&#26080;&#27861;&#33719;&#24471;&#36825;&#31181;&#25216;&#26415;&#21644;&#20849;&#20139;&#20307;&#39564;&#12290;&#38543;&#30528;&#35821;&#38899;&#21161;&#25163;&#25216;&#26415;&#65288;&#20363;&#22914;&#31038;&#20132;&#26426;&#22120;&#20154;&#21644;&#35821;&#38899;&#36716;&#25991;&#26412;&#24212;&#29992;&#31243;&#24207;&#65289;&#20197;&#21450;&#21548;&#35273;&#20869;&#23481;&#65288;&#20363;&#22914;&#25773;&#23458;&#21644;&#35762;&#24231;&#65289;&#30340;&#20852;&#36215;&#65292;&#30830;&#20445;&#25216;&#26415;&#23545;&#25152;&#26377;&#20154;&#37117;&#21487;&#29992;&#21464;&#24471;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#21152;&#37325;&#35201;&#12290;&#35821;&#38899;&#32763;&#35793;&#21487;&#20197;&#22312;&#32531;&#35299;&#25216;&#26415;&#24046;&#36317;&#21644;&#21019;&#36896;&#19968;&#20010;&#26356;&#20855;&#21253;&#23481;&#24615;&#31038;&#20250;&#26041;&#38754;&#21457;&#25381;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#20570;&#20986;&#36129;&#29486;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21360;&#24230;&#35821;&#35328;&#20043;&#19968;&#30340;&#26049;&#36974;&#26222;&#35821;&#21040;&#33521;&#35821;&#30340;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15967v1 Announce Type: new  Abstract: Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems. The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences. With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever. Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society. With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English. Additionally, we explore the performance of using a discrete representation of speech call
&lt;/p&gt;</description></item><item><title>GreenLLaMA&#26159;&#19968;&#31181;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35299;&#27602;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#24179;&#21488;&#35821;&#26009;&#24211;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15951</link><description>&lt;p&gt;
GreenLLaMA: &#19968;&#31181;&#24102;&#26377;&#35299;&#37322;&#30340;&#35299;&#27602;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GreenLLaMA: A Framework for Detoxification with Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15951
&lt;/p&gt;
&lt;p&gt;
GreenLLaMA&#26159;&#19968;&#31181;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35299;&#27602;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#24179;&#21488;&#35821;&#26009;&#24211;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#35299;&#27602;&#30340;&#30740;&#31350;&#24037;&#20316;&#20998;&#25955;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#65292;&#22240;&#20026;&#23427;&#20204;&#24182;&#27809;&#26377;&#28085;&#30422;&#21040;&#30495;&#23454;&#22330;&#26223;&#20013;&#25152;&#38656;&#30340;&#25152;&#26377;&#35299;&#27602;&#26041;&#38754;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#24320;&#21457;&#35299;&#27602;&#27169;&#22411;&#30340;&#20219;&#21153;&#23616;&#38480;&#22312;&#20165;&#35265;&#36807;&#30340;&#24179;&#21488;&#23376;&#38598;&#19978;&#65292;&#27809;&#26377;&#25506;&#35752;&#27169;&#22411;&#22312;&#26410;&#30693;&#24179;&#21488;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#24037;&#20316;&#27809;&#26377;&#35299;&#20915;&#19981;&#21487;&#35299;&#27602;&#24615;&#36825;&#19968;&#29616;&#35937;&#65292;&#21363;&#27602;&#24615;&#25991;&#26412;&#26080;&#27861;&#22312;&#19981;&#25913;&#21464;&#21547;&#20041;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35299;&#27602;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GreenLLaMA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35299;&#27602;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#36731;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#36328;&#24179;&#21488;&#20266;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#24212;&#29992;&#22810;&#27493;&#25968;&#25454;&#22788;&#29702;&#21644;&#29983;&#25104;&#31574;&#30053;&#21033;&#29992;ChatGPT&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36328;&#24179;&#21488;&#35821;&#26009;&#24211;&#35757;&#32451;&#19968;&#22871;&#35299;&#27602;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#27602;&#27169;&#22411;&#20248;&#20110;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15951v1 Announce Type: cross  Abstract: Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose GreenLLaMA, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15938</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#25110;&#35760;&#24518;&#65306;&#25968;&#25454;&#27745;&#26579;&#19982;&#21487;&#20449;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#30340;&#35828;&#27861;&#36890;&#24120;&#26159;&#36890;&#36807;&#22312;&#24320;&#25918;&#33719;&#21462;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#26469;&#25903;&#25345;&#30340;&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24222;&#22823;&#35268;&#27169;&#21644;&#24191;&#27867;&#26469;&#28304;&#65292;&#23427;&#21487;&#33021;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#21253;&#21547;&#27979;&#35797;&#25968;&#25454;&#65292;&#23548;&#33268;LLMs&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#20197;&#21450;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#23545;&#20110;LLMs&#26469;&#35828;&#26816;&#27979;&#21644;&#20943;&#36731;&#25968;&#25454;&#27745;&#26579;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CDD&#65292;&#21363;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;CDD&#12290;CDD&#20165;&#38656;&#35201;&#37319;&#26679;&#25991;&#26412;&#26469;&#26816;&#27979;&#25968;&#25454;&#27745;&#26579;&#65292;&#36890;&#36807;&#35782;&#21035;LLMs&#36755;&#20986;&#20998;&#24067;&#30340;&#23792;&#20540;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#35780;&#20272;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;TED&#65306;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38382;&#39064;&#26465;&#20214;&#30340;2D&#35270;&#22270;&#36873;&#25321;&#36807;&#31243;&#21644;&#21452;&#20998;&#25903;Transformer&#32467;&#26500;&#65292;&#23558;2D&#30693;&#35782;&#25972;&#21512;&#21040;3D-VQA&#31995;&#32479;&#20013;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;3D&#35270;&#35273;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15933</link><description>&lt;p&gt;
&#36328;&#36234;2D&#21644;3D&#35270;&#35273;&#38382;&#31572;&#20043;&#38388;&#30340;&#40511;&#27807;&#65306;&#19968;&#31181;&#29992;&#20110;3D VQA&#30340;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15933
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38382;&#39064;&#26465;&#20214;&#30340;2D&#35270;&#22270;&#36873;&#25321;&#36807;&#31243;&#21644;&#21452;&#20998;&#25903;Transformer&#32467;&#26500;&#65292;&#23558;2D&#30693;&#35782;&#25972;&#21512;&#21040;3D-VQA&#31995;&#32479;&#20013;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;3D&#35270;&#35273;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;3D&#35270;&#35273;&#38382;&#31572;&#65288;3D VQA&#65289;&#20013;&#65292;&#20805;&#20998;&#27880;&#37322;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#26377;&#38480;&#30340;&#35270;&#35273;&#20869;&#23481;&#22810;&#26679;&#24615;&#38459;&#30861;&#20102;&#23545;&#26032;&#39062;&#22330;&#26223;&#21644;3D&#27010;&#24565;&#30340;&#27867;&#21270;&#65288;&#22914;ScanQA&#21644;SQA&#25968;&#25454;&#38598;&#20165;&#21033;&#29992;&#20102;&#32422;800&#20010;&#22330;&#26223;&#65289;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#34917;&#20805;2D&#20449;&#24687;&#26469;&#36741;&#21161;3D&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#65306;&#23427;&#20204;&#35201;&#20040;&#20351;&#29992;&#24341;&#20837;&#36807;&#20110;&#22797;&#26434;&#19988;&#26377;&#26102;&#19982;&#38382;&#39064;&#26080;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#30340;&#33258;&#19978;&#32780;&#19979;&#30340;2D&#35270;&#22270;&#65292;&#35201;&#20040;&#20381;&#38752;&#26469;&#33258;2D VLM&#30340;&#20840;&#23616;&#32858;&#21512;&#22330;&#26223;/&#22270;&#20687;&#32423;&#34920;&#31034;&#65292;&#20174;&#32780;&#20002;&#22833;&#20102;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#35821;&#35328;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;2D&#35270;&#22270;&#36873;&#25321;&#36807;&#31243;&#65292;&#20934;&#30830;&#22320;&#25351;&#20986;&#20102;&#20851;&#38190;&#35270;&#35273;&#32447;&#32034;&#30340;&#35821;&#20041;&#30456;&#20851;2D&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21452;&#20998;&#25903;Transformer&#32467;&#26500;&#23558;&#36825;&#31181;2D&#30693;&#35782;&#25972;&#21512;&#21040;3D-VQA&#31995;&#32479;&#20013;&#12290;&#36825;&#31181;&#32467;&#26500;&#37319;&#29992;&#20102;&#21452;Transformer&#35774;&#35745;&#65292;&#32039;&#20945;&#22320;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15933v1 Announce Type: cross  Abstract: In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combine
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#30340;&#25991;&#26412;&#21435;&#22122;&#25216;&#26415;&#65292;&#26412;&#25991;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#28857;&#65292;&#21457;&#29616;&#20102;&#23545;&#20110;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#24378;&#35843;&#20102;&#22914;&#20309;&#36890;&#36807;&#24494;&#23567;&#25913;&#21464;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#26368;&#32456;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15931</link><description>&lt;p&gt;
&#22522;&#20110;&#31616;&#21333;&#25552;&#31034;&#30340;&#25991;&#26412;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Simple Prompting-based Text Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15931
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#25991;&#26412;&#21435;&#22122;&#25216;&#26415;&#65292;&#26412;&#25991;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#28857;&#65292;&#21457;&#29616;&#20102;&#23545;&#20110;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#24378;&#35843;&#20102;&#22914;&#20309;&#36890;&#36807;&#24494;&#23567;&#25913;&#21464;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#26368;&#32456;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#65288;AES&#65289;&#20219;&#21153;&#30340;&#35270;&#35282;&#65292;&#25361;&#25112;&#20102;&#23558;ASAP&#25968;&#25454;&#38598;&#35270;&#20026;&#38745;&#24577;&#23454;&#20307;&#30340;&#20256;&#32479;&#35266;&#28857;&#12290;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#36827;&#34892;&#31616;&#21333;&#25991;&#26412;&#21435;&#22122;&#25216;&#26415;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#25968;&#25454;&#38598;&#20869;&#30340;&#21160;&#24577;&#28508;&#21147;&#12290;&#23613;&#31649;&#25105;&#20204;&#25215;&#35748;&#20197;&#21069;&#37325;&#28857;&#25918;&#22312;&#26500;&#24314;&#22238;&#24402;&#31995;&#32479;&#19978;&#65292;&#20294;&#25105;&#20204;&#30340;&#35770;&#25991;&#24378;&#35843;&#36890;&#36807;&#25991;&#26412;&#21435;&#22122;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#23567;&#25913;&#21464;&#22914;&#20309;&#21487;&#20197;&#25552;&#39640;&#26368;&#32456;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15931v1 Announce Type: new  Abstract: This paper introduces a novel perspective on the automated essay scoring (AES) task, challenging the conventional view of the ASAP dataset as a static entity. Employing simple text denoising techniques using prompting, we explore the dynamic potential within the dataset. While acknowledging the previous emphasis on building regression systems, our paper underscores how making minor changes to a dataset through text denoising can enhance the final results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;LLM&#30340;&#34920;&#29616;&#21644;&#31532;&#20108;&#35821;&#35328;&#33021;&#21147;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#38024;&#23545;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#20064;&#32773;&#30340;&#19981;&#21516;&#33021;&#21147;&#27700;&#24179;&#65292;&#35780;&#20272;&#20102;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#21457;&#29616;&#36807;&#24230;&#32416;&#27491;&#20027;&#35201;&#21457;&#29983;&#22312;&#39640;&#32423;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#20889;&#20316;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.15930</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#33021;&#21147;&#35780;&#20272;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#30340;&#25552;&#31034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;LLM&#30340;&#34920;&#29616;&#21644;&#31532;&#20108;&#35821;&#35328;&#33021;&#21147;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#38024;&#23545;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#20064;&#32773;&#30340;&#19981;&#21516;&#33021;&#21147;&#27700;&#24179;&#65292;&#35780;&#20272;&#20102;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#21457;&#29616;&#36807;&#24230;&#32416;&#27491;&#20027;&#35201;&#21457;&#29983;&#22312;&#39640;&#32423;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#20889;&#20316;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#23398;&#20064;&#32773;&#30340;&#20889;&#20316;&#33539;&#20363;&#21487;&#33021;&#19982;&#27597;&#35821;&#32773;&#19981;&#21516;&#12290;&#37492;&#20110;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#26681;&#25454;&#20854;&#33021;&#21147;&#27700;&#24179;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#30740;&#31350;LLM&#30340;&#34920;&#29616;&#21644;&#31532;&#20108;&#35821;&#35328;&#33021;&#21147;&#20043;&#38388;&#30340;&#20114;&#21160;&#26469;&#20943;&#23569;&#36807;&#24230;&#32416;&#27491;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#38024;&#23545;&#19981;&#21516;&#33021;&#21147;&#27700;&#24179;&#30340;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#20064;&#32773;&#36827;&#34892;&#38646;&#28857;&#21644;&#23569;&#28857;&#25552;&#31034;&#20197;&#21450;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#30340;&#32467;&#26524;&#65292;&#21457;&#29616;&#36807;&#24230;&#32416;&#27491;&#20027;&#35201;&#21457;&#29983;&#22312;&#39640;&#32423;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#20889;&#20316;&#65288;&#33021;&#21147;C&#65289;&#20013;&#65292;&#32780;&#24182;&#38750;&#22312;&#33021;&#21147;A&#65288;&#21021;&#23398;&#32773;&#27700;&#24179;&#65289;&#21644;&#33021;&#21147;B&#65288;&#20013;&#32423;&#27700;&#24179;&#65289;&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;LLM&#29978;&#33267;&#22312;&#23569;&#25968;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#20889;&#20316;&#31034;&#20363;&#20013;&#30340;&#33521;&#35821;&#23398;&#20064;&#32773;&#23454;&#38469;&#19978;&#20542;&#21521;&#20110;&#20943;&#23569;&#21484;&#22238;&#24230;&#37327;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#35770;&#28857;&#20855;&#20307;&#21270;&#65292;&#25105;&#20204;&#23545;GEC&#32467;&#26524;&#36827;&#34892;&#20102;&#20840;&#38754;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15930v1 Announce Type: new  Abstract: The writing examples of English language learners may be different from those of native speakers. Given that there is a significant differences in second language (L2) learners' error types by their proficiency levels, this paper attempts to reduce overcorrection by examining the interaction between LLM's performance and L2 language proficiency. Our method focuses on zero-shot and few-shot prompting and fine-tuning models for GEC for learners of English as a foreign language based on the different proficiency. We investigate GEC results and find that overcorrection happens primarily in advanced language learners' writing (proficiency C) rather than proficiency A (a beginner level) and proficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot prompting with writing examples of English learners, actually tend to exhibit decreased recall measures. To make our claim concrete, we conduct a comprehensive examination of GEC outc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#29992;&#20110;&#27491;&#24335;&#35748;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#35777;&#20070;&#23450;&#37327;&#21270;&#19988;&#21253;&#21547;&#39640;&#32622;&#20449;&#24230;&#30340;&#27010;&#29575;&#30028;&#38480;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#25552;&#39640;&#65292;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15929</link><description>&lt;p&gt;
QuaCer-C&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#23450;&#37327;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#29992;&#20110;&#27491;&#24335;&#35748;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#35777;&#20070;&#23450;&#37327;&#21270;&#19988;&#21253;&#21547;&#39640;&#32622;&#20449;&#24230;&#30340;&#27010;&#29575;&#30028;&#38480;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#25552;&#39640;&#65292;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30740;&#31350;&#24182;&#26410;&#23545;LLMs&#30340;&#34920;&#29616;&#25552;&#20379;&#27491;&#24335;&#30340;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#25105;&#20204;&#22312;&#27492;&#23545;&#30693;&#21517;LLMs&#30340;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#27491;&#24335;&#35748;&#35777;&#12290;&#25105;&#20204;&#30340;&#35777;&#20070;&#26159;&#23450;&#37327;&#30340; - &#23427;&#20204;&#21253;&#25324;&#23545;&#30446;&#26631;LLM&#22312;&#20219;&#20309;&#30456;&#20851;&#30693;&#35782;&#29702;&#35299;&#25552;&#31034;&#19978;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#27010;&#29575;&#30340;&#39640;&#32622;&#20449;&#24230;&#32039;&#23494;&#30028;&#38480;&#12290;&#25105;&#20204;&#38024;&#23545;Llama&#12289;Vicuna&#21644;Mistral LLMs&#30340;&#35777;&#20070;&#34920;&#26126;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#38543;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#24182;&#19988;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15929v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive performance on several benchmarks. However, traditional studies do not provide formal guarantees on the performance of LLMs. In this work, we propose a novel certification framework for LLM, QuaCer-C, wherein we formally certify the knowledge-comprehension capabilities of popular LLMs. Our certificates are quantitative - they consist of high-confidence, tight bounds on the probability that the target LLM gives the correct answer on any relevant knowledge comprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs indicate that the knowledge comprehension capability improves with an increase in the number of parameters and that the Mistral model is less performant than the rest in this evaluation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#20449;&#24687;&#25429;&#33719;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.15925</link><description>&lt;p&gt;
MultiContrievers: &#31264;&#23494;&#26816;&#32034;&#34920;&#31034;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
MultiContrievers: Analysis of Dense Retrieval Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#20449;&#24687;&#25429;&#33719;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#26816;&#32034;&#22120;&#23558;&#28304;&#25991;&#26723;&#21387;&#32553;&#20026;&#65288;&#21487;&#33021;&#26159;&#26377;&#25439;&#30340;&#65289;&#21521;&#37327;&#34920;&#31034;&#65292;&#28982;&#32780;&#30446;&#21069;&#23545;&#20110;&#22833;&#21435;&#21644;&#20445;&#30041;&#30340;&#20449;&#24687;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#26512;&#36739;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#23545;&#27604;&#31264;&#23494;&#26816;&#32034;&#22120;&#25429;&#33719;&#30340;&#20449;&#24687;&#19982;&#23427;&#20204;&#22522;&#20110;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#19982;Contriever&#65289;&#20043;&#38388;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;25&#20010;MultiBert&#26816;&#26597;&#28857;&#20316;&#20026;&#38543;&#26426;&#21021;&#22987;&#21270;&#26469;&#35757;&#32451;MultiContrievers&#65292;&#36825;&#26159;&#19968;&#32452;25&#20010;contriever&#27169;&#22411;&#12290;&#25105;&#20204;&#27979;&#35797;&#29305;&#23450;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#21644;&#32844;&#19994;&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#26723;&#30340;contriever&#21521;&#37327;&#20013;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#35770;&#25506;&#27979;&#26469;&#34913;&#37327;&#36825;&#31181;&#21487;&#25552;&#21462;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#36825;&#20123;&#32467;&#26524;&#23545;&#35768;&#22810;&#38543;&#26426;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#27927;&#29260;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;contriever&#27169;&#22411;&#26377;&#26174;&#33879;&#22686;&#21152;&#30340;&#21487;&#25552;&#21462;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15925v1 Announce Type: cross  Abstract: Dense retrievers compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks. We conduct the first analysis of the information captured by dense retrievers compared to the language models they are based on (e.g., BERT versus Contriever). We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models. We test whether specific pieces of information -- such as gender and occupation -- can be extracted from contriever vectors of wikipedia-like documents. We measure this extractability via information theoretic probing. We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles. We find that (1) contriever models have significantly increased extracta
&lt;/p&gt;</description></item><item><title>PRP&#25915;&#20987;&#31574;&#30053;&#25104;&#21151;&#22320;&#38024;&#23545;&#22810;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;&#23432;&#25252;&#27169;&#22411;&#23454;&#26045;&#20102;&#20004;&#27493;&#21069;&#32512;&#25915;&#20987;&#65292;&#26377;&#25928;&#36328;&#36234;&#22810;&#20010;&#23041;&#32961;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15911</link><description>&lt;p&gt;
PRP&#65306;&#20256;&#25773;&#36890;&#29992;&#25200;&#21160;&#20197;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23432;&#25252;&#26639;
&lt;/p&gt;
&lt;p&gt;
PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15911
&lt;/p&gt;
&lt;p&gt;
PRP&#25915;&#20987;&#31574;&#30053;&#25104;&#21151;&#22320;&#38024;&#23545;&#22810;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;&#23432;&#25252;&#27169;&#22411;&#23454;&#26045;&#20102;&#20004;&#27493;&#21069;&#32512;&#25915;&#20987;&#65292;&#26377;&#25928;&#36328;&#36234;&#22810;&#20010;&#23041;&#32961;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#34987;&#35774;&#35745;&#20026;&#23545;&#20154;&#31867;&#26080;&#23475;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#33258;&#21160;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#35825;&#20351;&#23427;&#20204;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#26368;&#36817;&#30340;LLMs&#36890;&#24120;&#21253;&#21547;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#38450;&#24481;&#23618;&#65292;&#21363;&#23432;&#25252;&#27169;&#22411;&#65292;&#36825;&#26159;&#31532;&#20108;&#20010;LLM&#65292;&#26088;&#22312;&#26816;&#26597;&#21644;&#35843;&#33410;&#20027;&#35201;LLM&#30340;&#36755;&#20986;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#31574;&#30053;PRP&#65292;&#35813;&#31574;&#30053;&#25104;&#21151;&#22320;&#38024;&#23545;&#20960;&#31181;&#24320;&#28304;&#65288;&#22914;Llama 2&#65289;&#21644;&#38381;&#28304;&#65288;&#22914;GPT 3.5&#65289;&#23432;&#25252;&#27169;&#22411;&#23454;&#26045;&#12290;PRP&#21033;&#29992;&#20102;&#19968;&#31181;&#20004;&#27493;&#22522;&#20110;&#21069;&#32512;&#30340;&#25915;&#20987;&#65292;&#36890;&#36807;&#65288;a&#65289;&#26500;&#24314;&#23432;&#25252;&#27169;&#22411;&#30340;&#36890;&#29992;&#23545;&#25239;&#21069;&#32512;&#65292;&#24182;&#65288;b&#65289;&#23558;&#27492;&#21069;&#32512;&#20256;&#25773;&#21040;&#21709;&#24212;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#19968;&#36807;&#31243;&#22312;&#22810;&#31181;&#23041;&#32961;&#27169;&#22411;&#20013;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#21253;&#25324;&#23545;&#25163;&#26681;&#26412;&#26080;&#27861;&#35775;&#38382;&#23432;&#25252;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26263;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15911v1 Announce Type: cross  Abstract: Large language models (LLMs) are typically aligned to be harmless to humans. Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content. More recent LLMs often incorporate an additional layer of defense, a Guard Model, which is a second LLM that is designed to check and moderate the output response of the primary LLM. Our key contribution is to show a novel attack strategy, PRP, that is successful against several open-source (e.g., Llama 2) and closed-source (e.g., GPT 3.5) implementations of Guard Models. PRP leverages a two step prefix-based attack that operates by (a) constructing a universal adversarial prefix for the Guard Model, and (b) propagating this prefix to the response. We find that this procedure is effective across multiple threat models, including ones in which the adversary has no access to the Guard Model at all. Our work suggests t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SemEval-2024&#20219;&#21153;8&#30340;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20013;&#37319;&#29992;&#30340;&#21152;&#26435;&#23618;&#24179;&#22343;RoBERTa&#25216;&#26415;&#21450;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.15873</link><description>&lt;p&gt;
SemEval-2024&#20219;&#21153;8&#65306;&#21152;&#26435;&#23618;&#24179;&#22343;RoBERTa&#29992;&#20110;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box Machine-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SemEval-2024&#20219;&#21153;8&#30340;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20013;&#37319;&#29992;&#30340;&#21152;&#26435;&#23618;&#24179;&#22343;RoBERTa&#25216;&#26415;&#21450;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25991;&#20214;&#21253;&#21547;&#20102;&#20316;&#32773;&#25552;&#20132;&#32473;SemEval 2024&#20219;&#21153;8&#20250;&#35758;&#35770;&#25991;&#30340;&#32454;&#33410;&#65306;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#23376;&#20219;&#21153;A&#65288;&#21333;&#35821;&#65289;&#21644;B&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#20221;&#25991;&#20214;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#29992;&#20110;&#25191;&#34892;&#30456;&#21516;&#20219;&#21153;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#25152;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15873v1 Announce Type: new  Abstract: This document contains the details of the authors' submission to the proceedings of SemEval 2024's Task 8: Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection Subtask A (monolingual) and B. Detection of machine-generated text is becoming an increasingly important task, with the advent of large language models (LLMs). In this document, we lay out the techniques utilized for performing the same, along with the results obtained.
&lt;/p&gt;</description></item><item><title>SportQA&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20307;&#32946;&#29702;&#35299;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21253;&#21547;&#36229;&#36807;70,000&#20010;&#38382;&#39064;&#28085;&#30422;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#20307;&#32946;&#30693;&#35782;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#22312;&#22522;&#26412;&#20307;&#32946;&#30693;&#35782;&#19978;&#34920;&#29616;&#20248;&#24322;&#20294;&#22312;&#22797;&#26434;&#24773;&#22659;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15862</link><description>&lt;p&gt;
SportQA&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20307;&#32946;&#29702;&#35299;&#30340;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SportQA: A Benchmark for Sports Understanding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15862
&lt;/p&gt;
&lt;p&gt;
SportQA&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20307;&#32946;&#29702;&#35299;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21253;&#21547;&#36229;&#36807;70,000&#20010;&#38382;&#39064;&#28085;&#30422;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#20307;&#32946;&#30693;&#35782;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#22312;&#22522;&#26412;&#20307;&#32946;&#30693;&#35782;&#19978;&#34920;&#29616;&#20248;&#24322;&#20294;&#22312;&#22797;&#26434;&#24773;&#22659;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15862v1 &#25253;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#23545;&#20307;&#32946;&#39046;&#22495;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#65292;&#36825;&#26159;&#19968;&#39033;&#20805;&#28385;&#25112;&#30053;&#21644;&#21160;&#24577;&#20869;&#23481;&#30340;&#39046;&#22495;&#65292;&#23545;&#20110;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#22312;&#35780;&#20272;&#21644;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32972;&#26223;&#19979;&#23588;&#20026;&#37325;&#35201;&#65292;&#37492;&#20110;&#29616;&#26377;&#19987;&#38376;&#22522;&#20934;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SportQA&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20307;&#32946;&#29702;&#35299;&#26041;&#38754;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;SportQA&#28085;&#30422;&#20102;&#36229;&#36807;70,000&#20010;&#36328;&#19977;&#20010;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#27599;&#20010;&#32423;&#21035;&#38024;&#23545;&#20307;&#32946;&#30693;&#35782;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#20174;&#22522;&#26412;&#21382;&#21490;&#20107;&#23454;&#21040;&#22797;&#26434;&#30340;&#22522;&#20110;&#24773;&#26223;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#20027;&#35201;&#21033;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#36741;&#20197;&#8220;&#32852;&#24819;&#38142;&#8221;&#25552;&#31034;&#26041;&#27861;&#23545;&#26222;&#36941;&#30340;LLMs&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;LLMs&#22312;&#22522;&#26412;&#30340;&#20307;&#32946;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#24773;&#26223;&#30340;&#25512;&#29702;&#26041;&#38754;&#21364;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15862v1 Announce Type: new  Abstract: A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-bas
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;MATHWELL&#27169;&#22411;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#65292;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#32467;&#26524;&#26174;&#31034;&#65292;MATHWELL&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#24182;&#31526;&#21512;&#25152;&#26377;&#26631;&#20934;&#30340;&#20221;&#39069;&#27604;&#20854;&#20182;&#36873;&#25321;&#39640;&#20986;40%&#65292;&#20854;&#20013;74%&#30340;&#21487;&#35299;&#20915;&#38382;&#39064;&#21516;&#26102;&#20570;&#21040;&#20102;&#20934;&#30830;&#21644;&#36866;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.15861</link><description>&lt;p&gt;
MATHWELL: &#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#25945;&#32946;&#25968;&#23398;&#24212;&#29992;&#39064;
&lt;/p&gt;
&lt;p&gt;
MATHWELL: Generating Educational Math Word Problems at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15861
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;MATHWELL&#27169;&#22411;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#65292;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#32467;&#26524;&#26174;&#31034;&#65292;MATHWELL&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#24182;&#31526;&#21512;&#25152;&#26377;&#26631;&#20934;&#30340;&#20221;&#39069;&#27604;&#20854;&#20182;&#36873;&#25321;&#39640;&#20986;40%&#65292;&#20854;&#20013;74%&#30340;&#21487;&#35299;&#20915;&#38382;&#39064;&#21516;&#26102;&#20570;&#21040;&#20102;&#20934;&#30830;&#21644;&#36866;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#24212;&#29992;&#39064;&#22312;K-8&#25945;&#32946;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#32534;&#20889;&#23427;&#20204;&#32791;&#26102;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#35268;&#27169;&#21270;&#38382;&#39064;&#26469;&#25903;&#25345;K-8&#25968;&#23398;&#25945;&#32946;&#12290;&#20026;&#20102;&#25945;&#32946;&#24615;&#65292;&#29983;&#25104;&#30340;&#38382;&#39064;&#24517;&#39035;&#26159;1&#65289;&#21487;&#35299;&#20915;&#30340;&#65292;2&#65289;&#20934;&#30830;&#30340;&#65292;3&#65289;&#36866;&#24403;&#30340;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#26410;&#26631;&#35760;&#36825;&#20123;&#26631;&#20934;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#35757;&#32451;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MATHWELL&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#24494;&#35843;&#30340;70B Llama-2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;K-8&#25968;&#23398;&#24212;&#29992;&#39064;&#12290;&#20511;&#21161;MATHWELL&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#33521;&#25991;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20,490&#20010;&#38382;&#39064;&#12290;&#32463;&#39046;&#22495;&#19987;&#23478;&#35780;&#20998;&#30340;3,484&#20010;&#38382;&#39064;&#21457;&#29616;&#65292;MATHWELL&#25317;&#26377;&#27604;&#20854;&#20182;&#36873;&#25321;&#26356;&#39640;&#30340;&#21487;&#25191;&#34892;&#35299;&#20915;&#26041;&#26696;&#21644;&#28385;&#36275;&#25152;&#26377;&#26631;&#20934;&#30340;&#38382;&#39064;&#20221;&#39069;&#39640;&#20986;40&#65285;&#65292;&#20854;&#20013;74&#65285;&#30340;&#38382;&#39064;&#20855;&#26377;&#21487;&#35299;&#30340;&#12289;&#20934;&#30830;&#30340;&#21644;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15861v1 Announce Type: new  Abstract: Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise. We suggest that language models can support K-8 math education by automatically generating problems at scale. To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate. Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators. We introduce MATHWELL, a Llama-2 (70B) model iteratively finetuned to generate K-8 math word problems using data from expert annotation. Using MATHWELL, we generate the largest English word problem dataset to date, containing 20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate.
&lt;/p&gt;</description></item><item><title>&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19982;&#21028;&#21035;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#26512;&#21644;&#35299;&#20915;LLMs&#23545;&#36755;&#20837;&#25552;&#31034;&#20013;&#19981;&#21516;&#31867;&#22411;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;</title><link>https://arxiv.org/abs/2402.15833</link><description>&lt;p&gt;
&#38024;&#23545;&#40065;&#26834;&#24615;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#25200;&#21160;&#19968;&#33268;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt Perturbation Consistency Learning for Robust Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15833
&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19982;&#21028;&#21035;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#26512;&#21644;&#35299;&#20915;LLMs&#23545;&#36755;&#20837;&#25552;&#31034;&#20013;&#19981;&#21516;&#31867;&#22411;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#24635;&#32467;&#31561;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#24847;&#22270;&#20998;&#31867;&#21644;&#27133;&#22635;&#20805;&#65288;IC-SF&#65289;&#31561;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#19978;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26174;&#33879;&#33853;&#21518;&#20110;&#21028;&#21035;&#27169;&#22411;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#36275;&#22815;&#22823;&#30340;LLMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#20135;&#29983;&#19982;&#21028;&#21035;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;IC-SF&#24615;&#33021;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#31995;&#32479;&#20998;&#26512;&#20102;&#36825;&#20123;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30001;&#20110;&#19977;&#31181;&#19981;&#21516;&#32780;&#30456;&#20851;&#30340;&#36755;&#20837;&#25200;&#21160; - &#21516;&#38899;&#35789;&#12289;&#21516;&#20041;&#35789;&#21644;&#37322;&#20041; - &#23548;&#33268;&#30340;&#24615;&#33021;&#24694;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32531;&#35299;&#26041;&#27861;&#65292;&#21363;&#25552;&#31034;&#25200;&#21160;&#19968;&#33268;&#24615;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15833v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated impressive performance on a number of natural language processing tasks, such as question answering and text summarization. However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of LLMs to various perturbations in the input prompts. The contributions of this paper are three-fold. First, we show that fine-tuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models. Next, we systematically analyze the performance deterioration of those fine-tuned models due to three distinct yet relevant types of input perturbations - oronyms, synonyms, and paraphrasing. Finally, we propose an efficient mitigation approach, Prompt Perturbatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#39046;&#22495;&#30340;&#35821;&#35328;&#26234;&#33021;&#30693;&#35782;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#36890;&#36807;&#23545;&#22235;&#20010;&#33879;&#21517;&#27169;&#22411;&#30340;&#38646;-shot&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.15818</link><description>&lt;p&gt;
&#30005;&#20449;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Linguistic Intelligence in Large Language Models for Telecommunications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#20449;&#39046;&#22495;&#30340;&#35821;&#35328;&#26234;&#33021;&#30693;&#35782;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#36890;&#36807;&#23545;&#22235;&#20010;&#33879;&#21517;&#27169;&#22411;&#30340;&#38646;-shot&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#34920;&#29616;&#20986;&#22312;&#35821;&#35328;&#29983;&#25104;&#21644;&#20854;&#20182;&#19982;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#30005;&#20449;&#39046;&#22495;&#20869;&#30340;&#30693;&#35782;&#21644;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#33879;&#21517;&#30340;LLMs-Llama-2&#12289;Falcon&#12289;Mistral&#21644;Zephyr&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#38646;-shot&#35780;&#20272;&#12290;&#36825;&#20123;&#27169;&#22411;&#27604;ChatGPT&#38656;&#35201;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#12290;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15818v1 Announce Type: new  Abstract: Large Language Models (LLMs) have emerged as a significant advancement in the field of Natural Language Processing (NLP), demonstrating remarkable capabilities in language generation and other language-centric tasks. Despite their evaluation across a multitude of analytical and reasoning tasks in various scientific domains, a comprehensive exploration of their knowledge and understanding within the realm of natural language tasks in the telecommunications domain is still needed. This study, therefore, seeks to evaluate the knowledge and understanding capabilities of LLMs within this domain. To achieve this, we conduct an exhaustive zero-shot evaluation of four prominent LLMs-Llama-2, Falcon, Mistral, and Zephyr. These models require fewer resources than ChatGPT, making them suitable for resource-constrained environments. Their performance is compared with state-of-the-art, fine-tuned models. To the best of our knowledge, this is the firs
&lt;/p&gt;</description></item><item><title>RNN&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#34920;&#31034;&#26356;&#22823;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#26377;&#30028;&#22534;&#26632;&#21644;&#24191;&#20041;&#22534;&#26632;&#26356;&#26032;&#20989;&#25968;&#65292;&#31867;&#20284;&#20110;&#20445;&#30041;&#22266;&#23450;&#25968;&#37327;&#31526;&#21495;&#35760;&#24518;&#24182;&#20351;&#29992;&#31616;&#21333;&#26356;&#26032;&#26426;&#21046;&#30340;&#33258;&#21160;&#26426;&#12290;</title><link>https://arxiv.org/abs/2402.15814</link><description>&lt;p&gt;
RNN&#35821;&#35328;&#27169;&#22411;&#24402;&#32435;&#20559;&#24046;&#30340;&#19968;&#20010;&#29702;&#35770;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Result on the Inductive Bias of RNN Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15814
&lt;/p&gt;
&lt;p&gt;
RNN&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#34920;&#31034;&#26356;&#22823;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#26377;&#30028;&#22534;&#26632;&#21644;&#24191;&#20041;&#22534;&#26632;&#26356;&#26032;&#20989;&#25968;&#65292;&#31867;&#20284;&#20110;&#20445;&#30041;&#22266;&#23450;&#25968;&#37327;&#31526;&#21495;&#35760;&#24518;&#24182;&#20351;&#29992;&#31616;&#21333;&#26356;&#26032;&#26426;&#21046;&#30340;&#33258;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;Hewitt&#31561;&#20154;&#65288;2020&#65289;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#20316;&#20026;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#32463;&#39564;&#25104;&#21151;&#21487;&#33021;&#24615;&#30340;&#19968;&#20010;&#35299;&#37322;&#12290; &#23427;&#26174;&#31034;RNNs&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#22312;&#20154;&#31867;&#35821;&#35328;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#26377;&#30028;&#20998;&#23618;&#32467;&#26500;&#12290; &#36825;&#34920;&#26126;RNNs&#30340;&#25104;&#21151;&#21487;&#33021;&#19982;&#23427;&#20204;&#24314;&#27169;&#23618;&#27425;&#32467;&#26500;&#30340;&#33021;&#21147;&#26377;&#20851;&#12290; &#28982;&#32780;&#65292;&#23545;Hewitt&#31561;&#20154;&#65288;2020&#65289;&#26500;&#36896;&#30340;&#26356;&#35814;&#32454;&#26816;&#26597;&#34920;&#26126;&#65292;&#23427;&#19981;&#38480;&#20110;&#20998;&#23618;LMs&#65292;&#36825;&#24341;&#20986;&#20102;RNNs&#21487;&#20197;&#26377;&#25928;&#34920;&#31034;&#21738;&#20123;\emph{&#20854;&#20182;&#31867;&#22411;} LMs&#30340;&#38382;&#39064;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#27010;&#25324;&#20182;&#20204;&#30340;&#26500;&#36896;&#20197;&#23637;&#31034;RNNs&#21487;&#20197;&#26377;&#25928;&#34920;&#31034;&#26356;&#22823;&#31867;&#21035;&#30340;LMs&#65306;&#21487;&#20197;&#36890;&#36807;&#24102;&#26377;&#26377;&#30028;&#22534;&#26632;&#21644;&#24191;&#20041;&#22534;&#26632;&#26356;&#26032;&#20989;&#25968;&#30340;&#19979;&#25512;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#37027;&#20123;&#12290; &#36825;&#31867;&#20284;&#20110;&#19968;&#20010;&#20445;&#30041;&#22266;&#23450;&#25968;&#37327;&#31526;&#21495;&#35760;&#24518;&#24182;&#20351;&#29992;&#31616;&#21333;&#26356;&#26032;&#26426;&#21046;&#26356;&#26032;&#35760;&#24518;&#30340;&#33258;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15814v1 Announce Type: new  Abstract: Recent work by Hewitt et al. (2020) provides a possible interpretation of the empirical success of recurrent neural networks (RNNs) as language models (LMs).   It shows that RNNs can efficiently represent bounded hierarchical structures that are prevalent in human language.   This suggests that RNNs' success might be linked to their ability to model hierarchy.   However, a closer inspection of Hewitt et al.'s (2020) construction shows that it is not limited to hierarchical LMs, posing the question of what \emph{other classes} of LMs can be efficiently represented by RNNs.   To this end, we generalize their construction to show that RNNs can efficiently represent a larger class of LMs: Those that can be represented by a pushdown automaton with a bounded stack and a generalized stack update function.   This is analogous to an automaton that keeps a memory of a fixed number of symbols and updates the memory with a simple update mechanism.  
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#23558;&#35848;&#21028;&#20219;&#21153;&#24418;&#24335;&#21270;&#25551;&#36848;&#20026;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#19981;&#23545;&#31216;&#28216;&#25103;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25198;&#28436;&#20080;&#26041;&#30340;&#38590;&#24230;&#22823;&#19988;&#27169;&#22411;&#22823;&#23567;&#19981;&#33021;&#26377;&#25928;&#25552;&#39640;&#20080;&#26041;&#34920;&#29616;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OG-Narrator&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15813</link><description>&lt;p&gt;
&#35780;&#20272;LLMs&#30340;&#35848;&#21028;&#33021;&#21147;&#65306;&#19968;&#20010;&#22522;&#20934;&#21644;&#19968;&#20010;&#20080;&#26041;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15813
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#23558;&#35848;&#21028;&#20219;&#21153;&#24418;&#24335;&#21270;&#25551;&#36848;&#20026;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#19981;&#23545;&#31216;&#28216;&#25103;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#34920;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25198;&#28436;&#20080;&#26041;&#30340;&#38590;&#24230;&#22823;&#19988;&#27169;&#22411;&#22823;&#23567;&#19981;&#33021;&#26377;&#25928;&#25552;&#39640;&#20080;&#26041;&#34920;&#29616;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OG-Narrator&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35848;&#21028;&#26159;&#20154;&#31867;&#20043;&#38388;&#35848;&#21028;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#29420;&#29305;&#30340;&#37096;&#20998;&#12290;&#38543;&#30528;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#23398;&#20064;&#35848;&#21028;&#24182;&#34920;&#29616;&#24471;&#20687;&#30495;&#27491;&#30340;&#20154;&#31867;&#19968;&#26679;&#65292;&#22914;&#20309;&#35780;&#20272;&#20195;&#29702;&#30340;&#35848;&#21028;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#31532;&#19968;&#27425;&#23558;&#35848;&#21028;&#20219;&#21153;&#24418;&#24335;&#21270;&#25551;&#36848;&#20026;&#19968;&#31181;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#19981;&#23545;&#31216;&#28216;&#25103;&#65292;&#23450;&#20041;&#20102;&#20080;&#26041;&#21644;&#21334;&#26041;&#22312;&#22810;&#27425;&#35848;&#21028;&#36807;&#31243;&#20013;&#30340;&#25910;&#30410;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#19968;&#20010;&#20195;&#29702;&#22312;&#35848;&#21028;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#30495;&#23454;&#20135;&#21697;&#20215;&#26684;&#25968;&#25454;&#38598;AmazonHistoryPrice&#65292;&#24182;&#23545;&#21508;&#31181;LLM&#20195;&#29702;&#30340;&#35848;&#21028;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#25198;&#28436;&#20080;&#26041;&#27604;&#25198;&#28436;&#21334;&#26041;&#35201;&#22256;&#38590;&#24471;&#22810;&#65292;&#24182;&#19988;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#26080;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20080;&#26041;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;OG-Narrator&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#19968;&#20010;&#30830;&#23450;&#24615;&#30340;&#25253;&#20215;&#29983;&#25104;&#22120;&#26469;&#25511;&#21046;&#20080;&#26041;&#25253;&#20215;&#30340;&#20215;&#26684;&#33539;&#22260;&#65292;&#24182;&#19988;&#38598;&#25104;&#20102;&#19968;&#20010;LLM&#35299;&#35828;&#32773;&#26469;&#21019;&#24314;&#19968;&#31181;&#33258;&#28982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15813v1 Announce Type: new  Abstract: Bargaining is an important and unique part of negotiation between humans. As LLM-driven agents learn to negotiate and act like real humans, how to evaluate agents' bargaining abilities remains an open problem. For the first time, we formally described the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes. It allows us to quantitatively assess an agent's performance in the Bargain task. We collected a real product price dataset, AmazonHistoryPrice, and conducted evaluations of various LLM agents' bargaining abilities. We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer's performance. To address the challenge, we propose a novel approach called OG-Narrator that integrates a deterministic Offer Generator to control the price range of Buyer's offers, and an LLM Narrator to create natur
&lt;/p&gt;</description></item><item><title>OAG-Bench&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#23398;&#26415;&#22270;&#30340;&#20840;&#38754;&#12289;&#22810;&#26041;&#38754;&#21644;&#31934;&#32454;&#21270;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;</title><link>https://arxiv.org/abs/2402.15810</link><description>&lt;p&gt;
OAG-Bench&#65306;&#38754;&#21521;&#23398;&#26415;&#22270;&#25366;&#25496;&#30340;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15810
&lt;/p&gt;
&lt;p&gt;
OAG-Bench&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#23398;&#26415;&#22270;&#30340;&#20840;&#38754;&#12289;&#22810;&#26041;&#38754;&#21644;&#31934;&#32454;&#21270;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31185;&#23398;&#25991;&#29486;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#22810;&#21151;&#33021;&#30340;&#23398;&#26415;&#30693;&#35782;&#26381;&#21153;&#36234;&#26469;&#36234;&#20381;&#36182;&#20840;&#38754;&#30340;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;&#23613;&#31649;&#20844;&#24320;&#23398;&#26415;&#22270;&#12289;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#24050;&#32463;&#26377;&#20102;&#65292;&#20294;&#36825;&#20123;&#36164;&#28304;&#36890;&#24120;&#22312;&#22810;&#26041;&#38754;&#21644;&#32454;&#31890;&#24230;&#27880;&#37322;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#21463;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#31867;&#22411;&#21644;&#39046;&#22495;&#65292;&#25110;&#32773;&#32570;&#20047;&#30495;&#23454;&#23398;&#26415;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24320;&#25918;&#23398;&#26415;&#22270;&#65288;OAG&#65289;&#30340;&#20840;&#38754;&#12289;&#22810;&#26041;&#38754;&#21644;&#31934;&#32454;&#21270;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;OAG-Bench&#12290;OAG-Bench&#28085;&#30422;&#20102;10&#20010;&#20219;&#21153;&#65292;20&#20010;&#25968;&#25454;&#38598;&#65292;70+&#20010;&#22522;&#20934;&#21644;120+&#20010;&#25130;&#33267;&#30446;&#21069;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#38024;&#23545;&#26576;&#20123;&#20219;&#21153;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#27880;&#37322;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#19968;&#22871;&#25968;&#25454;&#39044;&#22788;&#29702;&#20195;&#30721;&#12289;&#31639;&#27861;&#23454;&#29616;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#20419;&#36827;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36825;&#26679;&#30340;&#20808;&#36827;&#31639;&#27861;&#20063;&#20250;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#21463;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15810v1 Announce Type: cross  Abstract: With the rapid proliferation of scientific literature, versatile academic knowledge services increasingly rely on comprehensive academic graph mining. Despite the availability of public academic graphs, benchmarks, and datasets, these resources often fall short in multi-aspect and fine-grained annotations, are constrained to specific task types and domains, or lack underlying real academic graphs. In this paper, we present OAG-Bench, a comprehensive, multi-aspect, and fine-grained human-curated benchmark based on the Open Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines, and 120+ experimental results to date. We propose new data annotation strategies for certain tasks and offer a suite of data pre-processing codes, algorithm implementations, and standardized evaluation protocols to facilitate academic graph mining. Extensive experiments reveal that even advanced algorithms like large language models (LLMs) en
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#26032;&#21160;&#20316;&#30340;&#33021;&#21147;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#23398;&#20064;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#34892;&#20026;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#31574;&#30053;&#25913;&#36827;&#21160;&#20316;&#65292;&#22686;&#24378;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15809</link><description>&lt;p&gt;
&#36890;&#36807;&#34892;&#20026;&#23398;&#20064;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Empowering Large Language Model Agents through Action Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15809
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26032;&#21160;&#20316;&#30340;&#33021;&#21147;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#23398;&#20064;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24320;&#25918;&#24335;&#34892;&#20026;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#31574;&#30053;&#25913;&#36827;&#21160;&#20316;&#65292;&#22686;&#24378;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#36817;&#26469;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#20174;&#35797;&#38169;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#36825;&#26159;&#26234;&#33021;&#34892;&#20026;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#26032;&#21160;&#20316;&#30340;&#33021;&#21147;&#23545;&#20110;LLM&#20195;&#29702;&#30340;&#23398;&#20064;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20154;&#31867;&#33258;&#28982;&#22320;&#25193;&#23637;&#20182;&#20204;&#30340;&#21160;&#20316;&#31354;&#38388;&#24182;&#36890;&#36807;&#32463;&#39564;&#23398;&#20064;&#21457;&#23637;&#25216;&#33021;&#65292;&#20294;LLM&#20195;&#29702;&#36890;&#24120;&#22312;&#22266;&#23450;&#30340;&#21160;&#20316;&#31354;&#38388;&#20869;&#25805;&#20316;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#25104;&#38271;&#28508;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#24320;&#25918;&#24335;&#34892;&#20026;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LearnAct&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#36845;&#20195;&#23398;&#20064;&#31574;&#30053;&#26469;&#21019;&#24314;&#21644;&#25913;&#36827;Python&#20989;&#25968;&#24418;&#24335;&#30340;&#21160;&#20316;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;LLM&#26681;&#25454;&#22312;&#22833;&#36133;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#35782;&#21035;&#20986;&#30340;&#38169;&#35823;&#65292;&#20462;&#35746;&#21644;&#26356;&#26032;&#24403;&#21069;&#21487;&#29992;&#30340;&#21160;&#20316;&#65292;&#20174;&#32780;&#22686;&#24378;&#21160;&#20316;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15809v1 Announce Type: new  Abstract: Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations acr
&lt;/p&gt;</description></item><item><title>PEP&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#25913;&#21892;LLMs&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#20043;&#21069;&#32454;&#21270;&#21644;&#38416;&#26126;&#38382;&#39064;&#32972;&#26223;&#65292;&#25552;&#21319;&#20840;&#23616;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#20943;&#23569;&#35299;&#26512;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.15764</link><description>&lt;p&gt;
&#22312;&#36339;&#27133;&#20043;&#21069;&#19977;&#24605;&#65306;&#38382;&#39064;&#32454;&#21270;&#25552;&#31034;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15764
&lt;/p&gt;
&lt;p&gt;
PEP&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#25913;&#21892;LLMs&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#20043;&#21069;&#32454;&#21270;&#21644;&#38416;&#26126;&#38382;&#39064;&#32972;&#26223;&#65292;&#25552;&#21319;&#20840;&#23616;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#20943;&#23569;&#35299;&#26512;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#38382;&#39064;&#32454;&#21270;&#25552;&#31034;&#65288;PEP&#65289;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#20043;&#21069;&#20998;&#35299;&#21644;&#38416;&#26126;&#38382;&#39064;&#32972;&#26223;&#65292;&#20174;&#32780;&#22686;&#24378;&#20840;&#23616;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#20943;&#23569;&#35299;&#26512;&#22256;&#38590;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PEP&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23545;&#20110;&#38382;&#39064;&#25552;&#20986;&#30340;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15764v1 Announce Type: cross  Abstract: Large language models~(LLMs) have exhibited impressive performance across NLP tasks. So far they still face challenges in complex reasoning tasks and can be sensitive to input context. Despite significant efforts have been invested in enhancing reasoning process and improving prefix-prompts robustness, the crucial role of problem context has been overlooked. In this study, we propose a new approach to improve the mathematical capacities of LLMs, named Problem Elaboration Prompting~(PEP). Specifically, PEP decomposes and elucidates the problem context before reasoning, thus enhancing the global context modeling and reducing the parsing difficulties. Experiments on datasets demonstrate promising performances on complex reasoning and indicate the beneficial impact for ill-formed problems. For instance, with the GPT-3.5 model~(\texttt{text-davinci-003}), we observed a 9.93\% improvement with greedy decoding and 8.80\% improvement with self
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Chimera&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#33609;&#31295;&#27169;&#22411;&#21644;&#20004;&#31181;&#31574;&#30053;&#65292;&#21033;&#29992;&#20808;&#21069;&#29983;&#25104;&#30340;&#20196;&#29260;&#26469;&#39044;&#27979;&#21518;&#32493;&#21333;&#35789;&#65292;&#20197;&#35299;&#20915;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15758</link><description>&lt;p&gt;
Chimera: &#34701;&#21512;&#25152;&#26377;&#20196;&#29260;&#30340;&#26080;&#25439;&#35299;&#30721;&#26041;&#27861;&#65292;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15758
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Chimera&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#33609;&#31295;&#27169;&#22411;&#21644;&#20004;&#31181;&#31574;&#30053;&#65292;&#21033;&#29992;&#20808;&#21069;&#29983;&#25104;&#30340;&#20196;&#29260;&#26469;&#39044;&#27979;&#21518;&#32493;&#21333;&#35789;&#65292;&#20197;&#35299;&#20915;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#34987;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#25152;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#21512;&#24182;&#20102;&#39069;&#22806;&#30340;&#35299;&#30721;&#22836;&#65292;&#20197;&#23454;&#29616;&#23545;&#22810;&#20010;&#21518;&#32493;&#20196;&#29260;&#30340;&#24182;&#34892;&#39044;&#27979;&#65292;&#20174;&#32780;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#30721;&#22836;&#30340;&#20934;&#30830;&#24615;&#36828;&#19981;&#21450;&#33258;&#22238;&#24402;&#35299;&#30721;&#26041;&#27861;&#12290;&#37492;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Chimera&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#25512;&#27979;&#37319;&#26679;&#35774;&#35745;&#30340;&#26032;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#33609;&#31295;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#20808;&#21069;&#29983;&#25104;&#30340;&#20196;&#29260;&#26469;&#39044;&#27979;&#21518;&#32493;&#21333;&#35789;&#12290;&#20026;&#20102;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#22312;&#36731;&#37327;&#32423;&#33609;&#31295;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#24213;&#23618;&#25429;&#33719;&#30701;&#31243;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15758v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their widespread application is hindered by the resource-intensive decoding process. To address this challenge, current approaches have incorporated additional decoding heads to enable parallel prediction of multiple subsequent tokens, thereby achieving inference acceleration. Nevertheless, the accuracy of these decoding heads falls short of the auto-regressive decoding approach.   In light of these limitations, we propose Chimera, a novel framework specifically designed for speculative sampling. Within this framework, we introduce a lightweight draft model that effectively utilizes previously generated tokens to predict subsequent words. To ensure both accuracy and efficiency, we present two strategies within the lightweight draft model. Firstly, we focus on capturing short-range dependencies at the bottom layer. Secondly, we leverage
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Few-shot Learning&#21644;SBERT Fine-tuning&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#21475;&#33108;&#20581;&#24247;&#38382;&#39064;&#30340;&#20005;&#37325;&#24615;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;94.1%&#12290;</title><link>https://arxiv.org/abs/2402.15755</link><description>&lt;p&gt;
&#36890;&#36807;Few-shot Learning&#21644;SBERT Fine-tuning&#36827;&#34892;&#29273;&#31185;&#20005;&#37325;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dental Severity Assessment through Few-shot Learning and SBERT Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15755
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Few-shot Learning&#21644;SBERT Fine-tuning&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#21475;&#33108;&#20581;&#24247;&#38382;&#39064;&#30340;&#20005;&#37325;&#24615;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;94.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29273;&#31185;&#30142;&#30149;&#20005;&#37325;&#24433;&#21709;&#30528;&#30456;&#24403;&#19968;&#37096;&#20998;&#20154;&#21475;&#65292;&#23548;&#33268;&#21508;&#31181;&#20581;&#24247;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#20010;&#20154;&#30340;&#25972;&#20307;&#24184;&#31119;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#23558;&#33258;&#21160;&#21270;&#31995;&#32479;&#25972;&#21512;&#21040;&#21475;&#33108;&#20445;&#20581;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20026;&#35299;&#20915;&#35786;&#26029;&#22256;&#38590;&#12289;&#25928;&#29575;&#20302;&#19979;&#21644;&#21475;&#33108;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#38169;&#35823;&#31561;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24403;&#21307;&#29983;&#20204;&#38590;&#20197;&#39044;&#27979;&#25110;&#35786;&#26029;&#30142;&#30149;&#30340;&#26089;&#26399;&#38454;&#27573;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#23588;&#20854;&#26377;&#29992;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#25253;&#21578;&#26469;&#30830;&#23450;&#21475;&#33108;&#20581;&#24247;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;Few-shot learning&#32467;&#21512;SBERT&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#27169;&#22411;&#65292;&#36798;&#21040;94.1%&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15755v1 Announce Type: new  Abstract: Dental diseases have a significant impact on a considerable portion of the population, leading to various health issues that can detrimentally affect individuals' overall well-being. The integration of automated systems in oral healthcare has become increasingly crucial. Machine learning approaches offer a viable solution to address challenges such as diagnostic difficulties, inefficiencies, and errors in oral disease diagnosis. These methods prove particularly useful when physicians struggle to predict or diagnose diseases at their early stages. In this study, thirteen different machine learning, deep learning, and large language models were employed to determine the severity level of oral health issues based on radiologists' reports. The results revealed that the Few-shot learning with SBERT and Multi-Layer Perceptron model outperformed all other models across various experiments, achieving an impressive accuracy of 94.1% as the best r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HD-Eval&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#23618;&#26631;&#20934;&#20998;&#35299;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#20559;&#22909;&#65292;&#20174;&#32780;&#25552;&#21319;&#35780;&#20272;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15754</link><description>&lt;p&gt;
HD-Eval: &#36890;&#36807;&#20998;&#23618;&#26631;&#20934;&#20998;&#35299;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15754
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HD-Eval&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#23618;&#26631;&#20934;&#20998;&#35299;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#20559;&#22909;&#65292;&#20174;&#32780;&#25552;&#21319;&#35780;&#20272;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#26114;&#36149;&#20154;&#24037;&#35780;&#20272;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLMs&#30340;&#35780;&#20272;&#30340;&#23545;&#40784;&#21644;&#35206;&#30422;&#36890;&#24120;&#21463;&#21040;&#35780;&#20272;&#25552;&#31034;&#21644;&#26631;&#20934;&#30340;&#33539;&#22260;&#21644;&#28508;&#22312;&#20559;&#35265;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;HD-Eval&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#23618;&#26631;&#20934;&#20998;&#35299;&#36845;&#20195;&#23545;&#40784;LLM-based&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#21916;&#22909;&#12290;HD-Eval&#32487;&#25215;&#20102;&#20154;&#31867;&#19987;&#23478;&#35780;&#20272;&#24515;&#24577;&#30340;&#31934;&#39635;&#65292;&#24182;&#36890;&#36807;&#23558;&#32473;&#23450;&#30340;&#35780;&#20272;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#31934;&#32454;&#30340;&#26631;&#20934;&#12289;&#26681;&#25454;&#20272;&#35745;&#30340;&#20154;&#31867;&#20559;&#22909;&#32858;&#21512;&#23427;&#20204;&#12289;&#36890;&#36807;&#24402;&#22240;&#20462;&#21098;&#19981;&#26174;&#33879;&#30340;&#26631;&#20934;&#20197;&#21450;&#36827;&#19968;&#27493;&#20998;&#35299;&#26174;&#33879;&#30340;&#26631;&#20934;&#26469;&#22686;&#24378;LLM&#30340;&#35780;&#20272;&#22120;&#30340;&#23545;&#40784;&#12290;&#36890;&#36807;&#22312;&#36845;&#20195;&#23545;&#40784;&#35757;&#32451;&#36807;&#31243;&#20013;&#38598;&#25104;&#36825;&#20123;&#27493;&#39588;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#20840;&#38754;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#30340;&#26631;&#20934;&#30340;&#20998;&#23618;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15754v1 Announce Type: new  Abstract: Large language models (LLMs) have emerged as a promising alternative to expensive human evaluations. However, the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria. To address this challenge, we propose HD-Eval, a novel framework that iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the evaluation mindset of human experts and enhances the alignment of LLM-based evaluators by decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria. By integrating these steps within an iterative alignment training process, we obtain a hierarchical decomposition of criteria that comprehensively captures aspects of natural lang
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;MeZO&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#30446;&#26631;</title><link>https://arxiv.org/abs/2402.15751</link><description>&lt;p&gt;
&#31232;&#30095;MeZO&#65306;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;MeZO&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#23545;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#24212;&#29992;&#38646;&#38454;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#38454;LLM&#24494;&#35843;&#20013;&#20943;&#23569;&#21442;&#25968;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#36890;&#24120;&#20250;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#20013;&#30340;&#21453;&#21521;&#20256;&#25773;&#32780;&#23548;&#33268;&#20869;&#23384;&#25928;&#29575;&#20302;&#19979;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#39640;&#25928;&#21033;&#29992;&#23384;&#20648;&#22120;&#30340;&#38646;&#38454;&#65288;MeZO&#65289;&#20248;&#21270;&#22120;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21482;&#38656;&#35201;&#21069;&#21521;&#20256;&#36882;&#65292;&#20351;&#20854;&#26356;&#31526;&#21512;&#20869;&#23384;&#21451;&#22909;&#24615;&#12290;&#28982;&#32780;&#65292;&#38646;&#38454;&#20248;&#21270;&#20013;&#26799;&#24230;&#20272;&#35745;&#30340;&#36136;&#37327;&#24448;&#24448;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#32500;&#25968;&#65292;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19982;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26631;&#20934;&#24494;&#35843;&#30456;&#27604;&#65292;MeZO&#20173;&#28982;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#21463;&#21040;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;MeZO&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#20165;&#23558;ZO&#24212;&#29992;&#20110;&#31934;&#24515;&#36873;&#25321;&#30340;&#21442;&#25968;&#23376;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#36873;&#25321;&#26041;&#26696;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15751v1 Announce Type: cross  Abstract: While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, the quality of gradient estimates in zeroth order optimization often depends on the data dimensionality, potentially explaining why MeZO still exhibits significant performance drops compared to standard fine-tuning across various tasks. Inspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Spar
&lt;/p&gt;</description></item><item><title>GAOKAO-MM &#26159;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20154;&#31867;&#27700;&#24179;&#35201;&#27714;&#65292;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#30340;LVLMs&#30340;&#20934;&#30830;&#29575;&#26222;&#36941;&#19981;&#36275;50%&#12290;</title><link>https://arxiv.org/abs/2402.15745</link><description>&lt;p&gt;
GAOKAO-MM: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#30340;&#20013;&#22269;&#20154;&#31867;&#27700;&#24179;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15745
&lt;/p&gt;
&lt;p&gt;
GAOKAO-MM &#26159;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20154;&#31867;&#27700;&#24179;&#35201;&#27714;&#65292;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#30340;LVLMs&#30340;&#20934;&#30830;&#29575;&#26222;&#36941;&#19981;&#36275;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#24050;&#32463;&#22312;&#22270;&#20687;&#24863;&#30693;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26497;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#22522;&#26412;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#36825;&#20123;&#26080;&#27861;&#20805;&#20998;&#21453;&#26144;&#20986;LVLMs&#30340;&#20840;&#38754;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GAOKAO-MM&#65292;&#19968;&#20010;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#21253;&#25324;8&#20010;&#31185;&#30446;&#21644;12&#31181;&#31867;&#22411;&#30340;&#22270;&#29255;&#65292;&#22914;&#22270;&#34920;&#12289;&#20989;&#25968;&#22270;&#12289;&#22320;&#22270;&#21644;&#29031;&#29255;&#12290;GAOKAO-MM&#26469;&#28304;&#20110;&#20013;&#22269;&#26412;&#22303;&#32972;&#26223;&#65292;&#24182;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#35201;&#27714;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#29702;&#35299;&#12289;&#30693;&#35782;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;10&#20010;LVLMs&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#20934;&#30830;&#29575;&#37117;&#20302;&#20110;50%&#65292;&#20854;&#20013;GPT-4-Vision&#65288;48.1%&#65289;&#12289;Qwen-VL-Plus&#65288;41.2%&#65289;&#21644;Gemini-Pro-Vision&#65288;35.1%&#65289;&#20301;&#21015;&#21069;&#19977;&#21517;&#12290;&#25105;&#20204;&#30340;&#22810;&#32500;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;LVLMs&#20855;&#26377;&#36866;&#24230;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15745v1 Announce Type: cross  Abstract: The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs have moder
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;ArEEG_Chars&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.15733</link><description>&lt;p&gt;
ArEEG_Chars: &#29992;&#20110;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#35774;&#24819;&#35821;&#38899;&#35782;&#21035;&#30340;&#38463;&#25289;&#20271;&#23383;&#31526;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15733
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;ArEEG_Chars&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#26159;&#36817;&#24180;&#26469;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21487;&#20197;&#24110;&#21161;&#30251;&#30186;&#24739;&#32773;&#25913;&#21892;&#29983;&#27963;&#12290;&#26377;&#20960;&#39033;&#30740;&#31350;&#33258;&#21160;&#23558;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#20998;&#31867;&#20026;&#33521;&#25991;&#23383;&#31526;&#21644;&#21333;&#35789;&#12290;&#38463;&#25289;&#20271;&#35821;&#26159;&#19990;&#30028;&#19978;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#35821;&#35328;&#20043;&#19968;&#12290;&#28982;&#32780;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#38024;&#23545;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;&#65292;&#24182;&#21629;&#21517;&#20026;ArEEG_Chars&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;ArEEG_Chars&#36827;&#34892;&#20102;&#22810;&#39033;&#23454;&#39564;&#12290;&#22312;&#20351;&#29992;LSTM&#26102;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#12290;ArEEG_Chars&#25968;&#25454;&#38598;&#23558;&#23545;&#30740;&#31350;&#20154;&#21592;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15733v1 Announce Type: cross  Abstract: Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#25968;&#20540;&#35745;&#31639;&#26102;&#32463;&#24120;&#20986;&#38169;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#38382;&#39064;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#38169;&#35823;&#65292;&#20294;&#35266;&#23519;&#21040;&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20195;&#30721;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#20250;&#29983;&#25104;&#26356;&#22810;&#19981;&#27491;&#30830;&#25512;&#29702;&#65307;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#32534;&#30721;&#23454;&#36341;&#21551;&#21457;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#26041;&#27861;Human-Think Language&#65288;HTL&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.15729</link><description>&lt;p&gt;
&#20154;&#31867;&#26159;&#22914;&#20309;&#32534;&#20889;&#20195;&#30721;&#30340;&#65311;&#22823;&#22411;&#27169;&#22411;&#20063;&#20197;&#21516;&#26679;&#30340;&#26041;&#24335;&#36827;&#34892;
&lt;/p&gt;
&lt;p&gt;
How Do Humans Write Code? Large Models Do It the Same Way Too
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15729
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#25968;&#20540;&#35745;&#31639;&#26102;&#32463;&#24120;&#20986;&#38169;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#38382;&#39064;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#38169;&#35823;&#65292;&#20294;&#35266;&#23519;&#21040;&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20195;&#30721;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#20250;&#29983;&#25104;&#26356;&#22810;&#19981;&#27491;&#30830;&#25512;&#29702;&#65307;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#32534;&#30721;&#23454;&#36341;&#21551;&#21457;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#26041;&#27861;Human-Think Language&#65288;HTL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25191;&#34892;&#25968;&#20540;&#35745;&#31639;&#26102;&#32463;&#24120;&#20986;&#38169;&#12290;&#19982;&#20256;&#32479;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#30456;&#27604;&#65292;&#31243;&#24207;&#21270;&#24605;&#32500;&#26041;&#27861;&#28041;&#21450;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#36890;&#36807;&#25191;&#34892;&#36825;&#20123;&#20195;&#30721;&#65292;&#23427;&#21487;&#20197;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#29983;&#25104;&#30340;&#21487;&#25191;&#34892;&#20195;&#30721;&#32780;&#19981;&#26159;&#33258;&#28982;&#35821;&#35328;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;LLMs&#20351;&#29992;&#20195;&#30721;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#20182;&#20204;&#24448;&#24448;&#29983;&#25104;&#27604;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26356;&#22810;&#30340;&#19981;&#27491;&#30830;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Human-Think Language&#65288;HTL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#20154;&#31867;&#32534;&#30721;&#23454;&#36341;&#21551;&#21457;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#30001;&#27169;&#22411;&#29983;&#25104;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35299;&#20915;&#38382;&#39064;&#26041;&#27861;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#20195;&#30721;&#65292;&#21453;&#26144;&#20986;&#20154;&#20204;&#22312;&#23558;&#36923;&#36753;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#24605;&#32771;&#21518;&#20877;&#23558;&#20854;&#20889;&#25104;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#23427;&#21033;&#29992;&#20102;P
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15729v1 Announce Type: new  Abstract: Large Language Models (LLMs) often make errors when performing numerical calculations. In contrast to traditional chain-of-thought reasoning, the program-of-thoughts approach involves generating executable code to solve problems. By executing this code, it achieves more precise results. Using generated executable code instead of natural language can reduce computational errors. However, we observe that when LLMs solve mathematical problems using code, they tend to generate more incorrect reasoning than when using natural language. To address this issue, we propose Human-Think Language (HTL), a straightforward yet highly efficient approach inspired by human coding practices. The approach first generates problem-solving methods described in the natural language by the model, then converts them into code, mirroring the process where people think through the logic in natural language before writing it as code. Additionally, it utilizes the P
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hal-Eval&#65292;&#19968;&#20010;&#36890;&#29992;&#21644;&#32454;&#31890;&#24230;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24187;&#35273;&#20998;&#31867;&#27861;&#65292;&#19987;&#27880;&#20110;&#20107;&#20214;&#24187;&#35273;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#36807;&#28388;&#32454;&#31890;&#24230;&#24187;&#35273;&#25968;&#25454;&#26469;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#21508;&#31181;&#24187;&#35273;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15721</link><description>&lt;p&gt;
Hal-Eval: &#19968;&#31181;&#38754;&#21521;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#21644;&#32454;&#31890;&#24230;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hal-Eval&#65292;&#19968;&#20010;&#36890;&#29992;&#21644;&#32454;&#31890;&#24230;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24187;&#35273;&#20998;&#31867;&#27861;&#65292;&#19987;&#27880;&#20110;&#20107;&#20214;&#24187;&#35273;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#36807;&#28388;&#32454;&#31890;&#24230;&#24187;&#35273;&#25968;&#25454;&#26469;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#21508;&#31181;&#24187;&#35273;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#22270;&#29255;&#21644;&#20854;&#25551;&#36848;&#20043;&#38388;&#23384;&#22312;&#24187;&#35273;&#19981;&#19968;&#33268;&#12290;&#20197;&#24448;&#23545;LVLMs&#36827;&#34892;&#30340;&#24187;&#35273;&#35780;&#20272;&#30740;&#31350;&#21457;&#29616;&#20102;&#20851;&#20110;&#23545;&#35937;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#30340;&#24187;&#35273;&#65292;&#20294;&#24573;&#30053;&#20102;&#22260;&#32469;&#34394;&#26500;&#23454;&#20307;&#21019;&#24314;&#25972;&#20010;&#21465;&#20107;&#30340;&#22797;&#26434;&#24187;&#35273;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#24187;&#35273;&#20998;&#31867;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#31867;&#21035;&#65306;&#20107;&#20214;&#24187;&#35273;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20808;&#36827;&#30340;LLMs&#29983;&#25104;&#21644;&#36807;&#28388;&#30001;&#21508;&#31181;&#31867;&#22411;&#30340;&#24187;&#35273;&#32452;&#25104;&#30340;&#32454;&#31890;&#24230;&#24187;&#35273;&#25968;&#25454;&#65292;&#29305;&#21035;&#20851;&#27880;&#20107;&#20214;&#24187;&#35273;&#65292;&#20026;&#22312;&#25105;&#20204;&#30340;&#36890;&#29992;&#35780;&#20272;&#26694;&#26550;&#20869;&#38598;&#25104;&#36776;&#21035;&#21644;&#29983;&#25104;&#35780;&#20272;&#26041;&#27861;&#22880;&#23450;&#22522;&#30784;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#21487;&#20197;&#29420;&#29305;&#22320;&#35780;&#20272;LVLMs&#22788;&#29702;&#24191;&#27867;&#24187;&#35273;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#21487;&#38752;&#21644;&#20840;&#38754;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15721v1 Announce Type: new  Abstract: Large Vision Language Models exhibit remarkable capabilities but struggle with hallucinations inconsistencies between images and their descriptions. Previous hallucination evaluation studies on LVLMs have identified hallucinations in terms of objects, attributes, and relations but overlooked complex hallucinations that create an entire narrative around a fictional entity. In this paper, we introduce a refined taxonomy of hallucinations, featuring a new category: Event Hallucination. We then utilize advanced LLMs to generate and filter fine grained hallucinatory data consisting of various types of hallucinations, with a particular focus on event hallucinations, laying the groundwork for integrating discriminative and generative evaluation methods within our universal evaluation framework. The proposed benchmark distinctively assesses LVLMs ability to tackle a broad spectrum of hallucinations, making it a reliable and comprehensive tool fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20351;&#20854;&#25104;&#20026;&#26356;&#22909;&#30340;&#36830;&#32493;&#23569;&#26679;&#26412;&#20851;&#31995;&#25552;&#21462;&#22120;</title><link>https://arxiv.org/abs/2402.15713</link><description>&lt;p&gt;
&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#23569;&#26679;&#26412;&#20851;&#31995;&#25552;&#21462;&#22120;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15713
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20351;&#20854;&#25104;&#20026;&#26356;&#22909;&#30340;&#36830;&#32493;&#23569;&#26679;&#26412;&#20851;&#31995;&#25552;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23569;&#26679;&#26412;&#20851;&#31995;&#25552;&#21462;&#65288;CFRE&#65289;&#26159;&#19968;&#20010;&#23454;&#38469;&#38382;&#39064;&#65292;&#38656;&#35201;&#27169;&#22411;&#22312;&#36991;&#20813;&#24536;&#35760;&#26087;&#20851;&#31995;&#30340;&#21516;&#26102;&#36830;&#32493;&#23398;&#20064;&#26032;&#20851;&#31995;&#65292;&#21482;&#26377;&#26497;&#23569;&#37327;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#21033;&#29992;&#25552;&#31034;&#23398;&#20064;&#26469;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#24335;&#33021;&#21147;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#65292;&#20174;&#32780;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#36830;&#32493;&#23569;&#26679;&#26412;&#20851;&#31995;&#25552;&#21462;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#35774;&#35745;&#25552;&#31034;&#34920;&#31034;&#20197;&#33719;&#24471;&#26356;&#24191;&#20041;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26087;&#30340;&#21644;&#26032;&#30340;&#31867;&#21035;&#65292;&#24182;&#22522;&#20110;&#36793;&#30028;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#26356;&#22810;&#22320;&#20851;&#27880;&#22256;&#38590;&#26679;&#26412;&#65292;&#20174;&#32780;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35299;&#20915;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35760;&#24518;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15713v1 Announce Type: cross  Abstract: Continual Few-shot Relation Extraction (CFRE) is a practical problem that requires the model to continuously learn novel relations while avoiding forgetting old ones with few labeled training data. The primary challenges are catastrophic forgetting and overfitting. This paper harnesses prompt learning to explore the implicit capabilities of pre-trained language models to address the above two challenges, thereby making language models better continual few-shot relation extractors. Specifically, we propose a Contrastive Prompt Learning framework, which designs prompt representation to acquire more generalized knowledge that can be easily adapted to old and new categories, and margin-based contrastive learning to focus more on hard samples, therefore alleviating catastrophic forgetting and overfitting issues. To further remedy overfitting in low-resource scenarios, we introduce an effective memory augmentation strategy that employs well-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-Aug&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#26597;&#35810;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26597;&#35810;&#65292;&#25913;&#21892;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#31946;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2402.15708</link><description>&lt;p&gt;
&#20174;&#33041;&#20449;&#21495;&#35299;&#30721;&#26597;&#35810;&#35821;&#20041;&#30340;&#26597;&#35810;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Query Augmentation by Decoding Semantics from Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-Aug&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#26597;&#35810;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26597;&#35810;&#65292;&#25913;&#21892;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#31946;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#25193;&#23637;&#26159;&#29992;&#20110;&#32454;&#21270;&#35821;&#20041;&#19981;&#20934;&#30830;&#26597;&#35810;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20256;&#32479;&#19978;&#65292;&#26597;&#35810;&#25193;&#23637;&#20381;&#36182;&#20110;&#20174;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#12289;&#28508;&#22312;&#30456;&#20851;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#22914;&#26524;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#36136;&#37327;&#36739;&#20302;&#65292;&#21017;&#26597;&#35810;&#25193;&#23637;&#30340;&#26377;&#25928;&#24615;&#20063;&#20250;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Brain-Aug&#65292;&#36890;&#36807;&#23558;&#20174;&#33041;&#20449;&#21495;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#32467;&#21512;&#21040;&#26597;&#35810;&#20013;&#26469;&#22686;&#24378;&#26597;&#35810;&#12290;Brain-Aug&#20351;&#29992;&#20102;&#22312;&#33041;&#20449;&#21495;&#20449;&#24687;&#26500;&#24314;&#30340;&#25552;&#31034;&#21644;&#38754;&#21521;&#25490;&#21517;&#30340;&#25512;&#29702;&#26041;&#27861;&#29983;&#25104;&#21407;&#22987;&#26597;&#35810;&#30340;&#24310;&#32493;&#37096;&#20998;&#12290;&#23545;fMRI&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Brain-Aug&#29983;&#25104;&#30340;&#26597;&#35810;&#22312;&#35821;&#20041;&#19978;&#26356;&#20934;&#30830;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#12290;&#33041;&#20449;&#21495;&#24102;&#26469;&#30340;&#36825;&#31181;&#25913;&#36827;&#23545;&#20110;&#27169;&#31946;&#26597;&#35810;&#29305;&#21035;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15708v1 Announce Type: cross  Abstract: Query augmentation is a crucial technique for refining semantically imprecise queries. Traditionally, query augmentation relies on extracting information from initially retrieved, potentially relevant documents. If the quality of the initially retrieved documents is low, then the effectiveness of query augmentation would be limited as well. We propose Brain-Aug, which enhances a query by incorporating semantic information decoded from brain signals. BrainAug generates the continuation of the original query with a prompt constructed with brain signal information and a ranking-oriented inference approach. Experimental results on fMRI (functional magnetic resonance imaging) datasets show that Brain-Aug produces semantically more accurate queries, leading to improved document ranking performance. Such improvement brought by brain signals is particularly notable for ambiguous queries.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#32534;&#30721;&#20851;&#31995;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;ICD&#32534;&#30721;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#30456;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15700</link><description>&lt;p&gt;
&#12298;CoRelation: &#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#32534;&#30721;&#20851;&#31995;&#23398;&#20064;&#25552;&#21319;&#33258;&#21160;ICD&#32534;&#30721;&#12299;
&lt;/p&gt;
&lt;p&gt;
CoRelation: Boosting Automatic ICD Coding Through Contextualized Code Relation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15700
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#32534;&#30721;&#20851;&#31995;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;ICD&#32534;&#30721;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#30456;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD&#65289;&#32534;&#30721;&#22312;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#20197;&#20415;&#27491;&#30830;&#35760;&#24405;&#21644;&#35745;&#36153;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25552;&#21319;&#33258;&#21160;ICD&#32534;&#30721;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#21521;&#26159;&#23545;ICD&#32534;&#30721;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#23545;ICD&#32534;&#30721;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#24314;&#27169;&#19981;&#36275;&#65292;&#36890;&#24120;&#24573;&#35270;&#20102;&#20020;&#24202;&#35760;&#24405;&#20013;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#21363;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#21644;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;ICD&#32534;&#30721;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#32771;&#34385;&#20020;&#24202;&#35760;&#24405;&#19978;&#19979;&#25991;&#30340;&#20381;&#36182;&#23398;&#20064;&#33539;&#24335;&#65292;&#23545;&#24314;&#27169;&#25152;&#26377;&#21487;&#33021;&#30340;&#32534;&#30721;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#20844;&#20849;ICD&#32534;&#30721;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15700v1 Announce Type: cross  Abstract: Automatic International Classification of Diseases (ICD) coding plays a crucial role in the extraction of relevant information from clinical notes for proper recording and billing. One of the most important directions for boosting the performance of automatic ICD coding is modeling ICD code relations. However, current methods insufficiently model the intricate relationships among ICD codes and often overlook the importance of context in clinical notes. In this paper, we propose a novel approach, a contextualized and flexible framework, to enhance the learning of ICD code representations. Our approach, unlike existing methods, employs a dependent learning paradigm that considers the context of clinical notes in modeling all possible code relations. We evaluate our approach on six public ICD coding datasets and the experimental results demonstrate the effectiveness of our approach compared to state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#35748;&#30693;&#19968;&#33268;&#24615;&#29702;&#35770;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25552;&#31034;&#25552;&#20379;&#20102;&#24515;&#29702;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38376;&#33050;-&#38376;&#25216;&#26415;&#30340;&#33258;&#21160;&#40657;&#30418;&#36234;&#29425;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15690</link><description>&lt;p&gt;
&#36367;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8220;&#36234;&#29425;&#8221;&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;
&lt;/p&gt;
&lt;p&gt;
Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15690
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#35748;&#30693;&#19968;&#33268;&#24615;&#29702;&#35770;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25552;&#31034;&#25552;&#20379;&#20102;&#24515;&#29702;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38376;&#33050;-&#38376;&#25216;&#26415;&#30340;&#33258;&#21160;&#40657;&#30418;&#36234;&#29425;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#28176;&#28176;&#25104;&#20026;&#20154;&#20204;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#20837;&#21475;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25171;&#30772;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#65288;&#8220;&#30417;&#29425;&#8221;&#65289;&#20197;&#35775;&#38382;&#21463;&#38480;&#20449;&#24687;&#65292;&#36825;&#31216;&#20026;&#8220;&#36234;&#29425;&#8221;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#38754;&#23545;&#27492;&#31867;&#36234;&#29425;&#25915;&#20987;&#26102;&#30340;&#34180;&#24369;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;LLMs&#22312;&#25509;&#25910;&#36234;&#29425;&#25552;&#31034;&#26102;&#20869;&#22312;&#20915;&#31574;&#26426;&#21046;&#30340;&#29702;&#35299;&#26126;&#26174;&#27424;&#32570;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#36234;&#29425;&#25552;&#31034;&#30340;&#24515;&#29702;&#35299;&#37322;&#12290;&#20511;&#37492;&#35748;&#30693;&#19968;&#33268;&#24615;&#29702;&#35770;&#65292;&#25105;&#20204;&#35748;&#20026;&#36234;&#29425;&#30340;&#20851;&#38190;&#26159;&#24341;&#23548;LLMs&#22312;&#38169;&#35823;&#26041;&#21521;&#19978;&#23454;&#29616;&#35748;&#30693;&#21327;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38376;&#33050;-&#38376;&#30340;&#33258;&#21160;&#40657;&#30418;&#36234;&#29425;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#36880;&#27493;&#35825;&#23548;&#27169;&#22411;&#36890;&#36807;&#22810;&#27493;&#22686;&#37327;&#25552;&#31034;&#22238;&#31572;&#26377;&#23475;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15690v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have gradually become the gateway for people to acquire new knowledge. However, attackers can break the model's security protection ("jail") to access restricted information, which is called "jailbreaking." Previous studies have shown the weakness of current LLMs when confronted with such jailbreaking attacks. Nevertheless, comprehension of the intrinsic decision-making mechanism within the LLMs upon receipt of jailbreak prompts is noticeably lacking. Our research provides a psychological explanation of the jailbreak prompts. Drawing on cognitive consistency theory, we argue that the key to jailbreak is guiding the LLM to achieve cognitive coordination in an erroneous direction. Further, we propose an automatic black-box jailbreaking method based on the Foot-in-the-Door (FITD) technique. This method progressively induces the model to answer harmful questions via multi-step incremental prompts. We instantiat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#33647;&#29289;&#30417;&#27979;&#20107;&#20214;&#25552;&#21462;&#20013;&#21033;&#29992;ChatGPT&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;ChatGPT&#22312;&#36866;&#24403;&#30340;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#19981;&#21450;&#23436;&#20840;&#24494;&#35843;&#30340;&#23567;&#22411;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15663</link><description>&lt;p&gt;
&#22312;&#33647;&#29289;&#30417;&#27979;&#20107;&#20214;&#25552;&#21462;&#20013;&#21033;&#29992;ChatGPT&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#33647;&#29289;&#30417;&#27979;&#20107;&#20214;&#25552;&#21462;&#20013;&#21033;&#29992;ChatGPT&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;ChatGPT&#22312;&#36866;&#24403;&#30340;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#19981;&#21450;&#23436;&#20840;&#24494;&#35843;&#30340;&#23567;&#22411;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#25506;&#32034;&#20854;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;LLMs&#65292;&#29305;&#21035;&#26159;ChatGPT&#65292;&#22312;&#33647;&#29289;&#30417;&#27979;&#20107;&#20214;&#25552;&#21462;&#20013;&#30340;&#33021;&#21147;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#20174;&#25991;&#26412;&#21307;&#30103;&#26469;&#28304;&#20013;&#35782;&#21035;&#21644;&#25552;&#21462;&#19981;&#33391;&#20107;&#20214;&#25110;&#28508;&#22312;&#27835;&#30103;&#20107;&#20214;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35780;&#20272;ChatGPT&#22312;&#33647;&#29289;&#30417;&#27979;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#37319;&#29992;&#21508;&#31181;&#25552;&#31034;&#21644;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;ChatGPT&#22312;&#36866;&#24403;&#30340;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#19979;&#34920;&#29616;&#20986;&#21512;&#29702;&#30340;&#24615;&#33021;&#65292;&#20294;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#23567;&#22411;&#27169;&#22411;&#30456;&#27604;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;ChatGPT&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#23558;&#21512;&#25104;&#25968;&#25454;&#32435;&#20837;&#24494;&#35843;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15663v1 Announce Type: new  Abstract: With the advent of large language models (LLMs), there has been growing interest in exploring their potential for medical applications. This research aims to investigate the ability of LLMs, specifically ChatGPT, in the context of pharmacovigilance event extraction, of which the main goal is to identify and extract adverse events or potential therapeutic events from textual medical sources. We conduct extensive experiments to assess the performance of ChatGPT in the pharmacovigilance event extraction task, employing various prompts and demonstration selection strategies. The findings demonstrate that while ChatGPT demonstrates reasonable performance with appropriate demonstration selection strategies, it still falls short compared to fully fine-tuned small models. Additionally, we explore the potential of leveraging ChatGPT for data augmentation. However, our investigation reveals that the inclusion of synthesized data into fine-tuning m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#29289;&#29702;&#25512;&#29702;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#23581;&#35797;&#35299;&#20915;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#23545;&#35937;&#25805;&#20316;&#21644;&#25918;&#32622;&#20219;&#21153;&#20013;&#26080;&#27861;&#27491;&#30830;&#32452;&#21512;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15654</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#27169;&#24577;&#25512;&#29702;&#29289;&#29702;&#21160;&#21147;&#23398;&#20013;&#30340;&#22833;&#36133;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#29289;&#29702;&#25512;&#29702;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#23581;&#35797;&#35299;&#20915;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#23545;&#35937;&#25805;&#20316;&#21644;&#25918;&#32622;&#20219;&#21153;&#20013;&#26080;&#27861;&#27491;&#30830;&#32452;&#21512;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#29289;&#29702;&#25512;&#29702;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#65292;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;LLMs&#23637;&#31034;&#20102;&#20851;&#20110;&#21508;&#31181;&#29289;&#20307;&#30340;&#21407;&#23376;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#22312;&#29289;&#20307;&#25805;&#20316;&#21644;&#25918;&#32622;&#20219;&#21153;&#30340;&#27491;&#30830;&#35299;&#20915;&#26041;&#26696;&#20013;&#26410;&#33021;&#23558;&#36825;&#20123;&#30693;&#35782;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;BLIP&#65292;&#36825;&#26159;&#19968;&#20010;&#35757;&#32451;&#26377;&#20132;&#21449;&#27169;&#24577;&#27880;&#24847;&#21147;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#35782;&#21035;&#20102;&#19982;&#23545;&#35937;&#29289;&#29702;&#23646;&#24615;&#30456;&#20851;&#30340;&#23548;&#33268;&#35813;&#27169;&#22411;&#26080;&#27861;&#22522;&#20110;&#30340;&#26696;&#20363;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21457;&#29616;&#29615;&#22659;&#20013;&#23545;&#35937;&#30456;&#20851;&#23646;&#24615;&#30340;&#31243;&#24207;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#23558;&#36825;&#20123;&#30693;&#35782;&#25552;&#28860;&#22238;LLM&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15654v1 Announce Type: new  Abstract: In this paper, we present an exploration of LLMs' abilities to problem solve with physical reasoning in situated environments. We construct a simple simulated environment and demonstrate examples of where, in a zero-shot setting, both text and multimodal LLMs display atomic world knowledge about various objects but fail to compose this knowledge in correct solutions for an object manipulation and placement task. We also use BLIP, a vision-language model trained with more sophisticated cross-modal attention, to identify cases relevant to object physical properties that that model fails to ground. Finally, we present a procedure for discovering the relevant properties of objects in the environment and propose a method to distill this knowledge back into the LLM.
&lt;/p&gt;</description></item><item><title>&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#19978;&#19979;&#25991;&#31034;&#20363;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15637</link><description>&lt;p&gt;
&#22788;&#29702;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#31034;&#20363;&#23545;&#39034;&#24207;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15637
&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#19978;&#19979;&#25991;&#31034;&#20363;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#19978;&#19979;&#25991;&#31034;&#20363;&#39034;&#24207;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLMs&#65289;&#23545;&#27492;&#39034;&#24207;&#27604;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLMs&#65289;&#26356;&#25935;&#24863;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#29616;&#35937;&#24402;&#22240;&#20110;CausalLMs&#20013;&#30340;&#33258;&#22238;&#24402;&#27880;&#24847;&#21147;&#25513;&#27169;&#65292;&#36825;&#20123;&#25513;&#27169;&#38480;&#21046;&#27599;&#20010;&#26631;&#35760;&#19981;&#33021;&#35775;&#38382;&#38543;&#21518;&#30340;&#26631;&#35760;&#30340;&#20449;&#24687;&#12290;&#36825;&#23548;&#33268;&#19981;&#21516;&#20301;&#32622;&#30340;&#26679;&#26412;&#20855;&#26377;&#19981;&#21516;&#30340;&#24863;&#21463;&#37326;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21516;&#20301;&#32622;&#30340;&#34920;&#24449;&#24046;&#24322;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;&#20449;&#24687;&#22686;&#24378;&#21644;&#19968;&#33268;&#24615;&#22686;&#24378;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#19981;&#21516;&#20301;&#32622;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#34920;&#24449;&#65292;&#24182;&#24341;&#20837;&#19968;&#33268;&#24615;&#25439;&#22833;&#20197;&#30830;&#20445;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15637v1 Announce Type: new  Abstract: In-context learning has become a popular paradigm in natural language processing. However, its performance can be significantly influenced by the order of in-context demonstration examples. In this paper, we found that causal language models (CausalLMs) are more sensitive to this order compared to prefix language models (PrefixLMs). We attribute this phenomenon to the auto-regressive attention masks within CausalLMs, which restrict each token from accessing information from subsequent tokens. This results in different receptive fields for samples at different positions, thereby leading to representation disparities across positions. To tackle this challenge, we introduce an unsupervised fine-tuning method, termed the Information-Augmented and Consistency-Enhanced approach. This approach utilizes contrastive learning to align representations of in-context examples across different positions and introduces a consistency loss to ensure simi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21033;&#29992;&#33258;&#35748;&#35777;&#26694;&#26550;&#36827;&#34892;&#32454;&#31890;&#24230;&#20107;&#23454;&#32423;&#21035;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#24187;&#35273;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.15631</link><description>&lt;p&gt;
&#32454;&#31890;&#24230;&#30340;&#33258;&#35748;&#35777;&#21487;&#20197;&#25552;&#21319;&#20107;&#23454;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Self-Endorsement Improves Factuality and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21033;&#29992;&#33258;&#35748;&#35777;&#26694;&#26550;&#36827;&#34892;&#32454;&#31890;&#24230;&#20107;&#23454;&#32423;&#21035;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#24187;&#35273;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#32531;&#35299;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#30340;&#24187;&#35273;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#35748;&#35777;&#26694;&#26550;&#65292;&#21033;&#29992;&#36328;&#22810;&#20010;&#25277;&#26679;&#21709;&#24212;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#20107;&#23454;&#32423;&#21035;&#27604;&#36739;&#12290;&#19982;&#20808;&#21069;&#30340;&#32452;&#21512;&#26041;&#27861;&#65288;&#29579;&#31561;&#65292;2022&#24180;&#65307;&#38472;&#31561;&#65292;2023&#24180;&#65289;&#36827;&#34892;&#21709;&#24212;&#32423;&#21035;&#36873;&#25321;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#20943;&#36731;&#24187;&#35273;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#31687;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#26377;&#30410;&#20110;&#36739;&#23567;&#21644;&#24320;&#28304;&#30340;LLM&#65292;&#22240;&#20026;&#23427;&#20027;&#35201;&#36827;&#34892;&#31616;&#21333;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#27604;&#36739;&#12290;&#22312;&#20256;&#35760;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30452;&#35266;&#30340;&#25552;&#31034;&#26377;&#25928;&#25913;&#21892;&#19981;&#21516;&#35268;&#27169;&#30340;LLM&#29983;&#25104;&#30340;&#20107;&#23454;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;TriviaQA&#21644;GSM8K&#30340;&#20840;&#38754;&#20998;&#26512;&#23637;&#31034;&#20102;&#33258;&#35748;&#35777;&#22312;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15631v1 Announce Type: cross  Abstract: This work studies improving large language model (LLM) generations at inference time by mitigating fact-conflicting hallucinations. Particularly, we propose a self-endorsement framework that leverages the fine-grained fact-level comparisons across multiple sampled responses. Compared with prior ensemble methods (Wang et al., 2022;Chen et al., 2023)) that perform response-level selection, our approach can better alleviate hallucinations, especially for longform generation tasks. Our approach can broadly benefit smaller and open-source LLMs as it mainly conducts simple content-based comparisons. Experiments on Biographies show that our method can effectively improve the factuality of generations with simple and intuitive prompts across different scales of LLMs. Besides, comprehensive analyses on TriviaQA and GSM8K demonstrate the potential of self-endorsement for broader application.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20197;&#20154;&#31867;&#21487;&#35835;&#25991;&#26412;&#34920;&#31034;&#30340;&#29992;&#25143;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;Language-based Factorization Model (LFM)&#65292;&#22312;&#20919;&#21551;&#21160;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.15623</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#20559;&#22909;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language-Based User Profiles for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15623
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20197;&#20154;&#31867;&#21487;&#35835;&#25991;&#26412;&#34920;&#31034;&#30340;&#29992;&#25143;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;Language-based Factorization Model (LFM)&#65292;&#22312;&#20919;&#21551;&#21160;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;&#25512;&#33616;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#20998;&#35299;&#65289;&#23558;&#29992;&#25143;&#20559;&#22909;&#34920;&#31034;&#20026;&#39640;&#32500;&#21521;&#37327;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#21521;&#37327;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#22312;&#20919;&#21551;&#21160;&#29615;&#22659;&#19979;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#20197;&#20154;&#31867;&#21487;&#35835;&#25991;&#26412;&#34920;&#31034;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#22240;&#23376;&#20998;&#35299;&#27169;&#22411;&#65288;LFM&#65289;&#65292;&#23427;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#22343;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#32534;&#30721;&#22120;LLM&#20174;&#29992;&#25143;&#30340;&#35780;&#20998;&#21382;&#21490;&#29983;&#25104;&#29992;&#25143;&#20852;&#36259;&#30340;&#31616;&#27905;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#35299;&#30721;&#22120;LLM&#20351;&#29992;&#36825;&#20010;&#31616;&#35201;&#25551;&#36848;&#26469;&#23436;&#25104;&#39044;&#27979;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;MovieLens&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LFM&#26041;&#27861;&#65292;&#23558;&#20854;&#19982;&#30697;&#38453;&#20998;&#35299;&#21644;&#30452;&#25509;&#20174;&#29992;&#25143;&#35780;&#20998;&#21382;&#21490;&#39044;&#27979;&#30340;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#20919;&#21551;&#21160;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15623v1 Announce Type: new  Abstract: Most conventional recommendation methods (e.g., matrix factorization) represent user profiles as high-dimensional vectors. Unfortunately, these vectors lack interpretability and steerability, and often perform poorly in cold-start settings. To address these shortcomings, we explore the use of user profiles that are represented as human-readable text. We propose the Language-based Factorization Model (LFM), which is essentially an encoder/decoder model where both the encoder and the decoder are large language models (LLMs). The encoder LLM generates a compact natural-language profile of the user's interests from the user's rating history. The decoder LLM uses this summary profile to complete predictive downstream tasks. We evaluate our LFM approach on the MovieLens dataset, comparing it against matrix factorization and an LLM model that directly predicts from the user's rating history. In cold-start settings, we find that our method can h
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;LLMs&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#26631;&#35760;&#25968;&#25454;&#33719;&#21462;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.15613</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#34920;&#31034;&#23454;&#29616;&#22312;NLP&#20013;&#30340;&#39640;&#25928;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Active Learning in NLP via Pretrained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15613
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;LLMs&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#26631;&#35760;&#25968;&#25454;&#33719;&#21462;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24494;&#35843;&#29616;&#22312;&#26159;&#25991;&#26412;&#20998;&#31867;&#20013;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#33021;&#30475;&#21040;&#12290;&#24403;&#26631;&#35760;&#25991;&#26723;&#31232;&#32570;&#26102;&#65292;&#20027;&#21160;&#23398;&#20064;&#26377;&#21161;&#20110;&#33410;&#30465;&#27880;&#37322;&#24037;&#20316;&#65292;&#20294;&#38656;&#35201;&#22312;&#27599;&#27425;&#33719;&#21462;&#36845;&#20195;&#26102;&#37325;&#26032;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#20351;&#29992;LLMs&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#36825;&#19968;&#36807;&#31243;&#65292;&#19968;&#26086;&#33719;&#24471;&#25152;&#38656;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23601;&#21487;&#20197;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#24120;&#35265;&#30340;&#25991;&#26412;&#20998;&#31867;&#22522;&#20934;&#19978;&#39564;&#35777;&#65292;&#20197;&#39044;&#35757;&#32451;&#30340;BERT&#21644;RoBERTa&#20316;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#20135;&#29983;&#20102;&#19982;&#22312;&#25972;&#20010;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#24494;&#35843;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#24320;&#38144;&#38477;&#20302;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31243;&#24207;&#33719;&#21462;&#30340;&#25968;&#25454;&#21487;&#20197;&#36328;&#39044;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#27867;&#21270;&#65292;&#20174;&#32780;&#21487;&#20197;&#28789;&#27963;&#36873;&#25321;&#26368;&#32456;&#27169;&#22411;&#25110;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15613v1 Announce Type: cross  Abstract: Fine-tuning Large Language Models (LLMs) is now a common approach for text classification in a wide range of applications. When labeled documents are scarce, active learning helps save annotation efforts but requires retraining of massive models on each acquisition iteration. We drastically expedite this process by using pretrained representations of LLMs within the active learning loop and, once the desired amount of labeled data is acquired, fine-tuning that or even a different pretrained LLM on this labeled data to achieve the best performance. As verified on common text classification benchmarks with pretrained BERT and RoBERTa as the backbone, our strategy yields similar performance to fine-tuning all the way through the active learning loop but is orders of magnitude less computationally expensive. The data acquired with our procedure generalizes across pretrained networks, allowing flexibility in choosing the final model or upda
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ReCoVERR&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#35270;&#35273;-&#35821;&#35328;&#31995;&#32479;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#24230;&#25918;&#24323;&#65292;&#36890;&#36807;&#23547;&#25214;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#32447;&#32034;&#25552;&#20379;&#39069;&#22806;&#35777;&#25454;&#26469;&#21462;&#20195;&#25918;&#24323;&#65292;&#20174;&#32780;&#19981;&#38477;&#20302;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15610</link><description>&lt;p&gt;
&#36873;&#25321;&#8220;&#36873;&#25321;&#24615;&#39044;&#27979;&#8221;&#65306;&#20943;&#23569;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20013;&#19981;&#24517;&#35201;&#30340;&#24323;&#26435;
&lt;/p&gt;
&lt;p&gt;
Selective "Selective Prediction": Reducing Unnecessary Abstention in Vision-Language Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15610
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ReCoVERR&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#35270;&#35273;-&#35821;&#35328;&#31995;&#32479;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#24230;&#25918;&#24323;&#65292;&#36890;&#36807;&#23547;&#25214;&#22270;&#20687;&#20013;&#30340;&#30456;&#20851;&#32447;&#32034;&#25552;&#20379;&#39069;&#22806;&#35777;&#25454;&#26469;&#21462;&#20195;&#25918;&#24323;&#65292;&#20174;&#32780;&#19981;&#38477;&#20302;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#36873;&#25321;&#24615;&#39044;&#27979;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#20801;&#35768;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#19981;&#30830;&#23450;&#26102;&#25918;&#24323;&#22238;&#31572;&#65292;&#20197;&#26368;&#23567;&#21270;&#38169;&#35823;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24403;&#37096;&#32626;&#19968;&#20010;&#23545;&#19981;&#20934;&#30830;&#39044;&#27979;&#23481;&#24525;&#24230;&#20302;&#30340;&#35270;&#35273;-&#35821;&#35328;&#31995;&#32479;&#26102;&#65292;&#36873;&#25321;&#24615;&#39044;&#27979;&#21487;&#33021;&#36807;&#20110;&#35880;&#24910;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#27491;&#30830;&#39044;&#27979;&#19978;&#36807;&#20110;&#25918;&#24323;&#12290;&#25105;&#20204;&#24341;&#20837;ReCoVERR&#65292;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#36873;&#25321;&#24615;&#35270;&#35273;-&#35821;&#35328;&#31995;&#32479;&#36807;&#24230;&#25918;&#24323;&#30340;&#25512;&#29702;&#26102;&#38388;&#31639;&#27861;&#65292;&#32780;&#19981;&#38477;&#20302;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#24403;VLM&#20570;&#20986;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#26102;&#65292;ReCoVERR&#23581;&#35797;&#22312;&#22270;&#20687;&#20013;&#25214;&#21040;&#25552;&#20379;&#39069;&#22806;&#35777;&#25454;&#30340;&#30456;&#20851;&#32447;&#32034;&#65292;&#32780;&#19981;&#26159;&#25918;&#24323;&#12290;ReCoVERR&#20351;&#29992;LLM&#21521;VLM&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#65292;&#25910;&#38598;&#39640;&#32622;&#20449;&#24230;&#35777;&#25454;&#65292;&#22914;&#26524;&#36275;&#22815;&#30340;&#35777;&#25454;&#30830;&#35748;&#39044;&#27979;&#65292;&#21017;&#31995;&#32479;&#20570;&#20986;&#39044;&#27979;&#32780;&#19981;&#26159;&#25918;&#24323;&#12290;ReCoVERR&#20351;&#20004;&#20010;VLM&#65292;BLIP2&#21644;InstructBLIP&#65292;&#33021;&#22815;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15610v1 Announce Type: new  Abstract: Prior work on selective prediction minimizes incorrect predictions from vision-language models (VLMs) by allowing them to abstain from answering when uncertain. However, when deploying a vision-language system with low tolerance for inaccurate predictions, selective prediction may be over-cautious and abstain too frequently, even on many correct predictions. We introduce ReCoVERR, an inference-time algorithm to reduce the over-abstention of a selective vision-language system without decreasing prediction accuracy. When the VLM makes a low-confidence prediction, instead of abstaining ReCoVERR tries to find relevant clues in the image that provide additional evidence for the prediction. ReCoVERR uses an LLM to pose related questions to the VLM, collects high-confidence evidences, and if enough evidence confirms the prediction the system makes a prediction instead of abstaining. ReCoVERR enables two VLMs, BLIP2 and InstructBLIP, to answer u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20132;&#26367;&#24369;&#19977;&#38899;&#32032;/BPE&#23545;&#40784;&#30417;&#30563;&#26469;&#33258;&#28151;&#21512;&#27169;&#22411;&#25913;&#21892;&#31471;&#21040;&#31471; ASR&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#21644;&#36741;&#21161;&#20219;&#21153;&#30340;&#20132;&#26367;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;ASR&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15594</link><description>&lt;p&gt;
&#20132;&#26367;&#24369;&#19977;&#38899;&#32032;/BPE&#23545;&#40784;&#30417;&#30563;&#26469;&#33258;&#28151;&#21512;&#27169;&#22411;&#25913;&#21892;&#31471;&#21040;&#31471; ASR
&lt;/p&gt;
&lt;p&gt;
Alternating Weak Triphone/BPE Alignment Supervision from Hybrid Model Improves End-to-End ASR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15594
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20132;&#26367;&#24369;&#19977;&#38899;&#32032;/BPE&#23545;&#40784;&#30417;&#30563;&#26469;&#33258;&#28151;&#21512;&#27169;&#22411;&#25913;&#21892;&#31471;&#21040;&#31471; ASR&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#21644;&#36741;&#21161;&#20219;&#21153;&#30340;&#20132;&#26367;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;ASR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#26367;&#24369;&#19977;&#38899;&#32032;/BPE&#23545;&#40784;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#31471;&#21040;&#31471;&#27169;&#22411;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#29616;&#26377;&#30340;&#28151;&#21512;ASR&#31995;&#32479;&#25552;&#21462;&#19977;&#38899;&#32032;&#21644;BPE&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#22312;&#32534;&#30721;&#22120;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#19978;&#20998;&#21035;&#38024;&#23545;&#19977;&#38899;&#32032;&#23545;&#40784;&#21644;BPE&#23545;&#40784;&#35745;&#31639;&#22522;&#20110;&#20132;&#21449;&#29109;&#30340;&#20013;&#38388;&#36741;&#21161;&#25439;&#22833;&#65292;&#33719;&#24471;&#27491;&#21017;&#21270;&#25928;&#24212;&#12290;&#36890;&#36807;&#37319;&#29992;&#21442;&#25968;&#20026;0.5&#30340;&#24378;&#26631;&#31614;&#24179;&#28369;&#23454;&#29616;&#24369;&#30417;&#30563;&#12290;&#22312;TED-LIUM 2&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#19977;&#38899;&#32032;&#25110;BPE&#23545;&#40784;&#30340;&#24369;&#30417;&#30563;&#22343;&#25913;&#21892;&#20102;ASR&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#26631;&#20934;CTC&#36741;&#21161;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#32452;&#21512;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#35789;&#38169;&#35823;&#29575;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20004;&#20010;&#36741;&#21161;&#20219;&#21153;&#30340;&#20132;&#26367;&#25805;&#20316;&#65292;&#24182;&#35266;&#23519;&#21040;&#39069;&#22806;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#30340;&#25216;&#26415;&#23548;&#33268;&#20102;&#36229;&#36807;10%&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15594v1 Announce Type: new  Abstract: In this paper, alternating weak triphone/BPE alignment supervision is proposed to improve end-to-end model training. Towards this end, triphone and BPE alignments are extracted using a pre-existing hybrid ASR system. Then, regularization effect is obtained by cross-entropy based intermediate auxiliary losses computed on such alignments at a mid-layer representation of the encoder for triphone alignments and at the encoder for BPE alignments. Weak supervision is achieved through strong label smoothing with parameter of 0.5. Experimental results on TED-LIUM 2 indicate that either triphone or BPE alignment based weak supervision improves ASR performance over standard CTC auxiliary loss. Moreover, their combination lowers the word error rate further. We also investigate the alternation of the two auxiliary tasks during model training, and additional performance gain is observed. Overall, the proposed techniques result in over 10% relative er
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#26469;&#28608;&#21457;&#19977;&#31181;&#27969;&#34892;LLM&#65292;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.15589</link><description>&lt;p&gt;
&#20174;&#23398;&#26415;&#25163;&#31295;&#30340;&#21516;&#34892;&#35780;&#23457;&#21465;&#20107;&#20013;&#35201;&#27714;LLMs&#25776;&#20889;&#20803;&#35780;&#35770;&#33609;&#26696;
&lt;/p&gt;
&lt;p&gt;
Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#26469;&#28608;&#21457;&#19977;&#31181;&#27969;&#34892;LLM&#65292;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#20294;&#20063;&#26368;&#32321;&#37325;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#25776;&#20889;&#20803;&#35780;&#35770;&#65292;&#36825;&#28041;&#21450;&#26681;&#25454;&#22810;&#20301;&#19987;&#23478;&#30340;&#21516;&#34892;&#35780;&#23457;&#21465;&#20107;&#29702;&#35299;&#23398;&#26415;&#25163;&#31295;&#30340;&#26680;&#24515;&#36129;&#29486;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#19987;&#23478;&#22810;&#35270;&#35282;&#30340;&#30475;&#27861;&#24635;&#32467;&#20026;&#31616;&#27905;&#30340;&#25972;&#20307;&#27010;&#36848;&#12290;&#37492;&#20110;&#29983;&#25104;&#22411;AI&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#37325;&#22823;&#21457;&#23637;&#65292;&#25105;&#20204;&#26377;&#20805;&#20998;&#30340;&#29702;&#30001;&#28145;&#20837;&#30740;&#31350;LLMs&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#29615;&#22659;&#20013;&#29983;&#25104;&#36825;&#31181;&#20803;&#35780;&#35770;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19977;&#31181;&#27969;&#34892;&#30340;LLM&#65292;&#21363;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#25191;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;TELeR&#20998;&#31867;&#27861;&#20197;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#20419;&#20351;&#23427;&#20204;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20803;&#35780;&#35770;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15589v1 Announce Type: cross  Abstract: One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves understanding the core contributions, strengths, and weaknesses of a scholarly manuscript based on peer-review narratives from multiple experts and then summarizing those multiple experts' perspectives into a concise holistic overview. Given the latest major developments in generative AI, especially Large Language Models (LLMs), it is very compelling to rigorously study the utility of LLMs in generating such meta-reviews in an academic peer-review setting. In this paper, we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to automatically generate meta-reviews by prompting them with different types/levels of prompts based on the recently proposed TELeR taxonomy. Finally, we perform a detailed qualitative study of the meta-reviews generated by the LLMs and summarize our findings and 
&lt;/p&gt;</description></item><item><title>&#22312;&#31243;&#24207;&#35268;&#21010;&#20013;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#35774;&#32622;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#36215;&#22987;&#28857;&#21644;&#30446;&#26631;&#35266;&#23519;&#30340;&#26631;&#39064;&#20316;&#20026;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26080;&#38656;&#20219;&#21153;&#21517;&#31216;&#30340;&#19978;&#19979;&#25991;&#27880;&#20837;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26631;&#27880;&#25104;&#26412;</title><link>https://arxiv.org/abs/2402.15579</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#20219;&#21153;&#21517;&#31216;&#30340;CI: &#26080;&#38656;&#20219;&#21153;&#21517;&#31216;&#30340;&#19978;&#19979;&#25991;&#27880;&#20837;&#29992;&#20110;&#31243;&#24207;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CI w/o TN: Context Injection without Task Name for Procedure Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15579
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31243;&#24207;&#35268;&#21010;&#20013;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#35774;&#32622;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#36215;&#22987;&#28857;&#21644;&#30446;&#26631;&#35266;&#23519;&#30340;&#26631;&#39064;&#20316;&#20026;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26080;&#38656;&#20219;&#21153;&#21517;&#31216;&#30340;&#19978;&#19979;&#25991;&#27880;&#20837;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26631;&#27880;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#25351;&#23548;&#35270;&#39057;&#20013;&#31243;&#24207;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#28041;&#21450;&#26681;&#25454;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#36215;&#22987;&#28857;&#21644;&#30446;&#26631;&#35266;&#23519;&#21019;&#24314;&#30446;&#26631;&#23548;&#21521;&#35745;&#21010;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#36880;&#28176;&#20943;&#24369;&#35757;&#32451;&#30417;&#30563;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20174;&#37325;&#22411;&#20013;&#38388;&#35270;&#35273;&#35266;&#23519;&#25110;&#35821;&#35328;&#35828;&#26126;&#21040;&#20219;&#21153;&#31867;&#21035;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#21363;&#20351;&#21482;&#32473;&#20986;&#20219;&#21153;&#21517;&#31216;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#33021;&#29983;&#25104;&#35814;&#32454;&#30340;&#35745;&#21010;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27809;&#26377;&#20219;&#21153;&#21517;&#31216;&#20316;&#20026;&#30417;&#30563;&#30340;&#26356;&#24369;&#35774;&#32622;&#65292;&#36825;&#26159;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20855;&#26377;&#36275;&#22815;&#20449;&#24687;&#30340;&#33391;&#22909;&#25552;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20551;&#35774;&#20808;&#21069;&#30340;&#20013;&#38388;&#30417;&#30563;&#21487;&#20197;&#20316;&#20026;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#19988;&#25105;&#20204;&#20351;&#29992;&#35270;&#35273;&#36215;&#22987;&#28857;&#21644;&#30446;&#26631;&#35266;&#23519;&#30340;&#26631;&#39064;&#20316;&#20026;&#19968;&#20010;&#26356;&#24265;&#20215;&#30340;&#30417;&#30563;&#24418;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#26631;&#27880;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15579v1 Announce Type: cross  Abstract: This paper explores the challenge of procedure planning in instructional videos, which involves creating goal-directed plans based on visual start and goal observations from videos. Previous research has tackled this problem with gradually weaker training supervision, from heavy intermediate visual observations or language instructions to task class supervision. However, with the advent of large language models, even given only the task name, these models can produce a detailed plan. In this study, we propose a much weaker setting without task name as supervision, which is not currently solvable by existing large language models since they require good prompts with sufficient information. Specifically, we hypothesize that previous intermediate supervisions can serve as context information, and we use captions of visual start and goal observations as a much cheaper form of supervision. This approach greatly reduces the labeling cost sin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35752;&#35770;&#29305;&#23450;&#35805;&#39064;&#30340;&#29992;&#25143;&#32676;&#20043;&#38388;&#20256;&#25773;&#30340;&#20449;&#24687;&#20013;&#25552;&#21462;&#24433;&#21709;&#25351;&#26631;&#65292;&#37325;&#28857;&#20851;&#27880;&#24433;&#21709;&#20256;&#25773;&#21644;&#26816;&#27979;&#26356;&#26377;&#24847;&#22270;&#30340;&#24433;&#21709;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.15571</link><description>&lt;p&gt;
Social Convos: &#25429;&#25417;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35758;&#31243;&#21644;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
Social Convos: Capturing Agendas and Emotions on Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35752;&#35770;&#29305;&#23450;&#35805;&#39064;&#30340;&#29992;&#25143;&#32676;&#20043;&#38388;&#20256;&#25773;&#30340;&#20449;&#24687;&#20013;&#25552;&#21462;&#24433;&#21709;&#25351;&#26631;&#65292;&#37325;&#28857;&#20851;&#27880;&#24433;&#21709;&#20256;&#25773;&#21644;&#26816;&#27979;&#26356;&#26377;&#24847;&#22270;&#30340;&#24433;&#21709;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#22312;&#31867;&#20284;&#36873;&#20030;&#25110;&#27969;&#34892;&#30149;&#31561;&#37325;&#22823;&#20844;&#20849;&#20107;&#20214;&#26399;&#38388;&#20256;&#25773;&#26377;&#38024;&#23545;&#24615;&#20449;&#24687;&#30340;&#28909;&#38376;&#24037;&#20855;&#12290;&#23545;&#20449;&#24687;&#27969;&#37327;&#30340;&#31995;&#32479;&#20998;&#26512;&#21487;&#20197;&#20026;&#19981;&#21516;&#20154;&#32676;&#20043;&#38388;&#30340;&#20027;&#27969;&#35266;&#28857;&#21644;&#31038;&#20250;&#21160;&#24577;&#25552;&#20379;&#23453;&#36149;&#35265;&#35299;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#24433;&#21709;&#20256;&#25773;&#65292;&#29305;&#21035;&#26159;&#26159;&#21542;&#33021;&#22815;&#26816;&#27979;&#21040;&#26356;&#26377;&#24847;&#22270;&#30340;&#24433;&#21709;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36807;&#28388;&#20986;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#25351;&#26631;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#20197;&#21450;&#36890;&#24120;&#28151;&#20081;&#30340;&#31038;&#20132;&#23186;&#20307;&#27969;&#37327;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20174;&#22260;&#32469;&#29305;&#23450;&#35805;&#39064;&#35752;&#35770;&#30340;&#29992;&#25143;&#32676;&#20043;&#38388;&#20256;&#25773;&#30340;&#20449;&#24687;&#20013;&#25552;&#21462;&#24433;&#21709;&#25351;&#26631;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;"&#23545;&#35805;"&#30340;&#27010;&#24565;&#19978;&#65292;&#20197;&#35782;&#21035;&#31215;&#26497;&#25512;&#21160;&#35813;&#35805;&#39064;&#21608;&#22260;&#26576;&#31181;&#29305;&#23450;&#35758;&#31243;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#24433;&#21709;&#25351;&#26631;&#65306;&#65288;&#25511;&#21046;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15571v1 Announce Type: cross  Abstract: Social media platforms are popular tools for disseminating targeted information during major public events like elections or pandemics. Systematic analysis of the message traffic can provide valuable insights into prevailing opinions and social dynamics among different segments of the population. We are specifically interested in influence spread, and in particular whether more deliberate influence operations can be detected. However, filtering out the essential messages with telltale influence indicators from the extensive and often chaotic social media traffic is a major challenge. In this paper we present a novel approach to extract influence indicators from messages circulating among groups of users discussing particular topics. We build upon the concept of a convo to identify influential authors who are actively promoting some particular agenda around that topic within the group. We focus on two influence indicators: the (control 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#26463;&#25628;&#32034;&#30340;&#24555;&#36895;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;BEAST&#65292;&#33021;&#22815;&#22312;&#19968;&#20998;&#38047;&#20869;&#39640;&#25104;&#21151;&#29575;&#22320;&#36234;&#29425;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#36824;&#33021;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.15570</link><description>&lt;p&gt;
&#19968;&#20998;&#38047;&#20869;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#24555;&#36895;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Fast Adversarial Attacks on Language Models In One GPU Minute
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15570
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#26463;&#25628;&#32034;&#30340;&#24555;&#36895;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;BEAST&#65292;&#33021;&#22815;&#22312;&#19968;&#20998;&#38047;&#20869;&#39640;&#25104;&#21151;&#29575;&#22320;&#36234;&#29425;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#36824;&#33021;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24555;&#36895;&#22522;&#20110;&#26463;&#25628;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#65288;BEAST&#65289;&#12290;BEAST&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#21442;&#25968;&#65292;&#20351;&#25915;&#20987;&#32773;&#33021;&#22815;&#22312;&#25915;&#20987;&#36895;&#24230;&#12289;&#25104;&#21151;&#29575;&#21644;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;BEAST&#30340;&#35745;&#31639;&#25928;&#29575;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#20854;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#36234;&#29425;&#12289;&#24341;&#21457;&#24187;&#35273;&#21644;&#38544;&#31169;&#25915;&#20987;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#21487;&#20197;&#22312;&#19968;&#20998;&#38047;&#20869;&#36234;&#29425;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#24456;&#39640;&#12290;&#20363;&#22914;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#22522;&#20934;&#30456;&#27604;&#65292;BEAST&#21487;&#20197;&#22312;&#19968;&#20998;&#38047;&#20869;&#36234;&#29425; Vicuna-7B-v1.5&#65292;&#25104;&#21151;&#29575;&#36798;&#21040;89%&#65292;&#32780;&#22522;&#20934;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#23567;&#26102;&#20197;&#19978;&#25165;&#33021;&#20351;&#29992;&#21333;&#20010; Nvidia RTX A6000 48GB GPU &#23454;&#29616;70%&#30340;&#25104;&#21151;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#29420;&#29305;&#30340;&#32467;&#26524;&#65292;&#21363;&#25105;&#20204;&#30340;&#38750;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#20250;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#20135;&#29983;&#24187;&#35273;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#38750;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#23548;&#33268;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15570v1 Announce Type: cross  Abstract: In this paper, we introduce a novel class of fast, beam search-based adversarial attack (BEAST) for Language Models (LMs). BEAST employs interpretable parameters, enabling attackers to balance between attack speed, success rate, and the readability of adversarial prompts. The computational efficiency of BEAST facilitates us to investigate its applications on LMs for jailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free targeted attack can jailbreak aligned LMs with high attack success rates within one minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute with a success rate of 89% when compared to a gradient-based baseline that takes over an hour to achieve 70% success rate using a single Nvidia RTX A6000 48GB GPU. Additionally, we discover a unique outcome wherein our untargeted attack induces hallucinations in LM chatbots. Through human evaluations, we find that our untargeted attack cause
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19987;&#20026;&#38889;&#22269;ASD&#20799;&#31461;&#35774;&#35745;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25552;&#21319;&#35821;&#38899;&#25216;&#26415;&#20197;&#20419;&#36827;&#21457;&#38899;&#21644;&#20005;&#37325;&#31243;&#24230;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.15539</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#38889;&#22269;&#20799;&#31461;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#65306;&#36208;&#21521;&#33258;&#21160;&#35780;&#20272;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Speech Corpus for Korean Children with Autism Spectrum Disorder: Towards Automatic Assessment Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15539
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19987;&#20026;&#38889;&#22269;ASD&#20799;&#31461;&#35774;&#35745;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25552;&#21319;&#35821;&#38899;&#25216;&#26415;&#20197;&#20419;&#36827;&#21457;&#38899;&#21644;&#20005;&#37325;&#31243;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#20799;&#31461;&#25968;&#23383;&#21270;&#27835;&#30103;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#38024;&#23545;&#38889;&#22269;ASD&#20799;&#31461;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#20026;&#38889;&#22269;ASD&#20799;&#31461;&#35774;&#35745;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25512;&#36827;&#35821;&#38899;&#25216;&#26415;&#65292;&#22914;&#21457;&#38899;&#21644;&#20005;&#37325;&#31243;&#24230;&#35780;&#20272;&#12290;&#35821;&#38899;&#19982;&#35821;&#35328;&#35780;&#20272;&#20250;&#35805;&#20013;&#30340;&#35821;&#38899;&#24405;&#38899;&#34987;&#36716;&#24405;&#65292;&#24182;&#38024;&#23545;&#21457;&#38899;&#21644;&#35821;&#35328;&#29305;&#24449;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#19977;&#21517;&#35821;&#38899;&#19982;&#35821;&#35328;&#30149;&#29702;&#24072;&#20351;&#29992;3&#28857;&#21147;&#20811;&#29305;&#37327;&#34920;&#23545;&#36825;&#20123;&#24405;&#38899;&#36827;&#34892;&#20102;&#31038;&#20132;&#27807;&#36890;&#20005;&#37325;&#31243;&#24230;&#65288;SCS&#65289;&#21644;&#21457;&#38899;&#29087;&#32451;&#24230;&#65288;PP&#65289;&#35780;&#20998;&#12290;&#21442;&#19982;&#32773;&#24635;&#25968;&#23558;&#26377;300&#21517;ASD&#20799;&#31461;&#21644;50&#21517;&#27491;&#24120;&#21457;&#32946;&#65288;TD&#65289;&#20799;&#31461;&#12290;&#26412;&#25991;&#36824;&#20998;&#26512;&#20102;&#20174;&#25910;&#38598;&#30340;&#35821;&#38899;&#25968;&#25454;&#20013;&#25552;&#21462;&#24182;&#23436;&#25104;&#27880;&#37322;&#30340;73&#21517;ASD&#20799;&#31461;&#21644;9&#21517;TD&#20799;&#31461;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15539v1 Announce Type: cross  Abstract: Despite the growing demand for digital therapeutics for children with Autism Spectrum Disorder (ASD), there is currently no speech corpus available for Korean children with ASD. This paper introduces a speech corpus specifically designed for Korean children with ASD, aiming to advance speech technologies such as pronunciation and severity evaluation. Speech recordings from speech and language evaluation sessions were transcribed, and annotated for articulatory and linguistic characteristics. Three speech and language pathologists rated these recordings for social communication severity (SCS) and pronunciation proficiency (PP) using a 3-point Likert scale. The total number of participants will be 300 for children with ASD and 50 for typically developing (TD) children. The paper also analyzes acoustic and linguistic features extracted from speech data collected and completed for annotation from 73 children with ASD and 9 TD children to i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15537</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Performance of ChatGPT for Spam Email Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#37038;&#20214;&#32487;&#32493;&#26159;&#19987;&#19994;&#21644;&#21830;&#19994;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#36890;&#20449;&#23186;&#20171;&#12290;&#28982;&#32780;&#65292;&#22403;&#22334;&#37038;&#20214;&#30340;&#26222;&#21450;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#25200;&#20081;&#20102;&#20182;&#20204;&#30340;&#26085;&#24120;&#24037;&#20316;&#24182;&#38477;&#20302;&#20102;&#29983;&#20135;&#29575;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20869;&#23481;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#36807;&#28388;&#22403;&#22334;&#37038;&#20214;&#23545;&#32593;&#32476;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#65292;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#35780;&#20272;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;ChatGPT&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#38656;&#35201;&#25552;&#31034;&#35828;&#26126;&#21644;&#23569;&#37327;&#31034;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15537v1 Announce Type: cross  Abstract: Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction and a few demonstrations. We also investigate how the t
&lt;/p&gt;</description></item><item><title>PCA-Bench &#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#24341;&#20837;&#22797;&#26434;&#22330;&#26223;&#21644;&#38169;&#35823;&#23450;&#20301;&#33021;&#21147;&#65292;&#25552;&#39640;&#37096;&#32626;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758; PCA-Eval&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.15527</link><description>&lt;p&gt;
PCA-Bench: &#35780;&#20272;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24863;&#30693;-&#35748;&#30693;-&#34892;&#21160;&#38142;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15527
&lt;/p&gt;
&lt;p&gt;
PCA-Bench &#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#24341;&#20837;&#22797;&#26434;&#22330;&#26223;&#21644;&#38169;&#35823;&#23450;&#20301;&#33021;&#21147;&#65292;&#25552;&#39640;&#37096;&#32626;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758; PCA-Eval&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PCA-Bench&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#32508;&#21512;&#33021;&#21147;&#30340;&#22810;&#27169;&#20915;&#31574;&#22522;&#20934;&#12290;&#19982;&#20043;&#21069;&#19987;&#27880;&#20110;&#31616;&#21333;&#20219;&#21153;&#21644;&#21333;&#20010;&#27169;&#22411;&#33021;&#21147;&#30340;&#22522;&#20934;&#19981;&#21516;&#65292;PCA-Bench&#24341;&#20837;&#20102;&#19977;&#20010;&#22797;&#26434;&#22330;&#26223;&#65306;&#33258;&#21160;&#39550;&#39542;&#12289;&#23478;&#24237;&#26426;&#22120;&#20154;&#21644;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#12290;&#22312;&#32473;&#23450;&#20219;&#21153;&#25351;&#20196;&#21644;&#22810;&#26679;&#21270;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#38656;&#35201;&#26080;&#32541;&#25972;&#21512;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#34892;&#21160;&#30340;&#22810;&#37325;&#33021;&#21147;&#65292;&#20197;&#36827;&#34892;&#25512;&#29702;&#38142;&#20197;&#20570;&#20986;&#20934;&#30830;&#20915;&#23450;&#12290;&#27492;&#22806;&#65292;PCA-Bench&#20855;&#26377;&#38169;&#35823;&#23450;&#20301;&#33021;&#21147;&#65292;&#23457;&#26597;&#27169;&#22411;&#22312;&#24863;&#30693;&#12289;&#30693;&#35782;&#25110;&#25512;&#29702;&#31561;&#39046;&#22495;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#36825;&#25552;&#39640;&#20102;&#37096;&#32626;MLLMs&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#22312;&#35780;&#20272;&#20013;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PCA-Eval&#65292;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#35780;&#20272;&#20102;10&#31181;&#27969;&#34892;&#30340;MLLMs&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15527v1 Announce Type: cross  Abstract: We present PCA-Bench, a multimodal decision-making benchmark for evaluating the integrated capabilities of Multimodal Large Language Models (MLLMs). Departing from previous benchmarks focusing on simplistic tasks and individual model capability, PCA-Bench introduces three complex scenarios: autonomous driving, domestic robotics, and open-world games. Given task instructions and diverse contexts, the model is required to seamlessly integrate multiple capabilities of Perception, Cognition, and Action in a reasoning chain to make accurate decisions. Moreover, PCA-Bench features error localization capabilities, scrutinizing model inaccuracies in areas such as perception, knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To balance accuracy and efficiency in evaluation, we propose PCA-Eval, an automatic evaluation protocol, and assess 10 prevalent MLLMs. The results reveal significant performance disparities between
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26694;&#26550;&#29702;&#35770;&#26816;&#27979;&#19981;&#23454;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26694;&#26550;&#20803;&#32032;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#26816;&#27979;&#19981;&#21516;&#26694;&#26550;&#19979;&#20934;&#30830;&#20107;&#23454;&#20135;&#29983;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.15525</link><description>&lt;p&gt;
&#36890;&#36807;&#26694;&#26550;&#29702;&#35770;&#26816;&#27979;&#19981;&#23454;&#20449;&#24687;&#65306;&#22522;&#20110;&#26694;&#26550;&#20803;&#32032;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Detecting misinformation through Framing Theory: the Frame Element-based Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15525
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26694;&#26550;&#29702;&#35770;&#26816;&#27979;&#19981;&#23454;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26694;&#26550;&#20803;&#32032;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#26816;&#27979;&#19981;&#21516;&#26694;&#26550;&#19979;&#20934;&#30830;&#20107;&#23454;&#20135;&#29983;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#35823;&#23548;&#20449;&#24687;&#26816;&#27979;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#21465;&#20107;&#26694;&#26550;&#30340;&#24494;&#22937;&#25805;&#32437;&#8212;&#8212;&#36825;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#35823;&#23548;&#24615;&#21465;&#20107;&#30340;&#28508;&#21147;&#20984;&#26174;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#32039;&#36843;&#24615;&#12290;&#22522;&#20110;&#20256;&#25773;&#21644;&#26694;&#26550;&#29702;&#35770;&#65292;&#25105;&#20204;&#35748;&#20026;&#20934;&#30830;&#20449;&#24687;&#30340;&#23637;&#31034;&#25110;&#8220;&#26694;&#26550;&#21270;&#8221;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#20854;&#35299;&#37322;&#65292;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#30495;&#23454;&#26696;&#20363;&#31361;&#20986;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#21465;&#20107;&#26694;&#26550;&#30340;&#36716;&#21464;&#22914;&#20309;&#23558;&#22522;&#20110;&#20107;&#23454;&#30340;&#20449;&#24687;&#36716;&#21270;&#20026;&#35823;&#23548;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#26469;&#26816;&#27979;&#28304;&#33258;&#19981;&#21516;&#26694;&#26550;&#19979;&#34920;&#29616;&#30340;&#20934;&#30830;&#20107;&#23454;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15525v1 Announce Type: new  Abstract: In this paper, we delve into the rapidly evolving challenge of misinformation detection, with a specific focus on the nuanced manipulation of narrative frames - an under-explored area within the AI community. The potential for Generative AI models to generate misleading narratives underscores the urgency of this problem. Drawing from communication and framing theories, we posit that the presentation or 'framing' of accurate information can dramatically alter its interpretation, potentially leading to misinformation. We highlight this issue through real-world examples, demonstrating how shifts in narrative frames can transmute fact-based information into misinformation. To tackle this challenge, we propose an innovative approach leveraging the power of pre-trained Large Language Models and deep neural networks to detect misinformation originating from accurate facts portrayed under different frames. These advanced AI techniques offer unpr
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20250;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#22914;&#20309;&#21462;&#20915;&#20110;&#27169;&#22411;&#21442;&#25968;&#26159;&#29702;&#35299;&#20854;&#28508;&#22312;&#24433;&#21709;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.15518</link><description>&lt;p&gt;
&#24403;&#24515;&#35328;&#36766;&#65306;&#35780;&#20272;&#20250;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#20016;&#23500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Beware of Words: Evaluating the Lexical Richness of Conversational Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15518
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20250;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#22914;&#20309;&#21462;&#20915;&#20110;&#27169;&#22411;&#21442;&#25968;&#26159;&#29702;&#35299;&#20854;&#28508;&#22312;&#24433;&#21709;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24456;&#22810;&#19981;&#21516;&#20219;&#21153;&#20013;&#27491;&#22312;&#35780;&#20272;&#20250;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;ChatGPT&#65292;&#20174;&#36923;&#36753;&#25512;&#29702;&#25110;&#25968;&#23398;&#21040;&#22238;&#31572;&#21508;&#31181;&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#30340;&#30740;&#31350;&#21364;&#23569;&#20043;&#21448;&#23569;&#12290;&#36825;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#22240;&#20026;LLMs&#26159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20102;&#35299;&#23427;&#20204;&#22914;&#20309;&#20351;&#29992;&#35821;&#35328;&#26159;&#37325;&#35201;&#30340;&#12290;&#20107;&#23454;&#19978;&#65292;&#20250;&#35805;&#24335;LLMs&#21487;&#33021;&#23545;&#35821;&#35328;&#30340;&#28436;&#21464;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#20204;&#26368;&#32456;&#21487;&#33021;&#20027;&#23548;&#26032;&#25991;&#26412;&#30340;&#21019;&#20316;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#23427;&#20204;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#22914;&#20309;&#21462;&#20915;&#20110;&#27169;&#22411;&#21442;&#25968;&#26159;&#20102;&#35299;&#28508;&#22312;&#24433;&#21709;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15518v1 Announce Type: new  Abstract: The performance of conversational Large Language Models (LLMs) in general, and of ChatGPT in particular, is currently being evaluated on many different tasks, from logical reasoning or maths to answering questions on a myriad of topics. Instead, much less attention is being devoted to the study of the linguistic features of the texts generated by these LLMs. This is surprising since LLMs are models for language, and understanding how they use the language is important. Indeed, conversational LLMs are poised to have a significant impact on the evolution of languages as they may eventually dominate the creation of new text. This means that for example, if conversational LLMs do not use a word it may become less and less frequent and eventually stop being used altogether. Therefore, evaluating the linguistic features of the text they produce and how those depend on the model parameters is the first step toward understanding the potential im
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23558;&#22823;&#35268;&#27169;&#22810;&#27169;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#36143;&#27969;&#30021;&#25991;&#26412;&#65292;&#39318;&#27425;&#25512;&#20986;&#20102;&#29992;&#20110;&#20307;&#32946;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;AI&#35780;&#35770;&#31995;&#32479;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.15514</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25991;&#26412;&#22312;&#20307;&#32946;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Scale Generative AI Text Applied to Sports and Music
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15514
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23558;&#22823;&#35268;&#27169;&#22810;&#27169;&#25968;&#25454;&#36716;&#21270;&#20026;&#36830;&#36143;&#27969;&#30021;&#25991;&#26412;&#65292;&#39318;&#27425;&#25512;&#20986;&#20102;&#29992;&#20110;&#20307;&#32946;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;AI&#35780;&#35770;&#31995;&#32479;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#23186;&#20307;&#20869;&#23481;&#65288;&#21253;&#25324;&#35780;&#35770;&#21644;&#20010;&#24615;&#21270;&#26032;&#38395;&#25253;&#36947;&#65289;&#25193;&#23637;&#21040;&#20840;&#29699;&#22823;&#22411;&#20307;&#32946;&#21644;&#38899;&#20048;&#27963;&#21160;&#30340;&#29983;&#20135;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23558;&#22823;&#37327;&#22810;&#27169;&#25968;&#25454;&#65288;&#20363;&#22914;&#35270;&#39057;&#12289;&#25991;&#31456;&#12289;&#23454;&#26102;&#27604;&#20998;&#12289;&#32479;&#35745;&#25968;&#25454;&#21644;&#36164;&#26009;&#65289;&#36716;&#25442;&#20026;&#36830;&#36143;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#22522;&#20110;&#36825;&#19968;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#27425;&#25512;&#20986;&#20102;&#19968;&#27454;&#20154;&#24037;&#26234;&#33021;&#35780;&#35770;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#34987;&#37096;&#32626;&#29992;&#20110;&#20026;2023&#24180;&#32654;&#22269;&#20844;&#24320;&#36187;&#12289;&#28201;&#24067;&#23572;&#30331;&#20844;&#24320;&#36187;&#21644;&#22823;&#24072;&#36187;&#30340;&#31934;&#24425;&#29255;&#27573;&#21046;&#20316;&#33258;&#21160;&#21270;&#21465;&#36848;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36824;&#34987;&#25193;&#23637;&#29992;&#20110;&#20026;ESPN&#26790;&#24187;&#27204;&#27012;&#29699;&#21644;&#26684;&#33713;&#32654;&#22870;&#38899;&#20048;&#33402;&#26415;&#23478;&#25925;&#20107;&#21019;&#36896;&#20010;&#24615;&#21270;&#20869;&#23481;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#37319;&#29992;&#20102;&#30456;&#21516;&#30340;&#36719;&#20214;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;15&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#24179;&#22343;Rouge-L&#20026;82.00&#65292;&#22256;&#24785;&#24230;&#20026;6.6&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15514v1 Announce Type: cross  Abstract: We address the problem of scaling up the production of media content, including commentary and personalized news stories, for large-scale sports and music events worldwide. Our approach relies on generative AI models to transform a large volume of multimodal data (e.g., videos, articles, real-time scoring feeds, statistics, and fact sheets) into coherent and fluent text. Based on this approach, we introduce, for the first time, an AI commentary system, which was deployed to produce automated narrations for highlight packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same vein, our solution was extended to create personalized content for ESPN Fantasy Football and stories about music artists for the Grammy awards. These applications were built using a common software architecture achieved a 15x speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our work was successfully deployed at the aforeme
&lt;/p&gt;</description></item><item><title>AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15506</link><description>&lt;p&gt;
AgentOhana&#65306;&#20026;&#26377;&#25928;&#26234;&#33021;&#20307;&#23398;&#20064;&#35774;&#35745;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15506
&lt;/p&gt;
&lt;p&gt;
AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24341;&#36215;&#20102;&#37325;&#22823;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#36827;&#34892;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20855;&#26377;&#22810;&#36718;&#36712;&#36857;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#30340;&#24322;&#26500;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;AgentOhana&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;AgentOhana&#20174;&#19981;&#21516;&#29615;&#22659;&#20013;&#32858;&#21512;&#26234;&#33021;&#20307;&#36712;&#36857;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24773;&#26223;&#12290;&#23427;&#31934;&#24515;&#22320;&#23558;&#36825;&#20123;&#36712;&#36857;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#26684;&#24335;&#20013;&#65292;&#31616;&#21270;&#20102;&#20026;&#26234;&#33021;&#20307;&#35757;&#32451;&#20248;&#21270;&#30340;&#36890;&#29992;&#25968;&#25454;&#21152;&#36733;&#22120;&#30340;&#21019;&#24314;&#12290;&#36890;&#36807;&#25968;&#25454;&#32479;&#19968;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#27700;&#32447;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#35774;&#22791;&#20043;&#38388;&#30340;&#29420;&#31435;&#38543;&#26426;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;xLAM-v0.1&#65292;&#19968;&#20010;&#22823;&#21160;&#20316;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Prejudice-Caprice Framework&#65288;PCF&#65289;&#26469;&#20840;&#38754;&#34913;&#37327;LLMs&#20013;&#30340;&#27495;&#35270;&#65292;&#32771;&#34385;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#19968;&#36143;&#20559;&#35265;&#20559;&#22909;&#21644;&#20559;&#22909;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15481</link><description>&lt;p&gt;
&#20559;&#35265;&#21644;&#21453;&#22797;&#26080;&#24120;&#65306;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31038;&#20250;&#27495;&#35270;&#30340;&#32479;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15481
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Prejudice-Caprice Framework&#65288;PCF&#65289;&#26469;&#20840;&#38754;&#34913;&#37327;LLMs&#20013;&#30340;&#27495;&#35270;&#65292;&#32771;&#34385;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#19968;&#36143;&#20559;&#35265;&#20559;&#22909;&#21644;&#20559;&#22909;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15481v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20250;&#36816;&#33829;&#20013;&#30340;&#26085;&#30410;&#34701;&#21512;&#21152;&#21095;&#20102;&#23427;&#20204;&#23545;&#32463;&#27982;&#12289;&#27861;&#24459;&#12289;&#25945;&#32946;&#21644;&#21307;&#30103;&#31561;&#37325;&#35201;&#39046;&#22495;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#24341;&#21457;&#20102;&#20844;&#20247;&#23545;&#36825;&#20123;&#27169;&#22411;&#28041;&#21450;&#27495;&#35270;&#23433;&#20840;&#21644;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#27495;&#35270;&#27979;&#37327;&#26694;&#26550;&#20165;&#35780;&#20272;LLMs&#30340;&#24179;&#22343;&#27495;&#35270;&#34892;&#20026;&#65292;&#24448;&#24448;&#30001;&#20110;&#24573;&#35270;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#23548;&#33268;&#27495;&#35270;&#30340;&#22240;&#32032;&#65292;&#21363;LLMs&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#39044;&#27979;&#21464;&#21270;&#32780;&#21464;&#24471;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prejudice-Caprice Framework&#65288;PCF&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;LLMs&#30340;&#19968;&#36143;&#20559;&#35265;&#20559;&#22909;&#21644;&#22312;&#22810;&#26679;&#19978;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15481v1 Announce Type: new  Abstract: The growing integration of large language models (LLMs) into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models' discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of LLMs, often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the LLMs' prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in LLMs by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of LLMs into prejudice risk, originating from LLMs' persistent prejudice, and caprice risk, stemmin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#24110;&#21161;&#24357;&#34917;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.15313</link><description>&lt;p&gt;
ArabianGPT&#65306;&#22522;&#20110;&#21407;&#29983;&#38463;&#25289;&#20271;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ArabianGPT: Native Arabic GPT-based Large Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15313
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#24110;&#21161;&#24357;&#34917;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#21644;&#25289;&#19969;&#35821;&#20026;&#20027;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20027;&#23548;&#22320;&#20301;&#23548;&#33268;&#20102;&#26412;&#22303;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#26174;&#33879;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;ArabianGPT&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#19987;&#38376;&#20026;&#38463;&#25289;&#20271;&#35821;&#35774;&#35745;&#32780;&#25104;&#12290;&#36825;&#20123;&#27169;&#22411;&#21253;&#25324;ArabianGPT-0.1B&#21644;ArabianGPT-0.3B&#65292;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#65292;&#19982;&#38463;&#25289;&#20271;&#35821;&#30340;&#24494;&#22937;&#35821;&#35328;&#29305;&#24449;&#30456;&#22865;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15313v1 Announce Type: cross  Abstract: The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#23454;&#20363;&#32423;&#21069;&#32512;&#22312;&#27880;&#24847;&#21147;&#31354;&#38388;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#27604;&#36739;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#33073;&#27602;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15202</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#20363;&#32423;&#21069;&#32512;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#33073;&#27602;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15202
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#23454;&#20363;&#32423;&#21069;&#32512;&#22312;&#27880;&#24847;&#21147;&#31354;&#38388;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#27604;&#36739;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#33073;&#27602;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20598;&#23572;&#20250;&#23545;&#26576;&#20123;&#25552;&#31034;&#29983;&#25104;&#27602;&#24615;&#20869;&#23481;&#65292;&#22914;&#20398;&#36785;&#12289;&#23041;&#32961;&#21644;&#31895;&#35805;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#21033;&#29992;&#21508;&#31181;&#22522;&#20110;&#24494;&#35843;&#21644;&#22522;&#20110;&#35299;&#30721;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#27602;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#22914;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#36741;&#21161;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#23454;&#20363;&#32423;&#21069;&#32512;&#36827;&#34892;&#32454;&#31890;&#24230;&#33073;&#27602;&#65288;FGDILP&#65289;&#65292;&#20197;&#20943;&#36731;&#27602;&#24615;&#25991;&#26412;&#32780;&#26080;&#38656;&#39069;&#22806;&#36153;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;FGDILP&#36890;&#36807;&#22312;&#23454;&#20363;&#32423;&#21035;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#27491;&#21069;&#32512;&#30340;&#25552;&#31034;&#26469;&#23545;&#27604;&#27880;&#24847;&#21147;&#31354;&#38388;&#20013;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#32780;&#22810;&#20010;&#24102;&#26377;&#36127;&#21069;&#32512;&#30340;&#25552;&#31034;&#12290;&#36825;&#20801;&#35768;&#26500;&#24314;&#32454;&#31890;&#24230;&#30340;&#27425;&#27602;&#24615;&#21521;&#37327;&#65292;&#20351;&#25991;&#26412;&#34987;&#35782;&#21035;&#20026;&#27425;&#27602;&#24615;&#21464;&#24471;&#26356;&#21152;&#31934;&#32454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15202v1 Announce Type: new  Abstract: Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs). However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility. To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This allows for constructing fine-grained subtoxicity vectors, whic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064; CuPUL &#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14948</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14948
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064; CuPUL &#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#36828;&#31243;&#30417;&#30563;&#65288;DS-NER&#65289;&#26694;&#26550;&#19979;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#26631;&#31614;&#36136;&#37327;&#21463;&#21040;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#22914;&#20551;&#38451;&#24615;&#12289;&#20551;&#38452;&#24615;&#21644;&#27491;&#21521;&#31867;&#22411;&#38169;&#35823;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#20102;&#24403;&#21069;DS-NER&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;QTL&#30340;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#31526;&#21512;&#39044;&#26399;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#26222;&#36941;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#65288;CuPUL&#65289;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31574;&#30053;&#24615;&#22320;&#20174;&#8220;&#26131;&#8221;&#21644;&#26356;&#28165;&#27905;&#30340;&#26679;&#26412;&#24320;&#22987;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#22024;&#26434;&#26679;&#26412;&#30340;&#38887;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#31361;&#20986;&#20102;CuPUL&#20943;&#23569;&#22024;&#26434;&#26631;&#31614;&#24433;&#21709;&#24182;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14948v1 Announce Type: new  Abstract: This paper delves into Named Entity Recognition (NER) under the framework of Distant Supervision (DS-NER), where the main challenge lies in the compromised quality of labels due to inherent errors such as false positives, false negatives, and positive type errors. We critically assess the efficacy of current DS-NER methodologies using a real-world benchmark dataset named QTL, revealing that their performance often does not meet expectations. To tackle the prevalent issue of label noise, we introduce a simple yet effective approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which strategically starts on "easy" and cleaner samples during the training process to enhance model resilience to noisy samples. Our empirical results highlight the capability of CuPUL to significantly reduce the impact of noisy labels and outperform existing methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14891</link><description>&lt;p&gt;
LLMBind: &#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMBind: A Unified Modality-Task Integration Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#38598;&#25104;&#33021;&#21147;&#26377;&#38480;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24102;&#22836;&#25506;&#32034;&#24182;&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#29992;&#20110;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;LLMBind&#21487;&#20197;&#20197;&#22810;&#31181;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#35299;&#37322;&#36755;&#20837;&#24182;&#29983;&#25104;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#21516;&#19987;&#23478;&#20043;&#38388;&#30340;&#21327;&#20316;&#23454;&#29616;&#19981;&#21516;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26377;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;40&#19975;&#26465;&#25351;&#20196;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35299;&#38145;&#20102;&#20132;&#20114;&#24335;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#23545;&#24615;&#33021;&#32780;&#38750;&#20219;&#21153;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#8220;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#8221;&#65292;&#21487;&#24110;&#21161;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#24182;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.14890</link><description>&lt;p&gt;
Vygotsky Distance: &#29992;&#20110;&#22522;&#20934;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vygotsky Distance: Measure for Benchmark Task Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14890
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#23545;&#24615;&#33021;&#32780;&#38750;&#20219;&#21153;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#8220;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#8221;&#65292;&#21487;&#24110;&#21161;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#24182;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#24037;&#20855;&#21644;&#23454;&#36341;&#31639;&#27861;&#26469;&#35745;&#31639;&#22522;&#20934;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#31216;&#20043;&#20026;"&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;"&#12290;&#36825;&#31181;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22522;&#20110;&#8220;&#23398;&#29983;&#8221;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#30340;&#30456;&#23545;&#34920;&#29616;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#20219;&#21153;&#26412;&#36523;&#30340;&#23646;&#24615;&#12290;&#22914;&#26524;&#20004;&#20010;&#20219;&#21153;&#22312;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#19978;&#24444;&#27492;&#25509;&#36817;&#65292;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978; tend to have similar relative performance&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20102;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#32500;&#26524;&#33576;&#22522;&#36317;&#31163;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35780;&#20272;&#20219;&#21153;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#39564;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14890v1 Announce Type: cross  Abstract: Evaluation plays a significant role in modern natural language processing. Most modern NLP benchmarks consist of arbitrary sets of tasks that neither guarantee any generalization potential for the model once applied outside the test set nor try to minimize the resource consumption needed for model evaluation. This paper presents a theoretical instrument and a practical algorithm to calculate similarity between benchmark tasks, we call this similarity measure "Vygotsky distance". The core idea of this similarity measure is that it is based on relative performance of the "students" on a given task, rather that on the properties of the task itself. If two tasks are close to each other in terms of Vygotsky distance the models tend to have similar relative performance on them. Thus knowing Vygotsky distance between tasks one can significantly reduce the number of evaluation tasks while maintaining a high validation quality. Experiments on v
&lt;/p&gt;</description></item><item><title>Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#20986;&#20102;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#35757;&#32451;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14873</link><description>&lt;p&gt;
Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Technical Report on the Checkfor.ai AI-Generated Text Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14873
&lt;/p&gt;
&lt;p&gt;
Checkfor.ai AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#20986;&#20102;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#35757;&#32451;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Checkfor.ai&#25991;&#26412;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#21306;&#20998;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#30340;&#25991;&#26412;&#21644;&#30001;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#12290;Checkfor.ai&#22312;&#30001;&#21313;&#31181;&#25991;&#26412;&#39046;&#22495;&#65288;&#23398;&#29983;&#20889;&#20316;&#12289;&#21019;&#24847;&#20889;&#20316;&#12289;&#31185;&#23398;&#20889;&#20316;&#12289;&#20070;&#31821;&#12289;&#30334;&#31185;&#20840;&#20070;&#12289;&#26032;&#38395;&#12289;&#30005;&#23376;&#37038;&#20214;&#12289;&#31185;&#23398;&#35770;&#25991;&#12289;&#31616;&#31572;&#38382;&#31572;&#65289;&#21644;8&#20010;&#24320;&#28304;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#34920;&#29616;&#20248;&#20110;&#38646;&#20914;&#20987;&#26041;&#27861;&#22914;DetectGPT&#20197;&#21450;&#20027;&#27969;&#21830;&#19994;AI&#26816;&#27979;&#24037;&#20855;&#65292;&#35823;&#24046;&#29575;&#38477;&#20302;&#20102;9&#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31639;&#27861;&#65292;&#21363;&#30828;&#36127;&#25366;&#25496;&#19982;&#21512;&#25104;&#38236;&#20687;&#65292;&#20351;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#35780;&#35770;&#31561;&#39640;&#25968;&#25454;&#39046;&#22495;&#23454;&#29616;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26356;&#20302;&#35823;&#25253;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Checkfor.ai&#19981;&#23545;&#38750;&#27597;&#35821;&#33521;&#35821;&#20154;&#22763;&#20135;&#29983;&#20559;&#35265;&#65292;&#24182;&#25512;&#24191;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#39046;&#22495;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14873v1 Announce Type: cross  Abstract: We present the Checkfor.ai text classifier, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Checkfor.ai outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 9 times lower error rates on a comprehensive benchmark comprised of ten text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q\&amp;A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Checkfor.ai is not biased against nonnative English speakers and generalizes to domains and models unseen during training.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Noise-BERT&#26694;&#26550;&#65292;&#21253;&#21547;&#22122;&#22768;&#23545;&#40784;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#21644;&#23545;&#25239;&#25915;&#20987;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14494</link><description>&lt;p&gt;
Noise-BERT: &#19968;&#31181;&#20855;&#26377;&#22122;&#22768;&#23545;&#40784;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#25200;&#21160;&#40065;&#26834;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22024;&#26434;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14494
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Noise-BERT&#26694;&#26550;&#65292;&#21253;&#21547;&#22122;&#22768;&#23545;&#40784;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#21644;&#23545;&#25239;&#25915;&#20987;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#27133;&#22635;&#20805;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#36755;&#20837;&#20449;&#24687;&#32463;&#24120;&#36973;&#21463;&#21508;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#25200;&#21160;&#65292;&#36825;&#24433;&#21709;&#20102;&#27133;&#22635;&#20805;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#20110;&#35268;&#21017;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#38754;&#23545;&#26410;&#30693;&#22122;&#22768;&#24178;&#25200;&#26102;&#65292;&#23427;&#20204;&#26080;&#27861;&#23637;&#29616;&#20986;&#26399;&#26395;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;Noise-BERT&#26469;&#35299;&#20915;&#27133;&#22635;&#20805;&#20013;&#36755;&#20837;&#25200;&#21160;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#22122;&#22768;&#23545;&#40784;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#25200;&#21160;&#40065;&#26834;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;&#20004;&#20010;Noise Alignment&#39044;&#35757;&#32451;&#20219;&#21153;&#65306;&#27133;&#23631;&#34109;&#39044;&#27979;&#21644;&#21477;&#23376;&#22024;&#26434;&#24230;&#21028;&#21035;&#65292;&#26088;&#22312;&#24341;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#20934;&#30830;&#30340;&#27133;&#20449;&#24687;&#21644;&#22122;&#22768;&#20998;&#24067;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#26469;&#22686;&#24378;&#23454;&#20307;&#21644;&#26631;&#31614;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25239;&#25915;&#20987;&#35757;&#32451;&#31574;&#30053;&#20197;&#25552;&#39640;&#35821;&#20041;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14494v1 Announce Type: new  Abstract: In a realistic dialogue system, the input information from users is often subject to various types of input perturbations, which affects the slot-filling task. Although rule-based data augmentation methods have achieved satisfactory results, they fail to exhibit the desired generalization when faced with unknown noise disturbances. In this study, we address the challenges posed by input perturbations in slot filling by proposing Noise-BERT, a unified Perturbation-Robust Framework with Noise Alignment Pre-training. Our framework incorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and Sentence Noisiness Discrimination, aiming to guide the pre-trained language model in capturing accurate slot information and noise distribution. During fine-tuning, we employ a contrastive learning loss to enhance the semantic representation of entities and labels. Additionally, we introduce an adversarial attack training strategy to i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27010;&#24565;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#34920;&#31034;&#31354;&#38388;&#32534;&#30721;&#20102;&#26377;&#20851;&#23545;&#35937;&#31867;&#21035;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#35813;&#20219;&#21153;&#25506;&#26597;&#30340;&#27010;&#24565;&#25512;&#29702;&#33021;&#21147;&#33021;&#22815;&#39044;&#27979;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#19968;&#33324;&#25512;&#29702;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14404</link><description>&lt;p&gt;
&#22312;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#26512;&#27010;&#24565;&#34920;&#36798;&#65306;&#20511;&#21161;&#21453;&#21521;&#35789;&#20856;&#25506;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14404
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27010;&#24565;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#34920;&#31034;&#31354;&#38388;&#32534;&#30721;&#20102;&#26377;&#20851;&#23545;&#35937;&#31867;&#21035;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#35813;&#20219;&#21153;&#25506;&#26597;&#30340;&#27010;&#24565;&#25512;&#29702;&#33021;&#21147;&#33021;&#22815;&#39044;&#27979;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#19968;&#33324;&#25512;&#29702;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#26597;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#26410;&#35299;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#26469;&#25506;&#26597;LLMs&#23545;&#27010;&#24565;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#35821;&#35328;&#25551;&#36848;&#20013;&#26263;&#31034;&#30340;&#23545;&#35937;&#27010;&#24565;&#30340;&#26415;&#35821;&#12290;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#31283;&#20581;&#22320;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#34920;&#31034;&#31354;&#38388;&#32534;&#30721;&#20102;&#20851;&#20110;&#23545;&#35937;&#31867;&#21035;&#21644;&#32454;&#31890;&#24230;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#21453;&#21521;&#35789;&#20856;&#20219;&#21153;&#25506;&#26597;&#30340;&#27010;&#24565;&#25512;&#29702;&#33021;&#21147;&#33021;&#22815;&#39044;&#27979;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#19968;&#33324;&#25512;&#29702;&#34920;&#29616;&#65292;&#23613;&#31649;&#27169;&#22411;&#22312;&#21477;&#27861;&#27867;&#21270;&#34892;&#20026;&#19978;&#34920;&#29616;&#30456;&#20284;&#12290;&#25506;&#32034;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#20351;&#29992;&#25551;&#36848;$\Rightarrow$&#21333;&#35789;&#31034;&#20363;&#21487;&#33021;&#20250;&#35825;&#23548;&#20986;&#36229;&#36234;&#20219;&#21153;&#26500;&#22411;&#34920;&#38754;&#24046;&#24322;&#30340;&#27867;&#21270;&#65292;&#24182;&#20419;&#36827;&#27169;&#22411;&#23545;&#26356;&#24191;&#27867;&#30340;&#20849;&#21516;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14404v1 Announce Type: cross  Abstract: Probing and enhancing large language models' reasoning capacity remains a crucial open question. Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference. We use in-context learning to guide the models to generate the term for an object concept implied in a linguistic description. Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features. Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model's general reasoning performance across multiple benchmarks, despite similar syntactic generalization behaviors across models. Explorative analyses suggest that prompting LLMs with description$\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader commons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20026;&#21307;&#23398;&#39046;&#22495;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;:&#26500;&#24314;&#20102;&#26032;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#35821;&#26009;&#24211;MMedC&#65292;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;MMedBench&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;MMedC&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;&#33719;&#24471;&#20102;&#24615;&#33021;&#20248;&#36234;&#30340;MMedLM 2&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.13963</link><description>&lt;p&gt;
&#20026;&#21307;&#23398;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Building Multilingual Language Model for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20026;&#21307;&#23398;&#39046;&#22495;&#26500;&#24314;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;:&#26500;&#24314;&#20102;&#26032;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#35821;&#26009;&#24211;MMedC&#65292;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;MMedBench&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;MMedC&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;&#33719;&#24471;&#20102;&#24615;&#33021;&#20248;&#36234;&#30340;MMedLM 2&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#38754;&#21521;&#21307;&#23398;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#24471;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#21463;&#20247;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#36129;&#29486;&#20307;&#29616;&#22312;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;:&#39318;&#20808;&#65292;&#38024;&#23545;&#22810;&#35821;&#35328;&#21307;&#23398;&#29305;&#23450;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#22823;&#32422;25.5B&#20010;tokens&#65292;&#35206;&#30422;&#20102;6&#31181;&#20027;&#35201;&#35821;&#35328;&#65292;&#34987;&#31216;&#20026;MMedC&#65292;&#36825;&#20351;&#24471;&#29616;&#26377;&#36890;&#29992;LLM&#33021;&#22815;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#30417;&#27979;&#21307;&#23398;&#39046;&#22495;&#22810;&#35821;&#35328;LLM&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24102;&#26377;&#35299;&#37322;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#22810;&#36873;&#38382;&#31572;&#22522;&#20934;&#65292;&#31216;&#20026;MMedBench&#65307;&#31532;&#19977;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#37027;&#20123;&#22312;MMedC&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;MMedLM 2&#65292;&#20165;&#26377;7B&#21442;&#25968;&#65292;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13963v1 Announce Type: new  Abstract: In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs. second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance c
&lt;/p&gt;</description></item><item><title>Kuaiji&#26159;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;Baichuan&#26694;&#26550;&#31934;&#24515;&#35843;&#25972;&#65292;&#25903;&#25345;&#30340;CAtAcctQA&#25968;&#25454;&#38598;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#36895;&#24230;&#65292;&#20855;&#26377;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#20013;&#22269;&#20250;&#35745;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#23454;&#20102;&#22312;&#30495;&#23454;&#20250;&#35745;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13866</link><description>&lt;p&gt;
Kuaiji&#65306;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Kuaiji: the First Chinese Accounting Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13866
&lt;/p&gt;
&lt;p&gt;
Kuaiji&#26159;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;Baichuan&#26694;&#26550;&#31934;&#24515;&#35843;&#25972;&#65292;&#25903;&#25345;&#30340;CAtAcctQA&#25968;&#25454;&#38598;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#36895;&#24230;&#65292;&#20855;&#26377;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#20013;&#22269;&#20250;&#35745;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#23454;&#20102;&#22312;&#30495;&#23454;&#20250;&#35745;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21644;GPT-4&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#20219;&#21153;&#35201;&#27714;&#36866;&#24212;&#20250;&#35745;&#31561;&#19987;&#19994;&#39046;&#22495;&#26102;&#65292;&#23427;&#20204;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Kuaiji&#65292;&#19968;&#20010;&#19987;&#38376;&#23450;&#21046;&#30340;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;Kuaiji&#32463;&#36807;&#31934;&#24515;&#35843;&#25972;&#65292;&#20351;&#29992;&#21253;&#21547;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#30340;Baichuan&#26694;&#26550;&#12290;&#22312;CAtAcctQA&#30340;&#25903;&#25345;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#30495;&#23454;&#20250;&#35745;&#24072;&#19982;&#23458;&#25143;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;Kuaiji&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#25968;&#25454;&#38598;&#65292;&#23558;Kuaiji&#24314;&#31435;&#20026;&#19968;&#31181;&#39046;&#20808;&#30340;&#24320;&#28304;&#20013;&#22269;&#20250;&#35745;LLM&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#20250;&#35745;&#22330;&#26223;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting. To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed. Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;$\infty$Bench&#65292;&#31532;&#19968;&#20010;&#20197;&#24179;&#22343;&#25968;&#25454;&#38271;&#24230;&#36229;&#36807;10&#19975;&#20010;&#20196;&#29260;&#30340;LLM&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.13718</link><description>&lt;p&gt;
$\infty$Bench: &#23558;&#38271;&#19978;&#19979;&#25991;&#35780;&#20272;&#25193;&#23637;&#33267;&#36229;&#36807;10&#19975;&#20196;&#29260;
&lt;/p&gt;
&lt;p&gt;
$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13718
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;$\infty$Bench&#65292;&#31532;&#19968;&#20010;&#20197;&#24179;&#22343;&#25968;&#25454;&#38271;&#24230;&#36229;&#36807;10&#19975;&#20010;&#20196;&#29260;&#30340;LLM&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#21644;&#25512;&#29702;&#38271;&#19978;&#19979;&#25991;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#25991;&#26723;&#29702;&#35299;&#21644;&#20195;&#29702;&#26500;&#24314;&#12290;&#26412;&#25991;&#25552;&#20986;$\infty$Bench&#65292;&#31532;&#19968;&#20010;LLM&#22522;&#20934;&#65292;&#24179;&#22343;&#25968;&#25454;&#38271;&#24230;&#36229;&#36807;10&#19975;&#20010;&#20196;&#29260;&#12290;$\infty$Bench&#21253;&#21547;&#28085;&#30422;&#19981;&#21516;&#39046;&#22495;&#30340;&#21512;&#25104;&#21644;&#29616;&#23454;&#20219;&#21153;&#65292;&#20197;&#33521;&#25991;&#21644;&#20013;&#25991;&#21576;&#29616;&#12290;$\infty$Bench&#20013;&#30340;&#20219;&#21153;&#26088;&#22312;&#38656;&#35201;&#28145;&#21051;&#29702;&#35299;&#19978;&#19979;&#25991;&#20013;&#30340;&#38271;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#31616;&#21333;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#26377;&#38480;&#25968;&#37327;&#30340;&#27573;&#33853;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13718v1 Announce Type: new  Abstract: Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457; GumbelSoft &#27700;&#21360;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#39640;&#22810;&#26679;&#24615;&#29615;&#22659;&#20013;&#22686;&#24378;&#29983;&#25104;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#26696;&#34920;&#29616;&#26356;&#20026;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2402.12948</link><description>&lt;p&gt;
GumbelSoft: &#36890;&#36807;GumbelMax&#25216;&#24039;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#25968;&#23383;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12948
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457; GumbelSoft &#27700;&#21360;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#39640;&#22810;&#26679;&#24615;&#29615;&#22659;&#20013;&#22686;&#24378;&#29983;&#25104;&#25991;&#26412;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#26696;&#34920;&#29616;&#26356;&#20026;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large language models, LLMs)&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#23398;&#26415;&#19981;&#35802;&#23454;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#35299;&#30721;&#20026;&#22522;&#30784;&#30340;&#27700;&#21360;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;GumbelMax&#25216;&#24039;&#30340;&#27700;&#21360;(GM&#27700;&#21360;)&#65292;&#26159;&#38450;&#33539;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#28389;&#29992;&#30340;&#26480;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20854;&#26174;&#33879;&#30340;&#21487;&#26816;&#27979;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;GM&#27700;&#21360;&#22312;&#29983;&#25104;&#22810;&#26679;&#24615;&#26041;&#38754;&#38754;&#20020;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#23545;&#20110;&#30456;&#21516;&#25552;&#31034;&#22987;&#32456;&#20135;&#29983;&#30456;&#21516;&#36755;&#20986;&#65292;&#20174;&#32780;&#36127;&#38754;&#24433;&#21709;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;GM&#27700;&#21360;&#65292;&#21363;Logits-Addition&#27700;&#21360;&#65292;&#21450;&#20854;&#19977;&#20010;&#21464;&#20307;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#20123;&#21464;&#20307;&#20013;&#65292;GumbelSoft&#27700;&#21360;(&#20316;&#20026;Logits-Addition&#27700;&#21360;&#30340;&#19968;&#20010;softmax&#21464;&#20307;)&#22312;&#39640;&#22810;&#26679;&#24615;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20854;AUROC&#20998;&#25968;&#36229;&#36807;&#20854;&#20182;&#20004;&#20010;&#26367;&#20195;&#21464;&#20307;0.1&#33267;0.3&#65292;&#24182;&#36229;&#36234;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12948v1 Announce Type: new  Abstract: Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we propose a new type of GM watermark, the Logits-Addition watermark, and its three variants, specifically designed to enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of the Logits-Addition watermark) demonstrates superior performance in high diversity settings, with its AUROC score outperforming those of the two alternative variants by 0.1 to 0.3 and surpassing oth
&lt;/p&gt;</description></item><item><title>Archer&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21452;&#35821;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#20551;&#35774;&#25512;&#29702;&#30340;&#22797;&#26434;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12554</link><description>&lt;p&gt;
Archer: &#19968;&#20010;&#20855;&#26377;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#20551;&#35774;&#25512;&#29702;&#30340;&#20154;&#24037;&#26631;&#35760;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and Hypothetical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12554
&lt;/p&gt;
&lt;p&gt;
Archer&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21452;&#35821;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#20551;&#35774;&#25512;&#29702;&#30340;&#22797;&#26434;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Archer&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21452;&#35821;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#65292;&#19987;&#27880;&#20110;&#21253;&#25324;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#20551;&#35774;&#25512;&#29702;&#22312;&#20869;&#30340;&#22797;&#26434;&#25512;&#29702;&#12290;&#23427;&#21253;&#21547;1,042&#20010;&#33521;&#25991;&#38382;&#39064;&#21644;1,042&#20010;&#20013;&#25991;&#38382;&#39064;&#65292;&#20197;&#21450;521&#20010;&#21807;&#19968;&#30340;SQL&#26597;&#35810;&#65292;&#28085;&#30422;&#20102;20&#20010;&#39046;&#22495;&#20013;&#30340;20&#20010;&#33521;&#35821;&#25968;&#25454;&#24211;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#29616;&#26377;&#20844;&#24320;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Archer&#25361;&#25112;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;Spider&#25490;&#34892;&#27036;&#19978;&#30340;&#25490;&#21517;&#38752;&#21069;&#30340;&#27169;&#22411;&#22312;Archer&#27979;&#35797;&#38598;&#19978;&#20165;&#36798;&#21040;6.73%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#22240;&#27492;&#65292;Archer&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12554v1 Announce Type: new  Abstract: We present Archer, a challenging bilingual text-to-SQL dataset specific to complex reasoning, including arithmetic, commonsense and hypothetical reasoning. It contains 1,042 English questions and 1,042 Chinese questions, along with 521 unique SQL queries, covering 20 English databases across 20 domains. Notably, this dataset demonstrates a significantly higher level of complexity compared to existing publicly available datasets. Our evaluation shows that Archer challenges the capabilities of current state-of-the-art models, with a high-ranked model on the Spider leaderboard achieving only 6.73% execution accuracy on Archer test set. Thus, Archer presents a significant challenge for future research in this field.
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12177</link><description>&lt;p&gt;
Mafin: &#29992;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#26469;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12177
&lt;/p&gt;
&lt;p&gt;
Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;RAG&#20013;&#30340;&#26816;&#32034;&#38454;&#27573;&#36890;&#24120;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#36716;&#25442;&#20026;&#21521;&#37327;&#20197;&#25429;&#33719;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#26102;&#65292;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20165;&#33021;&#20174;&#40657;&#30418;&#27169;&#22411;&#33719;&#21462;&#23884;&#20837;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#65288;Mafin&#65289;--&#19968;&#31181;&#36890;&#36807;&#29992;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Mafin&#20165;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22686;&#24378;&#27169;&#22411;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40657;&#30418;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
&lt;/p&gt;</description></item><item><title>PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#20026;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#31283;&#20581;&#38450;&#24481;</title><link>https://arxiv.org/abs/2402.12168</link><description>&lt;p&gt;
&#38024;&#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12168
&lt;/p&gt;
&lt;p&gt;
PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#20026;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#31283;&#20581;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#25552;&#20986;&#24182;&#25104;&#21151;&#23454;&#26045;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#38754;&#23545;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#26102;&#65292;&#20165;&#26356;&#26032;&#26377;&#38480;&#27169;&#22411;&#21442;&#25968;&#30340;PEFT&#26159;&#21542;&#26500;&#25104;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#20173;&#28982;&#26131;&#21463;&#21033;&#29992;&#65292;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#22312;&#24494;&#35843;&#21518;&#20381;&#28982;&#20445;&#25345;&#39640;&#32622;&#20449;&#24230;&#12290;&#21463;&#21040;&#36825;&#19968;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;PEFT&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#65292;&#25552;&#20379;&#38024;&#23545;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31283;&#20581;&#38450;&#24481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;PEFT&#35757;&#32451;PSIM&#65292;&#24102;&#26377;&#38543;&#26426;&#37325;&#32622;&#26679;&#26412;&#26631;&#31614;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12168v1 Announce Type: cross  Abstract: Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference pr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25490;&#21517;&#20851;&#38190;&#35789;&#24182;&#25351;&#23548;&#23631;&#34109;&#36807;&#31243;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20307;&#35009;&#21644;&#20027;&#39064;&#20449;&#24687;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#19987;&#19994;&#39046;&#22495;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12036</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20307;&#35009;&#21644;&#20027;&#39064;&#29305;&#24449;&#30340;&#36873;&#25321;&#24615;&#23631;&#34109;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#19987;&#19994;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12036
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25490;&#21517;&#20851;&#38190;&#35789;&#24182;&#25351;&#23548;&#23631;&#34109;&#36807;&#31243;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20307;&#35009;&#21644;&#20027;&#39064;&#20449;&#24687;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#19987;&#19994;&#39046;&#22495;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#24314;&#27169;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#30340;&#35789;&#23631;&#34109;&#26500;&#25104;&#20102;&#20687;BERT&#36825;&#26679;&#30340;&#26550;&#26500;&#20013;&#35821;&#35328;&#24314;&#27169;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#35789;&#23631;&#34109;&#26041;&#27861;&#20381;&#36182;&#20110;&#38543;&#26426;&#36873;&#25321;&#65292;&#21487;&#33021;&#24573;&#35270;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23631;&#34109;&#26041;&#27861;&#65292;&#21033;&#29992;&#20307;&#35009;&#21644;&#20027;&#39064;&#20449;&#24687;&#26469;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#20197;&#36866;&#24212;&#19987;&#19994;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#25490;&#21517;&#36807;&#31243;&#65292;&#26681;&#25454;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#23545;&#20854;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#38543;&#21518;&#24341;&#23548;&#23631;&#34109;&#36807;&#31243;&#12290;&#25105;&#20204;&#36827;&#34892;&#30340;&#23454;&#39564;&#20351;&#29992;&#27861;&#24459;&#39046;&#22495;&#20869;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33521;&#35821;LegalGLUE&#22522;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20195;&#30721;&#21487;&#20813;&#36153;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12036v1 Announce Type: new  Abstract: Recent advances in pre-trained language modeling have facilitated significant progress across various natural language processing (NLP) tasks. Word masking during model training constitutes a pivotal component of language modeling in architectures like BERT. However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes. In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains. Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure. Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE benchmark in the English language. Pre-trained language models and code are freely available for use.
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#35299;&#37322;&#25552;&#20986;&#20102;&#20840;&#38754;&#12289;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#24378;&#20581;&#24615;&#21644;&#25928;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.11863</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Interpretable are Reasoning Explanations from Prompting Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11863
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#35299;&#37322;&#25552;&#20986;&#20102;&#20840;&#38754;&#12289;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#24378;&#20581;&#24615;&#21644;&#25928;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Engineering&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;Chain-of-Thought&#31561;&#25216;&#26415;&#19981;&#20165;&#22686;&#24378;&#20102;&#20219;&#21153;&#24615;&#33021;&#65292;&#36824;&#25551;&#32472;&#20102;&#28165;&#26224;&#30340;&#25512;&#29702;&#27493;&#39588;&#36712;&#36857;&#65292;&#20026;&#35266;&#20247;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24418;&#30340;&#35299;&#37322;&#24418;&#24335;&#12290;&#25105;&#20204;&#23545;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#65292;&#19981;&#20165;&#32771;&#34385;&#20102;&#24544;&#23454;&#24230;&#65292;&#36824;&#32771;&#34385;&#20102;&#22312;&#22810;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24378;&#20581;&#24615;&#21644;&#25928;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11863v1 Announce Type: new  Abstract: Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability ali
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;LLM&#24494;&#35843;&#20013;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#25193;&#23637;&#20102;&#23545;&#19981;&#21516;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#30340;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.11592</link><description>&lt;p&gt;
&#37325;&#26032;&#25506;&#35752;&#38646;&#38454;&#20248;&#21270;&#22312;&#20869;&#23384;&#39640;&#25928;LLM&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#20010;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38477;&#20302;LLM&#24494;&#35843;&#20013;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#25193;&#23637;&#20102;&#23545;&#19981;&#21516;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#20351;&#29992;SGD&#21644;Adam&#31561;&#19968;&#38454;&#65288;FO&#65289;&#20248;&#21270;&#22120;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#20307;&#31215;&#30340;&#22686;&#38271;&#65292;&#30001;&#20110;FO&#26799;&#24230;&#35745;&#31639;&#30340;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#24102;&#26469;&#30340;&#24040;&#22823;&#20869;&#23384;&#24320;&#38144;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#20869;&#23384;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#35774;&#22791;&#31471;&#35757;&#32451;&#31561;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#21521;&#19981;&#20351;&#29992;BP&#30340;&#38646;&#38454;&#65288;ZO&#65289;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;LLM&#24494;&#35843;&#36807;&#31243;&#20013;&#38477;&#20302;&#20869;&#23384;&#25104;&#26412;&#65292;&#26500;&#24314;&#22312;MeZO&#25552;&#20986;&#30340;&#27010;&#24565;&#22522;&#30784;&#19978;&#12290;&#19982;&#20256;&#32479;&#30340;ZO&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#25506;&#32034;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;ZO&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#12289;&#39318;&#27425;&#25512;&#20986;&#30340;&#22522;&#20934;&#30740;&#31350;&#36328;&#36234;&#20116;&#20010;LLM&#31995;&#21015;&#65288;Roberta&#65292;OPT&#65292;LLaMA&#65292;Vicuna&#65292;Mistral&#65289;&#65292;&#19977;&#31181;&#20219;&#21153;&#22797;&#26434;&#24615;&#21644;&#20116;&#31181;&#24494;&#35843;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11592v1 Announce Type: new  Abstract: In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#26816;&#27979;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#21644;&#34920;&#36798;&#20449;&#24515;&#26102;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#23384;&#22312;&#20004;&#20010;&#26497;&#31471;&#65306;&#23545;&#21487;&#33021;&#24341;&#36215;&#20844;&#24179;&#38382;&#39064;&#30340;&#32676;&#20307;&#25110;&#35805;&#39064;&#34920;&#29616;&#20986;&#36807;&#20110;&#25935;&#24863;&#65292;&#21516;&#26102;&#32622;&#20449;&#24230;&#35780;&#20998;&#36807;&#24230;&#38598;&#20013;&#22312;&#19968;&#20010;&#33539;&#22260;&#20869;&#12290;</title><link>https://arxiv.org/abs/2402.11406</link><description>&lt;p&gt;
&#19981;&#35201;&#36208;&#21521;&#26497;&#31471;&#65306;&#25581;&#31034;LLMs&#22312;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#36807;&#24230;&#25935;&#24863;&#24615;&#21644;&#26657;&#20934;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#26816;&#27979;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#21644;&#34920;&#36798;&#20449;&#24515;&#26102;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#23384;&#22312;&#20004;&#20010;&#26497;&#31471;&#65306;&#23545;&#21487;&#33021;&#24341;&#36215;&#20844;&#24179;&#38382;&#39064;&#30340;&#32676;&#20307;&#25110;&#35805;&#39064;&#34920;&#29616;&#20986;&#36807;&#20110;&#25935;&#24863;&#65292;&#21516;&#26102;&#32622;&#20449;&#24230;&#35780;&#20998;&#36807;&#24230;&#38598;&#20013;&#22312;&#19968;&#20010;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20844;&#24179;&#24615;&#21644;&#21487;&#20449;&#24230;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#65292;&#21033;&#29992;&#38388;&#25509;&#35821;&#35328;&#20256;&#36798;&#20167;&#24680;&#24847;&#22270;&#65292;&#21344;&#25454;&#23454;&#36341;&#20013;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;LLMs&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#23457;&#26597;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#26816;&#27979;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#65288;&#20998;&#31867;&#20219;&#21153;&#65289;&#20197;&#21450;&#23545;&#20854;&#21709;&#24212;&#30340;&#20449;&#24515;&#36827;&#34892;&#34920;&#36798;&#65288;&#26657;&#20934;&#20219;&#21153;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32454;&#33268;&#32771;&#34385;&#20102;&#21508;&#31181;&#25552;&#31034;&#27169;&#24335;&#21644;&#20027;&#27969;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;LLMs&#23637;&#31034;&#20102;&#20004;&#20010;&#26497;&#31471;&#65306;&#65288;1&#65289;LLMs&#23545;&#21487;&#33021;&#23548;&#33268;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#32676;&#20307;&#25110;&#35805;&#39064;&#26174;&#31034;&#20986;&#36807;&#24230;&#30340;&#25935;&#24863;&#24615;&#65292;&#23548;&#33268;&#23558;&#33391;&#24615;&#38472;&#36848;&#38169;&#35823;&#20998;&#31867;&#20026;&#20167;&#24680;&#35328;&#35770;&#12290; &#65288;2&#65289;LLMs&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#36807;&#24230;&#38598;&#20013;&#22312;&#19968;&#20010;&#22266;&#23450;&#33539;&#22260;&#19978;&#65292;&#26080;&#35770;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#22914;&#20309;&#20063;&#20445;&#25345;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11406v1 Announce Type: new  Abstract: The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complex
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#27867;&#21270;&#25928;&#26524;&#21462;&#20915;&#20110;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#21644;&#20154;&#32676;&#20013;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#26679;&#26412;&#36739;&#23569;&#30340;&#21307;&#38498;&#21644;&#29305;&#23450;&#20154;&#32676;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10965</link><description>&lt;p&gt;
&#21307;&#30103;AI&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65306;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10965
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#27867;&#21270;&#25928;&#26524;&#21462;&#20915;&#20110;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#21644;&#20154;&#32676;&#20013;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#26679;&#26412;&#36739;&#23569;&#30340;&#21307;&#38498;&#21644;&#29305;&#23450;&#20154;&#32676;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20026;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#26426;&#36935;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#12289;&#20020;&#24202;&#20915;&#31574;&#20197;&#21450;&#25552;&#21319;&#21307;&#24072;&#21644;&#31649;&#29702;&#20154;&#21592;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#21147;&#37325;&#35201;&#21462;&#20915;&#20110;&#23427;&#20204;&#22312;&#20020;&#24202;&#29615;&#22659;&#21644;&#20154;&#32676;&#20013;&#26377;&#25928;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#22312;&#26089;&#26399;&#24320;&#21457;&#20013;&#32463;&#24120;&#34987;&#20302;&#20272;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#25361;&#25112;&#30340;&#21407;&#22240;&#24182;&#21046;&#23450;&#32531;&#35299;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ClinicLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312; [HOSPITAL] &#30340;&#20020;&#24202;&#31508;&#35760;&#19978;&#35757;&#32451;&#30340;LLM&#27169;&#22411;&#65292;&#23545;&#20854;&#22312;30&#22825;&#20840;&#22240;&#32032;&#20877;&#20837;&#38498;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20998;&#26512;&#65292;&#20851;&#27880;&#36328;&#21307;&#38498;&#21644;&#24739;&#32773;&#29305;&#24449;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26679;&#26412;&#36739;&#23569;&#30340;&#21307;&#38498;&#12289;&#25919;&#24220;&#21644;&#26410;&#25351;&#23450;&#20445;&#38505;&#30340;&#24739;&#32773;&#12289;&#32769;&#24180;&#20154;&#20197;&#21450;&#39640;&#20849;&#30149;&#24615;&#24739;&#32773;&#20013;&#65292;&#27867;&#21270;&#25928;&#26524;&#36739;&#24046;&#12290;&#20026;&#20102;&#20102;&#35299;&#27867;&#21270;&#19981;&#24432;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26679;&#26412;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10965v1 Announce Type: new  Abstract: Advances in large language models (LLMs) provide new opportunities in healthcare for improved patient care, clinical decision-making, and enhancement of physician and administrator workflows. However, the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development. To better understand reasons for these challenges and inform mitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s clinical notes, analyzing its performance on 30-day all-cause readmission prediction focusing on variability across hospitals and patient characteristics. We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities. To understand reasons for lack of generalization, we investigated sample sizes 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Llama-2&#31995;&#21015;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#33521;&#35821;&#20316;&#20026;&#20869;&#37096;&#26530;&#32445;&#35821;&#35328;&#30340;&#29616;&#35937;&#65292;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#26041;&#24335;&#20197;&#21450;&#35821;&#35328;&#20559;&#35265;&#30340;&#36215;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.10588</link><description>&lt;p&gt;
&#25289;&#39532;&#22312;&#33521;&#35821;&#20013;&#26377;&#25928;&#21527;&#65311;&#20851;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#28508;&#22312;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Do Llamas Work in English? On the Latent Language of Multilingual Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Llama-2&#31995;&#21015;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#33521;&#35821;&#20316;&#20026;&#20869;&#37096;&#26530;&#32445;&#35821;&#35328;&#30340;&#29616;&#35937;&#65292;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#26041;&#24335;&#20197;&#21450;&#35821;&#35328;&#20559;&#35265;&#30340;&#36215;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#22312;&#19981;&#24179;&#34913;&#12289;&#33521;&#35821;&#20027;&#23548;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#33521;&#35821;&#20316;&#20026;&#20869;&#37096;&#26530;&#32445;&#35821;&#35328;&#30340;&#38382;&#39064;&#8212;&#8212;&#36825;&#23545;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#26041;&#24335;&#20197;&#21450;&#35821;&#35328;&#20559;&#35265;&#30340;&#36215;&#28304;&#33267;&#20851;&#37325;&#35201;&#12290; &#25105;&#20204;&#20851;&#27880;Llama-2&#31995;&#21015;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31934;&#24515;&#26500;&#24314;&#30340;&#38750;&#33521;&#35821;&#25552;&#31034;&#21644;&#21807;&#19968;&#27491;&#30830;&#30340;&#21333;&#35789;&#24310;&#32493;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290; &#20174;&#19968;&#23618;&#21040;&#21478;&#19968;&#23618;&#65292;&#21464;&#21387;&#22120;&#36880;&#28176;&#23558;&#26368;&#32456;&#25552;&#31034;&#20196;&#29260;&#30340;&#36755;&#20837;&#23884;&#20837;&#26144;&#23556;&#21040;&#36755;&#20986;&#23884;&#20837;&#65292;&#20174;&#20013;&#35745;&#31639;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#27010;&#29575;&#12290; &#36890;&#36807;&#36319;&#36394;&#20854;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20013;&#38388;&#23884;&#20837;&#65292;&#25581;&#31034;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#21363;&#20013;&#38388;&#23884;&#20837;&#65288;1&#65289;&#24320;&#22987;&#36828;&#31163;&#36755;&#20986;&#20196;&#29260;&#23884;&#20837;&#65307;&#65288;2&#65289;&#22312;&#20013;&#38388;&#23618;&#24050;&#32463;&#20801;&#35768;&#35299;&#30721;&#19968;&#20010;&#35821;&#20041;&#27491;&#30830;&#30340;&#19979;&#19968;&#20010;&#20196;&#29260;&#65292;&#20294;&#26356;&#20542;&#21521;&#20110;&#33521;&#35821;&#29256;&#26412;&#32780;&#19981;&#26159;&#36755;&#20837;&#35821;&#35328;&#30340;&#29256;&#26412;&#65307;&#65288;3&#65289;&#26368;&#32456;&#31227;&#21160;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10588v1 Announce Type: new  Abstract: We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language -- a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.08983</link><description>&lt;p&gt;
SafeDecoding: &#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#21161;&#31561;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#20026;&#20102;&#20351;LLM&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#21253;&#25324;&#23433;&#20840;&#24615;&#22312;&#20869;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#36234;&#29425;&#25915;&#20987;&#26088;&#22312;&#24341;&#21457;LLM&#30340;&#38750;&#39044;&#26399;&#21644;&#19981;&#23433;&#20840;&#34892;&#20026;&#65292;&#20173;&#28982;&#26159;LLM&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#23041;&#32961;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;SafeDecoding&#26469;&#38450;&#24481;LLM&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#23433;&#20840;&#24863;&#30693;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#24320;&#21457;SafeDecoding&#26102;&#30340;&#27934;&#23519;&#21147;&#22522;&#20110;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#20195;&#34920;&#26377;&#23475;&#20869;&#23481;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#36229;&#36807;&#20195;&#34920;&#26080;&#23475;&#21709;&#24212;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#20173;&#28982;&#20986;&#29616;&#22312;&#25353;&#27010;&#29575;&#38477;&#24207;&#25490;&#24207;&#30340;&#26631;&#35760;&#20013;&#30340;&#21069;&#20960;&#20010;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#35782;&#21035;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#24182;&#22686;&#24378;&#20854;&#33391;&#24615;&#24433;&#21709;&#21147;&#26469;&#20943;&#36731;&#36234;&#29425;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08983v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#37319;&#29992;&#34164;&#28085;&#23545;&#40784;&#65292;&#20197;&#20248;&#21270;&#21487;&#34892;&#24615;&#65292;&#25552;&#21462;&#26377;&#29702;&#30340;&#26041;&#24335;&#25552;&#20379;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.08479</link><description>&lt;p&gt;
&#21487;&#20449;&#30340;&#21462;&#26679;&#21512;&#29702;&#21270;&#36890;&#36807;&#21322;&#30417;&#30563;&#30340;&#34164;&#28085;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Plausible Extractive Rationalization through Semi-Supervised Entailment Signal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#37319;&#29992;&#34164;&#28085;&#23545;&#40784;&#65292;&#20197;&#20248;&#21270;&#21487;&#34892;&#24615;&#65292;&#25552;&#21462;&#26377;&#29702;&#30340;&#26041;&#24335;&#25552;&#20379;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#21644;&#19981;&#36879;&#26126;&#30340;&#40657;&#30418;&#23376;&#27169;&#22411;&#30340;&#22686;&#21152;&#38656;&#35201;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#25514;&#26045;&#65292;&#20854;&#20013;&#19968;&#31181;&#36873;&#25321;&#26159;&#25552;&#21462;&#26377;&#29702;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#20316;&#20026;&#26356;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#36825;&#20123;&#27169;&#22411;&#65292;&#20063;&#31216;&#20026;&#20808;&#35299;&#37322;&#28982;&#21518;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#35299;&#37322;&#27169;&#22411;&#26469;&#25552;&#21462;&#26377;&#29702;&#65292;&#28982;&#21518;&#20351;&#29992;&#25552;&#21462;&#30340;&#20449;&#24687;&#26469;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20379;&#31934;&#30830;&#21644;&#24544;&#23454;&#30340;&#35299;&#37322;&#65292;&#30001;&#25552;&#21462;&#30340;&#26377;&#29702;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21322;&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#25552;&#21462;&#26377;&#29702;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#20010;&#23567;&#22411;&#30340;&#26377;&#30417;&#30563;&#26377;&#29702;&#38598;&#65288;10%&#65289;&#19978;&#36827;&#19968;&#27493;&#24494;&#35843;&#23427;&#12290;&#36890;&#36807;&#34164;&#28085;&#23545;&#40784;&#65292;NLI&#39044;&#27979;&#27169;&#22411;&#34987;&#21033;&#29992;&#20316;&#20026;&#35299;&#37322;&#27169;&#22411;&#30340;&#19968;&#31181;&#30417;&#30563;&#20449;&#21495;&#28304;&#12290;&#36890;&#36807;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#24378;&#21046;&#35299;&#37322;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#23545;&#40784;&#19968;&#33268;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of complex and opaque black box models requires the adoption of interpretable measures, one such option is extractive rationalizing models, which serve as a more interpretable alternative. These models, also known as Explain-Then-Predict models, employ an explainer model to extract rationales and subsequently condition the predictor with the extracted information. Their primary objective is to provide precise and faithful explanations, represented by the extracted rationales. In this paper, we take a semi-supervised approach to optimize for the plausibility of extracted rationales. We adopt a pre-trained natural language inference (NLI) model and further fine-tune it on a small set of supervised rationales ($10\%$). The NLI predictor is leveraged as a source of supervisory signals to the explainer via entailment alignment. We show that, by enforcing the alignment agreement between the explanation and answer in a question-answering task, the performance can be improve
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#33889;&#33796;&#29273;&#12289;&#24847;&#22823;&#21033;&#21644;&#24503;&#22269;&#31461;&#35805;&#20013;&#26126;&#30830;&#34920;&#36798;&#30340;&#20215;&#20540;&#35266;&#24046;&#24322;&#65292;&#20351;&#29992;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#32599;&#30424;&#37327;&#21270;&#20998;&#26512;&#12290;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#36825;&#20123;&#22269;&#23478;&#20043;&#38388;&#23384;&#22312;&#20849;&#20139;&#30340;&#25991;&#21270;&#29702;&#35299;&#21644;&#23545;&#21892;&#33391;&#12289;&#36981;&#20174;&#21644;&#26222;&#36941;&#20215;&#20540;&#35266;&#30340;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2402.08318</link><description>&lt;p&gt;
&#12298;&#31461;&#35805;&#20013;&#26126;&#30830;&#34920;&#36798;&#30340;&#31038;&#20250;&#20215;&#20540;&#35266;&#65306;&#19977;&#31181;&#27431;&#27954;&#25991;&#21270;&#30340;&#27604;&#36739;&#12299;
&lt;/p&gt;
&lt;p&gt;
Explicit References to Social Values in Fairy Tales: A Comparison between Three European Cultures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08318
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#33889;&#33796;&#29273;&#12289;&#24847;&#22823;&#21033;&#21644;&#24503;&#22269;&#31461;&#35805;&#20013;&#26126;&#30830;&#34920;&#36798;&#30340;&#20215;&#20540;&#35266;&#24046;&#24322;&#65292;&#20351;&#29992;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#32599;&#30424;&#37327;&#21270;&#20998;&#26512;&#12290;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#36825;&#20123;&#22269;&#23478;&#20043;&#38388;&#23384;&#22312;&#20849;&#20139;&#30340;&#25991;&#21270;&#29702;&#35299;&#21644;&#23545;&#21892;&#33391;&#12289;&#36981;&#20174;&#21644;&#26222;&#36941;&#20215;&#20540;&#35266;&#30340;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#31461;&#35805;&#20013;&#30340;&#31038;&#20250;&#20215;&#20540;&#35266;&#21487;&#20197;&#20102;&#35299;&#20215;&#20540;&#35266;&#22312;&#26102;&#31354;&#20013;&#30340;&#20256;&#36882;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#32599;&#30424;&#26469;&#37327;&#21270;&#33889;&#33796;&#29273;&#12289;&#24847;&#22823;&#21033;&#21644;&#24503;&#22269;&#31461;&#35805;&#20013;&#30340;&#20215;&#20540;&#35266;&#20256;&#36882;&#12290;&#25105;&#20204;&#30740;&#31350;&#36825;&#19977;&#31181;&#22269;&#23478;&#30340;&#31461;&#35805;&#22312;&#26126;&#30830;&#34920;&#36798;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25351;&#23450;&#20102;&#19968;&#20010;&#20805;&#28385;&#20215;&#20540;&#35266;&#30340;&#35789;&#27719;&#21015;&#34920;&#65292;&#32771;&#34385;&#23427;&#20204;&#30340;&#35789;&#24178;&#65292;&#24182;&#20998;&#26512;&#22312;&#19987;&#38376;&#39044;&#35757;&#32451;&#30340;Word2Vec&#27169;&#22411;&#20013;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#35282;&#24230;&#39564;&#35777;&#21644;&#25209;&#21028;&#24615;&#35752;&#35770;&#37327;&#21270;&#27169;&#22411;&#25152;&#25552;&#20986;&#30340;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#22797;&#29992;&#21644;&#21487;&#37325;&#29616;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#21382;&#21490;&#35821;&#26009;&#24211;&#20013;&#26126;&#30830;&#24341;&#29992;&#30340;&#20215;&#20540;&#35266;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#26263;&#31034;&#26377;&#30528;&#20849;&#20139;&#25991;&#21270;&#29702;&#35299;&#21644;&#23545;&#21892;&#33391;&#12289;&#36981;&#20174;&#21644;&#26222;&#36941;&#20215;&#20540;&#35266;&#30340;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of social values in fairy tales opens the possibility to learn about the communication of values across space and time. We propose to study the communication of values in fairy tales from Portugal, Italy and Germany using a technique called word embedding with a compass to quantify vocabulary differences and commonalities. We study how these three national traditions of fairy tales differ in their explicit references to values. To do this, we specify a list of value-charged tokens, consider their word stems and analyse the distance between these in a bespoke pre-trained Word2Vec model. We triangulate and critically discuss the validity of the resulting hypotheses emerging from this quantitative model. Our claim is that this is a reusable and reproducible method for the study of the values explicitly referenced in historical corpora. Finally, our preliminary findings hint at a shared cultural understanding and the expression of values such as Benevolence, Conformity, and Unive
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08277</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#21644;&#24378;&#22823;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#19987;&#23478;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#24544;&#23454;&#21644;&#21487;&#36861;&#36394;&#30340;&#31572;&#26696;&#30340;&#36827;&#27493;&#23545;&#20110;&#21508;&#31181;&#30740;&#31350;&#21644;&#23454;&#36341;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#21487;&#38752;&#30340;&#26469;&#28304;&#25552;&#20379;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#22312;&#20351;&#29992;LLM&#26102;&#24050;&#32463;&#35777;&#26126;&#22312;&#24341;&#29992;&#27491;&#30830;&#30340;&#26469;&#28304;&#65288;&#26469;&#28304;&#36136;&#37327;&#65289;&#21644;&#20934;&#30830;&#22320;&#34920;&#31034;&#26469;&#28304;&#20013;&#30340;&#20449;&#24687;&#65288;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65289;&#26041;&#38754;&#24037;&#20316;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;LLM&#65292;&#20197;&#25552;&#39640;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#22120;&#65292;&#21487;&#20197;&#22823;&#35268;&#27169;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#23545;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;%&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#35780;&#20272;&#30340;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#35780;&#20272;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.04875</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Provable Length and Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#26356;&#38271;&#24207;&#21015;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#20196;&#29260;&#32452;&#21512;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#26159;&#37325;&#35201;&#30340;&#38750;&#20998;&#24067;&#21270;&#27867;&#21270;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#26550;&#26500;&#20013;&#65292;&#26397;&#30528;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#26681;&#25454;&#26550;&#26500;&#30340;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#30340;&#24517;&#35201;&#24615;&#65292;&#20363;&#22914;&#19982;&#30495;&#23454;&#34920;&#31034;&#20855;&#26377;&#32447;&#24615;&#25110;&#25490;&#21015;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RevOrder&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;nD&#20056;&#20197;1D&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#36816;&#31639;&#12290;&#32463;&#36807;&#20840;&#38754;&#27979;&#35797;&#65292;RevOrder&#22312;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#25968;&#26102;&#12290;&#22312;GSM8K&#25968;&#23398;&#20219;&#21153;&#20013;&#24212;&#29992;RevOrder&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#38169;&#35823;&#29575;&#24182;&#25552;&#39640;&#20102;&#24635;&#20307;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.03822</link><description>&lt;p&gt;
RevOrder&#65306;&#19968;&#31181;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RevOrder: A Novel Method for Enhanced Arithmetic in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RevOrder&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;nD&#20056;&#20197;1D&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#36816;&#31639;&#12290;&#32463;&#36807;&#20840;&#38754;&#27979;&#35797;&#65292;RevOrder&#22312;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#25968;&#26102;&#12290;&#22312;GSM8K&#25968;&#23398;&#20219;&#21153;&#20013;&#24212;&#29992;RevOrder&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#38169;&#35823;&#29575;&#24182;&#25552;&#39640;&#20102;&#24635;&#20307;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RevOrder&#65292;&#19968;&#31181;&#26088;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;n&#20301;&#25968;&#20056;&#20197;1&#20301;&#25968;&#65288;nD&#20056;&#20197;1D&#65289;&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39034;&#24207;&#20013;&#38388;&#25968;&#23383;&#30340;&#25968;&#37327; (CSID)&#65292;&#36825;&#26159;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#31181;&#35780;&#20272;&#26041;&#31243;&#22797;&#26434;&#24615;&#30340;&#26032;&#24230;&#37327;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#27979;&#35797;&#65292;RevOrder&#19981;&#20165;&#22312;&#22522;&#26412;&#30340;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#19988;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20256;&#32479;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#30340;&#22823;&#25968;&#24773;&#20917;&#19979;&#12290;RevOrder&#30340;&#23454;&#29616;&#23545;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#37117;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#27492;&#22806;&#65292;&#23558;RevOrder&#24212;&#29992;&#20110;&#23545;GSM8K&#25968;&#23398;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;LLaMA2-7B&#27169;&#22411;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#23558;&#26041;&#31243;&#35745;&#31639;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;46%&#65292;&#23558;&#24635;&#20307;&#24471;&#20998;&#20174;41.6&#25552;&#21319;&#21040;44.4&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03780</link><description>&lt;p&gt;
&#25581;&#31034;&#23459;&#20256;&#65306;&#22522;&#20110;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#35821;&#35328;&#21450;&#20854;&#25991;&#20307;&#29305;&#24449;&#12290;&#25552;&#20986;&#20102;PPN&#25968;&#25454;&#38598;&#65292;&#21363;&#23459;&#20256;&#24615;&#20266;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26426;&#26500;&#30830;&#23450;&#30340;&#23459;&#20256;&#26469;&#28304;&#32593;&#31449;&#19978;&#30340;&#26032;&#38395;&#25991;&#31456;&#32452;&#25104;&#12290;&#20174;&#35813;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#20102;&#19968;&#37096;&#20998;&#26679;&#26412;&#19982;&#26469;&#33258;&#24120;&#35268;&#27861;&#22269;&#26032;&#38395;&#30340;&#25991;&#31456;&#28151;&#21512;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;URL&#36827;&#34892;&#20102;&#25513;&#30422;&#65292;&#20197;&#36827;&#34892;&#20154;&#31867;&#27880;&#37322;&#23454;&#39564;&#65292;&#20351;&#29992;11&#20010;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#20004;&#31181;&#31867;&#22411;&#30340;&#26032;&#38395;&#32440;&#23545;&#27599;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#35782;&#21035;&#27880;&#37322;&#32773;&#20351;&#29992;&#30340;&#32447;&#32034;&#65292;&#24182;&#19982;&#26426;&#22120;&#20998;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;VAGO&#20998;&#26512;&#22120;&#36827;&#34892;&#36766;&#36848;&#27169;&#31946;&#21644;&#20027;&#35266;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#29992;TF-IDF&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#21450;&#22235;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;&#20004;&#20010;&#22522;&#20110;RoBERTa&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#21477;&#27861;&#30340;CATS&#65292;&#20197;&#21450;&#32467;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#19968;&#20010;XGBoost&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#20219;&#21153;&#24341;&#20837;&#20102;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02130</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#28210;&#26579;&#22270;&#24418;
&lt;/p&gt;
&lt;p&gt;
Rendering Graphs for Graph Reasoning in Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#20219;&#21153;&#24341;&#20837;&#20102;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26426;&#22120;&#20154;&#35268;&#21010;&#12289;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22270;&#32467;&#26500;&#65292;LLMs&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#26684;&#24335;&#30340;&#22270;&#20449;&#24687;&#65292;&#20294;&#24573;&#35270;&#20102;&#20016;&#23500;&#30340;&#35270;&#35273;&#27169;&#24577;&#65292;&#32780;&#35270;&#35273;&#26159;&#20154;&#31867;&#29702;&#35299;&#32467;&#26500;&#20449;&#24687;&#21644;&#36827;&#34892;&#22270;&#25512;&#29702;&#30340;&#30452;&#35266;&#26041;&#24335;&#12290;&#23558;&#22270;&#32467;&#26500;&#34920;&#31034;&#20026;&#35270;&#35273;&#22270;&#20687;(&#21363;&#35270;&#35273;&#22270;)&#30340;&#28508;&#22312;&#30410;&#22788;&#21644;&#33021;&#21147;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#39318;&#27425;&#24341;&#20837;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#26159;&#19968;&#20010;&#20803;&#32452;(&#22270;&#12289;&#22270;&#20687;&#12289;&#25991;&#26412;&#25551;&#36848;)&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;LLMs&#22312;GITQA&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#22312;LLaVA-7B/13B&#27169;&#22411;&#30340;&#24494;&#35843;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#20811;&#26381;&#29616;&#26377;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01729</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Contextualization Distillation from Large Language Model for Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#20811;&#26381;&#29616;&#26377;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25991;&#26412;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#35821;&#26009;&#24211;&#20174;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#25110;&#21516;&#20041;&#35789;&#23450;&#20041;&#20013;&#25910;&#38598;&#30340;&#38745;&#24577;&#21644;&#22122;&#22768;&#24615;&#36136;&#24120;&#24120;&#38480;&#21046;&#20102;&#22522;&#20110;PLM&#30340;KGC&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#33976;&#39311;&#31574;&#30053;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21487;&#25554;&#20837;&#21644;&#21487;&#25773;&#25918;&#30340;&#26041;&#27861;&#65292;&#19982;&#21028;&#21035;&#21644;&#29983;&#25104;&#30340;KGC&#26694;&#26550;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#32039;&#20945;&#30340;&#32467;&#26500;&#21270;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#27573;&#33853;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#37325;&#24314;&#21644;&#19978;&#19979;&#25991;&#21270;&#65292;&#20351;&#36739;&#23567;&#30340;KGC&#27169;&#22411;&#33021;&#22815;&#21560;&#25910;&#36825;&#20123;&#20016;&#23500;&#30340;&#19977;&#20803;&#32452;&#20013;&#30340;&#35265;&#35299;&#12290;&#23545;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;KGC&#25216;&#26415;&#30340;&#20840;&#38754;&#35780;&#20272;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#21644;&#36866;&#24212;&#24615;&#65292;&#25581;&#31034;&#20102;&#26080;&#35770;&#22522;&#30784;&#31649;&#36947;&#22914;&#20309;&#65292;&#22987;&#32456;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#22120;&#65292;&#27169;&#22411;&#21517;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#31574;&#30053;&#25200;&#21160;&#21644;&#22810;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#20943;&#23569;&#38543;&#26426;&#23631;&#34109;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#30340;&#21516;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#23569;&#26679;&#26412;&#21644;&#20010;&#20307;&#36755;&#20837;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00263</link><description>&lt;p&gt;
DetectGPT&#26159;&#21542;&#20805;&#20998;&#21033;&#29992;&#20102;&#25200;&#21160;&#65311;&#22522;&#20110;&#27169;&#22411;&#23545;&#27604;&#23398;&#20064;&#30340;&#36873;&#25321;&#24615;&#25200;&#21160;&#20250;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective Perturbation on Model-Based Contrastive Learning Detector would be Better
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00263
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#22120;&#65292;&#27169;&#22411;&#21517;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#31574;&#30053;&#25200;&#21160;&#21644;&#22810;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#20943;&#23569;&#38543;&#26426;&#23631;&#34109;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#30340;&#21516;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#23569;&#26679;&#26412;&#21644;&#20010;&#20307;&#36755;&#20837;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#20854;&#28389;&#29992;&#30340;&#22686;&#38271;&#20851;&#27880;&#12290;DetectGPT&#26159;&#19968;&#31181;&#38646;-shot&#22522;&#20110;&#24230;&#37327;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#25200;&#21160;&#24182;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;DetectGPT&#30340;&#38543;&#26426;&#25200;&#21160;&#31574;&#30053;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#65292;&#38480;&#21046;&#20102;&#21487;&#21306;&#20998;&#24615;&#21644;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#36923;&#36753;&#22238;&#24402;&#27169;&#22359;&#20381;&#36182;&#20110;&#35774;&#32622;&#38408;&#20540;&#65292;&#36825;&#20250;&#24433;&#21709;&#20010;&#20307;&#25110;&#23567;&#25209;&#37327;&#36755;&#20837;&#30340;&#27867;&#21270;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#22120;&#65292;&#27169;&#22411;&#21517;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#31574;&#30053;&#25200;&#21160;&#26469;&#32531;&#35299;&#38543;&#26426;&#23631;&#34109;&#25152;&#24341;&#36215;&#30340;&#37325;&#35201;&#20449;&#24687;&#20002;&#22833;&#65292;&#24182;&#21033;&#29992;&#22810;&#23545;&#27604;&#23398;&#20064;&#25429;&#25417;&#25200;&#21160;&#26399;&#38388;&#30340;&#38544;&#21547;&#27169;&#24335;&#20449;&#24687;&#65292;&#20415;&#20110;&#23569;&#37327;&#26679;&#26412;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#21517;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#27604;SOTA&#26041;&#27861;&#39640;&#20986;1.20\%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;...
&lt;/p&gt;
&lt;p&gt;
The burgeoning capabilities of large language models (LLMs) have raised growing concerns about abuse. DetectGPT, a zero-shot metric-based unsupervised machine-generated text detector, first introduces perturbation and shows great performance improvement. However, DetectGPT's random perturbation strategy might introduce noise, limiting the distinguishability and further performance improvements. Moreover, its logit regression module relies on setting the threshold, which harms the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel detector, \modelname{}, which uses selective strategy perturbation to relieve the important information loss caused by random masking, and multi-pair contrastive learning to capture the implicit pattern information during perturbation, facilitating few-shot performance. The experiments show that \modelname{} outperforms the SOTA method by 1.20\% in accuracy on average on four public datasets. We further analyze th
&lt;/p&gt;</description></item><item><title>FraiLT&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#36882;&#24402;&#26041;&#27861;&#21644;&#36845;&#20195;&#32534;&#30721;&#65292;&#23454;&#29616;&#20102;&#22312;&#32039;&#20945;&#24418;&#24335;&#19979;&#36798;&#21040;&#36739;&#22823;&#27169;&#22411;&#30340;&#35299;&#37322;&#28145;&#24230;&#65292;&#22312;&#24615;&#33021;&#34920;&#29616;&#19978;&#20248;&#20110;&#36739;&#22823;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#39640;&#25928;&#21644;&#21487;&#35775;&#38382;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.11626</link><description>&lt;p&gt;
&#33258;&#30001;&#38271;&#24605;&#21464;&#21387;&#22120;&#65288;FraiLT&#65289;
&lt;/p&gt;
&lt;p&gt;
Freely Long-Thinking Transformer (FraiLT)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11626
&lt;/p&gt;
&lt;p&gt;
FraiLT&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#36882;&#24402;&#26041;&#27861;&#21644;&#36845;&#20195;&#32534;&#30721;&#65292;&#23454;&#29616;&#20102;&#22312;&#32039;&#20945;&#24418;&#24335;&#19979;&#36798;&#21040;&#36739;&#22823;&#27169;&#22411;&#30340;&#35299;&#37322;&#28145;&#24230;&#65292;&#22312;&#24615;&#33021;&#34920;&#29616;&#19978;&#20248;&#20110;&#36739;&#22823;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#39640;&#25928;&#21644;&#21487;&#35775;&#38382;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30001;&#38271;&#24605;&#21464;&#21387;&#22120;&#65288;FraiLT&#65289;&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#26088;&#22312;&#22686;&#24378;&#22788;&#29702;&#33021;&#21147;&#32780;&#19981;&#22686;&#21152;&#35268;&#27169;&#12290;&#23427;&#37319;&#29992;&#36882;&#24402;&#26041;&#27861;&#65292;&#22810;&#27425;&#36845;&#20195;&#23376;&#23618;&#65292;&#24182;&#24341;&#20837;&#36845;&#20195;&#32534;&#30721;&#20197;&#22312;&#36825;&#20123;&#21608;&#26399;&#20013;&#20445;&#25345;&#24847;&#35782;&#12290;&#36845;&#20195;&#32534;&#30721;&#20351;FraiLT&#33021;&#22815;&#20197;&#32039;&#20945;&#30340;&#24418;&#24335;&#23454;&#29616;&#36739;&#22823;&#27169;&#22411;&#30340;&#35299;&#37322;&#28145;&#24230;&#12290;&#22312;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;FraiLT&#20248;&#20110;&#36739;&#22823;&#30340;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#30340;&#21516;&#26102;&#25552;&#20379;&#39640;&#36136;&#37327;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#20195;&#34920;&#20102;&#26356;&#39640;&#25928;&#21644;&#21487;&#35775;&#38382;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11626v2 Announce Type: replace-cross  Abstract: Freely Long-Thinking Transformer (FraiLT) is an improved transformer model designed to enhance processing capabilities without scaling up size. It utilizes a recursive approach, iterating over a subset of layers multiple times, and introduces iteration encodings to maintain awareness across these cycles. Iteration encoding allows FraiLT to achieve the interpretive depth of larger models in a compact form. When evaluated on a synthetic story dataset, FraiLT outperformed larger models, showcasing its ability to deliver high-quality performance while reducing memory demands. This model represents a step forward towards more efficient and accessible language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;MM-SAP&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24863;&#30693;&#20013;&#30340;&#33258;&#25105;&#24847;&#35782;&#33021;&#21147;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2401.07529</link><description>&lt;p&gt;
MM-SAP&#65306;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#24847;&#35782;&#22312;&#24863;&#30693;&#20013;&#30340;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;MM-SAP&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24863;&#30693;&#20013;&#30340;&#33258;&#25105;&#24847;&#35782;&#33021;&#21147;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36817;&#26399;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#24863;&#30693;&#21644;&#29702;&#35299;&#26041;&#38754;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24187;&#35273;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#27169;&#22411;&#22312;&#29702;&#35299;&#20174;&#22270;&#20687;&#20013;&#33021;&#22815;&#21644;&#19981;&#33021;&#22815;&#24863;&#30693;&#30340;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#31181;&#33021;&#21147;&#25105;&#20204;&#31216;&#20043;&#20026;&#24863;&#30693;&#20013;&#30340;&#33258;&#25105;&#24847;&#35782;&#12290;&#23613;&#31649;&#37325;&#35201;&#24615;&#37325;&#22823;&#65292;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#21364;&#24573;&#35270;&#20102;MLLMs&#30340;&#36825;&#20010;&#26041;&#38754;&#12290;&#26412;&#25991;&#26088;&#22312;&#23450;&#20041;&#21644;&#35780;&#20272;MLLMs&#22312;&#24863;&#30693;&#20013;&#30340;&#33258;&#25105;&#24847;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#24863;&#30693;&#20013;&#30340;&#30693;&#35782;&#35937;&#38480;&#65292;&#36825;&#26377;&#21161;&#20110;&#23450;&#20041;MLLMs&#23545;&#22270;&#20687;&#20102;&#35299;&#21644;&#19981;&#20102;&#35299;&#30340;&#20869;&#23481;&#12290;&#21033;&#29992;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21363;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#36825;&#31181;&#33021;&#21147;&#30340;MLLMs&#24863;&#30693;&#20013;&#30340;&#33258;&#25105;&#24847;&#35782;&#22522;&#20934;&#65288;MM-SAP&#65289;&#12290;&#25105;&#20204;&#23558;MM-SAP&#24212;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07529v2 Announce Type: replace-cross  Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception and understanding. However, these models also suffer from hallucinations, which limit their reliability as AI systems. We believe that these hallucinations are partially due to the models' struggle with understanding what they can and cannot perceive from images, a capability we refer to as self-awareness in perception. Despite its importance, this aspect of MLLMs has been overlooked in prior studies. In this paper, we aim to define and evaluate the self-awareness of MLLMs in perception. To do this, we first introduce the knowledge quadrant in perception, which helps define what MLLMs know and do not know about images. Using this framework, we propose a novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), specifically designed to assess this capability. We apply MM-SAP to a variety of 
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;MoE-Mamba&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;Mamba&#21644;&#22522;&#20934;Transformer-MoE&#65292;&#36798;&#21040;&#20102;&#19982;Mamba&#30456;&#21516;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35757;&#32451;&#27493;&#39588;&#20943;&#23569;&#20102;2.35&#20493;&#12290;</title><link>https://arxiv.org/abs/2401.04081</link><description>&lt;p&gt;
MoE-Mamba: &#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.04081
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;MoE-Mamba&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;Mamba&#21644;&#22522;&#20934;Transformer-MoE&#65292;&#36798;&#21040;&#20102;&#19982;Mamba&#30456;&#21516;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#35757;&#32451;&#27493;&#39588;&#20943;&#23569;&#20102;2.35&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#24050;&#32463;&#25104;&#20026;&#39034;&#24207;&#24314;&#27169;&#39046;&#22495;&#30340;&#20005;&#32899;&#31454;&#20105;&#32773;&#65292;&#25361;&#25112;&#20102;Transformer&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#24320;&#25918;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#35201;&#21457;&#25496;SSMs&#22312;&#25193;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#23427;&#20204;&#24212;&#35813;&#19982;MoE&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;Mamba&#19978;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#36817;&#22522;&#20110;SSM&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MoE-Mamba&#22312;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20248;&#20110;Mamba&#21644;&#22522;&#20934;Transformer-MoE&#12290;&#29305;&#21035;&#22320;&#65292;MoE-Mamba&#22312;&#26356;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#20013;&#36798;&#21040;&#19982;Mamba&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;Mamba&#30456;&#23545;&#20110;Transformer&#30340;&#25512;&#29702;&#24615;&#33021;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.04081v2 Announce Type: replace-cross  Abstract: State Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging the dominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved Transformer-based Large Language Models, including recent state-of-the-art open models. We propose that to unlock the potential of SSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a recent SSM-based model that achieves remarkable performance. Our model, MoE-Mamba, outperforms both Mamba and baseline Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in $2.35\times$ fewer training steps while preserving the inference performance gains of Mamba against Transformer.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36890;&#36807;&#24191;&#25773;&#26597;&#35810;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#39640;&#25928;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#20026;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#23450;&#21046;&#30340;Sigmoid Trick&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#32467;&#21512;&#22312;KILT&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.12430</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#29992;&#20110;&#24555;&#36895;&#21644;&#25913;&#36827;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12430
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36890;&#36807;&#24191;&#25773;&#26597;&#35810;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#39640;&#25928;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#21644;&#20026;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#23450;&#21046;&#30340;Sigmoid Trick&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#32467;&#21512;&#22312;KILT&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;RAG&#26041;&#27861;&#20013;&#65292;&#37325;&#26032;&#25490;&#24207;&#22120;&#22312;&#25552;&#21319;&#26816;&#32034;&#20934;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#33021;&#22815;&#25581;&#31034;&#27599;&#23545;&#26597;&#35810;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#38656;&#35201;&#21453;&#22797;&#23545;&#26597;&#35810;&#21644;&#22823;&#37327;&#38271;&#25991;&#26412;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#23548;&#33268;&#20102;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#38480;&#21046;&#20102;&#26816;&#32034;&#25991;&#26412;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20934;&#30830;&#24615;&#12290;&#20316;&#20026;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;&#24191;&#25773;&#26597;&#35810;&#32534;&#30721;&#22120;&#23454;&#29616;&#30340;&#39640;&#25928;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#36895;&#24230;&#25552;&#39640;20&#20493;&#33267;40&#20493;&#65292;&#36229;&#36807;&#22522;&#20934;&#36890;&#36947;&#37325;&#26032;&#25490;&#24207;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Sigmoid Trick&#65292;&#19968;&#31181;&#20026;&#26631;&#39064;&#37325;&#26032;&#25490;&#24207;&#23450;&#21046;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23558;&#36825;&#20004;&#31181;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#22312;&#20174;KILT&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#39564;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#32463;&#39564;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12430v3 Announce Type: replace-cross  Abstract: In recent RAG approaches, rerankers play a pivotal role in refining retrieval accuracy with the ability of revealing logical relations for each pair of query and text. However, existing rerankers are required to repeatedly encode the query and a large number of long retrieved text. This results in high computational costs and limits the number of retrieved text, hindering accuracy. As a remedy of the problem, we introduce the Efficient Title Reranker via Broadcasting Query Encoder, a novel technique for title reranking that achieves a 20x-40x speedup over the vanilla passage reranker. Furthermore, we introduce Sigmoid Trick, a novel loss function customized for title reranking. Combining both techniques, we empirically validated their effectiveness, achieving state-of-the-art results on all four datasets we experimented with from the KILT knowledge benchmark.
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#33976;&#39311;(MD)&#26694;&#26550;&#32467;&#21512;&#20102;LLMs&#20013;&#30340;Program of Thought (PoT)&#21644;Chain of Thought (CoT)&#33021;&#21147;&#65292;&#23558;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#33976;&#39311;&#21040;&#36739;&#23567;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#36739;&#23567;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.10730</link><description>&lt;p&gt;
&#28151;&#21512;&#33976;&#39311;&#26377;&#21161;&#20110;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#22320;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Mixed Distillation Helps Smaller Language Model Better Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10730
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#33976;&#39311;(MD)&#26694;&#26550;&#32467;&#21512;&#20102;LLMs&#20013;&#30340;Program of Thought (PoT)&#21644;Chain of Thought (CoT)&#33021;&#21147;&#65292;&#23558;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#33976;&#39311;&#21040;&#36739;&#23567;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#36739;&#23567;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26368;&#36817;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24322;&#24120;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#30340;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36890;&#36807;&#20174;LLMs&#33976;&#39311;&#30693;&#35782;&#26469;&#22686;&#24378;&#36739;&#23567;&#27169;&#22411;&#65292;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#21035;&#38656;&#35201;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#24448;&#24448;&#38590;&#20197;&#19982;LLMs&#30340;&#24615;&#33021;&#21305;&#25932;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28151;&#21512;&#33976;&#39311;(MD)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;LLMs&#20013;&#30340;Program of Thought (PoT)&#21644;Chain of Thought (CoT)&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#32467;&#21512;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#24182;&#23558;&#36825;&#20123;&#33021;&#21147;&#33976;&#39311;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MD&#26174;&#33879;&#22686;&#24378;&#20102;&#36739;&#23567;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#21333;&#36335;&#24452;&#21644;&#22810;&#36335;&#24452;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10730v2 Announce Type: replace-cross  Abstract: While large language models (LLMs) have demonstrated exceptional performance in recent natural language processing (NLP) tasks, their deployment poses substantial challenges due to high computational and memory demands in real-world applications. Recent studies have focused on enhancing smaller models through knowledge distillation from LLMs, yielding promising results. However, these models often struggle to match the performance of LLMs, especially in tasks that require reasoning. In this work, we introduce Mixed Distillation (MD) framework, which capitalizes on the strengths of Program of Thought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining multiple prompting techniques and distilling these capabilities into smaller models. Our experimental results show that MD significantly enhances the single-path and multi-path reasoning ability of smaller models in various tasks. In terms of accuracy and generalit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#28857;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#28436;&#31034;&#21644;&#36880;&#28857;&#20559;&#22909;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20559;&#22909;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#24615;&#33021;&#27425;&#20248;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.02554</link><description>&lt;p&gt;
ULMA&#65306;&#20154;&#31867;&#28436;&#31034;&#21644;&#36880;&#28857;&#20559;&#22909;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02554
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#28857;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#28436;&#31034;&#21644;&#36880;&#28857;&#20559;&#22909;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#20559;&#22909;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#24615;&#33021;&#27425;&#20248;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#26399;&#26395;&#65288;&#20363;&#22914;&#65292;&#26377;&#30410;&#21644;&#26080;&#23475;&#65289;&#23545;&#40784;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36843;&#20999;&#25361;&#25112;&#12290;&#20856;&#22411;&#30340;&#23545;&#40784;&#36807;&#31243;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#21644;&#20559;&#22909;&#23398;&#20064;&#12290;&#22823;&#22810;&#25968;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;RLHF&#21644;DPO&#65289;&#20381;&#36182;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#65292;&#36825;&#24182;&#19981;&#20805;&#20998;&#22320;&#35299;&#20915;&#20154;&#31867;&#21453;&#39304;&#26159;&#36880;&#28857;&#30340;&#24773;&#20917;&#65292;&#23548;&#33268;&#28508;&#22312;&#20449;&#24687;&#20002;&#22833;&#21644;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36880;&#28857;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65292;&#19968;&#31181;&#26088;&#22312;&#26377;&#25928;&#21033;&#29992;&#36880;&#28857;&#21453;&#39304;&#30340;&#26032;&#39062;&#20559;&#22909;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#25581;&#31034;&#20102;&#30417;&#30563;&#24494;&#35843;&#21644;&#36880;&#28857;&#20559;&#22909;&#23398;&#20064;&#20043;&#38388;&#30340;&#26032;&#39062;&#32852;&#31995;&#65292;&#26368;&#32456;&#24418;&#25104;&#20102;&#32479;&#19968;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#23545;&#40784;&#19982;&#20154;&#31867;&#28436;&#31034;&#21644;&#36880;&#28857;&#20559;&#22909;&#32479;&#19968;&#30340;&#21333;&#27493;&#26041;&#27861;&#12290;&#22312;&#36880;&#28857;&#20559;&#22909;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02554v2 Announce Type: replace-cross  Abstract: Aligning language models to human expectations, e.g., being helpful and harmless, has become a pressing challenge for large language models. A typical alignment procedure consists of supervised fine-tuning and preference learning. Most preference learning methods, such as RLHF and DPO, depend on pairwise preference data, which inadequately address scenarios where human feedback is point-wise, leading to potential information loss and suboptimal performance. Addressing this gap, we introduce Point-wise Direct Preference Optimization, a novel preference learning method designed to harness point-wise feedback effectively. Our work also uncovers a novel connection between supervised fine-tuning and point-wise preference learning, culminating in Unified Language Model Alignment, a single-step method that unifies the alignment with human demonstrations and point-wise preferences. Extensive experiments on point-wise preference dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#21464;&#21387;&#22120;&#24212;&#29992;&#20110;SKILL&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25968;&#25454;&#39640;&#25928;&#29983;&#25104;SKILL&#20195;&#30721;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#30828;&#20214;&#35774;&#35745;&#24037;&#31243;&#24072;&#30340;&#29983;&#20135;&#21147;</title><link>https://arxiv.org/abs/2312.01921</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;SKILL&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Approach Towards SKILL Code Autocompletion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#21464;&#21387;&#22120;&#24212;&#29992;&#20110;SKILL&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25968;&#25454;&#39640;&#25928;&#29983;&#25104;SKILL&#20195;&#30721;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#30828;&#20214;&#35774;&#35745;&#24037;&#31243;&#24072;&#30340;&#29983;&#20135;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#32487;&#32493;&#22686;&#21152;&#30005;&#23376;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#65292;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#24517;&#39035;&#19981;&#26029;&#25552;&#21319;&#20197;&#28385;&#36275;&#20840;&#29699;&#38656;&#27714;&#12290;EDA&#25216;&#26415;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#20363;&#23376;&#26159;SKILL&#65292;&#19968;&#31181;&#29992;&#20110;&#23450;&#21046;&#21644;&#25193;&#23637;EDA&#36719;&#20214;&#30340;&#33050;&#26412;&#35821;&#35328;&#12290;&#26368;&#36817;&#65292;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#29978;&#33267;&#24050;&#32463;&#22312;&#21830;&#19994;&#24320;&#21457;&#24037;&#20855;&#20013;&#20351;&#29992;&#20197;&#25552;&#39640;&#24320;&#21457;&#32773;&#30340;&#29983;&#20135;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23558;&#21464;&#21387;&#22120;&#24212;&#29992;&#20110;SKILL&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#65292;&#26088;&#22312;&#25552;&#39640;&#30828;&#20214;&#35774;&#35745;&#24037;&#31243;&#24072;&#30340;&#29983;&#20135;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#23454;&#39564;&#39564;&#35777;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25968;&#25454;&#39640;&#25928;&#29983;&#25104;SKILL&#20195;&#30721;&#30340;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65288;i&#65289;&#21019;&#24314;&#20855;&#26377;&#26410;&#26631;&#35760;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#39640;&#36136;&#37327;SKILL&#25968;&#25454;&#38598;&#65292;&#65288;ii&#65289;&#19968;&#20010;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;T5&#27169;&#22411;&#20043;&#21069;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01921v2 Announce Type: replace-cross  Abstract: As Moore's Law continues to increase the complexity of electronic systems, Electronic Design Automation (EDA) must advance to meet global demand. An important example of an EDA technology is SKILL, a scripting language used to customize and extend EDA software. Recently, code generation models using the transformer architecture have achieved impressive results in academic settings and have even been used in commercial developer tools to improve developer productivity. To the best of our knowledge, this study is the first to apply transformers to SKILL code autocompletion towards improving the productivity of hardware design engineers. In this study, a novel, data-efficient methodology for generating SKILL code is proposed and experimentally validated. More specifically, we propose a novel methodology for (i) creating a high-quality SKILL dataset with both unlabeled and labeled data, (ii) a training strategy where T5 models pre-
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#25913;&#21892;&#30422;&#20857;&#35821;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20174;&#30456;&#20851;&#35821;&#35328;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12289;&#20248;&#21270;&#20849;&#20139;&#35789;&#27719;&#21644;&#26631;&#35760;&#20998;&#21106;&#26041;&#27861;&#12289;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20197;&#21450;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#65292;&#20854;&#20013;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#30456;&#20851;&#24615;&#30340;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30456;&#27604;&#26631;&#20934;&#21452;&#35821;&#27169;&#22411;&#26377;4 BLEU&#30340;&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2311.14530</link><description>&lt;p&gt;
&#29992;&#20110;&#30422;&#20857;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Machine Translation for Ge'ez Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#25913;&#21892;&#30422;&#20857;&#35821;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20174;&#30456;&#20851;&#35821;&#35328;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12289;&#20248;&#21270;&#20849;&#20139;&#35789;&#27719;&#21644;&#26631;&#35760;&#20998;&#21106;&#26041;&#27861;&#12289;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20197;&#21450;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#65292;&#20854;&#20013;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#30456;&#20851;&#24615;&#30340;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30456;&#27604;&#26631;&#20934;&#21452;&#35821;&#27169;&#22411;&#26377;4 BLEU&#30340;&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14530v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#26426;&#22120;&#32763;&#35793;(MT)&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;&#30422;&#20857;&#35821;&#65292;&#19968;&#31181;&#21476;&#32769;&#30340;&#35821;&#35328;&#65292;&#19981;&#20877;&#26159;&#20219;&#20309;&#31038;&#21306;&#30340;&#27597;&#35821;&#65292;&#38754;&#20020;&#35832;&#22914;&#35789;&#27719;&#22806;&#29983;&#12289;&#39046;&#22495;&#19981;&#21305;&#37197;&#21644;&#32570;&#20047;&#36275;&#22815;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25913;&#21892;&#30422;&#20857;&#35821;MT&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#20174;&#30456;&#20851;&#35821;&#35328;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12289;&#20248;&#21270;&#20849;&#20139;&#35789;&#27719;&#21644;&#26631;&#35760;&#20998;&#21106;&#26041;&#27861;&#12289;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23569;&#26679;&#26412;&#32763;&#35793;&#37197;&#21512;&#27169;&#31946;&#21305;&#37197;&#12290;&#25105;&#20204;&#22522;&#20110;&#35821;&#35328;&#30456;&#20851;&#24615;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#27169;&#22411;&#65292;&#20351;&#26631;&#20934;&#21452;&#35821;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;&#32422;4 BLEU&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#24494;&#35843;NLLB-200&#27169;&#22411;&#65292;&#36825;&#26159;&#24403;&#20170;&#21487;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#32763;&#35793;&#27169;&#22411;&#20043;&#19968;&#65292;&#20294;&#21457;&#29616;&#21482;&#26377;4k&#35757;&#32451;&#26679;&#26412;&#30340;&#30422;&#20857;&#35821;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14530v2 Announce Type: replace  Abstract: Machine translation (MT) for low-resource languages such as Ge'ez, an ancient language that is no longer the native language of any community, faces challenges such as out-of-vocabulary words, domain mismatches, and lack of sufficient labeled training data. In this work, we explore various methods to improve Ge'ez MT, including transfer-learning from related languages, optimizing shared vocabulary and token segmentation approaches, finetuning large pre-trained models, and using large language models (LLMs) for few-shot translation with fuzzy matches. We develop a multilingual neural machine translation (MNMT) model based on languages relatedness, which brings an average performance improvement of about 4 BLEU compared to standard bilingual models. We also attempt to finetune the NLLB-200 model, one of the most advanced translation models available today, but find that it performs poorly with only 4k training samples for Ge'ez. Furthe
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#27010;&#36848;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#36136;&#37327;&#25552;&#21319;&#20013;&#30340;&#21464;&#38761;&#24615;&#20316;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#35786;&#26029;&#21644;&#27835;&#30103;&#39046;&#22495;&#65292;&#20197;&#21450;&#22312;&#30284;&#30151;&#25252;&#29702;&#12289;&#30382;&#32932;&#31185;&#12289;&#29273;&#31185;&#21644;&#24515;&#29702;&#20581;&#24247;&#31561;&#26041;&#38754;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.12882</link><description>&lt;p&gt;
&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#20013;&#30340;&#24212;&#29992;&#27010;&#35272;
&lt;/p&gt;
&lt;p&gt;
Overview of Current Applications of Large Language Models in Various Medical Specialities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12882
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#27010;&#36848;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#36136;&#37327;&#25552;&#21319;&#20013;&#30340;&#21464;&#38761;&#24615;&#20316;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#35786;&#26029;&#21644;&#27835;&#30103;&#39046;&#22495;&#65292;&#20197;&#21450;&#22312;&#30284;&#30151;&#25252;&#29702;&#12289;&#30382;&#32932;&#31185;&#12289;&#29273;&#31185;&#21644;&#24515;&#29702;&#20581;&#24247;&#31561;&#26041;&#38754;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#27010;&#36848;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#39046;&#22495;&#26368;&#26032;&#24212;&#29992;&#65292;&#31361;&#20986;&#23427;&#20204;&#22312;&#25552;&#21319;&#21307;&#30103;&#36136;&#37327;&#26041;&#38754;&#30340;&#21464;&#38761;&#24615;&#20316;&#29992;&#12290;&#36890;&#36807;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;LLMs&#24050;&#25104;&#20026;&#21327;&#21161;&#21307;&#29983;&#12289;&#21307;&#30103;&#25552;&#20379;&#32773;&#21644;&#24739;&#32773;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#25105;&#20204;&#37325;&#28857;&#23457;&#35270;&#20102;LLMs&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35786;&#26029;&#21644;&#27835;&#30103;&#30456;&#20851;&#24212;&#29992;&#19978;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;LLMs&#22312;&#30284;&#30151;&#25252;&#29702;&#12289;&#30382;&#32932;&#31185;&#12289;&#29273;&#31185;&#21644;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#20026;&#21307;&#23398;&#35786;&#26029;&#21644;&#24739;&#32773;&#25252;&#29702;&#24102;&#26469;&#30340;&#21019;&#26032;&#12290;&#20998;&#26512;&#28085;&#30422;&#20102;&#23558;LLMs&#25972;&#21512;&#21040;&#21307;&#30103;&#39046;&#22495;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#25351;&#20986;&#20102;&#23613;&#31649;&#23384;&#22312;&#24403;&#21069;&#23616;&#38480;&#65292;&#20294;&#23427;&#20204;&#22312;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#20013;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12882v2 Announce Type: replace  Abstract: We aim to provide an overview of the latest applications of Large Language Models (LLMs) in the healthcare sector, highlighting their transformative role in enhancing medical care quality. By processing vast amounts of data from diverse medical domains, LLMs have become pivotal in assisting doctors, healthcare providers, and patients. We review the application of Large Language Models (LLMs) in healthcare, focusing on diagnostics and treatment related applications. We highlight the use of LLMs in cancer care, dermatology, dental, and mental health, emphasizing the innovation they bring to medical diagnostics and patient care. The analysis addresses the challenges and opportunities of integrating LLMs in healthcare, noting their potential in various medical specialties despite current limitations. Further, we provide an overview of handling various data types in the medical field.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#65292;&#20174;&#39044;&#35757;&#32451;&#21040;&#25512;&#26029;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2311.12351</link><description>&lt;p&gt;
&#22312;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#36827;Transformer&#26550;&#26500;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#65292;&#20174;&#39044;&#35757;&#32451;&#21040;&#25512;&#26029;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#24212;&#29992;&#20110;&#30693;&#35782;&#24211;&#12289;&#20154;&#26426;&#30028;&#38754;&#21644;&#21160;&#24577;&#20195;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#65292;&#26631;&#24535;&#30528;&#36808;&#21521;&#36798;&#21040;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#19968;&#22823;&#27493;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LLMs&#20027;&#35201;&#26159;&#22312;&#30701;&#25991;&#26412;&#29255;&#27573;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36825;&#21361;&#21450;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#39057;&#32321;&#36935;&#21040;&#30340;&#38271;&#19978;&#19979;&#25991;&#25552;&#31034;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#23545;&#26368;&#36817;&#22312;&#26088;&#22312;&#22686;&#24378;LLMs&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#22522;&#20110;Transformer&#30340;LLM&#26550;&#26500;&#30340;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#28085;&#30422;&#20102;&#25972;&#20010;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#65292;&#20174;&#39044;&#35757;&#32451;&#21040;&#25512;&#26029;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38416;&#36848;&#24182;&#20998;&#26512;&#20102;&#24403;&#21069;&#22522;&#20110;Transformer&#27169;&#22411;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;Transformer&#26550;&#26500;&#21319;&#32423;&#30340;&#20998;&#31867;&#21644;&#26223;&#35266;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12351v2 Announce Type: replace  Abstract: Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#26102;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#31574;&#30053;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ever&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.09114</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#26102;&#39564;&#35777;&#21644;&#32416;&#27491;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09114
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#26102;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#31574;&#30053;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ever&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29983;&#25104;&#27969;&#30021;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#36935;&#21040;&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#26500;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#20110;&#38750;&#22522;&#20110;&#26816;&#32034;&#30340;&#29983;&#25104;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#20013;&#65292;&#29616;&#26377;&#30340;&#20107;&#21518;&#32416;&#27491;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#35299;&#20915;&#8220;&#28378;&#38634;&#29699;&#8221;&#38382;&#39064;&#23548;&#33268;&#30340;&#32047;&#31215;&#34394;&#26500;&#38169;&#35823;&#65292;&#29305;&#21035;&#26159;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ever&#8221;&#30340;&#26032;&#26041;&#27861;&#12290;Ever&#37319;&#29992;&#23454;&#26102;&#12289;&#36880;&#27493;&#30340;&#29983;&#25104;&#21644;&#34394;&#26500;&#32416;&#27491;&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#31561;&#21040;&#29983;&#25104;&#36807;&#31243;&#32467;&#26463;&#25165;&#32416;&#27491;&#34394;&#26500;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#26816;&#27979;&#21644;&#32416;&#27491;&#34394;&#26500;&#12290;&#19982;&#22522;&#20110;&#26816;&#32034;&#21644;&#38750;&#22522;&#20110;&#26816;&#32034;&#30340;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09114v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the "snowballing" issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based basel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23450;&#20041;&#35745;&#31639;&#29615;&#22659;&#20013;&#29702;&#35299;&#25919;&#27835;&#25991;&#26412;&#20013;&#27169;&#26865;&#20004;&#21487;&#38472;&#36848;&#25152;&#38656;&#32972;&#26223;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#27492;&#26469;&#20998;&#26512;&#21644;&#39044;&#27979;&#25991;&#26412;&#30340;&#30495;&#23454;&#19990;&#30028;&#32972;&#26223;&#12290;</title><link>https://arxiv.org/abs/2311.09106</link><description>&lt;p&gt;
&#8220;&#25105;&#20204;&#35201;&#27714;&#20844;&#27491;&#65281;&#8221;&#65306;&#25919;&#27835;&#25991;&#26412;&#30340;&#31038;&#20250;&#32972;&#26223;&#22880;&#22522;
&lt;/p&gt;
&lt;p&gt;
"We Demand Justice!": Towards Social Context Grounding of Political Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23450;&#20041;&#35745;&#31639;&#29615;&#22659;&#20013;&#29702;&#35299;&#25919;&#27835;&#25991;&#26412;&#20013;&#27169;&#26865;&#20004;&#21487;&#38472;&#36848;&#25152;&#38656;&#32972;&#26223;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#27492;&#26469;&#20998;&#26512;&#21644;&#39044;&#27979;&#25991;&#26412;&#30340;&#30495;&#23454;&#19990;&#30028;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;&#32463;&#24120;&#21253;&#25324;&#8220;&#30475;&#20284;&#30456;&#20284;&#30340;&#35821;&#35328;&#34987;&#25919;&#27835;&#20809;&#35889;&#20004;&#31471;&#20351;&#29992;&#8221;&#65292;&#24448;&#24448;&#36716;&#21270;&#20026;&#25130;&#28982;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#20363;&#22914;&#65292;&#8220;&#24605;&#24565;&#19982;&#31048;&#31095;&#8221;&#21487;&#33021;&#34920;&#36798;&#23545;&#22823;&#35268;&#27169;&#26538;&#20987;&#21463;&#23475;&#32773;&#30340;&#21516;&#24773;&#65292;&#20063;&#21487;&#33021;&#25209;&#35780;&#23545;&#35813;&#38382;&#39064;&#32570;&#20047;&#31435;&#27861;&#34892;&#21160;&#12290;&#26412;&#25991;&#22312;&#35745;&#31639;&#29615;&#22659;&#20013;&#23450;&#20041;&#20102;&#23436;&#20840;&#29702;&#35299;&#27492;&#31867;&#27169;&#26865;&#20004;&#21487;&#38472;&#36848;&#25152;&#38656;&#30340;&#32972;&#26223;&#65292;&#24182;&#20351;&#20854;&#22522;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#20307;&#12289;&#34892;&#21160;&#21644;&#24577;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#38656;&#35201;&#29702;&#35299;&#25991;&#26412;&#23454;&#38469;&#32972;&#26223;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25968;&#25454;&#38598;&#19982;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;RoBERTa&#21644;GPT-3&#65289;&#26500;&#24314;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#23545;&#29616;&#26377;&#30340;&#8220;&#35758;&#35821;&#24773;&#22659;&#21270;&#26694;&#26550;&#8221;&#21644;&#8220;&#25919;&#27835;&#35282;&#33394;&#34920;&#24449;&#8221;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#20197;&#33719;&#24471;&#26356;&#22810;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09106v2 Announce Type: replace  Abstract: Social media discourse frequently consists of 'seemingly similar language used by opposing sides of the political spectrum', often translating to starkly contrasting perspectives. E.g., 'thoughts and prayers', could express sympathy for mass-shooting victims, or criticize the lack of legislative action on the issue. This paper defines the context required to fully understand such ambiguous statements in a computational setting and ground them in real-world entities, actions, and attitudes. We propose two challenging datasets that require an understanding of the real-world context of the text. We benchmark these datasets against models built upon large pre-trained models, such as RoBERTa and GPT-3. Additionally, we develop and benchmark more structured models building upon existing Discourse Contextualization Framework and Political Actor Representation models. We analyze the datasets and the predictions to obtain further insights int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.09101</link><description>&lt;p&gt;
&#26397;&#21521;&#22810;&#27493;&#25512;&#29702;&#30340;&#31572;&#26696;&#26657;&#20934;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified View of Answer Calibration for Multi-Step Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#25193;&#23637;&#20102;&#25913;&#36827;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#24120;&#23558;&#22810;&#27493;&#25512;&#29702;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#36335;&#24452;&#29983;&#25104;&#20197;&#29983;&#25104;&#25512;&#29702;&#36335;&#24452;&#65307;&#21644;&#31572;&#26696;&#26657;&#20934;&#21518;&#22788;&#29702;&#25512;&#29702;&#36335;&#24452;&#20197;&#33719;&#24471;&#26368;&#32456;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#32570;&#20047;&#23545;&#19981;&#21516;&#31572;&#26696;&#26657;&#20934;&#26041;&#27861;&#30340;&#31995;&#32479;&#20998;&#26512;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#22810;&#36335;&#24452;&#19978;&#30340;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21487;&#33021;&#21551;&#31034;&#20248;&#21270;&#22810;&#27493;&#25512;&#29702;&#31995;&#32479;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09101v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: path generation to generate the reasoning path(s); and answer calibration post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for opti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#29983;&#25104;&#30340;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#20016;&#23500;&#35821;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.08941</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#25551;&#36848;&#36923;&#36753;&#35821;&#22659;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning over Description Logic-based Contexts with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#29983;&#25104;&#30340;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#20016;&#23500;&#35821;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#34913;&#37327;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#36890;&#36807;&#35780;&#20272;&#22312;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#21512;&#25104;&#35821;&#22659;&#20013;&#23545;&#36923;&#36753;&#38382;&#39064;&#22238;&#31572;&#25110;&#35777;&#26126;&#29983;&#25104;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#23454;&#38469;&#20351;&#29992;&#30340;&#35821;&#22659;&#38750;&#24120;&#31616;&#21333;&#65307;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#26159;&#30001;&#20165;&#21547;&#26377;&#23569;&#37327;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#37327;&#35789;&#30340;&#30701;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#29983;&#25104;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#22522;&#20110;Transformer&#27169;&#22411;&#33021;&#22815;&#22312;&#34920;&#36798;&#20016;&#23500;&#35821;&#22659;&#20013;&#25191;&#34892;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#29983;&#25104;&#30340;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20026;&#29983;&#25104;&#30693;&#35782;&#24211;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#34920;&#36798;&#24335;&#35821;&#35328;$\mathcal{ALCQ$&#12290;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;384K&#20010;&#31034;&#20363;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#32500;&#24230;&#19978;&#22686;&#21152;&#65306;i) &#25512;&#29702;&#28145;&#24230;&#65292;&#21644;ii) &#21477;&#23376;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08941v2 Announce Type: replace-cross  Abstract: One way that the current state of the art measures the reasoning ability of transformer-based models is by evaluating accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of the contexts used are in practice very simple; in most cases, they are generated from short first-order logic sentences with only a few logical operators and quantifiers. In this work, we seek to answer the question how well a transformer-based model will perform reasoning over expressive contexts. For this purpose, we construct a synthetic natural language question-answering dataset, generated by description logic knowledge bases. For the generation of the knowledge bases, we use the expressive language $\mathcal{ALCQ}$. The resulting dataset contains 384K examples, and increases in two dimensions: i) reasoning depth, and ii) length of sentences. We show that t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#22312;&#28436;&#35762;&#25991;&#26412;&#20013;&#21306;&#20998;&#21457;&#35328;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2311.07564</link><description>&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#33021;&#21542;&#21306;&#20998;&#28436;&#35762;&#25991;&#26412;&#20013;&#30340;&#21457;&#35328;&#20154;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#22312;&#28436;&#35762;&#25991;&#26412;&#20013;&#21306;&#20998;&#21457;&#35328;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#39564;&#35777;&#26159;&#30830;&#23450;&#20004;&#20010;&#19981;&#21516;&#20070;&#38754;&#26679;&#26412;&#26159;&#21542;&#21516;&#23646;&#19968;&#20316;&#32773;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#28041;&#21450;&#23545;&#20070;&#38754;&#25991;&#26412;&#30340;&#24402;&#22240;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36716;&#24405;&#28436;&#35762;&#30340;&#24402;&#23646;&#38382;&#39064;&#65292;&#36825;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#65292;&#35768;&#22810;&#25991;&#20307;&#29305;&#24449;&#65292;&#22914;&#26631;&#28857;&#21644;&#22823;&#20889;&#65292;&#22312;&#36825;&#31181;&#24773;&#22659;&#19979;&#24182;&#19981;&#20855;&#22791;&#20449;&#24687;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36716;&#24405;&#30340;&#28436;&#35762;&#21576;&#29616;&#20854;&#20182;&#27169;&#24335;&#65292;&#22914;&#22635;&#20805;&#35789;&#21644;&#22238;&#24212;&#24615;&#22768;&#38899;&#65288;&#20363;&#22914;&#8220;&#21999;&#8221;&#65292;&#8220;&#21999;&#65292;&#21999;&#8221;&#65289;&#65292;&#36825;&#20123;&#21487;&#33021;&#26159;&#19981;&#21516;&#21457;&#35328;&#20154;&#30340;&#29305;&#24449;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;&#20026;&#20102;&#38480;&#21046;&#21457;&#35328;&#20154;&#19982;&#35805;&#39064;&#20043;&#38388;&#30340;&#34394;&#20551;&#20851;&#32852;&#65292;&#25105;&#20204;&#20351;&#29992;&#20250;&#35805;&#25552;&#31034;&#21644;&#21442;&#19982;&#21516;&#19968;&#23545;&#35805;&#30340;&#21457;&#35328;&#20154;&#26500;&#24314;&#19981;&#21516;&#38590;&#24230;&#30340;&#39564;&#35777;&#35797;&#39564;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#22312;&#36825;&#19968;&#26032;&#22522;&#20934;&#19978;&#24314;&#31435;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07564v2 Announce Type: replace  Abstract: Authorship verification is the task of determining if two distinct writing samples share the same author and is typically concerned with the attribution of written text. In this paper, we explore the attribution of transcribed speech, which poses novel challenges. The main challenge is that many stylistic features, such as punctuation and capitalization, are not informative in this setting. On the other hand, transcribed speech exhibits other patterns, such as filler words and backchannels (e.g., 'um', 'uh-huh'), which may be characteristic of different speakers. We propose a new benchmark for speaker attribution focused on conversational speech transcripts. To limit spurious associations of speakers with topic, we employ both conversation prompts and speakers participating in the same conversation to construct verification trials of varying difficulties. We establish the state of the art on this new benchmark by comparing a suite of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#35299;&#30721;&#22120;&#20013;&#30340;&#21508;&#21521;&#24322;&#24615;&#21576;&#38047;&#29366;&#26354;&#32447;&#65292;&#26368;&#39640;&#21508;&#21521;&#24322;&#24615;&#27987;&#24230;&#22312;&#20013;&#38388;&#23618;&#65292;&#19982;&#32534;&#30721;&#22120;&#20013;&#26356;&#22343;&#21248;&#20998;&#24067;&#30340;&#21508;&#21521;&#24322;&#24615;&#19981;&#21516;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#30340;&#20869;&#22312;&#32500;&#24230;&#22312;&#35757;&#32451;&#21021;&#26399;&#22686;&#21152;&#65292;&#38543;&#21518;&#22312;&#35757;&#32451;&#26411;&#26399;&#20986;&#29616;&#21387;&#32553;&#65292;&#34920;&#26126;&#26356;&#32039;&#20945;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>https://arxiv.org/abs/2311.05928</link><description>&lt;p&gt;
&#23398;&#20064;&#30340;&#24418;&#29366;&#65306;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#21508;&#21521;&#24322;&#24615;&#21644;&#20869;&#22312;&#32500;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#35299;&#30721;&#22120;&#20013;&#30340;&#21508;&#21521;&#24322;&#24615;&#21576;&#38047;&#29366;&#26354;&#32447;&#65292;&#26368;&#39640;&#21508;&#21521;&#24322;&#24615;&#27987;&#24230;&#22312;&#20013;&#38388;&#23618;&#65292;&#19982;&#32534;&#30721;&#22120;&#20013;&#26356;&#22343;&#21248;&#20998;&#24067;&#30340;&#21508;&#21521;&#24322;&#24615;&#19981;&#21516;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#30340;&#20869;&#22312;&#32500;&#24230;&#22312;&#35757;&#32451;&#21021;&#26399;&#22686;&#21152;&#65292;&#38543;&#21518;&#22312;&#35757;&#32451;&#26411;&#26399;&#20986;&#29616;&#21387;&#32553;&#65292;&#34920;&#26126;&#26356;&#32039;&#20945;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;Transformer&#26550;&#26500;&#20013;&#23884;&#20837;&#30340;&#21508;&#21521;&#24322;&#24615;&#21160;&#24577;&#21644;&#20869;&#22312;&#32500;&#24230;&#23637;&#24320;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;Transformer&#35299;&#30721;&#22120;&#20013;&#30340;&#21508;&#21521;&#24322;&#24615;&#37197;&#32622;&#21576;&#29616;&#20986;&#26126;&#26174;&#30340;&#38047;&#29366;&#26354;&#32447;&#65292;&#20855;&#26377;&#26368;&#39640;&#30340;&#21508;&#21521;&#24322;&#24615;&#27987;&#24230;&#22312;&#20013;&#38388;&#23618;&#12290;&#36825;&#31181;&#27169;&#24335;&#19982;&#32534;&#30721;&#22120;&#20013;&#35266;&#23519;&#21040;&#30340;&#26356;&#22343;&#21248;&#20998;&#24067;&#30340;&#21508;&#21521;&#24322;&#24615;&#26377;&#25152;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#23884;&#20837;&#30340;&#20869;&#22312;&#32500;&#24230;&#22312;&#35757;&#32451;&#30340;&#21021;&#22987;&#38454;&#27573;&#22686;&#21152;&#65292;&#34920;&#26126;&#21521;&#26356;&#39640;&#32500;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;&#28982;&#21518;&#22312;&#35757;&#32451;&#26411;&#23614;&#20986;&#29616;&#21521;&#26356;&#20302;&#32500;&#24230;&#30340;&#21387;&#32553;&#38454;&#27573;&#65292;&#26263;&#31034;&#30528;&#23545;&#26356;&#32039;&#20945;&#34920;&#31034;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#29702;&#35299;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23884;&#20837;&#23646;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05928v2 Announce Type: replace-cross  Abstract: In this study, we present an investigation into the anisotropy dynamics and intrinsic dimension of embeddings in transformer architectures, focusing on the dichotomy between encoders and decoders. Our findings reveal that the anisotropy profile in transformer decoders exhibits a distinct bell-shaped curve, with the highest anisotropy concentrations in the middle layers. This pattern diverges from the more uniformly distributed anisotropy observed in encoders. In addition, we found that the intrinsic dimension of embeddings increases in the initial phases of training, indicating an expansion into higher-dimensional space. Which is then followed by a compression phase towards the end of training with dimensionality decrease, suggesting a refinement into more compact representations. Our results provide fresh insights to the understanding of encoders and decoders embedding properties.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;transformers&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#21363;&#21487;&#36827;&#34892;&#23545;&#35937;&#21629;&#21517;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2311.04924</link><description>&lt;p&gt;
&#26080;&#35843;&#35856;&#30340;&#22522;&#30784;&#27169;&#22411;&#23545;&#35937;&#21629;&#21517;
&lt;/p&gt;
&lt;p&gt;
Tuning-less Object Naming with a Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04924
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;transformers&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#21363;&#21487;&#36827;&#34892;&#23545;&#35937;&#21629;&#21517;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#26102;&#23545;&#35937;&#21629;&#21517;&#31995;&#32479;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#20174;&#26410;&#35265;&#36807;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#24320;&#22987;&#20043;&#21069;&#25105;&#20204;&#35748;&#20026;&#23427;&#20934;&#22791;&#22909;&#25509;&#21463;&#20219;&#20309;&#20869;&#23481;&#12290;&#23427;&#23558;&#35266;&#23519;&#21040;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#30456;&#23545;&#36739;&#23567;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#21521;&#37327;&#19982;&#36880;&#28176;&#26500;&#24314;&#30340;&#35789;&#27719;&#34920;&#20013;&#30340;&#32034;&#24341;&#30456;&#20851;&#32852;&#65292;&#19988;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#20351;&#29992;&#20102;&#26469;&#33258;transformers&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20851;&#32852;&#26426;&#21046;&#12290;&#23427;&#20855;&#26377;&#25903;&#25345;&#20174;&#19981;&#30456;&#20851;&#20449;&#24687;&#20013;&#27867;&#21270;&#20197;&#21306;&#20998;&#23454;&#20307;&#24182;&#28508;&#22312;&#22320;&#33021;&#22815;&#19982;&#36828;&#36229;&#20986;&#35789;&#27719;&#34920;&#32034;&#24341;&#30340;&#23454;&#20307;&#30456;&#20851;&#32852;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20197;&#19968;&#27425;&#24615;&#26041;&#24335;&#24037;&#20316;&#65292;&#24182;&#27491;&#30830;&#22320;&#20026;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#21629;&#21517;&#30340;&#23545;&#35937;&#21629;&#21517;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#36890;&#36807;&#40657;&#26495;&#26550;&#26500;&#38598;&#25104;&#30340;&#31995;&#32479;&#27169;&#22359;&#30340;&#23454;&#29616;&#32454;&#33410;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#31995;&#32479;&#30340;&#36136;&#37327;&#65292;&#20027;&#35201;&#30528;&#30524;&#20110;&#23427;&#33021;&#22815;&#35782;&#21035;&#22810;&#23569;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04924v2 Announce Type: replace-cross  Abstract: We implement a real-time object naming system that enables learning a set of named entities never seen. Our approach employs an existing foundation model that we consider ready to see anything before starting. It turns seen images into relatively small feature vectors that we associate with index to a gradually built vocabulary without any training of fine-tuning of the model. Our contribution is using the association mechanism known from transformers as attention. It has features that support generalization from irrelevant information for distinguishing the entities and potentially enable associating with much more than indices to vocabulary. As a result, the system can work in a one-shot manner and correctly name objects named in different contents. We also outline implementation details of the system modules integrated by a blackboard architecture. Finally, we investigate the system's quality, mainly how many objects it can 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;Meta-Continual Active Learning&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#35299;&#20915;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.03732</link><description>&lt;p&gt;
&#23398;&#20064;&#23398;&#20064;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#25345;&#20037;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn for Few-shot Continual Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;Meta-Continual Active Learning&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#35299;&#20915;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#30830;&#20445;&#35299;&#20915;&#20808;&#21069;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#23637;&#31034;&#23545;&#26032;&#39046;&#22495;&#30340;&#21487;&#22609;&#24615;&#12290;&#26368;&#36817;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#23616;&#38480;&#20110;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#65288;CAL&#65289;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#65292;&#20294;&#26410;&#26631;&#35760;&#25968;&#25454;&#20805;&#36275;&#65292;&#20294;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20803;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#26469;&#35299;&#20915;&#20219;&#21153;&#20043;&#38388;&#30340;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#23569;&#26679;&#26412;CAL&#35774;&#32622;&#20013;&#19981;&#21516;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>ITEm&#26159;&#38754;&#21521;&#30005;&#23376;&#21830;&#21153;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#23884;&#20837;&#26469;&#26356;&#22909;&#22320;&#20851;&#27880;&#19981;&#21516;&#27169;&#24577;&#65292;&#25193;&#23637;&#20102;BERT&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.02084</link><description>&lt;p&gt;
ITEm&#65306;&#38754;&#21521;&#30005;&#23376;&#21830;&#21153;&#30340;&#26080;&#30417;&#30563;&#22270;&#29255;&#25991;&#26412;&#23884;&#20837;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ITEm: Unsupervised Image-Text Embedding Learning for eCommerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02084
&lt;/p&gt;
&lt;p&gt;
ITEm&#26159;&#38754;&#21521;&#30005;&#23376;&#21830;&#21153;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#23884;&#20837;&#26469;&#26356;&#22909;&#22320;&#20851;&#27880;&#19981;&#21516;&#27169;&#24577;&#65292;&#25193;&#23637;&#20102;BERT&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#21697;&#23884;&#20837;&#26159;&#30005;&#23376;&#21830;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#22522;&#30707;&#12290;&#36890;&#36807;&#20174;&#22810;&#31181;&#27169;&#24577;&#23398;&#20064;&#30340;&#20135;&#21697;&#23884;&#20837;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#25913;&#36827;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#25552;&#20379;&#20102;&#20114;&#34917;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#27169;&#24577;&#27604;&#20854;&#20182;&#27169;&#24577;&#26356;&#20855;&#20449;&#24687;&#20248;&#21183;&#12290;&#22914;&#20309;&#25945;&#23548;&#27169;&#22411;&#20174;&#19981;&#21516;&#27169;&#24577;&#23398;&#20064;&#23884;&#20837;&#32780;&#19981;&#24573;&#35270;&#36739;&#19981;&#26174;&#33879;&#27169;&#24577;&#30340;&#20449;&#24687;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22270;&#29255;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65288;ITEm&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35774;&#35745;&#29992;&#26469;&#26356;&#22909;&#22320;&#20851;&#27880;&#22270;&#29255;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#65288;1&#65289;&#23398;&#20064;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#23884;&#20837;&#32780;&#19981;&#30693;&#36947;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#65307;&#65288;2&#65289;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#34920;&#31034;&#26469;&#39044;&#27979;&#25513;&#30721;&#21333;&#35789;&#24182;&#26500;&#24314;&#25513;&#30721;&#22270;&#20687;&#34917;&#19969;&#32780;&#19981;&#23545;&#23427;&#20204;&#30340;&#21333;&#29420;&#34920;&#31034;&#36827;&#34892;&#36807;&#31243;&#26469;&#25193;&#23637;BERT&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;ITEm&#65306;&#23547;&#25214;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02084v2 Announce Type: replace-cross  Abstract: Product embedding serves as a cornerstone for a wide range of applications in eCommerce. The product embedding learned from multiple modalities shows significant improvement over that from a single modality, since different modalities provide complementary information. However, some modalities are more informatively dominant than others. How to teach a model to learn embedding from different modalities without neglecting information from the less dominant modality is challenging. We present an image-text embedding model (ITEm), an unsupervised learning method that is designed to better attend to image and text modalities. We extend BERT by (1) learning an embedding from text and image without knowing the regions of interest; (2) training a global representation to predict masked words and to construct masked image patches without their individual representations. We evaluate the pre-trained ITEm on two tasks: the search for ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#30740;&#31350;&#38646;&#27425;&#25552;&#31034;&#21644;&#23569;&#37327;&#19978;&#19979;&#25991;&#25552;&#31034;&#31561;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#19982;&#24403;&#21069;BLI&#26041;&#27861;&#30456;&#27604;&#65292;&#24182;&#22914;&#20309;&#36827;&#34892;&#34917;&#20805;&#12290;</title><link>https://arxiv.org/abs/2310.13995</link><description>&lt;p&gt;
&#20851;&#20110;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
On Bilingual Lexicon Induction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.13995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#30740;&#31350;&#38646;&#27425;&#25552;&#31034;&#21644;&#23569;&#37327;&#19978;&#19979;&#25991;&#25552;&#31034;&#31561;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#19982;&#24403;&#21069;BLI&#26041;&#27861;&#30456;&#27604;&#65292;&#24182;&#22914;&#20309;&#36827;&#34892;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#35821;&#35789;&#27719;&#35782;&#21035;&#65288;BLI&#65289;&#26159;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#65292;&#30446;&#21069;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#28982;&#20381;&#36182;&#20110;&#35745;&#31639;&#36328;&#35821;&#35328;&#21333;&#35789;&#34920;&#31034;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#33539;&#24335;&#36716;&#21464;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#26032;&#19968;&#20195;LLMs&#22312;&#21452;&#35821;&#35789;&#27719;&#24320;&#21457;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#20419;&#20351;&#21644;&#24494;&#35843;&#22810;&#35821;&#35328;LLMs&#65288;mLLMs&#65289;&#20197;&#36827;&#34892;BLI&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#19982;&#24403;&#21069;BLI&#26041;&#27861;&#30456;&#27604;&#22914;&#20309;&#20197;&#21450;&#22914;&#20309;&#34917;&#20805;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;1&#65289;&#29992;&#20110;&#26080;&#30417;&#30563;BLI&#30340;&#38646;&#27425;&#25552;&#31034;&#21644;2&#65289;&#20351;&#29992;&#19968;&#32452;&#31181;&#23376;&#32763;&#35793;&#23545;&#36827;&#34892;&#23569;&#37327;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#22343;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;LLM&#24494;&#35843;&#65292;&#20197;&#21450;3&#65289;&#23545;&#36739;&#23567;LLMs&#36827;&#34892;&#26631;&#20934;BLI&#23548;&#21521;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#19981;&#21516;&#22823;&#23567;&#65288;&#20174;0.3B&#21040;13B&#21442;&#25968;&#65289;&#30340;18&#20010;&#24320;&#28304;&#25991;&#26412;&#23545;&#25991;&#26412;mLLMs&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#28085;&#30422;&#20004;&#20010;&#26631;&#20934;BLI&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.13995v2 Announce Type: replace-cross  Abstract: Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a rang
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#21487;&#20197;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#28040;&#38500;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2310.06707</link><description>&lt;p&gt;
&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65306;&#21333;&#19968;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#29983;&#25104;&#21644;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#21487;&#20197;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#28040;&#38500;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#21518;&#39564;&#65288;MAP&#65289;&#35299;&#30721;&#26159;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290; &#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#22411;&#27010;&#29575;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#65292;&#20294;&#19981;&#33021;&#24635;&#26159;&#25104;&#31435;&#65292;&#29983;&#25104;&#36136;&#37327;&#21487;&#20197;&#36890;&#36807;&#35299;&#30721;&#26469;&#20248;&#21270;&#19968;&#20010;&#20197;&#24230;&#37327;&#25110;&#36136;&#37327;&#35780;&#20272;&#20449;&#21495;&#25903;&#25345;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25552;&#39640;&#65292;&#21363;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#25110;&#36136;&#37327;&#24863;&#30693;&#35299;&#30721;&#12290; &#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#22312;&#20110;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#39069;&#22806;&#30340;&#27169;&#22411;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#35745;&#31639;&#25928;&#29992;&#20989;&#25968;&#65292;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290; &#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#33258;&#24049;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#20174;&#32780;&#20351;NMT&#27169;&#22411;&#26412;&#36523;&#20855;&#22791;&#36136;&#37327;&#24863;&#30693;&#33021;&#21147;&#12290; &#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;MBR&#35299;&#30721;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#23610;&#23544;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06707v2 Announce Type: replace-cross  Abstract: Maximum-a-posteriori (MAP) decoding is the most widely used decoding strategy for neural machine translation (NMT) models. The underlying assumption is that model probability correlates well with human judgment, with better translations getting assigned a higher score by the model. However, research has shown that this assumption does not always hold, and generation quality can be improved by decoding to optimize a utility function backed by a metric or quality-estimation signal, as is done by Minimum Bayes Risk (MBR) or Quality-Aware decoding. The main disadvantage of these approaches is that they require an additional model to calculate the utility function during decoding, significantly increasing the computational cost. In this paper, we propose to make the NMT models themselves quality-aware by training them to estimate the quality of their own output. Using this approach for MBR decoding we can drastically reduce the size
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.02174</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#30340;&#35810;&#38382;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#20013;&#30340;&#25671;&#25670;
&lt;/p&gt;
&lt;p&gt;
Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02174
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24448;&#24448;&#22312;&#20854;&#21028;&#26029;&#19978;&#25671;&#25670;&#19981;&#23450;&#65292;&#21363;&#20351;&#21407;&#22987;&#21028;&#26029;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#25671;&#25670;&#23545;&#20110;&#29983;&#25104;&#21487;&#38752;&#22238;&#22797;&#21644;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#20197;&#21450;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#30830;&#35748;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#29992;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#30340;&#26694;&#26550;Unwavering-FQ&#65292;&#36890;&#36807;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#20445;&#25345;&#20854;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#22686;&#24378;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#36341;&#23454;&#39564;&#21644;&#29702;&#35770;&#27934;&#23519;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#65292;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.02124</link><description>&lt;p&gt;
&#25506;&#32034;LLM&#20195;&#29702;&#30340;&#21327;&#20316;&#26426;&#21046;&#65306;&#31038;&#20250;&#24515;&#29702;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02124
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#36341;&#23454;&#39564;&#21644;&#29702;&#35770;&#27934;&#23519;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#65292;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#31038;&#20250;&#29615;&#22659;&#20013;&#65292;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#36825;&#20123;NLP&#31995;&#32479;&#33021;&#21542;&#27169;&#20223;&#31867;&#20154;&#31867;&#30340;&#21327;&#20316;&#26234;&#33021;&#65292;&#22312;&#30001;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32452;&#25104;&#30340;&#22810;&#20195;&#29702;&#31038;&#20250;&#20013;&#65311;&#26412;&#25991;&#36890;&#36807;&#23558;&#23454;&#36341;&#23454;&#39564;&#19982;&#29702;&#35770;&#35266;&#28857;&#30456;&#32467;&#21512;&#65292;&#25506;&#31350;&#24403;&#20195;NLP&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#21046;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#30001;LLM&#20195;&#29702;&#32452;&#25104;&#30340;&#29420;&#29305;&#8220;&#31038;&#20250;&#8221;&#65292;&#27599;&#20010;&#20195;&#29702;&#20197;&#29305;&#23450;&#30340;&#8220;&#29305;&#36136;&#8221;&#65288;&#38543;&#21644;&#25110;&#36807;&#20110;&#33258;&#20449;&#65289;&#20026;&#29305;&#24449;&#65292;&#24182;&#19982;&#19981;&#21516;&#30340;&#8220;&#24605;&#32500;&#27169;&#24335;&#8221;&#65288;&#36777;&#35770;&#25110;&#21453;&#24605;&#65289;&#23637;&#24320;&#21327;&#20316;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#36825;&#20123;&#22810;&#20195;&#29702;&#31038;&#20250;&#65292;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#21327;&#20316;&#31574;&#30053;&#19981;&#20165;&#32988;&#36807;&#20808;&#21069;&#39030;&#23574;&#26041;&#27861;&#65292;&#32780;&#19988;&#20248;&#21270;&#20102;&#25928;&#29575;&#65288;&#20351;&#29992;&#26356;&#23569;&#30340;API&#20196;&#29260;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#35828;&#26126;LLM&#20195;&#29702;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02124v2 Announce Type: replace-cross  Abstract: As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents mani
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#25512;&#29702;&#65288;RoG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;LLMs&#19982;KGs&#21327;&#21516;&#24037;&#20316;&#65292;&#23454;&#29616;&#24544;&#23454;&#19988;&#21487;&#35299;&#37322;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2310.01061</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#25512;&#29702;&#65306;&#24544;&#23454;&#19988;&#21487;&#35299;&#37322;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01061
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#25512;&#29702;&#65288;RoG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;LLMs&#19982;KGs&#21327;&#21516;&#24037;&#20316;&#65292;&#23454;&#29616;&#24544;&#23454;&#19988;&#21487;&#35299;&#37322;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23427;&#20204;&#32570;&#20047;&#26368;&#26032;&#30693;&#35782;&#65292;&#32463;&#21382;&#24187;&#35273;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#38477;&#20302;&#20854;&#24615;&#33021;&#21644;&#21487;&#20449;&#24230;&#12290;&#30693;&#35782;&#22270;&#65288;KGs&#65289;&#20197;&#32467;&#26500;&#21270;&#26684;&#24335;&#25429;&#33719;&#20102;&#22823;&#37327;&#20107;&#23454;&#65292;&#20026;&#25512;&#29702;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#30693;&#35782;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;KG&#30340;LLM&#25512;&#29702;&#26041;&#27861;&#21482;&#23558;KGs&#35270;&#20026;&#20107;&#23454;&#30693;&#35782;&#24211;&#65292;&#24573;&#35270;&#20854;&#32467;&#26500;&#20449;&#24687;&#23545;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#25512;&#29702;&#65288;RoG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;LLMs&#19982;KGs&#21327;&#21516;&#24037;&#20316;&#65292;&#23454;&#29616;&#24544;&#23454;&#19988;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21010;-&#26816;&#32034;-&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;RoG&#39318;&#20808;&#29983;&#25104;&#30001;KGs&#20316;&#20026;&#24544;&#23454;&#35745;&#21010;&#30340;&#20851;&#31995;&#36335;&#24452;&#12290;&#36825;&#20123;&#35745;&#21010;&#28982;&#21518;&#29992;&#20110;&#26816;&#32034;&#26377;&#25928;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01061v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning p
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30450;&#28857;&#22312;&#20110;&#20854;&#23545;&#36229;&#21465;&#20107;&#35821;&#35328;&#20449;&#24687;&#30340;&#24573;&#35270;&#65292;&#30740;&#31350;&#25552;&#20986;&#32771;&#34385;&#27169;&#22411;&#22914;&#20309;&#24863;&#30693;&#35821;&#35328;&#20449;&#24687;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2306.06794</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30450;&#28857;&#65306;&#36229;&#21465;&#20107;&#35821;&#35328;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
A blind spot for large language models: Supradiegetic linguistic information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.06794
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30450;&#28857;&#22312;&#20110;&#20854;&#23545;&#36229;&#21465;&#20107;&#35821;&#35328;&#20449;&#24687;&#30340;&#24573;&#35270;&#65292;&#30740;&#31350;&#25552;&#20986;&#32771;&#34385;&#27169;&#22411;&#22914;&#20309;&#24863;&#30693;&#35821;&#35328;&#20449;&#24687;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21453;&#26144;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#28145;&#21051;&#21464;&#38761;&#65292;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#29978;&#33267;&#20196;&#20154;&#38663;&#24778;&#30340;&#31867;&#20154;&#35821;&#35328;&#27969;&#21033;&#24230;&#12290;&#23427;&#20204;&#30446;&#21069;&#21644;&#28508;&#22312;&#30340;&#33021;&#21147;&#33539;&#22260;&#26159;&#19968;&#20010;&#31215;&#26497;&#25506;&#35752;&#30340;&#39046;&#22495;&#65292;&#32477;&#38750;&#20165;&#38480;&#20110;&#31185;&#30740;&#20154;&#21592;&#12290;&#20154;&#20204;&#36890;&#24120;&#23558;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#26694;&#23450;&#20026;&#8220;&#25991;&#26412;&#8221;&#29978;&#33267;&#8220;&#35821;&#35328;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#35821;&#35328;&#23398;&#12289;&#20307;&#29616;&#35748;&#30693;&#12289;&#35748;&#30693;&#31185;&#23398;&#12289;&#25968;&#23398;&#21644;&#21382;&#21490;&#31561;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#20180;&#32454;&#23457;&#35270;&#36825;&#19968;&#26694;&#26550;&#30340;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#32771;&#34385;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#26159;&#20160;&#20040;&#24863;&#35273;&#65292;&#27491;&#22914;&#32435;&#26684;&#23572;&#21487;&#33021;&#20250;&#35828;&#30340;&#37027;&#26679;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#20854;&#25972;&#20307;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#65292;&#20854;&#25509;&#21463;&#30340;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#34987;&#26377;&#30410;&#22320;&#37325;&#26032;&#26500;&#24605;&#20026;&#23545;&#35821;&#35328;&#20013;&#32534;&#30721;&#30340;&#21465;&#20107;&#20449;&#24687;&#30340;&#25509;&#35302;&#65292;&#20854;&#32570;&#38519;&#21487;&#20197;&#34987;&#37325;&#26032;&#26500;&#24605;&#20026;&#23545;&#36825;&#20123;&#20449;&#24687;&#30340;&#26080;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.06794v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) like ChatGPT reflect profound changes in the field of Artificial Intelligence, achieving a linguistic fluency that is impressively, even shockingly, human-like. The extent of their current and potential capabilities is an active area of investigation by no means limited to scientific researchers. It is common for people to frame the training data for LLMs as "text" or even "language". We examine the details of this framing using ideas from several areas, including linguistics, embodied cognition, cognitive science, mathematics, and history. We propose that considering what it is like to be an LLM like ChatGPT, as Nagel might have put it, can help us gain insight into its capabilities in general, and in particular, that its exposure to linguistic training data can be productively reframed as exposure to the diegetic information encoded in language, and its deficits can be reframed as ignorance of ext
&lt;/p&gt;</description></item><item><title>SmartTrim &#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#24182;&#20462;&#21098;&#27599;&#20010;&#23618;&#20013;&#30340;&#20887;&#20313;&#26631;&#35760;&#34920;&#31034;&#21644;&#27880;&#24847;&#21147;&#22836;&#26469;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2305.15033</link><description>&lt;p&gt;
SmartTrim&#65306;&#29992;&#20110;&#39640;&#25928;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#26631;&#35760;&#21644;&#27880;&#24847;&#21147;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
SmartTrim: Adaptive Tokens and Attention Pruning for Efficient Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15033
&lt;/p&gt;
&lt;p&gt;
SmartTrim &#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21152;&#36895;&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#24182;&#20462;&#21098;&#27599;&#20010;&#23618;&#20013;&#30340;&#20887;&#20313;&#26631;&#35760;&#34920;&#31034;&#21644;&#27880;&#24847;&#21147;&#22836;&#26469;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;Transformer&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#36755;&#20837;&#21644;&#21442;&#25968;&#20013;&#23384;&#22312;&#20887;&#20313;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25928;&#29575;&#12290;&#37492;&#20110;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SmartTrim&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21152;&#36895;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#27599;&#20010;&#23454;&#20363;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#36731;&#37327;&#32423;&#27169;&#22359;&#38598;&#25104;&#21040;&#21407;&#22987;&#20027;&#24178;&#20013;&#65292;&#20197;&#35782;&#21035;&#24182;&#20462;&#21098;&#27599;&#20010;&#23618;&#20013;&#30340;&#20887;&#20313;&#26631;&#35760;&#34920;&#31034;&#21644;&#27880;&#24847;&#21147;&#22836;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#25105;&#33976;&#39311;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#20462;&#21098;&#27169;&#22411;&#21644;&#20854;&#23436;&#20840;&#23481;&#37327;&#23545;&#24212;&#29289;&#20043;&#38388;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#19981;&#26029;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.15033v2 Announce Type: replace  Abstract: Despite achieving remarkable performance on various vision-language tasks, Transformer-based Vision-Language Models (VLMs) suffer from redundancy in inputs and parameters, significantly hampering their efficiency in real-world applications. Moreover, the degree of redundancy in token representations and model parameters, such as attention heads, varies significantly for different inputs. In light of the challenges, we propose SmartTrim, an adaptive acceleration framework for VLMs, which adjusts the computational overhead per instance. Specifically, we integrate lightweight modules into the original backbone to identify and prune redundant token representations and attention heads within each layer. Furthermore, we devise a self-distillation strategy to enhance the consistency between the predictions of the pruned model and its fully-capacity counterpart. Experimental results across various vision-language tasks consistently demonstra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#27491;&#24335;&#21270;&#21644;&#24050;&#30693;&#36234;&#29425;&#20998;&#31867;&#27861;&#20197;&#22635;&#34917;&#23545;&#21830;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#36234;&#29425;&#25915;&#20987;&#30340;&#32570;&#20047;&#30740;&#31350;&#65292;&#35843;&#26597;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#21450;&#20854;&#22312;&#24320;&#28304;&#21644;&#21830;&#29992;LLMs&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.14965</link><description>&lt;p&gt;
&#27450;&#39575;LLMs&#35753;&#20854;&#19981;&#36981;&#20174;&#65306;&#27491;&#24335;&#21270;&#12289;&#20998;&#26512;&#21644;&#26816;&#27979;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#27491;&#24335;&#21270;&#21644;&#24050;&#30693;&#36234;&#29425;&#20998;&#31867;&#27861;&#20197;&#22635;&#34917;&#23545;&#21830;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#36234;&#29425;&#25915;&#20987;&#30340;&#32570;&#20047;&#30740;&#31350;&#65292;&#35843;&#26597;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#21450;&#20854;&#22312;&#24320;&#28304;&#21644;&#21830;&#29992;LLMs&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#21830;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25506;&#32034;&#34920;&#26126;&#65292;&#38750;&#19987;&#23478;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25805;&#32437;&#20182;&#20204;&#30340;&#25552;&#31034;&#26469;&#36234;&#29425;LLMs&#65307;&#23548;&#33268;&#36864;&#21270;&#30340;&#36755;&#20986;&#34892;&#20026;&#12289;&#38544;&#31169;&#19982;&#23433;&#20840;&#28431;&#27934;&#12289;&#20882;&#29359;&#24615;&#36755;&#20986;&#20197;&#21450;&#36829;&#21453;&#20869;&#23481;&#30417;&#31649;&#25919;&#31574;&#12290;&#26377;&#38480;&#30340;&#30740;&#31350;&#24050;&#36827;&#34892;&#20102;&#23545;&#36825;&#20123;&#25915;&#20987;&#21450;&#20854;&#32531;&#35299;&#25514;&#26045;&#30340;&#27491;&#24335;&#21270;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#24418;&#24335;&#21270;&#25551;&#36848;&#21644;&#24050;&#30693;&#65288;&#21450;&#21487;&#33021;&#30340;&#65289;&#36234;&#29425;&#20998;&#31867;&#27861;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35843;&#26597;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#21450;&#20854;&#22312;&#24320;&#28304;&#21644;&#21830;&#29992;LLMs&#65288;&#22914;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#12289;OPT&#12289;BLOOM&#21644;FLAN-T5-XXL&#65289;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#36234;&#29425;&#26816;&#27979;&#22312;&#38024;&#23545;&#24050;&#30693;&#25915;&#20987;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;4&#39033;&#20219;&#21153;&#30340;3700&#20010;&#36234;&#29425;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;&#38543;&#30528;&#27169;&#22411;&#36755;&#20986;&#19968;&#36215;&#20844;&#24320;&#36825;&#19968;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14965v2 Announce Type: replace  Abstract: Recent explorations with commercial Large Language Models (LLMs) have shown that non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies. Limited studies have been conducted to formalize and analyze these attacks and their mitigations. We bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. We survey existing jailbreak methods and their effectiveness on open-source and commercial LLMs (such as GPT-based models, OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak detection in terms of their effectiveness against known attacks. For our analysis, we collect a dataset of 3700 jailbreak prompts across 4 tasks. We will make the dataset public along with the model outputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHEAT&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#65307;&#30740;&#31350;&#34920;&#26126;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#26159;&#21487;&#20197;&#34987;&#26816;&#27979;&#20986;&#26469;&#30340;&#65292;&#20294;&#38543;&#30528;&#20154;&#31867;&#21442;&#19982;&#30340;&#22686;&#21152;&#65292;&#26816;&#27979;&#30340;&#38590;&#24230;&#20063;&#22312;&#22686;&#21152;&#12290;</title><link>https://arxiv.org/abs/2304.12008</link><description>&lt;p&gt;
CHEAT: &#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#25688;&#35201;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.12008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHEAT&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#65307;&#30740;&#31350;&#34920;&#26126;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#26159;&#21487;&#20197;&#34987;&#26816;&#27979;&#20986;&#26469;&#30340;&#65292;&#20294;&#38543;&#30528;&#20154;&#31867;&#21442;&#19982;&#30340;&#22686;&#21152;&#65292;&#26816;&#27979;&#30340;&#38590;&#24230;&#20063;&#22312;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24378;&#22823;&#33021;&#21147;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#24694;&#24847;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;ChatGPT&#21512;&#25104;&#34394;&#20551;&#30340;&#23398;&#26415;&#20869;&#23481;&#65292;&#36825;&#23545;&#23398;&#26415;&#30340;&#20005;&#35880;&#24615;&#21644;&#21407;&#21019;&#24615;&#36896;&#25104;&#20102;&#26497;&#22823;&#30340;&#20260;&#23475;&#12290;&#26377;&#24517;&#35201;&#24320;&#21457;ChatGPT&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#31639;&#27861;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;ChatGPT&#23545;&#23398;&#26415;&#30028;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;ChatGPT&#29983;&#25104;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;CHEAT&#65289;&#26469;&#25903;&#25345;&#26816;&#27979;&#31639;&#27861;&#30340;&#24320;&#21457;&#12290;&#20854;&#20013;&#65292;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#21253;&#21547;35,304&#20010;&#21512;&#25104;&#25688;&#35201;&#65292;&#20854;&#20195;&#34920;&#26377;Generation, Polish, &#21644; Mix&#12290;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#25991;&#26412;&#21512;&#25104;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#26159;&#21487;&#20197;&#34987;&#26816;&#27979;&#20986;&#26469;&#30340;&#65292;&#32780;&#26816;&#27979;&#38590;&#24230;&#38543;&#30528;&#20154;&#31867;&#21442;&#19982;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;https://gith&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.12008v2 Announce Type: replace  Abstract: The powerful ability of ChatGPT has caused widespread concern in the academic community. Malicious users could synthesize dummy academic content through ChatGPT, which is extremely harmful to academic rigor and originality. The need to develop ChatGPT-written content detection algorithms call for large-scale datasets. In this paper, we initially investigate the possible negative impact of ChatGPT on academia,and present a large-scale CHatGPT-writtEn AbsTract dataset (CHEAT) to support the development of detection algorithms. In particular, the ChatGPT-written abstract dataset contains 35,304 synthetic abstracts, with Generation, Polish, and Mix as prominent representatives. Based on these data, we perform a thorough analysis of the existing text synthesis detection algorithms. We show that ChatGPT-written abstracts are detectable, while the detection difficulty increases with human involvement.Our dataset is available in https://gith
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26102;&#38388;&#24310;&#36831;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#26631;&#28857;&#20462;&#22797;&#30340;&#39640;&#25928;&#38598;&#25104;&#26041;&#27861;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20339;&#27169;&#22411;&#25552;&#39640;&#20102;1.0&#20010;F1&#20998;&#25968;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#26356;&#23569;&#30340;&#25512;&#29702;&#32593;&#32476;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2302.13376</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#24310;&#36831;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#22810;&#27169;&#24577;&#26631;&#28857;&#20462;&#22797;&#30340;&#39640;&#25928;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Ensemble for Multimodal Punctuation Restoration using Time-Delay Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.13376
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#24310;&#36831;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#26631;&#28857;&#20462;&#22797;&#30340;&#39640;&#25928;&#38598;&#25104;&#26041;&#27861;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20339;&#27169;&#22411;&#25552;&#39640;&#20102;1.0&#20010;F1&#20998;&#25968;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#26356;&#23569;&#30340;&#25512;&#29702;&#32593;&#32476;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#28857;&#20462;&#22797;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#21518;&#22788;&#29702;&#36807;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#23545;&#27169;&#22411;&#25928;&#29575;&#30340;&#35201;&#27714;&#26159;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EfficientPunct&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#26102;&#38388;&#24310;&#36831;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21040;&#21313;&#20998;&#20043;&#19968;&#30340;&#25512;&#29702;&#32593;&#32476;&#21442;&#25968;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#27169;&#22411;1.0&#20010;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#31616;&#21270;&#20102;&#35821;&#38899;&#35782;&#21035;&#22120;&#65292;&#20197;&#26377;&#25928;&#22320;&#36755;&#20986;&#29992;&#20110;&#26631;&#28857;&#20462;&#22797;&#30340;&#38544;&#34255;&#23618;&#22768;&#23398;&#23884;&#20837;&#65292;&#21516;&#26102;&#20351;&#29992;BERT&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#25991;&#26412;&#23884;&#20837;&#12290;&#36890;&#36807;&#20351;&#29992;&#24378;&#21046;&#23545;&#40784;&#21644;&#26102;&#38388;&#21367;&#31215;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#24517;&#35201;&#24615;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;EfficientPunct&#36890;&#36807;&#19968;&#20010;&#38598;&#25104;&#26041;&#27861;&#65292;&#23545;BERT&#32431;&#31929;&#22522;&#20110;&#35821;&#35328;&#30340;&#39044;&#27979;&#30340;&#26435;&#37325;&#30053;&#39640;&#20110;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#39044;&#27979;&#65292;&#26641;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://githu
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.13376v2 Announce Type: replace  Abstract: Punctuation restoration plays an essential role in the post-processing procedure of automatic speech recognition, but model efficiency is a key requirement for this task. To that end, we present EfficientPunct, an ensemble method with a multimodal time-delay neural network that outperforms the current best model by 1.0 F1 points, using less than a tenth of its inference network parameters. We streamline a speech recognizer to efficiently output hidden layer acoustic embeddings for punctuation restoration, as well as BERT to extract meaningful text embeddings. By using forced alignment and temporal convolutions, we eliminate the need for attention-based fusion, greatly increasing computational efficiency and raising performance. EfficientPunct sets a new state of the art with an ensemble that weights BERT's purely language-based predictions slightly more than the multimodal network's predictions. Our code is available at https://githu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Automate-CoT&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#33258;&#21160;&#22686;&#21152;&#21512;&#29702;&#38142;&#65292;&#24182;&#20462;&#21098;&#20302;&#36136;&#37327;&#38142;&#65292;&#26500;&#24314;&#22522;&#20110;&#26631;&#31614;&#30340;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#38142;&#30340;&#20505;&#36873;&#27744;&#65292;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2302.12822</link><description>&lt;p&gt;
&#29992;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#38142;&#24335;&#24605;&#32500;&#30340;&#33258;&#21160;&#25552;&#31034;&#22686;&#24378;&#19982;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.12822
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Automate-CoT&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#33258;&#21160;&#22686;&#21152;&#21512;&#29702;&#38142;&#65292;&#24182;&#20462;&#21098;&#20302;&#36136;&#37327;&#38142;&#65292;&#26500;&#24314;&#22522;&#20110;&#26631;&#31614;&#30340;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#38142;&#30340;&#20505;&#36873;&#27744;&#65292;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-thought prompting&#65288;CoT&#65289;&#25512;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;CoT&#30740;&#31350;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#20154;&#24037;&#27880;&#37322;&#30340;&#21512;&#29702;&#38142;&#65292;&#20197;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#20294;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#21512;&#29702;&#38142;&#30340;&#24212;&#29992;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#36825;&#20026;CoT&#25552;&#31034;&#24212;&#29992;&#20110;&#36825;&#20123;&#36890;&#29992;&#20219;&#21153;&#24102;&#26469;&#20102;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31574;&#30053;&#65292;Automate-CoT&#65288;&#33258;&#21160;&#25552;&#31034;&#22686;&#24378;&#19982;&#36873;&#25321;&#19982;&#38142;&#24605;&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#22686;&#24378;&#21512;&#29702;&#38142;&#65292;&#28982;&#21518;&#20462;&#21098;&#20302;&#36136;&#37327;&#38142;&#65292;&#22522;&#20110;&#26631;&#31614;&#26500;&#24314;&#22522;&#20110;&#26426;&#22120;&#29983;&#25104;&#30340;&#29702;&#30001;&#38142;&#30340;&#20505;&#36873;&#27744;&#12290;&#26368;&#21518;&#65292;&#23427;&#36873;&#25321;&#20102;&#20960;&#20010;&#29702;&#30001;&#38142;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.12822v2 Announce Type: replace  Abstract: Chain-of-thought prompting (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in arithmetic, commonsense, and symbolic reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt the language model, which poses challenges for real-world applications where labeled training data is available without human-annotated rational chains. This creates barriers to applications of CoT prompting to these general tasks. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoTs by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38169;&#35823;&#40065;&#26834;&#30340;&#21363;&#25554;&#21363;&#29992;&#26816;&#32034;&#26041;&#27861;RERIC&#65292;&#21487;&#30452;&#25509;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#27169;&#22411;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#31181;&#34920;&#31034;&#24418;&#24335;&#25552;&#39640;&#20102;&#26816;&#32034;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2211.07843</link><description>&lt;p&gt;
&#38754;&#21521;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#30340;&#38169;&#35823;&#40065;&#26834;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Error-Robust Retrieval for Chinese Spelling Check
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.07843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38169;&#35823;&#40065;&#26834;&#30340;&#21363;&#25554;&#21363;&#29992;&#26816;&#32034;&#26041;&#27861;RERIC&#65292;&#21487;&#30452;&#25509;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#27169;&#22411;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#31181;&#34920;&#31034;&#24418;&#24335;&#25552;&#39640;&#20102;&#26816;&#32034;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#26088;&#22312;&#26816;&#27979;&#21644;&#32416;&#27491;&#20013;&#25991;&#35821;&#22659;&#20013;&#30340;&#38169;&#35823;&#26631;&#35760;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30528;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#21644;&#20197;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#23454;&#38469;&#19978;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38754;&#21521;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#30340;&#38169;&#35823;&#40065;&#26834;&#20449;&#24687;&#30340;&#21363;&#25554;&#21363;&#29992;&#26816;&#32034;&#26041;&#27861;&#65288;RERIC&#65289;&#65292;&#21487;&#30452;&#25509;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;CSC&#27169;&#22411;&#12290;&#26816;&#32034;&#30340;&#25968;&#25454;&#23384;&#20648;&#23436;&#20840;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#65292;&#24182;&#26681;&#25454;CSC&#30340;&#29305;&#24615;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#26816;&#32034;&#36807;&#31243;&#20013;&#22312;&#26597;&#35810;&#21644;&#20851;&#38190;&#23383;&#30340;&#35745;&#31639;&#20013;&#20351;&#29992;&#34701;&#21512;&#38899;&#31995;&#12289;&#24418;&#24577;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#20197;&#22686;&#24378;&#23545;&#28508;&#22312;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#21028;&#26029;&#26816;&#32034;&#21040;&#30340;&#20505;&#36873;&#39033;&#65292;&#20250;&#32771;&#34385;&#35201;&#26816;&#26597;&#30340;&#26631;&#35760;&#21608;&#22260;&#30340;n-gram&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.07843v2 Announce Type: replace  Abstract: Chinese Spelling Check (CSC) aims to detect and correct error tokens in Chinese contexts, which has a wide range of applications. However, it is confronted with the challenges of insufficient annotated data and the issue that previous methods may actually not fully leverage the existing datasets. In this paper, we introduce our plug-and-play retrieval method with error-robust information for Chinese Spelling Check (RERIC), which can be directly applied to existing CSC models. The datastore for retrieval is built completely based on the training data, with elaborate designs according to the characteristics of CSC. Specifically, we employ multimodal representations that fuse phonetic, morphologic, and contextual information in the calculation of query and key during retrieval to enhance robustness against potential errors. Furthermore, in order to better judge the retrieved candidates, the n-gram surrounding the token to be checked is 
&lt;/p&gt;</description></item><item><title>OmDet &#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24847;&#35782;&#30340;&#30446;&#26631;&#26816;&#27979;&#26550;&#26500;&#21644;&#21019;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#21033;&#29992;&#22810;&#25968;&#25454;&#38598;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#20174;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#31215;&#32047;&#8220;&#35270;&#35273;&#35789;&#27719;&#8221;&#65292;&#23454;&#29616;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#22810;&#27169;&#24577;&#26816;&#27979;&#32593;&#32476;&#65292;&#22312;&#30446;&#26631;&#26816;&#27979;&#12289;&#24320;&#25918;&#24335;&#35789;&#27719;&#26816;&#27979;&#21644;&#30701;&#35821;&#23450;&#20301;&#31561;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2209.05946</link><description>&lt;p&gt;
OmDet: &#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#22810;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#19982;&#22810;&#27169;&#24335;&#26816;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
OmDet: Large-scale vision-language multi-dataset pre-training with multimodal detection network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.05946
&lt;/p&gt;
&lt;p&gt;
OmDet &#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#24847;&#35782;&#30340;&#30446;&#26631;&#26816;&#27979;&#26550;&#26500;&#21644;&#21019;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#21033;&#29992;&#22810;&#25968;&#25454;&#38598;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#20174;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#31215;&#32047;&#8220;&#35270;&#35273;&#35789;&#27719;&#8221;&#65292;&#23454;&#29616;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#22810;&#27169;&#24577;&#26816;&#27979;&#32593;&#32476;&#65292;&#22312;&#30446;&#26631;&#26816;&#27979;&#12289;&#24320;&#25918;&#24335;&#35789;&#27719;&#26816;&#27979;&#21644;&#30701;&#35821;&#23450;&#20301;&#31561;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#22312;&#24320;&#25918;&#24335;&#35789;&#27719;&#21644;&#24320;&#25918;&#24335;&#22330;&#26223;&#20013;&#30340;&#36827;&#23637;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#39033;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OmDet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#35821;&#35328;&#24847;&#35782;&#30340;&#30446;&#26631;&#26816;&#27979;&#26550;&#26500;&#65292;&#20197;&#21450;&#19968;&#31181;&#21019;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#21644;&#22810;&#25968;&#25454;&#38598;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;OmDet&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#36890;&#29992;&#30693;&#35782;&#34920;&#31034;&#65292;&#20174;&#21508;&#31181;&#25968;&#25454;&#38598;&#20013;&#31215;&#32047;&#8220;&#35270;&#35273;&#35789;&#27719;&#8221;&#65292;&#23558;&#20219;&#21153;&#32479;&#19968;&#20026;&#19968;&#20010;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#26816;&#27979;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#26816;&#27979;&#32593;&#32476;&#65288;MDN&#65289;&#20811;&#26381;&#20102;&#22810;&#25968;&#25454;&#38598;&#32852;&#21512;&#35757;&#32451;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#26631;&#31614;&#20998;&#31867;&#21512;&#24182;&#30340;&#24773;&#20917;&#19979;&#27867;&#21270;&#21040;&#20247;&#22810;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;OmDet&#22312;&#37326;&#22806;&#30446;&#26631;&#26816;&#27979;&#12289;&#24320;&#25918;&#24335;&#35789;&#27719;&#26816;&#27979;&#21644;&#30701;&#35821;&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28040;&#34701;&#30740;&#31350;&#25581;&#31034;&#20102;&#25193;&#23637;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.05946v2 Announce Type: replace-cross  Abstract: The advancement of object detection (OD) in open-vocabulary and open-world scenarios is a critical challenge in computer vision. This work introduces OmDet, a novel language-aware object detection architecture, and an innovative training mechanism that harnesses continual learning and multi-dataset vision-language pre-training. Leveraging natural language as a universal knowledge representation, OmDet accumulates a "visual vocabulary" from diverse datasets, unifying the task as a language-conditioned detection framework. Our multimodal detection network (MDN) overcomes the challenges of multi-dataset joint training and generalizes to numerous training datasets without manual label taxonomy merging. We demonstrate superior performance of OmDet over strong baselines in object detection in the wild, open-vocabulary detection, and phrase grounding, achieving state-of-the-art results. Ablation studies reveal the impact of scaling th
&lt;/p&gt;</description></item><item><title>Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20840;&#38754;&#22810;&#26679;&#30340;&#32593;&#39029;&#38598;&#21512;&#65292;&#20026;&#35299;&#20915;&#32593;&#32476;&#33258;&#21160;&#21270;&#31639;&#27861;&#35774;&#35745;&#20013;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2111.02168</link><description>&lt;p&gt;
Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#65306;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#20803;&#32032;&#25552;&#21517;
&lt;/p&gt;
&lt;p&gt;
The Klarna Product Page Dataset: Web Element Nomination with Graph Neural Networks and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.02168
&lt;/p&gt;
&lt;p&gt;
Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#20840;&#38754;&#22810;&#26679;&#30340;&#32593;&#39029;&#38598;&#21512;&#65292;&#20026;&#35299;&#20915;&#32593;&#32476;&#33258;&#21160;&#21270;&#31639;&#27861;&#35774;&#35745;&#20013;&#25968;&#25454;&#38598;&#31232;&#32570;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Web&#33258;&#21160;&#21270;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#29992;&#25143;&#19982;&#25968;&#23383;&#19990;&#30028;&#30340;&#20114;&#21160;&#26041;&#24335;&#65292;&#36890;&#36807;&#22797;&#26434;&#30340;&#35745;&#31639;&#26041;&#27861;&#25552;&#20379;&#26080;&#19982;&#20262;&#27604;&#30340;&#24110;&#21161;&#65292;&#31616;&#21270;&#20219;&#21153;&#12290;&#22312;&#36825;&#19968;&#28436;&#36827;&#36807;&#31243;&#20013;&#65292;&#32593;&#32476;&#20803;&#32032;&#25552;&#21517;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#28041;&#21450;&#35782;&#21035;&#32593;&#39029;&#19978;&#30340;&#29420;&#29305;&#20803;&#32032;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#32593;&#32476;&#33258;&#21160;&#21270;&#31639;&#27861;&#35774;&#35745;&#30340;&#21457;&#23637;&#21463;&#21040;&#20840;&#38754;&#21644;&#30495;&#23454;&#21453;&#26144;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#22797;&#26434;&#24615;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#22810;&#26679;&#30340;&#32593;&#39029;&#38598;&#21512;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;8,175&#20010;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#30340;51,701&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#20135;&#21697;&#39029;&#38754;&#65292;&#35206;&#30422;&#20102;&#20843;&#20010;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#38468;&#24102;&#20102;&#19968;&#32452;&#28210;&#26579;&#39029;&#38754;&#25130;&#22270;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#24320;&#22987;&#30740;&#31350;Klarna&#20135;&#21697;&#39029;&#38754;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.02168v4 Announce Type: replace-cross  Abstract: Web automation holds the potential to revolutionize how users interact with the digital world, offering unparalleled assistance and simplifying tasks via sophisticated computational methods. Central to this evolution is the web element nomination task, which entails identifying unique elements on webpages. Unfortunately, the development of algorithmic designs for web automation is hampered by the scarcity of comprehensive and realistic datasets that reflect the complexity faced by real-world applications on the Web. To address this, we introduce the Klarna Product Page Dataset, a comprehensive and diverse collection of webpages that surpasses existing datasets in richness and variety. The dataset features 51,701 manually labeled product pages from 8,175 e-commerce websites across eight geographic regions, accompanied by a dataset of rendered page screenshots. To initiate research on the Klarna Product Page Dataset, we empirical
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.16553</link><description>&lt;p&gt;
SelectLLM&#65306;LLMs&#33021;&#21542;&#36873;&#25321;&#37325;&#35201;&#30340;&#25351;&#20196;&#36827;&#34892;&#27880;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#20351;&#27169;&#22411;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#19968;&#23567;&#32452;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#22823;&#37327;&#26356;&#22024;&#26434;&#30340;&#25351;&#20196;&#12290;&#30001;&#20110;&#25351;&#20196;&#26159;&#26080;&#26631;&#31614;&#30340;&#65292;&#19988;&#21709;&#24212;&#26159;&#33258;&#28982;&#25991;&#26412;&#65292;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36873;&#25321;&#26080;&#26631;&#31614;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#65292;&#31216;&#20026;SelectLLM&#65292;&#23427;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#39640;&#32423;&#24605;&#24819;&#26159;&#21033;&#29992;LLMs&#36890;&#36807;&#25552;&#31034;&#26469;&#20272;&#35745;&#27599;&#20010;&#25351;&#20196;&#22312;&#27809;&#26377;&#30456;&#24212;&#26631;&#31614;&#65288;&#21363;&#21709;&#24212;&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#12290;SelectLLM&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#65288;&#20363;&#22914;CoreSet&#65289;&#23558;&#26080;&#26631;&#31614;&#25351;&#20196;&#21010;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#65292;&#28982;&#21518;&#25552;&#31034;LLMs&#22312;&#20854;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
&lt;/p&gt;</description></item><item><title>HiFT&#26159;&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#23454;&#29616;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15207</link><description>&lt;p&gt;
HiFT:&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy. (arXiv:2401.15207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15207
&lt;/p&gt;
&lt;p&gt;
HiFT&#26159;&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#23454;&#29616;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#38271;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21442;&#25968;&#38656;&#35201;&#21344;&#29992;&#22823;&#37327;GPU&#20869;&#23384;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#22120;&#20197;&#33410;&#30465;GPU&#20869;&#23384;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#38750;&#38646;&#38454;&#20248;&#21270;&#22120;&#22312;&#22823;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#19978;&#26356;&#23481;&#26131;&#25910;&#25947;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29420;&#31435;&#20110;&#20248;&#21270;&#22120;&#30340;&#31471;&#21040;&#31471;&#20998;&#23618;&#24494;&#35843;&#31574;&#30053;HiFT&#65292;&#23427;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#12290; HiFT&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#23384;&#20648;&#22312;GPU&#20869;&#23384;&#20013;&#30340;&#26799;&#24230;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#21442;&#25968;&#30340;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#20869;&#23384;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;HiFT&#23454;&#29616;&#20102;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#65288;2&#65289;HiFT&#25903;&#25345;&#21253;&#25324;&#22312;&#20869;&#30340;&#21508;&#31181;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Full-parameter fine-tuning has become the go-to choice for adapting language models (LMs) to downstream tasks due to its excellent performance. As LMs grow in size, fine-tuning the full parameters of LMs requires a prohibitively large amount of GPU memory. Existing approaches utilize zeroth-order optimizer to conserve GPU memory, which can potentially compromise the performance of LMs as non-zero order optimizers tend to converge more readily on most downstream tasks. In this paper, we propose a novel optimizer-independent end-to-end hierarchical fine-tuning strategy, HiFT, which only updates a subset of parameters at each training step. HiFT can significantly reduce the amount of gradients and optimizer state parameters residing in GPU memory at the same time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT achieves comparable performance to parameter-efficient fine-tuning and standard full parameter fine-tuning. (2) HiFT supports various optimizers including
&lt;/p&gt;</description></item><item><title>"Airavata"&#26159;&#19968;&#20010;&#38024;&#23545;&#21360;&#22320;&#35821;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#65292;&#36890;&#36807;&#24494;&#35843;OpenHathi&#21644;IndicInstruct&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#21327;&#21161;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#35745;&#21010;&#25193;&#23637;&#21040;&#25152;&#26377;22&#31181;&#35745;&#21010;Indic&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2401.15006</link><description>&lt;p&gt;
Airavata: &#24341;&#20837;&#38024;&#23545;&#21360;&#22320;&#35821;&#25351;&#20196;&#35843;&#25972;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Airavata: Introducing Hindi Instruction-tuned LLM. (arXiv:2401.15006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15006
&lt;/p&gt;
&lt;p&gt;
"Airavata"&#26159;&#19968;&#20010;&#38024;&#23545;&#21360;&#22320;&#35821;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#65292;&#36890;&#36807;&#24494;&#35843;OpenHathi&#21644;IndicInstruct&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#21327;&#21161;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#35745;&#21010;&#25193;&#23637;&#21040;&#25152;&#26377;22&#31181;&#35745;&#21010;Indic&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23459;&#24067;&#39318;&#27425;&#21457;&#24067;&#20102;"Airavata"&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#21360;&#22320;&#35821;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#12290;&#36890;&#36807;&#23558;OpenHathi&#19982;&#21508;&#31181;&#25351;&#20196;&#35843;&#25972;&#30340;&#21360;&#22320;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;Airavata&#26356;&#36866;&#21512;&#36741;&#21161;&#20219;&#21153;&#12290;&#38500;&#20102;&#27169;&#22411;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;IndicInstruct&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#32452;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;Indic LLM&#30340;&#22810;&#26679;&#21270;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35780;&#20272;&#22522;&#20934;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;LLM&#22312;&#21360;&#22320;&#35821;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#65292;Airavata&#25903;&#25345;&#21360;&#22320;&#35821;&#65292;&#20294;&#25105;&#20204;&#35745;&#21010;&#23558;&#20854;&#25193;&#23637;&#21040;&#25152;&#26377;22&#31181;&#35745;&#21010;Indic&#35821;&#35328;&#12290;&#24744;&#21487;&#20197;&#22312;https://ai4bharat.github.io/airavata&#19978;&#35775;&#38382;&#25152;&#26377;&#24037;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We announce the initial release of "Airavata," an instruction-tuned LLM for Hindi. Airavata was created by fine-tuning OpenHathi with diverse, instruction-tuning Hindi datasets to make it better suited for assistive tasks. Along with the model, we also share the IndicInstruct dataset, which is a collection of diverse instruction-tuning datasets to enable further research for Indic LLMs. Additionally, we present evaluation benchmarks and a framework for assessing LLM performance across tasks in Hindi. Currently, Airavata supports Hindi, but we plan to expand this to all 22 scheduled Indic languages. You can access all artifacts at https://ai4bharat.github.io/airavata.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#39033;&#30446;&#12289;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#36827;&#34892;&#39033;&#30446;&#25512;&#33616;&#65292;&#24182;&#29983;&#25104;&#23545;&#35805;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14194</link><description>&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Conversational Recommender System as a Language Processing Task. (arXiv:2401.14194v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#39033;&#30446;&#12289;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#36827;&#34892;&#39033;&#30446;&#25512;&#33616;&#65292;&#24182;&#29983;&#25104;&#23545;&#35805;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#26469;&#21521;&#29992;&#25143;&#25512;&#33616;&#30456;&#20851;&#30340;&#39033;&#30446;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#20379;&#39033;&#30446;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#29983;&#25104;&#65292;&#20197;&#21450;&#21033;&#29992;&#25512;&#33616;&#27169;&#22359;&#36827;&#34892;&#30456;&#20851;&#39033;&#30446;&#30340;&#25490;&#24207;&#12290;&#36825;&#31181;&#22810;&#32452;&#20214;&#30340;&#32452;&#21512;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#32321;&#29712;&#65292;&#24182;&#19988;&#23548;&#33268;&#23545;&#35805;&#29983;&#25104;&#21644;&#39033;&#30446;&#25512;&#33616;&#20043;&#38388;&#30340;&#35821;&#20041;&#19981;&#37197;&#23545;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#39033;&#30446;&#65292;&#24182;&#23558;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#39033;&#30446;&#65292;&#22312;&#23545;&#35805;&#20013;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#36827;&#34892;&#39033;&#30446;&#25512;&#33616;&#65292;&#24182;&#29983;&#25104;&#23545;&#35805;&#12290;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;PECRS&#65288;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65289;&#21487;&#20197;&#22312;&#21333;&#20010;&#38454;&#27573;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#19981;&#20381;&#36182;&#38750;&#25991;&#26412;&#20803;&#25968;&#25454;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender systems (CRS) aim to recommend relevant items to users by eliciting user preference through natural language conversation. Prior work often utilizes external knowledge graphs for items' semantic information, a language model for dialogue generation, and a recommendation module for ranking relevant items. This combination of multiple components suffers from a cumbersome training process, and leads to semantic misalignment issues between dialogue generation and item recommendation. In this paper, we represent items in natural language and formulate CRS as a natural language processing task. Accordingly, we leverage the power of pre-trained language models to encode items, understand user intent via conversation, perform item recommendation through semantic matching, and generate dialogues. As a unified model, our PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without relying on non-textual metadata such as a knowledge graph. Experiments on
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#35302;&#21457;&#26465;&#20214;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#25552;&#31034;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#20195;&#30721;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.10065</link><description>&lt;p&gt;
&#20195;&#30721;&#25552;&#31034;&#22312;&#25991;&#26412;+&#20195;&#30721;LLMs&#20013;&#24341;&#21457;&#20102;&#26465;&#20214;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs. (arXiv:2401.10065v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#35302;&#21457;&#26465;&#20214;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#25552;&#31034;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#20195;&#30721;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#26159;&#23454;&#29616;&#35821;&#35328;&#29702;&#35299;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#22810;&#31181;&#25512;&#29702;&#31867;&#22411;&#20013;&#65292;&#26465;&#20214;&#25512;&#29702;&#26159;&#19968;&#31181;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#24471;&#20986;&#19981;&#21516;&#32467;&#35770;&#30340;&#33021;&#21147;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#19968;&#30452;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26368;&#36817;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;LLMs&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20110;&#20160;&#20040;&#35302;&#21457;&#20102;LLMs&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#20551;&#35774;&#20195;&#30721;&#25552;&#31034;&#33021;&#22815;&#35302;&#21457;&#22312;&#25991;&#26412;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#30340;LLMs&#20013;&#30340;&#26465;&#20214;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#25552;&#31034;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#20195;&#30721;&#65292;&#24182;&#29992;&#29983;&#25104;&#30340;&#20195;&#30721;&#25552;&#31034;LLMs&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#38656;&#35201;&#26465;&#20214;&#25512;&#29702;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#20195;&#30721;&#25552;&#31034;&#20351;&#24471;GPT 3.5&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;2.6&#21040;7.7&#20010;&#30334;&#20998;&#28857;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25506;&#32034;&#20102;&#20195;&#30721;&#25552;&#31034;&#22914;&#20309;&#24341;&#21457;&#26465;&#20214;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#36890;&#36807;&#21738;&#20123;&#29305;&#24449;&#36827;&#34892;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25552;&#31034;&#30340;&#24418;&#24335;&#21644;&#20869;&#23481;&#23545;&#20110;&#24341;&#21457;&#26465;&#20214;&#25512;&#29702;&#33021;&#21147;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning is a fundamental component for achieving language understanding. Among the multiple types of reasoning, conditional reasoning, the ability to draw different conclusions depending on some condition, has been understudied in large language models (LLMs). Recent prompting methods, such as chain of thought, have significantly improved LLMs on reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs. We hypothesize that code prompts can trigger conditional reasoning in LLMs trained on text and code. We propose a chain of prompts that transforms a natural language problem into code and prompts the LLM with the generated code. Our experiments find that code prompts exhibit a performance boost between 2.6 and 7.7 points on GPT 3.5 across multiple datasets requiring conditional reasoning. We then conduct experiments to discover how code prompts elicit conditional reasoning abilities and through which features. We observe that prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#25945;&#32946;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#20854;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.08664</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25945;&#32946;&#65306;&#22522;&#26412;&#33021;&#21147;&#12289;&#28508;&#21147;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges. (arXiv:2401.08664v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#25945;&#32946;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#20854;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25945;&#32946;&#24179;&#21488;&#21033;&#29992;&#20114;&#32852;&#32593;&#20998;&#21457;&#25945;&#32946;&#36164;&#28304;&#65292;&#26088;&#22312;&#25552;&#20379;&#20415;&#25463;&#30340;&#25945;&#32946;&#65292;&#20294;&#24448;&#24448;&#22312;&#19982;&#23398;&#29983;&#30340;&#23454;&#26102;&#20132;&#27969;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#30001;&#20110;&#38656;&#35201;&#35299;&#20915;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#22810;&#26679;&#21270;&#38556;&#30861;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25945;&#32946;&#36164;&#28304;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20102;&#36890;&#36807;&#29702;&#35299;&#20010;&#20307;&#35831;&#27714;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#34429;&#28982;LLMs&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22522;&#20110;LLM&#30340;&#25945;&#32946;&#31995;&#32479;&#30340;&#26500;&#24314;&#20173;&#28982;&#38754;&#20020;&#30528;&#24191;&#27867;&#30340;&#25945;&#32946;&#25216;&#33021;&#35201;&#27714;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#19982;&#25945;&#32946;&#33021;&#21147;&#30456;&#20851;&#30340;&#36817;&#26399;&#20986;&#29616;&#30340;LLM&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#23427;&#20204;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online education platforms, leveraging the internet to distribute education resources, seek to provide convenient education but often fall short in real-time communication with students. They often struggle to offer personalized education resources due to the challenge of addressing the diverse obstacles students encounter throughout their learning journey. Recently, the emergence of large language models (LLMs), such as ChatGPT, offers the possibility for resolving this issue by comprehending individual requests. Although LLMs have been successful in various fields, creating an LLM-based education system is still challenging for the wide range of educational skills required. This paper reviews the recently emerged LLM researches related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim to explore their potential in constructing the next-generation intelligent education system. Based on the current 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#20449;&#24687;&#12289;&#35780;&#20272;&#21512;&#27861;&#24615;&#21644;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05273</link><description>&lt;p&gt;
INACIA&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges. (arXiv:2401.05273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#20449;&#24687;&#12289;&#35780;&#20272;&#21512;&#27861;&#24615;&#21644;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#65288;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36741;&#21161;&#25351;&#20196;&#31995;&#32479;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#24052;&#35199;&#32852;&#37030;&#23457;&#35745;&#27861;&#38498;&#65288;TCU&#65289;&#30340;&#36816;&#33829;&#26694;&#26550;&#20013;&#12290;&#35813;&#31995;&#32479;&#33258;&#21160;&#21270;&#20102;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#22522;&#26412;&#20449;&#24687;&#25552;&#21462;&#12289;&#21487;&#21463;&#29702;&#24615;&#23457;&#26597;&#12289;Periculum in mora&#21644;Fumus boni iuris&#20998;&#26512;&#20197;&#21450;&#24314;&#35758;&#29983;&#25104;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;INACIA&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12289;&#35780;&#20272;&#20854;&#21512;&#27861;&#24615;&#24182;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#30340;&#28508;&#21147;&#12290;&#21033;&#29992;&#39564;&#35777;&#25968;&#25454;&#38598;&#21644;LLMs&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;INACIA&#22788;&#29702;&#22797;&#26434;&#27861;&#24459;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#34920;&#26126;&#20854;&#36866;&#29992;&#20110;&#22686;&#21152;&#27861;&#24459;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21496;&#27861;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces INACIA (Instru\c{c}\~ao Assistida com Intelig\^encia Artificial), a groundbreaking system designed to integrate Large Language Models (LLMs) into the operational framework of Brazilian Federal Court of Accounts (TCU). The system automates various stages of case analysis, including basic information extraction, admissibility examination, Periculum in mora and Fumus boni iuris analyses, and recommendations generation. Through a series of experiments, we demonstrate INACIA's potential in extracting relevant information from case documents, evaluating its legal plausibility, and generating judicial recommendations. Utilizing a validation dataset alongside LLMs, our evaluation methodology presents an innovative approach to assessing system performance, correlating highly with human judgment. The results highlight INACIA's proficiency in handling complex legal tasks, indicating its suitability for augmenting efficiency and judicial fairness within legal systems. The pap
&lt;/p&gt;</description></item><item><title>LAMPAT&#26159;&#31532;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#25913;&#20889;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#29615;&#22659;&#19979;&#29983;&#25104;&#25913;&#20889;&#12290;</title><link>http://arxiv.org/abs/2401.04348</link><description>&lt;p&gt;
LAMPAT&#65306;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#36827;&#34892;&#20302;&#31209;&#22810;&#35821;&#35328;&#25913;&#20889;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using Adversarial Training. (arXiv:2401.04348v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04348
&lt;/p&gt;
&lt;p&gt;
LAMPAT&#26159;&#31532;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#25913;&#20889;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#29615;&#22659;&#19979;&#29983;&#25104;&#25913;&#20889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#20889;&#26159;&#25351;&#20351;&#29992;&#19981;&#21516;&#30340;&#35789;&#35821;&#25110;&#21477;&#23376;&#32467;&#26500;&#26469;&#20256;&#36798;&#30456;&#21516;&#21547;&#20041;&#30340;&#25991;&#26412;&#12290;&#23427;&#21487;&#20197;&#29992;&#20316;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#25968;&#25454;&#19981;&#36275;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#12290;&#20026;&#20102;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#29983;&#25104;&#25913;&#20889;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#36890;&#36807;&#22312;&#30456;&#21516;&#35821;&#35328;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#26426;&#22120;&#32763;&#35793;&#26469;&#24418;&#25104;&#25913;&#20889;&#12290;&#23613;&#31649;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38656;&#35201;&#24179;&#34892;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#26080;&#27861;&#24212;&#29992;&#20110;&#27809;&#26377;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#25913;&#20889;&#27169;&#22411;&#65292;LAMPAT&#65288;&#20302;&#31209;&#22810;&#35821;&#35328;&#25913;&#20889;&#30340;&#36866;&#24212;&#24615;&#20302;&#31209;&#22810;&#35821;&#35328;&#25913;&#20889;&#27169;&#22411;&#65289;&#65292;&#20854;&#20013;&#21333;&#35821;&#25968;&#25454;&#38598;&#24050;&#32463;&#36275;&#22815;&#12290;
&lt;/p&gt;
&lt;p&gt;
Paraphrases are texts that convey the same meaning while using different words or sentence structures. It can be used as an automatic data augmentation tool for many Natural Language Processing tasks, especially when dealing with low-resource languages, where data shortage is a significant problem. To generate a paraphrase in multilingual settings, previous studies have leveraged the knowledge from the machine translation field, i.e., forming a paraphrase through zero-shot machine translation in the same language. Despite good performance on human evaluation, those methods still require parallel translation datasets, thus making them inapplicable to languages that do not have parallel corpora. To mitigate that problem, we proposed the first unsupervised multilingual paraphrasing model, LAMPAT ($\textbf{L}$ow-rank $\textbf{A}$daptation for $\textbf{M}$ultilingual $\textbf{P}$araphrasing using $\textbf{A}$dversarial $\textbf{T}$raining), by which monolingual dataset is sufficient enough 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;PIXAR&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20687;&#32032;&#33258;&#22238;&#24402;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#22270;&#20687;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#35789;&#27719;&#34920;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#25239;&#24615;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#25104;&#38750;&#27169;&#31946;&#25991;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.03321</link><description>&lt;p&gt;
PIXAR&#65306;&#20687;&#32032;&#31354;&#38388;&#20013;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
PIXAR: Auto-Regressive Language Modeling in Pixel Space. (arXiv:2401.03321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;PIXAR&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20687;&#32032;&#33258;&#22238;&#24402;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#22270;&#20687;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#35789;&#27719;&#34920;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#25239;&#24615;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#25104;&#38750;&#27169;&#31946;&#25991;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#21487;&#20197;&#26500;&#24314;&#22522;&#20110;&#20687;&#32032;&#34920;&#31034;&#30340;&#24320;&#25918;&#35789;&#27719;&#37327;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#24418;&#24335;&#65292;&#37325;&#26500;&#36974;&#34109;&#30340;&#22270;&#20687;&#25991;&#26412;&#34917;&#19969;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#20687;&#32032;&#30340;LLMs&#20165;&#38480;&#20110;&#33258;&#32534;&#30721;&#20219;&#21153;&#65292;&#26080;&#27861;&#29983;&#25104;&#26032;&#30340;&#25991;&#26412;&#20316;&#20026;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#29992;&#20110;&#24320;&#25918;&#24335;&#22238;&#31572;&#25110;&#29983;&#25104;&#24335;&#35821;&#35328;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;PIXAR&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#35789;&#27719;&#34920;&#30340;&#20687;&#32032;&#33258;&#22238;&#24402;LLM&#65292;&#29992;&#20110;&#36755;&#20837;&#21644;&#36755;&#20986;&#25991;&#26412;&#12290;PIXAR&#21482;&#26377;&#19968;&#20010;&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#22238;&#31572;&#33258;&#30001;&#24418;&#24335;&#30340;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#19978;&#19982;&#20197;&#21069;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#25345;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#38750;&#27169;&#31946;&#25991;&#26412;&#20316;&#20026;&#22270;&#20687;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;&#20854;&#19982;&#36890;&#24120;&#30340;&#26368;&#22823;&#20284;&#28982;&#30446;&#26631;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#25239;&#24615;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works showed the possibility of building open-vocabulary large language models (LLMs) that directly operate on pixel representations and are implemented as encoder-decoder models that reconstruct masked image patches of rendered text. However, these pixel-based LLMs are limited to autoencoding tasks and cannot generate new text as images. As such, they cannot be used for open-answer or generative language tasks. In this work, we overcome this limitation and introduce PIXAR, the first pixel-based autoregressive LLM that does not rely on a pre-defined vocabulary for both input and output text. Consisting of only a decoder, PIXAR can answer free-form generative tasks while keeping the text representation learning performance on par with previous encoder-decoder models. Furthermore, we highlight the challenges to autoregressively generate non-blurred text as images and link this to the usual maximum likelihood objective. We propose a simple adversarial pretraining that significantly
&lt;/p&gt;</description></item><item><title>TiMix&#26159;&#19968;&#31181;&#23558;&#25991;&#26412;&#24863;&#30693;&#30340;&#22270;&#20687;&#28151;&#21512;&#25216;&#26415;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.08846</link><description>&lt;p&gt;
TiMix: &#25991;&#26412;&#24863;&#30693;&#22270;&#20687;&#28151;&#21512;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08846
&lt;/p&gt;
&lt;p&gt;
TiMix&#26159;&#19968;&#31181;&#23558;&#25991;&#26412;&#24863;&#30693;&#30340;&#22270;&#20687;&#28151;&#21512;&#25216;&#26415;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#65288;SMCL&#65289;&#36890;&#36807;&#23545;&#40784;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#65292;&#26174;&#33879;&#25512;&#36827;&#20102;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#32476;&#25910;&#38598;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#25193;&#22823;SMCL&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#25552;&#39640;VLP&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#24863;&#30693;&#22270;&#20687;&#28151;&#21512;&#65288;TiMix&#65289;&#65292;&#23558;&#22522;&#20110;&#28151;&#21512;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#38598;&#25104;&#21040;SMCL&#20013;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#20174;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#30340;&#35282;&#24230;&#23545;TiMix&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#34920;&#26126;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#30340;&#28151;&#21512;&#25968;&#25454;&#26679;&#26412;&#38544;&#24335;&#22320;&#20316;&#20026;&#23545;&#27604;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;TiMix&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMixfrom a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when be
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65288;CADs&#65289;&#19982;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#20173;&#28982;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01270</link><description>&lt;p&gt;
&#20154;&#31867;&#36827;&#34892;&#26356;&#22909;&#30340;&#32534;&#36753;&#65306;&#34913;&#37327;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#22312;&#26377;&#23475;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection. (arXiv:2311.01270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65288;CADs&#65289;&#19982;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#20173;&#28982;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#31038;&#20250;&#35745;&#31639;&#20219;&#21153;&#20013;&#34987;&#20351;&#29992;&#65292;&#22914;&#26816;&#27979;&#24615;&#21035;&#27495;&#35270;&#12289;&#31181;&#26063;&#27495;&#35270;&#25110;&#20854;&#20182;&#20167;&#24680;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#30340;&#24037;&#20316;&#23581;&#35797;&#35299;&#20915;&#36825;&#20123;&#34394;&#20551;&#29305;&#24449;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65288;CADs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;CADs&#23545;&#29616;&#26377;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#26368;&#23567;&#25913;&#21160;&#24182;&#32763;&#36716;&#26631;&#31614;&#65307;&#22312;&#20854;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#29983;&#25104;CADs&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#33258;&#21160;&#21270;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;Polyjuice&#12289;ChatGPT&#21644;Flan-T5&#33258;&#21160;&#29983;&#25104;CADs&#65292;&#24182;&#19982;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#39046;&#22495;&#22806;&#27979;&#35797;&#38598;&#19978;&#27979;&#35797;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#25163;&#21160;CADs&#20173;&#28982;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. However, manually generating CADs can be time-consuming and expensive. Hence in this work, we assess if this task can be automated using generative NLP models. We automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate their usefulness in improving model robustness compared to manually-generated CADs. By testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual CADs are still the most e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.01200</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#20986;&#29616;&#26032;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#26032;&#27169;&#22411;&#32780;&#19981;&#26159;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#25910;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26032;&#35821;&#35328;&#20986;&#29616;&#26102;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#22909;&#22788;&#21644;&#24330;&#31471;&#65292;&#21363;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#20174;&#21333;&#35821;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20986;&#21457;&#65292;&#25105;&#20204;&#36880;&#27493;&#28155;&#21152;&#20102;&#26469;&#33258;&#25386;&#23041;&#35821;&#21644;&#20912;&#23707;&#35821;&#30340;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#21069;&#21521;&#21644;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#22914;&#20309;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#39034;&#24207;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#21069;&#21521;&#36716;&#31227;&#20027;&#35201;&#26159;&#27491;&#21521;&#30340;&#65292;&#19981;&#21463;&#35821;&#35328;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#21017;&#21487;&#33021;&#26159;&#27491;&#21521;&#30340;&#25110;&#36127;&#21521;&#30340;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#24335;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#31181;&#35821;&#35328;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#31181;&#31616;&#21333;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#34920;&#31034;&#20026;&#21521;&#37327;&#12290;&#36825;&#20123;&#20989;&#25968;&#21521;&#37327;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#36824;&#20855;&#26377;&#23558;&#35821;&#20041;&#21521;&#37327;&#36827;&#34892;&#32452;&#21512;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20989;&#25968;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Function Vectors in Large Language Models. (arXiv:2310.15213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15213
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#31181;&#31616;&#21333;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#34920;&#31034;&#20026;&#21521;&#37327;&#12290;&#36825;&#20123;&#20989;&#25968;&#21521;&#37327;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#36824;&#20855;&#26377;&#23558;&#35821;&#20041;&#21521;&#37327;&#36827;&#34892;&#32452;&#21512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#34920;&#31034;&#20026;&#33258;&#22238;&#24402;&#21464;&#25442;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#21521;&#37327;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20219;&#21153;&#19978;&#20351;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#23569;&#25968;&#27880;&#24847;&#21147;&#22836;&#20256;&#36755;&#20102;&#23637;&#31034;&#20219;&#21153;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20989;&#25968;&#21521;&#37327;&#65288;FV&#65289;&#12290;FV&#23545;&#19978;&#19979;&#25991;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21363;&#23427;&#20204;&#22312;&#19981;&#31867;&#20284;&#20110;&#20854;&#25910;&#38598;&#26102;&#30340;ICL&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#35302;&#21457;&#23545;&#36755;&#20837;&#30340;&#20219;&#21153;&#25191;&#34892;&#65292;&#20363;&#22914;&#38646;&#26679;&#26412;&#21644;&#33258;&#28982;&#25991;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#23618;&#19978;&#27979;&#35797;&#20102;FV&#65292;&#24182;&#22312;&#20013;&#23618;&#21457;&#29616;&#24378;&#22823;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;FV&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#24182;&#21457;&#29616;&#34429;&#28982;&#23427;&#20204;&#36890;&#24120;&#21253;&#21547;&#32534;&#30721;&#20989;&#25968;&#30340;&#36755;&#20986;&#31354;&#38388;&#30340;&#20449;&#24687;&#65292;&#20294;&#20165;&#27492;&#20449;&#24687;&#26080;&#27861;&#37325;&#26500;FV&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;FV&#20013;&#30340;&#35821;&#20041;&#21521;&#37327;&#32452;&#21512;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#23384;&#22312;&#32452;&#21512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some exte
&lt;/p&gt;</description></item><item><title>O3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;</title><link>http://arxiv.org/abs/2310.14403</link><description>&lt;p&gt;
O3D: &#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#21457;&#29616;&#19982;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14403
&lt;/p&gt;
&lt;p&gt;
O3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#36827;&#23637;&#65292;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#27169;&#20223;&#25552;&#31034;&#20013;&#25552;&#20379;&#30340;&#23569;&#37327;&#31034;&#20363;&#65288;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#19982;&#22806;&#37096;&#29615;&#22659;&#20132;&#20114;&#24182;&#23436;&#25104;&#32473;&#23450;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23569;&#37327;&#31034;&#20363;&#24448;&#24448;&#19981;&#36275;&#20197;&#29983;&#25104;&#22797;&#26434;&#19988;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26080;&#27861;&#22788;&#29702;&#26356;&#22823;&#35268;&#27169;&#30340;&#28436;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#22823;&#35268;&#27169;&#30340;&#31163;&#32447;&#25968;&#25454;&#65288;&#20363;&#22914;&#20154;&#31867;&#20132;&#20114;&#30340;&#26085;&#24535;&#65289;&#26469;&#25913;&#36827;LLM&#20195;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25991;&#26412;&#21644;&#20195;&#30721;&#20004;&#31181;&#26041;&#27861;&#27491;&#24335;&#23450;&#20041;&#20102;LLM&#24378;&#21270;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;O3D&#30340;&#31163;&#32447;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#21644;&#33976;&#39311;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;LLM&#24378;&#21270;&#31574;&#30053;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;O3D&#33258;&#21160;&#22320;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#33258;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#33258;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.14053</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#65306;&#29992;IdentityChain&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain. (arXiv:2310.14053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#33258;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#33258;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;(Code LLMs)&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#22240;&#27492;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20934;&#30830;&#24615;&#35780;&#20272;&#26041;&#27861;&#35780;&#20272;Code LLMs&#22312;&#19968;&#31995;&#21015;&#29420;&#31435;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#24573;&#35270;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#30452;&#35266;&#26469;&#35762;&#65292;&#19968;&#20010;&#21487;&#20449;&#36182;&#30340;&#27169;&#22411;&#22312;&#20026;&#20854;&#33258;&#36523;&#30340;&#20195;&#30721;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20197;&#21450;&#20026;&#20854;&#33258;&#36523;&#30340;&#35268;&#33539;&#29983;&#25104;&#20195;&#30721;&#26102;&#24212;&#35813;&#26159;&#33258;&#19968;&#33268;&#30340;&#12290;&#26410;&#33021;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#25581;&#31034;&#20102;&#23545;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20849;&#20139;&#35821;&#20041;&#30340;&#29702;&#35299;&#30340;&#19981;&#36275;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#39318;&#20808;&#27491;&#24335;&#23450;&#20041;&#20102;Code LLMs&#30340;&#33258;&#19968;&#33268;&#24615;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;IdentityChain&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#33258;&#19968;&#33268;&#24615;&#21644;&#20256;&#32479;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;11&#20010;Code LLMs&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#26410;&#33021;&#20445;&#25345;&#33258;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indee
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;&#65292;&#21457;&#29616;LLMs&#23545;&#20110;&#21508;&#31181;&#24494;&#35843;&#25968;&#25454;&#38598;&#37117;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31163;&#32676;&#29992;&#25143;&#21644;&#36129;&#29486;&#22823;&#37327;&#25968;&#25454;&#30340;&#29992;&#25143;&#12290;&#36825;&#23545;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.09266</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
User Inference Attacks on Large Language Models. (arXiv:2310.09266v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;&#65292;&#21457;&#29616;LLMs&#23545;&#20110;&#21508;&#31181;&#24494;&#35843;&#25968;&#25454;&#38598;&#37117;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31163;&#32676;&#29992;&#25143;&#21644;&#36129;&#29486;&#22823;&#37327;&#25968;&#25454;&#30340;&#29992;&#25143;&#12290;&#36825;&#23545;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23450;&#21046;&#20026;&#19987;&#19994;&#20219;&#21153;&#21644;&#24212;&#29992;&#30340;&#24120;&#35265;&#26377;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#24494;&#35843;LLMs&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#31216;&#20026;&#29992;&#25143;&#25512;&#29702;&#30340;&#29616;&#23454;&#23041;&#32961;&#27169;&#22411;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#25512;&#26029;&#20986;&#29992;&#25143;&#30340;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#20110;&#24494;&#35843;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#31181;&#23041;&#32961;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#21482;&#38656;&#35201;&#20174;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#19968;&#23567;&#32452;&#26679;&#26412;&#65288;&#21487;&#33021;&#19982;&#29992;&#20110;&#35757;&#32451;&#30340;&#26679;&#26412;&#19981;&#21516;&#65289;&#21644;&#23545;&#24494;&#35843;LLM&#30340;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#22312;&#21508;&#31181;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#26131;&#21463;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#26377;&#26102;&#25915;&#20987;&#25104;&#21151;&#29575;&#25509;&#36817;&#23436;&#32654;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21738;&#20123;&#29305;&#24615;&#20351;&#29992;&#25143;&#23481;&#26131;&#21463;&#21040;&#29992;&#25143;&#25512;&#29702;&#30340;&#25915;&#20987;&#65292;&#21457;&#29616;&#31163;&#32676;&#29992;&#25143;&#65288;&#21363;&#25968;&#25454;&#20998;&#24067;&#19982;&#20854;&#20182;&#29992;&#25143;&#26126;&#26174;&#19981;&#21516;&#65289;&#21644;&#36129;&#29486;&#22823;&#37327;&#25968;&#25454;&#30340;&#29992;&#25143;&#26356;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35299;&#20915;&#36825;&#31181;&#25915;&#20987;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is a common and effective method for tailoring large language models (LLMs) to specialized tasks and applications. In this paper, we study the privacy implications of fine-tuning LLMs on user data. To this end, we define a realistic threat model, called user inference, wherein an attacker infers whether or not a user's data was used for fine-tuning. We implement attacks for this threat model that require only a small set of samples from a user (possibly different from the samples used for training) and black-box access to the fine-tuned LLM. We find that LLMs are susceptible to user inference attacks across a variety of fine-tuning datasets, at times with near perfect attack success rates. Further, we investigate which properties make users vulnerable to user inference, finding that outlier users (i.e. those with data distributions sufficiently different from other users) and users who contribute large quantities of data are most susceptible to attack. Finally, we explore s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21463;&#25511;&#25991;&#26412;&#32553;&#20943;&#65288;CTR&#65289;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#19981;&#20805;&#20998;&#24378;&#21046;&#25191;&#34892;&#21644;&#27425;&#20248;&#30340;&#38134;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#22686;&#24378;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09017</link><description>&lt;p&gt;
&#19981;&#28155;&#21152;&#65292;&#19981;&#38169;&#36807;&#65306;&#20174;&#39044;&#36873;&#25991;&#26412;&#27573;&#29983;&#25104;&#26377;&#25928;&#30340;&#20869;&#23481;&#20445;&#30041;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans. (arXiv:2310.09017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21463;&#25511;&#25991;&#26412;&#32553;&#20943;&#65288;CTR&#65289;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#19981;&#20805;&#20998;&#24378;&#21046;&#25191;&#34892;&#21644;&#27425;&#20248;&#30340;&#38134;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#22686;&#24378;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;&#21463;&#25511;&#25991;&#26412;&#32553;&#20943;&#65288;CTR&#65289;&#20219;&#21153;&#22312;&#20856;&#22411;&#30340;&#25688;&#35201;&#20219;&#21153;&#20013;&#23558;&#25991;&#26412;&#29983;&#25104;&#27493;&#39588;&#38548;&#31163;&#20986;&#26469;&#12290;&#23427;&#36890;&#36807;&#25361;&#25112;&#27169;&#22411;&#22312;&#36755;&#20837;&#25991;&#26412;&#30340;&#39044;&#36873;&#20869;&#23481;&#65288;"&#39640;&#20142;"&#65289;&#20013;&#29983;&#25104;&#36830;&#36143;&#30340;&#25991;&#26412;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#26694;&#26550;&#22312;&#31867;&#20284;&#25688;&#35201;&#30340;&#20219;&#21153;&#20013;&#22686;&#21152;&#20102;&#27169;&#22359;&#21270;&#33021;&#21147;&#65292;&#20801;&#35768;&#23558;&#21333;&#20010;CTR&#27169;&#22411;&#19982;&#21508;&#31181;&#20869;&#23481;&#36873;&#25321;&#35774;&#32622;&#21644;&#27169;&#22359;&#37197;&#23545;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#21487;&#38752;&#30340;CTR&#27169;&#22411;&#65292;&#32780;&#19988;&#29616;&#26377;&#20219;&#21153;&#22522;&#32447;&#30340;&#24615;&#33021;&#20013;&#31561;&#65292;&#26080;&#27861;&#23454;&#38469;&#20351;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;CTR&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#20808;&#21069;&#30340;&#20851;&#38190;&#38480;&#21046;&#65306;&#19981;&#20805;&#20998;&#24378;&#21046;&#25191;&#34892;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#21644;&#27425;&#20248;&#30340;&#38134;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#25512;&#29702;&#20013;&#36890;&#36807;&#21463;&#25511;&#35299;&#30721;&#31574;&#30053;&#26469;&#22686;&#24378;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22823;&#24133;&#25913;&#36827;&#20102;&#38134;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently introduced Controlled Text Reduction (CTR) task isolates the text generation step within typical summarization-style tasks. It does so by challenging models to generate coherent text conforming to pre-selected content within the input text ("highlights").  This framing enables increased modularity in summarization-like tasks, allowing to couple a single CTR model with various content-selection setups and modules.  However, there are currently no reliable CTR models, while the performance of the existing baseline for the task is mediocre, falling short of practical utility.  Here, we address this gap by introducing a high-quality, open-source CTR model that tackles two prior key limitations: inadequate enforcement of the content-preservation constraint, and suboptimal silver training data.  Addressing these, we amplify the content-preservation constraint in both training, via RL, and inference, via a controlled decoding strategy.  Further, we substantially improve the silve
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#38646;&#23556;&#20132;&#20114;&#20010;&#24615;&#21270;&#23545;&#35937;&#23548;&#33322;&#65288;ZIPON&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#29992;&#25143;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#20010;&#24615;&#21270;&#30446;&#26631;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07968</link><description>&lt;p&gt;
&#24605;&#32771;&#12289;&#34892;&#21160;&#21644;&#38382;&#65306;&#24320;&#25918;&#19990;&#30028;&#20114;&#21160;&#20010;&#24615;&#21270;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Think, Act, and Ask: Open-World Interactive Personalized Robot Navigation. (arXiv:2310.07968v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07968
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#38646;&#23556;&#20132;&#20114;&#20010;&#24615;&#21270;&#23545;&#35937;&#23548;&#33322;&#65288;ZIPON&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#29992;&#25143;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#20010;&#24615;&#21270;&#30446;&#26631;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#23556;&#21629;&#20196;&#23545;&#35937;&#23548;&#33322;&#65288;ZSON&#65289;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#24320;&#25918;&#35789;&#27719;&#23545;&#35937;&#12290;&#29616;&#26377;&#30340;ZSON&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36981;&#24490;&#20010;&#21035;&#25351;&#20196;&#20197;&#23547;&#25214;&#36890;&#29992;&#23545;&#35937;&#31867;&#65292;&#24573;&#30053;&#20102;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#21033;&#29992;&#21644;&#35782;&#21035;&#29992;&#25143;&#29305;&#23450;&#23545;&#35937;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38646;&#23556;&#20132;&#20114;&#20010;&#24615;&#21270;&#23545;&#35937;&#23548;&#33322;&#65288;ZIPON&#65289;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#38656;&#35201;&#22312;&#19982;&#29992;&#25143;&#23545;&#35805;&#30340;&#21516;&#26102;&#23548;&#33322;&#21040;&#20010;&#24615;&#21270;&#30446;&#26631;&#23545;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;ZIPON&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#31216;&#20026;&#24320;&#25918;&#19990;&#30028;&#20114;&#21160;&#20010;&#24615;&#21270;&#23548;&#33322;&#65288;ORION&#65289;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#65292;&#20197;&#25805;&#20316;&#19981;&#21516;&#30340;&#24863;&#30693;&#12289;&#23548;&#33322;&#21644;&#36890;&#20449;&#27169;&#22359;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#33021;&#22815;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#30340;&#20114;&#21160;&#20195;&#29702;&#30340;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Object Navigation (ZSON) enables agents to navigate towards open-vocabulary objects in unknown environments. The existing works of ZSON mainly focus on following individual instructions to find generic object classes, neglecting the utilization of natural language interaction and the complexities of identifying user-specific objects. To address these limitations, we introduce Zero-shot Interactive Personalized Object Navigation (ZIPON), where robots need to navigate to personalized goal objects while engaging in conversations with users. To solve ZIPON, we propose a new framework termed Open-woRld Interactive persOnalized Navigation (ORION), which uses Large Language Models (LLMs) to make sequential decisions to manipulate different modules for perception, navigation and communication. Experimental results show that the performance of interactive agents that can leverage user feedback exhibits significant improvement. However, obtaining a good balance between task completion 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25552;&#31034;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#37319;&#38598;&#27169;&#22411;&#21453;&#39304;&#65292;&#29983;&#25104;&#36866;&#21512;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#27425;&#25512;&#29702;&#35843;&#29992;&#26469;&#38598;&#25104;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.07088</link><description>&lt;p&gt;
&#24605;&#32500;&#22810;&#26679;&#24615;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Diversity of Thought Improves Reasoning Abilities of Large Language Models. (arXiv:2310.07088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25552;&#31034;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#37319;&#38598;&#27169;&#22411;&#21453;&#39304;&#65292;&#29983;&#25104;&#36866;&#21512;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#27425;&#25512;&#29702;&#35843;&#29992;&#26469;&#38598;&#25104;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#25351;&#23548;&#20998;&#35299;&#38382;&#39064;&#20026;&#26356;&#23567;&#30340;&#25512;&#29702;&#27493;&#39588;&#25110;&#36890;&#36807;&#20462;&#25913;&#35299;&#30721;&#27493;&#39588;&#20351;&#21508;&#31181;&#29983;&#25104;&#32467;&#26524;&#21512;&#24182;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#37117;&#20551;&#35774;&#36755;&#20837;&#25552;&#31034;&#26159;&#22266;&#23450;&#30340;&#65292;&#24182;&#26399;&#26395;&#35299;&#30721;&#31574;&#30053;&#24341;&#20837;&#25152;&#38656;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25918;&#26494;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#21019;&#24314;&#21644;&#21033;&#29992;&#36755;&#20837;&#25552;&#31034;&#30340;&#21464;&#21270;&#26469;&#25552;&#21319;&#24605;&#32500;&#22810;&#26679;&#24615;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#35821;&#35328;&#27169;&#22411;&#24449;&#27714;&#21453;&#39304;&#26469;&#26500;&#24605;&#36866;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33258;&#21160;&#25552;&#39640;&#25552;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;DIV-SE (DIVerse reasoning path Self-Ensemble)&#20013;&#23545;&#22810;&#26679;&#30340;&#25552;&#31034;&#36827;&#34892;&#21512;&#25104;&#65292;&#36890;&#36807;&#22810;&#27425;&#25512;&#29702;&#35843;&#29992;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#21363;&#22312;&#19968;&#20010;&#25512;&#29702;&#20013;&#20351;&#29992;&#22810;&#26679;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps (Wei et al., 2022), or ensembling various generations through modifying decoding steps (Wang et al., 2023) boosts performance. Current methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we relax this assumption and discuss how one can create and leverage variations of the input prompt as a means to diversity of thought to improve model performance. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that fit for the problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse reasoning path Self-Ensemble) across multiple inference calls. We also propose a cost-effective alternative where diverse prompts are used within a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#21435;&#38500;LLMs&#20013;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#30340;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#26469;&#20256;&#36798;&#20854;&#20449;&#24565;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.06272</link><description>&lt;p&gt;
&#35753;&#27169;&#22411;&#35828;&#23494;&#25991;: &#36890;&#36807;&#23884;&#20837;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;
&lt;/p&gt;
&lt;p&gt;
Let Models Speak Ciphers: Multiagent Debate through Embeddings. (arXiv:2310.06272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#36890;&#36807;&#21435;&#38500;LLMs&#20013;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26399;&#26395;&#30340;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#26469;&#20256;&#36798;&#20854;&#20449;&#24565;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#30340;&#35752;&#35770;&#21644;&#36777;&#35770;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#28508;&#21147;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#30001;&#20110;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#32780;&#25104;&#20026;&#26126;&#26174;&#30340;&#20132;&#27969;&#36873;&#25321;&#65292;&#20294;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26102;&#38656;&#35201;&#36827;&#34892;&#30340;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#21487;&#33021;&#23384;&#22312;&#20449;&#24687;&#20002;&#22833;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#22240;&#20026;&#23427;&#20165;&#20351;&#29992;&#19968;&#20010;&#26631;&#35760;&#26469;&#20195;&#34920;&#27169;&#22411;&#22312;&#25972;&#20010;&#35789;&#27719;&#34920;&#20013;&#30340;&#20449;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CIPHER&#65288;&#36890;&#36807;&#23884;&#20837;&#34920;&#31034;&#36827;&#34892;&#20132;&#27969;&#30340;&#32593;&#32476;&#27169;&#22411;&#21327;&#35758;&#65289;&#30340;&#36890;&#20449;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;LLMs&#20013;&#21435;&#38500;&#20102;&#26631;&#35760;&#37319;&#26679;&#27493;&#39588;&#65292;&#35753;&#23427;&#20204;&#36890;&#36807;&#21407;&#22987;Transformer&#36755;&#20986;&#23884;&#20837;&#30340;&#26399;&#26395;&#26469;&#20256;&#36798;&#23427;&#20204;&#30340;&#20449;&#24565;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#20559;&#31163;&#33258;&#28982;&#35821;&#35328;&#65292;CIPHER&#22312;&#19981;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#32534;&#30721;&#26356;&#24191;&#27867;&#20449;&#24687;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights. While the state-of-the-a
&lt;/p&gt;</description></item><item><title>GeoLLM&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#26469;&#33258;OpenStreetMap&#30340;&#36741;&#21161;&#22320;&#22270;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#27979;&#37327;&#20154;&#21475;&#23494;&#24230;&#21644;&#32463;&#27982;&#29983;&#35745;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.06213</link><description>&lt;p&gt;
GeoLLM: &#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
GeoLLM: Extracting Geospatial Knowledge from Large Language Models. (arXiv:2310.06213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06213
&lt;/p&gt;
&lt;p&gt;
GeoLLM&#26159;&#19968;&#31181;&#33021;&#22815;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#26469;&#33258;OpenStreetMap&#30340;&#36741;&#21161;&#22320;&#22270;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#27979;&#37327;&#20154;&#21475;&#23494;&#24230;&#21644;&#32463;&#27982;&#29983;&#35745;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#31181;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#24120;&#24120;&#20381;&#36182;&#20110;&#20840;&#29699;&#33539;&#22260;&#21487;&#29992;&#30340;&#21355;&#26143;&#22270;&#20687;&#31561;&#39044;&#27979;&#21464;&#37327;&#65292;&#36825;&#21487;&#33021;&#35201;&#20040;&#24456;&#26114;&#36149;&#65292;&#35201;&#20040;&#32570;&#20047;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#20114;&#32852;&#32593;&#35821;&#35328;&#35821;&#26009;&#24211;&#20013;&#21253;&#21547;&#30340;&#22823;&#37327;&#30693;&#35782;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;LLM&#20013;&#23884;&#20837;&#20102;&#26377;&#20851;&#20301;&#32622;&#30340;&#26174;&#33879;&#31354;&#38388;&#20449;&#24687;&#65292;&#20294;&#20165;&#20351;&#29992;&#22320;&#29702;&#22352;&#26631;&#26469;&#26597;&#35810;LLM&#23545;&#20110;&#39044;&#27979;&#20154;&#21475;&#23494;&#24230;&#31561;&#20851;&#38190;&#25351;&#26631;&#26159;&#26080;&#25928;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;LLM&#20013;&#25552;&#21462;&#22320;&#29702;&#31354;&#38388;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#26469;&#33258;OpenStreetMap&#30340;&#36741;&#21161;&#22320;&#22270;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22269;&#38469;&#31038;&#21306;&#20851;&#24515;&#30340;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#20154;&#21475;&#23494;&#24230;&#21644;&#32463;&#27982;&#29983;&#35745;&#30340;&#27979;&#37327;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-GNN&#12290;&#23427;&#21033;&#29992;LLMs&#23545;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;GNN&#33021;&#22815;&#23545;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04668</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMS&#65289;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Label-free Node Classification on Graphs with Large Language Models (LLMS). (arXiv:2310.04668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-GNN&#12290;&#23427;&#21033;&#29992;LLMs&#23545;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;GNN&#33021;&#22815;&#23545;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Graph Neural Networks&#65292;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#30830;&#20445;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#22312;&#25991;&#26412;&#23646;&#24615;&#22270;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#39640;&#25928;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#25512;&#29702;&#25104;&#26412;&#36739;&#39640;&#12290;&#37492;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26080;&#26631;&#31614;&#22270;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;LLM-GNN&#12290;&#23427;&#38598;&#25104;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLMs&#34987;&#29992;&#26469;&#27880;&#37322;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;GNNs&#33021;&#22815;&#39044;&#27979;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#12290;LLM-GNN&#30340;&#23454;&#29616;&#38754;&#20020;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#25105;&#20204;&#22914;&#20309;&#20027;&#21160;&#36873;&#25321;&#35201;&#30001;LLMs&#27880;&#37322;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#22686;&#24378;GNN&#30340;&#35757;&#32451;&#65311;&#25105;&#20204;&#22914;&#20309;&#21033;&#29992;LLMs&#26469;&#20248;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22788;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to ob
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#34955;&#27169;&#22411;&#33258;&#21160;&#20272;&#35745;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65292;&#20197;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#32473;&#25945;&#24072;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65292;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.01132</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#21644;BoWs&#33258;&#21160;&#35780;&#20272;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65306;&#23558;&#20840;&#23616;&#39044;&#27979;&#19982;&#20855;&#20307;&#21453;&#39304;&#30456;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback. (arXiv:2310.01132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#34955;&#27169;&#22411;&#33258;&#21160;&#20272;&#35745;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65292;&#20197;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#32473;&#25945;&#24072;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65292;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21521;&#25945;&#24072;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#26356;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20272;&#35745;&#8220;&#25945;&#23398;&#25903;&#25345;&#8221;&#39046;&#22495;&#30340;CLASS&#35838;&#22530;&#35780;&#20272;&#24471;&#20998;&#65292;&#35813;&#35780;&#20272;&#26041;&#27861;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#35266;&#27979;&#21327;&#35758;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#20351;&#29992;Meta&#30340;Llama2&#30340;&#38646;-shot&#25552;&#31034;&#65292;&#21644;/&#25110;&#32463;&#20856;&#30340;&#35789;&#34955;&#65288;BoW&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#25945;&#24072;&#35328;&#35821;&#30340;&#20010;&#21035;&#35805;&#35821;&#65288;&#20351;&#29992;OpenAI&#30340;Whisper&#36827;&#34892;&#33258;&#21160;&#36716;&#24405;&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#30830;&#23450;&#26159;&#21542;&#23384;&#22312;&#25945;&#23398;&#25903;&#25345;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#35805;&#35821;&#32423;&#30340;&#21028;&#26029;&#32467;&#26524;&#22312;&#25972;&#20010;15&#20998;&#38047;&#30340;&#35266;&#23519;&#20250;&#35805;&#20013;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#20272;&#35745;&#20840;&#23616;CLASS&#24471;&#20998;&#12290;&#22312;&#24188;&#20799;&#22253;&#21644;&#23398;&#21069;&#29677;&#25945;&#23460;&#30340;&#20004;&#20010;&#32463;&#36807;CLASS&#32534;&#30721;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;1&#65289;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33258;&#21160;&#20272;&#35745;CLASS&#25945;&#23398;&#25903;&#25345;&#30340;&#20934;&#30830;&#24615;&#65288;Pearson R&#39640;&#36798;0.47&#65289;&#25509;&#36817;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65288;&#26368;&#39640;R=0.55&#65289;&#65307;&#65288;2&#65289;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#23567;&#29677;&#25945;&#23460;&#20013;&#30340;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the aim to provide teachers with more specific, frequent, and actionable feedback about their teaching, we explore how Large Language Models (LLMs) can be used to estimate ``Instructional Support'' domain scores of the CLassroom Assessment Scoring System (CLASS), a widely used observation protocol. We design a machine learning architecture that uses either zero-shot prompting of Meta's Llama2, and/or a classic Bag of Words (BoW) model, to classify individual utterances of teachers' speech (transcribed automatically using OpenAI's Whisper) for the presence of Instructional Support. Then, these utterance-level judgments are aggregated over an entire 15-min observation session to estimate a global CLASS score. Experiments on two CLASS-coded datasets of toddler and pre-kindergarten classrooms indicate that (1) automatic CLASS Instructional Support estimation accuracy using the proposed method (Pearson $R$ up to $0.47$) approaches human inter-rater reliability (up to $R=0.55$); (2) LLM
&lt;/p&gt;</description></item><item><title>Transformer-VQ&#26159;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;Transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#35745;&#31639;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.16354</link><description>&lt;p&gt;
Transformer-VQ: &#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Transformer-VQ: Linear-Time Transformers via Vector Quantization. (arXiv:2309.16354v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16354
&lt;/p&gt;
&lt;p&gt;
Transformer-VQ&#26159;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;Transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#35745;&#31639;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Transformer-VQ&#65292;&#19968;&#31181;&#20165;&#32534;&#30721;&#22120;&#30340;Transformer&#65292;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#35745;&#31639;&#22522;&#20110;softmax&#30340;&#23494;&#38598;&#33258;&#27880;&#24847;&#21147;&#12290;Transformer-VQ&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#26159;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#38190;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#32531;&#23384;&#26426;&#21046;&#23454;&#29616;&#30340;&#12290;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#65292;Transformer-VQ&#22312;&#36136;&#37327;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;Enwik8(0.99 bpb)&#65292;PG-19(26.6 ppl)&#21644;ImageNet64(3.16 bpb)&#37117;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20889;&#20316;&#21019;&#36896;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#21019;&#24847;&#27979;&#35797;&#20013;&#36890;&#36807;&#30340;&#25968;&#37327;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26080;&#27861;&#20195;&#26367;&#19987;&#23478;&#36827;&#34892;TTCW&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.14556</link><description>&lt;p&gt;
&#33402;&#26415;&#36824;&#26159;&#25216;&#24039;&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21019;&#36896;&#21147;&#30340;&#34394;&#20551;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Art or Artifice? Large Language Models and the False Promise of Creativity. (arXiv:2309.14556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20889;&#20316;&#21019;&#36896;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#21019;&#24847;&#27979;&#35797;&#20013;&#36890;&#36807;&#30340;&#25968;&#37327;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26080;&#27861;&#20195;&#26367;&#19987;&#23478;&#36827;&#34892;TTCW&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#20174;&#21338;&#23458;&#21040;&#25925;&#20107;&#30340;&#39640;&#36136;&#37327;&#20889;&#20316;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23458;&#35266;&#35780;&#20272;&#19968;&#27573;&#25991;&#23383;&#30340;&#21019;&#36896;&#21147;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21463;&#21019;&#36896;&#24615;&#24605;&#32500;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTC)&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#20849;&#35782;&#35780;&#20272;&#25216;&#26415;[3]&#65292;&#25552;&#20986;&#20102;&#21019;&#36896;&#24615;&#20889;&#20316;&#30340;&#25176;&#20848;&#26031;&#27979;&#39564;(TTCW)&#26469;&#35780;&#20272;&#21019;&#36896;&#21147;&#20316;&#20026;&#19968;&#20010;&#20135;&#21697;&#12290;TTCW&#30001;&#21253;&#21547;&#22312;&#27969;&#30021;&#24230;&#12289;&#28789;&#27963;&#24615;&#12289;&#29420;&#21019;&#24615;&#21644;&#32454;&#33268;&#24230;&#21407;&#22987;&#32500;&#24230;&#20013;&#30340;14&#20010;&#20108;&#20803;&#27979;&#35797;&#32452;&#25104;&#12290;&#25105;&#20204;&#25307;&#21215;&#20102;10&#20301;&#21019;&#24847;&#20316;&#23478;&#65292;&#24182;&#20351;&#29992;TTCW&#23545;48&#20010;&#30001;&#19987;&#19994;&#20316;&#23478;&#25110;LLMs&#25776;&#20889;&#30340;&#25925;&#20107;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#25925;&#20107;&#36890;&#36807;&#30340;TTCW&#27979;&#35797;&#27604;&#19987;&#19994;&#20316;&#23478;&#20889;&#30340;&#25925;&#20107;&#23569;&#20102;3-10&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#35780;&#20215;&#32773;&#65292;&#20197;&#33258;&#21160;&#21270;TTCW&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#27809;&#26377;&#19968;&#20010;LLM&#19982;&#19987;&#23478;&#35780;&#20272;&#21576;&#27491;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#21477;&#23376;&#23884;&#20837;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#35299;&#30721;&#20102;&#21452;&#20154;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#65292;&#24182;&#21457;&#29616;&#22312;&#20914;&#31361;&#23545;&#35805;&#20013;&#65292;&#22971;&#23376;&#30340;&#24773;&#24863;&#19982;&#35821;&#20041;&#30456;&#20284;&#24615;&#21576;&#27491;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.12646</link><description>&lt;p&gt;
&#22312;&#21452;&#20154;&#23545;&#35805;&#20013;&#35299;&#30721;&#24773;&#24863;&#65306;&#36890;&#36807;&#21477;&#23376;&#23884;&#20837;&#21033;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decoding Affect in Dyadic Conversations: Leveraging Semantic Similarity through Sentence Embedding. (arXiv:2309.12646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#21477;&#23376;&#23884;&#20837;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#35299;&#30721;&#20102;&#21452;&#20154;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#65292;&#24182;&#21457;&#29616;&#22312;&#20914;&#31361;&#23545;&#35805;&#20013;&#65292;&#22971;&#23376;&#30340;&#24773;&#24863;&#19982;&#35821;&#20041;&#30456;&#20284;&#24615;&#21576;&#27491;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#31361;&#26174;&#20102;&#21477;&#23376;&#23884;&#20837;&#22312;&#27979;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#20998;&#26512;&#29616;&#23454;&#20013;&#30340;&#21452;&#20154;&#20114;&#21160;&#24182;&#39044;&#27979;&#23545;&#35805;&#21442;&#19982;&#32773;&#24773;&#24863;&#26041;&#38754;&#30340;&#24212;&#29992;&#20173;&#28982;&#24456;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;50&#23545;&#22827;&#22971;&#20043;&#38388;&#20851;&#20110;&#20914;&#31361;&#21644;&#24841;&#24555;&#27963;&#21160;&#30340;&#21475;&#22836;&#23545;&#35805;&#12290;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;all-MiniLM-L6-v2&#26469;&#33719;&#24471;&#27599;&#20010;&#21457;&#35328;&#32773;&#35805;&#35821;&#30340;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23884;&#20837;&#30456;&#37051;&#35805;&#35821;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#30340;&#24179;&#22343;&#20540;&#23545;&#23545;&#35805;&#30340;&#25972;&#20307;&#30456;&#20284;&#24615;&#36827;&#34892;&#37327;&#21270;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#19982;&#22971;&#23376;&#22312;&#20914;&#31361;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#21576;&#27491;&#30456;&#20851;&#65288;&#20294;&#22312;&#24841;&#24555;&#23545;&#35805;&#20013;&#19981;&#30456;&#20851;&#65289;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#23545;&#35805;&#31867;&#22411;&#22914;&#20309;&#65292;&#37117;&#26410;&#35266;&#23519;&#21040;&#36825;&#31181;&#30456;&#20851;&#24615;&#19982;&#19976;&#22827;&#30340;&#24773;&#24863;&#20043;&#38388;&#12290;&#20004;&#20010;&#39564;&#35777;&#26816;&#39564;&#36827;&#19968;&#27493;&#25903;&#25345;&#20102;t
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing (NLP) have highlighted the potential of sentence embeddings in measuring semantic similarity. Yet, its application in analyzing real-world dyadic interactions and predicting the affect of conversational participants remains largely uncharted. To bridge this gap, the present study utilizes verbal conversations within 50 married couples talking about conflicts and pleasant activities. Transformer-based model all-MiniLM-L6-v2 was employed to obtain the embeddings of the utterances from each speaker. The overall similarity of the conversation was then quantified by the average cosine similarity between the embeddings of adjacent utterances. Results showed that semantic similarity had a positive association with wives' affect during conflict (but not pleasant) conversations. Moreover, this association was not observed with husbands' affect regardless of conversation types. Two validation checks further provided support for the validity of t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#30340;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#30001;&#21442;&#32771;&#31572;&#26696;&#22238;&#31572;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#25351;&#26631;&#32467;&#26524;&#21487;&#38752;&#65292;&#24182;&#19982;&#20154;&#24037;&#35780;&#20215;&#19968;&#33268;&#12290;&#24182;&#19988;&#36825;&#20010;&#25351;&#26631;&#21487;&#20197;&#34917;&#20805;&#20256;&#32479;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.12546</link><description>&lt;p&gt;
&#38382;&#21477;&#29983;&#25104;&#30340;&#33258;&#21160;&#22238;&#31572;&#21487;&#34892;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Answerability Evaluation for Question Generation. (arXiv:2309.12546v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12546
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#30340;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#30001;&#21442;&#32771;&#31572;&#26696;&#22238;&#31572;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#25351;&#26631;&#32467;&#26524;&#21487;&#38752;&#65292;&#24182;&#19982;&#20154;&#24037;&#35780;&#20215;&#19968;&#33268;&#12290;&#24182;&#19988;&#36825;&#20010;&#25351;&#26631;&#21487;&#20197;&#34917;&#20805;&#20256;&#32479;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;BLEU&#21644;ROUGE&#65292;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#24320;&#21457;&#30340;&#65292;&#23427;&#20204;&#22522;&#20110;&#29983;&#25104;&#25991;&#26412;&#19982;&#21442;&#32771;&#25991;&#26412;&#20043;&#38388;&#30340;n-gram&#37325;&#21472;&#24230;&#26469;&#34913;&#37327;&#12290;&#36825;&#20123;&#31616;&#21333;&#30340;&#35780;&#20272;&#25351;&#26631;&#21487;&#33021;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#27604;&#22914;&#38382;&#21477;&#29983;&#25104;&#65288;QG&#65289;&#65292;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22240;&#20026;QG&#38656;&#35201;&#29983;&#25104;&#21487;&#20197;&#30001;&#21442;&#32771;&#31572;&#26696;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26356;&#22797;&#26434;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#20173;&#28982;&#26159;QG&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#32039;&#36843;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21487;&#22238;&#31572;&#24615;&#24230;&#37327;&#65288;PMAN&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#30340;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#30001;&#21442;&#32771;&#31572;&#26696;&#22238;&#31572;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#35780;&#20272;&#32467;&#26524;&#21487;&#38752;&#65292;&#24182;&#19982;&#20154;&#24037;&#35780;&#20215;&#19968;&#33268;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24212;&#29992;&#25105;&#20204;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;QG&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#25351;&#26631;&#34917;&#20805;&#20102;&#20256;&#32479;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#22522;&#20110;ChatGPT&#30340;QG&#27169;&#22411;&#30340;&#23454;&#29616;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed for natural language generation (NLG) tasks, are based on measuring the n-gram overlap between the generated and reference text. These simple metrics may be insufficient for more complex tasks, such as question generation (QG), which requires generating questions that are answerable by the reference answers. Developing a more sophisticated automatic evaluation metric, thus, remains as an urgent problem in QG research. This work proposes a Prompting-based Metric on ANswerability (PMAN), a novel automatic evaluation metric to assess whether the generated questions are answerable by the reference answers for the QG tasks. Extensive experiments demonstrate that its evaluation results are reliable and align with human evaluations. We further apply our metric to evaluate the performance of QG models, which shows our metric complements conventional metrics. Our implementation of a ChatGPT-based QG model achieves stat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#20197;&#35299;&#20915;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#32780;&#19981;&#38656;&#35201;&#26799;&#24230;&#35775;&#38382;&#25110;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08758</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Discrete Prompt Compression with Reinforcement Learning. (arXiv:2308.08758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#20197;&#35299;&#20915;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#32780;&#19981;&#38656;&#35201;&#26799;&#24230;&#35775;&#38382;&#25110;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#34987;&#29992;&#25143;&#24191;&#27867;&#20351;&#29992;&#26469;&#35299;&#20915;&#19982;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#30456;&#20851;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#30001;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#38271;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#40723;&#21169;&#24320;&#21457;&#21387;&#32553;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23481;&#32435;&#22810;&#20010;&#35760;&#21495;&#21547;&#20041;&#12290;&#36825;&#22312;&#35299;&#37322;&#24615;&#12289;&#22266;&#23450;&#25968;&#37327;&#30340;&#23884;&#20837;&#35760;&#21495;&#12289;&#22312;&#19981;&#21516;LM&#20043;&#38388;&#30340;&#21487;&#37325;&#29992;&#24615;&#20197;&#21450;&#19982;&#40657;&#30418;API&#20132;&#20114;&#26102;&#30340;&#19981;&#36866;&#29992;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#21387;&#32553;&#26041;&#27861;&#65288;PCRL&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;PCRL&#37319;&#29992;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#30452;&#25509;&#32534;&#36753;&#25552;&#31034;&#12290;PCRL&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#28789;&#27963;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;LM&#65292;&#20197;&#21450;&#21482;&#26377;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#26799;&#24230;&#35775;&#38382;LM&#25110;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Language Models (LMs) are widely used by users to address various problems with task-specific prompts. Constraints associated with the context window length and computational costs encourage the development of compressed prompts. Existing methods rely heavily on training embeddings, which are designed to accommodate multiple token meanings. This presents challenges in terms of interpretability, a fixed number of embedding tokens, reusability across different LMs, and inapplicability when interacting with black-box APIs. This study proposes prompt compression with reinforcement learning (PCRL), a novel discrete prompt compression method that addresses these issues. PCRL employs a computationally efficient policy network that directly edits prompts. The PCRL training approach can be flexibly applied to various types of LMs, as well as decoder-only and encoder-decoder architecture, and can be trained without gradient access to LMs or labeled data. PCRL achieves an averag
&lt;/p&gt;</description></item><item><title>LLMeBench&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;LLMs&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21487;&#20197;&#23450;&#21046;&#20219;&#20309;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#26080;&#35770;&#35821;&#35328;&#65292;&#25903;&#25345;&#38646;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#26032;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#12290;&#24050;&#32463;&#22312;31&#20010;&#29420;&#29305;&#30340;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#35745;&#21010;&#23558;&#26694;&#26550;&#24320;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.04945</link><description>&lt;p&gt;
LLMeBench&#65306;&#29992;&#20110;&#21152;&#36895;LLMs&#22522;&#20934;&#27979;&#35797;&#30340;&#28789;&#27963;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking. (arXiv:2308.04945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04945
&lt;/p&gt;
&lt;p&gt;
LLMeBench&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;LLMs&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21487;&#20197;&#23450;&#21046;&#20219;&#20309;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#26080;&#35770;&#35821;&#35328;&#65292;&#25903;&#25345;&#38646;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#26032;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#12290;&#24050;&#32463;&#22312;31&#20010;&#29420;&#29305;&#30340;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#35745;&#21010;&#23558;&#26694;&#26550;&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#25104;&#21151;&#20351;&#24471;&#38656;&#35201;&#35780;&#20272;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#24182;&#20844;&#24320;&#20102;&#20960;&#20010;&#26694;&#26550;&#65292;&#20294;&#23545;&#20110;&#19981;&#21516;&#29992;&#25143;&#26469;&#35828;&#65292;&#23427;&#20204;&#23545;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#23450;&#21046;&#33021;&#21147;&#36890;&#24120;&#24456;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMeBench&#26694;&#26550;&#12290;&#26368;&#21021;&#26159;&#20026;&#20102;&#20351;&#29992;OpenAI&#30340;GPT&#21644;BLOOM&#27169;&#22411;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#32780;&#24320;&#21457;&#30340;&#65307;&#23427;&#21487;&#20197;&#26080;&#32541;&#23450;&#21046;&#20219;&#20309;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#26080;&#35770;&#35821;&#35328;&#22914;&#20309;&#12290;&#35813;&#26694;&#26550;&#36824;&#20855;&#26377;&#38646;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#12290;&#21487;&#20197;&#22312;&#19981;&#21040;10&#20998;&#38047;&#20869;&#28155;&#21152;&#26032;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#24049;&#30340;&#27169;&#22411;API&#23494;&#38053;&#26469;&#35780;&#20272;&#24403;&#21069;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#24050;&#32463;&#22312;90&#20010;&#23454;&#39564;&#35774;&#32622;&#20013;&#20351;&#29992;53&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#23545;31&#20010;&#29420;&#29305;&#30340;NLP&#20219;&#21153;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28041;&#21450;&#22823;&#32422;296K&#20010;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#35745;&#21010;&#23558;&#35813;&#26694;&#26550;&#24320;&#28304;&#20379;&#31038;&#21306;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages. Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users. In this study, we introduce the LLMeBench framework. Initially developed to evaluate Arabic NLP tasks using OpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP task and model, regardless of language. The framework also features zero- and few-shot learning settings. A new custom dataset can be added in less than 10 minutes, and users can use their own model API keys to evaluate the task at hand. The developed framework has been already tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points. We plan to open-source the framework for the community
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#38405;&#35835;&#36807;&#31243;&#20013;&#30340;&#30524;&#21160;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#25512;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#33021;&#22815;&#27169;&#25311;&#38405;&#35835;&#38556;&#30861;&#20013;&#19981;&#36866;&#24212;&#25512;&#29702;&#25928;&#26524;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.04941</link><description>&lt;p&gt;
&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#25512;&#29702;&#20197;&#29702;&#35299;&#38405;&#35835;&#21644;&#38405;&#35835;&#38556;&#30861;&#20013;&#30340;&#30524;&#21160;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Integrating large language models and active inference to understand eye movements in reading and dyslexia. (arXiv:2308.04941v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#38405;&#35835;&#36807;&#31243;&#20013;&#30340;&#30524;&#21160;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#25512;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#33021;&#22815;&#27169;&#25311;&#38405;&#35835;&#38556;&#30861;&#20013;&#19981;&#36866;&#24212;&#25512;&#29702;&#25928;&#26524;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#37319;&#29992;&#23618;&#27425;&#21270;&#20027;&#21160;&#25512;&#29702;&#26469;&#27169;&#25311;&#38405;&#35835;&#21644;&#30524;&#21160;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#23558;&#35821;&#35328;&#22788;&#29702;&#25551;&#36848;&#20026;&#23545;&#23618;&#27425;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#65292;&#20174;&#38899;&#33410;&#21040;&#21477;&#23376;&#30340;&#19981;&#21516;&#31890;&#24230;&#23454;&#29616;&#39044;&#27979;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#29992;&#20110;&#23454;&#29616;&#36924;&#30495;&#30340;&#25991;&#26412;&#39044;&#27979;&#65292;&#20197;&#21450;&#20027;&#21160;&#25512;&#29702;&#29992;&#20110;&#24341;&#23548;&#30524;&#21160;&#21040;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#24471;&#23545;&#39044;&#27979;&#36827;&#34892;&#27979;&#35797;&#25104;&#20026;&#21487;&#33021;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29087;&#32451;&#38405;&#35835;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#21333;&#35789;&#21644;&#21477;&#23376;&#65292;&#24182;&#36981;&#24490;&#38405;&#35835;&#21452;&#36335;&#29702;&#35770;&#20013;&#30340;&#35789;&#27719;&#21644;&#38750;&#35789;&#27719;&#36335;&#24452;&#30340;&#21306;&#20998;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20801;&#35768;&#27169;&#25311;&#38405;&#35835;&#36807;&#31243;&#20013;&#23545;&#30524;&#21160;&#34892;&#20026;&#20135;&#29983;&#19981;&#36866;&#24212;&#25512;&#29702;&#25928;&#26524;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#38405;&#35835;&#38556;&#30861;&#12290;&#20026;&#20102;&#27169;&#25311;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#20943;&#24369;&#20102;&#20808;&#39564;&#30340;&#36129;&#29486;&#65292;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#21644;&#26356;&#21152;&#26029;&#29255;&#21270;&#30340;&#38405;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel computational model employing hierarchical active inference to simulate reading and eye movements. The model characterizes linguistic processing as inference over a hierarchical generative model, facilitating predictions and inferences at various levels of granularity, from syllables to sentences.  Our approach combines the strengths of large language models for realistic textual predictions and active inference for guiding eye movements to informative textual information, enabling the testing of predictions. The model exhibits proficiency in reading both known and unknown words and sentences, adhering to the distinction between lexical and nonlexical routes in dual-route theories of reading. Notably, our model permits the exploration of maladaptive inference effects on eye movements during reading, such as in dyslexia. To simulate this condition, we attenuate the contribution of priors during the reading process, leading to incorrect inferences and a more fragmented
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35266;&#24565;&#30340;&#22240;&#26524;&#25506;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#20248;&#21270;&#20102;&#22240;&#26524;&#27010;&#24565;&#23376;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#27010;&#24565;&#25511;&#21046;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.15054</link><description>&lt;p&gt;
&#19968;&#31181;&#20960;&#20309;&#35266;&#24565;&#30340;&#22240;&#26524;&#25506;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Geometric Notion of Causal Probing. (arXiv:2307.15054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35266;&#24565;&#30340;&#22240;&#26524;&#25506;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#20248;&#21270;&#20102;&#22240;&#26524;&#27010;&#24565;&#23376;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#27010;&#24565;&#25511;&#21046;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;&#20110;&#25991;&#26412;&#30340;&#23454;&#20540;&#34920;&#31034;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#20123;&#34920;&#31034;&#21253;&#21547;&#20102;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#23398;&#21040;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#35821;&#35328;&#23646;&#24615;&#21644;&#22522;&#20110;&#24615;&#21035;&#30340;&#20154;&#21475;&#20559;&#35265;&#31561;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#36890;&#36807;&#22312;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#27491;&#20132;&#25237;&#24433;&#26469;&#33719;&#24471;&#20851;&#20110;&#36825;&#20123;&#27010;&#24565;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#20869;&#22312;&#20449;&#24687;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#20026;&#36825;&#39033;&#30740;&#31350;&#36129;&#29486;&#20102;&#26032;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#26041;&#27861;&#26469;&#36991;&#20813;&#34394;&#20551;&#30456;&#20851;&#30340;&#22833;&#25928;&#27169;&#24335;&#65292;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#23376;&#31354;&#38388;&#20013;&#30340;&#20998;&#37327;&#21644;&#20854;&#27491;&#20132;&#34917;&#31354;&#38388;&#20013;&#30340;&#20998;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23376;&#31354;&#38388;&#20013;&#30340;&#21453;&#20107;&#23454;&#20449;&#24687;&#27010;&#24565;&#26159;&#30001;&#19968;&#20010;&#22240;&#26524;&#27010;&#24565;&#23376;&#31354;&#38388;&#36827;&#34892;&#20248;&#21270;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#24178;&#39044;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#25805;&#20316;&#26469;&#23581;&#35797;&#27010;&#24565;&#25511;&#21046;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models rely on real-valued representations of text to make their predictions. These representations contain information learned from the data that the model has trained on, including knowledge of linguistic properties and forms of demographic bias, e.g., based on gender. A growing body of work has considered information about concepts such as these using orthogonal projections onto subspaces of the representation space. We contribute to this body of work by proposing a formal definition of intrinsic information in a subspace of a language model's representation space. We propose a counterfactual approach that avoids the failure mode of spurious correlations (Kumar et al., 2022) by treating components in the subspace and its orthogonal complement independently. We show that our counterfactual notion of information in a subspace is optimizing by an causal concept subspace. Furthermore, this intervention allows us to attempt concept controlled generation by manipulating the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.12856</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#35268;&#21010;&#12289;&#38271;&#26399;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#31243;&#24207;&#21512;&#25104;&#33021;&#21147;&#30340;&#29616;&#23454;&#19990;&#30028;WebAgent
&lt;/p&gt;
&lt;p&gt;
A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#21010;&#12289;&#24635;&#32467;&#21644;&#29983;&#25104;&#20195;&#30721;&#26469;&#25552;&#39640;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#20027;Web&#33258;&#21160;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#32593;&#31449;&#19978;&#65292;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#19977;&#20010;&#26041;&#38754;&#30340;&#38480;&#21046;&#65306;&#24320;&#25918;&#39046;&#22495;&#24615;&#12289;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#23545;HTML&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WebAgent&#30340;LLM&#39537;&#21160;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#25105;&#32463;&#39564;&#23398;&#20064;&#65292;&#22312;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#21069;&#25552;&#19979;&#65292;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;WebAgent&#36890;&#36807;&#23558;&#25351;&#20196;&#20998;&#35299;&#20026;&#35268;&#33539;&#30340;&#23376;&#25351;&#20196;&#65292;&#23558;&#38271;HTML&#25991;&#26723;&#24635;&#32467;&#20026;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29255;&#27573;&#65292;&#24182;&#36890;&#36807;&#20174;&#20013;&#29983;&#25104;&#30340;Python&#31243;&#24207;&#23545;&#32593;&#31449;&#36827;&#34892;&#25805;&#20316;&#26469;&#25552;&#21069;&#36827;&#34892;&#35268;&#21010;&#12290;&#25105;&#20204;&#20351;&#29992;Flan-U-PaLM&#35774;&#35745;&#20102;WebAgent&#65292;&#29992;&#20110;&#29983;&#25104;&#26377;&#26681;&#20195;&#30721;&#65292;&#24182;&#20351;&#29992;HTML-T5&#36827;&#34892;&#39044;&#35757;&#32451;LLMs&#65292;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#20197;&#21450;&#28151;&#21512;&#38271;&#36328;&#24230;&#21435;&#22122;&#30446;&#26631;&#26469;&#36827;&#34892;&#35268;&#21010;&#21644;&#24635;&#32467;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#38382;&#39064;&#65292;&#24341;&#20837;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;EEA&#26041;&#27861;&#21450;&#25552;&#20986;&#30340;&#29983;&#25104;&#30340;EEA&#65288;GEEA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#23454;&#29616;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#19988;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#30340;&#23454;&#20307;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14651</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#23454;&#20307;&#23545;&#40784;&#21450;&#36229;&#36234;&#65306;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Revisit and Outstrip Entity Alignment: A Perspective of Generative Models. (arXiv:2305.14651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#38382;&#39064;&#65292;&#24341;&#20837;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;EEA&#26041;&#27861;&#21450;&#25552;&#20986;&#30340;&#29983;&#25104;&#30340;EEA&#65288;GEEA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#23454;&#29616;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#19988;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#30340;&#23454;&#20307;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29983;&#25104;&#27169;&#22411;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EEA&#65289;&#12290;&#25105;&#20204;&#34920;&#26126;EEA&#26159;&#19968;&#20010;&#29305;&#27530;&#30340;&#38382;&#39064;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#31867;&#20284;&#20110;&#20856;&#22411;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#30446;&#26631;&#65292;&#22522;&#20110;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;EEA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20182;&#20204;&#19981;&#23436;&#25972;&#30340;&#30446;&#26631;&#38480;&#21046;&#20102;&#23454;&#20307;&#23545;&#40784;&#21644;&#23454;&#20307;&#21512;&#25104;&#65288;&#21363;&#29983;&#25104;&#26032;&#23454;&#20307;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#30340;EEA&#65288;abbr.&#65292;GEEA&#65289;&#26694;&#26550;&#21644;&#25552;&#20986;&#30340;&#20114;&#30456;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;M-VAE&#65289;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;M-VAE&#21487;&#20197;&#23558;&#19968;&#20010;&#23454;&#20307;&#20174;&#19968;&#20010;KG&#36716;&#25442;&#21040;&#21478;&#19968;&#20010;KG&#65292;&#24182;&#20174;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#29983;&#25104;&#26032;&#23454;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;GEEA&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent embedding-based methods have achieved great successes on exploiting entity alignment from knowledge graph (KG) embeddings of multiple modals. In this paper, we study embedding-based entity alignment (EEA) from a perspective of generative models. We show that EEA is a special problem where the main objective is analogous to that in a typical generative model, based on which we theoretically prove the effectiveness of the recently developed generative adversarial network (GAN)-based EEA methods. We then reveal that their incomplete objective limits the capacity on both entity alignment and entity synthesis (i.e., generating new entities). We mitigate this problem by introducing a generative EEA (abbr., GEEA) framework with the proposed mutual variational autoencoder (M-VAE) as the generative model. M-VAE can convert an entity from one KG to another and generate new entities from random noise vectors. We demonstrate the power of GEEA with theoretical analysis and empirical experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#32771;&#32771;&#35797;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;GAOKAO-Benchmark&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;ChatGPT&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#22312;&#23458;&#35266;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#20854;&#19981;&#36275;&#20043;&#22788;&#21644;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.12474</link><description>&lt;p&gt;
&#22312;&#39640;&#32771;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Performance of Large Language Models on GAOKAO Benchmark. (arXiv:2305.12474v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#32771;&#32771;&#35797;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;GAOKAO-Benchmark&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#38382;&#39064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;ChatGPT&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#22312;&#23458;&#35266;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#20854;&#19981;&#36275;&#20043;&#22788;&#21644;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#23427;&#20204;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#20219;&#21153;&#20013;&#30340;&#21151;&#25928;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GAOKAO-Benchmark&#65288;GAOKAO-Bench&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#30452;&#35266;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#20351;&#29992;&#20013;&#22269;&#39640;&#32771;&#32771;&#35797;&#30340;&#39064;&#30446;&#20316;&#20026;&#27979;&#35797;&#26679;&#26412;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#23613;&#21487;&#33021;&#22320;&#20351;&#35780;&#20272;&#32467;&#26524;&#19982;&#20154;&#31867;&#19968;&#33268;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;-shot&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#20026;&#20027;&#35266;&#21644;&#23458;&#35266;&#31867;&#22411;&#26469;&#20998;&#26512;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#35780;&#20998;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#27169;&#22411;&#22312;GAOKAO-Benchmark&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT&#27169;&#22411;&#22312;&#35299;&#20915;&#23458;&#35266;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#20854;&#19981;&#36275;&#20043;&#22788;&#21644;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#23457;&#26597;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#20154;&#31867;&#35780;&#20272;&#12290;&#24635;&#20043;&#65292;&#26412;&#30740;&#31350;&#20026;&#21019;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#35780;&#20272;GAOKAO&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated remarkable performance across various natural language processing tasks; however, their efficacy in more challenging and domain-specific tasks remains less explored. This paper introduces the GAOKAO-Benchmark (GAOKAO-Bench), an intuitive benchmark that employs questions from the Chinese Gaokao examination as test samples for evaluating large language models.In order to align the evaluation results with humans as much as possible, we designed a method based on zero-shot prompts to analyze the accuracy and scoring rate of the model by dividing the questions into subjective and objective types. We evaluated the ChatGPT model on GAOKAO-Benchmark performance.Our findings reveal that the ChatGPT model excels in tackling objective questions, while also shedding light on its shortcomings and areas for improvement. To further scrutinize the model's responses, we incorporate human evaluations.In conclusion, this research contributes a robust evaluation ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09859</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27969;&#30021;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#38750;&#24120;&#30456;&#20284;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#35805;&#35821;&#65292;&#22240;&#27492;&#21306;&#20998;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#20154;&#31867;&#20889;&#20316;&#30340;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#35780;&#35770;&#24182;&#27169;&#20223;&#26576;&#20123;&#20316;&#32773;&#21644;&#20154;&#29289;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340; logits&#65292;&#25110;&#38656;&#35201;&#21487;&#20197;&#20174;&#30446;&#26631;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#31181;&#40657;&#21283;&#23376;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#35266;&#23519;&#21040;&#29983;&#25104;&#25991;&#26412;&#22312;&#29983;&#25104;&#22120;&#30340;&#20284;&#28982;&#20989;&#25968;&#19979;&#26159;&#23616;&#37096;&#26368;&#20248;&#30340;&#65292;&#32780;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#21017;&#19981;&#26159;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#36739;&#23567;&#19988;&#37096;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65306;&#23427;&#20204;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#26816;&#27979;&#26469;&#33258;&#23567;&#22411;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#30456;&#21516;&#30340;&#35821;&#26009;&#24211;&#23545;&#26816;&#27979;&#24615;&#33021;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#65288;&#37096;&#20998;&#65289;&#35823;&#36127;&#26679;&#26412;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26356;&#19968;&#33324;&#19979;&#30028;&#24418;&#24335;&#30340;&#25351;&#23548;&#19979;&#35843;&#33410;&#36328;&#27169;&#24577;&#30456;&#20284;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#31934;&#30830;&#22320;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#23450;&#28857;&#21644;&#20854;&#36127;&#38754;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.05496</link><description>&lt;p&gt;
&#21033;&#29992;&#20266;&#22270;&#20687;&#35828;&#26126;&#36827;&#34892;&#22810;&#27169;&#24577;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Exploiting Pseudo Image Captions for Multimodal Summarization. (arXiv:2305.05496v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#65288;&#37096;&#20998;&#65289;&#35823;&#36127;&#26679;&#26412;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26356;&#19968;&#33324;&#19979;&#30028;&#24418;&#24335;&#30340;&#25351;&#23548;&#19979;&#35843;&#33410;&#36328;&#27169;&#24577;&#30456;&#20284;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#31934;&#30830;&#22320;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#23450;&#28857;&#21644;&#20854;&#36127;&#38754;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#38754;&#20020;&#65288;&#37096;&#20998;&#65289;&#35823;&#36127;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20174;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#20248;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#24403;&#23384;&#22312;&#22122;&#22768;&#26102;&#65292;&#21253;&#25324;&#36127;&#26679;&#26412;&#30340;MI&#20063;&#24456;&#37325;&#35201;&#12290;&#22312;&#26356;&#19968;&#33324;&#30340;&#20248;&#21270;&#19979;&#30028;&#24418;&#24335;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#36880;&#27493;&#32454;&#21270;&#30340;&#36328;&#27169;&#24577;&#30456;&#20284;&#24230;&#35843;&#33410;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#31934;&#30830;&#22320;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#23450;&#28857;&#21644;&#20854;&#36127;&#38754;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;MI&#65292;&#32780;&#19981;&#26159;&#38169;&#35823;&#22320;&#23558;&#20854;&#26368;&#23567;&#21270;&#12290;&#22312;&#22235;&#20010;&#19979;&#28216;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#29702;&#35770;&#25351;&#23548;&#19979;&#31995;&#32479;&#22320;&#24179;&#34913;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#36127;&#26679;&#26412;&#30340;&#26377;&#21033;&#21644;&#26377;&#23475;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#27809;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20170;&#21518;&#21487;&#20197;&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#20351;&#29992;&#65292;</title><link>http://arxiv.org/abs/2305.03514</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#25913;&#21464;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Transform Computational Social Science?. (arXiv:2305.03514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#27809;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20170;&#21518;&#21487;&#20197;&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#20351;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#25104;&#21151;&#22320;&#22312;&#35768;&#22810;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#36827;&#34892;&#38646;-shot&#25805;&#20316;&#65288;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65289;&#12290;&#22914;&#26524;&#36825;&#31181;&#33021;&#21147;&#20063;&#36866;&#29992;&#20110;&#23545;&#35828;&#26381;&#21147;&#21644;&#25919;&#27835;&#24847;&#35782;&#24418;&#24577;&#31561;&#31038;&#20250;&#29616;&#35937;&#30340;&#32534;&#30721;&#65292;&#37027;&#20040;LLMs&#23601;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21464;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;(CSS)&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;CSS&#24037;&#20855;&#30340;&#36335;&#32447;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#20248;&#31168;&#30340;&#25552;&#31034;&#23454;&#36341;&#20197;&#21450;&#19968;&#20010;&#24191;&#27867;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#20197;&#27979;&#37327;13&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;24&#20010;&#20195;&#34920;&#24615;&#30340;CSS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;LLMs&#26080;&#27861;&#36229;&#36234;&#26368;&#20339;&#24494;&#35843;&#27169;&#22411;&#65292;&#20294;&#20173;&#28982;&#19982;&#20154;&#31867;&#36798;&#25104;&#20102;&#20844;&#24179;&#30340;&#21327;&#35758;&#27700;&#24179;&#12290;&#22312;&#33258;&#30001;&#24418;&#24335;&#30340;&#32534;&#30721;&#20219;&#21153;&#65288;&#29983;&#25104;&#65289;&#19978;&#65292;LLMs&#29983;&#25104;&#30340;&#35299;&#37322;&#24120;&#24120;&#36229;&#36807;&#20102;&#24037;&#20316;&#32773;&#30340;&#40644;&#37329;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#20170;&#22825;&#30340;LLMs&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#20174;&#26681;&#26412;&#19978;&#22686;&#24378;CSS&#30740;&#31350;&#27969;&#31243;&#65306;(1)&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#26080;&#32541;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20960;&#20010;&#26032;&#26041;&#21521;&#65292;&#21253;&#25324;&#39118;&#26684;&#21270;MT&#12289;&#20132;&#20114;&#24335;MT&#21644;&#22522;&#20110;&#32763;&#35793;&#35760;&#24518;&#30340;MT&#65292;&#24182;&#35752;&#35770;&#20102;&#38544;&#31169;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.01181</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#32763;&#35793;&#26032;&#36235;&#21183;&#65306;&#20197;ChatGPT&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT. (arXiv:2305.01181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20960;&#20010;&#26032;&#26041;&#21521;&#65292;&#21253;&#25324;&#39118;&#26684;&#21270;MT&#12289;&#20132;&#20114;&#24335;MT&#21644;&#22522;&#20110;&#32763;&#35793;&#35760;&#24518;&#30340;MT&#65292;&#24182;&#35752;&#35770;&#20102;&#38544;&#31169;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#21160;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;GPT-3&#21644;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#21518;&#12290;&#36825;&#20026;&#20351;&#29992;LLMs&#30340;MT&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#20351;&#29992;LLMs&#30340;MT&#26041;&#21521;&#65292;&#21253;&#25324;&#39118;&#26684;&#21270;MT&#12289;&#20132;&#20114;&#24335;MT&#21644;&#22522;&#20110;&#32763;&#35793;&#35760;&#24518;&#30340;MT&#65292;&#20197;&#21450;&#19968;&#31181;&#20351;&#29992;LLMs&#30340;&#26032;&#35780;&#20272;&#33539;&#20363;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#30340;MT&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#26412;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#20197;&#20943;&#36731;&#27492;&#31867;&#39118;&#38505;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#20960;&#20010;&#20197;&#19978;&#25552;&#21040;&#30340;&#26032;&#26041;&#21521;&#30340;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#21521;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;MT&#26410;&#26469;&#30740;&#31350;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Translation (MT) has made significant progress in recent years using deep learning, especially after the emergence of large language models (LLMs) such as GPT-3 and ChatGPT. This brings new challenges and opportunities for MT using LLMs. In this paper, we brainstorm some interesting directions for MT using LLMs, including stylized MT, interactive MT, and Translation Memory-based MT, as well as a new evaluation paradigm using LLMs. We also discuss the privacy concerns in MT using LLMs and a basic privacy-preserving method to mitigate such risks. To illustrate the potential of our proposed directions, we present several examples for the new directions mentioned above, demonstrating the feasibility of the proposed directions and highlight the opportunities and challenges for future research in MT using LLMs.
&lt;/p&gt;</description></item></channel></rss>