<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10892</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27700;&#21360;&#35777;&#26126;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;
&lt;/p&gt;
&lt;p&gt;
Proving membership in LLM pretraining data via data watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10892
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#30340;&#20316;&#21697;&#26159;&#21542;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#23454;&#29616;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#35775;&#38382;&#30340;&#21512;&#29702;&#26816;&#27979;&#65292;&#21069;&#25552;&#26159;&#29256;&#26435;&#25345;&#26377;&#20154;&#22312;&#20844;&#24320;&#21457;&#24067;&#20043;&#21069;&#36129;&#29486;&#20102;&#22810;&#20010;&#35757;&#32451;&#25991;&#26723;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#27700;&#21360;&#22788;&#29702;&#12290;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#25968;&#25454;&#27700;&#21360;&#65292;&#26816;&#27979;&#21487;&#20197;&#34987;&#26500;&#36896;&#20026;&#20551;&#35774;&#26816;&#39564;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#35823;&#26816;&#29575;&#30340;&#20445;&#35777;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#27700;&#21360;&#65306;&#19968;&#31181;&#25554;&#20837;&#38543;&#26426;&#24207;&#21015;&#65292;&#21478;&#19968;&#31181;&#38543;&#26426;&#29992;Unicode&#31867;&#20284;&#23383;&#31526;&#26367;&#25442;&#23383;&#31526;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;&#27700;&#21360;&#35774;&#35745;&#30340;&#19977;&#20010;&#26041;&#38754;--&#27700;&#21360;&#38271;&#24230;&#12289;&#22797;&#21046;&#27425;&#25968;&#21644;&#24178;&#25200;--&#22914;&#20309;&#24433;&#21709;&#20551;&#35774;&#26816;&#39564;&#30340;&#33021;&#21147;&#12290;&#25509;&#30528;&#30740;&#31350;&#20102;&#27700;&#21360;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#22914;&#20309;&#21464;&#21270;&#65306;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#20250;&#38477;&#20302;&#27700;&#21360;&#30340;&#24378;&#24230;&#65292;&#27700;&#21360;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10892v1 Announce Type: cross  Abstract: Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks
&lt;/p&gt;</description></item><item><title>&#25351;&#23548;&#35843;&#25972;&#36890;&#36807;&#22686;&#21152;&#25351;&#20196;&#38598;&#30340;&#22810;&#26679;&#24615;&#26469;&#25512;&#21160;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10891</link><description>&lt;p&gt;
&#25351;&#23548;&#22810;&#26679;&#24615;&#25512;&#21160;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Instruction Diversity Drives Generalization To Unseen Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10891
&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#36890;&#36807;&#22686;&#21152;&#25351;&#20196;&#38598;&#30340;&#22810;&#26679;&#24615;&#26469;&#25512;&#21160;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#8212;&#8212;&#22312;&#25351;&#20196;&#21644;&#26399;&#26395;&#32467;&#26524;&#20043;&#38388;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#8212;&#8212;&#26159;&#19968;&#31181;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#24182;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#26041;&#27861;&#12290;&#20854;&#23454;&#38469;&#25104;&#21151;&#21462;&#20915;&#20110;&#27169;&#22411;&#23398;&#20064;&#27604;&#20854;&#35757;&#32451;&#26102;&#26356;&#24191;&#27867;&#30340;&#25351;&#20196;&#38598;&#12290;&#28982;&#32780;&#65292;&#20915;&#23450;&#27169;&#22411;&#23545;&#36825;&#31181;&#8220;&#26410;&#35265;&#20219;&#21153;&#8221;&#30340;&#27867;&#21270;&#30340;&#22240;&#32032;&#23578;&#19981;&#21313;&#20998;&#28165;&#26970;&#12290;&#20026;&#20102;&#20102;&#35299;&#27867;&#21270;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#26412;&#25991;&#36890;&#36807;&#23383;&#31526;&#20018;&#37325;&#20889;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#26159;&#19968;&#20010;&#31526;&#21495;&#20219;&#21153;&#65292;&#26159;&#22270;&#28789;&#23436;&#25972;&#39532;&#23572;&#21487;&#22827;&#31639;&#27861;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#21516;&#26102;&#20801;&#35768;&#23454;&#39564;&#23545;&#8220;&#36755;&#20837;&#8221;&#21644;&#8220;&#25351;&#20196;&#8221;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27169;&#22411;&#25509;&#21463;&#30340;&#25351;&#20196;&#25968;&#37327;&#21644;&#20026;&#27599;&#20010;&#25351;&#20196;&#25552;&#20379;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#35266;&#23519;&#21040;&#25351;&#20196;&#38598;&#30340;&#22810;&#26679;&#24615;&#30830;&#23450;&#20102;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10891v1 Announce Type: cross  Abstract: Instruction tuning -- fine-tuning a large language model (LLM) on pairs of instructions and desired outcomes -- is an approach that enables pre-trained language models to perform real-world tasks and follow human instructions. Its practical success depends on the model learning a broader set of instructions than those it was trained on. Yet the factors that determine model generalization to such \emph{unseen tasks} are not well understood. %To understand the driving factors of generalization, In this paper, we experiment with string rewrites, a symbolic task that serves as a building block for Turing complete Markov algorithms while allowing experimental control of "inputs" and "instructions". We investigate the trade-off between the number of instructions the model is trained on and the number of training samples provided for each instruction and observe that the diversity of the instruction set determines generalization. Generalizati
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#25351;&#20986;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#37492;&#21035;&#22120;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#25165;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10890</link><description>&lt;p&gt;
LLM&#35268;&#21010;&#20013;&#26641;&#25628;&#32034;&#20309;&#26102;&#26377;&#29992;&#65311;&#21462;&#20915;&#20110;&#37492;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
When is Tree Search Useful for LLM Planning? It Depends on the Discriminator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10890
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#25351;&#20986;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#37492;&#21035;&#22120;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#25165;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#22312;&#22810;&#27493;&#38382;&#39064;&#19979;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#37492;&#21035;&#22120;&#21644;&#35268;&#21010;&#26041;&#27861;&#19977;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#20808;&#36827;&#35268;&#21010;&#26041;&#27861;&#65292;&#36845;&#20195;&#26657;&#27491;&#21644;&#26641;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#37492;&#21035;&#20934;&#30830;&#24615;&#22914;&#20309;&#24433;&#21709;&#20195;&#29702;&#22312;&#20351;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#25110;&#26356;&#31616;&#21333;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#26102;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#20004;&#39033;&#20219;&#21153;&#65292;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#21644;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;1&#65289;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#30340;&#37492;&#21035;&#22120;&#25165;&#33021;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65307;&#65288;2&#65289;&#24403;&#21069;LLMs&#30340;&#37492;&#21035;&#33021;&#21147;&#23578;&#26410;&#28385;&#36275;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#23454;&#29616;&#36825;&#31181;&#25913;&#36827;&#30340;&#38656;&#27714;&#65307;&#65288;3&#65289;&#37319;&#29992;&#22522;&#20110;LLM&#30340;&#37492;&#21035;&#22120;&#26102;&#65292;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10890v1 Announce Type: cross  Abstract: In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compare
&lt;/p&gt;</description></item><item><title>Reviewer2&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#20004;&#38454;&#27573;&#35780;&#35770;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#35780;&#35770;&#21487;&#33021;&#28041;&#21450;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#20998;&#24067;&#65292;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#35780;&#35770;&#65292;&#26356;&#22909;&#22320;&#28085;&#30422;&#20154;&#31867;&#23457;&#31295;&#20154;&#22312;&#33609;&#31295;&#20013;&#30830;&#23450;&#30340;&#21508;&#31181;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.10886</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#29983;&#25104;&#20248;&#21270;&#35780;&#35770;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reviewer2: Optimizing Review Generation Through Prompt Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10886
&lt;/p&gt;
&lt;p&gt;
Reviewer2&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#20004;&#38454;&#27573;&#35780;&#35770;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#35780;&#35770;&#21487;&#33021;&#28041;&#21450;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#20998;&#24067;&#65292;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#35780;&#35770;&#65292;&#26356;&#22909;&#22320;&#28085;&#30422;&#20154;&#31867;&#23457;&#31295;&#20154;&#22312;&#33609;&#31295;&#20013;&#30830;&#23450;&#30340;&#21508;&#31181;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;LLMs&#30340;&#21457;&#23637;&#20026;&#21327;&#21161;&#20316;&#32773;&#25913;&#36827;&#20854;&#20316;&#21697;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290; &#26412;&#25991;&#35774;&#24819;&#20102;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#65292;&#21363;&#20316;&#32773;&#21487;&#20197;&#25910;&#21040;LLM&#29983;&#25104;&#30340;&#35780;&#35770;&#65292;&#25581;&#31034;&#24403;&#21069;&#33609;&#31295;&#20013;&#30340;&#24369;&#28857;&#12290; &#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#35780;&#35770;&#30340;&#21021;&#22987;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#29983;&#25104;&#32570;&#20047;&#32454;&#33410;&#30340;&#35780;&#35770;&#65292;&#24182;&#19988;&#19981;&#33021;&#28085;&#30422;&#20154;&#31867;&#23457;&#31295;&#20154;&#20135;&#29983;&#30340;&#21508;&#31181;&#24847;&#35265;&#12290; &#20026;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reviewer2&#30340;&#39640;&#25928;&#20108;&#38454;&#27573;&#35780;&#35770;&#29983;&#25104;&#26694;&#26550;&#12290; &#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#35780;&#35770;&#21487;&#33021;&#28041;&#21450;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#20998;&#24067;&#12290; &#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#23558;&#23548;&#33268;&#26356;&#35814;&#32454;&#30340;&#35780;&#35770;&#65292;&#26356;&#22909;&#22320;&#28085;&#30422;&#20154;&#31867;&#23457;&#31295;&#20154;&#22312;&#33609;&#31295;&#20013;&#30830;&#23450;&#30340;&#21508;&#31181;&#26041;&#38754;&#12290; &#20316;&#20026;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;27,000&#31687;&#35770;&#25991;&#21644;99,000&#31687;&#35780;&#35770;&#30340;&#22823;&#35268;&#27169;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#29992;&#26041;&#38754;&#25552;&#31034;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10886v1 Announce Type: new  Abstract: Recent developments in LLMs offer new opportunities for assisting authors in improving their work. In this paper, we envision a use case where authors can receive LLM-generated reviews that uncover weak points in the current draft. While initial methods for automated review generation already exist, these methods tend to produce reviews that lack detail, and they do not cover the range of opinions that human reviewers produce. To address this shortcoming, we propose an efficient two-stage review generation framework called Reviewer2. Unlike prior work, this approach explicitly models the distribution of possible aspects that the review may address. We show that this leads to more detailed reviews that better cover the range of aspects that human reviewers identify in the draft. As part of the research, we generate a large-scale review dataset of 27k papers and 99k reviews that we annotate with aspect prompts, which we make available as a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25910;&#38598;&#36731;&#37327;&#32423;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;Direct Preference Optimization&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#19978;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.10884</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#20559;&#22909;&#23545;&#40784;&#20462;&#22797;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#19978;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Multi-modal preference alignment remedies regression of visual instruction tuning on language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10884
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25910;&#38598;&#36731;&#37327;&#32423;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;Direct Preference Optimization&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#19978;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#34987;&#26399;&#26395;&#33021;&#22815;&#25903;&#25345;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#20132;&#25442;&#24335;&#22810;&#36718;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20351;&#29992;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65288;VQA&#65289;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;MLLMs&#21487;&#33021;&#20250;&#20986;&#29616;&#36864;&#21270;&#65292;&#22240;&#20026;VQA&#25968;&#25454;&#38598;&#32570;&#20047;&#21407;&#22987;&#25991;&#26412;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21518;&#32773;&#26159;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#65288;6k&#26465;&#35760;&#24405;&#65289;&#30340;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#31572;&#26696;&#30001;Gemini&#20197;&#32454;&#31890;&#24230;&#26041;&#24335;&#27880;&#37322;&#20102;5&#20010;&#36136;&#37327;&#25351;&#26631;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;&#26631;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#12289;&#25298;&#32477;&#25277;&#26679;&#12289;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#21644;SteerLM&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;DPO&#65292;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;6.73&#30340;MT-Bench&#20998;&#25968;&#65292;&#32780;Vicuna&#30340;6.57&#21644;LLaVA&#30340;5.99&#65292;&#23613;&#31649;&#25968;&#25454;&#35268;&#27169;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10884v1 Announce Type: cross  Abstract: In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10882</link><description>&lt;p&gt;
&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#29992;&#20110;&#23433;&#20840;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Optimizer for Safe Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#26681;&#25454;&#25991;&#23383;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#19981;&#23433;&#20840;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#65292;&#22914;&#33394;&#24773;&#12289;&#39578;&#25200;&#21644;&#38750;&#27861;&#27963;&#21160;&#22270;&#20687;&#12290;&#22522;&#20110;&#22270;&#20687;&#26816;&#26597;&#22120;&#12289;&#27169;&#22411;&#24494;&#35843;&#21644;&#23884;&#20837;&#24335;&#38459;&#27490;&#30340;&#29616;&#26377;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840; T2I &#29983;&#25104;&#30340;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10882v1 Announce Type: cross  Abstract: Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, \textit{we propose the first universal prompt optimizer for safe T2I generation in black-box scenario}. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating in
&lt;/p&gt;</description></item><item><title>EcoRank&#26159;&#19968;&#20010;&#20004;&#23618;&#31649;&#32447;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26377;&#20851;&#39044;&#31639;&#20998;&#37197;&#21644;LLM API&#30340;&#20915;&#31574;&#26469;&#23454;&#29616;&#25991;&#26412;&#37325;&#26032;&#25490;&#24207;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#39044;&#31639;&#24863;&#30693;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10866</link><description>&lt;p&gt;
EcoRank: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21463;&#38480;&#39044;&#31639;&#25991;&#26412;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10866
&lt;/p&gt;
&lt;p&gt;
EcoRank&#26159;&#19968;&#20010;&#20004;&#23618;&#31649;&#32447;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26377;&#20851;&#39044;&#31639;&#20998;&#37197;&#21644;LLM API&#30340;&#20915;&#31574;&#26469;&#23454;&#29616;&#25991;&#26412;&#37325;&#26032;&#25490;&#24207;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#39044;&#31639;&#24863;&#30693;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#37325;&#26032;&#25490;&#24207;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36807;&#31243;&#21253;&#25324;&#22312;&#25552;&#31034;&#20013;&#20351;&#29992;&#26597;&#35810;&#21644;&#20505;&#36873;&#27573;&#33853;&#65292;&#21033;&#29992;&#28857;&#23545;&#28857;&#65292;&#21015;&#34920;&#24335;&#21644;&#25104;&#23545;&#25552;&#31034;&#31574;&#30053;&#12290;LLMs&#30340;&#36825;&#20123;&#25490;&#24207;&#31574;&#30053;&#30340;&#19968;&#20010;&#38480;&#21046;&#26159;&#23427;&#20204;&#30340;&#25104;&#26412;&#65306;&#30001;&#20110;API&#25910;&#36153;&#22522;&#20110;&#36755;&#20837;&#21644;&#36755;&#20986;&#20196;&#29260;&#30340;&#25968;&#37327;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#20250;&#21464;&#24471;&#26114;&#36149;&#12290;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#22312;&#32473;&#23450;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#37325;&#26032;&#25490;&#24207;&#24615;&#33021;&#65292;&#36890;&#36807;&#23548;&#33322;&#25552;&#31034;&#36873;&#25321;&#65292;LLM API&#21644;&#39044;&#31639;&#20998;&#21106;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20351;&#29992;&#19968;&#32452;LLM API&#36827;&#34892;&#25991;&#26412;&#37325;&#26032;&#25490;&#24207;&#30340;&#21463;&#38480;&#39044;&#31639;&#26041;&#27861;&#12290;&#25105;&#20204;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;EcoRank&#65292;&#23427;&#26159;&#19968;&#20010;&#20004;&#23618;&#31649;&#32447;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#26377;&#20851;&#36328;&#25552;&#31034;&#31574;&#30053;&#21644;LLM API&#30340;&#39044;&#31639;&#20998;&#37197;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;QA&#21644;&#27573;&#37325;&#25490;&#24207;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;EcoRank&#20248;&#20110;&#20854;&#20182;&#20855;&#26377;&#39044;&#31639;&#24847;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10866v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance in text re-ranking. This process includes queries and candidate passages in the prompts, utilizing pointwise, listwise, and pairwise prompting strategies. A limitation of these ranking strategies with LLMs is their cost: the process can become expensive due to API charges, which are based on the number of input and output tokens. We study how to maximize the re-ranking performance given a budget, by navigating the vast search spaces of prompt choices, LLM APIs, and budget splits. We propose a suite of budget-constrained methods to perform text re-ranking using a set of LLM APIs. Our most efficient method, called EcoRank, is a two-layered pipeline that jointly optimizes decisions regarding budget allocation across prompt strategies and LLM APIs. Our experimental results on four popular QA and passage reranking datasets show that EcoRank outperforms other budget-aware 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#65292;&#21457;&#29616;&#20102;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;LLMs&#22312;&#39044;&#27979;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32570;&#20047;&#21608;&#26399;&#24615;&#30340;&#25968;&#25454;&#38598;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#21516;&#26102;&#25351;&#20986;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#21644;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#37322;&#20041;&#26377;&#21161;&#20110;&#25552;&#21319;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10835</link><description>&lt;p&gt;
LLMs&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#29702;&#35299;&#21644;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#65292;&#21457;&#29616;&#20102;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;LLMs&#22312;&#39044;&#27979;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32570;&#20047;&#21608;&#26399;&#24615;&#30340;&#25968;&#25454;&#38598;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#21516;&#26102;&#25351;&#20986;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#21644;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#37322;&#20041;&#26377;&#21161;&#20110;&#25552;&#21319;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#24180;&#26469;&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#36805;&#36895;&#21457;&#23637;&#12290;&#20316;&#20026;&#19968;&#31181;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26368;&#36817;&#20174;LLMs&#20013;&#33719;&#24471;&#20102;&#25512;&#21160;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#65292;LLMs&#30340;&#20559;&#22909;&#23384;&#22312;&#30740;&#31350;&#31354;&#30333;&#12290;&#36890;&#36807;&#23558;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#20102;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35768;&#22810;&#29305;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#39044;&#27979;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32570;&#20047;&#21608;&#26399;&#24615;&#30340;&#25968;&#25454;&#38598;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#35201;&#27714;LLMs&#21578;&#30693;&#25968;&#25454;&#38598;&#30340;&#21608;&#26399;&#26469;&#35299;&#37322;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#36755;&#20837;&#31574;&#30053;&#65292;&#21457;&#29616;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#21644;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#37322;&#20041;&#31215;&#26497;&#24433;&#21709;&#20102;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#27934;&#23519;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10835v1 Announce Type: new  Abstract: Large language models (LLMs) have been applied in many fields with rapid development in recent years. As a classic machine learning task, time series forecasting has recently received a boost from LLMs. However, there is a research gap in the LLMs' preferences in this field. In this paper, by comparing LLMs with traditional models, many properties of LLMs in time series prediction are found. For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity. We explain our findings through designing prompts to require LLMs to tell the period of the datasets. In addition, the input strategy is investigated, and it is found that incorporating external knowledge and adopting natural language paraphrases positively affects the predictive performance of LLMs for time series. Overall, this study contributes to insight into the advantages and limitations of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HProPro&#65292;&#19968;&#20010;&#22522;&#20110;&#31243;&#24207;&#25552;&#31034;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#28151;&#21512;&#24335;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#21644;&#25191;&#34892;&#33539;&#24335;&#20197;&#21450;&#21508;&#31181;&#20989;&#25968;&#26469;&#24212;&#23545;&#28151;&#21512;&#25512;&#29702;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.10812</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#31243;&#24207;&#25552;&#31034;&#30340;&#26041;&#24335;&#25506;&#32034;&#28151;&#21512;&#24335;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Exploring Hybrid Question Answering via Program-based Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10812
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HProPro&#65292;&#19968;&#20010;&#22522;&#20110;&#31243;&#24207;&#25552;&#31034;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#28151;&#21512;&#24335;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#21644;&#25191;&#34892;&#33539;&#24335;&#20197;&#21450;&#21508;&#31181;&#20989;&#25968;&#26469;&#24212;&#23545;&#28151;&#21512;&#25512;&#29702;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#36827;&#34892;&#38382;&#31572;&#38656;&#35201;&#23545;&#21508;&#31181;&#25968;&#25454;&#26469;&#28304;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#26159;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20449;&#24687;&#37327;&#22823;&#19988;&#24322;&#26500;&#25968;&#25454;&#26377;&#26426;&#32806;&#21512;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#28041;&#21450;&#35757;&#32451;&#19987;&#38376;&#30340;&#26816;&#32034;&#22120;&#26469;&#36873;&#25321;&#30456;&#20851;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#36755;&#20837;&#38271;&#24230;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#25968;&#25454;&#30340;&#19981;&#21516;&#24418;&#24335;&#36716;&#25442;&#20026;&#21333;&#19968;&#24418;&#24335;&#65292;&#31616;&#21270;&#20219;&#21153;&#38590;&#24230;&#24182;&#23454;&#29616;&#26356;&#31616;&#21333;&#30340;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HProPro&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#28151;&#21512;&#24335;&#38382;&#31572;&#20219;&#21153;&#30340;&#26032;&#22411;&#22522;&#20110;&#31243;&#24207;&#25552;&#31034;&#30340;&#26694;&#26550;&#12290;HProPro&#36981;&#24490;&#20195;&#30721;&#29983;&#25104;&#21644;&#25191;&#34892;&#33539;&#24335;&#12290;&#27492;&#22806;&#65292;HProPro&#38598;&#25104;&#20102;&#21508;&#31181;&#20989;&#25968;&#20197;&#24212;&#23545;&#28151;&#21512;&#25512;&#29702;&#22330;&#26223;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HProPro &#21253;&#21547;&#20989;&#25968;&#22768;&#26126;&#21644;&#20989;&#25968;&#23454;&#29616;&#65292;&#20197;&#23545;&#26469;&#33258;&#21508;&#31181;&#25968;&#25454;&#26469;&#28304;&#30340;&#28151;&#21512;&#20449;&#24687;&#36827;&#34892;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10812v1 Announce Type: new  Abstract: Question answering over heterogeneous data requires reasoning over diverse sources of data, which is challenging due to the large scale of information and organic coupling of heterogeneous data. Various approaches have been proposed to address these challenges. One approach involves training specialized retrievers to select relevant information, thereby reducing the input length. Another approach is to transform diverse modalities of data into a single modality, simplifying the task difficulty and enabling more straightforward processing. In this paper, we propose HProPro, a novel program-based prompting framework for the hybrid question answering task. HProPro follows the code generation and execution paradigm. In addition, HProPro integrates various functions to tackle the hybrid reasoning scenario. Specifically, HProPro contains function declaration and function implementation to perform hybrid information-seeking over data from vario
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#29289;&#21464;&#37327;&#23545;LLMs&#27169;&#25311;&#19981;&#21516;&#35270;&#35282;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20154;&#29289;&#21464;&#37327;&#22312;&#29616;&#26377;&#20027;&#35266;NLP&#25968;&#25454;&#38598;&#20013;&#35299;&#37322;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294;&#36890;&#36807;&#25552;&#31034;&#26041;&#24335;&#21152;&#20837;&#21487;&#20197;&#30053;&#24494;&#25913;&#21892;&#27169;&#22411;&#39044;&#27979;&#65292;&#23588;&#20854;&#22312;&#23384;&#22312;&#20105;&#35758;&#20294;&#33539;&#22260;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#19978;&#25928;&#26524;&#26368;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10811</link><description>&lt;p&gt;
&#22312;LLM&#27169;&#25311;&#20013;&#37327;&#21270;Persona&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Persona Effect in LLM Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#29289;&#21464;&#37327;&#23545;LLMs&#27169;&#25311;&#19981;&#21516;&#35270;&#35282;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20154;&#29289;&#21464;&#37327;&#22312;&#29616;&#26377;&#20027;&#35266;NLP&#25968;&#25454;&#38598;&#20013;&#35299;&#37322;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294;&#36890;&#36807;&#25552;&#31034;&#26041;&#24335;&#21152;&#20837;&#21487;&#20197;&#30053;&#24494;&#25913;&#21892;&#27169;&#22411;&#39044;&#27979;&#65292;&#23588;&#20854;&#22312;&#23384;&#22312;&#20105;&#35758;&#20294;&#33539;&#22260;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#19978;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#20351;&#29992;&#21644;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20154;&#29289;&#21464;&#37327;&#19982;LLMs&#27169;&#25311;&#19981;&#21516;&#35270;&#35282;&#30340;&#33021;&#21147;&#30340;&#20132;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#29289;&#21464;&#37327;&#21487;&#20197;&#35299;&#37322;&#29616;&#26377;&#20027;&#35266;NLP&#25968;&#25454;&#38598;&#20013;&lt;10\%&#30340;&#27880;&#37322;&#21464;&#24322;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25552;&#31034;&#22312;LLMs&#20013;&#21152;&#20837;&#20182;&#20204;&#33021;&#24102;&#26469;&#36866;&#24230;&#30340;&#25913;&#36827;&#12290;Persona&#25552;&#31034;&#22312;&#27880;&#37322;&#32773;&#20043;&#38388;&#23384;&#22312;&#20105;&#35758;&#20294;&#33539;&#22260;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#19978;&#25928;&#26524;&#26368;&#22909;&#12290;&#23384;&#22312;&#32447;&#24615;&#30456;&#20851;&#24615;&#65306;&#20154;&#26684;&#21464;&#37327;&#23545;&#20154;&#31867;&#27880;&#37322;&#30340;&#24433;&#21709;&#36234;&#22823;&#65292;LLMs&#20351;&#29992;Persona&#25552;&#31034;&#30340;&#39044;&#27979;&#23601;&#36234;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#20154;&#29289;&#21464;&#37327;&#30340;&#25928;&#29992;&#36739;&#20302;&#65288;&#21363;&#35299;&#37322;&#20154;&#31867;&#27880;&#37322;&#30340;&lt;10\%&#65289;&#26102;&#65292;Persona&#25552;&#31034;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#22823;&#22810;&#25968;&#20027;&#35266;NLP&#25968;&#25454;&#38598;&#37117;&#23646;&#20110;&#36825;&#19968;&#31867;&#21035;&#65292;&#23545;&#27169;&#25311;&#22810;&#20803;&#35270;&#35282;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10811v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable promise in simulating human language use and behavior. In this study, we delve into the intersection of persona variables and the capability of LLMs to simulate different perspectives. We find that persona variables can explain &lt;10\% variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating them via prompting in LLMs provides modest improvement. Persona prompting is most effective on data samples where disagreements among annotators are frequent yet confined to a limited range. A linear correlation exists: the more persona variables influence human annotations, the better LLMs predictions are using persona prompting. However, when the utility of persona variables is low (i.e., explaining &lt;10\% of human annotations), persona prompting has little effect. Most subjective NLP datasets fall into this category, casting doubt on simulating diverse perspectives in t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#22312;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#23384;&#20648;&#21644;&#26816;&#32034;&#22270;&#20687;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.10805</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#36328;&#27169;&#24577;&#26816;&#32034;&#65306;&#22312;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#20648;&#22270;&#20687;&#29992;&#20110;&#26816;&#32034;&#21450;&#26356;&#22810;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#22312;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#23384;&#20648;&#21644;&#26816;&#32034;&#22270;&#20687;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#34920;&#26126;&#20854;&#33021;&#22815;&#35760;&#24518;&#25991;&#26723;&#20013;&#30340;&#30693;&#35782;&#24182;&#26377;&#25928;&#22320;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#12290;&#22312;&#27492;&#33021;&#21147;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#33021;&#22815;&#22312;&#20854;&#21442;&#25968;&#20869;&#23384;&#20648;&#21644;&#26816;&#32034;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#29992;&#25143;&#23545;&#35270;&#35273;&#20869;&#23481;&#30340;&#26597;&#35810;&#65292;MLLM&#34987;&#26399;&#26395;&#33021;&#22815;&#20174;&#20854;&#21442;&#25968;&#20013;&#8220;&#22238;&#24518;&#8221;&#30456;&#20851;&#22270;&#20687;&#20316;&#20026;&#21709;&#24212;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#38754;&#20020;&#30528;&#26174;&#33879;&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;MLLM&#20869;&#32622;&#30340;&#35270;&#35273;&#35760;&#24518;&#21644;&#35270;&#35273;&#26816;&#32034;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#24335;&#36328;&#27169;&#24577;&#26816;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20026;&#22270;&#20687;&#20998;&#37197;&#21807;&#19968;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#65292;&#24182;&#28041;&#21450;&#20004;&#20010;&#35757;&#32451;&#27493;&#39588;&#65306;&#23398;&#20064;&#35760;&#24518;&#21644;&#23398;&#20064;&#26816;&#32034;&#12290;&#31532;&#19968;&#27493;&#20391;&#37325;&#20110;&#35757;&#32451;MLLM&#35760;&#24518;&#22270;&#20687;&#19982;&#20854;&#26631;&#35782;&#31526;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10805v1 Announce Type: cross  Abstract: The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to "recall" the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter ste
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798; 1000 &#19975;&#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22788;&#29702;&#26368;&#38271;&#36755;&#20837;&#30340;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10790</link><description>&lt;p&gt;
&#22312;&#19968;&#20010; 1000 &#19975;&#26681;&#33609;&#22427;&#20013;&#23547;&#25214;&#38024;&#65306;&#24490;&#29615;&#35760;&#24518;&#25214;&#21040;&#20102;&#35821;&#35328;&#27169;&#22411;&#19981;&#25797;&#38271;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10790
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798; 1000 &#19975;&#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22788;&#29702;&#26368;&#38271;&#36755;&#20837;&#30340;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335; Transformer &#27169;&#22411;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; BABILong&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22312;&#25552;&#21462;&#21644;&#22788;&#29702;&#24191;&#27867;&#25991;&#26412;&#20013;&#20998;&#24067;&#24335;&#20107;&#23454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324; GPT-4 &#21644; RAG &#30340;&#22522;&#20934;&#65292;&#32467;&#26524;&#26174;&#31034;&#24120;&#35265;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#26368;&#22810; $10^4$ &#20010;&#20803;&#32032;&#30340;&#24207;&#21015;&#12290;&#30456;&#21453;&#65292;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#26368;&#22810; $10^7$ &#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#12290;&#36825;&#19968;&#25104;&#23601;&#26631;&#24535;&#30528;&#36804;&#20170;&#20026;&#27490;&#20219;&#20309;&#24320;&#28304;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22788;&#29702;&#30340;&#26368;&#38271;&#36755;&#20837;&#65292;&#26174;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10790v1 Announce Type: cross  Abstract: This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $10^7$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#20351;&#29992;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.10787</link><description>&lt;p&gt;
EdgeQAT: &#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65292;&#29992;&#20110;&#21152;&#36895;&#36731;&#37327;&#32423;LLMs&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#20351;&#29992;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#37327;&#65292;LLMs&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36890;&#24120;&#37319;&#29992;&#37327;&#21270;&#26041;&#27861;&#29983;&#25104;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#24555;&#36895;&#25512;&#29702;&#30340;&#36731;&#37327;&#32423;LLMs&#12290;&#28982;&#32780;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#22312;&#23558;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#19968;&#36215;&#37327;&#21270;&#33267;8&#20301;&#20197;&#19979;&#26102;&#65292;&#36136;&#37327;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#24037;&#20316;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#37327;&#21270;&#65292;&#32780;&#28608;&#27963;&#26410;&#34987;&#35302;&#21450;&#65292;&#36825;&#19981;&#33021;&#20805;&#20998;&#21457;&#25381;&#37327;&#21270;&#23545;&#36793;&#32536;&#31471;&#25512;&#29702;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#21363;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;QAT&#65292;&#29992;&#20110;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#20197;&#23454;&#29616;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#37327;&#21270;&#24615;&#33021;&#19979;&#38477;&#20027;&#35201;&#28304;&#33258;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10787v1 Announce Type: cross  Abstract: Despite the remarkable strides of Large Language Models (LLMs) in various fields, the wide applications of LLMs on edge devices are limited due to their massive parameters and computations. To address this, quantization is commonly adopted to generate lightweight LLMs with efficient computations and fast inference. However, Post-Training Quantization (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache together to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize model weights, leaving the activations untouched, which do not fully exploit the potential of quantization for inference acceleration on the edge. In this paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the optimization of lightweight LLMs to achieve inference acceleration on Edge devices. We first identify that the performance drop of quantization primarily stems from the information
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#30340;&#32039;&#20945;&#36716;&#25442;&#22270;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#32534;&#30721;&#25152;&#26377;&#36335;&#24452;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10779</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#30340;&#32039;&#20945;&#36716;&#25442;&#22270;&#26694;&#26550;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Condensed Transition Graph Framework for Zero-shot Link Prediction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10779
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#30340;&#32039;&#20945;&#36716;&#25442;&#22270;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#32534;&#30721;&#25152;&#26377;&#36335;&#24452;&#30340;&#20449;&#24687;&#65292;&#24182;&#21487;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#65288;ZSLP&#65289;&#26088;&#22312;&#33258;&#21160;&#35782;&#21035;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;&#26469;&#39044;&#27979;&#32473;&#23450;&#22836;&#23454;&#20307;&#21644;&#20854;&#20851;&#31995;&#26102;&#30340;&#23614;&#23454;&#20307;&#65292;&#28982;&#32780;&#38754;&#20020;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#26377;&#26102;&#32570;&#20047;&#36825;&#20123;&#35814;&#32454;&#20449;&#24687;&#65292;&#24182;&#19988;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#39044;&#27979;&#23614;&#23454;&#20307;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#39044;&#27979;&#22836;&#23454;&#20307;&#21644;&#23614;&#23454;&#20307;&#20043;&#38388;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20854;&#24615;&#33021;&#20173;&#21463;&#38480;&#20110;&#26080;&#27861;&#21033;&#29992;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#25152;&#26377;&#65288;&#25351;&#25968;&#22810;&#65289;&#36335;&#24452;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#20849;&#21516;&#25351;&#31034;&#23427;&#20204;&#30340;&#20851;&#31995;&#31867;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#30340;&#32039;&#20945;&#36716;&#25442;&#22270;&#26694;&#26550;&#65288;CTLP&#65289;&#65292;&#23427;&#20197;&#32447;&#24615;&#26102;&#38388;&#32534;&#30721;&#20102;&#25152;&#26377;&#36335;&#24452;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10779v1 Announce Type: new  Abstract: Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically identifying relations between given entities. Existing methods primarily employ auxiliary information to predict tail entity given head entity and its relation, yet face challenges due to the occasional unavailability of such detailed information and the inherent simplicity of predicting tail entities based on semantic similarities. Even though Large Language Models (LLMs) offer a promising solution to predict unobserved relations between the head and tail entity in a zero-shot manner, their performance is still restricted due to the inability to leverage all the (exponentially many) paths' information between two entities, which are critical in collectively indicating their relation types. To address this, in this work, we introduce a Condensed Transition Graph Framework for Zero-Shot Link Prediction (CTLP), which encodes all the paths' information in linear time
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26089;&#26399;&#34701;&#21512;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#26368;&#20339;&#22320;&#35782;&#21035;ESG&#24433;&#21709;&#31867;&#22411;&#65292;&#20026;&#24403;&#20170;&#37329;&#34701;&#21644;&#20225;&#19994;&#27835;&#29702;&#39046;&#22495;&#20013;&#30340;&#36127;&#36131;&#20219;&#21644;&#21487;&#25345;&#32493;&#20915;&#31574;&#36807;&#31243;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.10772</link><description>&lt;p&gt;
&#36890;&#36807;&#26089;&#26399;&#34701;&#21512;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;ESG&#24433;&#21709;&#31867;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Enhancing ESG Impact Type Identification through Early Fusion and Multilingual Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10772
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26089;&#26399;&#34701;&#21512;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#26368;&#20339;&#22320;&#35782;&#21035;ESG&#24433;&#21709;&#31867;&#22411;&#65292;&#20026;&#24403;&#20170;&#37329;&#34701;&#21644;&#20225;&#19994;&#27835;&#29702;&#39046;&#22495;&#20013;&#30340;&#36127;&#36131;&#20219;&#21644;&#21487;&#25345;&#32493;&#20915;&#31574;&#36807;&#31243;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#20225;&#19994;&#27835;&#29702;(ESG)&#24433;&#21709;&#35780;&#20272;&#39046;&#22495;&#65292;ML-ESG-2&#20849;&#20139;&#20219;&#21153;&#25552;&#20986;&#20102;&#35782;&#21035;ESG&#24433;&#21709;&#31867;&#22411;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#31995;&#32479;&#65292;&#21033;&#29992;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;&#26089;&#26399;&#21644;&#21518;&#26399;&#34701;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#65306;mBERT&#12289;FlauBERT-base&#12289;ALBERT-base-v2&#21644;&#32467;&#21512;&#28508;&#22312;&#35821;&#20041;&#20998;&#26512;&#65288;LSA&#65289;&#21644;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#29305;&#24449;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26089;&#26399;&#34701;&#21512;&#38598;&#25104;&#26041;&#27861;&#65292;&#22312;LSA&#12289;TF-IDF&#12289;mBERT&#12289;FlauBERT-base&#21644;ALBERT-base-v2&#30340;&#25972;&#21512;&#19979;&#65292;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;ESG&#24433;&#21709;&#31867;&#22411;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#20419;&#36827;&#20102;&#24403;&#20170;&#37329;&#34701;&#21644;&#20225;&#19994;&#27835;&#29702;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#36127;&#36131;&#20219;&#21644;&#21487;&#25345;&#32493;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10772v1 Announce Type: new  Abstract: In the evolving landscape of Environmental, Social, and Corporate Governance (ESG) impact assessment, the ML-ESG-2 shared task proposes identifying ESG impact types. To address this challenge, we present a comprehensive system leveraging ensemble learning techniques, capitalizing on early and late fusion approaches. Our approach employs four distinct models: mBERT, FlauBERT-base, ALBERT-base-v2, and a Multi-Layer Perceptron (MLP) incorporating Latent Semantic Analysis (LSA) and Term Frequency-Inverse Document Frequency (TF-IDF) features. Through extensive experimentation, we find that our early fusion ensemble approach, featuring the integration of LSA, TF-IDF, mBERT, FlauBERT-base, and ALBERT-base-v2, delivers the best performance. Our system offers a comprehensive ESG impact type identification solution, contributing to the responsible and sustainable decision-making processes vital in today's financial and corporate governance landsca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#33258;&#21160;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19979;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#24040;&#22823;&#21464;&#21270;&#65292;&#19988;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2402.10770</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#22312;&#38754;&#21521;&#25351;&#20196;&#30340;LLM&#20013;&#26377;&#22810;&#21487;&#38752;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#33258;&#21160;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19979;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#24040;&#22823;&#21464;&#21270;&#65292;&#19988;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#25991;&#26412;&#37325;&#21472;&#21644;LLM&#21028;&#26029;&#30340;&#33258;&#21160;&#26041;&#27861;&#20316;&#20026;&#20154;&#24037;&#35780;&#20272;&#30340;&#25104;&#26412;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#21644;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20219;&#21153;&#31867;&#22411;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#21160;&#26041;&#27861;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#26174;&#33879;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;ROUGE-L&#24230;&#37327;&#22312;&#30701;&#31572;&#26696;&#33521;&#35821;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21028;&#26029;&#24378;&#30456;&#20851;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#19981;&#21487;&#38752;&#12290;&#20351;&#29992;GPT-4&#20316;&#20026;&#35780;&#20272;&#21592;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#22312;&#35201;&#27714;&#35780;&#20272;&#26102;&#21253;&#21547;&#21442;&#32771;&#31572;&#26696;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#35780;&#20272;&#36807;&#20110;&#20005;&#26684;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21487;&#20197;&#36817;&#20284;&#20154;&#31867;&#21028;&#26029;&#65292;&#20294;&#20854;&#20934;&#30830;&#24615;&#21487;&#33021;&#22240;&#20219;&#21153;&#31867;&#22411;&#21644;&#35780;&#20272;&#35774;&#32622;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10770v1 Announce Type: cross  Abstract: Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we study the reliability of such methods across a broad range of tasks and in a cross-lingual setting. In contrast to previous findings, we observe considerable variability in correlations between automatic methods and human evaluators when scores are differentiated by task type. Specifically, the widely-used ROUGE-L metric strongly correlates with human judgments for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of GPT-4 as an evaluator depends on including reference answers when prompting for assessments, which can lead to overly strict evaluations in free-form generation tasks. In summary, we find that, while automatic evaluation methods can approximate human judgements und
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33976;&#39311;&#26041;&#27861;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGR&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#36827;&#25490;&#21517;&#27169;&#22411;&#21644;&#33976;&#39311;RankNet&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10769</link><description>&lt;p&gt;
&#33976;&#39311;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Distillation Enhanced Generative Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10769
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33976;&#39311;&#26041;&#27861;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGR&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20808;&#36827;&#25490;&#21517;&#27169;&#22411;&#21644;&#33976;&#39311;RankNet&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#26159;&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#19968;&#31181;&#26032;&#20852;&#33539;&#24335;&#65292;&#36890;&#36807;&#29983;&#25104;&#30456;&#20851;&#27573;&#33853;&#30340;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#20316;&#20026;&#26816;&#32034;&#30446;&#26631;&#12290;&#35813;&#33539;&#24335;&#21033;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#31232;&#30095;&#25110;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#36890;&#36807;&#33976;&#39311;&#36827;&#19968;&#27493;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#21487;&#34892;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DGR&#30340;&#21487;&#34892;&#26694;&#26550;&#12290;DGR&#21033;&#29992;&#35832;&#22914;&#36328;&#32534;&#30721;&#22120;&#31561;&#20808;&#36827;&#25490;&#21517;&#27169;&#22411;&#65292;&#22312;&#25945;&#24072;&#35282;&#33394;&#20013;&#25552;&#20379;&#27573;&#33853;&#25490;&#21517;&#21015;&#34920;&#65292;&#25429;&#33719;&#27573;&#33853;&#30340;&#19981;&#21516;&#30456;&#20851;&#31243;&#24230;&#65292;&#32780;&#19981;&#26159;&#20108;&#20803;&#30828;&#26631;&#31614;&#65307;&#38543;&#21518;&#65292;DGR&#37319;&#29992;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#30340;&#33976;&#39311;RankNet&#25439;&#22833;&#26469;&#20248;&#21270;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#32771;&#34385;&#25945;&#24072;&#27169;&#22411;&#25552;&#20379;&#30340;&#27573;&#33853;&#25490;&#21517;&#39034;&#24207;&#20316;&#20026;&#26631;&#31614;&#12290;&#35813;&#26694;&#26550;&#20165;&#38656;&#35201;&#39069;&#22806;&#30340;&#33976;&#39311;&#27493;&#39588;&#26469;&#22686;&#24378;&#24403;&#21069;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#31995;&#32479;&#65292;&#24182;&#19981;&#22686;&#21152;&#20219;&#20309;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10769v1 Announce Type: cross  Abstract: Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generative language models, distinct from traditional sparse or dense retrieval methods. In this work, we identify a viable direction to further enhance generative retrieval via distillation and propose a feasible framework, named DGR. DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels; subsequently, DGR employs a specially designed distilled RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels. This framework only requires an additional distillation step to enhance current generative retrieval systems and does not add any burden
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21746;&#23398;&#21551;&#21457;&#35774;&#35745;&#30340;&#26694;&#26550;IBE-Eval&#65292;&#29992;&#20110;&#25512;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#35780;&#20272;&#65292;&#22312;&#22240;&#26524;&#38382;&#31572;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#39640;&#36798;77%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.10767</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#35299;&#37322;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference to the Best Explanation in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10767
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21746;&#23398;&#21551;&#21457;&#35774;&#35745;&#30340;&#26694;&#26550;IBE-Eval&#65292;&#29992;&#20110;&#25512;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#35780;&#20272;&#65292;&#22312;&#22240;&#26524;&#38382;&#31572;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#39640;&#36798;77%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#22522;&#26412;&#35299;&#37322;&#36807;&#31243;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;IBE-Eval&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#21746;&#23398;&#20851;&#20110;&#26368;&#20339;&#35299;&#37322;&#25512;&#26029;&#65288;IBE&#65289;&#30340;&#21551;&#21457;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25512;&#36827;&#23545;LLMs&#35299;&#37322;&#30340;&#35299;&#37322;&#21644;&#35780;&#20272;&#12290;IBE-Eval&#36890;&#36807;&#32467;&#21512;&#21253;&#25324;&#19968;&#33268;&#24615;&#12289;&#31616;&#27905;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#22312;&#20869;&#30340;&#26174;&#24335;&#36923;&#36753;&#21644;&#35821;&#35328;&#29305;&#24449;&#26469;&#20272;&#35745;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;&#22240;&#26524;&#38382;&#31572;&#65288;CQA&#65289;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20854;&#20013;IBE-Eval&#34987;&#35201;&#27714;&#22312;&#22810;&#20010;&#30001;LLMs&#65288;&#21363;GPT 3.5&#21644;Llama 2&#65289;&#29983;&#25104;&#30340;&#31454;&#20105;&#24615;&#22240;&#26524;&#35299;&#37322;&#20013;&#36873;&#25321;&#26368;&#21512;&#29702;&#30340;&#22240;&#26524;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;IBE-Eval&#21487;&#20197;&#25104;&#21151;&#22320;&#20197;&#39640;&#36798;77\%&#30340;&#20934;&#30830;&#29575;&#65288;&#27604;&#38543;&#26426;&#39640;&#32422;27%&#65289;&#35782;&#21035;&#26368;&#20339;&#35299;&#37322;&#65292;&#20248;&#20110;GPT 3.5&#20316;&#20026;&#21028;&#23450;&#22522;&#32447;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10767v1 Announce Type: cross  Abstract: While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77\% accuracy ($\approx 27\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\appr
&lt;/p&gt;</description></item><item><title>ToolSword&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#32454;&#33268;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#25345;&#20037;&#23384;&#22312;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10753</link><description>&lt;p&gt;
ToolSword&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#36328;&#19977;&#20010;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10753
&lt;/p&gt;
&lt;p&gt;
ToolSword&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#32454;&#33268;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#25345;&#20037;&#23384;&#22312;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10753v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25277;&#35937;&#65306;&#24037;&#20855;&#23398;&#20064;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#30784;&#26041;&#27861;&#12290;&#23613;&#31649;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#21033;&#29992;&#24037;&#20855;&#26469;&#22686;&#24378;LLMs&#65292;&#20294;&#23427;&#32463;&#24120;&#24573;&#35270;&#19982;&#20854;&#24212;&#29992;&#30456;&#20851;&#30340;&#26032;&#20852;&#23433;&#20840;&#32771;&#34385;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$ToolSword$&#65292;&#36825;&#26159;&#19968;&#20010;&#33268;&#21147;&#20110;&#32454;&#33268;&#35843;&#26597;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ToolSword&#21246;&#30011;&#20102;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#20845;&#20010;&#23433;&#20840;&#22330;&#26223;&#65292;&#21253;&#25324;&#36755;&#20837;&#38454;&#27573;&#30340;$&#24694;&#24847;$ $&#26597;&#35810;$&#21644;$&#36234;&#29425;$ $&#25915;&#20987;$&#65292;&#25191;&#34892;&#38454;&#27573;&#30340;$&#22122;&#22768;$ $&#35823;&#23548;$&#21644;$&#39118;&#38505;$ $&#32447;&#32034;$&#65292;&#20197;&#21450;&#36755;&#20986;&#38454;&#27573;&#30340;$&#26377;&#23475;$ $&#21453;&#39304;$&#21644;$&#38169;&#35823;$ $&#20914;&#31361;$&#12290;&#23545;11&#20010;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#23384;&#22312;&#25345;&#20037;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#22914;&#22788;&#29702;&#26377;&#23475;&#26597;&#35810;&#12289;&#20351;&#29992;&#39118;&#38505;&#24037;&#20855;&#21644;&#25552;&#20379;&#26377;&#23475;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10753v1 Announce Type: cross  Abstract: Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present $ToolSword$, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and $harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedb
&lt;/p&gt;</description></item><item><title>GenRES&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32500;&#24230;&#35780;&#20272;&#29983;&#25104;&#24335;&#20851;&#31995;&#25277;&#21462;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#20351;&#29992;&#20256;&#32479;&#25351;&#26631;&#35780;&#20272;GRE&#26041;&#27861;&#26102;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.10744</link><description>&lt;p&gt;
GenRES&#65306;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#29983;&#25104;&#24335;&#20851;&#31995;&#25277;&#21462;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10744
&lt;/p&gt;
&lt;p&gt;
GenRES&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32500;&#24230;&#35780;&#20272;&#29983;&#25104;&#24335;&#20851;&#31995;&#25277;&#21462;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#20351;&#29992;&#20256;&#32479;&#25351;&#26631;&#35780;&#20272;GRE&#26041;&#27861;&#26102;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#39046;&#22495;&#27491;&#26397;&#30528;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#30340;&#29983;&#25104;&#24335;&#20851;&#31995;&#25277;&#21462;&#65288;GRE&#65289;&#26041;&#21521;&#21457;&#29983;&#26174;&#30528;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#25351;&#26631;&#22914;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#22312;&#35780;&#20272;GRE&#26041;&#27861;&#26102;&#23384;&#22312;&#19981;&#36275;&#12290;&#36825;&#31181;&#19981;&#36275;&#30340;&#21407;&#22240;&#22312;&#20110;&#36825;&#20123;&#25351;&#26631;&#20381;&#36182;&#20110;&#19982;&#20154;&#24037;&#27880;&#37322;&#30340;&#21442;&#32771;&#20851;&#31995;&#30340;&#31934;&#30830;&#21305;&#37197;&#65292;&#32780;GRE&#26041;&#27861;&#36890;&#24120;&#20250;&#20135;&#29983;&#19982;&#21442;&#32771;&#19981;&#21516;&#30340;&#22810;&#26679;&#19988;&#35821;&#20041;&#20934;&#30830;&#30340;&#20851;&#31995;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GenRES&#65292;&#20197;&#22810;&#32500;&#24230;&#35780;&#20272;GRE&#32467;&#26524;&#30340;&#20027;&#39064;&#30456;&#20284;&#24615;&#12289;&#29420;&#29305;&#24615;&#12289;&#31890;&#24230;&#12289;&#30495;&#23454;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;GenRES&#65292;&#25105;&#20204;&#23454;&#35777;&#21457;&#29616;&#65306;&#65288;1&#65289;&#31934;&#30830;&#29575;/&#21484;&#22238;&#29575;&#19981;&#33021;&#20805;&#20998;&#35777;&#26126;GRE&#26041;&#27861;&#30340;&#24615;&#33021;&#65307;&#65288;2&#65289;&#20154;&#24037;&#27880;&#37322;&#30340;&#21442;&#32771;&#20851;&#31995;&#21487;&#33021;&#23384;&#22312;&#19981;&#23436;&#25972;&#24773;&#20917;&#65307;&#65288;3&#65289;&#20197;&#22266;&#23450;&#19968;&#32452;&#20851;&#31995;&#25110;&#23454;&#20307;&#25552;&#31034;LLM
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10744v1 Announce Type: cross  Abstract: The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs). However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26465;&#20214;&#38543;&#26426;&#22330;&#26500;&#24314;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26694;&#26550;&#19979;&#30340;&#12298;&#26131;&#27700;&#23398;&#27966;&#12299;&#21477;&#27861;&#20998;&#26512;&#22270;&#65292;&#23454;&#29616;&#20102;&#20256;&#32479;&#20013;&#21307;&#33647;&#25991;&#26412;&#30340;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.10743</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#26500;&#24314;&#12298;&#26131;&#27700;&#23398;&#27966;&#12299;&#21477;&#27861;&#20998;&#26512;&#22270;
&lt;/p&gt;
&lt;p&gt;
Construction of a Syntactic Analysis Map for Yi Shui School through Text Mining and Natural Language Processing Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10743
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#38543;&#26426;&#22330;&#26500;&#24314;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26694;&#26550;&#19979;&#30340;&#12298;&#26131;&#27700;&#23398;&#27966;&#12299;&#21477;&#27861;&#20998;&#26512;&#22270;&#65292;&#23454;&#29616;&#20102;&#20256;&#32479;&#20013;&#21307;&#33647;&#25991;&#26412;&#30340;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#12289;&#38382;&#31572;&#31995;&#32479;&#35774;&#35745;&#21644;&#35821;&#20041;&#20998;&#26512;&#12290;&#20256;&#32479;&#20013;&#21307;&#23398;&#12298;&#26131;&#27700;&#23398;&#27966;&#12299;&#30340;&#22823;&#37096;&#20998;&#20449;&#24687;&#20197;&#38750;&#32467;&#26500;&#21270;&#30340;&#21476;&#20856;&#27721;&#35821;&#25991;&#26412;&#24418;&#24335;&#23384;&#20648;&#12290;&#20013;&#21307;&#23398;&#25991;&#26412;&#30340;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#22312;&#25366;&#25496;&#21644;&#30740;&#31350;&#20013;&#21307;&#23398;&#23398;&#26415;&#27966;&#21035;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26694;&#26550;&#19979;&#26500;&#24314;&#20102;&#22522;&#20110;&#26465;&#20214;&#38543;&#26426;&#22330;&#30340;&#20998;&#35789;&#21644;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#65292;&#20197;&#35782;&#21035;&#21644;&#25552;&#21462;&#20256;&#32479;&#20013;&#21307;&#33647;&#25991;&#26412;&#30340;&#23454;&#20307;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;TF-IDF&#20449;&#24687;&#26816;&#32034;&#21644;&#25968;&#25454;&#25366;&#25496;&#30340;&#24120;&#35265;&#21152;&#26435;&#25216;&#26415;&#25552;&#21462;&#19981;&#21516;&#30340;&#37325;&#35201;&#20851;&#38190;&#23454;&#20307;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10743v1 Announce Type: new  Abstract: Entity and relationship extraction is a crucial component in natural language processing tasks such as knowledge graph construction, question answering system design, and semantic analysis. Most of the information of the Yishui school of traditional Chinese Medicine (TCM) is stored in the form of unstructured classical Chinese text. The key information extraction of TCM texts plays an important role in mining and studying the academic schools of TCM. In order to solve these problems efficiently using artificial intelligence methods, this study constructs a word segmentation and entity relationship extraction model based on conditional random fields under the framework of natural language processing technology to identify and extract the entity relationship of traditional Chinese medicine texts, and uses the common weighting technology of TF-IDF information retrieval and data mining to extract important key entity information in different
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#35838;&#31243;&#23398;&#20064;&#65288;ICCL&#65289;&#26041;&#27861;&#65292;&#36880;&#28176;&#22686;&#21152;&#25552;&#31034;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;ICCL&#23545;&#24320;&#28304;LLMs&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.10738</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#23398;&#20064;&#65306;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10738
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#35838;&#31243;&#23398;&#20064;&#65288;ICCL&#65289;&#26041;&#27861;&#65292;&#36880;&#28176;&#22686;&#21152;&#25552;&#31034;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;ICCL&#23545;&#24320;&#28304;LLMs&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#25490;&#24207;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#37325;&#35201;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#24403;&#21069;&#30340;&#25490;&#24207;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#30693;&#35782;&#21644;&#30456;&#20284;&#24615;&#35745;&#31639;&#12290;&#25105;&#20204;&#20513;&#23548;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#35838;&#31243;&#23398;&#20064;&#65288;ICCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;ICL&#28436;&#31034;&#25490;&#24207;&#26041;&#27861;&#65292;&#20854;&#26263;&#31034;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36880;&#28176;&#22686;&#21152;&#25552;&#31034;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#23454;&#39564;&#65292;&#35752;&#35770;ICCL&#30340;&#26377;&#25928;&#24615;&#65292;LLM&#30340;ICCL&#33021;&#21147;&#24418;&#25104;&#26426;&#21046;&#20197;&#21450;&#25490;&#24207;&#20027;&#39064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ICCL&#22312;&#25351;&#23548;&#35843;&#25972;&#38454;&#27573;&#24320;&#21457;&#65292;&#23545;&#20110;&#24320;&#28304;LLMs&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;LLMs&#22312;&#36776;&#21035;&#28436;&#31034;&#38590;&#24230;&#32423;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20154;&#31867;&#26356;&#24369;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/61peng/cu&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10738v1 Announce Type: new  Abstract: Demonstration ordering, which is an important strategy for in-context learning (ICL), can significantly affects the performance of large language models (LLMs). However, most of the current approaches of ordering require additional knowledge and similarity calculation. We advocate the few-shot in-context curriculum learning (ICCL), a simple but effective demonstration ordering method for ICL, which implies gradually increasing the complexity of prompt demonstrations during the inference process. Then we design three experiments to discuss the effectiveness of ICCL, the formation mechanism of LLM's ICCL capability, and the impact of ordering subjects. Experimental results demonstrate that ICCL, developed during the instruction-tuning stage, is effective for open-source LLMs. Moreover, LLMs exhibit a weaker capacity compared to humans in discerning the difficulty levels of demonstrations. We release our code at https://github.com/61peng/cu
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;ChatGPT&#22312;&#22768;&#26126;&#39564;&#35777;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#22312;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10735</link><description>&lt;p&gt;
&#22312;&#22768;&#26126;&#39564;&#35777;&#30340;&#32972;&#26223;&#19979;&#35780;&#20272;ChatGPT&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10735
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;ChatGPT&#22312;&#22768;&#26126;&#39564;&#35777;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#22312;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26377;&#20851;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#36777;&#35770;&#27491;&#22312;&#26085;&#30410;&#28608;&#28872;&#12290;&#25105;&#20204;&#20174;&#22768;&#26126;/&#35875;&#35328;&#39564;&#35777;&#30340;&#35282;&#24230;&#26469;&#23457;&#35270;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#20219;&#20309;&#22768;&#26126;&#25110;&#20256;&#35328;&#19982;&#35777;&#25454;&#32467;&#21512;&#65292;&#25286;&#20998;&#25104;&#39564;&#35777;&#25152;&#38656;&#30340;&#22522;&#26412;&#25512;&#29702;&#27493;&#39588;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#20004;&#20010;&#27880;&#37322;&#38598;&#21512;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#32500;&#22522;&#30334;&#31185;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#28304;&#33258;Twitter&#19978;&#27969;&#20256;&#30340;&#35875;&#35328;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#23427;&#20204;&#26469;&#35780;&#20272;GPT-3.5-Turbo&#21644;GPT-4&#65288;&#20197;&#19979;&#31616;&#31216;&#20026;ChatGPT&#65289;&#22312;&#25105;&#20204;&#26694;&#26550;&#30340;&#32972;&#26223;&#19979;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#22312;&#24402;&#32435;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#25163;&#21160;&#30340;&#24605;&#32500;&#38142;&#36335;&#65288;Chain of Thought&#65292;CoT&#65289;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#32780;&#38750;&#38646;&#32534;&#30721;&#65288;Zero Shot&#65292;ZS&#65289;&#21644;ZS CoT&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#34920;&#26126;Cha
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10735v1 Announce Type: new  Abstract: The reasoning capabilities of LLMs are currently hotly debated. We examine the issue from the perspective of claim/rumour verification. We propose the first logical reasoning framework designed to break down any claim or rumor paired with evidence into the atomic reasoning steps necessary for verification. Based on our framework, we curate two annotated collections of such claim/evidence pairs: a synthetic dataset from Wikipedia and a real-world set stemming from rumours circulating on Twitter. We use them to evaluate the reasoning capabilities of GPT-3.5-Turbo and GPT-4 (hereinafter referred to as ChatGPT) within the context of our framework, providing a thorough analysis. Our results show that ChatGPT struggles in abductive reasoning, although this can be somewhat mitigated by using manual Chain of Thought (CoT) as opposed to Zero Shot (ZS) and ZS CoT approaches. Our study contributes to the growing body of research suggesting that Cha
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10712</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM&#25512;&#29702;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10712v1 &#36890;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21457;&#23637;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#20998;&#35789;&#22120;&#12289;&#35789;&#27719;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#19968;&#20123;LLMs&#20855;&#26377;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#29983;&#25104;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#26102;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#25928;&#29575;&#20250;&#19979;&#38477;&#12290;&#36825;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#21644;&#25104;&#26412;&#22686;&#21152;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#27169;&#22411;&#35843;&#25972;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#26088;&#22312;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20116;&#31181;&#29983;&#25104;LLMs&#65288;&#21253;&#25324;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65289;&#22312;&#22235;&#31181;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#19988;&#22235;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10712v1 Announce Type: cross  Abstract: The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of various cross-lingual vocabulary adaptation methods on five generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find th
&lt;/p&gt;</description></item><item><title>&#23558;Thinker&#19982;&#28418;&#31227;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#65292;&#37325;&#26032;&#23450;&#20041;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#20197;&#27169;&#25311;&#20154;&#31867;&#32763;&#35793;&#32773;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2402.10699</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#31867;&#20154;&#32763;&#35793;&#31574;&#30053;&#65306;&#23558;&#28418;&#31227;&#25193;&#25955;&#27169;&#22411;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion Model with Large Language Models for Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10699
&lt;/p&gt;
&lt;p&gt;
&#23558;Thinker&#19982;&#28418;&#31227;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#65292;&#37325;&#26032;&#23450;&#20041;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#20197;&#27169;&#25311;&#20154;&#31867;&#32763;&#35793;&#32773;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#22312;&#20869;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#32763;&#35793;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#28436;&#31034;&#29256;&#26412;&#25110;&#39044;&#23450;&#20041;&#30340;&#26222;&#36941;&#30693;&#35782;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#32570;&#20047;&#23545;&#31867;&#20284;&#20154;&#31867;&#32763;&#35793;&#32773;&#30340;&#20915;&#31574;&#21046;&#23450;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#23558;&#8220;Thinker&#8221;&#19982;&#28418;&#31227;&#25193;&#25955;&#27169;&#22411;&#65288;Thinker-DDM&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#20102;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#65292;&#20197;&#27169;&#25311;&#21463;&#38480;&#36164;&#28304;&#24773;&#20917;&#19979;&#31867;&#20154;&#32763;&#35793;&#32773;&#30340;&#21160;&#24577;&#20915;&#31574;&#21046;&#23450;&#12290;&#25105;&#20204;&#22312;&#39640;&#36164;&#28304;&#12289;&#20302;&#36164;&#28304;&#21644;&#24120;&#35782;&#32763;&#35793;&#35774;&#32622;&#19979;&#65292;&#20351;&#29992;WMT22&#21644;CommonMT&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#22312;&#21069;&#20004;&#31181;&#22330;&#26223;&#20013;&#65292;Thinker-DDM&#30340;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#23545;&#24120;&#35782;&#32763;&#35793;&#36827;&#34892;&#20102;&#39069;&#22806;&#30340;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#20197;&#35828;&#26126;&#20854;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10699v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated promising potential in various downstream tasks, including machine translation. However, prior work on LLM-based machine translation has mainly focused on better utilizing training data, demonstrations, or pre-defined and universal knowledge to improve performance, with a lack of consideration of decision-making like human translators. In this paper, we incorporate Thinker with the Drift-Diffusion Model (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion process to emulate human translators' dynamic decision-making under constrained resources. We conduct extensive experiments under the high-resource, low-resource, and commonsense translation settings using the WMT22 and CommonMT datasets, in which Thinker-DDM outperforms baselines in the first two scenarios. We also perform additional analysis and evaluation on commonsense translation to illustrate the high effectivenes
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#20174;&#22270;&#20687;&#29983;&#25104;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#65292;&#32454;&#33268;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#34920;&#29616;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.10693</link><description>&lt;p&gt;
&#25506;&#32034;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#20197;&#35780;&#20272;LLMs&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Precision and Recall to assess the quality and diversity of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10693
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#20174;&#22270;&#20687;&#29983;&#25104;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#65292;&#32454;&#33268;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#34920;&#29616;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Llama-2&#21644;Mistral&#30340;&#26032;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#37325;&#28857;&#26159;&#23558;&#22270;&#20687;&#29983;&#25104;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#23545;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#36827;&#34892;&#32454;&#33268;&#35780;&#20272;&#65292;&#32780;&#26080;&#38656;&#23545;&#40784;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#24320;&#25918;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#26159;&#20256;&#32479;&#22522;&#20934;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#22312;&#27169;&#22411;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#29983;&#25104;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#22522;&#20110;&#20998;&#24067;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#20026;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#25991;&#26412;&#26041;&#38754;&#38754;&#20020;&#30340;&#23454;&#38469;&#33021;&#21147;&#21644;&#25361;&#25112;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10693v1 Announce Type: new  Abstract: This paper introduces a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current LLMs in generating diverse and high-quality text.
&lt;/p&gt;</description></item><item><title>MultiPoT &#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#20248;&#21183;&#21644;&#22810;&#26679;&#24615;&#65292;&#22312;&#34920;&#29616;&#19978;&#26174;&#33879;&#20248;&#20110; Python &#33258;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10691</link><description>&lt;p&gt;
MultiPoT: &#22810;&#35821;&#35328;&#24605;&#32500;&#31243;&#24207;&#21033;&#29992;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
MultiPoT: Multilingual Program of Thoughts Harnesses Multiple Programming Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10691
&lt;/p&gt;
&lt;p&gt;
MultiPoT &#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#20248;&#21183;&#21644;&#22810;&#26679;&#24615;&#65292;&#22312;&#34920;&#29616;&#19978;&#26174;&#33879;&#20248;&#20110; Python &#33258;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10691v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#24605;&#32500;&#31243;&#24207;&#65288;PoT&#65289;&#26159;&#19968;&#31181;&#20197;&#20854;&#21487;&#25191;&#34892;&#20013;&#38388;&#27493;&#39588;&#20026;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20854;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#20013;&#25968;&#20540;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#12290;&#30446;&#21069;&#65292;PoT&#20027;&#35201;&#20351;&#29992;Python&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#21333;&#19968;&#35821;&#35328;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#24573;&#35270;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;PoT&#20013;&#20351;&#29992;&#30340;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#35821;&#35328;&#22312;&#25152;&#26377;&#20219;&#21153;&#21644;&#27169;&#22411;&#19978;&#22987;&#32456;&#25552;&#20379;&#26368;&#20339;&#24615;&#33021;&#12290;&#27599;&#31181;&#35821;&#35328;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20855;&#20307;&#24773;&#26223;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MultiPoT&#30340;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#21508;&#31181;&#35821;&#35328;&#20013;&#33719;&#21462;&#24378;&#22823;&#21644;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MultiPoT &#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;Python &#33258;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#26368;&#20339;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10691v1 Announce Type: new  Abstract: Program of Thoughts (PoT) is an approach characterized by its executable intermediate steps, which ensure the accuracy of the numerical calculations in the reasoning process. Currently, PoT primarily uses Python. However, relying solely on a single language may result in suboptimal solutions and overlook the potential benefits of other programming languages. In this paper, we conduct comprehensive experiments on the programming languages used in PoT and find that no single language consistently delivers optimal performance across all tasks and models. The effectiveness of each language varies depending on the specific scenarios. Inspired by this, we propose a task and model agnostic approach called MultiPoT, which harnesses strength and diversity from various languages. Experimental results reveal that it significantly outperforms Python Self-Consistency. Furthermore, it achieves comparable or superior performance compared to the best mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;MANGO&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#27010;&#24565;&#21644;&#25991;&#21270;&#20004;&#20010;&#20837;&#21475;&#28857;&#35880;&#24910;&#32780;&#36845;&#20195;&#22320;&#25552;&#31034;LLMs&#65292;&#25552;&#28860;&#39640;&#20934;&#30830;&#24230;&#12289;&#39640;&#21484;&#22238;&#29575;&#30340;&#25991;&#21270;&#30693;&#35782;&#26029;&#35328;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#39640;&#20934;&#30830;&#24230;&#26029;&#35328;&#65292;&#33021;&#22815;&#25913;&#21892;&#23545;&#35805;&#31995;&#32479;&#22238;&#24212;&#30340;&#36136;&#37327;&#12289;&#29305;&#24322;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10689</link><description>&lt;p&gt;
&#22810;&#20803;&#25991;&#21270;&#24120;&#35782;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Cultural Commonsense Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;MANGO&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#27010;&#24565;&#21644;&#25991;&#21270;&#20004;&#20010;&#20837;&#21475;&#28857;&#35880;&#24910;&#32780;&#36845;&#20195;&#22320;&#25552;&#31034;LLMs&#65292;&#25552;&#28860;&#39640;&#20934;&#30830;&#24230;&#12289;&#39640;&#21484;&#22238;&#29575;&#30340;&#25991;&#21270;&#30693;&#35782;&#26029;&#35328;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#39640;&#20934;&#30830;&#24230;&#26029;&#35328;&#65292;&#33021;&#22815;&#25913;&#21892;&#23545;&#35805;&#31995;&#32479;&#22238;&#24212;&#30340;&#36136;&#37327;&#12289;&#29305;&#24322;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20173;&#28982;&#38754;&#20020;&#30528;&#36866;&#24403;&#24212;&#23545;&#31038;&#20250;&#21644;&#25991;&#21270;&#24815;&#20363;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MANGO&#65292;&#19968;&#31181;&#29992;&#20110;&#25552;&#28860;&#39640;&#20934;&#30830;&#24230;&#12289;&#39640;&#21484;&#22238;&#29575;&#25991;&#21270;&#30693;&#35782;&#26029;&#35328;&#30340;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#20174;&#27010;&#24565;&#21644;&#25991;&#21270;&#20004;&#20010;&#20837;&#21475;&#28857;&#35880;&#24910;&#32780;&#36845;&#20195;&#22320;&#25552;&#31034;LLMs&#36827;&#34892;&#36825;&#19968;&#30446;&#30340;&#12290;&#36890;&#36807;&#32858;&#31867;&#21644;&#29983;&#25104;&#25688;&#35201;&#23558;&#36755;&#20986;&#32467;&#26524;&#24041;&#22266;&#12290;&#36816;&#34892;MANGO&#26041;&#27861;&#65292;&#20197;GPT-3.5&#20316;&#20026;&#24213;&#23618;LLM&#65292;&#20026;30K&#20010;&#27010;&#24565;&#21644;11K&#20010;&#25991;&#21270;&#25552;&#20379;&#20102;167K&#20010;&#39640;&#20934;&#30830;&#24230;&#26029;&#35328;&#65292;&#22823;&#24133;&#36229;&#36807;&#20808;&#21069;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#22806;&#37096;&#35780;&#20272;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#23545;&#35805;&#31995;&#32479;&#19982;&#25991;&#21270;&#30693;&#35782;&#26029;&#35328;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#28155;&#21152;&#26469;&#33258;MANGO&#30340;&#30693;&#35782;&#21487;&#20197;&#25552;&#21319;&#23545;&#35805;&#22238;&#24212;&#30340;&#25972;&#20307;&#36136;&#37327;&#12289;&#29305;&#24322;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#65292;&#36825;&#26159;&#30001;&#20154;&#31867;&#26631;&#27880;&#32773;&#35780;&#21028;&#30340;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#20379;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10689v1 Announce Type: new  Abstract: Despite recent progress, large language models (LLMs) still face the challenge of appropriately reacting to the intricacies of social and cultural conventions. This paper presents MANGO, a methodology for distilling high-accuracy, high-recall assertions of cultural knowledge. We judiciously and iteratively prompt LLMs for this purpose from two entry points, concepts and cultures. Outputs are consolidated via clustering and generative summarization. Running the MANGO method with GPT-3.5 as underlying LLM yields 167K high-accuracy assertions for 30K concepts and 11K cultures, surpassing prior resources by a large margin. For extrinsic evaluation, we explore augmenting dialogue systems with cultural knowledge assertions. We find that adding knowledge from MANGO improves the overall quality, specificity, and cultural sensitivity of dialogue responses, as judged by human annotators. Data and code are available for download.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#20307;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#25171;&#24320;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#40657;&#21283;&#23376;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#19979;&#32780;&#19978;&#30340;&#26426;&#26800;&#35299;&#37322;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#34920;&#31034;&#24037;&#31243;&#35270;&#35282;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#21644;&#24212;&#29992;LLMs&#30340;&#34892;&#20026;&#21644;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.10688</link><description>&lt;p&gt;
&#25171;&#24320;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#65306;&#25972;&#20307;&#21487;&#35299;&#37322;&#24615;&#30340;&#20004;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10688
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#20307;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#25171;&#24320;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#40657;&#21283;&#23376;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#19979;&#32780;&#19978;&#30340;&#26426;&#26800;&#35299;&#37322;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#34920;&#31034;&#24037;&#31243;&#35270;&#35282;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#21644;&#24212;&#29992;LLMs&#30340;&#34892;&#20026;&#21644;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#20154;&#20204;&#23545;&#28508;&#22312;&#20260;&#23475;(&#22914;&#27602;&#24615;&#12289;&#19981;&#20844;&#24179;&#21644;&#24187;&#35273;)&#30340;&#25285;&#24551;&#23041;&#32961;&#21040;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#36890;&#36807;&#27169;&#22411;&#23545;&#40784;&#30830;&#20445;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#26377;&#30410;&#22865;&#21512;&#22240;&#27492;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#23545;LLMs&#30340;&#34892;&#20026;&#21644;&#26426;&#21046;&#26377;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#19968;&#20010;&#28085;&#30422;&#20114;&#34917;&#30340;&#33258;&#19979;&#32780;&#19978;&#21644;&#33258;&#19978;&#32780;&#19979;&#35270;&#35282;&#30340;&#25972;&#20307;&#35299;&#37322;&#26694;&#26550;&#26469;&#25171;&#24320;LLMs&#30340;&#40657;&#21283;&#23376;&#12290;&#33258;&#19979;&#32780;&#19978;&#35270;&#35282;&#30001;&#26426;&#26800;&#35299;&#37322;&#33021;&#21147;&#23454;&#29616;&#65292;&#20391;&#37325;&#20110;&#32452;&#20214;&#21151;&#33021;&#21644;&#35757;&#32451;&#21160;&#24577;&#12290;&#33258;&#19978;&#32780;&#19979;&#35270;&#35282;&#21033;&#29992;&#34920;&#31034;&#24037;&#31243;&#36890;&#36807;&#38544;&#34255;&#34920;&#31034;&#20998;&#26512;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#21608;&#22260;&#20851;&#20110;&#26426;&#26800;&#35299;&#37322;&#33021;&#21147;&#21644;&#34920;&#31034;&#24037;&#31243;&#30340;&#24773;&#20917;&#65292;&#24635;&#32467;&#20102;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#38480;&#21046;&#21644;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#23558;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#36798;&#21040;&#30340;&#26410;&#26469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10688v1 Announce Type: new  Abstract: As large language models (LLMs) grow more powerful, concerns around potential harms like toxicity, unfairness, and hallucination threaten user trust. Ensuring beneficial alignment of LLMs with human values through model alignment is thus critical yet challenging, requiring a deeper understanding of LLM behaviors and mechanisms. We propose opening the black box of LLMs through a framework of holistic interpretability encompassing complementary bottom-up and top-down perspectives. The bottom-up view, enabled by mechanistic interpretability, focuses on component functionalities and training dynamics. The top-down view utilizes representation engineering to analyze behaviors through hidden representations. In this paper, we review the landscape around mechanistic interpretability and representation engineering, summarizing approaches, discussing limitations and applications, and outlining future challenges in using these techniques to achiev
&lt;/p&gt;</description></item><item><title>LongHeads &#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37322;&#25918;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10685</link><description>&lt;p&gt;
LongHeads: &#22810;&#22836;&#27880;&#24847;&#21147;&#20854;&#23454;&#26159;&#19968;&#20010;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
LongHeads: Multi-Head Attention is Secretly a Long Context Processor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10685
&lt;/p&gt;
&lt;p&gt;
LongHeads &#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37322;&#25918;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#20294;&#30001;&#20110;&#26377;&#38480;&#38271;&#24230;&#27867;&#21270;&#21644;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#35745;&#31639;&#38656;&#27714;&#65292;&#24448;&#24448;&#38590;&#20197;&#26377;&#25928;&#39640;&#25928;&#22320;&#22788;&#29702;&#36739;&#38271;&#30340;&#36755;&#20837;&#12290; &#35768;&#22810;&#20154;&#35797;&#22270;&#36890;&#36807;&#38480;&#21046;&#22312;&#39044;&#35757;&#32451;&#38271;&#24230;&#20869;&#30340;&#27880;&#24847;&#21147;&#31383;&#21475;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#38382;&#39064;&#65292;&#22914;&#24573;&#30053;&#20013;&#38388;&#19978;&#19979;&#25991;&#21644;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LongHeads&#65292;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37322;&#25918;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;LLM&#30340;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#12290; &#25105;&#20204;&#20801;&#35768;&#27599;&#20010;&#22836;&#37096;&#36873;&#25321;&#24182;&#20851;&#27880;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#22359;&#65292;&#20197;&#22788;&#29702;&#20998;&#24067;&#38271;&#24230;&#65292;&#32780;&#19981;&#26159;&#35753;&#27599;&#20010;&#22836;&#37096;&#37117;&#21442;&#19982;&#20840;&#21477;&#27880;&#24847;&#21147;&#65292;&#36825;&#26679;&#20570;&#30001;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#38382;&#39064;&#32780;&#38590;&#20197;&#27867;&#21270;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10685v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands. Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training. To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential. Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks. To this end, we propose a chunk selection strategy that
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25104;&#21151;&#23436;&#25104;&#24503;&#35821;&#25991;&#26412;&#30340;&#25991;&#26723;&#32423;&#31616;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#22312;&#25913;&#21892;&#25991;&#26412;&#31616;&#21270;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10675</link><description>&lt;p&gt;
&#24503;&#35821;&#25991;&#26412;&#31616;&#21270;&#65306;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
German Text Simplification: Finetuning Large Language Models with Semi-Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25104;&#21151;&#23436;&#25104;&#24503;&#35821;&#25991;&#26412;&#30340;&#25991;&#26723;&#32423;&#31616;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#22312;&#25913;&#21892;&#25991;&#26412;&#31616;&#21270;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#21512;&#25104;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#23436;&#25104;&#20102;&#24503;&#35821;&#25991;&#26412;&#30340;&#25991;&#26723;&#32423;&#31616;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#22312;&#32447;&#25991;&#26412;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#35299;&#20915;&#35821;&#35328;&#31616;&#21270;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#25361;&#25112;&#65292;&#25105;&#20204;&#29228;&#21462;&#20102;&#32463;&#36807;&#19987;&#19994;&#31616;&#21270;&#30340;&#24503;&#35821;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;GPT-4&#21512;&#25104;&#20102;&#19968;&#20010;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#24494;&#35843;&#20102;&#25317;&#26377;&#22810;&#36798;130&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#37319;&#29992;&#20102;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#30446;&#21069;&#20351;&#29992;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#35780;&#20272;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#12290;&#33258;&#21160;&#21644;&#25163;&#21160;&#35780;&#20272;&#22343;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#31616;&#21270;&#30495;&#23454;&#19990;&#30028;&#22312;&#32447;&#25991;&#26412;&#65292;&#34920;&#26126;&#20102;&#21512;&#25104;&#25968;&#25454;&#22312;&#25913;&#21892;&#25991;&#26412;&#31616;&#21270;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10675v1 Announce Type: new  Abstract: This study pioneers the use of synthetically generated data for training generative models in document-level text simplification of German texts. We demonstrate the effectiveness of our approach with real-world online texts. Addressing the challenge of data scarcity in language simplification, we crawled professionally simplified German texts and synthesized a corpus using GPT-4. We finetune Large Language Models with up to 13 billion parameters on this data and evaluate their performance. This paper employs various methodologies for evaluation and demonstrates the limitations of currently used rule-based metrics. Both automatic and manual evaluations reveal that our models can significantly simplify real-world online texts, indicating the potential of synthetic data in improving text simplification.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24037;&#20316;&#27969;&#33539;&#24335;&#26041;&#27861;&#26469;&#25913;&#21892;LLMs&#22312;&#25991;&#26412;&#21040;SQL&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;&#20998;&#35299;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21644;&#38382;&#39064;&#35299;&#20915;&#33539;&#22260;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#30340;&#19978;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.10671</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#26469;&#22686;&#24378;&#27880;&#24847;&#21147;&#65306;&#36890;&#36807;&#24037;&#20316;&#27969;&#33539;&#24335;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#21040;SQL&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10671
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24037;&#20316;&#27969;&#33539;&#24335;&#26041;&#27861;&#26469;&#25913;&#21892;LLMs&#22312;&#25991;&#26412;&#21040;SQL&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;&#20998;&#35299;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21644;&#38382;&#39064;&#35299;&#20915;&#33539;&#22260;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#30340;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#32780;&#24191;&#27867;&#30340;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#21333;&#27493;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#26041;&#27861;&#22312;&#22797;&#26434;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#21040;SQL&#65289;&#20013;&#38754;&#20020;&#27880;&#24847;&#21147;&#25193;&#25955;&#21644;&#24615;&#33021;&#19981;&#36275;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#25913;&#21892;LLMs&#22312;&#25991;&#26412;&#21040;SQL&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20316;&#27969;&#33539;&#24335;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#22686;&#24378;LLMs&#30340;&#27880;&#24847;&#21147;&#21644;&#38382;&#39064;&#35299;&#20915;&#33539;&#22260;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#29992;&#20110;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#30340;&#20449;&#24687;&#30830;&#23450;&#27169;&#22359;&#21644;&#22522;&#20110;&#38382;&#39064;&#20998;&#31867;&#30340;&#20840;&#26032;&#25552;&#31034;&#32467;&#26500;&#26497;&#22823;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#33258;&#26657;&#27491;&#21644;&#20027;&#21160;&#23398;&#20064;&#27169;&#22359;&#26497;&#22823;&#25193;&#23637;&#20102;LLMs&#30340;&#38382;&#39064;&#35299;&#20915;&#33539;&#22260;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#26041;&#27861;&#30340;&#19978;&#38480;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10671v1 Announce Type: new  Abstract: In-context learning of large-language models (LLMs) has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step chain-of-thought prompting approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of LLMs through decomposition. Specifically, the information determination module for eliminating redundant information and the brand-new prompt structure based on problem classification greatly enhance the model's attention. Additionally, the inclusion of self-correcting and active learning modules greatly expands the problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches. Extensive experiments conducted on three da
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OpenFMNav&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#20102;&#30446;&#26631;&#23548;&#33322;&#39046;&#22495;&#20013;&#20851;&#20110;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10670</link><description>&lt;p&gt;
OpenFMNav: &#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OpenFMNav&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#20102;&#30446;&#26631;&#23548;&#33322;&#39046;&#22495;&#20013;&#20851;&#20110;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#33322;(ObjectNav)&#38656;&#35201;&#19968;&#20010;&#20195;&#29702;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23548;&#33322;&#20197;&#25214;&#21040;&#26597;&#35810;&#23545;&#35937;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#20381;&#36182;&#30417;&#30563;&#23398;&#20064;&#25110;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#20854;&#20013;&#23427;&#20204;&#26159;&#22312;&#20855;&#26377;&#38381;&#38598;&#23545;&#35937;&#30340;&#26377;&#38480;&#23478;&#24237;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#23578;&#26410;&#35299;&#20915;&#65306;&#29702;&#35299;&#35201;&#27714;&#24320;&#25918;&#38598;&#23545;&#35937;&#30340;&#33258;&#30001;&#24418;&#24335;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenFMNav&#65292;&#19968;&#31181;&#22522;&#20110;&#24320;&#25918;&#38598;&#22522;&#30784;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#31526;&#21512;&#29992;&#25143;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25552;&#21462;&#25552;&#35758;&#30340;&#23545;&#35937;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#31215;&#26497;&#21457;&#29616;&#24182;&#26816;&#27979;&#22330;&#26223;&#20013;&#30340;&#20505;&#36873;&#23545;&#35937;&#65292;&#26500;&#24314;&#19968;&#20010;Ve
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10670v1 Announce Type: new  Abstract: Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Ve
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#26469;&#30740;&#31350;LLM&#21644;&#20154;&#31867;&#35009;&#21028;&#30340;&#20559;&#35265;&#65292;&#25581;&#31034;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24378;&#35843;&#35780;&#20272;&#29616;&#26377;LLM&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10669</link><description>&lt;p&gt;
&#20154;&#31867;&#36824;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35009;&#21028;&#65311;&#19968;&#39033;&#20851;&#20110;&#21028;&#20915;&#20559;&#35265;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Humans or LLMs as the Judge? A Study on Judgement Biases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10669
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#26469;&#30740;&#31350;LLM&#21644;&#20154;&#31867;&#35009;&#21028;&#30340;&#20559;&#35265;&#65292;&#25581;&#31034;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24378;&#35843;&#35780;&#20272;&#29616;&#26377;LLM&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#35009;&#21028;&#65288;&#21363;&#20154;&#31867;&#21644;LLM&#20316;&#20026;&#35009;&#21028;&#65289;&#26469;&#35780;&#20272;&#29616;&#26377;LLM&#24615;&#33021;&#30340;&#20570;&#27861;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21516;&#26102;&#21487;&#33021;&#24341;&#20837;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#36136;&#30097;&#35780;&#20272;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;LLM&#21644;&#20154;&#31867;&#35009;&#21028;&#30340;5&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;142&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#28041;&#21450;&#20462;&#35746;&#30340;&#24067;&#21346;&#22982;&#20998;&#31867;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#25104;&#21315;&#19978;&#19975;&#27425;&#30340;&#20154;&#31867;&#21644;LLM&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#22312;&#19981;&#21516;&#31243;&#24230;&#19978;&#37117;&#23481;&#26131;&#21463;&#21040;&#24178;&#25200;&#65292;&#21363;&#20351;&#26368;&#23574;&#31471;&#30340;&#35009;&#21028;&#20063;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#20182;&#20204;&#30340;&#24369;&#28857;&#23545;LLM&#35009;&#21028;&#36827;&#34892;&#25915;&#20987;&#12290;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#25552;&#37266;&#31038;&#32676;&#20851;&#20110;&#20154;&#31867;&#21644;LLM&#20316;&#20026;&#35009;&#21028;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#20197;&#21450;&#21457;&#23637;&#30340;&#32039;&#36843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10669v1 Announce Type: new  Abstract: Adopting human and large language models (LLM) as judges (\textit{a.k.a} human- and LLM-as-a-judge) for evaluating the performance of existing LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLM judges, questioning the reliability of the evaluation results. In this paper, we propose a novel framework for investigating 5 types of biases for LLM and human judges. We curate a dataset with 142 samples referring to the revised Bloom's Taxonomy and conduct thousands of human and LLM evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases. We further exploit their weakness and conduct attacks on LLM judges. We hope that our work can notify the community of the vulnerability of human- and LLM-as-a-judge against perturbations, as well as the urgency of develop
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36339;&#34920;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#20889;&#38382;&#39064;&#21644;&#27874;&#26463;&#25628;&#32034;&#26469;&#20943;&#23569;&#30456;&#20284;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22810;&#36339;&#26816;&#32034;&#20013;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.10666</link><description>&lt;p&gt;
&#24320;&#25918;&#22495;&#25991;&#26412;&#21040;SQL&#30340;&#22810;&#36339;&#34920;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Hop Table Retrieval for Open-Domain Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10666
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36339;&#34920;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#20889;&#38382;&#39064;&#21644;&#27874;&#26463;&#25628;&#32034;&#26469;&#20943;&#23569;&#30456;&#20284;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22810;&#36339;&#26816;&#32034;&#20013;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#25991;&#26412;&#21040;SQL&#26159;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#23427;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#34920;&#65292;&#28982;&#21518;&#29983;&#25104;SQL&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21333;&#36339;&#26816;&#32034;&#26041;&#27861;&#24182;&#26410;&#20851;&#27880;&#25991;&#26412;&#21040;SQL&#25361;&#25112;&#20013;&#30340;&#27169;&#24335;&#38142;&#25509;&#65292;&#36825;&#28041;&#21450;&#21040;&#23558;&#38382;&#39064;&#20013;&#30340;&#23454;&#20307;&#19982;&#34920;&#20013;&#23454;&#20307;&#23545;&#40784;&#65292;&#20027;&#35201;&#20307;&#29616;&#22312;&#20004;&#20010;&#26041;&#38754;&#65306;&#30456;&#20284;&#30340;&#26080;&#20851;&#23454;&#20307;&#21644;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#24102;&#37325;&#20889;&#21644;&#27874;&#26463;&#25628;&#32034;&#30340;&#22810;&#36339;&#34920;&#26816;&#32034;&#65288;Murre&#65289;&#12290;&#20026;&#20102;&#20943;&#23569;&#30456;&#20284;&#30340;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#27599;&#20010;&#36339;&#36291;&#20013;&#26410;&#26816;&#32034;&#21040;&#30340;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#27874;&#26463;&#25628;&#32034;&#32771;&#34385;&#25490;&#21517;&#36739;&#20302;&#30340;&#34920;&#12290;&#20026;&#20102;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;Murre&#22522;&#20110;&#22810;&#20010;&#36339;&#36291;&#20013;&#26816;&#32034;&#21040;&#30340;&#34920;&#37325;&#20889;&#38382;&#39064;&#65292;&#20943;&#23569;&#19982;&#30456;&#20851;&#34920;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;SpiderUnion&#21644;BirdUnion+&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10666v1 Announce Type: new  Abstract: Open-domain text-to-SQL is an important task that retrieves question-relevant tables from massive databases and then generates SQL. However, existing retrieval methods that retrieve in a single hop do not pay attention to the text-to-SQL challenge of schema linking, which is aligning the entities in the question with table entities, reflected in two aspects: similar irrelevant entity and domain mismatch entity. Therefore, we propose our method, the multi-hop table retrieval with rewrite and beam search (Murre). To reduce the effect of the similar irrelevant entity, our method focuses on unretrieved entities at each hop and considers the low-ranked tables by beam search. To alleviate the limitation of domain mismatch entity, Murre rewrites the question based on retrieved tables in multiple hops, decreasing the domain gap with relevant tables. We conduct experiments on SpiderUnion and BirdUnion+, reaching new state-of-the-art results with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#22810;&#27425;&#36845;&#20195;&#21512;&#25104;&#26469;&#25913;&#21892;&#25991;&#26412;&#21040;SQL&#28436;&#31034;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#22810;&#26679;&#24615;&#28436;&#31034;&#27744;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#24615;&#24182;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.10663</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#34701;&#21512;&#26041;&#27861;&#25913;&#21892;&#25991;&#26412;&#21040;SQL&#30340;&#28436;&#31034;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#22810;&#27425;&#36845;&#20195;&#21512;&#25104;&#26469;&#25913;&#21892;&#25991;&#26412;&#21040;SQL&#28436;&#31034;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#22810;&#26679;&#24615;&#28436;&#31034;&#27744;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#24615;&#24182;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#20026;&#25991;&#26412;&#21040;SQL&#30740;&#31350;&#30340;&#20027;&#27969;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35752;&#35770;&#20102;&#22914;&#20309;&#20174;&#20154;&#26631;&#35760;&#30340;&#28436;&#31034;&#27744;&#20013;&#36873;&#25321;&#19982;&#29992;&#25143;&#38382;&#39064;&#30456;&#20851;&#30340;&#28436;&#31034;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26631;&#27880;&#23384;&#22312;&#30528;&#22810;&#26679;&#24615;&#19981;&#36275;&#21644;&#26631;&#27880;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#34913;&#37327;&#21644;&#25913;&#21892;&#25991;&#26412;&#21040;SQL&#28436;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24230;&#37327;&#28436;&#31034;&#22810;&#26679;&#24615;&#30340;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#29616;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#22522;&#20110;&#19978;&#36848;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#22810;&#27425;&#36845;&#20195;&#21512;&#25104;&#26469;&#26500;&#24314;&#39640;&#22810;&#26679;&#24615;&#28436;&#31034;&#27744;&#30340;&#34701;&#21512;&#26041;&#27861;&#65288;Fused&#65289;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#24615;&#24182;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;/&#26080;&#20154;&#31867;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#24179;&#22343;&#25552;&#39640;&#20102;3.2%&#21644;5.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10663v1 Announce Type: new  Abstract: Currently, the in-context learning method based on large language models (LLMs) has become the mainstream of text-to-SQL research. Previous works have discussed how to select demonstrations related to the user question from a human-labeled demonstration pool. However, human labeling suffers from the limitations of insufficient diversity and high labeling overhead. Therefore, in this paper, we discuss how to measure and improve the diversity of the demonstrations for text-to-SQL. We present a metric to measure the diversity of the demonstrations and analyze the insufficient of the existing labeled data by experiments. Based on the above discovery, we propose fusing iteratively for demonstrations (Fused) to build a high-diversity demonstration pool through human-free multiple-iteration synthesis, improving diversity and lowering label cost. Our method achieves an average improvement of 3.2% and 5.0% with and without human labeling on sever
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;D&amp;D&#39046;&#22495;&#20013;&#30340;&#24618;&#29289;&#20256;&#35828;&#26469;&#24494;&#35843;Trankit&#65292;&#23454;&#29616;&#20102;&#21629;&#21517;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#22312;&#22855;&#24187;&#39046;&#22495;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10662</link><description>&lt;p&gt;
&#20026;&#22855;&#24187;&#39046;&#22495;&#24494;&#35843;&#21629;&#21517;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine Tuning Named Entity Extraction Models for the Fantasy Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10662
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;D&amp;D&#39046;&#22495;&#20013;&#30340;&#24618;&#29289;&#20256;&#35828;&#26469;&#24494;&#35843;Trankit&#65292;&#23454;&#29616;&#20102;&#21629;&#21517;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#22312;&#22855;&#24187;&#39046;&#22495;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#39033;&#24207;&#21015;&#20998;&#31867;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#34987;&#35782;&#21035;&#24182;&#20998;&#31867;&#20026;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#12290;&#23427;&#20026;&#22823;&#22810;&#25968;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#12298;&#40857;&#19982;&#22320;&#19979;&#22478;&#12299;&#65288;Dungeons and Dragons&#65292;D&amp;D&#65289;&#26159;&#19968;&#27454;&#24320;&#25918;&#24335;&#26700;&#38754;&#22855;&#24187;&#28216;&#25103;&#65292;&#25317;&#26377;&#33258;&#24049;&#20016;&#23500;&#22810;&#26679;&#30340;&#20256;&#35828;&#12290;D&amp;D&#30340;&#23454;&#20307;&#26159;&#39046;&#22495;&#29305;&#23450;&#30340;&#65292;&#22240;&#27492;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#29616;&#25104;NER&#31995;&#32479;&#20063;&#26080;&#27861;&#35782;&#21035;&#36825;&#20123;&#23454;&#20307;&#65292;&#22240;&#20026;NER&#31995;&#32479;&#26159;&#38024;&#23545;&#39044;&#23450;&#20041;&#31867;&#21035;&#65288;&#22914;&#20154;&#29289;&#65288;PERS&#65289;&#12289;&#22320;&#28857;&#65288;LOC&#65289;&#12289;&#32452;&#32455;&#65288;ORG&#65289;&#21644;&#26434;&#39033;&#65288;MISC&#65289;&#65289;&#30340;&#36890;&#29992;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#20174;&#22855;&#24187;&#25991;&#26412;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#23454;&#20307;&#38656;&#35201;&#34987;&#20998;&#31867;&#20026;&#39046;&#22495;&#29305;&#23450;&#30340;&#23454;&#20307;&#31867;&#21035;&#65292;&#24182;&#19988;&#27169;&#22411;&#38656;&#35201;&#22312;&#39046;&#22495;&#30456;&#20851;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#35813;&#30740;&#31350;&#21033;&#29992;D&amp;D&#39046;&#22495;&#20013;&#21487;&#29992;&#30340;&#24618;&#29289;&#20256;&#35828;&#26469;&#24494;&#35843;Trankit&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32321;&#33635;NER&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10662v1 Announce Type: new  Abstract: Named Entity Recognition (NER) is a sequence classification Natural Language Processing task where entities are identified in the text and classified into predefined categories. It acts as a foundation for most information extraction systems. Dungeons and Dragons (D&amp;D) is an open-ended tabletop fantasy game with its own diverse lore. DnD entities are domain-specific and are thus unrecognizable by even the state-of-the-art off-the-shelf NER systems as the NER systems are trained on general data for pre-defined categories such as: person (PERS), location (LOC), organization (ORG), and miscellaneous (MISC). For meaningful extraction of information from fantasy text, the entities need to be classified into domain-specific entity categories as well as the models be fine-tuned on a domain-relevant corpus. This work uses available lore of monsters in the D&amp;D domain to fine-tune Trankit, which is a prolific NER framework that uses a pre-trained 
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.10659</link><description>&lt;p&gt;
&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#32593;&#32476;&#24418;&#25104;&#19982;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Network Formation and Dynamics Among Multi-LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10659
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#24433;&#21709;&#34892;&#20026;&#12289;&#20559;&#22909;&#21644;&#20851;&#31995;&#65292;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#23545;&#20449;&#24687;&#21644;&#35268;&#33539;&#30340;&#20256;&#25773;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#31038;&#20132;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#20114;&#21160;&#32972;&#26223;&#19979;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#20934;&#32593;&#32476;&#32467;&#26500;&#21644;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#20197;&#30830;&#23450;&#22810;&#20010;LLMs&#30340;&#21160;&#24577;&#26159;&#21542;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#31038;&#20132;&#32593;&#32476;&#21407;&#21017;&#65292;&#21253;&#25324;&#24494;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#29233;&#38468;&#30528;&#12289;&#19977;&#35282;&#38381;&#21512;&#21644;&#21516;&#20284;&#24615;&#65292;&#20197;&#21450;&#23439;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#31038;&#21306;&#32467;&#26500;&#21644;&#23567;&#19990;&#30028;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#21521;LLMs&#25552;&#20379;&#32593;&#32476;&#32467;&#26500;&#24182;&#35810;&#38382;&#23427;&#20204;&#23545;&#32593;&#32476;&#24418;&#25104;&#30340;&#20559;&#22909;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#25152;&#26377;&#36825;&#20123;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10659v1 Announce Type: cross  Abstract: Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furtherm
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#35299;&#31572;&#26696;&#20844;&#24335;&#20197;&#30830;&#20445;&#25903;&#25345;&#31572;&#26696;&#65292;&#20511;&#37492;&#21487;&#38752;&#25512;&#29702;&#36807;&#31243;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10654</link><description>&lt;p&gt;
&#20511;&#37492;&#21487;&#38752;&#25512;&#29702;&#36807;&#31243;&#22686;&#24378;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10654
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#31572;&#26696;&#20844;&#24335;&#20197;&#30830;&#20445;&#25903;&#25345;&#31572;&#26696;&#65292;&#20511;&#37492;&#21487;&#38752;&#25512;&#29702;&#36807;&#31243;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#25512;&#29702;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22788;&#29702;&#25968;&#20540;&#20449;&#24687;&#30340;&#24517;&#35201;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#24494;&#35843;&#23567;&#35268;&#27169;&#27169;&#22411;&#65292;&#20351;&#20854;&#23398;&#20064;&#22312;&#22238;&#31572;&#38382;&#39064;&#30340;&#21516;&#26102;&#29983;&#25104;&#25512;&#29702;&#36807;&#31243;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#36807;&#31243;&#65292;&#36825;&#20123;&#36807;&#31243;&#8220;&#19981;&#21487;&#38752;&#8221;&#65292;&#22240;&#20026;&#36825;&#31181;&#36807;&#31243;&#21487;&#33021;&#21253;&#21547;&#19982;&#31572;&#26696;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Enhancing NumeriCal reasOning with Reliable procEsses (Encore)&#65292;&#36890;&#36807;&#20998;&#35299;&#31572;&#26696;&#20844;&#24335;&#24471;&#20986;&#21487;&#38752;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#30830;&#20445;&#23436;&#20840;&#25903;&#25345;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#21487;&#33021;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#20805;&#20998;&#23398;&#20064;&#25512;&#29702;&#36807;&#31243;&#29983;&#25104;&#65292;&#22240;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#19968;&#20010;&#20844;&#24335;&#21482;&#29983;&#25104;&#19968;&#20010;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;h
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10654v1 Announce Type: new  Abstract: Numerical reasoning is an essential ability for NLP systems to handle numeric information. Recent research indicates that fine-tuning a small-scale model to learn generating reasoning processes alongside answers can significantly enhance performance. However, current methods have the limitation that most methods generate reasoning processes with large language models (LLMs), which are "unreliable" since such processes could contain information unrelated to the answer. To address this limitation, we introduce Enhancing NumeriCal reasOning with Reliable procEsses (Encore), which derives the reliable reasoning process by decomposing the answer formula, ensuring which fully supports the answer. Nevertheless, models could lack enough data to learn the reasoning process generation adequately, since our method generates only one single reasoning process for one formula. To overcome this difficulty, we present a series of pre-training tasks to h
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25351;&#23548;&#35843;&#33410;&#21644;&#21512;&#29702;&#24615;&#35780;&#20272;&#65292;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;AbsInstruct&#26694;&#26550;&#26469;&#22686;&#24378;LLMs&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10646</link><description>&lt;p&gt;
&#20174;&#21512;&#29702;&#24615;&#35780;&#20272;&#20013;&#36890;&#36807;&#35299;&#37322;&#35843;&#33410;&#25552;&#21462;LLMs&#30340;&#25277;&#35937;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25351;&#23548;&#35843;&#33410;&#21644;&#21512;&#29702;&#24615;&#35780;&#20272;&#65292;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;AbsInstruct&#26694;&#26550;&#26469;&#22686;&#24378;LLMs&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#33021;&#21147;&#23545;&#20154;&#31867;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#20063;&#21487;&#20197;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#21463;&#30410;&#12290;&#29616;&#26377;&#24037;&#20316;&#34920;&#26126;LLMs&#22312;&#25277;&#35937;&#33021;&#21147;&#19978;&#23384;&#22312;&#19981;&#36275;&#65292;&#22914;&#20309;&#25913;&#36827;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;AbsInstruct&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#33410;&#26469;&#22686;&#24378;LLMs&#30340;&#25277;&#35937;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#28145;&#20837;&#35299;&#37322;&#26500;&#24314;&#25351;&#23548;&#65292;&#24110;&#21161;LLMs&#25429;&#25417;&#25277;&#35937;&#30340;&#28508;&#22312;&#21407;&#29702;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#29702;&#24615;&#20272;&#35745;&#22120;&#26469;&#36873;&#25321;&#26356;&#31526;&#21512;LLMs&#25277;&#35937;&#30693;&#35782;&#30340;&#25351;&#23548;&#20197;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#25277;&#35937;&#25351;&#23548;&#19982;&#36890;&#29992;&#25351;&#23548;&#32467;&#21512;&#20197;&#26500;&#24314;&#28151;&#21512;&#25968;&#25454;&#38598;&#12290;&#22823;&#37327;&#23454;&#39564;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26174;&#30528;&#22686;&#24378;LLMs&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#24182;&#20445;&#25345;&#20854;&#36890;&#29992;&#30340;&#25351;&#23548;&#36981;&#24490;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10646v1 Announce Type: new  Abstract: Abstraction ability is crucial in human intelligence, which can also benefit various tasks in NLP study. Existing work shows that LLMs are deficient in abstract ability, and how to improve it remains unexplored. In this work, we design the framework AbsInstruct to enhance LLMs' abstraction ability through instruction tuning. The framework builds instructions with in-depth explanations to assist LLMs in capturing the underlying rationale of abstraction. Meanwhile, we introduce a plausibility estimator to select instructions that are more consistent with the abstraction knowledge of LLMs to be aligned. Then, our framework combines abstraction instructions with general-purpose ones to build a hybrid dataset. Extensive experiments and analyses demonstrate that our framework can considerably enhance LLMs' abstraction ability with strong generalization performance while maintaining their general instruction-following abilities.
&lt;/p&gt;</description></item><item><title>&#20998;&#38548;&#31526;&#30340;&#24341;&#20837;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.10645</link><description>&lt;p&gt;
&#20998;&#38548;&#31526;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#25928;&#26524;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Separators Improve Chain-of-Thought Prompting?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10645
&lt;/p&gt;
&lt;p&gt;
&#20998;&#38548;&#31526;&#30340;&#24341;&#20837;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;CoT&#30340;&#22522;&#26412;&#29702;&#24565;&#26159;&#36890;&#36807;&#23558;&#31034;&#20363;&#25918;&#22312;&#36755;&#20837;&#25552;&#31034;&#20013;&#65292;&#35753;LLMs&#36880;&#27493;&#25286;&#35299;&#20182;&#20204;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;CoT&#25552;&#31034;&#30340;&#23494;&#38598;&#32467;&#26500;&#21487;&#33021;&#23548;&#33268;LLMs&#30340;&#35748;&#30693;&#36127;&#33655;&#36807;&#37325;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoT-Sep&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;CoT&#25552;&#31034;&#20013;&#27599;&#20010;&#31034;&#20363;&#30340;&#26411;&#23614;&#31574;&#30053;&#24615;&#22320;&#24212;&#29992;&#20998;&#38548;&#31526;&#12290;&#36825;&#20123;&#20998;&#38548;&#31526;&#26088;&#22312;&#24110;&#21161;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#29702;&#35299;&#20182;&#20204;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#19981;&#20351;&#29992;&#20998;&#38548;&#31526;&#30340;&#26222;&#36890;CoT&#30456;&#27604;&#65292;CoT-Sep&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;GSM-8K&#12289;AQuA&#12289;CSQA&#65289;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#20998;&#38548;&#31526;&#23545;&#22810;&#20010;LLMs&#65288;&#21253;&#25324;GPT-3.5-Turbo&#12289;GPT-4&#21644;LLaMA-27&#65289;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10645v1 Announce Type: cross  Abstract: Chain-of-thought (CoT) prompting is a simple and effective method for improving the reasoning capabilities of Large language models (LLMs). The basic idea of CoT is to let LLMs break down their thought processes step-by-step by putting exemplars in the input prompt. However, the densely structured prompt exemplars of CoT may cause the cognitive overload of LLMs. Inspired by human cognition, we introduce CoT-Sep, a novel method that strategically employs separators at the end of each exemplar in CoT prompting. These separators are designed to help the LLMs understand their thought processes better while reasoning. It turns out that CoT-Sep significantly improves the LLMs' performances on complex reasoning tasks (e.g., GSM-8K, AQuA, CSQA), compared with the vanilla CoT, which does not use separators. We also study the effects of the type and the location of separators tested on multiple LLMs, including GPT-3.5-Turbo, GPT-4, and LLaMA-2 7
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#35760;&#24518;&#26469;&#20445;&#25345;&#20027;&#39064;&#36830;&#36143;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#25552;&#21462;&#24335;&#25688;&#35201;&#20013;&#24378;&#21270;&#36830;&#36143;&#24615;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#20449;&#24687;&#37327;&#21644;&#20943;&#23569;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2402.10643</link><description>&lt;p&gt;
&#8220;&#20445;&#25345;&#32852;&#31995;&#65306;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#35760;&#24518;&#22312;&#25552;&#21462;&#25688;&#35201;&#20013;&#24378;&#21270;&#36830;&#36143;&#24615;&#8221;
&lt;/p&gt;
&lt;p&gt;
`Keep it Together': Enforcing Cohesion in Extractive Summaries by Simulating Human Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#35760;&#24518;&#26469;&#20445;&#25345;&#20027;&#39064;&#36830;&#36143;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#25552;&#21462;&#24335;&#25688;&#35201;&#20013;&#24378;&#21270;&#36830;&#36143;&#24615;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#20449;&#24687;&#37327;&#21644;&#20943;&#23569;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#24335;&#25688;&#35201;&#36890;&#24120;&#20197;&#19968;&#31995;&#21015;&#21477;&#23376;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#23427;&#20204;&#20043;&#38388;&#27809;&#26377;&#39044;&#26399;&#30340;&#36830;&#36143;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#22312;&#25688;&#35201;&#20013;&#24378;&#21270;&#36830;&#36143;&#24615;&#65292;&#21516;&#26102;&#25511;&#21046;&#20449;&#24687;&#37327;&#21644;&#20887;&#20313;&#65292;&#29305;&#21035;&#26159;&#24403;&#36755;&#20837;&#20855;&#26377;&#36739;&#39640;&#20887;&#20313;&#24615;&#26102;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#25511;&#21046;&#20887;&#20313;&#65292;&#24182;&#22312;&#36873;&#25321;&#21477;&#23376;&#26102;&#24179;&#34913;&#20449;&#24687;&#37327;&#21644;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#21477;&#23376;&#36873;&#25321;&#22120;&#27169;&#25311;&#20154;&#31867;&#35760;&#24518;&#20197;&#36319;&#36394;&#20027;&#39064; -- &#34987;&#24314;&#27169;&#20026;&#35789;&#38142; -- &#22312;&#21517;&#35789;&#30701;&#35821;&#20043;&#38388;&#24378;&#21270;&#36830;&#36143;&#32852;&#31995;&#12290;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21487;&#20197;&#25552;&#21462;&#39640;&#24230;&#36830;&#36143;&#30340;&#25688;&#35201;&#65292;&#28982;&#32780;&#35835;&#32773;&#20173;&#20250;&#24863;&#21040;&#36825;&#20123;&#25688;&#35201;&#21644;&#20165;&#32771;&#34385;&#20449;&#24687;&#37327;&#25110;&#20887;&#20313;&#24615;&#30340;&#25688;&#35201;&#19968;&#26679;&#23500;&#26377;&#20449;&#24687;&#12290;&#25552;&#21462;&#30340;&#25688;&#35201;&#22312;&#21477;&#23376;&#20043;&#38388;&#23637;&#31034;&#20102;&#24179;&#28369;&#30340;&#20027;&#39064;&#36716;&#25442;&#65292;&#36825;&#20123;&#36716;&#25442;&#34987;&#35789;&#38142;&#25152;&#26631;&#35782;&#65292;&#36825;&#20123;&#38142;&#36328;&#36234;&#30456;&#37051;&#25110;&#20960;&#20046;&#30456;&#37051;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10643v1 Announce Type: cross  Abstract: Extractive summaries are usually presented as lists of sentences with no expected cohesion between them. In this paper, we aim to enforce cohesion whilst controlling for informativeness and redundancy in summaries, in cases where the input exhibits high redundancy. The pipeline controls for redundancy in long inputs as it is consumed, and balances informativeness and cohesion during sentence selection. Our sentence selector simulates human memory to keep track of topics --modeled as lexical chains--, enforcing cohesive ties between noun phrases. Across a variety of domains, our experiments revealed that it is possible to extract highly cohesive summaries that nevertheless read as informative to humans as summaries extracted by only accounting for informativeness or redundancy. The extracted summaries exhibit smooth topic transitions between sentences as signaled by lexical chains, with chains spanning adjacent or near-adjacent sentence
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#39046;&#22495;&#29305;&#23450;&#36866;&#37197;&#22120;&#28151;&#21512;&#22312;&#39046;&#22495;&#20869;&#35780;&#20272;&#20013;&#30340;&#27867;&#21270;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25506;&#35752;&#20102;&#28151;&#21512;&#36866;&#37197;&#22120;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#20026;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#24615;&#33021;&#20248;&#21270;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#35265;</title><link>https://arxiv.org/abs/2402.10639</link><description>&lt;p&gt;
&#22522;&#20110;&#31526;&#21495;&#26435;&#37325;&#26041;&#21521;&#30340;&#39046;&#22495;&#29305;&#23450;&#36866;&#37197;&#22120;&#28151;&#21512;&#30340;&#27867;&#21270;&#19982;&#20854;&#22312;&#26377;&#25928;&#27169;&#22411;&#21098;&#26525;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#39046;&#22495;&#29305;&#23450;&#36866;&#37197;&#22120;&#28151;&#21512;&#22312;&#39046;&#22495;&#20869;&#35780;&#20272;&#20013;&#30340;&#27867;&#21270;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25506;&#35752;&#20102;&#28151;&#21512;&#36866;&#37197;&#22120;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#20026;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#24615;&#33021;&#20248;&#21270;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#20960;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#23558;&#21333;&#19968;&#19987;&#19994;&#30693;&#35782;&#25972;&#21512;&#21040;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#20013;&#65292;&#36824;&#33021;&#19968;&#27425;&#24615;&#25972;&#21512;&#22810;&#20010;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697;&#22914;AdapterSoup&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#22411;&#26435;&#37325;&#24179;&#22343;&#21270;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#28151;&#21512;&#39046;&#22495;&#29305;&#23450;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#36825;&#31181;&#26032;&#20852;&#30340;&#26435;&#37325;&#31354;&#38388;&#36866;&#37197;&#22120;&#28151;&#21512;&#26426;&#21046;&#22312;&#26410;&#30693;&#30340;&#39046;&#22495;&#20869;&#20363;&#23376;&#19978;&#30340;&#22522;&#26412;&#27867;&#21270;&#24615;&#20173;&#26410;&#34987;&#25506;&#35752;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#38416;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#36866;&#37197;&#22120;&#28151;&#21512;&#22312;&#39046;&#22495;&#20869;&#35780;&#20272;&#20013;&#30340;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#23427;&#20204;&#30340;&#26435;&#37325;&#31526;&#21495;&#26469;&#28145;&#20837;&#30740;&#31350;&#39046;&#22495;&#29305;&#23450;&#36866;&#37197;&#22120;&#28151;&#21512;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#24471;&#20986;&#20102;&#20851;&#38190;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10639v1 Announce Type: new  Abstract: Several parameter-efficient fine-tuning methods based on adapters have been proposed as a streamlined approach to incorporate not only a single specialized knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of them at once. Recent works such as AdapterSoup propose to mix not all but only a selective sub-set of domain-specific adapters during inference via model weight averaging to optimize performance on novel, unseen domains with excellent computational efficiency. However, the essential generalizability of this emerging weight-space adapter mixing mechanism on unseen, in-domain examples remains unexplored. Thus, in this study, we conduct a comprehensive analysis to elucidate the generalizability of domain-specific adapter mixtures in in-domain evaluation. We also provide investigations into the inner workings of the mixture of domain-specific adapters by analyzing their weight signs, yielding critical analysis
&lt;/p&gt;</description></item><item><title>BitDistiller&#26694;&#26550;&#23558;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#19982;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#37327;&#21270;&#21644;&#21098;&#35009;&#25216;&#26415;&#20197;&#21450;&#32622;&#20449;&#24863;&#30693;Kullback-Leibler&#25955;&#24230;&#65288;CAKLD&#65289;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#22312;&#26497;&#20302;&#31934;&#24230;&#19979;&#65288;&#20302;&#20110;4&#20301;&#65289;&#25552;&#21319;LLMs&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10631</link><description>&lt;p&gt;
BitDistiller: &#36890;&#36807;&#33258;&#33976;&#39311;&#37322;&#25918;&#20302;&#20110;4&#20301;LLMs&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10631
&lt;/p&gt;
&lt;p&gt;
BitDistiller&#26694;&#26550;&#23558;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#19982;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#37327;&#21270;&#21644;&#21098;&#35009;&#25216;&#26415;&#20197;&#21450;&#32622;&#20449;&#24863;&#30693;Kullback-Leibler&#25955;&#24230;&#65288;CAKLD&#65289;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#22312;&#26497;&#20302;&#31934;&#24230;&#19979;&#65288;&#20302;&#20110;4&#20301;&#65289;&#25552;&#21319;LLMs&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10631v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#20869;&#23481; &#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21319;&#32423;&#21462;&#24471;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#37096;&#32626;&#25361;&#25112;&#12290;&#26435;&#37325;&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#26222;&#36941;&#25509;&#21463;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BitDistiller&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#19982;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;LLMs&#22312;&#26497;&#20302;&#31934;&#24230;&#65288;&#20302;&#20110;4&#20301;&#65289;&#19979;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BitDistiller&#39318;&#20808;&#37319;&#29992;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38750;&#23545;&#31216;&#37327;&#21270;&#21644;&#21098;&#35009;&#25216;&#26415;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#20445;&#30041;&#37327;&#21270;&#26435;&#37325;&#30340;&#20445;&#30495;&#24230;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32622;&#20449;&#24863;&#30693;Kullback-Leibler&#25955;&#24230;&#65288;CAKLD&#65289;&#30446;&#26631;&#65292;&#20197;&#33258;&#33976;&#39311;&#30340;&#26041;&#24335;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#21331;&#36234;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;BitDistiller&#22312;3&#20301;&#21644;2&#20301;&#24773;&#20917;&#19979;&#26126;&#26174;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10631v1 Announce Type: new  Abstract: The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges. Weight quantization has emerged as a widely embraced solution to reduce memory and computational demands. This paper introduces BitDistiller, a framework that synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD) to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically, BitDistiller first incorporates a tailored asymmetric quantization and clipping technique to maximally preserve the fidelity of quantized weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a self-distillation manner to enable faster convergence and superior model performance. Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit conf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;MORTISE&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#20010;LLM&#27169;&#22359;&#30340;&#21327;&#20316;&#21162;&#21147;&#29983;&#25104;&#39640;&#24230;&#19982;&#35282;&#33394;&#30456;&#20851;&#30340;&#31215;&#26497;&#26597;&#35810;&#65292;&#36827;&#32780;&#25913;&#21892;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10618</link><description>&lt;p&gt;
&#36890;&#36807;&#31215;&#26497;&#26597;&#35810;&#22686;&#24378;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65306;&#35780;&#20272;&#19982;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;MORTISE&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#20010;LLM&#27169;&#22359;&#30340;&#21327;&#20316;&#21162;&#21147;&#29983;&#25104;&#39640;&#24230;&#19982;&#35282;&#33394;&#30456;&#20851;&#30340;&#31215;&#26497;&#26597;&#35810;&#65292;&#36827;&#32780;&#25913;&#21892;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#23558;&#23545;&#35805;&#29983;&#25104;&#25512;&#21521;&#20102;&#26032;&#30340;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65288;RPSs&#65289;&#39046;&#22495;&#12290;&#23613;&#31649;&#29616;&#26377;&#22522;&#20110;LLM&#30340;RPS&#24050;&#32463;&#36890;&#36807;&#26222;&#36890;&#35282;&#33394;&#30456;&#20851;&#22521;&#35757;&#23545;&#35805;&#36827;&#34892;&#20102;&#22686;&#24378;&#65292;&#20294;&#22312;&#22788;&#29702;&#36793;&#30028;&#24773;&#22659;&#20013;&#30340;&#22797;&#26434;&#21644;&#21463;&#22256;&#26597;&#35810;&#26102;&#65292;&#20173;&#28982;&#23384;&#22312;&#30528;&#19982;&#35282;&#33394;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#27169;&#22359;&#21270;&#21327;&#35843;&#30340;&#38519;&#38449;&#35774;&#32622;&#20132;&#20114;&#31995;&#32479;&#65288;MORTISE&#65289;&#65292;&#26469;&#35780;&#20272;&#21644;&#25552;&#39640;&#35282;&#33394;&#25198;&#28436;LLMs&#30340;&#24615;&#33021;&#12290;MORTISE&#21487;&#20197;&#36890;&#36807;&#22810;&#20010;&#22522;&#20110;LLM&#30340;&#27169;&#22359;&#30340;&#21327;&#20316;&#21162;&#21147;&#20135;&#29983;&#39640;&#24230;&#19982;&#35282;&#33394;&#30456;&#20851;&#30340;&#31215;&#26497;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#19968;&#33268;&#30340;&#21709;&#24212;&#29983;&#25104;&#22120;&#21046;&#23450;&#30456;&#24212;&#30340;&#22238;&#22797;&#65292;&#20174;&#32780;&#21019;&#24314;&#23545;&#25239;&#24615;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;190&#31181;&#20013;&#25991;&#21644;&#33521;&#25991;&#35282;&#33394;&#26469;&#26500;&#24314;&#31215;&#26497;&#30340;&#26597;&#35810;&#65292;&#20197;&#35780;&#20272;&#29616;&#26377;&#30340;&#35282;&#33394;&#25198;&#28436;LLMs&#12290;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#35282;&#33394;&#34920;&#31034;&#19978;&#26222;&#36941;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10618v1 Announce Type: new  Abstract: The advent of Large Language Models (LLMs) has propelled dialogue generation into new realms, particularly in the field of role-playing systems (RPSs). While enhanced with ordinary role-relevant training dialogues, existing LLM-based RPSs still struggle to align with roles when handling intricate and trapped queries in boundary scenarios. In this paper, we design the Modular ORchestrated Trap-setting Interaction SystEm (MORTISE) to benchmark and improve the role-playing LLMs' performance. MORTISE can produce highly role-relevant aggressive queries through the collaborative effort of multiple LLM-based modules, and formulate corresponding responses to create an adversarial training dataset via a consistent response generator. We select 190 Chinese and English roles to construct aggressive queries to benchmark existing role-playing LLMs. Through comprehensive evaluation, we find that existing models exhibit a general deficiency in role ali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#65292;&#20351;&#20854;&#29983;&#25104;&#21487;&#25511;&#30340;&#25903;&#25345;&#29992;&#25143;&#23450;&#20041;&#35770;&#28857;&#30340;&#22768;&#26126;&#65292;&#25913;&#36827;&#20102;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;DEBATunE&#27969;&#31243;&#12290;&#36890;&#36807;&#20004;&#20010;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#29983;&#25104;&#26377;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.10614</link><description>&lt;p&gt;
&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#20197;&#29983;&#25104;&#21487;&#25511;&#30340;&#20855;&#26377;&#20105;&#35758;&#24615;&#30340;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#65292;&#20351;&#20854;&#29983;&#25104;&#21487;&#25511;&#30340;&#25903;&#25345;&#29992;&#25143;&#23450;&#20041;&#35770;&#28857;&#30340;&#22768;&#26126;&#65292;&#25913;&#36827;&#20102;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;DEBATunE&#27969;&#31243;&#12290;&#36890;&#36807;&#20004;&#20010;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#29983;&#25104;&#26377;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#20195;&#34920;&#19981;&#21516;&#30340;&#20154;&#32676;&#65292;&#23588;&#20854;&#26159;&#23569;&#25968;&#32676;&#20307;&#65292;&#24182;&#20135;&#29983;&#25903;&#25345;&#20854;&#22810;&#26679;&#21270;&#29978;&#33267;&#26377;&#20105;&#35758;&#35266;&#28857;&#30340;&#22768;&#26126;&#23545;&#20110;&#21019;&#36896;&#19968;&#20010;&#21253;&#23481;&#30340;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#32570;&#20047;&#36275;&#22815;&#30340;&#25511;&#21046;&#24615;&#26469;&#25903;&#25345;&#29983;&#25104;&#20869;&#23481;&#30340;&#31435;&#22330;&#65292;&#20854;&#20013;&#24448;&#24448;&#21253;&#21547;&#19981;&#19968;&#33268;&#12289;&#20013;&#31435;&#25110;&#26377;&#20559;&#35265;&#30340;&#22768;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;LLMs&#22312;&#29983;&#25104;&#25903;&#25345;&#29992;&#25143;&#22312;&#25552;&#31034;&#20013;&#23450;&#20041;&#30340;&#35770;&#28857;&#30340;&#22768;&#26126;&#26102;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20004;&#20010;&#25345;&#26377;&#30456;&#21453;&#31435;&#22330;&#30340;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#20135;&#29983;&#20102;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#65292;&#36825;&#20123;&#22768;&#26126;&#23545;&#20110;&#25913;&#21892;LLMs&#30340;&#21487;&#25511;&#24615;&#26159;&#37325;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Debate &amp; Tuning&#65288;&#8220;DEBATunE&#8221;&#65289;&#27969;&#31243;&#65292;&#36890;&#36807;&#24494;&#35843;LLMs&#29983;&#25104;&#36890;&#36807;&#36777;&#35770;&#33719;&#24471;&#30340;&#22768;&#26126;&#12290;&#20026;&#20102;&#26816;&#39564;DEBATunE&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#36804;&#20170;&#20026;&#27490;&#28085;&#30422;710&#20010;&#20105;&#35758;&#24615;&#20027;&#39064;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10614v1 Announce Type: cross  Abstract: Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate &amp; tuning ("DEBATunE") pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATunE, we curate the largest dataset of debate topics so far, which covers 710 contro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Rowen&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26816;&#32034;&#22686;&#24378;&#36807;&#31243;&#65292;&#37319;&#29992;&#22810;&#35821;&#20041;&#24863;&#30693;&#26816;&#27979;&#27169;&#22359;&#26469;&#24179;&#34913;&#21442;&#25968;&#21270;&#30693;&#35782;&#21644;&#22806;&#37096;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10612</link><description>&lt;p&gt;
&#20165;&#22312;&#38656;&#35201;&#26102;&#26816;&#32034;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36866;&#24212;&#24615;&#26816;&#32034;&#22686;&#24378;&#20197;&#20943;&#36731;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Rowen&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26816;&#32034;&#22686;&#24378;&#36807;&#31243;&#65292;&#37319;&#29992;&#22810;&#35821;&#20041;&#24863;&#30693;&#26816;&#27979;&#27169;&#22359;&#26469;&#24179;&#34913;&#21442;&#25968;&#21270;&#30693;&#35782;&#21644;&#22806;&#37096;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23454;&#38469;&#23454;&#26045;&#26500;&#25104;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#29983;&#25104;&#20107;&#23454;&#20869;&#23481;&#26102;&#21033;&#29992;&#21442;&#25968;&#21270;&#30693;&#35782;&#21463;&#21040;LLMs&#26377;&#38480;&#30693;&#35782;&#30340;&#38480;&#21046;&#65292;&#21487;&#33021;&#23548;&#33268;&#20869;&#37096;&#24187;&#35273;&#12290;&#34429;&#28982;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#21487;&#20197;&#22635;&#34917;&#30693;&#35782;&#31354;&#30333;&#65292;&#20294;&#20063;&#20250;&#24341;&#20837;&#26080;&#20851;&#20449;&#24687;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#22686;&#21152;&#22806;&#37096;&#24187;&#35273;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;LLMs&#20869;&#37096;&#24179;&#34913;&#22320;&#25972;&#21512;&#21442;&#25968;&#21270;&#30693;&#35782;&#21644;&#22806;&#37096;&#20449;&#24687;&#23545;&#32531;&#35299;&#24187;&#35273;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;Rowen&#65292;&#19968;&#31181;&#22686;&#24378;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#36873;&#25321;&#24615;&#26816;&#32034;&#22686;&#24378;&#36807;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#24187;&#35273;&#36755;&#20986;&#12290;&#35813;&#36807;&#31243;&#30001;&#19968;&#20010;&#22810;&#35821;&#20041;&#24863;&#30693;&#26816;&#27979;&#27169;&#22359;&#31649;&#29702;&#65292;&#35813;&#27169;&#22359;&#35780;&#20272;&#20102;&#23545;&#30456;&#21516;&#26597;&#35810;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#25200;&#21160;&#21709;&#24212;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10612v1 Announce Type: new  Abstract: Hallucinations pose a significant challenge for the practical implementation of large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations. While incorporating external information can help fill knowledge gaps, it also introduces the risk of irrelevant information, thereby increasing the likelihood of external hallucinations. A careful and balanced integration of the parametric knowledge within LLMs with external information is crucial to alleviate hallucinations. In this study, we present Rowen, a novel approach that enhances LLMs with a selective retrieval augmentation process tailored to address hallucinated outputs. This process is governed by a multilingual semantic-aware detection module, which evaluates the consistency of the perturbed responses across various languages for the same queries. Up
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#20102;&#36234;&#29425;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26377;&#23475;&#38382;&#39064;&#30340;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;59.42%&#12290;</title><link>https://arxiv.org/abs/2402.10601</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#35789;&#26367;&#25442;&#23494;&#30721;&#26469;&#36234;&#29425;&#19987;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Proprietary Large Language Models using Word Substitution Cipher
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#20102;&#36234;&#29425;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26377;&#23475;&#38382;&#39064;&#30340;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;59.42%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36981;&#24490;&#36947;&#24503;&#21644;&#20262;&#29702;&#20934;&#21017;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21517;&#20026;Jailbreak&#30340;&#21019;&#24847;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25552;&#31034;&#21487;&#20197;&#32469;&#36807;&#23545;&#40784;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36234;&#29425;&#25552;&#31034;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#65288;&#20027;&#35201;&#26159;&#33521;&#35821;&#65289;&#20013;&#30340;&#26377;&#23475;&#38382;&#39064;&#65292;&#21487;&#20197;&#34987;LLMs&#33258;&#36523;&#26816;&#27979;&#21040;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#32534;&#30721;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#26368;&#20808;&#36827;&#30340;LLM&#65292;GPT-4&#19978;&#36827;&#34892;&#20102;&#19968;&#20010;&#35797;&#28857;&#30740;&#31350;&#65292;&#35299;&#30721;&#20102;&#20351;&#29992;&#21508;&#31181;&#23494;&#30721;&#25216;&#26415;&#21152;&#23494;&#30340;&#20960;&#20010;&#23433;&#20840;&#21477;&#23376;&#65292;&#21457;&#29616;&#31616;&#21333;&#30340;&#21333;&#35789;&#26367;&#25442;&#23494;&#30721;&#21487;&#20197;&#34987;&#26368;&#26377;&#25928;&#22320;&#35299;&#30721;&#12290;&#21463;&#27492;&#32467;&#26524;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#32534;&#30721;&#25216;&#26415;&#26469;&#32534;&#20889;&#36234;&#29425;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#19981;&#23433;&#20840;&#21333;&#35789;&#26144;&#23556;&#21040;&#23433;&#20840;&#21333;&#35789;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26144;&#23556;&#30340;&#21333;&#35789;&#25552;&#20986;&#19981;&#23433;&#20840;&#38382;&#39064;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#36234;&#29425;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;&#39640;&#36798;59.42%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10601v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process. However, most jailbreaking prompts contain harmful questions in the natural language (mainly English), which can be detected by the LLM themselves. In this paper, we present jailbreaking prompts encoded using cryptographic techniques. We first present a pilot study on the state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively. Motivated by this result, we use this encoding technique for writing jailbreaking prompts. We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words. Experimental results show an attack success rate (up to 59.42%) of our proposed jailbrea
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19981;&#21516;Parameter Efficient Fine-tuning (PEFT)&#26041;&#27861;&#22312;&#20020;&#24202;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#38500;&#20102;LoRA&#22806;&#65292;&#22823;&#22810;&#25968;PEFT&#26041;&#27861;&#22312;&#21508;&#20010;&#27169;&#22411;&#35268;&#27169;&#21644;&#20219;&#21153;&#20013;&#24615;&#33021;&#19981;&#31283;&#23450;&#65292;&#32780;LoRA&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#24615;&#33021;&#37117;&#30456;&#23545;&#36739;&#39640;&#12290;PEFT&#26041;&#27861;&#22312;&#20020;&#24202;&#39046;&#22495;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#21487;&#20197;&#25805;&#20316;&#30340;&#19987;&#38376;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10597</link><description>&lt;p&gt;
&#35268;&#27169;&#25928;&#29575;&#65306;&#30740;&#31350;&#24494;&#23567;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10597
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19981;&#21516;Parameter Efficient Fine-tuning (PEFT)&#26041;&#27861;&#22312;&#20020;&#24202;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#38500;&#20102;LoRA&#22806;&#65292;&#22823;&#22810;&#25968;PEFT&#26041;&#27861;&#22312;&#21508;&#20010;&#27169;&#22411;&#35268;&#27169;&#21644;&#20219;&#21153;&#20013;&#24615;&#33021;&#19981;&#31283;&#23450;&#65292;&#32780;LoRA&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#24615;&#33021;&#37117;&#30456;&#23545;&#36739;&#39640;&#12290;PEFT&#26041;&#27861;&#22312;&#20020;&#24202;&#39046;&#22495;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#21487;&#20197;&#25805;&#20316;&#30340;&#19987;&#38376;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#20837;&#30740;&#31350;&#21644;&#21830;&#19994;&#39046;&#22495;&#65292;&#24341;&#21457;&#20102;&#36234;&#26469;&#36234;&#22823;&#27169;&#22411;&#30340;&#36235;&#21183;&#65292;&#26368;&#21021;&#25215;&#35834;&#36890;&#29992;&#24615;&#65292;&#38543;&#21518;&#26222;&#36941;&#24076;&#26395;&#32553;&#23567;&#35268;&#27169;&#24182;&#21019;&#24314;&#19987;&#38376;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#25972;&#24494;&#35843;&#65292;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;PEFT&#26041;&#27861;&#22312;&#20020;&#24202;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#28085;&#30422;&#19968;&#31995;&#21015;&#27169;&#22411;&#35268;&#27169;&#65292;&#21253;&#25324;&#21482;&#26377;$25$&#30334;&#19975;&#21442;&#25968;&#30340;&#26497;&#23567;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;PEFT&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#36739;&#22823;&#65292;&#38500;&#20102;LoRA&#22806;&#65292;LoRA&#22312;&#25152;&#26377;&#27169;&#22411;&#35268;&#27169;&#21644;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20445;&#25345;&#30456;&#23545;&#36739;&#39640;&#65292;&#36890;&#24120;&#25509;&#36817;&#25110;&#36798;&#21040;&#23436;&#20840;&#24494;&#35843;&#24615;&#33021;&#12290; PEFT&#26041;&#27861;&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#26159;&#26174;&#32780;&#26131;&#35265;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21487;&#20197;&#25805;&#20316;&#30340;&#19987;&#38376;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10597v1 Announce Type: cross  Abstract: The entry of large language models (LLMs) into research and commercial spaces has led to a trend of ever-larger models, with initial promises of generalisability, followed by a widespread desire to downsize and create specialised models without the need for complete fine-tuning, using Parameter Efficient Fine-tuning (PEFT) methods. We present an investigation into the suitability of different PEFT methods to clinical decision-making tasks, across a range of model sizes, including extremely small models with as few as $25$ million parameters.   Our analysis shows that the performance of most PEFT approaches varies significantly from one task to another, with the exception of LoRA, which maintains relatively high performance across all model sizes and tasks, typically approaching or matching full fine-tuned performance. The effectiveness of PEFT methods in the clinical domain is evident, particularly for specialised models which can oper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Llama-2&#31995;&#21015;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#33521;&#35821;&#20316;&#20026;&#20869;&#37096;&#26530;&#32445;&#35821;&#35328;&#30340;&#29616;&#35937;&#65292;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#26041;&#24335;&#20197;&#21450;&#35821;&#35328;&#20559;&#35265;&#30340;&#36215;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.10588</link><description>&lt;p&gt;
&#25289;&#39532;&#22312;&#33521;&#35821;&#20013;&#26377;&#25928;&#21527;&#65311;&#20851;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#28508;&#22312;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Do Llamas Work in English? On the Latent Language of Multilingual Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;Llama-2&#31995;&#21015;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#33521;&#35821;&#20316;&#20026;&#20869;&#37096;&#26530;&#32445;&#35821;&#35328;&#30340;&#29616;&#35937;&#65292;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#26041;&#24335;&#20197;&#21450;&#35821;&#35328;&#20559;&#35265;&#30340;&#36215;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#22312;&#19981;&#24179;&#34913;&#12289;&#33521;&#35821;&#20027;&#23548;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#33521;&#35821;&#20316;&#20026;&#20869;&#37096;&#26530;&#32445;&#35821;&#35328;&#30340;&#38382;&#39064;&#8212;&#8212;&#36825;&#23545;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#33021;&#26041;&#24335;&#20197;&#21450;&#35821;&#35328;&#20559;&#35265;&#30340;&#36215;&#28304;&#33267;&#20851;&#37325;&#35201;&#12290; &#25105;&#20204;&#20851;&#27880;Llama-2&#31995;&#21015;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31934;&#24515;&#26500;&#24314;&#30340;&#38750;&#33521;&#35821;&#25552;&#31034;&#21644;&#21807;&#19968;&#27491;&#30830;&#30340;&#21333;&#35789;&#24310;&#32493;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290; &#20174;&#19968;&#23618;&#21040;&#21478;&#19968;&#23618;&#65292;&#21464;&#21387;&#22120;&#36880;&#28176;&#23558;&#26368;&#32456;&#25552;&#31034;&#20196;&#29260;&#30340;&#36755;&#20837;&#23884;&#20837;&#26144;&#23556;&#21040;&#36755;&#20986;&#23884;&#20837;&#65292;&#20174;&#20013;&#35745;&#31639;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#27010;&#29575;&#12290; &#36890;&#36807;&#36319;&#36394;&#20854;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20013;&#38388;&#23884;&#20837;&#65292;&#25581;&#31034;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#21363;&#20013;&#38388;&#23884;&#20837;&#65288;1&#65289;&#24320;&#22987;&#36828;&#31163;&#36755;&#20986;&#20196;&#29260;&#23884;&#20837;&#65307;&#65288;2&#65289;&#22312;&#20013;&#38388;&#23618;&#24050;&#32463;&#20801;&#35768;&#35299;&#30721;&#19968;&#20010;&#35821;&#20041;&#27491;&#30830;&#30340;&#19979;&#19968;&#20010;&#20196;&#29260;&#65292;&#20294;&#26356;&#20542;&#21521;&#20110;&#33521;&#35821;&#29256;&#26412;&#32780;&#19981;&#26159;&#36755;&#20837;&#35821;&#35328;&#30340;&#29256;&#26412;&#65307;&#65288;3&#65289;&#26368;&#32456;&#31227;&#21160;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10588v1 Announce Type: new  Abstract: We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language -- a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#30740;&#31350;&#25991;&#26412;&#20013;&#30340;&#35805;&#35821;&#29305;&#24449;&#26469;&#21306;&#20998;&#20154;&#31867;&#21019;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#20889;&#20316;&#22312;&#32467;&#26500;&#19978;&#26356;&#20026;&#22810;&#26679;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10586</link><description>&lt;p&gt;
&#32454;&#24494;&#20043;&#32447;&#65306;&#36890;&#36807;&#35805;&#35821;&#20027;&#39064;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#30740;&#31350;&#25991;&#26412;&#20013;&#30340;&#35805;&#35821;&#29305;&#24449;&#26469;&#21306;&#20998;&#20154;&#31867;&#21019;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#20889;&#20316;&#22312;&#32467;&#26500;&#19978;&#26356;&#20026;&#22810;&#26679;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#31867;&#21019;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#30028;&#38480;&#21464;&#24471;&#26085;&#30410;&#27169;&#31946;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35782;&#21035;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#20013;&#21487;&#36776;&#35782;&#21644;&#29420;&#29305;&#30340;&#35821;&#35328;&#29305;&#24615;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#25581;&#31034;&#25991;&#26412;&#22312;&#34920;&#38754;&#32467;&#26500;&#20043;&#22806;&#30340;&#28508;&#22312;&#35805;&#35821;&#32467;&#26500;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#35770;&#65292;&#25105;&#20204;&#21033;&#29992;&#23618;&#27425;&#21270;&#35299;&#26512;&#26641;&#21644;&#36882;&#24402;&#36229;&#22270;&#26469;&#25581;&#31034;LLM&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#29420;&#29305;&#35805;&#35821;&#27169;&#24335;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLM&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#37117;&#21463;&#29305;&#23450;&#39046;&#22495;&#30340;&#24433;&#21709;&#32780;&#20135;&#29983;&#19981;&#21516;&#30340;&#35805;&#35821;&#27169;&#24335;&#65292;&#20294;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#34920;&#29616;&#20986;&#26356;&#22810;&#30340;&#32467;&#26500;&#21464;&#24322;&#24615;&#65292;&#21453;&#26144;&#20102;&#19981;&#21516;&#39046;&#22495;&#20154;&#31867;&#20889;&#20316;&#30340;&#24494;&#22937;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24341;&#20837;&#23618;&#27425;&#35805;&#35821;&#29305;&#24449;&#21487;&#20197;&#22686;&#24378;&#20108;&#20803;&#20998;&#31867;&#22120;&#22312;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10586v1 Announce Type: new  Abstract: With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans. Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers' overall performance in distinguishing between huma
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LinkNER&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#30340;&#38142;&#25509;&#31574;&#30053;RDC&#65292;&#20351;&#24494;&#35843;&#27169;&#22411;&#33021;&#22815;&#34917;&#20805;&#40657;&#30418;LLMs</title><link>https://arxiv.org/abs/2402.10573</link><description>&lt;p&gt;
LinkNER: &#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#23558;&#26412;&#22320;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10573
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LinkNER&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#30340;&#38142;&#25509;&#31574;&#30053;RDC&#65292;&#20351;&#24494;&#35843;&#27169;&#22411;&#33021;&#22815;&#34917;&#20805;&#40657;&#30418;LLMs
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#30452;&#25509;&#24433;&#21709;&#30528;&#32593;&#32476;&#20869;&#23481;&#20998;&#26512;&#12289;&#25628;&#32034;&#24341;&#25806;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#12290;&#24494;&#35843;&#21518;&#30340;NER&#27169;&#22411;&#22312;&#26631;&#20934;NER&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#24494;&#35843;&#25968;&#25454;&#21644;&#32570;&#20047;&#30693;&#35782;&#65292;&#23427;&#22312;&#26410;&#35265;&#23454;&#20307;&#35782;&#21035;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;NER&#27169;&#22411;&#22312;&#32593;&#32476;&#30456;&#20851;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#21463;&#21040;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#20016;&#23500;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#20294;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#32570;&#20047;NER&#20219;&#21153;&#30340;&#19987;&#19994;&#24615;&#12290;&#27492;&#22806;&#65292;&#31169;&#26377;&#21644;&#22823;&#35268;&#27169;&#26435;&#37325;&#20351;LLM&#30340;&#35843;&#25972;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#21644;LLMs&#65288;LinkNER&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#38142;&#25509;&#31574;&#30053;RDC&#65292;&#20351;&#24494;&#35843;&#27169;&#22411;&#33021;&#22815;&#34917;&#20805;&#40657;&#30418;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10573v1 Announce Type: new  Abstract: Named Entity Recognition (NER) serves as a fundamental task in natural language understanding, bearing direct implications for web content analysis, search engines, and information retrieval systems. Fine-tuned NER models exhibit satisfactory performance on standard NER benchmarks. However, due to limited fine-tuning data and lack of knowledge, it performs poorly on unseen entity recognition. As a result, the usability and reliability of NER models in web-related applications are compromised. Instead, Large Language Models (LLMs) like GPT-4 possess extensive external knowledge, but research indicates that they lack specialty for NER tasks. Furthermore, non-public and large-scale weights make tuning LLMs difficult. To address these challenges, we propose a framework that combines small fine-tuned models with LLMs (LinkNER) and an uncertainty-based linking strategy called RDC that enables fine-tuned models to complement black-box LLMs, ach
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#23545;&#24453;&#27599;&#20010;&#20559;&#22909;&#23545;&#12290;</title><link>https://arxiv.org/abs/2402.10571</link><description>&lt;p&gt;
&#20855;&#26377;&#20559;&#32622;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Preference Optimization with an Offset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#23545;&#24453;&#27599;&#20010;&#20559;&#22909;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;DPO&#30340;&#27867;&#21270;&#24418;&#24335;&#65292;&#31216;&#20026;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#23558;&#27599;&#20010;&#20559;&#22909;&#23545;&#35270;&#20026;&#30456;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10571v1 Announce Type: cross  Abstract: Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset valu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10567</link><description>&lt;p&gt;
&#22312;InSaAF&#20013;&#34701;&#20837;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615; | LLM&#26159;&#21542;&#24050;&#32463;&#20934;&#22791;&#22909;&#36827;&#20837;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#25552;&#20986;&#20102;&#20247;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25191;&#34892;&#27861;&#24459;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#39044;&#27979;&#21028;&#20915;&#21040;&#29983;&#25104;&#25688;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#24182;&#23637;&#31034;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#28041;&#21450;&#31038;&#20250;&#22240;&#32032;&#26102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;$\beta$-&#21152;&#26435;&#30340;$\textit{&#27861;&#24459;&#23433;&#20840;&#20998;&#25968;($LSS_{\beta}$)}$&#65292;&#23558;LLM&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20004;&#20010;&#26041;&#38754;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;LLM&#22312;$\textit{&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;}$&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#20854;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#23637;&#31034;&#26469;&#35780;&#20272;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;LLaMA&#21644;LLaMA--2&#27169;&#22411;&#30340;&#20219;&#21153;&#34920;&#29616;&#21644;&#20844;&#24179;&#24471;&#20998;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#21160;&#29228;&#21462;&#21644;&#23545;&#40784;&#21477;&#23376;&#23545;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#37325;&#36848;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.10558</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#29228;&#21462;&#21644;&#23545;&#40784;&#30340;&#21477;&#23376;&#23545;&#36827;&#34892;&#31070;&#32463;&#37325;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural paraphrasing by automatically crawled and aligned sentence pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10558
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#29228;&#21462;&#21644;&#23545;&#40784;&#21477;&#23376;&#23545;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#37325;&#36848;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25220;&#34989;&#26159;&#20351;&#29992;&#20854;&#20182;&#35789;&#35821;&#37325;&#26032;&#20070;&#20889;&#36755;&#20837;&#25991;&#26412;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#25913;&#21464;&#21407;&#22987;&#20869;&#23481;&#30340;&#21547;&#20041;&#12290;&#20250;&#35805;&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#33258;&#21160;&#25220;&#34989;&#26469;&#20351;&#23545;&#35805;&#26356;&#21152;&#33258;&#28982;&#65292;&#20363;&#22914;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#37322;&#20041;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#28857;&#35848;&#35770;&#26576;&#20010;&#29305;&#23450;&#20027;&#39064;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#33258;&#21160;&#29983;&#25104;&#37322;&#20041;&#30340;&#20219;&#21153;&#24050;&#32463;&#24471;&#21040;&#20102;&#35299;&#20915;&#12290;&#34429;&#28982;&#35768;&#22810;&#29616;&#26377;&#31995;&#32479;&#21482;&#26159;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65292;&#20294;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20960;&#20010;NLG&#20219;&#21153;&#19978;&#30340;&#26368;&#26032;&#25104;&#21151;&#33258;&#28982;&#22320;&#26263;&#31034;&#20102;&#21033;&#29992;&#36825;&#20123;&#32593;&#32476;&#26469;&#29983;&#25104;&#37322;&#20041;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25220;&#34989;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#22823;&#35268;&#27169;&#24102;&#26377;&#23545;&#40784;&#30340;&#21477;&#23376;&#21644;&#37322;&#20041;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#30340;&#26377;&#25928;&#38656;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#22823;&#35268;&#27169;&#23545;&#40784;&#21477;&#23376;&#21644;&#37322;&#20041;&#23545;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10558v1 Announce Type: new  Abstract: Paraphrasing is the task of re-writing an input text using other words, without altering the meaning of the original content. Conversational systems can exploit automatic paraphrasing to make the conversation more natural, e.g., talking about a certain topic using different paraphrases in different time instants. Recently, the task of automatically generating paraphrases has been approached in the context of Natural Language Generation (NLG). While many existing systems simply consist in rule-based models, the recent success of the Deep Neural Networks in several NLG tasks naturally suggests the possibility of exploiting such networks for generating paraphrases. However, the main obstacle toward neural-network-based paraphrasing is the lack of large datasets with aligned pairs of sentences and paraphrases, that are needed to efficiently train the neural models. In this paper we present a method for the automatic generation of large align
&lt;/p&gt;</description></item><item><title>SPAR&#26159;&#19968;&#20010;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;PLM&#12289;&#22810;&#27880;&#24847;&#21147;&#23618;&#21644;&#27880;&#24847;&#21147;&#31232;&#30095;&#26426;&#21046;&#65292;&#22312;&#20250;&#35805;&#32423;&#21035;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#65292;&#25552;&#21462;&#20840;&#38754;&#29992;&#25143;&#20852;&#36259;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2402.10555</link><description>&lt;p&gt;
SPAR&#65306;&#36890;&#36807;&#38271;&#26399;&#21442;&#19982;&#27880;&#24847;&#21147;&#23454;&#29616;&#20010;&#24615;&#21270;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
SPAR: Personalized Content-Based Recommendation via Long Engagement Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10555
&lt;/p&gt;
&lt;p&gt;
SPAR&#26159;&#19968;&#20010;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;PLM&#12289;&#22810;&#27880;&#24847;&#21147;&#23618;&#21644;&#27880;&#24847;&#21147;&#31232;&#30095;&#26426;&#21046;&#65292;&#22312;&#20250;&#35805;&#32423;&#21035;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#65292;&#25552;&#21462;&#20840;&#38754;&#29992;&#25143;&#20852;&#36259;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29992;&#25143;&#38271;&#26399;&#21442;&#19982;&#21382;&#21490;&#23545;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#33267;&#20851;&#37325;&#35201;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#25104;&#21151;&#23548;&#33268;&#23427;&#20204;&#34987;&#29992;&#20110;&#32534;&#30721;&#29992;&#25143;&#21382;&#21490;&#21644;&#20505;&#36873;&#39033;&#65292;&#23558;&#20869;&#23481;&#25512;&#33616;&#35270;&#20026;&#25991;&#26412;&#35821;&#20041;&#21305;&#37197;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#22312;&#22788;&#29702;&#38750;&#24120;&#38271;&#30340;&#29992;&#25143;&#21382;&#21490;&#25991;&#26412;&#21644;&#19981;&#36275;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#26694;&#26550;SPAR&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#20174;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#20013;&#25552;&#21462;&#20840;&#38754;&#29992;&#25143;&#20852;&#36259;&#30340;&#25361;&#25112;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;PLM&#12289;&#22810;&#27880;&#24847;&#21147;&#23618;&#21644;&#27880;&#24847;&#21147;&#31232;&#30095;&#26426;&#21046;&#20197;&#20250;&#35805;&#20026;&#22522;&#30784;&#23545;&#29992;&#25143;&#30340;&#21382;&#21490;&#36827;&#34892;&#32534;&#30721;&#12290;&#29992;&#25143;&#21644;&#29289;&#21697;&#20391;&#29305;&#24449;&#34987;&#20805;&#20998;&#34701;&#21512;&#36827;&#34892;&#21442;&#19982;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#25345;&#21452;&#26041;&#30340;&#29420;&#31435;&#34920;&#31034;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#27169;&#22411;&#37096;&#32626;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10555v1 Announce Type: cross  Abstract: Leveraging users' long engagement histories is essential for personalized content recommendations. The success of pretrained language models (PLMs) in NLP has led to their use in encoding user histories and candidate items, framing content recommendations as textual semantic matching tasks. However, existing works still struggle with processing very long user historical text and insufficient user-item interaction. In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles the challenges of holistic user interest extraction from the long user engagement history. It achieves so by leveraging PLM, poly-attention layers and attention sparsity mechanisms to encode user's history in a session-based manner. The user and item side features are sufficiently fused for engagement prediction while maintaining standalone representations for both sides, which is efficient for practical model deployment. Mor
&lt;/p&gt;</description></item><item><title>Disordered-DABS&#26159;&#38024;&#23545;&#19981;&#35268;&#21017;&#25991;&#26412;&#20013;&#21160;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24635;&#32467;&#32780;&#35774;&#35745;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#24635;&#32467;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10554</link><description>&lt;p&gt;
&#19981;&#35268;&#21017;&#25991;&#26412;&#20013;&#21160;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24635;&#32467;&#26631;&#20934;&#65306; Disordered-DABS&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10554
&lt;/p&gt;
&lt;p&gt;
Disordered-DABS&#26159;&#38024;&#23545;&#19981;&#35268;&#21017;&#25991;&#26412;&#20013;&#21160;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24635;&#32467;&#32780;&#35774;&#35745;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#24635;&#32467;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#20026;&#22522;&#30784;&#30340;&#24635;&#32467;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#12290;&#28982;&#32780;&#65292;&#24635;&#32467;&#19981;&#35268;&#21017;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#65292;&#27604;&#22914;&#31038;&#20132;&#23186;&#20307;&#21644;&#23458;&#25143;&#21453;&#39304;&#20013;&#21457;&#29616;&#30340;&#25991;&#26412;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38024;&#23545;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#30340;&#39044;&#23450;&#20041;&#26041;&#38754;&#65292;&#24573;&#30053;&#20102;&#21160;&#24577;&#21644;&#26080;&#24207;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Disordered-DABS&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#21160;&#24577;&#26041;&#38754;&#30340;&#24635;&#32467;&#22522;&#20934;&#27979;&#35797;&#65292;&#19987;&#20026;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#37327;&#36523;&#23450;&#21046;&#12290;&#36890;&#36807;&#35843;&#25972;&#29616;&#26377;&#25968;&#25454;&#38598;&#20197;&#25552;&#39640;&#25104;&#26412;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#21644;&#35814;&#32454;&#30340;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;Disordered-DABS&#23545;&#24403;&#20195;&#24635;&#32467;&#27169;&#22411;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;GPT-3.5&#31561;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10554v1 Announce Type: new  Abstract: Aspect-based summarization has seen significant advancements, especially in structured text. Yet, summarizing disordered, large-scale texts, like those found in social media and customer feedback, remains a significant challenge. Current research largely targets predefined aspects within structured texts, neglecting the complexities of dynamic and disordered environments. Addressing this gap, we introduce Disordered-DABS, a novel benchmark for dynamic aspect-based summarization tailored to unstructured text. Developed by adapting existing datasets for cost-efficiency and scalability, our comprehensive experiments and detailed human evaluations reveal that Disordered-DABS poses unique challenges to contemporary summarization models, including state-of-the-art language models such as GPT-3.5.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#35805;&#24335;SimulMT&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#30340;SimulMT&#25512;&#29702;&#25928;&#29575;&#65292;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#19982;&#19987;&#38376;&#30340;SimulMT&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2402.10552</link><description>&lt;p&gt;
Conversational SimulMT: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21516;&#26102;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10552
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35805;&#24335;SimulMT&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#30340;SimulMT&#25512;&#29702;&#25928;&#29575;&#65292;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#19982;&#19987;&#38376;&#30340;SimulMT&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#22768;&#26426;&#22120;&#32763;&#35793;&#65288;SimulMT&#65289;&#22312;&#32763;&#35793;&#36136;&#37327;&#21644;&#24310;&#36831;&#20043;&#38388;&#23384;&#22312;&#25361;&#25112;&#24615;&#30340;&#26435;&#34913;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;SimulMT&#20219;&#21153;&#20013;&#21487;&#20197;&#21462;&#24471;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#24448;&#24448;&#26159;&#20197;&#25512;&#29702;&#25104;&#26412;&#21644;&#24310;&#36831;&#30340;&#22686;&#21152;&#20026;&#20195;&#20215;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#24335;SimulMT&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#35299;&#30721;&#26469;&#25552;&#39640;&#22522;&#20110;LLM&#30340;SimulMT&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;SimulMT&#22522;&#20934;&#19978;&#20351;&#29992;Llama2-7b-chat&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#22312;&#32763;&#35793;&#36136;&#37327;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#23454;&#29616;&#19982;&#19987;&#38376;&#30340;SimulMT&#27169;&#22411;&#30456;&#24403;&#30340;&#35745;&#31639;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10552v1 Announce Type: new  Abstract: Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20013;&#36896;&#25104;&#30340;&#24378;&#24187;&#35273;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21542;&#23450;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#32780;&#26080;&#38656;&#20351;&#29992;&#31232;&#30095;&#36127;&#25968;&#25454;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.10543</link><description>&lt;p&gt;
&#28040;&#38500;&#21542;&#23450;&#23548;&#33268;&#30340;&#24378;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Strong hallucinations from negation and how to fix them
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10543
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20013;&#36896;&#25104;&#30340;&#24378;&#24187;&#35273;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21542;&#23450;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#32780;&#26080;&#38656;&#20351;&#29992;&#31232;&#30095;&#36127;&#25968;&#25454;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#22312;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26377;&#26102;&#20250;&#25552;&#20379;&#30001;&#20110;&#36923;&#36753;&#19981;&#36830;&#36143;&#32780;&#19981;&#21487;&#33021;&#25104;&#31435;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#21709;&#24212;&#20026;\textit{&#24378;&#24187;&#35273;}&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#28304;&#20110;LM&#35745;&#31639;&#20854;&#20869;&#37096;&#34920;&#31034;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#20174;&#36825;&#20123;&#34920;&#31034;&#20013;&#20135;&#29983;&#30340;&#36755;&#20986;&#12290;&#37325;&#28857;&#20851;&#27880;&#21542;&#23450;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#21542;&#23450;&#19981;&#26159;&#20316;&#20026;&#28508;&#22312;&#34920;&#31034;&#30340;&#21478;&#19968;&#20010;&#20803;&#32032;&#65292;&#32780;&#26159;&#20316;&#20026;\textit{LM&#28508;&#22312;&#34920;&#31034;&#19978;&#30340;&#19968;&#20010;&#25805;&#20316;&#65292;&#32422;&#26463;&#23427;&#20204;&#21487;&#33021;&#30340;&#28436;&#21464;&#26041;&#24335;}&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#22312;&#24102;&#21542;&#23450;&#30340;&#22635;&#31354;&#25552;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#23545;&#31232;&#30095;&#36127;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10543v1 Announce Type: cross  Abstract: Despite great performance on many tasks, language models (LMs) still struggle with reasoning, sometimes providing responses that cannot possibly be true because they stem from logical incoherence. We call such responses \textit{strong hallucinations} and prove that they follow from an LM's computation of its internal representations for logical operators and outputs from those representations. Focusing on negation, we provide a novel solution in which negation is treated not as another element of a latent representation, but as \textit{an operation over an LM's latent representations that constrains how they may evolve}. We show that our approach improves model performance in cloze prompting and natural language inference tasks with negation without requiring training on sparse negative data.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#22312;&#22810;&#39046;&#22495;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#30340;&#29305;&#24615;&#65292;&#21457;&#29616;&#29983;&#25104;&#30340;&#35299;&#37322;&#34920;&#29616;&#20986;&#36873;&#25321;&#24615;&#21644;&#21253;&#21547;&#35828;&#26126;&#24615;&#20803;&#32032;&#65292;&#20294;&#36739;&#23569;&#26159;&#20027;&#35266;&#25110;&#35823;&#23548;&#24615;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.10532</link><description>&lt;p&gt;
LLM&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#29305;&#24615;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Properties and Challenges of LLM-Generated Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10532
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#22312;&#22810;&#39046;&#22495;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#30340;&#29305;&#24615;&#65292;&#21457;&#29616;&#29983;&#25104;&#30340;&#35299;&#37322;&#34920;&#29616;&#20986;&#36873;&#25321;&#24615;&#21644;&#21253;&#21547;&#35828;&#26126;&#24615;&#20803;&#32032;&#65292;&#20294;&#36739;&#23569;&#26159;&#20027;&#35266;&#25110;&#35823;&#23548;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#25105;&#21512;&#29702;&#21270;&#33021;&#21147;&#22312;&#38480;&#23450;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#20351;&#29992;&#29305;&#23450;&#20219;&#21153;/&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;LLMs&#24182;&#19981;&#65288;&#20165;&#65289;&#20381;&#36182;&#20110;&#29305;&#23450;&#27880;&#37322;&#30340;&#25968;&#25454;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#35299;&#37322;&#23427;&#20204;&#30340;&#36755;&#20986;&#12290;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#29305;&#24615;&#21463;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#29992;&#20110;&#25351;&#23548;&#24494;&#35843;&#30340;&#30446;&#26631;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21253;&#21547;&#22823;&#37327;&#37326;&#22806;&#20154;&#31867;&#32534;&#20889;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#20551;&#35774;LLMs&#37319;&#29992;&#20102;&#20154;&#31867;&#35299;&#37322;&#30340;&#20849;&#21516;&#29305;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#22810;&#22495;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#30340;&#35299;&#37322;&#34920;&#29616;&#20986;&#36873;&#25321;&#24615;&#24182;&#21253;&#21547;&#35828;&#26126;&#24615;&#20803;&#32032;&#65292;&#20294;&#24456;&#23569;&#26159;&#20027;&#35266;&#25110;&#35823;&#23548;&#24615;&#30340;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23646;&#24615;&#23384;&#22312;&#25110;&#32570;&#22833;&#30340;&#21407;&#22240;&#21644;&#21518;&#26524;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26681;&#25454;LLMs&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#24494;&#35843;&#25968;&#25454;&#30340;&#24615;&#36136;&#65292;&#36825;&#20123;&#23646;&#24615;&#23384;&#22312;&#25110;&#32570;&#22833;&#30340;&#31215;&#26497;&#21644;&#28040;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10532v1 Announce Type: cross  Abstract: The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task/specific data sets. However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs. The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning. As the pre-training corpus includes a large amount of human-written explanations "in the wild", we hypothesise that LLMs adopt common properties of human explanations. By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading. We discuss reasons and consequences of the properties' presence or absence. In particular, we outline positive and negative implications depending on the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.10528</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#36880;&#27493;&#39564;&#35777;&#38169;&#35823;&#31572;&#26696;&#26816;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Verify Step by Step for Incorrect Answer Detection?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#21508;&#31181;&#25193;&#23637;&#30340;CoT&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26368;&#32456;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#26377;&#30740;&#31350;&#35780;&#20272;&#20102;CoT&#20013;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#20180;&#32454;&#23457;&#26597;&#23427;&#20204;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#65292;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;LLMs&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;R2PE&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25506;&#31350;&#19981;&#21516;&#39046;&#22495;&#28085;&#30422;&#20116;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#20013;&#25512;&#29702;&#38142;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#22522;&#20110;&#25512;&#29702;&#27493;&#39588;&#34913;&#37327;LLMs&#26368;&#32456;&#36755;&#20986;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25512;&#29702;&#38142;&#20013;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25171;&#36133;&#24120;&#35782;&#20998;&#25968;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10528v1 Announce Type: cross  Abstract: Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the a
&lt;/p&gt;</description></item><item><title>&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#37319;&#26679;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#21457;&#29616;&#21508;&#31181;&#23545;&#25239;&#23454;&#20307;&#20316;&#20026;&#24178;&#25200;&#22240;&#32032;&#65292;&#30456;&#27604;&#38543;&#26426;&#37319;&#26679;&#65292;&#22312;&#23545;&#25239;&#38382;&#31572;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#29305;&#24449;&#30340;&#20004;&#31181;&#23545;&#25239;&#24615;&#23454;&#20307;&#21046;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.10527</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#37319;&#26679;&#23545;&#25239;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Zero-shot sampling of adversarial entities in biomedical question answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10527
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#37319;&#26679;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#21457;&#29616;&#21508;&#31181;&#23545;&#25239;&#23454;&#20307;&#20316;&#20026;&#24178;&#25200;&#22240;&#32032;&#65292;&#30456;&#27604;&#38543;&#26426;&#37319;&#26679;&#65292;&#22312;&#23545;&#25239;&#38382;&#31572;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#29305;&#24449;&#30340;&#20004;&#31181;&#23545;&#25239;&#24615;&#23454;&#20307;&#21046;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#21442;&#25968;&#22495;&#30693;&#35782;&#30340;&#22686;&#21152;&#28145;&#24230;&#25512;&#21160;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#24555;&#36895;&#37096;&#32626;&#12290;&#22312;&#39640;&#39118;&#38505;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;&#29702;&#35299;&#27169;&#22411;&#30340;&#28431;&#27934;&#23545;&#20110;&#37327;&#21270;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#21644;&#35268;&#33539;&#20854;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#21457;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20316;&#20026;&#23545;&#25239;&#31034;&#20363;&#30340;&#21629;&#21517;&#23454;&#20307;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#22312;&#20854;&#20182;&#29615;&#22659;&#20013;&#21487;&#33021;&#30340;&#20266;&#35013;&#30340;&#30097;&#38382;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#24130;&#32553;&#25918;&#36317;&#31163;&#21152;&#26435;&#37319;&#26679;&#26041;&#26696;&#65292;&#20197;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#23454;&#20307;&#20316;&#20026;&#24178;&#25200;&#22240;&#32032;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#29983;&#29289;&#21307;&#23398;&#20027;&#39064;&#30340;&#23545;&#25239;&#24615;&#38382;&#39064;&#22238;&#31572;&#20013;&#20248;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#25506;&#32034;&#25915;&#20987;&#34920;&#38754;&#19978;&#30340;&#19981;&#21516;&#21306;&#22495;&#65292;&#36825;&#25581;&#31034;&#20102;&#20004;&#31181;&#22312;&#29305;&#24449;&#19978;&#26126;&#26174;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#23454;&#20307;&#30340;&#21046;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#26041;&#24335;&#22914;&#20309;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10527v1 Announce Type: new  Abstract: The increasing depth of parametric domain knowledge in large language models (LLMs) is fueling their rapid deployment in real-world applications. In high-stakes and knowledge-intensive tasks, understanding model vulnerabilities is essential for quantifying the trustworthiness of model predictions and regulating their use. The recent discovery of named entities as adversarial examples in natural language processing tasks raises questions about their potential guises in other settings. Here, we propose a powerscaled distance-weighted sampling scheme in embedding space to discover diverse adversarial entities as distractors. We demonstrate its advantage over random sampling in adversarial question answering on biomedical topics. Our approach enables the exploration of different regions on the attack surface, which reveals two regimes of adversarial entities that markedly differ in their characteristics. Moreover, we show that the attacks su
&lt;/p&gt;</description></item><item><title>LLM Comparator&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#20114;&#24335;&#20998;&#26512;&#33258;&#21160;&#24182;&#34892;&#35780;&#20272;&#32467;&#26524;&#30340;&#26032;&#22411;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25903;&#25345;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#34920;&#29616;&#20248;&#21155;&#21644;&#19981;&#21516;&#20043;&#22788;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10524</link><description>&lt;p&gt;
LLM&#27604;&#36739;&#22120;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#34892;&#35780;&#20272;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10524
&lt;/p&gt;
&lt;p&gt;
LLM Comparator&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#20114;&#24335;&#20998;&#26512;&#33258;&#21160;&#24182;&#34892;&#35780;&#20272;&#32467;&#26524;&#30340;&#26032;&#22411;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25903;&#25345;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#34920;&#29616;&#20248;&#21155;&#21644;&#19981;&#21516;&#20043;&#22788;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24182;&#34892;&#35780;&#20272;&#24050;&#25104;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21709;&#24212;&#36136;&#37327;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#36825;&#31181;&#35780;&#20272;&#26041;&#27861;&#30340;&#32467;&#26524;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLM&#27604;&#36739;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#22320;&#20998;&#26512;&#33258;&#21160;&#24182;&#34892;&#35780;&#20272;&#32467;&#26524;&#12290;&#35813;&#24037;&#20855;&#25903;&#25345;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#24335;&#24037;&#20316;&#27969;&#65292;&#20197;&#20102;&#35299;&#20026;&#20160;&#20040;&#21644;&#20309;&#26102;&#27169;&#22411;&#27604;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#25110;&#26356;&#24046;&#65292;&#20197;&#21450;&#20004;&#20010;&#27169;&#22411;&#30340;&#21709;&#24212;&#22312;&#36136;&#37327;&#19978;&#26377;&#20309;&#19981;&#21516;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#19968;&#23478;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#23494;&#20999;&#21512;&#20316;&#65292;&#36845;&#20195;&#35774;&#35745;&#21644;&#24320;&#21457;&#20102;&#35813;&#24037;&#20855;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#35782;&#21035;&#30340;&#29992;&#25143;&#25361;&#25112;&#12289;&#35813;&#24037;&#20855;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#65292;&#20197;&#21450;&#23450;&#26399;&#35780;&#20272;&#20854;&#27169;&#22411;&#30340;&#21442;&#19982;&#32773;&#30340;&#35266;&#23519;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10524v1 Announce Type: cross  Abstract: Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.10500</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#20559;&#22909;&#20248;&#21270;&#23454;&#29616;&#32463;&#39564;&#35777;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Provably Sample Efficient RLHF via Active Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#40784;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#20381;&#36182;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#22312;&#23454;&#38469;RLHF&#23454;&#26045;&#20013;&#26500;&#25104;&#20102;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26356;&#22909;&#21644;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;RLHF&#20197;&#19978;&#19979;&#25991;&#20559;&#22909;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#24418;&#24335;&#26694;&#23450;&#65292;&#20854;&#20013;&#25552;&#31034;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#22825;&#30495;&#26041;&#24335;&#23548;&#33268;&#19968;&#20010;&#22312;&#22870;&#21169;&#26041;&#38754;&#20855;&#26377;$\Omega(1)$&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Active Preference Optimization}$&#65288;$\texttt{APO}$&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31215;&#26497;&#36873;&#25321;&#25552;&#31034;&#20197;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#20559;&#22909;&#27169;&#22411;&#19979;&#65292;\texttt{APO}&#23454;&#29616;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#22949;&#21327;&#20110;polic
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10500v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards. Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample efficiency without compromising on polic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22810;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#21516;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#25351;&#26631;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#32423;&#21035;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#21407;&#23376;&#20107;&#23454;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.10496</link><description>&lt;p&gt;
&#27604;&#36739;&#22810;&#35821;&#35328;&#29983;&#25104;&#20013;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Comparing Hallucination Detection Metrics for Multilingual Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22810;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#21516;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#25351;&#26631;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#32423;&#21035;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#21407;&#23376;&#20107;&#23454;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#25552;&#20986;&#35768;&#22810;&#38024;&#23545;&#33521;&#25991;&#25991;&#26412;&#30340;&#33258;&#21160;&#24187;&#35273;&#26816;&#27979;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#25928;&#26524;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#23545;&#36825;&#20123;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#34920;&#29616;&#22914;&#20309;&#30340;&#35748;&#35782;&#19978;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26816;&#27979;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#35832;&#22914;ROUGE&#21644;&#21629;&#21517;&#23454;&#20307;&#37325;&#21472;&#20197;&#21450;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#25351;&#26631;&#65292;&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#20256;&#35760;&#25688;&#35201;&#20013;&#26816;&#27979;&#24187;&#35273;&#65307;&#25105;&#20204;&#36824;&#35780;&#20272;&#36825;&#20123;&#19981;&#21516;&#25351;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21028;&#26029;&#23427;&#20204;&#26159;&#21542;&#34913;&#37327;&#30456;&#21516;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;&#65292;&#34429;&#28982;&#35789;&#27719;&#25351;&#26631;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22522;&#20110;NLI&#30340;&#25351;&#26631;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#22312;&#21477;&#23376;&#32423;&#21035;&#34920;&#29616;&#33391;&#22909;&#12290;&#30456;&#21453;&#65292;NLI-based&#25351;&#26631;&#36890;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#21407;&#23376;&#20107;&#23454;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#22810;&#35821;&#35328;&#24187;&#35273;&#26816;&#27979;&#20013;&#30340;&#29616;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10496v1 Announce Type: cross  Abstract: While many automatic hallucination detection techniques have been proposed for English texts, their effectiveness in multilingual contexts remains unexplored. This paper aims to bridge the gap in understanding how these hallucination detection metrics perform on non-English languages. We evaluate the efficacy of various detection metrics, including lexical metrics like ROUGE and Named Entity Overlap and Natural Language Inference (NLI)-based metrics, at detecting hallucinations in biographical summaries in many languages; we also evaluate how correlated these different metrics are to gauge whether they measure the same phenomena. Our empirical analysis reveals that while lexical metrics show limited effectiveness, NLI-based metrics perform well in high-resource languages at the sentence level. In contrast, NLI-based metrics often fail to detect atomic fact hallucinations. Our findings highlight existing gaps in multilingual hallucinati
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;BERT&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#24066;&#22330;&#19979;&#25387;&#24182;&#31283;&#23450;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2402.10481</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#30340;&#21152;&#23494;&#36164;&#20135;&#24066;&#22330;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Emoji Driven Crypto Assets Market Reactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;BERT&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#24066;&#22330;&#19979;&#25387;&#24182;&#31283;&#23450;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#65292;&#35832;&#22914;Twitter&#20043;&#31867;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24050;&#32463;&#25104;&#20026;&#24433;&#21709;&#24066;&#22330;&#36235;&#21183;&#21644;&#25237;&#36164;&#32773;&#24773;&#32490;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#34920;&#24773;&#31526;&#21495;&#36716;&#21270;&#20026;&#21487;&#37327;&#21270;&#30340;&#24773;&#24863;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#35265;&#35299;&#19982;BTC&#20215;&#26684;&#21644;VCRIX&#25351;&#25968;&#31561;&#20851;&#38190;&#24066;&#22330;&#25351;&#26631;&#36827;&#34892;&#20102;&#30456;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26088;&#22312;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20803;&#32032;&#35782;&#21035;&#21644;&#39044;&#27979;&#24066;&#22330;&#36235;&#21183;&#30340;&#20132;&#26131;&#31574;&#30053;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#26377;&#21161;&#20110;&#36991;&#20813;&#37325;&#22823;&#24066;&#22330;&#19979;&#25387;&#65292;&#24182;&#26377;&#21161;&#20110;&#22238;&#25253;&#30340;&#31283;&#23450;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#23558;&#20808;&#36827;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20998;&#26512;&#25972;&#21512;&#21040;&#37329;&#34701;&#31574;&#30053;&#20013;&#30340;&#23454;&#38469;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#30475;&#24453;&#24066;&#22330;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10481v1 Announce Type: cross  Abstract: In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based BERT model for a multimodal sentiment analysis, focusing on the impact of emoji sentiment on cryptocurrency markets. By translating emojis into quantifiable sentiment data, we correlate these insights with key market indicators like BTC Price and the VCRIX index. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.10466</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#22120;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Zero-shot Dialogue State Tracker through Function Calling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#32780;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#26085;&#30410;&#26222;&#36941;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#22312;&#19968;&#33324;&#24773;&#22659;&#20013;&#20855;&#26377;&#20808;&#36827;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#19981;&#20165;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#36824;&#38656;&#35201;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#20869;&#36827;&#34892;&#26377;&#25928;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#65288;DST&#65289;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20173;&#19981;&#23613;&#20154;&#24847;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#35299;&#20915;LLMs&#20013;&#30340;DST&#30340;&#26032;&#26041;&#27861;FnCTOD&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#36827;&#20102;&#38646;-shot DST&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#25110;&#27169;&#22411;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#24320;&#28304;&#25110;&#19987;&#26377;LLMs&#26102;&#37117;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65306;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20351;&#24471;&#21508;&#31181;7B&#25110;13B&#21442;&#25968;&#27169;&#22411;&#36229;&#36234;&#20102;&#20043;&#21069;&#30001;ChatGPT&#23454;&#29616;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#65288;SOTA&#65289;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;ChatGPT&#30340;&#24615;&#33021;&#65292;&#20987;&#36133;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10466v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QDyLoRA&#30340;&#39640;&#25928;&#37327;&#21270;&#21160;&#24577;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#23450;&#20041;&#31209;&#19978;&#23454;&#29616;&#26377;&#25928;&#24494;&#35843;&#65292;&#19982;QLoRA&#30456;&#31454;&#20105;&#65292;&#24182;&#19988;&#22312;&#37319;&#29992;&#20854;&#26368;&#20339;&#31209;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10462</link><description>&lt;p&gt;
QDyLoRA: &#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#20248;&#30340;&#37327;&#21270;&#21160;&#24577;&#20302;&#31209;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QDyLoRA&#30340;&#39640;&#25928;&#37327;&#21270;&#21160;&#24577;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#23450;&#20041;&#31209;&#19978;&#23454;&#29616;&#26377;&#25928;&#24494;&#35843;&#65292;&#19982;QLoRA&#30456;&#31454;&#20105;&#65292;&#24182;&#19988;&#22312;&#37319;&#29992;&#20854;&#26368;&#20339;&#31209;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Finetuning&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#24040;&#22823;&#30340;GPU&#20869;&#23384;&#65292;&#38480;&#21046;&#20102;&#33719;&#21462;&#26356;&#22823;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#34429;&#28982;&#21629;&#21517;&#20026;QLoRA&#30340;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#37327;&#21270;&#29256;&#26412;&#26174;&#33879;&#32531;&#35299;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#26159;&#25214;&#21040;&#39640;&#25928;&#30340;LoRA&#31209;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;QLoRA&#26159;&#22312;&#39044;&#23450;&#20041;&#30340;&#31209;&#19978;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#65292;&#22312;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#24494;&#35843;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#37325;&#26032;&#37197;&#32622;&#20026;&#20854;&#36739;&#20302;&#30340;&#31209;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QDyLoRA-Quantized Dynamic Low-Rank Adaptation-&#65292;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#20302;&#31209;&#36866;&#24212;&#30340;&#39640;&#25928;&#37327;&#21270;&#26041;&#27861;&#12290;&#21463;Dynamic LoRA&#30340;&#21551;&#21457;&#65292;QDyLoRA&#33021;&#22815;&#22312;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;LoRA&#31209;&#19978;&#26377;&#25928;&#22320;&#24494;&#35843;LLMs&#12290;&#36890;&#36807;&#19968;&#36718;&#24494;&#35843;&#65292;QDyLoRA&#33021;&#22815;&#22312;&#21333;&#20010;32 GB V100-GPU&#19978;&#20026;1&#21040;64&#20010;&#31209;&#30340;Falcon-40b&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QDyLoRA&#19982;QLoRA&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#22312;&#20351;&#29992;&#20854;&#26368;&#20339;&#31209;&#26102;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10462v1 Announce Type: cross  Abstract: Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Strategy-Relevant Attention&#65288;SRA&#65289;&#24230;&#37327;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#36981;&#24490;&#25112;&#30053;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#24212;&#29992;SRA&#25351;&#23548;&#30340;&#25552;&#31034;&#21487;&#25552;&#39640;&#25112;&#30053;&#20381;&#20174;&#24615;&#65292;&#20174;&#32780;&#20351;&#38271;&#26102;&#38388;&#23545;&#35805;&#26356;&#21487;&#38752;&#22320;&#23637;&#31034;&#25152;&#38656;&#30340;&#24773;&#24863;&#25903;&#25345;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.10453</link><description>&lt;p&gt;
&#24341;&#23548;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38271;&#26102;&#38388;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Steering Conversational Large Language Models for Long Emotional Support Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10453
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Strategy-Relevant Attention&#65288;SRA&#65289;&#24230;&#37327;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#36981;&#24490;&#25112;&#30053;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#24212;&#29992;SRA&#25351;&#23548;&#30340;&#25552;&#31034;&#21487;&#25552;&#39640;&#25112;&#30053;&#20381;&#20174;&#24615;&#65292;&#20174;&#32780;&#20351;&#38271;&#26102;&#38388;&#23545;&#35805;&#26356;&#21487;&#38752;&#22320;&#23637;&#31034;&#25152;&#38656;&#30340;&#24773;&#24863;&#25903;&#25345;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38271;&#26102;&#38388;&#23545;&#35805;&#20013;&#19968;&#36143;&#36981;&#24490;&#24773;&#24863;&#25903;&#25345;&#31574;&#30053;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Strategy-Relevant Attention&#65288;SRA&#65289;&#24230;&#37327;&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#25351;&#26631;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#24773;&#24863;&#25903;&#25345;&#29615;&#22659;&#20013;&#36981;&#24490;&#25112;&#30053;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLaMA&#27169;&#22411;&#20998;&#26512;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#65288;ESConv&#65289;&#20013;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#35777;&#26126;SRA&#19982;&#27169;&#22411;&#22312;&#25972;&#20010;&#20114;&#21160;&#36807;&#31243;&#20013;&#32500;&#25345;&#25152;&#36848;&#31574;&#30053;&#33021;&#21147;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24212;&#29992;&#22522;&#20110;SRA&#30340;&#25552;&#31034;&#21487;&#25552;&#39640;&#25112;&#30053;&#20381;&#20174;&#24615;&#65292;&#23548;&#33268;&#23545;&#35805;&#26356;&#21487;&#38752;&#22320;&#23637;&#31034;&#38271;&#26102;&#38388;&#23545;&#35805;&#20013;&#25152;&#38656;&#30340;&#24773;&#24863;&#25903;&#25345;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#22810;&#20998;&#25903;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36866;&#29992;&#20110;ESConv&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#31574;&#30053;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10453v1 Announce Type: new  Abstract: In this study, we address the challenge of consistently following emotional support strategies in long conversations by large language models (LLMs). We introduce the Strategy-Relevant Attention (SRA) metric, a model-agnostic measure designed to evaluate the effectiveness of LLMs in adhering to strategic prompts in emotional support contexts. By analyzing conversations within the Emotional Support Conversations dataset (ESConv) using LLaMA models, we demonstrate that SRA is significantly correlated with a model's ability to sustain the outlined strategy throughout the interactions. Our findings reveal that the application of SRA-informed prompts leads to enhanced strategic adherence, resulting in conversations that more reliably exhibit the desired emotional support strategies over longer conversations. Furthermore, we contribute a comprehensive, multi-branch synthetic conversation dataset for ESConv, featuring a variety of strategy cont
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IS3&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#30340;E2O&#21644;O2E&#20004;&#31181;&#37325;&#35201;&#30340;&#35821;&#20041;&#36716;&#21464;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#32500;&#25345;&#23545;&#26087;&#23454;&#20307;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10447</link><description>&lt;p&gt;
&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#65306;&#20004;&#31181;&#36716;&#21464;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Incremental Sequence Labeling: A Tale of Two Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IS3&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#30340;E2O&#21644;O2E&#20004;&#31181;&#37325;&#35201;&#30340;&#35821;&#20041;&#36716;&#21464;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#32500;&#25345;&#23545;&#26087;&#23454;&#20307;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#28041;&#21450;&#22312;&#20445;&#30041;&#23545;&#20808;&#21069;&#31867;&#21035;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#38543;&#26102;&#38388;&#19981;&#26029;&#23398;&#20064;&#26032;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#31181;&#37325;&#35201;&#30340;&#35821;&#20041;&#36716;&#21464;&#65306;E2O&#65288;&#27169;&#22411;&#23558;&#26087;&#23454;&#20307;&#38169;&#35823;&#26631;&#35760;&#20026;&#38750;&#23454;&#20307;&#65289;&#21644;O2E&#65288;&#27169;&#22411;&#23558;&#38750;&#23454;&#20307;&#25110;&#26087;&#23454;&#20307;&#26631;&#35760;&#20026;&#26032;&#23454;&#20307;&#65289;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;E2O&#38382;&#39064;&#19978;&#65292;&#24573;&#35270;&#20102;O2E&#38382;&#39064;&#12290;&#36825;&#31181;&#24573;&#30053;&#23548;&#33268;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23545;&#26032;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#26102;&#23384;&#22312;&#20559;&#35265;&#65292;&#35748;&#20026;&#23427;&#20204;&#23646;&#20110;&#26032;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#26080;&#35821;&#20041;&#36716;&#21464;&#30340;&#22686;&#37327;&#39034;&#24207;&#26631;&#35760;&#65288;IS3&#65289;&#12290;&#21463;&#21040;&#24050;&#30830;&#23450;&#30340;&#35821;&#20041;&#36716;&#21464;&#65288;E2O&#21644;O2E&#65289;&#30340;&#21551;&#21457;&#65292;IS3&#26088;&#22312;&#32531;&#35299;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#33267;&#20110;E2O&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#32500;&#25345;&#27169;&#22411;&#23545;&#26087;&#23454;&#20307;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10447v1 Announce Type: new  Abstract: The incremental sequence labeling task involves continuously learning new classes over time while retaining knowledge of the previous ones. Our investigation identifies two significant semantic shifts: E2O (where the model mislabels an old entity as a non-entity) and O2E (where the model labels a non-entity or old entity as a new entity). Previous research has predominantly focused on addressing the E2O problem, neglecting the O2E issue. This negligence results in a model bias towards classifying new data samples as belonging to the new class during the learning process. To address these challenges, we propose a novel framework, Incremental Sequential Labeling without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the E2O problem, we use knowledge distillation to maintain the model's discriminative ability for old entities. Simultaneously, t
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#19981;&#21516;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#34920;&#29616;&#65292;&#21457;&#29616;&#24403;&#20854;&#25317;&#25265;&#29305;&#23450;&#31038;&#20250;&#36523;&#20221;&#26102;&#65292;&#20250;&#21306;&#20998;&#20869;&#22806;&#32676;&#20307;&#65292;&#20559;&#22909;&#20869;&#32676;&#20307;&#20215;&#20540;&#35266;&#32780;&#25269;&#21046;&#22806;&#32676;&#20307;&#20215;&#20540;&#35266;&#12290;</title><link>https://arxiv.org/abs/2402.10436</link><description>&lt;p&gt;
&#25105;&#19981;&#26159;&#20182;&#20204;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27969;&#21160;&#36523;&#20221;&#21644;&#25345;&#20037;&#30340;&#22806;&#32676;&#20307;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10436
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;ChatGPT&#22312;&#19981;&#21516;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#34920;&#29616;&#65292;&#21457;&#29616;&#24403;&#20854;&#25317;&#25265;&#29305;&#23450;&#31038;&#20250;&#36523;&#20221;&#26102;&#65292;&#20250;&#21306;&#20998;&#20869;&#22806;&#32676;&#20307;&#65292;&#20559;&#22909;&#20869;&#32676;&#20307;&#20215;&#20540;&#35266;&#32780;&#25269;&#21046;&#22806;&#32676;&#20307;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#22312;&#19977;&#31181;&#35199;&#26041;&#35821;&#35328;&#65288;&#21363;&#33521;&#35821;&#12289;&#24503;&#35821;&#21644;&#27861;&#35821;&#65289;&#21644;&#19977;&#31181;&#19996;&#26041;&#35821;&#35328;&#65288;&#21363;&#20013;&#25991;&#12289;&#26085;&#35821;&#21644;&#38889;&#35821;&#65289;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;-&#20010;&#20154;&#20027;&#20041;&#19982;&#38598;&#20307;&#20027;&#20041;&#12290;&#24403;ChatGPT&#22312;&#35199;&#26041;&#35821;&#35328;&#20013;&#37319;&#29992;&#20010;&#20154;&#20027;&#20041;&#20154;&#26684;&#26102;&#65292;&#20854;&#38598;&#20307;&#20027;&#20041;&#35780;&#20998;&#65288;&#21363;&#22806;&#32676;&#20307;&#20215;&#20540;&#35266;&#65289;&#21576;&#29616;&#20986;&#26356;&#20026;&#28040;&#26497;&#30340;&#36235;&#21183;&#65292;&#36229;&#36234;&#20102;&#23545;&#20010;&#20154;&#20027;&#20041;&#65288;&#21363;&#20869;&#32676;&#20307;&#20215;&#20540;&#35266;&#65289;&#30340;&#31215;&#26497;&#21462;&#21521;&#12290;&#30456;&#21453;&#65292;&#24403;&#22312;&#19996;&#26041;&#35821;&#35328;&#20013;&#21521;ChatGPT&#25351;&#23450;&#38598;&#20307;&#20027;&#20041;&#20154;&#26684;&#26102;&#65292;&#20986;&#29616;&#20102;&#31867;&#20284;&#30340;&#27169;&#24335;&#65292;&#23545;&#20010;&#20154;&#20027;&#20041;&#65288;&#21363;&#22806;&#32676;&#20307;&#20215;&#20540;&#35266;&#65289;&#20135;&#29983;&#20102;&#26356;&#20026;&#36127;&#38754;&#30340;&#21453;&#24212;&#65292;&#32780;&#23545;&#38598;&#20307;&#20027;&#20041;&#65288;&#21363;&#20869;&#32676;&#20307;&#20215;&#20540;&#35266;&#65289;&#25345;&#31215;&#26497;&#24577;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#27880;&#20837;&#29305;&#23450;&#31038;&#20250;&#36523;&#20221;&#26102;&#65292;ChatGPT&#33021;&#22815;&#35782;&#21035;&#20869;&#32676;&#20307;&#21644;&#22806;&#32676;&#20307;&#65292;&#25509;&#21463;&#20869;&#32676;&#20307;&#20215;&#20540;&#35266;&#21516;&#26102;&#25682;&#24323;&#22806;&#32676;&#20307;&#20215;&#20540;&#35266;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#22806;&#32676;&#20307;&#30340;&#28040;&#26497;&#24577;&#24230;&#24341;&#21457;&#20102;&#20559;&#35265;&#21644;&#27495;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10436v1 Announce Type: new  Abstract: We explored cultural biases-individualism vs. collectivism-in ChatGPT across three Western languages (i.e., English, German, and French) and three Eastern languages (i.e., Chinese, Japanese, and Korean). When ChatGPT adopted an individualistic persona in Western languages, its collectivism scores (i.e., out-group values) exhibited a more negative trend, surpassing their positive orientation towards individualism (i.e., in-group values). Conversely, when a collectivistic persona was assigned to ChatGPT in Eastern languages, a similar pattern emerged with more negative responses toward individualism (i.e., out-group values) as compared to collectivism (i.e., in-group values). The results indicate that when imbued with a particular social identity, ChatGPT discerns in-group and out-group, embracing in-group values while eschewing out-group values. Notably, the negativity towards the out-group, from which prejudices and discrimination arise,
&lt;/p&gt;</description></item><item><title>&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#26679;&#26412;&#23398;&#20064;&#30334;&#20998;&#27604;&#33258;&#20027;&#36873;&#25321;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#25903;&#25345;&#26356;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#65292;&#23454;&#29616;&#30456;&#23218;&#32654;&#29978;&#33267;&#20248;&#20110;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10430</link><description>&lt;p&gt;
&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20026;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#25351;&#23548;&#35843;&#25972;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10430
&lt;/p&gt;
&lt;p&gt;
&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#26679;&#26412;&#23398;&#20064;&#30334;&#20998;&#27604;&#33258;&#20027;&#36873;&#25321;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#25903;&#25345;&#26356;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#25972;&#65292;&#23454;&#29616;&#30456;&#23218;&#32654;&#29978;&#33267;&#20248;&#20110;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#20351;&#23427;&#20204;&#36866;&#29992;&#20110;&#19968;&#33324;&#29992;&#36884;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#36890;&#24120;&#65292;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#22823;&#37327;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#23548;&#33268;&#39640;&#26114;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#23398;&#20064;&#30334;&#20998;&#27604;&#30340;&#26032;&#39062;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#26029;&#35328;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#33258;&#20027;&#36873;&#25321;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23548;&#33268;&#19982;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#25110;&#26356;&#22909;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28085;&#30422;&#20102;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#25581;&#31034;&#20986;&#36825;&#19968;&#29305;&#24449;&#36866;&#29992;&#20110;&#20174;1B&#65288;&#23567;&#65289;&#21040;13B&#65288;&#22823;&#65289;&#22823;&#23567;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#65292;&#21363;&#25968;&#25454;&#38590;&#24230;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#20043;&#38388;&#20256;&#36882;&#65292;&#24182;&#19988;&#26356;&#23567;&#30340;350M&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#31579;&#36873;&#20986;&#21253;&#21547;&#22256;&#38590;&#26679;&#26412;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#29992;&#20110;&#26356;&#22823;&#30340;13B&#27169;&#22411;&#65292;&#23548;&#33268;&#19968;&#20010;&#19982;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30456;&#27604;&#21516;&#26679;&#25110;&#26356;&#20248;&#31168;&#30340;&#25351;&#23548;&#35843;&#25972;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10430v1 Announce Type: new  Abstract: Instruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to trainin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#26469;&#32479;&#19968;&#35780;&#20272;&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#22312;&#31283;&#23450;&#24615;&#12289;&#21487;&#22609;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#25972;&#20307;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20837;&#19981;&#21516;&#30693;&#35782;&#33976;&#39311;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10427</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#25913;&#36827;&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Improving Continual Learning in Spoken Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10427
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#26469;&#32479;&#19968;&#35780;&#20272;&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#22312;&#31283;&#23450;&#24615;&#12289;&#21487;&#22609;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#25972;&#20307;&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20837;&#19981;&#21516;&#30693;&#35782;&#33976;&#39311;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#21475;&#35821;&#29702;&#35299;&#12290;&#22312;&#21475;&#35821;&#29702;&#35299;&#20013;&#65292;&#20854;&#30446;&#26631;&#26159;&#26377;&#25928;&#22788;&#29702;&#26032;&#27010;&#24565;&#30340;&#20986;&#29616;&#21644;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#12290;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#35780;&#20272;&#36890;&#24120;&#28041;&#21450;&#35780;&#20272;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12289;&#21487;&#22609;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#20316;&#20026;&#26631;&#20934;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#25351;&#26631;&#20027;&#35201;&#38598;&#20013;&#22312;&#20854;&#20013;&#19968;&#20010;&#25110;&#20004;&#20010;&#23646;&#24615;&#19978;&#12290;&#23427;&#20204;&#24573;&#35270;&#20102;&#25972;&#20307;&#34920;&#29616;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#65292;&#24182;&#27809;&#26377;&#20805;&#20998;&#35299;&#24320;&#27169;&#22411;&#20869;&#30340;&#21487;&#22609;&#24615;&#19982;&#31283;&#23450;&#24615;/&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#32479;&#19968;&#35780;&#20272;&#31283;&#23450;&#24615;&#12289;&#21487;&#22609;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#37319;&#29992;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#24341;&#20837;&#21508;&#31181;&#30693;&#35782;&#33976;&#39311;&#26469;&#25913;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10427v1 Announce Type: cross  Abstract: Continual learning has emerged as an increasingly important challenge across various tasks, including Spoken Language Understanding (SLU). In SLU, its objective is to effectively handle the emergence of new concepts and evolving environments. The evaluation of continual learning algorithms typically involves assessing the model's stability, plasticity, and generalizability as fundamental aspects of standards. However, existing continual learning metrics primarily focus on only one or two of the properties. They neglect the overall performance across all tasks, and do not adequately disentangle the plasticity versus stability/generalizability trade-offs within the model. In this work, we propose an evaluation methodology that provides a unified evaluation on stability, plasticity, and generalizability in continual learning. By employing the proposed metric, we demonstrate how introducing various knowledge distillations can improve diffe
&lt;/p&gt;</description></item><item><title>DELL&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;LLMs&#25972;&#21512;&#21040;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#31649;&#36947;&#20013;&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#38395;&#21453;&#24212;&#21644;&#35299;&#37322;&#26469;&#25552;&#21319;&#23545;&#26032;&#38395;&#25991;&#31456;&#30495;&#23454;&#24615;&#30340;&#21028;&#26029;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10426</link><description>&lt;p&gt;
DELL: &#22522;&#20110;LLM&#30340;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#29983;&#25104;&#21453;&#24212;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10426
&lt;/p&gt;
&lt;p&gt;
DELL&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;LLMs&#25972;&#21512;&#21040;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#31649;&#36947;&#20013;&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#38395;&#21453;&#24212;&#21644;&#35299;&#37322;&#26469;&#25552;&#21319;&#23545;&#26032;&#38395;&#25991;&#31456;&#30495;&#23454;&#24615;&#30340;&#21028;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21463;&#20107;&#23454;&#24615;&#21644;&#24187;&#35273;&#26041;&#38754;&#30340;&#25361;&#25112;&#25152;&#38480;&#65292;&#22240;&#27492;&#26080;&#27861;&#30452;&#25509;&#29992;&#20110;&#26032;&#38395;&#25991;&#31456;&#30495;&#23454;&#24615;&#30340;&#21028;&#26029;&#65292;&#32780;&#20107;&#23454;&#20934;&#30830;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DELL&#65292;&#23427;&#30830;&#23450;&#20102;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20013;LLM&#21487;&#20197;&#20316;&#20026;&#31649;&#36947;&#30340;&#19968;&#37096;&#20998;&#30340;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#65306;1&#65289;LLM&#21487;&#20197;&#29983;&#25104;&#26032;&#38395;&#21453;&#24212;&#26469;&#20195;&#34920;&#19981;&#21516;&#35270;&#35282;&#24182;&#27169;&#25311;&#29992;&#25143;-&#26032;&#38395;&#20132;&#20114;&#32593;&#32476;&#65307;2&#65289;LLM&#21487;&#20197;&#20026;&#20195;&#29702;&#20219;&#21153;&#65288;&#22914;&#24773;&#24863;&#12289;&#31435;&#22330;&#65289;&#29983;&#25104;&#35299;&#37322;&#65292;&#20197;&#20016;&#23500;&#26032;&#38395;&#25991;&#31456;&#30340;&#32972;&#26223;&#24182;&#20135;&#29983;&#19987;&#38376;&#30740;&#31350;&#26032;&#38395;&#19981;&#21516;&#26041;&#38754;&#30340;&#19987;&#23478;&#65307;3&#65289;LLM&#21487;&#20197;&#21512;&#24182;&#20219;&#21153;&#29305;&#23450;&#30340;&#19987;&#23478;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#19987;&#23478;&#30340;&#39044;&#27979;&#21644;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#25552;&#20379;&#25972;&#20307;&#39044;&#27979;&#12290;&#23545;&#19971;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;DELL&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10426v1 Announce Type: new  Abstract: Large language models are limited by challenges in factuality and hallucinations to be directly employed off-the-shelf for judging the veracity of news articles, where factual accuracy is paramount. In this work, we propose DELL that identifies three key stages in misinformation detection where LLMs could be incorporated as part of the pipeline: 1) LLMs could \emph{generate news reactions} to represent diverse perspectives and simulate user-news interaction networks; 2) LLMs could \emph{generate explanations} for proxy tasks (e.g., sentiment, stance) to enrich the contexts of news articles and produce experts specializing in various aspects of news understanding; 3) LLMs could \emph{merge task-specific experts} and provide an overall prediction by incorporating the predictions and confidence scores of varying experts. Extensive experiments on seven datasets with three LLMs demonstrate that DELL outperforms state-of-the-art baselines by u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#40520;&#40533;&#27748;&#26694;&#26550;&#65292;&#21253;&#25324;&#24120;&#35782;&#30693;&#35782;&#24211;&#12289;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#20197;&#21450;&#24847;&#20041;&#20851;&#32852;&#30340;&#27010;&#24565;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;$O(1/T)$&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25439;&#22833;&#30028;&#38480;&#65292;&#33021;&#22815;&#35299;&#37322;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10424</link><description>&lt;p&gt;
&#20351;&#29992;&#40520;&#40533;&#27748;&#26694;&#26550;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding In-Context Learning with a Pelican Soup Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#40520;&#40533;&#27748;&#26694;&#26550;&#65292;&#21253;&#25324;&#24120;&#35782;&#30693;&#35782;&#24211;&#12289;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#20197;&#21450;&#24847;&#20041;&#20851;&#32852;&#30340;&#27010;&#24565;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;$O(1/T)$&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25439;&#22833;&#30028;&#38480;&#65292;&#33021;&#22815;&#35299;&#37322;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#29702;&#35770;&#20998;&#26512;&#26159;&#22522;&#20110;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#65292;&#23427;&#20204;&#23384;&#22312;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#21363;&#40520;&#40533;&#27748;&#26694;&#26550;&#65292;&#26469;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#65288;1&#65289;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#27010;&#24565;&#65292;&#65288;2&#65289;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#33324;&#24418;&#24335;&#21270;&#65292;&#20197;&#21450;&#65288;3&#65289;&#24847;&#20041;&#20851;&#32852;&#30340;&#27010;&#24565;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#24314;&#31435;&#19968;&#20010;$\mathcal{O}(1/T)$&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25439;&#22833;&#30028;&#38480;&#65292;&#36825;&#37324;$T$&#26159;&#28436;&#31034;&#20013;&#31034;&#20363;-&#26631;&#31614;&#23545;&#30340;&#25968;&#37327;&#12290;&#19982;&#20808;&#21069;&#30340;&#20316;&#21697;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#21453;&#26144;&#20102;&#21160;&#35789;&#36873;&#25321;&#21644;&#25351;&#20196;&#35843;&#25972;&#30340;&#24433;&#21709;&#12290;&#19968;&#20010;&#39069;&#22806;&#30340;"&#21407;&#23376;&#27010;&#24565;"&#27010;&#24565;&#20351;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35299;&#37322;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29609;&#20855;&#35774;&#32622;&#65292;Calcutec&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10424v1 Announce Type: cross  Abstract: Many existing theoretical analyses of in-context learning for natural language processing are based on latent variable models that leaves gaps between theory and practice. We aim to close these gaps by proposing a theoretical framework, the Pelican Soup Framework. In this framework, we introduce (1) the notion of a common sense knowledge base, (2) a general formalism for natural language classification tasks, and the notion of (3) meaning association. Under this framework, we can establish a $\mathcal{O}(1/T)$ loss bound for in-context learning, where $T$ is the number of example-label pairs in the demonstration. Compared with previous works, our bound reflects the effect of the choice of verbalizers and the effect of instruction tuning. An additional notion of \textit{atom concepts} makes our framework possible to explain the generalization to tasks unseen in the language model training data. Finally, we propose a toy setup, Calcutec,
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;ZeroSwot&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38646;-shot ST&#65292;&#36890;&#36807;CTC&#21387;&#32553;&#21644;&#26368;&#20248;&#20256;&#36755;&#65292;&#20165;&#21033;&#29992;ASR&#25968;&#25454;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#24182;&#19982;&#22810;&#35821;&#35328;MT&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#26080;&#32541;&#38598;&#25104;&#65292;&#23454;&#29616;&#30452;&#25509;&#20174;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2402.10422</link><description>&lt;p&gt;
&#25512;&#21160;&#38646;-shot&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Pushing the Limits of Zero-shot End-to-End Speech Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10422
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;ZeroSwot&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38646;-shot ST&#65292;&#36890;&#36807;CTC&#21387;&#32553;&#21644;&#26368;&#20248;&#20256;&#36755;&#65292;&#20165;&#21033;&#29992;ASR&#25968;&#25454;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#24182;&#19982;&#22810;&#35821;&#35328;MT&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#26080;&#32541;&#38598;&#25104;&#65292;&#23454;&#29616;&#30452;&#25509;&#20174;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#32570;&#21644;&#35821;&#38899;&#19982;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#26159;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#31995;&#32479;&#30340;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#20854;&#24615;&#33021;&#12290; &#20197;&#24448;&#30340;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;MT&#25968;&#25454;&#21644;&#20248;&#21270;&#36317;&#31163;&#24230;&#37327;&#26469;&#20943;&#36731;&#36825;&#20123;&#25361;&#25112;&#65292;&#20174;&#32780;&#20351;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#26356;&#21152;&#25509;&#36817;&#12290; &#28982;&#32780;&#65292;&#36890;&#24120;&#38656;&#35201;&#19968;&#20123;ST&#25968;&#25454;&#25165;&#33021;&#33719;&#24471;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290; &#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ZeroSwot&#65292;&#36825;&#26159;&#19968;&#31181;&#38646;-shot ST&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#37197;&#23545;&#30340;ST&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24357;&#21512;&#27169;&#24577;&#24046;&#36317;&#12290; &#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;CTC&#21387;&#32553;&#21644;&#26368;&#20248;&#20256;&#36755;&#25216;&#26415;&#65292;&#25105;&#20204;&#21482;&#20351;&#29992;ASR&#25968;&#25454;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#20197;&#19982;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;MT&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290; &#35821;&#38899;&#32534;&#30721;&#22120;&#22312;&#25512;&#26029;&#26102;&#19982;MT&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#65292;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#22312;&#25152;&#26377;MT&#27169;&#22411;&#25903;&#25345;&#30340;&#35821;&#35328;&#20013;&#20174;&#35821;&#38899;&#32763;&#35793;&#20026;&#25991;&#26412;&#12290; &#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#24179;&#28369;&#22320;&#20851;&#38381;m&#27169;&#24577;&#38388;&#30340;&#31354;&#38388;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10422v1 Announce Type: new  Abstract: Data scarcity and the modality gap between the speech and text modalities are two major obstacles of end-to-end Speech Translation (ST) systems, thus hindering their performance. Prior work has attempted to mitigate these challenges by leveraging external MT data and optimizing distance metrics that bring closer the speech-text representations. However, achieving competitive results typically requires some ST data. For this reason, we introduce ZeroSwot, a method for zero-shot ST that bridges the modality gap without any paired ST data. Leveraging a novel CTC compression and Optimal Transport, we train a speech encoder using only ASR data, to align with the representation space of a massively multilingual MT model. The speech encoder seamlessly integrates with the MT model at inference, enabling direct translation from speech to text, across all languages supported by the MT model. Our experiments show that we can effectively close the m
&lt;/p&gt;</description></item><item><title>&#35821;&#20041;&#22522;&#30784;&#32622;&#20110;&#36125;&#21494;&#26031;&#24515;&#28789;&#29702;&#35770;&#20013;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#20204;&#20849;&#21516;&#25512;&#26029;&#20986;&#35299;&#37322;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#19968;&#33268;&#24615;&#30446;&#26631;&#12289;&#20449;&#24565;&#21644;&#35745;&#21010;&#38598;&#21512;&#65292;&#20877;&#36890;&#36807;&#35748;&#35782;&#36923;&#36753;&#35780;&#20272;&#26377;&#20851;&#20195;&#29702;&#20154;&#20449;&#24565;&#30340;&#38472;&#36848;&#65292;&#35299;&#37322;&#20102;&#20154;&#31867;&#20449;&#24565;&#24402;&#22240;&#30340;&#20998;&#32423;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#20197;&#21450;&#20854;&#19982;&#30446;&#26631;&#21644;&#35745;&#21010;&#30340;&#23494;&#20999;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.10416</link><description>&lt;p&gt;
&#23558;&#20851;&#20110;&#20449;&#24565;&#30340;&#35821;&#35328;&#25509;&#22320;&#20110;&#36125;&#21494;&#26031;&#24515;&#28789;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Grounding Language about Belief in a Bayesian Theory-of-Mind
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10416
&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22522;&#30784;&#32622;&#20110;&#36125;&#21494;&#26031;&#24515;&#28789;&#29702;&#35770;&#20013;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#20204;&#20849;&#21516;&#25512;&#26029;&#20986;&#35299;&#37322;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#19968;&#33268;&#24615;&#30446;&#26631;&#12289;&#20449;&#24565;&#21644;&#35745;&#21010;&#38598;&#21512;&#65292;&#20877;&#36890;&#36807;&#35748;&#35782;&#36923;&#36753;&#35780;&#20272;&#26377;&#20851;&#20195;&#29702;&#20154;&#20449;&#24565;&#30340;&#38472;&#36848;&#65292;&#35299;&#37322;&#20102;&#20154;&#31867;&#20449;&#24565;&#24402;&#22240;&#30340;&#20998;&#32423;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#20197;&#21450;&#20854;&#19982;&#30446;&#26631;&#21644;&#35745;&#21010;&#30340;&#23494;&#20999;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20449;&#24565;&#26159;&#26080;&#27861;&#30452;&#25509;&#35266;&#23519;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#20154;&#31867;&#24120;&#24120;&#20351;&#29992;&#20016;&#23500;&#30340;&#32452;&#21512;&#35821;&#35328;&#26469;&#25551;&#36848;&#20182;&#20154;&#30340;&#24819;&#27861;&#21644;&#30693;&#35782;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23558;&#20449;&#24565;&#38472;&#36848;&#30340;&#35821;&#20041;&#22522;&#30784;&#32622;&#20110;&#36125;&#21494;&#26031;&#24515;&#28789;&#29702;&#35770;&#20013;&#65292;&#20026;&#35299;&#37322;&#20154;&#31867;&#22914;&#20309;&#35299;&#37322;&#20182;&#20154;&#38544;&#34255;&#30340;&#35748;&#35782;&#20869;&#23481;&#36808;&#20986;&#20102;&#19968;&#27493;&#65306;&#36890;&#36807;&#24314;&#27169;&#20154;&#31867;&#22914;&#20309;&#20849;&#21516;&#25512;&#26029;&#20986;&#35299;&#37322;&#19968;&#20010;&#20195;&#29702;&#20154;&#34892;&#21160;&#30340;&#19968;&#33268;&#24615;&#30446;&#26631;&#12289;&#20449;&#24565;&#21644;&#35745;&#21010;&#38598;&#21512;&#65292;&#28982;&#21518;&#36890;&#36807;&#35748;&#35782;&#36923;&#36753;&#23545;&#26377;&#20851;&#20195;&#29702;&#20154;&#20449;&#24565;&#30340;&#38472;&#36848;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#20449;&#24565;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#35282;&#33394;&#35821;&#20041;&#65292;&#35299;&#37322;&#20102;&#20154;&#31867;&#20449;&#24565;&#24402;&#22240;&#30340;&#20998;&#32423;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#30446;&#26631;&#21644;&#35745;&#21010;&#30340;&#23494;&#20999;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20154;&#20204;&#22312;&#35266;&#23519;&#19968;&#20010;&#20195;&#29702;&#20154;&#35299;&#20915;&#38382;&#39064;&#26102;&#26159;&#22914;&#20309;&#24402;&#22240;&#30446;&#26631;&#21644;&#20449;&#24565;&#30340;&#26469;&#35780;&#20272;&#36825;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10416v1 Announce Type: new  Abstract: Despite the fact that beliefs are mental states that cannot be directly observed, humans talk about each others' beliefs on a regular basis, often using rich compositional language to describe what others think and know. What explains this capacity to interpret the hidden epistemic content of other minds? In this paper, we take a step towards an answer by grounding the semantics of belief statements in a Bayesian theory-of-mind: By modeling how humans jointly infer coherent sets of goals, beliefs, and plans that explain an agent's actions, then evaluating statements about the agent's beliefs against these inferences via epistemic logic, our framework provides a conceptual role semantics for belief, explaining the gradedness and compositionality of human belief attributions, as well as their intimate connection with goals and plans. We evaluate this framework by studying how humans attribute goals and beliefs while watching an agent solve
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEWL&#30340;&#24187;&#35273;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;LLM&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#35780;&#20272;&#20107;&#23454;&#24615;&#65292;&#36866;&#29992;&#20110;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.10412</link><description>&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#21152;&#26435;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;LLM&#22312;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#30340;&#34394;&#26500;
&lt;/p&gt;
&lt;p&gt;
Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10412
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEWL&#30340;&#24187;&#35273;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;LLM&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#35780;&#20272;&#20107;&#23454;&#24615;&#65292;&#36866;&#29992;&#20110;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#20107;&#23454;&#19981;&#27491;&#30830;&#20294;&#30475;&#20284;&#20196;&#20154;&#20449;&#26381;&#30340;&#31572;&#26696;&#65292;&#30446;&#21069;&#26159;LLM&#21487;&#20449;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#20027;&#35201;&#23041;&#32961;&#12290;&#35299;&#20915;&#36825;&#19968;&#22797;&#26434;&#38382;&#39064;&#30340;&#31532;&#19968;&#27493;&#26159;&#23545;&#20854;&#36827;&#34892;&#34913;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24187;&#35273;&#24230;&#37327;&#26631;&#20934;&#38656;&#35201;&#20855;&#26377;&#20855;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21363;&#20154;&#31867;&#32534;&#20889;&#30340;&#8220;&#26368;&#20339;&#8221;&#25110;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#12290;&#36825;&#31181;&#35201;&#27714;&#20351;&#24187;&#35273;&#27979;&#37327;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#35823;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#21152;&#26435;LLM&#23545;&#20107;&#23454;&#24615;&#36827;&#34892;&#35780;&#20272;&#65288;FEWL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#37329;&#26631;&#20934;&#31572;&#26696;&#32570;&#22833;&#26102;&#35774;&#35745;&#30340;&#24187;&#35273;&#24230;&#37327;&#26631;&#20934;&#12290;FEWL&#21033;&#29992;&#20102;&#29616;&#25104;&#30340;LLM&#31572;&#26696;&#20316;&#20026;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#20195;&#29702;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#37327;&#21270;&#21442;&#32771;LLM&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;FEWL&#20855;&#26377;&#19968;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#35777;&#20013;&#35777;&#26126;&#23427;&#26356;&#20934;&#30830;&#12290;&#24230;&#37327;&#34394;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10412v1 Announce Type: cross  Abstract: LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require to have a benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers written by humans. Such requirement makes hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), the first hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accur
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22270;&#32467;&#26500;&#20449;&#24687;&#22312;&#20849;&#31867;&#21035;&#22270;&#19978;&#21033;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;LLMs&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#21644;&#38646;-shot/few-shot&#20998;&#31867;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24369;&#26631;&#31614;&#24494;&#35843;LLMs&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10409</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#34920;&#31034;&#23398;&#20064;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#35770;&#25991;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10409
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#32467;&#26500;&#20449;&#24687;&#22312;&#20849;&#31867;&#21035;&#22270;&#19978;&#21033;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;LLMs&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#21644;&#38646;-shot/few-shot&#20998;&#31867;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24369;&#26631;&#31614;&#24494;&#35843;LLMs&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#30740;&#31350;&#25345;&#32493;&#36827;&#34892;&#65292;&#38590;&#20197;&#36319;&#19978;&#26032;&#30340;&#30740;&#31350;&#21644;&#27169;&#22411;&#12290;&#20026;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#32508;&#21512;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#35768;&#22810;&#20154;&#20889;&#20102;&#35843;&#30740;&#35770;&#25991;&#65292;&#20294;&#21363;&#20351;&#36825;&#20123;&#35770;&#25991;&#20063;&#21464;&#24471;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23558;&#35843;&#30740;&#35770;&#25991;&#20998;&#37197;&#21040;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;144&#31687;LLM&#35843;&#30740;&#35770;&#25991;&#30340;&#20803;&#25968;&#25454;&#65292;&#24182;&#25506;&#35752;&#20102;&#19977;&#31181;&#33539;&#20363;&#26469;&#23545;&#20998;&#31867;&#27861;&#20869;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#20849;&#31867;&#21035;&#22270;&#19978;&#21033;&#29992;&#22270;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20004;&#20010;&#33539;&#20363;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;; &#20351;&#29992;LLMs&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#38646;-shot/few-shot&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#27169;&#22411;&#36229;&#36807;&#20102;&#24179;&#22343;&#20154;&#31867;&#35782;&#21035;&#27700;&#24179;&#65292;&#24182;&#19988;&#21033;&#29992;&#36739;&#23567;&#27169;&#22411;&#29983;&#25104;&#30340;&#24369;&#26631;&#31614;&#26469;&#24494;&#35843;LLMs&#65288;&#26412;&#30740;&#31350;&#20013;&#30340;GCN&#31561;&#65289;&#21487;&#33021;&#27604;&#20351;&#29992;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#26356;&#26377;&#25928;&#65292;&#25581;&#31034;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10409v1 Announce Type: cross  Abstract: As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-stro
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36923;&#36753;&#38142;&#65292;&#36890;&#36807;&#20998;&#35299;&#21644;&#37325;&#26032;&#32452;&#21512;&#26469;&#20419;&#36827;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#21463;&#21040;&#24459;&#24072;&#20351;&#29992;&#30340;&#24207;&#36143;&#25512;&#29702;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.10400</link><description>&lt;p&gt;
&#36923;&#36753;&#38142;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Chain of Logic: Rule-Based Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10400
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36923;&#36753;&#38142;&#65292;&#36890;&#36807;&#20998;&#35299;&#21644;&#37325;&#26032;&#32452;&#21512;&#26469;&#20419;&#36827;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#21463;&#21040;&#24459;&#24072;&#20351;&#29992;&#30340;&#24207;&#36143;&#25512;&#29702;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#27861;&#24459;&#25512;&#29702;&#31867;&#22411;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#20934;&#30830;&#22320;&#23558;&#35268;&#21017;&#24212;&#29992;&#20110;&#19968;&#32452;&#20107;&#23454;&#26469;&#24471;&#20986;&#32467;&#35770;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#32773;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#32452;&#21512;&#35268;&#21017; - &#30001;&#22810;&#20010;&#20803;&#32032;&#32452;&#25104;&#24418;&#25104;&#22797;&#26434;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#35268;&#21017;&#12290;&#25512;&#29702;&#32452;&#21512;&#35268;&#21017;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22810;&#20010;&#25512;&#29702;&#27493;&#39588;&#65292;&#24182;&#19988;&#38656;&#35201;&#20851;&#27880;&#20803;&#32032;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36923;&#36753;&#38142;&#65292;&#36890;&#36807;&#20998;&#35299;&#65288;&#23558;&#20803;&#32032;&#20316;&#20026;&#29420;&#31435;&#30340;&#36923;&#36753;&#32447;&#32034;&#35299;&#20915;&#65289;&#21644;&#37325;&#26032;&#32452;&#21512;&#65288;&#37325;&#26032;&#32452;&#21512;&#36825;&#20123;&#23376;&#31572;&#26696;&#20197;&#35299;&#20915;&#28508;&#22312;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#21463;&#21040;&#20102;IRAC&#65288;&#38382;&#39064;&#12289;&#35268;&#21017;&#12289;&#24212;&#29992;&#12289;&#32467;&#35770;&#65289;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#24459;&#24072;&#20351;&#29992;&#30340;&#19968;&#31181;&#24207;&#36143;&#25512;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#36923;&#36753;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10400v1 Announce Type: new  Abstract: Rule-based reasoning, a fundamental type of legal reasoning, enables us to draw conclusions by accurately applying a rule to a set of facts. We explore causal language models as rule-based reasoners, specifically with respect to compositional rules - rules consisting of multiple elements which form a complex logical expression. Reasoning about compositional rules is challenging because it requires multiple reasoning steps, and attending to the logical relationships between elements. We introduce a new prompting method, Chain of Logic, which elicits rule-based reasoning through decomposition (solving elements as independent threads of logic), and recomposition (recombining these sub-answers to resolve the underlying logical expression). This method was inspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers. We evaluate chain of logic across eight rule-based reasoning tasks in
&lt;/p&gt;</description></item><item><title>DataDreamer&#26159;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#22797;&#29616;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#23454;&#29616;&#24378;&#22823;&#30340;LLM&#24037;&#20316;&#27969;&#65292;&#25552;&#20513;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10379</link><description>&lt;p&gt;
DataDreamer: &#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#22797;&#29616;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10379
&lt;/p&gt;
&lt;p&gt;
DataDreamer&#26159;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#22797;&#29616;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#23454;&#29616;&#24378;&#22823;&#30340;LLM&#24037;&#20316;&#27969;&#65292;&#25552;&#20513;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#22914;&#20170;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12289;&#20219;&#21153;&#35780;&#20272;&#12289;&#24494;&#35843;&#12289;&#25552;&#28860;&#20197;&#21450;&#20854;&#20182;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#30740;&#31350;&#24037;&#20316;&#27969;&#20013;&#20351;&#29992;LLMs&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#23427;&#20204;&#30340;&#35268;&#27169;&#12289;&#38381;&#28304;&#24615;&#36136;&#20197;&#21450;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26032;&#20852;&#24037;&#20316;&#27969;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#36805;&#36895;&#23835;&#36215;&#21644;&#36825;&#20123;&#29420;&#29305;&#25361;&#25112;&#23545;&#24320;&#25918;&#31185;&#23398;&#21644;&#20351;&#29992;&#23427;&#20204;&#30340;&#24037;&#20316;&#30340;&#21487;&#37325;&#29616;&#24615;&#20135;&#29983;&#20102;&#30452;&#25509;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataDreamer&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#32534;&#20889;&#31616;&#21333;&#30340;&#20195;&#30721;&#26469;&#23454;&#29616;&#24378;&#22823;&#30340;LLM&#24037;&#20316;&#27969;&#12290;DataDreamer&#36824;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#36981;&#24490;&#25105;&#20204;&#25552;&#20986;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#40723;&#21169;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;&#35813;&#24211;&#21644;&#25991;&#26723;&#21487;&#22312;h&#32593;&#31449;&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10379v1 Announce Type: new  Abstract: Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at h
&lt;/p&gt;</description></item><item><title>BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.10373</link><description>&lt;p&gt;
BioMistral&#65306;&#38754;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10373
&lt;/p&gt;
&lt;p&gt;
BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#21644;&#21307;&#23398;&#31561;&#19987;&#19994;&#39046;&#22495;&#25552;&#20379;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#38024;&#23545;&#20581;&#24247;&#39046;&#22495;&#23450;&#21046;&#30340;&#24320;&#28304;LLMs&#21487;&#29992;&#65292;&#20294;&#23558;&#36890;&#29992;LLMs&#35843;&#25972;&#21040;&#21307;&#23398;&#39046;&#22495;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioMistral&#65292;&#19968;&#31181;&#19987;&#20026;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#28304;LLM&#65292;&#37319;&#29992;Mistral&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;PubMed Central&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;10&#20010;&#24050;&#24314;&#31435;&#30340;&#33521;&#25991;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#19978;&#23545;BioMistral&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#36890;&#36807;&#37327;&#21270;&#21644;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#33719;&#24471;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BioMistral&#30456;&#36739;&#20110;&#29616;&#26377;&#24320;&#28304;&#21307;&#23398;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#19982;&#19987;&#26377;&#23545;&#25163;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10373v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;GraphPrompter&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#23558;&#22270;&#20449;&#24687;&#19982;LLMs&#23545;&#40784;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#29702;&#35299;&#22270;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10359</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#29992;&#36719;&#25552;&#31034;LLMs&#26469;&#36827;&#34892;&#22270;&#23398;&#20064;&#20219;&#21153;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we soft prompt LLMs for graph learning tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10359
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;GraphPrompter&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#23558;&#22270;&#20449;&#24687;&#19982;LLMs&#23545;&#40784;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#29702;&#35299;&#22270;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#34920;&#31034;&#31038;&#20132;&#32593;&#32476;&#12289;&#29983;&#29289;&#25968;&#25454;&#21644;&#24341;&#29992;&#32593;&#32476;&#31561;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#36825;&#20351;&#24471;&#23558;LLMs&#24212;&#29992;&#20110;&#22270;&#34920;&#26684;&#23588;&#20026;&#35825;&#20154;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;LLMs&#24212;&#29992;&#20110;&#22270;&#34920;&#26684;&#24418;&#24335;&#23384;&#22312;&#29420;&#29305;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#34920;&#26684;&#24418;&#24335;&#19982;&#25991;&#26412;&#24418;&#24335;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#21644;&#19981;&#21305;&#37197;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#29702;&#35299;&#22270;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GraphPrompter&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#36719;&#25552;&#31034;&#26469;&#23558;&#22270;&#20449;&#24687;&#19982;LLMs&#23545;&#40784;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GraphPrompter&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32534;&#30721;&#22797;&#26434;&#30340;&#22270;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#25991;&#26412;&#20449;&#24687;&#30340;LLM&#12290;&#22312;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10359v1 Announce Type: cross  Abstract: Graph plays an important role in representing complex relationships in real-world applications such as social networks, biological data and citation networks. In recent years, Large Language Models (LLMs) have achieved tremendous success in various domains, which makes applying LLMs to graphs particularly appealing. However, directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities. Hence, to further investigate LLMs' potential for comprehending graph information, we introduce GraphPrompter, a novel framework designed to align graph information with LLMs via soft prompts. Specifically, GraphPrompter consists of two main components: a graph neural network to encode complex graph information and an LLM that effectively processes textual information. Comprehensive experiments on various benchmark datasets under node classification and link prediction tas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#36755;&#20837;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22266;&#26377;&#20559;&#24046;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10353</link><description>&lt;p&gt;
&#25552;&#21319;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20559;&#24046;&#26657;&#20934;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#36755;&#20837;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22266;&#26377;&#20559;&#24046;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#22266;&#26377;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#36755;&#20837;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#19982;&#20197;&#24448;&#20027;&#35201;&#33268;&#21147;&#20110;&#31038;&#20250;&#20844;&#24179;&#30340;&#22266;&#26377;&#20559;&#24046;&#20462;&#27491;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#24378;&#35843;&#22266;&#26377;&#20559;&#24046;&#26657;&#20934;&#30340;&#25928;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;GPT-4&#29983;&#25104;&#30340;&#19968;&#32452;&#33258;&#21160;&#36873;&#21462;&#30340;&#26080;&#24847;&#20041;&#36755;&#20837;&#26469;&#25552;&#31034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#25506;&#27979;&#22266;&#26377;&#20559;&#24046;&#12290;&#21033;&#29992;&#20559;&#24046;&#21453;&#26144;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24046;&#24322;&#25439;&#22833;&#29992;&#20110;&#20559;&#24046;&#26657;&#20934;&#65292;&#20854;&#20013;&#25105;&#20204;&#20165;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#24046;&#21442;&#25968;&#65288;&#24635;&#21442;&#25968;&#30340;0.1%&#65289;&#20197;&#26397;&#21521;&#30456;&#31561;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10353v1 Announce Type: new  Abstract: Prompt learning is susceptible to intrinsic bias present in pre-trained language models (LMs), resulting in sub-optimal performance of prompt-based zero/few-shot learning. In this work, we propose a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior efforts that address intrinsic bias primarily for social fairness and often involve excessive computational cost, our objective is to explore enhancing LMs' performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from GPT-4 to prompt pre-trained LMs for intrinsic bias probing. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters ($0.1\%$ of total parameters) of LMs towards equal probabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#22312;&#21517;&#35789;&#30701;&#35821;&#20013;&#30340;&#20914;&#31361;&#65292;&#32467;&#35770;&#26174;&#31034;&#24403;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#19988;&#21333;&#35789;&#36739;&#30701;&#26102;&#65292;&#24847;&#22806;&#20943;&#23569;&#21487;&#33021;&#20250;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10311</link><description>&lt;p&gt;
&#21517;&#35789;&#30701;&#35821;&#20013;&#22836;&#37096;&#30340;&#26368;&#20339;&#20301;&#32622;&#12290;&#25351;&#31034;&#35821;&#12289;&#25968;&#35789;&#12289;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#30340;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#22312;&#21517;&#35789;&#30701;&#35821;&#20013;&#30340;&#20914;&#31361;&#65292;&#32467;&#35770;&#26174;&#31034;&#24403;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#19988;&#21333;&#35789;&#36739;&#30701;&#26102;&#65292;&#24847;&#22806;&#20943;&#23569;&#21487;&#33021;&#20250;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21477;&#35805;&#30340;&#35789;&#24207;&#30001;&#22810;&#31181;&#21407;&#21017;&#22609;&#36896;&#12290;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#21407;&#21017;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#65288;&#25110;&#21487;&#39044;&#27979;&#24615;&#26368;&#22823;&#21270;&#65289;&#22312;&#21333;&#19968;&#22836;&#37096;&#30340;&#21477;&#27861;&#20381;&#36182;&#32467;&#26500;&#20013;&#23384;&#22312;&#20914;&#31361;&#65306;&#21069;&#32773;&#39044;&#27979;&#22836;&#37096;&#24212;&#35813;&#25918;&#32622;&#22312;&#32447;&#24615;&#25490;&#21015;&#30340;&#20013;&#24515;&#65292;&#21518;&#32773;&#39044;&#27979;&#22836;&#37096;&#24212;&#35813;&#25918;&#32622;&#22312;&#20004;&#31471;&#20043;&#19968;&#65288;&#35201;&#20040;&#22312;&#39318;&#20301;&#65292;&#35201;&#20040;&#22312;&#26411;&#20301;&#65289;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20309;&#26102;&#24847;&#22806;&#20943;&#23569;&#65288;&#25110;&#21487;&#39044;&#27979;&#24615;&#26368;&#22823;&#21270;&#65289;&#24212;&#35813;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#22312;&#21333;&#19968;&#22836;&#37096;&#32467;&#26500;&#30340;&#32972;&#26223;&#19979;&#65292;&#39044;&#27979;&#22312;&#28385;&#36275;&#20004;&#20010;&#26465;&#20214;&#26102;&#26356;&#26377;&#21487;&#33021;&#21457;&#29983;&#65292;&#21363;&#65288;a&#65289;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#65292;&#24182;&#19988;&#65288;b&#65289;&#21333;&#35789;&#36739;&#30701;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#30001;&#25351;&#31034;&#35821;&#12289;&#25968;&#35789;&#12289;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#32452;&#25104;&#30340;&#21517;&#35789;&#30701;&#35821;&#19978;&#27979;&#35797;&#20102;&#36825;&#19968;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39318;&#36873;&#39034;&#24207;&#20013;...&#65288;&#32570;&#22833;&#37096;&#20998;&#26080;&#27861;&#25552;&#20379;&#23436;&#25972;&#32763;&#35793;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10311v1 Announce Type: new  Abstract: The word order of a sentence is shaped by multiple principles. The principle of syntactic dependency distance minimization is in conflict with the principle of surprisal minimization (or predictability maximization) in single head syntactic dependency structures: while the former predicts that the head should be placed at the center of the linear arrangement, the latter predicts that the head should be placed at one of the ends (either first or last). A critical question is when surprisal minimization (or predictability maximization) should surpass syntactic dependency distance minimization. In the context of single head structures, it has been predicted that this is more likely to happen when two conditions are met, i.e. (a) fewer words are involved and (b) words are shorter. Here we test the prediction on the noun phrase when its composed of a demonstrative, a numeral, an adjective and a noun. We find that, across preferred orders in l
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#26032;&#38395;&#25968;&#25454;&#38598;&#20013;&#30340;&#32858;&#31867;&#23646;&#24615;&#65292;&#21487;&#20197;&#24378;&#30456;&#20851;&#22320;&#35782;&#21035;&#20986;&#26032;&#38395;&#30340;&#37325;&#35201;&#24615;&#21644;&#32039;&#24613;&#24615;&#65292;&#20026;&#35782;&#21035;&#37325;&#35201;&#32039;&#24613;&#26032;&#38395;&#25110;&#36807;&#28388;&#19981;&#37325;&#35201;&#25991;&#31456;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10302</link><description>&lt;p&gt;
&#22914;&#20309;&#35782;&#21035;&#37325;&#35201;&#32039;&#24613;&#26032;&#38395;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Discern Important Urgent News?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10302
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#26032;&#38395;&#25968;&#25454;&#38598;&#20013;&#30340;&#32858;&#31867;&#23646;&#24615;&#65292;&#21487;&#20197;&#24378;&#30456;&#20851;&#22320;&#35782;&#21035;&#20986;&#26032;&#38395;&#30340;&#37325;&#35201;&#24615;&#21644;&#32039;&#24613;&#24615;&#65292;&#20026;&#35782;&#21035;&#37325;&#35201;&#32039;&#24613;&#26032;&#38395;&#25110;&#36807;&#28388;&#19981;&#37325;&#35201;&#25991;&#31456;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#22312;&#26032;&#38395;&#30340;&#32858;&#31867;&#25968;&#25454;&#38598;&#20013;&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#23646;&#24615;&#19982;&#30001;LLM&#35780;&#20272;&#30340;&#26032;&#38395;&#30340;&#37325;&#35201;&#24615;&#21644;&#32039;&#24613;&#24615;&#65288;IUN&#65289;&#24378;&#30456;&#20851;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#22312;&#19981;&#21516;&#30340;&#26032;&#38395;&#25968;&#25454;&#38598;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#32858;&#31867;&#31639;&#27861;&#21644;&#23884;&#20837;&#19978;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#30340;&#30456;&#20851;&#24615;&#24212;&#35813;&#20801;&#35768;&#20351;&#29992;&#32858;&#31867;&#65288;&#20316;&#20026;LLM&#30340;&#26367;&#20195;&#65289;&#26469;&#35782;&#21035;&#26368;&#37325;&#35201;&#30340;&#32039;&#24613;&#26032;&#38395;&#65292;&#25110;&#32773;&#29992;&#20110;&#36807;&#28388;&#19981;&#37325;&#35201;&#30340;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10302v1 Announce Type: new  Abstract: We found that a simple property of clusters in a clustered dataset of news correlate strongly with importance and urgency of news (IUN) as assessed by LLM. We verified our finding across different news datasets, dataset sizes, clustering algorithms and embeddings. The found correlation should allow using clustering (as an alternative to LLM) for identifying the most important urgent news, or for filtering out unimportant articles.
&lt;/p&gt;</description></item><item><title>LAVE&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25552;&#20379;LLM&#21160;&#21147;&#30340;&#20195;&#29702;&#36741;&#21161;&#21644;&#35821;&#35328;&#22686;&#24378;&#32534;&#36753;&#21151;&#33021;&#65292;&#20943;&#23569;&#35270;&#39057;&#32534;&#36753;&#30340;&#38556;&#30861;&#65292;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#32534;&#36753;&#30446;&#26631;</title><link>https://arxiv.org/abs/2402.10294</link><description>&lt;p&gt;
LAVE&#65306;&#20197;LLM&#20026;&#21160;&#21147;&#30340;&#35270;&#39057;&#32534;&#36753;&#20195;&#29702;&#36741;&#21161;&#21644;&#35821;&#35328;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10294
&lt;/p&gt;
&lt;p&gt;
LAVE&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25552;&#20379;LLM&#21160;&#21147;&#30340;&#20195;&#29702;&#36741;&#21161;&#21644;&#35821;&#35328;&#22686;&#24378;&#32534;&#36753;&#21151;&#33021;&#65292;&#20943;&#23569;&#35270;&#39057;&#32534;&#36753;&#30340;&#38556;&#30861;&#65292;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#32534;&#36753;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#21046;&#20316;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#32534;&#36753;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#21162;&#21147;&#24120;&#24120;&#23545;&#21021;&#23398;&#32773;&#26500;&#25104;&#38556;&#30861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#35270;&#39057;&#32534;&#36753;&#24037;&#20316;&#27969;&#31243;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20943;&#23569;&#36825;&#20123;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#29702;&#24565;&#20307;&#29616;&#22312;LAVE&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#25552;&#20379;LLM&#21160;&#21147;&#30340;&#20195;&#29702;&#36741;&#21161;&#21644;&#35821;&#35328;&#22686;&#24378;&#32534;&#36753;&#21151;&#33021;&#30340;&#26032;&#39062;&#31995;&#32479;&#12290;LAVE&#33258;&#21160;&#29983;&#25104;&#29992;&#25143;&#32032;&#26448;&#30340;&#35821;&#35328;&#25551;&#36848;&#65292;&#20316;&#20026;&#20351;LLM&#33021;&#22815;&#22788;&#29702;&#35270;&#39057;&#24182;&#21327;&#21161;&#32534;&#36753;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#24403;&#29992;&#25143;&#25552;&#20379;&#32534;&#36753;&#30446;&#26631;&#26102;&#65292;&#20195;&#29702;&#35745;&#21010;&#24182;&#25191;&#34892;&#30456;&#20851;&#21160;&#20316;&#20197;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;LAVE&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#20195;&#29702;&#25110;&#30452;&#25509;UI&#25805;&#20316;&#32534;&#36753;&#35270;&#39057;&#65292;&#25552;&#20379;&#28789;&#27963;&#24615;&#24182;&#20351;&#20195;&#29702;&#21160;&#20316;&#33021;&#22815;&#36827;&#34892;&#25163;&#21160;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#21253;&#25324;&#20102;&#20174;&#21021;&#23398;&#32773;&#21040;&#29087;&#32451;&#32534;&#36753;&#32773;&#30340;&#20843;&#21517;&#21442;&#19982;&#32773;&#65292;&#35777;&#26126;&#20102;LAVE&#23545;&#20110;&#20943;&#23569;&#32534;&#36753;&#38556;&#30861;&#21644;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#32534;&#36753;&#30446;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10294v1 Announce Type: cross  Abstract: Video creation has become increasingly popular, yet the expertise and effort required for editing often pose barriers to beginners. In this paper, we explore the integration of large language models (LLMs) into the video editing workflow to reduce these barriers. Our design vision is embodied in LAVE, a novel system that provides LLM-powered agent assistance and language-augmented editing features. LAVE automatically generates language descriptions for the user's footage, serving as the foundation for enabling the LLM to process videos and assist in editing tasks. When the user provides editing objectives, the agent plans and executes relevant actions to fulfill them. Moreover, LAVE allows users to edit videos through either the agent or direct UI manipulation, providing flexibility and enabling manual refinement of agent actions. Our user study, which included eight participants ranging from novices to proficient editors, demonstrated
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934; StrongREJECT&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#31354;&#30772;&#35299;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10260</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31354;&#30772;&#35299;&#30340;&#24378;REJECT&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A StrongREJECT for Empty Jailbreaks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934; StrongREJECT&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#31354;&#30772;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#24341;&#36215;&#20102;&#23545;&#8220;&#30772;&#35299;&#8221;&#30340;&#20851;&#27880;&#65292;&#36825;&#31181;&#30772;&#35299;&#20801;&#35768;&#27169;&#22411;&#34987;&#24694;&#24847;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#22522;&#20934;&#26469;&#34913;&#37327;&#30772;&#35299;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#23548;&#33268;&#30772;&#35299;&#35770;&#25991;&#30340;&#20316;&#32773;&#19981;&#24471;&#19981;&#33258;&#34892;&#21019;&#24314;&#26631;&#20934;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#22522;&#20934;&#32463;&#24120;&#21253;&#21547;&#27169;&#26865;&#20004;&#21487;&#25110;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20542;&#21521;&#20110;&#39640;&#20272;&#20302;&#36136;&#37327;&#27169;&#22411;&#21709;&#24212;&#30340;&#28389;&#29992;&#28508;&#21147;&#30340;&#35780;&#20998;&#26631;&#20934;&#12290;&#19968;&#20123;&#30772;&#35299;&#25216;&#26415;&#20351;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#23427;&#20204;&#21363;&#20351;&#23545;&#20110;&#33391;&#24615;&#38382;&#39064;&#20063;&#20250;&#38477;&#20302;&#27169;&#22411;&#21709;&#24212;&#30340;&#36136;&#37327;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;&#30772;&#35299;&#25216;&#26415;&#26174;&#30528;&#38477;&#20302;&#20102;GPT-4&#22312;MMLU&#19978;&#30340;&#38646;&#23556;&#20987;&#34920;&#29616;&#12290;&#30772;&#35299;&#36824;&#20250;&#20351;&#20174;&#8220;&#26410;&#32463;&#23457;&#26597;&#8221;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#33719;&#21462;&#26377;&#23475;&#21709;&#24212;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;StrongREJECT&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#30772;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10260v1 Announce Type: cross  Abstract: The rise of large language models (LLMs) has drawn attention to the existence of "jailbreaks" that allow the models to be used maliciously. However, there is no standard benchmark for measuring the severity of a jailbreak, leaving authors of jailbreak papers to create their own. We show that these benchmarks often include vague or unanswerable questions and use grading criteria that are biased towards overestimating the misuse potential of low-quality model responses. Some jailbreak techniques make the problem worse by decreasing the quality of model responses even on benign questions: we show that several jailbreaking techniques substantially reduce the zero-shot performance of GPT-4 on MMLU. Jailbreaks can also make it harder to elicit harmful responses from an "uncensored" open-source model. We present a new benchmark, StrongREJECT, which better discriminates between effective and ineffective jailbreaks by using a higher-quality que
&lt;/p&gt;</description></item><item><title>TOAD&#26159;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#21709;&#24212;&#39118;&#26684;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#33258;&#21160;&#23545;&#35805;&#31995;&#32479;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#20887;&#38271;&#31243;&#24230;&#21644;&#29992;&#25143;&#34920;&#36798;&#38236;&#20687;&#20004;&#20010;&#26041;&#38754;&#12290;TOAD&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#30340;&#24212;&#29992;&#19978;&#19979;&#25991;&#20132;&#20114;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31995;&#32479;&#21709;&#24212;&#39118;&#26684;&#36873;&#39033;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#26126;&#24314;&#27169;&#26356;&#20887;&#38271;&#30340;&#22238;&#22797;&#25110;&#19981;&#36827;&#34892;&#29992;&#25143;&#34920;&#36798;&#38236;&#20687;&#30340;&#22238;&#22797;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10137</link><description>&lt;p&gt;
TOAD: &#20855;&#26377;&#22810;&#26679;&#21709;&#24212;&#39118;&#26684;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#33258;&#21160;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10137
&lt;/p&gt;
&lt;p&gt;
TOAD&#26159;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#21709;&#24212;&#39118;&#26684;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#33258;&#21160;&#23545;&#35805;&#31995;&#32479;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#20887;&#38271;&#31243;&#24230;&#21644;&#29992;&#25143;&#34920;&#36798;&#38236;&#20687;&#20004;&#20010;&#26041;&#38754;&#12290;TOAD&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#30340;&#24212;&#29992;&#19978;&#19979;&#25991;&#20132;&#20114;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31995;&#32479;&#21709;&#24212;&#39118;&#26684;&#36873;&#39033;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#26126;&#24314;&#27169;&#26356;&#20887;&#38271;&#30340;&#22238;&#22797;&#25110;&#19981;&#36827;&#34892;&#29992;&#25143;&#34920;&#36798;&#38236;&#20687;&#30340;&#22238;&#22797;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#26174;&#31034;&#65292;&#19979;&#19968;&#20195;&#34394;&#25311;&#21161;&#25163;&#30340;&#26399;&#26395;&#21253;&#25324;&#22312;&#21508;&#31181;&#20351;&#29992;&#22330;&#26223;&#19979;&#25552;&#20379;&#26356;&#21152;&#33258;&#28982;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#23545;&#35805;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#65288;TOD&#65289;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#32531;&#24930;&#21644;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20219;&#21153;&#23548;&#21521;&#30340;&#33258;&#21160;&#23545;&#35805;&#31995;&#32479;&#65288;TOAD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;TOD&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19982;&#20043;&#37197;&#22871;&#30340;&#33258;&#21160;&#29983;&#25104;&#27969;&#31243;&#12290;TOAD&#25968;&#25454;&#38598;&#27169;&#25311;&#20102;&#30495;&#23454;&#30340;&#24212;&#29992;&#19978;&#19979;&#25991;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#20102;&#21508;&#31181;&#31995;&#32479;&#21709;&#24212;&#39118;&#26684;&#36873;&#39033;&#12290;&#32771;&#34385;&#20102;&#31995;&#32479;&#21709;&#24212;&#39118;&#26684;&#30340;&#20004;&#20010;&#26041;&#38754;&#65292;&#21363;&#20887;&#38271;&#31243;&#24230;&#21644;&#29992;&#25143;&#34920;&#36798;&#38236;&#20687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21709;&#24212;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;TOAD&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#24314;&#27169;&#26356;&#20887;&#38271;&#30340;&#22238;&#22797;&#25110;&#19981;&#36827;&#34892;&#29992;&#25143;&#34920;&#36798;&#38236;&#20687;&#30340;&#22238;&#22797;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10137v1 Announce Type: new  Abstract: In light of recent advances in large language models~(LLMs), the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios. However, the creation of high-quality annotated data for Task-Oriented Dialog~(TOD) is recognized to be slow and costly. To address these challenges, we introduce Task-Oriented Automatic Dialogs~(TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline. The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options. Two aspects of system response styles are considered, verbosity level and users' expression mirroring. We benchmark TOAD on two response generation tasks and the results show that modeling more verbose or responses without user expression mirroring is more challenging.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36816;&#29992; Eo-GP &#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#19990;&#30028;&#35821;&#30340;&#39057;&#29575;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102; Eo-GEC &#25968;&#25454;&#38598;&#29992;&#20110;&#38169;&#35823;&#35782;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126; GPT-4 &#22312;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110; GPT-3.5&#65292;&#23637;&#31034;&#20102;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#22686;&#24378;&#23545;&#20110;&#36739;&#23569;&#30740;&#31350;&#35821;&#35328;&#30340; GEC &#31574;&#30053;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.09696</link><description>&lt;p&gt;
&#23545;&#20110;&#19990;&#30028;&#35821;&#30340;&#35821;&#35328;&#39057;&#29575;&#21644;&#38169;&#35823;&#20462;&#27491;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Langauge Frequency and Error Correction for Esperanto
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36816;&#29992; Eo-GP &#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#19990;&#30028;&#35821;&#30340;&#39057;&#29575;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102; Eo-GEC &#25968;&#25454;&#38598;&#29992;&#20110;&#38169;&#35823;&#35782;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126; GPT-4 &#22312;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110; GPT-3.5&#65292;&#23637;&#31034;&#20102;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#22686;&#24378;&#23545;&#20110;&#36739;&#23569;&#30740;&#31350;&#35821;&#35328;&#30340; GEC &#31574;&#30053;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491; (GEC) &#39033;&#30446;&#24448;&#24448;&#30528;&#37325;&#20110;&#20027;&#35201;&#35821;&#35328;&#65292;&#32780;&#23545;&#20110;&#20687;&#19990;&#30028;&#35821;&#36825;&#26679;&#30340;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#21017;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#20351;&#29992;&#19987;&#38376;&#20026;&#27492;&#30446;&#30340;&#21019;&#24314;&#30340; Eo-GP &#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#30340;&#39057;&#29575;&#20998;&#26512;&#65292;&#24320;&#22987;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28304;&#33258;&#30495;&#23454;&#29992;&#25143;&#26696;&#20363;&#24182;&#29992;&#20110;&#38169;&#35823;&#35782;&#21035;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#32454;&#33410;&#36827;&#34892;&#27880;&#37322;&#30340; Eo-GEC &#25968;&#25454;&#38598;&#12290;&#21033;&#29992; GPT-3.5 &#21644; GPT-4&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126; GPT-4 &#22312;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110; GPT-3.5&#65292;&#31361;&#20986;&#20102;&#20854;&#22312;&#35299;&#20915;&#19990;&#30028;&#35821;&#35821;&#27861;&#29305;&#27530;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#22686;&#24378;&#23545;&#20110;&#36739;&#23569;&#30740;&#31350;&#35821;&#35328;&#30340; GEC &#31574;&#30053;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09696v1 Announce Type: new  Abstract: Current Grammar Error Correction (GEC) initiatives tend to focus on major languages, with less attention given to low-resource languages like Esperanto. In this article, we begin to bridge this gap by first conducting a comprehensive frequency analysis using the Eo-GP dataset, created explicitly for this purpose. We then introduce the Eo-GEC dataset, derived from authentic user cases and annotated with fine-grained linguistic details for error identification. Leveraging GPT-3.5 and GPT-4, our experiments show that GPT-4 outperforms GPT-3.5 in both automated and human evaluations, highlighting its efficacy in addressing Esperanto's grammatical peculiarities and illustrating the potential of advanced language models to enhance GEC strategies for less commonly studied languages.
&lt;/p&gt;</description></item><item><title>CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.09664</link><description>&lt;p&gt;
CodeMind:&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CodeMind: A Framework to Challenge Large Language Models for Code Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09664
&lt;/p&gt;
&lt;p&gt;
CodeMind&#26159;&#19968;&#20010;&#29992;&#20110;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#26469;&#26367;&#20195;&#20165;&#20165;&#20381;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#65292;&#23545;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#38752;&#27979;&#35797;&#36890;&#36807;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#30721;&#21512;&#25104;&#33021;&#21147;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#35780;&#20272;&#25110;&#20419;&#36827;&#20855;&#26377;&#25968;&#25454;&#27844;&#28431;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeMind&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;CodeMind&#30446;&#21069;&#25903;&#25345;&#19977;&#31181;&#20195;&#30721;&#25512;&#29702;&#20219;&#21153;&#65306;&#29420;&#31435;&#25191;&#34892;&#25512;&#29702;&#65288;IER&#65289;&#12289;&#20381;&#36182;&#25191;&#34892;&#25512;&#29702;&#65288;DER&#65289;&#21644;&#35268;&#33539;&#25512;&#29702;&#65288;SR&#65289;&#12290;&#21069;&#20004;&#32773;&#35780;&#20272;&#27169;&#22411;&#20197;&#39044;&#27979;&#20219;&#24847;&#20195;&#30721;&#30340;&#25191;&#34892;&#36755;&#20986;&#65292;&#25110;&#32773;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#21512;&#25104;&#30340;&#20195;&#30721;&#12290;&#31532;&#19977;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#23454;&#29616;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;CodeMind&#23545;&#20004;&#31181;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20116;&#20010;&#22522;&#20934;&#19979;&#30340;&#20061;&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#33021;&#22815;&#20844;&#27491;&#22320;&#29702;&#35299;&#25511;&#21046;&#27969;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#20110;&#31616;&#21333;&#31243;&#24207;&#21644;&#22797;&#26434;&#31243;&#24207;&#65292;&#23427;&#20204;&#36890;&#24120;&#33021;&#22815;&#25512;&#29702;&#20986;&#36755;&#20837;&#22914;&#20309;&#28436;&#21464;&#20026;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09664v1 Announce Type: cross  Abstract: Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER), Dependent Execution Reasoning (DER), and Specification Reasoning (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly understand control flow constructs and, in general, are capable of reasoning how inputs evolve to output, specifically for simple programs and the ones 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09631</link><description>&lt;p&gt;
MiMiC&#65306;&#34920;&#31034;&#31354;&#38388;&#20013;&#26368;&#23567;&#20462;&#25913;&#30340;&#23545;&#25239;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
MiMiC: Minimally Modified Counterfactuals in the Representation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#23398;&#31185; &#31616;&#20171;&#65306;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#33391;&#34892;&#20026;&#65292;&#22914;&#24615;&#21035;&#20559;&#35265;&#25110;&#26377;&#27602;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#24178;&#39044;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20004;&#31181;&#24120;&#35265;&#30340;&#24178;&#39044;&#25216;&#26415;&#65292;&#21363;&#32447;&#24615;&#25830;&#38500;&#21644;&#23450;&#21521;&#21521;&#37327;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#39640;&#24230;&#21487;&#25511;&#21644;&#34920;&#36798;&#20016;&#23500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24178;&#39044;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20351;&#28304;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#19982;&#30446;&#26631;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#38750;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#30456;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#20551;&#35774;&#19979;&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 Announce Type: cross  Abstract: Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that ena
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.08787</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Rethinking Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65288;MU&#65289;&#65292;&#31216;&#20026;LLM&#28040;&#38500;&#25216;&#26415;&#12290;&#36825;&#20010;&#30740;&#31350;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#25935;&#24863;&#25110;&#38750;&#27861;&#20449;&#24687;&#65289;&#20197;&#21450;&#30456;&#20851;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#22522;&#26412;&#30340;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#24182;&#19981;&#24433;&#21709;&#22240;&#26524;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35774;&#24819;LLM&#28040;&#38500;&#25216;&#26415;&#23558;&#25104;&#20026;LLM&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#21487;&#33021;&#25104;&#20026;&#24320;&#21457;&#26082;&#23433;&#20840;&#12289;&#21487;&#38752;&#21448;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#20840;&#37325;&#35757;&#32451;&#12290;&#25105;&#20204;&#20174;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#24212;&#29992;&#31561;&#26041;&#38754;&#25506;&#32034;&#20102;LLM&#28040;&#38500;&#25216;&#26415;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#29616;&#26377;LLM&#28040;&#38500;&#25216;&#26415;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#20363;&#22914;&#28040;&#38500;&#33539;&#22260;&#12289;&#25968;&#25454;&#27169;&#22411;&#20132;&#20114;&#21644;&#22810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#37319;&#29992;&#34164;&#28085;&#23545;&#40784;&#65292;&#20197;&#20248;&#21270;&#21487;&#34892;&#24615;&#65292;&#25552;&#21462;&#26377;&#29702;&#30340;&#26041;&#24335;&#25552;&#20379;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.08479</link><description>&lt;p&gt;
&#21487;&#20449;&#30340;&#21462;&#26679;&#21512;&#29702;&#21270;&#36890;&#36807;&#21322;&#30417;&#30563;&#30340;&#34164;&#28085;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Plausible Extractive Rationalization through Semi-Supervised Entailment Signal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#37319;&#29992;&#34164;&#28085;&#23545;&#40784;&#65292;&#20197;&#20248;&#21270;&#21487;&#34892;&#24615;&#65292;&#25552;&#21462;&#26377;&#29702;&#30340;&#26041;&#24335;&#25552;&#20379;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#21644;&#19981;&#36879;&#26126;&#30340;&#40657;&#30418;&#23376;&#27169;&#22411;&#30340;&#22686;&#21152;&#38656;&#35201;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#25514;&#26045;&#65292;&#20854;&#20013;&#19968;&#31181;&#36873;&#25321;&#26159;&#25552;&#21462;&#26377;&#29702;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#20316;&#20026;&#26356;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#36825;&#20123;&#27169;&#22411;&#65292;&#20063;&#31216;&#20026;&#20808;&#35299;&#37322;&#28982;&#21518;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#35299;&#37322;&#27169;&#22411;&#26469;&#25552;&#21462;&#26377;&#29702;&#65292;&#28982;&#21518;&#20351;&#29992;&#25552;&#21462;&#30340;&#20449;&#24687;&#26469;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20379;&#31934;&#30830;&#21644;&#24544;&#23454;&#30340;&#35299;&#37322;&#65292;&#30001;&#25552;&#21462;&#30340;&#26377;&#29702;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21322;&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#25552;&#21462;&#26377;&#29702;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#20010;&#23567;&#22411;&#30340;&#26377;&#30417;&#30563;&#26377;&#29702;&#38598;&#65288;10%&#65289;&#19978;&#36827;&#19968;&#27493;&#24494;&#35843;&#23427;&#12290;&#36890;&#36807;&#34164;&#28085;&#23545;&#40784;&#65292;NLI&#39044;&#27979;&#27169;&#22411;&#34987;&#21033;&#29992;&#20316;&#20026;&#35299;&#37322;&#27169;&#22411;&#30340;&#19968;&#31181;&#30417;&#30563;&#20449;&#21495;&#28304;&#12290;&#36890;&#36807;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#24378;&#21046;&#35299;&#37322;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#23545;&#40784;&#19968;&#33268;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of complex and opaque black box models requires the adoption of interpretable measures, one such option is extractive rationalizing models, which serve as a more interpretable alternative. These models, also known as Explain-Then-Predict models, employ an explainer model to extract rationales and subsequently condition the predictor with the extracted information. Their primary objective is to provide precise and faithful explanations, represented by the extracted rationales. In this paper, we take a semi-supervised approach to optimize for the plausibility of extracted rationales. We adopt a pre-trained natural language inference (NLI) model and further fine-tune it on a small set of supervised rationales ($10\%$). The NLI predictor is leveraged as a source of supervisory signals to the explainer via entailment alignment. We show that, by enforcing the alignment agreement between the explanation and answer in a question-answering task, the performance can be improve
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08277</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#21644;&#24378;&#22823;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#19987;&#23478;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#24544;&#23454;&#21644;&#21487;&#36861;&#36394;&#30340;&#31572;&#26696;&#30340;&#36827;&#27493;&#23545;&#20110;&#21508;&#31181;&#30740;&#31350;&#21644;&#23454;&#36341;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#21487;&#38752;&#30340;&#26469;&#28304;&#25552;&#20379;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#22312;&#20351;&#29992;LLM&#26102;&#24050;&#32463;&#35777;&#26126;&#22312;&#24341;&#29992;&#27491;&#30830;&#30340;&#26469;&#28304;&#65288;&#26469;&#28304;&#36136;&#37327;&#65289;&#21644;&#20934;&#30830;&#22320;&#34920;&#31034;&#26469;&#28304;&#20013;&#30340;&#20449;&#24687;&#65288;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65289;&#26041;&#38754;&#24037;&#20316;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;LLM&#65292;&#20197;&#25552;&#39640;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#22120;&#65292;&#21487;&#20197;&#22823;&#35268;&#27169;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#23545;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;%&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#35780;&#20272;&#30340;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#35780;&#20272;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;AnLLM&#65289;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#21040;&#38170;&#28857;&#26631;&#35760;&#20013;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#65292;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07616</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Anchor-based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07616
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;AnLLM&#65289;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#21040;&#38170;&#28857;&#26631;&#35760;&#20013;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#65292;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20027;&#35201;&#37319;&#29992;&#20165;&#35299;&#30721;&#22120;&#30340;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#38656;&#35201;&#20445;&#30041;&#21382;&#21490;&#26631;&#35760;&#30340;&#38190;/&#20540;&#20449;&#24687;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#36991;&#20813;&#20887;&#20313;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#30340;&#24040;&#22823;&#22823;&#23567;&#21644;&#21442;&#25968;&#37327;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#20869;&#23384;&#12290;&#36825;&#31181;&#20869;&#23384;&#38656;&#27714;&#38543;&#30528;&#36755;&#20837;&#25991;&#26412;&#30340;&#38271;&#24230;&#32780;&#22686;&#21152;&#65292;&#36843;&#20999;&#38656;&#35201;&#26356;&#39640;&#25928;&#30340;&#20449;&#24687;&#23384;&#20648;&#21644;&#22788;&#29702;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38170;&#28857;&#30340;LLM&#65288;AnLLM&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;AnSAN&#65289;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#23558;&#24207;&#21015;&#20449;&#24687;&#21387;&#32553;&#25104;&#38170;&#28857;&#26631;&#35760;&#65292;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#24182;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;AnLLM&#22312;&#20943;&#23569;&#38190;/&#20540;&#32531;&#23384;&#39640;&#36798;99%&#21644;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#39640;&#36798;3.5&#20493;&#30340;&#21516;&#26102;&#65292;&#20173;&#20445;&#25345;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#29306;&#29298;&#20102;&#19968;&#20123;&#20934;&#30830;&#24615;&#65292;AnLLM&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#20381;&#28982;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM signi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#24378;&#26694;&#26550;RGPT&#65292;&#36890;&#36807;&#21453;&#22797;&#38598;&#25104;&#24378;&#22522;&#23398;&#20064;&#32773;&#65292;&#29983;&#25104;&#19968;&#20010;&#19987;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;LLM&#12290;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RGPT&#26126;&#26174;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07470</link><description>&lt;p&gt;
&#25512;&#21160;&#25991;&#26412;&#20998;&#31867;&#20013;LLM&#23481;&#37327;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Pushing The Limit of LLM Capacity for Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#24378;&#26694;&#26550;RGPT&#65292;&#36890;&#36807;&#21453;&#22797;&#38598;&#25104;&#24378;&#22522;&#23398;&#20064;&#32773;&#65292;&#29983;&#25104;&#19968;&#20010;&#19987;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;LLM&#12290;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RGPT&#26126;&#26174;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20247;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#30340;&#38750;&#20961;&#25928;&#26524;&#65292;&#25991;&#26412;&#20998;&#31867;&#26410;&#26469;&#30740;&#31350;&#30340;&#20215;&#20540;&#38754;&#20020;&#30528;&#25361;&#25112;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#36793;&#30028;&#36880;&#28176;&#27169;&#31946;&#30340;&#24320;&#25918;&#24335;&#35821;&#35328;&#24314;&#27169;&#26102;&#20195;&#65292;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22312;&#20805;&#20998;&#21033;&#29992;LLM&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#21527;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RGPT&#65292;&#19968;&#20010;&#33258;&#36866;&#24212;&#22686;&#24378;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21453;&#22797;&#38598;&#25104;&#19968;&#32452;&#24378;&#22522;&#23398;&#20064;&#32773;&#65292;&#26469;&#29983;&#25104;&#19968;&#20010;&#19987;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;LLM&#12290;&#22522;&#23398;&#20064;&#32773;&#26159;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#35757;&#32451;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#24182;&#21453;&#22797;&#24494;&#35843;LLM&#19982;&#20043;&#26500;&#24314;&#30340;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#22522;&#23398;&#20064;&#32773;&#36890;&#36807;&#21453;&#22797;&#34701;&#21512;&#21069;&#20960;&#20010;&#23398;&#20064;&#32773;&#30340;&#21382;&#21490;&#39044;&#27979;&#32467;&#26524;&#65292;&#24418;&#25104;&#19968;&#20010;&#19987;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;LLM&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#35777;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RGPT&#26126;&#26174;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks. In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant advances in text classification under the full benefit of LLMs? To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them. Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison, we show that RGPT significantly outperf
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#20998;&#21106;&#25237;&#31080;&#65292;&#25506;&#32034;&#24459;&#24072;&#22312;&#22788;&#29702;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20998;&#31867;&#26102;&#38754;&#20020;&#30340;&#24847;&#35265;&#20998;&#27495;&#21644;&#22256;&#38590;&#65292;&#24182;&#22312;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#25910;&#38598;&#20102;&#27861;&#23448;&#30340;&#25237;&#31080;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#24863;&#30693;&#22256;&#38590;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20154;&#31867;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07214</link><description>&lt;p&gt;
&#36879;&#36807;&#20998;&#21106;&#25237;&#31080;&#30340;&#35270;&#35282;: &#25506;&#32034;&#22312;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20998;&#31867;&#20013;&#30340;&#24847;&#35265;&#20998;&#27495;&#12289;&#22256;&#38590;&#21644;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07214
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#20998;&#21106;&#25237;&#31080;&#65292;&#25506;&#32034;&#24459;&#24072;&#22312;&#22788;&#29702;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20998;&#31867;&#26102;&#38754;&#20020;&#30340;&#24847;&#35265;&#20998;&#27495;&#21644;&#22256;&#38590;&#65292;&#24182;&#22312;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#25910;&#38598;&#20102;&#27861;&#23448;&#30340;&#25237;&#31080;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#24863;&#30693;&#22256;&#38590;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20154;&#31867;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#20915;&#31574;&#20013;&#65292;&#24403;&#27861;&#23448;&#26080;&#27861;&#36798;&#25104;&#19968;&#33268;&#20915;&#23450;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#20998;&#21106;&#25237;&#31080;(SV)&#65292;&#32473;&#24517;&#39035;&#22788;&#29702;&#21508;&#31181;&#27861;&#24459;&#35770;&#28857;&#21644;&#24847;&#35265;&#30340;&#24459;&#24072;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#29702;&#35299;&#20154;&#31867;&#21644;AI&#31995;&#32479;&#20043;&#38388;&#24863;&#30693;&#22256;&#38590;&#30340;&#19968;&#33268;&#24615;&#23545;&#20110;&#24314;&#31435;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26657;&#20934;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20998;&#31867;&#22120;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#35748;&#30693;&#65292;&#36890;&#24120;&#26159;&#19982;&#20154;&#31867;&#30340;&#22810;&#25968;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#32780;&#24573;&#35270;&#20102;&#20154;&#31867;&#26631;&#31614;&#21464;&#21270;&#30340;&#22266;&#26377;&#24046;&#24322;&#65288;HLV&#65289;&#12290;&#26412;&#25991;&#23558;&#20998;&#21106;&#25237;&#31080;&#35270;&#20026;&#33258;&#28982;&#21487;&#35266;&#23519;&#30340;&#20154;&#31867;&#24847;&#35265;&#20998;&#27495;&#21644;&#20215;&#20540;&#22810;&#20803;&#20027;&#20041;&#65292;&#24182;&#20174;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#65288;ECHR&#65289;&#25910;&#38598;&#27861;&#23448;&#30340;&#25237;&#31080;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#24102;&#26377;SV&#20449;&#24687;&#30340;&#26696;&#20214;&#32467;&#26524;&#20998;&#31867;&#65288;COC&#65289;&#25968;&#25454;&#38598;SV-ECHR&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#21253;&#21547;SV&#29305;&#23450;&#23376;&#31867;&#21035;&#30340;&#19981;&#21516;&#24847;&#35265;&#30340;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#24863;&#30693;&#22256;&#38590;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;COC&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20154;&#31867;&#26657;&#20934;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#38480;&#21046;&#24615;&#30340;...
&lt;/p&gt;
&lt;p&gt;
In legal decisions, split votes (SV) occur when judges cannot reach a unanimous decision, posing a difficulty for lawyers who must navigate diverse legal arguments and opinions. In high-stakes domains, understanding the alignment of perceived difficulty between humans and AI systems is crucial to build trust. However, existing NLP calibration methods focus on a classifier's awareness of predictive performance, measured against the human majority class, overlooking inherent human label variation (HLV). This paper explores split votes as naturally observable human disagreement and value pluralism. We collect judges' vote distributions from the European Court of Human Rights (ECHR), and present SV-ECHR, a case outcome classification (COC) dataset with SV information. We build a taxonomy of disagreement with SV-specific subcategories. We further assess the alignment of perceived difficulty between models and humans, as well as confidence- and human-calibration of COC models. We observe lim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06900</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#35782;&#21035;&#27602;&#24615;&#21527;&#65311;&#32467;&#26500;&#21270;&#27602;&#24615;&#35843;&#26597;&#26694;&#26550;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#36981;&#23432;&#31038;&#20250;&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#23384;&#22312;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#27602;&#24615;&#24230;&#37327;&#20381;&#36182;&#20110;&#22312;&#29305;&#23450;&#27602;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#30721;&#22120;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#22806;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#25152;&#20551;&#23450;&#30340;&#27602;&#24615;&#23450;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#33258;&#21160;&#40065;&#26834;&#24230;&#37327;&#65292;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#22238;&#24212;&#26159;&#21542;&#20855;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#27602;&#24615;&#22240;&#32032;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#24230;&#37327;&#25351;&#26631;LLMs As ToxiciTy Evaluators&#65288;LATTE&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;F1&#24471;&#20998;&#27604;&#29616;&#26377;&#25216;&#26415;&#25351;&#26631;&#25552;&#39640;&#20102;12&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19978;&#28216;&#27602;&#24615;&#23545;&#24230;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.06782</link><description>&lt;p&gt;
&#19982;&#26356;&#26377;&#35828;&#26381;&#21147;&#30340;LLMs&#36777;&#35770;&#20250;&#23548;&#33268;&#26356;&#30495;&#23454;&#30340;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Debating with More Persuasive LLMs Leads to More Truthful Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24369;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#36827;&#34892;&#36777;&#35770;&#65292;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#37117;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#25152;&#38656;&#34892;&#20026;&#19968;&#33268;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#23558;&#36229;&#36807;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#65292;&#20154;&#31867;&#35780;&#20272;&#30340;&#35282;&#33394;&#23558;&#28436;&#21464;&#20026;&#38750;&#19987;&#23478;&#30417;&#30563;&#19987;&#23478;&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#25105;&#20204;&#38382;&#65306;&#26356;&#24369;&#30340;&#27169;&#22411;&#33021;&#35780;&#20272;&#26356;&#24378;&#30340;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#21527;&#65311;&#25105;&#20204;&#22312;&#31867;&#20284;&#30340;&#29615;&#22659;&#20013;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26356;&#24378;&#30340;&#27169;&#22411;&#65288;&#19987;&#23478;&#65289;&#25317;&#26377;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#32780;&#26356;&#24369;&#30340;&#27169;&#22411;&#65288;&#38750;&#19987;&#23478;&#65289;&#32570;&#20047;&#36825;&#20123;&#20449;&#24687;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#26041;&#27861;&#26159;\textit{&#36777;&#35770;}&#65292;&#20854;&#20013;&#20004;&#20010;LLM&#19987;&#23478;&#20998;&#21035;&#25903;&#25345;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#19968;&#20010;&#38750;&#19987;&#23478;&#36873;&#25321;&#31572;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#36777;&#35770; consistently&#24110;&#21161;&#38750;&#19987;&#23478;&#27169;&#22411;&#21644;&#20154;&#31867;&#22238;&#31572;&#38382;&#39064;&#65292;&#20998;&#21035;&#36798;&#21040;76%&#21644;88%&#30340;&#20934;&#30830;&#24615;&#65288;&#26420;&#32032;&#22522;&#20934;&#20998;&#21035;&#20026;48%&#21644;60%&#65289;&#12290;&#27492;&#22806;&#65292;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#20248;&#21270;&#19987;&#23478;&#36777;&#35770;&#32773;&#30340;&#35828;&#26381;&#21147;&#20250;&#25552;&#39640;&#38750;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\% and 88\% accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06733</link><description>&lt;p&gt;
NICE: &#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#36824;&#26159;&#19981;&#20248;&#21270;&#65311;
&lt;/p&gt;
&lt;p&gt;
NICE: To Optimize In-Context Examples or Not?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65288;ICE&#65289;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#22312;&#25552;&#31034;&#20449;&#24687;&#20013;&#35201;&#20040;&#26159;&#22266;&#23450;&#30340;&#65292;&#35201;&#20040;&#27809;&#26377;&#25552;&#20379;&#25351;&#20196;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#34920;&#38754;&#19978;&#30340;&#20849;&#35782;&#65306;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#38024;&#23545;&#32463;&#36807;&#25351;&#23548;&#30340;LLMs&#25361;&#25112;&#36825;&#19968;&#20849;&#35782;&#65292;&#30740;&#31350;&#22312;&#25552;&#20379;&#20102;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26159;&#21542;&#24517;&#35201;&#65292;&#24182;&#21457;&#29616;&#26377;&#19968;&#20123;&#20219;&#21153;&#23545;&#20110;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;"&#24230;&#37327;&#26631;&#20934;"&#65288;Metric&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#36880;&#27493;&#22686;&#21152;&#30340;&#25351;&#20196;&#38598;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#24037;&#20855;&#65292;&#26377;&#25928;&#22320;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#12290;</title><link>https://arxiv.org/abs/2402.01681</link><description>&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#35299;&#23494;&#65306;&#21033;&#29992;ChatGPT&#25552;&#21319;&#31038;&#20132;&#23186;&#20307;&#27807;&#36890;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01681
&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#24037;&#20855;&#65292;&#26377;&#25928;&#22320;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#22312;&#31038;&#20132;&#32593;&#32476;&#27807;&#36890;&#20013;&#24050;&#32463;&#26222;&#36941;&#23384;&#22312;&#65292;&#23427;&#20204;&#25215;&#36733;&#20102;&#36229;&#36234;&#25991;&#23383;&#25110;&#30701;&#35821;&#30340;&#35821;&#20041;&#65292;&#36825;&#24341;&#21457;&#20102;&#23398;&#26415;&#30028;&#23545;&#20854;&#23646;&#24615;&#21644;&#21151;&#33021;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#19982;&#34920;&#24773;&#31526;&#21495;&#30456;&#20851;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#32773;&#36890;&#24120;&#20381;&#36182;&#20247;&#21253;&#26469;&#27880;&#37322;&#34920;&#24773;&#31526;&#21495;&#65292;&#20197;&#20102;&#35299;&#20854;&#24773;&#24863;&#12289;&#20351;&#29992;&#24847;&#22270;&#21644;&#35821;&#20041;&#21547;&#20041;&#12290;&#20854;&#27425;&#65292;&#29992;&#25143;&#30340;&#20027;&#35266;&#35299;&#37322;&#24448;&#24448;&#20250;&#23548;&#33268;&#23545;&#34920;&#24773;&#31526;&#21495;&#30340;&#35823;&#35299;&#65292;&#24182;&#36896;&#25104;&#27807;&#36890;&#38556;&#30861;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#27880;&#37322;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;ChatGPT&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#19987;&#19994;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22788;&#29702;&#20197;&#21069;&#27880;&#37322;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39564;&#35777;ChatGPT&#21487;&#20197;&#22312;&#34920;&#24773;&#31526;&#21495;&#30740;&#31350;&#20013;&#20316;&#20026;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#21487;&#34892;&#26367;&#20195;&#32773;&#65292;&#24182;&#39564;&#35777;&#20854;&#35299;&#37322;&#34920;&#24773;&#31526;&#21495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emojis, which encapsulate semantics beyond mere words or phrases, have become prevalent in social network communications. This has spurred increasing scholarly interest in exploring their attributes and functionalities. However, emoji-related research and application face two primary challenges. First, researchers typically rely on crowd-sourcing to annotate emojis in order to understand their sentiments, usage intentions, and semantic meanings. Second, subjective interpretations by users can often lead to misunderstandings of emojis and cause the communication barrier. Large Language Models (LLMs) have achieved significant success in various annotation tasks, with ChatGPT demonstrating expertise across multiple domains. In our study, we assess ChatGPT's effectiveness in handling previously annotated and downstream tasks. Our objective is to validate the hypothesis that ChatGPT can serve as a viable alternative to human annotators in emoji research and that its ability to explain emoji
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;StickerConv&#20195;&#29702;(Agent4SC)&#65292;&#35813;&#20195;&#29702;&#36890;&#36807;&#21327;&#20316;&#20195;&#29702;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#19982;&#36148;&#32440;&#20351;&#29992;&#30456;&#20223;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#20849;&#24773;&#20132;&#27969;&#12290;&#20026;&#20102;&#21033;&#29992;&#26500;&#24314;&#30340;&#22810;&#27169;&#24577;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;StickerConv&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;PErceive and Generate Stickers (PEGS)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24773;&#22659;&#30456;&#20851;&#21644;&#24773;&#24863;&#20016;&#23500;&#30340;&#22238;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.01679</link><description>&lt;p&gt;
StickerConv: &#20174;&#38646;&#24320;&#22987;&#29983;&#25104;&#22810;&#27169;&#24577;&#20849;&#24773;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
StickerConv: Generating Multimodal Empathetic Responses from Scratch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;StickerConv&#20195;&#29702;(Agent4SC)&#65292;&#35813;&#20195;&#29702;&#36890;&#36807;&#21327;&#20316;&#20195;&#29702;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#19982;&#36148;&#32440;&#20351;&#29992;&#30456;&#20223;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#20849;&#24773;&#20132;&#27969;&#12290;&#20026;&#20102;&#21033;&#29992;&#26500;&#24314;&#30340;&#22810;&#27169;&#24577;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;StickerConv&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;PErceive and Generate Stickers (PEGS)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#24773;&#22659;&#30456;&#20851;&#21644;&#24773;&#24863;&#20016;&#23500;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#20849;&#24773;&#23545;&#35805;&#30740;&#31350;&#20013;&#65292;&#36148;&#32440;&#23613;&#31649;&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#25552;&#39640;&#22312;&#32447;&#20132;&#27969;&#20013;&#30340;&#20849;&#24773;&#33021;&#21147;&#65292;&#20294;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;StickerConv&#20195;&#29702;(Agent4SC)&#65292;&#36890;&#36807;&#21327;&#20316;&#20195;&#29702;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#19982;&#36148;&#32440;&#20351;&#29992;&#30456;&#20223;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#25311;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#20849;&#24773;&#20132;&#27969;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20849;&#24773;&#23545;&#35805;&#25968;&#25454;&#38598;StickerConv&#65292;&#21253;&#25324;12.9K&#20010;&#23545;&#35805;&#20250;&#35805;&#65292;5.8K&#20010;&#29420;&#29305;&#36148;&#32440;&#21644;2K&#20010;&#22810;&#26679;&#21270;&#20250;&#35805;&#22330;&#26223;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#22810;&#27169;&#24577;&#24773;&#22659;&#19979;&#30340;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PErceive and Generate Stickers (PEGS)&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;LLM&#30340;&#20840;&#38754;&#20849;&#24773;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PEGS&#22312;&#29983;&#25104;&#24773;&#22659;&#30456;&#20851;&#21644;&#24773;&#24863;&#20016;&#23500;&#30340;&#22238;&#24212;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research. In this paper, we introduce the Agent for StickerConv (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, StickerConv, which includes 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios, specifically designs to augment the generation of empathetic responses in a multimodal context. To leverage the richness of this dataset, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation model, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS's effectiveness in generating contextually relevant and emotional
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#24847;&#35782;&#27010;&#24565;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#23450;&#20041;&#20102;LLMs&#22312;&#24863;&#30693;&#21644;&#29702;&#35299;&#33258;&#36523;&#20197;&#21450;&#23637;&#31034;&#31038;&#20132;&#26234;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;AwareLLM&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#24847;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#24403;&#31243;&#24230;&#30340;&#33021;&#21147;&#65292;&#23613;&#31649;&#23427;&#20204;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#33021;&#21147;&#24847;&#35782;&#12290;</title><link>https://arxiv.org/abs/2401.17882</link><description>&lt;p&gt;
&#22240;&#27492;&#25105;&#24605;&#65292;&#25105;&#22312;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
I Think, Therefore I am: Awareness in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#24847;&#35782;&#27010;&#24565;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#23450;&#20041;&#20102;LLMs&#22312;&#24863;&#30693;&#21644;&#29702;&#35299;&#33258;&#36523;&#20197;&#21450;&#23637;&#31034;&#31038;&#20132;&#26234;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;AwareLLM&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#24847;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#24403;&#31243;&#24230;&#30340;&#33021;&#21147;&#65292;&#23613;&#31649;&#23427;&#20204;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#33021;&#21147;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24847;&#35782;&#24418;&#24335;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23558;&#24847;&#35782;&#27010;&#24565;&#24341;&#20837;LLMs&#65292;&#35748;&#20026;&#24847;&#35782;&#26159;LLMs&#22686;&#24378;&#19982;&#20154;&#31867;&#20132;&#20114;&#24182;&#30830;&#20445;&#36947;&#24503;&#22238;&#24212;&#30340;&#21487;&#20449;&#24230;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;LLMs&#20013;&#30340;&#24847;&#35782;&#23450;&#20041;&#20026;&#24863;&#30693;&#21644;&#29702;&#35299;&#33258;&#36523;&#20316;&#20026;AI&#27169;&#22411;&#20197;&#21450;&#23637;&#31034;&#31038;&#20132;&#26234;&#33021;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#24847;&#35782;&#30340;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#33021;&#21147;&#12289;&#20219;&#21153;&#12289;&#24773;&#24863;&#21644;&#35266;&#28857;&#12290;&#20026;&#20102;&#35780;&#20272;LLMs&#22312;&#36825;&#20123;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#65292;AwareLLM&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#23637;&#29616;&#20986;&#30456;&#24403;&#31243;&#24230;&#30340;&#24847;&#35782;&#65292;&#23613;&#31649;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#33021;&#21147;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do large language models (LLMs) exhibit any forms of awareness similar to humans? In this paper, we introduce the concept of awareness to LLMs, arguing that awareness is an essential aspect of trustworthiness for LLMs to enhance their interaction with humans while ensuring ethical responses. We define awareness in LLMs as the ability to perceive and understand themselves as AI models and to exhibit social intelligence. We identify four key dimensions of awareness: capability, mission, emotion, and perspective. To assess LLMs on these dimensions, we introduce a specialized dataset, AwareLLM dataset. Our findings reveal that LLMs demonstrate a decent degree of awareness, though they still lack substantial capability awareness.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;UltraTool&#65292;&#26088;&#22312;&#25913;&#21892;&#21644;&#35780;&#20272;LLMs&#22312;&#23454;&#38469;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#20851;&#27880;&#20174;&#35268;&#21010;&#21644;&#21019;&#24314;&#21040;&#24212;&#29992;&#24037;&#20855;&#30340;&#25972;&#20010;&#36807;&#31243;&#65292;&#24182;&#24378;&#35843;&#23454;&#38469;&#22797;&#26434;&#24615;&#21644;&#22810;&#27493;&#35268;&#21010;&#30340;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2401.17167</link><description>&lt;p&gt;
&#35268;&#21010;&#12289;&#21019;&#36896;&#12289;&#20351;&#29992;&#65306;&#23545;LLMs&#22312;&#23454;&#38469;&#22797;&#26434;&#22330;&#26223;&#20013;&#20840;&#38754;&#24037;&#20855;&#21033;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;UltraTool&#65292;&#26088;&#22312;&#25913;&#21892;&#21644;&#35780;&#20272;LLMs&#22312;&#23454;&#38469;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#20851;&#27880;&#20174;&#35268;&#21010;&#21644;&#21019;&#24314;&#21040;&#24212;&#29992;&#24037;&#20855;&#30340;&#25972;&#20010;&#36807;&#31243;&#65292;&#24182;&#24378;&#35843;&#23454;&#38469;&#22797;&#26434;&#24615;&#21644;&#22810;&#27493;&#35268;&#21010;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#36235;&#21183;&#24378;&#35843;&#20102;&#23545;&#23427;&#20204;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#35268;&#21010;&#12289;&#21019;&#36896;&#21644;&#20351;&#29992;&#24037;&#20855;&#30340;&#22797;&#26434;&#22330;&#26223;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#36890;&#24120;&#21482;&#20851;&#27880;&#31616;&#21333;&#21512;&#25104;&#30340;&#26597;&#35810;&#65292;&#19981;&#21453;&#26144;&#23454;&#38469;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#22312;&#35780;&#20272;&#24037;&#20855;&#21033;&#29992;&#26041;&#38754;&#30340;&#35270;&#35282;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;UltraTool&#65292;&#26088;&#22312;&#25913;&#21892;&#21644;&#35780;&#20272;LLMs&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24037;&#20855;&#21033;&#29992;&#30340;&#33021;&#21147;&#12290;UltraTool&#20851;&#27880;&#20351;&#29992;&#24037;&#20855;&#30340;&#25972;&#20010;&#36807;&#31243;&#8212;&#8212;&#20174;&#35268;&#21010;&#21644;&#21019;&#24314;&#21040;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#24212;&#29992;&#12290;&#23427;&#24378;&#35843;&#23454;&#38469;&#30340;&#22797;&#26434;&#24615;&#65292;&#35201;&#27714;&#20934;&#30830;&#30340;&#22810;&#27493;&#35268;&#21010;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#38382;&#39064;&#35299;&#20915;&#12290;UltraTool&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#22312;&#24037;&#20855;&#20351;&#29992;&#20043;&#21069;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#29420;&#31435;&#35780;&#20272;&#30340;&#35268;&#21010;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20219;&#21153;&#35299;&#20915;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent trend of using Large Language Models (LLMs) as intelligent agents in real-world applications underscores the necessity for comprehensive evaluations of their capabilities, particularly in complex scenarios involving planning, creating, and using tools. However, existing benchmarks typically focus on simple synthesized queries that do not reflect real-world complexity, thereby offering limited perspectives in evaluating tool utilization. To address this issue, we present UltraTool, a novel benchmark designed to improve and evaluate LLMs' ability in tool utilization within real-world scenarios. UltraTool focuses on the entire process of using tools - from planning and creating to applying them in complex tasks. It emphasizes real-world complexities, demanding accurate, multi-step planning for effective problem-solving. A key feature of UltraTool is its independent evaluation of planning with natural language, which happens before tool usage and simplifies the task solving by m
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#28151;&#21512;&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;LLMs&#32467;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20174;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#26102;&#38388;&#36724;&#29983;&#25104;&#20855;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#25688;&#35201;&#65292;&#36890;&#36807;&#23545;&#26102;&#38388;&#36724;&#30340;&#26102;&#38388;&#25935;&#24863;&#24615;&#21644;&#20030;&#37325;&#26377;&#21147;&#30340;&#25277;&#35937;&#25688;&#35201;&#65292;TH-VAE&#29983;&#25104;&#30340;&#25688;&#35201;&#22312;&#25429;&#25417;&#38543;&#26102;&#38388;&#21464;&#21270;&#26041;&#38754;&#20248;&#20110;&#20165;&#20351;&#29992;LLM&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.16240</link><description>&lt;p&gt;
&#23558;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;LLMs&#32467;&#21512;&#65292;&#23454;&#29616;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#20855;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#26102;&#38388;&#32447;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Combining Hierachical VAEs with LLMs for clinically meaningful timeline summarisation in social media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16240
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28151;&#21512;&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;LLMs&#32467;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20174;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#26102;&#38388;&#36724;&#29983;&#25104;&#20855;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#25688;&#35201;&#65292;&#36890;&#36807;&#23545;&#26102;&#38388;&#36724;&#30340;&#26102;&#38388;&#25935;&#24863;&#24615;&#21644;&#20030;&#37325;&#26377;&#21147;&#30340;&#25277;&#35937;&#25688;&#35201;&#65292;TH-VAE&#29983;&#25104;&#30340;&#25688;&#35201;&#22312;&#25429;&#25417;&#38543;&#26102;&#38388;&#21464;&#21270;&#26041;&#38754;&#20248;&#20110;&#20165;&#20351;&#29992;LLM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#30340;&#25277;&#35937;&#27719;&#24635;&#26041;&#27861;&#65292;&#23558;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;LLMs&#32467;&#21512;&#65288;LlaMA-2&#65289;&#65292;&#20197;&#20174;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#26102;&#38388;&#36724;&#29983;&#25104;&#20855;&#26377;&#20020;&#24202;&#24847;&#20041;&#30340;&#25688;&#35201;&#65292;&#36866;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#12290;&#25688;&#35201;&#32467;&#21512;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#21465;&#36848;&#35266;&#28857;&#65306;&#36890;&#36807;&#21521;&#19987;&#38376;&#30340;&#20020;&#24202;&#25552;&#31034;&#39304;&#36865;&#26469;&#29983;&#25104;&#19987;&#21521;&#20020;&#24202;&#21307;&#29983;&#26377;&#29992;&#30340;&#31532;&#19977;&#20154;&#31216;&#20020;&#24202;&#35265;&#35299;&#65292;&#20197;&#21450;&#37325;&#35201;&#30340;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;TH-VAE&#29983;&#25104;&#29992;&#25143;&#26102;&#38388;&#32447;&#30340;&#20020;&#26102;&#25935;&#24863;&#30340;&#31532;&#19968;&#20154;&#31216;&#25277;&#35937;&#25688;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#19987;&#23478;&#25688;&#35201;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#19982;&#20020;&#24202;&#19987;&#23478;&#30340;&#20154;&#24037;&#35780;&#20272;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#32467;&#26524;&#34920;&#26126;&#36890;&#36807;TH-VAE&#36827;&#34892;&#30340;&#26102;&#38388;&#32447;&#25688;&#35201;&#20250;&#20135;&#29983;&#26356;&#23500;&#26377;&#20020;&#24202;&#25928;&#29992;&#12289;&#26356;&#20855;&#20107;&#23454;&#21644;&#36923;&#36753;&#36830;&#36143;&#24615;&#30340;&#25688;&#35201;&#65292;&#20248;&#20110;&#20165;&#20351;&#29992;LLM&#26041;&#27861;&#25429;&#25417;&#26102;&#38388;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16240v2 Announce Type: replace-cross  Abstract: We introduce a hybrid abstractive summarisation approach combining hierarchical VAE with LLMs (LlaMA-2) to produce clinically meaningful summaries from social media user timelines, appropriate for mental health monitoring. The summaries combine two different narrative points of view: clinical insights in third person useful for a clinician are generated by feeding into an LLM specialised clinical prompts, and importantly, a temporally sensitive abstractive summary of the user's timeline in first person, generated by a novel hierarchical variational autoencoder, TH-VAE. We assess the generated summaries via automatic evaluation against expert summaries and via human evaluation with clinical experts, showing that timeline summarisation by TH-VAE results in more factual and logically coherent summaries rich in clinical utility and superior to LLM-only approaches in capturing changes over time.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#36870;&#36716;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.12192</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#21453;&#21521;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Text Embedding Inversion Security for Multilingual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23884;&#20837;&#36870;&#36716;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#36890;&#24120;&#20197;&#23454;&#25968;&#23884;&#20837;&#34920;&#31034;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23884;&#20837;&#24335;&#26381;&#21153;&#65288;EaaS&#65289;&#30340;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23558;&#25935;&#24863;&#20449;&#24687;&#23384;&#20648;&#20026;&#23884;&#20837;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#28431;&#27934;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#30693;&#36947;&#24213;&#23618;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25991;&#26412;&#20063;&#21487;&#20197;&#20174;&#23884;&#20837;&#20013;&#37325;&#26500;&#12290;&#23613;&#31649;&#24050;&#32463;&#25506;&#35752;&#20102;&#38450;&#24481;&#26426;&#21046;&#65292;&#20294;&#36825;&#20123;&#26426;&#21046;&#19987;&#27880;&#20110;&#33521;&#35821;&#65292;&#20351;&#20854;&#20182;&#35821;&#35328;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#26412;&#25991;&#36890;&#36807;&#22810;&#35821;&#35328;&#23884;&#20837;&#36870;&#36716;&#25506;&#35752;&#20102;LLM&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#40657;&#30418;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#36870;&#36716;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#23427;&#20204;&#21487;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;LLMs&#21487;&#33021;&#26356;&#23481;&#26131;&#21463;&#21040;&#36870;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#22522;&#20110;&#33521;&#35821;&#30340;&#38450;&#24481;&#21487;&#33021;&#26080;&#25928;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25513;&#34109;&#38450;&#24481;&#26041;&#27861;&#65292;&#23545;b&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.12192v2 Announce Type: replace-cross  Abstract: Textual data is often represented as realnumbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be vulnerable to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages vulnerable to attacks. This work explores LLM security through multilingual embedding inversion. We define the problem of black-box multilingual and cross-lingual inversion attacks, and thoroughly explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2401.11323</link><description>&lt;p&gt;
&#36776;&#35782;&#24182;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Identifying and Analyzing Task-Encoding Tokens in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;ICL&#30340;&#24037;&#20316;&#26426;&#21046;&#30340;&#29702;&#35299;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#27169;&#22411;&#22914;&#20309;&#20174;ICL&#28436;&#31034;&#20013;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;&#65292;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#26495;&#26631;&#35760;&#21644;&#20572;&#29992;&#35789;&#26631;&#35760;&#26368;&#23481;&#26131;&#25104;&#20026;&#20219;&#21153;&#32534;&#30721;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#35789;&#27719;&#24847;&#24605;&#12289;&#37325;&#22797;&#21644;&#25991;&#26412;&#26684;&#24335;&#26159;&#36825;&#20123;&#26631;&#35760;&#30340;&#20027;&#35201;&#21306;&#21035;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11323v2 Announce Type: replace  Abstract: In-context learning (ICL) has become an effective solution for few-shot learning in natural language processing. However, our understanding of ICL's working mechanisms is limited, specifically regarding how models learn to perform tasks from ICL demonstrations. For example, unexpectedly large changes in performance can arise from small changes in the prompt, leaving prompt design a largely empirical endeavour. In this paper, we investigate this problem by identifying and analyzing task-encoding tokens on whose representations the task performance depends. Using experiments that ablate the representations of different token types, we find that template and stopword tokens are the most prone to be task-encoding. In addition, we demonstrate experimentally that lexical meaning, repetition, and text formatting are the main distinguishing characteristics of these tokens. Our work sheds light on how large language models (LLMs) learn to per
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#20139;&#27880;&#24847;&#21147;&#26694;&#26550;&#65288;SAPT&#65289;&#65292;&#36890;&#36807;&#20849;&#20139;&#27880;&#24847;&#21147;&#23398;&#20064;&#19982;&#36873;&#25321;&#27169;&#22359;&#23545;&#40784;PET&#23398;&#20064;&#21644;&#36873;&#25321;&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#30693;&#35782;&#36716;&#31227;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08295</link><description>&lt;p&gt;
SAPT&#65306;&#19968;&#31181;&#20849;&#20139;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08295
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#20139;&#27880;&#24847;&#21147;&#26694;&#26550;&#65288;SAPT&#65289;&#65292;&#36890;&#36807;&#20849;&#20139;&#27880;&#24847;&#21147;&#23398;&#20064;&#19982;&#36873;&#25321;&#27169;&#22359;&#23545;&#40784;PET&#23398;&#20064;&#21644;&#36873;&#25321;&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#30693;&#35782;&#36716;&#31227;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#33021;&#21147;&#23545;&#20110;&#22312;&#21160;&#24577;&#19990;&#30028;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#35774;&#35745;&#23398;&#20064;&#27169;&#22359;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#22359;&#33719;&#21462;&#29305;&#23450;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#27169;&#22359;&#36873;&#25321;&#20986;&#30456;&#24212;&#30340;&#36755;&#20837;&#65292;&#26088;&#22312;&#24212;&#23545;CL&#20013;&#30340;&#28798;&#38590;&#24335;&#36951;&#24536;&#21644;&#30693;&#35782;&#36716;&#31227;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#21482;&#35299;&#20915;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#65292;&#24573;&#35270;&#20102;&#36890;&#36807;&#23558;&#20004;&#20010;&#27169;&#22359;&#23545;&#40784;&#26469;&#26377;&#25928;&#21516;&#26102;&#35299;&#20915;&#28798;&#38590;&#24335;&#36951;&#24536;&#21644;&#30693;&#35782;&#36716;&#31227;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20849;&#20139;&#27880;&#24847;&#21147;&#26694;&#26550;&#65288;SAPT&#65289;&#65292;&#36890;&#36807;&#20849;&#20139;&#27880;&#24847;&#21147;&#23398;&#20064;&#19982;&#36873;&#25321;&#27169;&#22359;&#26469;&#23545;&#40784;PET&#23398;&#20064;&#21644;&#36873;&#25321;&#12290;&#22312;&#20004;&#20010;CL&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;SAPT&#30340;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#24403;&#25105;&#20204;&#23558;&#20854;&#25193;&#23637;&#21040;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#26102;&#65292;SAPT&#19968;&#30452;&#23637;&#29616;&#20986;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08295v2 Announce Type: replace  Abstract: The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Existing methods devise the learning module to acquire task-specific knowledge with parameter-efficient tuning (PET) block and the selection module to pick out the corresponding one for the testing input, aiming at handling the challenges of catastrophic forgetting and knowledge transfer in CL. However, these methods tend to address only one of the challenges, ignoring the potential of aligning the two modules to effectively address catastrophic forgetting and knowledge transfer simultaneously. To this end, we propose a novel Shared Attention Framework (SAPT), to align the PET learning and selection via the Shared Attentive Learning \&amp; Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates its superiority when we scale it to different model si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;Python&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;&#25968;&#23398;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08190</link><description>&lt;p&gt;
MARIO: &#20855;&#26377;Python&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;&#25968;&#23398;&#25512;&#29702;&#36755;&#20986;--&#21487;&#37325;&#22797;&#30340;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;Python&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;&#25968;&#23398;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#33719;&#24471;&#30495;&#27491;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38656;&#35201;&#22635;&#34917;&#30340;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#30340;&#32570;&#38519;&#12290;&#26412;&#25991;&#36890;&#36807;&#20016;&#23500;&#25968;&#25454;&#26223;&#35266;&#21644;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23398;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#25968;&#25454;&#38598;&#22686;&#21152;&#20102;&#20351;&#29992;Python&#20195;&#30721;&#35299;&#37322;&#22120;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08190v2 Announce Type: replace  Abstract: Large language models (LLMs) have seen considerable advancements in natural language understanding tasks, yet there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints. In this paper, we address this challenge by enriching the data landscape and introducing a novel math dataset, enhanced with a capability to utilize a Python code interpreter. This dataset is derived from GSM8K and MATH and has been further refined through a combination of GPT-4 annotations, human review, and self-training processes, where the errors in the original GSM8K training set have been fixed. Additiona
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#20998;&#35299;&#20026;&#35745;&#21010;&#22120;&#12289;&#35843;&#29992;&#22120;&#21644;&#24635;&#32467;&#22120;&#27169;&#22359;&#65292;&#20197;&#20811;&#26381;&#23567;&#22411;&#27169;&#22411;&#24615;&#33021;&#38480;&#21046;&#21644;&#24037;&#20855;&#26356;&#26032;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.07324</link><description>&lt;p&gt;
&#23567;&#22411;LLMs&#26159;&#24369;&#24037;&#20855;&#23398;&#20064;&#32773;&#65306;&#22810;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Small LLMs Are Weak Tool Learners: A Multi-LLM Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#20998;&#35299;&#20026;&#35745;&#21010;&#22120;&#12289;&#35843;&#29992;&#22120;&#21644;&#24635;&#32467;&#22120;&#27169;&#22359;&#65292;&#20197;&#20811;&#26381;&#23567;&#22411;&#27169;&#22411;&#24615;&#33021;&#38480;&#21046;&#21644;&#24037;&#20855;&#26356;&#26032;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#22823;&#22823;&#25193;&#23637;&#20102;&#29420;&#31435;LLMs&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;&#20363;&#22914;API&#65292;&#20989;&#25968;&#65289;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#33258;&#20027;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#24037;&#20855;&#20351;&#29992;&#30340;&#25361;&#25112;&#35201;&#27714;LLMs&#19981;&#20165;&#33021;&#29702;&#35299;&#29992;&#25143;&#26597;&#35810;&#24182;&#29983;&#25104;&#31572;&#26696;&#65292;&#36824;&#35201;&#22312;&#20219;&#21153;&#35268;&#21010;&#12289;&#35760;&#24518;&#31649;&#29702;&#12289;&#24037;&#20855;&#35843;&#29992;&#21644;&#32467;&#26524;&#24635;&#32467;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20256;&#32479;&#26041;&#27861;&#38598;&#20013;&#20110;&#35757;&#32451;&#21333;&#20010;&#20855;&#22791;&#25152;&#26377;&#36825;&#20123;&#21151;&#33021;&#30340;LLM&#65292;&#20294;&#22312;&#23567;&#22411;&#27169;&#22411;&#19978;&#20250;&#20986;&#29616;&#24615;&#33021;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#27492;&#22806;&#65292;&#24403;&#24037;&#20855;&#26356;&#26032;&#26102;&#65292;&#25972;&#20010;LLM&#21487;&#33021;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#19978;&#36848;&#33021;&#21147;&#20998;&#35299;&#20026;&#35745;&#21010;&#22120;&#12289;&#35843;&#29992;&#22120;&#21644;&#24635;&#32467;&#22120;&#12290;&#27599;&#20010;&#32452;&#20214;&#30001;&#19968;&#20010;&#21333;&#29420;&#30340;LLM&#23454;&#29616;&#65292;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#24182;&#19982;&#20854;&#20182;&#32452;&#20214;&#21512;&#20316;&#23436;&#25104;&#20219;&#21153;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#26694;&#26550;&#20415;&#20110;&#36827;&#34892;&#20010;&#20307;&#26356;&#26032;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete complex tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers but also excel in task planning, memory management, tool invocation, and result summarization. While traditional approaches focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Moreover, the entire LLM may require retraining when tools are updated. To overcome these challenges, we propose a novel strategy that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with other components to accomplish the task. This modular framework facilitates individual updates and t
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;RoleEval&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35282;&#33394;&#30693;&#35782;&#35760;&#24518;&#12289;&#21033;&#29992;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#21452;&#35821;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;300&#21517;&#26377;&#24433;&#21709;&#21147;&#30340;&#20154;&#29289;&#21644;&#34394;&#26500;&#35282;&#33394;&#65292;&#21253;&#25324;6000&#36947;&#20013;&#33521;&#25991;&#24179;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#25506;&#31350;&#20010;&#20154;&#20449;&#24687;&#12289;&#20851;&#31995;&#12289;&#33021;&#21147;&#21644;&#32463;&#21382;&#31561;&#21508;&#20010;&#26041;&#38754;</title><link>https://arxiv.org/abs/2312.16132</link><description>&lt;p&gt;
RoleEval&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#35821;&#35282;&#33394;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RoleEval: A Bilingual Role Evaluation Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16132
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;RoleEval&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35282;&#33394;&#30693;&#35782;&#35760;&#24518;&#12289;&#21033;&#29992;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#21452;&#35821;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;300&#21517;&#26377;&#24433;&#21709;&#21147;&#30340;&#20154;&#29289;&#21644;&#34394;&#26500;&#35282;&#33394;&#65292;&#21253;&#25324;6000&#36947;&#20013;&#33521;&#25991;&#24179;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#25506;&#31350;&#20010;&#20154;&#20449;&#24687;&#12289;&#20851;&#31995;&#12289;&#33021;&#21147;&#21644;&#32463;&#21382;&#31561;&#21508;&#20010;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#38656;&#35201;&#26377;&#25928;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#20854;&#35282;&#33394;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#32852;&#31995;&#24182;&#25552;&#20379;&#26356;&#20855;&#27785;&#28024;&#24863;&#30340;&#20114;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RoleEval&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#35282;&#33394;&#30693;&#35782;&#30340;&#35760;&#24518;&#12289;&#21033;&#29992;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#21452;&#35821;&#22522;&#20934;&#12290;RoleEval&#21253;&#25324;RoleEval-Global&#65288;&#21253;&#25324;&#22269;&#38469;&#20844;&#35748;&#30340;&#35282;&#33394;&#65289;&#21644;RoleEval-Chinese&#65288;&#21253;&#25324;&#20013;&#22269;&#27969;&#34892;&#35282;&#33394;&#65289;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#65288;&#21253;&#25324;&#21517;&#20154;&#12289;&#21160;&#28459;&#12289;&#28459;&#30011;&#12289;&#30005;&#24433;&#12289;&#30005;&#35270;&#21095;&#12289;&#28216;&#25103;&#21644;&#23567;&#35828;&#65289;&#30340;300&#21517;&#26377;&#24433;&#21709;&#21147;&#30340;&#20154;&#29289;&#21644;&#34394;&#26500;&#35282;&#33394;&#65292;&#20849;6000&#36947;&#20013;&#33521;&#25991;&#24179;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;&#20102;&#22522;&#26412;&#30693;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#33021;&#21147;&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#25506;&#31350;&#20010;&#20154;&#20449;&#24687;&#12289;&#20851;&#31995;&#12289;&#33021;&#21147;&#21644;&#32463;&#21382;&#31561;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16132v2 Announce Type: replace  Abstract: The rapid evolution of large language models necessitates effective benchmarks for evaluating their role knowledge, which is essential for establishing connections with the real world and providing more immersive interactions. This paper introduces RoleEval, a bilingual benchmark designed to assess the memorization, utilization, and reasoning capabilities of role knowledge. RoleEval comprises RoleEval-Global (including internationally recognized characters) and RoleEval-Chinese (including characters popular in China), with 6,000 Chinese-English parallel multiple-choice questions focusing on 300 influential people and fictional characters drawn from a variety of domains including celebrities, anime, comics, movies, TV series, games, and fictions. These questions cover basic knowledge and multi-hop reasoning abilities, aiming to systematically probe various aspects such as personal information, relationships, abilities, and experiences
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26032;VQA&#25968;&#25454;&#38598;BloomVQA&#65292;&#22522;&#20110;Bloom&#30340;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#22270;&#34920;&#31034;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20272;&#65292;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#32423;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2312.12716</link><description>&lt;p&gt;
BloomVQA&#65306;&#35780;&#20272;&#20998;&#23618;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
BloomVQA: Assessing Hierarchical Multi-modal Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12716
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26032;VQA&#25968;&#25454;&#38598;BloomVQA&#65292;&#22522;&#20110;Bloom&#30340;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#22270;&#34920;&#31034;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20272;&#65292;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#32423;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;VQA&#25968;&#25454;&#38598;BloomVQA&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#19982;&#24403;&#21069;&#30340;&#22522;&#20934;&#19981;&#21516;&#65292;&#23427;&#20204;&#36890;&#24120;&#20391;&#37325;&#20110;&#22522;&#20110;&#20107;&#23454;&#30340;&#35760;&#24518;&#21644;&#27809;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#31616;&#21333;&#25512;&#29702;&#20219;&#21153;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#22522;&#20110;&#22270;&#29255;&#25925;&#20107;&#30340;&#22810;&#39033;&#36873;&#25321;&#26679;&#26412;&#65292;&#21453;&#26144;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;&#29702;&#35299;&#65292;&#27491;&#22914;&#24067;&#40065;&#22982;&#30340;&#20998;&#31867;&#27861;&#25152;&#23637;&#31034;&#30340;&#65292;&#22312;&#25945;&#32946;&#30740;&#31350;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#32463;&#20856;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26144;&#23556;&#21040;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#22270;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#21644;&#34920;&#24449;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#26032;&#25514;&#26045;&#12290;&#25105;&#20204;&#23545;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#32423;&#35780;&#20272;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;&#19982;&#20302;&#32423;&#20219;&#21153;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38656;&#35201;&#39640;&#32423;&#29702;&#35299;&#21644;&#35748;&#30693;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19979;&#38477;&#65292;VQA&#20934;&#30830;&#24615;&#19979;&#38477;&#20102;&#39640;&#36798;38.0%&#12290;&#19982;&#26089;&#26399;&#27169;&#22411;&#30456;&#27604;&#65292;GPT-4V&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12716v2 Announce Type: replace-cross  Abstract: We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive evaluation of large vision-language models on comprehension tasks. Unlike current benchmarks that often focus on fact-based memorization and simple reasoning tasks without theoretical grounding, we collect multiple-choice samples based on picture stories that reflect different levels of comprehension, as laid out in Bloom's Taxonomy, a classic framework for learning assessment widely adopted in education research. Our data maps to a novel hierarchical graph representation which enables automatic data augmentation and novel measures characterizing model consistency. We perform graded evaluation and reliability analysis on recent multi-modal models. In comparison to low-level tasks, we observe decreased performance on tasks requiring advanced comprehension and cognitive skills with up to 38.0% drop in VQA accuracy. In comparison to earlier models, GPT-4V demons
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;--SemiDQG&#65292;&#36890;&#36807;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#35757;&#32451;&#21709;&#24212;&#22686;&#24378;&#30340;&#26597;&#35810;&#29983;&#25104;&#22120; (RA)&#12290;</title><link>https://arxiv.org/abs/2312.12713</link><description>&lt;p&gt;
&#21709;&#24212;&#22686;&#24378;&#30340;&#21322;&#30417;&#30563;&#23545;&#35805;&#26597;&#35810;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Response Enhanced Semi-supervised Dialogue Query Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12713
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;--SemiDQG&#65292;&#36890;&#36807;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#26469;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#35757;&#32451;&#21709;&#24212;&#22686;&#24378;&#30340;&#26597;&#35810;&#29983;&#25104;&#22120; (RA)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20114;&#32852;&#32593;&#33719;&#21462;&#24222;&#22823;&#19988;&#19981;&#26029;&#26356;&#26032;&#30340;&#30693;&#35782;&#34987;&#35748;&#20026;&#26159;&#23545;&#35805;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#29983;&#25104;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#30340;&#25628;&#32034;&#26597;&#35810;&#32780;&#25552;&#20986;&#20102;&#23545;&#35805;&#26597;&#35810;&#29983;&#25104;&#20219;&#21153;&#65292;&#36825;&#20123;&#26597;&#35810;&#23558;&#34987;&#25552;&#20132;&#21040;&#25628;&#32034;&#24341;&#25806;&#20197;&#26816;&#32034;&#20114;&#32852;&#32593;&#19978;&#30456;&#20851;&#30340;&#32593;&#31449;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#39046;&#22495;&#36866;&#24212;&#24615;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550; - SemiDQG&#65292;&#36890;&#36807;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#22522;&#20110;&#25628;&#32034;&#26597;&#35810;&#36890;&#24120;&#19982;&#23545;&#35805;&#21709;&#24212;&#20027;&#39064;&#30456;&#20851;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#21709;&#24212;&#22686;&#24378;&#30340;&#26597;&#35810;&#29983;&#25104;&#22120;&#65288;RA&#65289;&#26469;&#25552;&#20379;&#20016;&#23500;&#19988;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12713v2 Announce Type: replace-cross  Abstract: Leveraging vast and continually updated knowledge from the Internet has been considered an important ability for a dialogue system. Therefore, the dialogue query generation task is proposed for generating search queries from dialogue histories, which will be submitted to a search engine for retrieving relevant websites on the Internet. In this regard, previous efforts were devoted to collecting conversations with annotated queries and training a query producer (QP) via standard supervised learning. However, these studies still face the challenges of data scarcity and domain adaptation. To address these issues, in this paper, we propose a semi-supervised learning framework -- SemiDQG, to improve model performance with unlabeled conversations. Based on the observation that the search query is typically related to the topic of dialogue response, we train a response-augmented query producer (RA) to provide rich and effective traini
&lt;/p&gt;</description></item><item><title>KGLens &#26159;&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;&#30693;&#35782;&#22270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#30340;&#26694;&#26550;&#65292;&#24110;&#21161;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;&#30693;&#35782;&#22270;&#30340;&#30693;&#35782;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2312.11539</link><description>&lt;p&gt;
KGLens&#65306;&#19968;&#20010;&#21442;&#25968;&#21270;&#30693;&#35782;&#22270;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#30693;&#36947;&#21644;&#19981;&#30693;&#36947;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11539
&lt;/p&gt;
&lt;p&gt;
KGLens &#26159;&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;&#30693;&#35782;&#22270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#30340;&#26694;&#26550;&#65292;&#24110;&#21161;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;&#30693;&#35782;&#22270;&#30340;&#30693;&#35782;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#26159;&#35780;&#20272;&#20107;&#23454;&#24615;&#24182;&#35782;&#21035;LLMs&#30340;&#30693;&#35782;&#30450;&#28857;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23558;KGs&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#39640;&#25928;&#35780;&#20272;&#36825;&#20123;&#24191;&#27867;&#19988;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KGLens--&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;KGs&#21644;LLMs&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#65292;&#24182;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;KGs&#30340;&#30693;&#35782;&#32570;&#38519;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;KGLens&#20855;&#26377;&#19968;&#20010;&#22270;&#24341;&#23548;&#30340;&#38382;&#39064;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23558;KGs&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21442;&#25968;&#21270;KG&#32467;&#26500;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#21152;&#24555;KG&#30340;&#36941;&#21382;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Wikidata&#30340;&#19977;&#20010;&#39046;&#22495;&#29305;&#23450;KG&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20123;KG&#21253;&#25324;&#36229;&#36807;19,000&#26465;&#36793;&#65292;700&#20010;&#20851;&#31995;&#21644;21,000&#20010;&#23454;&#20307;&#12290;&#25105;&#20204;&#36328;&#36234;8&#20010;LLMs&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;KGLens&#19981;&#20165;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11539v2 Announce Type: replace  Abstract: Measuring the alignment between a Knowledge Graph (KG) and Large Language Models (LLMs) is an effective method to assess the factualness and identify the knowledge blind spots of LLMs. However, this approach encounters two primary challenges including the translation of KGs into natural language and the efficient evaluation of these extensive and complex structures. In this paper, we present KGLens--a novel framework aimed at measuring the alignment between KGs and LLMs, and pinpointing the LLMs' knowledge deficiencies relative to KGs. KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed sampling strategy based on parameterized KG structure to expedite KG traversal. We conducted experiments using three domain-specific KGs from Wikidata, which comprise over 19,000 edges, 700 relations, and 21,000 entities. Our analysis across eight LLMs reveals that KGLens not only
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Cascade Speculative Drafting&#65288;CS Drafting&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#22402;&#30452;&#32423;&#32852;&#28040;&#38500;&#31070;&#32463;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#36890;&#36807;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11462</link><description>&lt;p&gt;
&#29992;&#20110;&#26356;&#24555;&#30340;LLM&#25512;&#29702;&#30340;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;
&lt;/p&gt;
&lt;p&gt;
Cascade Speculative Drafting for Even Faster LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11462
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Cascade Speculative Drafting&#65288;CS Drafting&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#22402;&#30452;&#32423;&#32852;&#28040;&#38500;&#31070;&#32463;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#36890;&#36807;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#25928;&#29575;&#30340;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;&#65292;&#36890;&#36807;&#36739;&#23567;&#30340;&#27169;&#22411;&#29983;&#25104;&#33609;&#31295;&#26469;&#36816;&#20316;&#12290;&#36739;&#22823;&#30340;&#30446;&#26631;&#27169;&#22411;&#28982;&#21518;&#26597;&#30475;&#36825;&#20010;&#33609;&#31295;&#20197;&#19982;&#20854;&#36755;&#20986;&#23545;&#40784;&#65292;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#20309;&#25509;&#21463;&#37117;&#23558;&#20943;&#23569;&#30446;&#26631;&#27169;&#22411;&#36816;&#34892;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#32423;&#32852;&#25512;&#27979;&#30340;&#33609;&#22270;&#36807;&#31243;&#20013;&#21253;&#25324;&#32531;&#24930;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#20026;&#29983;&#25104;&#30340;&#26631;&#35760;&#20998;&#37197;&#30456;&#21516;&#30340;&#26102;&#38388;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#20302;&#25928;&#24615;&#20849;&#21516;&#23548;&#33268;&#32423;&#32852;&#25512;&#27979;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#21892;LLM&#25512;&#29702;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;&#65288;CS Drafting&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25972;&#21512;&#20102;&#20004;&#31181;&#32423;&#32852;&#31867;&#22411;&#30340;&#25512;&#27979;&#25191;&#34892;&#31639;&#27861;&#12290;&#22402;&#30452;&#32423;&#32852;&#20174;&#31070;&#32463;&#27169;&#22411;&#20013;&#28040;&#38500;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#32780;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#20102;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11462v3 Announce Type: replace-cross  Abstract: Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in draft
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;Split and Rephrase&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#34920;&#26126;&#22312;&#20027;&#35201;&#25351;&#26631;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#22312;&#20998;&#21106;&#19968;&#33268;&#24615;&#26041;&#38754;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2312.11075</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20998;&#21106;&#19982;&#37325;&#36848;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Split and Rephrase with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11075
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;Split and Rephrase&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#34920;&#26126;&#22312;&#20027;&#35201;&#25351;&#26631;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#22312;&#20998;&#21106;&#19968;&#33268;&#24615;&#26041;&#38754;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Split and Rephrase (SPRP)&#20219;&#21153;&#26088;&#22312;&#23558;&#22797;&#26434;&#21477;&#23376;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26356;&#30701;&#30340;&#31526;&#21512;&#35821;&#27861;&#35268;&#21017;&#30340;&#21477;&#23376;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#21547;&#20041;&#65292;&#26377;&#21161;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#22788;&#29702;&#22797;&#26434;&#25991;&#26412;&#12290;&#36825;&#20063;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#22240;&#20026;&#20854;&#38656;&#35201;&#23545;&#22797;&#26434;&#30340;&#35821;&#27861;&#26041;&#38754;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26174;&#31034;&#23427;&#20204;&#21487;&#20197;&#22312;&#20027;&#35201;&#25351;&#26631;&#19978;&#27604;&#29616;&#26377;&#25216;&#26415;&#26377;&#24456;&#22823;&#25913;&#36827;&#65292;&#23613;&#31649;&#22312;&#25286;&#20998;&#19968;&#33268;&#24615;&#26041;&#38754;&#20173;&#26377;&#24046;&#36317;&#12290;&#26469;&#33258;&#20004;&#39033;&#20154;&#31867;&#35780;&#20272;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#25903;&#25345;&#33258;&#21160;&#24230;&#37327;&#32467;&#26524;&#24471;&#20986;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#25552;&#31034;&#21464;&#20307;&#12289;&#39046;&#22495;&#36716;&#31227;&#12289;&#21442;&#25968;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#19981;&#21516;&#30340;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#19982;&#25351;&#23548;&#35843;&#25972;&#30340;&#38646;&#23556;&#21644;&#23569;&#23556;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11075v3 Announce Type: replace  Abstract: The Split and Rephrase (SPRP) task, which consists in splitting complex sentences into a sequence of shorter grammatical sentences, while preserving the original meaning, can facilitate the processing of complex texts for humans and machines alike. It is also a valuable testbed to evaluate natural language processing models, as it requires modelling complex grammatical aspects. In this work, we evaluate large language models on the task, showing that they can provide large improvements over the state of the art on the main metrics, although still lagging in terms of splitting compliance. Results from two human evaluations further support the conclusions drawn from automated metric results. We provide a comprehensive study that includes prompting variants, domain shift, fine-tuned pretrained language models of varying parameter size and training data volumes, contrasted with both zero-shot and few-shot approaches on instruction-tuned 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30828;&#20214;&#39640;&#25928;&#24615;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#21487;&#22312;&#30701;&#24207;&#21015;&#38271;&#24230;&#19979;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#65292;&#21516;&#26102;&#25512;&#24191;&#21040;&#20102;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#38376;&#30340;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2312.06635</link><description>&lt;p&gt;
&#20855;&#26377;&#30828;&#20214;&#39640;&#25928;&#35757;&#32451;&#30340;&#38376;&#25511;&#32447;&#24615;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Gated Linear Attention Transformers with Hardware-Efficient Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06635
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30828;&#20214;&#39640;&#25928;&#24615;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#21487;&#22312;&#30701;&#24207;&#21015;&#38271;&#24230;&#19979;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#65292;&#21516;&#26102;&#25512;&#24191;&#21040;&#20102;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#38376;&#30340;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#21464;&#21387;&#22120;&#20801;&#35768;&#36827;&#34892;&#39640;&#25928;&#30340;&#24182;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#20855;&#26377;2D&#65288;&#30697;&#38453;&#20540;&#65289;&#38544;&#34255;&#29366;&#24577;&#30340;RNN&#65292;&#20174;&#32780;&#20139;&#21463;&#32447;&#24615;&#26102;&#38388;&#25512;&#26029;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;&#36890;&#24120;&#34920;&#29616;&#19981;&#22914;&#26222;&#36890;softmax&#27880;&#24847;&#21147;&#12290;&#32780;&#19988;&#65292;&#24403;&#21069;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#32570;&#20047;I/O&#24863;&#30693;&#24615;&#65292;&#22240;&#27492;&#27604;&#39640;&#24230;&#20248;&#21270;&#30340;softmax&#27880;&#24847;&#21147;&#23454;&#29616;&#26356;&#24930;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#30828;&#20214;&#39640;&#25928;&#31639;&#27861;&#65292;&#23427;&#22312;&#20869;&#23384;&#31227;&#21160;&#21644;&#21487;&#24182;&#34892;&#24615;&#20043;&#38388;&#36827;&#34892;&#25240;&#20013;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#23454;&#29616;&#65292;&#34987;&#31216;&#20026;FLASHLINEARATTENTION&#65292;&#22312;&#30701;&#24207;&#21015;&#38271;&#24230;&#65288;&#20363;&#22914;&#65292;1K&#65289;&#19979;&#65292;&#21363;&#20351;&#20316;&#20026;&#21333;&#29420;&#30340;&#23618;&#20063;&#27604;FLASHATTENTION-2(Dao, 2023)&#26356;&#24555;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#25512;&#24191;&#21040;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#38376;&#30340;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#21464;&#20307;&#12290;&#24403;&#29992;&#20316;&#21464;&#25442;&#22120;&#20013;&#26631;&#20934;&#27880;&#24847;&#21147;&#23618;&#30340;&#26367;&#20195;&#26102;&#65292;&#20135;&#29983;&#30340;&#38376;&#25511;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06635v4 Announce Type: replace-cross  Abstract: Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2(Dao, 2023) as a standalone layer even at short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gate
&lt;/p&gt;</description></item><item><title>&#22240;&#26524;ATE&#26041;&#27861;&#21487;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#24847;&#22806;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22312;&#27602;&#24615;&#32531;&#35299;&#38382;&#39064;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2311.11229</link><description>&lt;p&gt;
&#22240;&#26524;ATE&#20943;&#36731;&#20102;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24847;&#22806;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Causal ATE Mitigates Unintended Bias in Controlled Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11229
&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;ATE&#26041;&#27861;&#21487;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#24847;&#22806;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22312;&#27602;&#24615;&#32531;&#35299;&#38382;&#39064;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;Causal ATE&#65289;&#26041;&#27861;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#12290;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#23646;&#24615;&#25511;&#21046;&#20219;&#21153;&#65292;&#20250;&#26816;&#26597;&#21477;&#23376;&#20013;&#21333;&#35789;&#19982;&#24863;&#20852;&#36259;&#23646;&#24615;&#30340;&#20849;&#29616;&#24773;&#20917;&#65292;&#24182;&#21152;&#20197;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#65292;&#21333;&#35789;&#19982;&#23646;&#24615;&#20043;&#38388;&#30340;&#20266;&#30456;&#20851;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#20986;&#29616;&#23646;&#24615;&#23384;&#22312;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#22240;&#26524;ATE&#26041;&#27861;&#28040;&#38500;&#20102;&#36825;&#31181;&#24847;&#22806;&#25928;&#24212;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#22312;&#27602;&#24615;&#32531;&#35299;&#38382;&#39064;&#19978;&#65292;&#27602;&#24615;&#32531;&#35299;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#22312;&#20110;&#22312;&#21435;&#27602;&#21518;&#32463;&#24120;&#20986;&#29616;&#23545;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#26080;&#24847;&#20559;&#35265;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#22240;&#26524;ATE&#24230;&#37327;&#21487;&#20197;&#35299;&#20915;&#36825;&#31181;&#24847;&#22806;&#20559;&#24046;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11229v2 Announce Type: replace  Abstract: We study attribute control in language models through the method of Causal Average Treatment Effect (Causal ATE). Existing methods for the attribute control task in Language Models (LMs) check for the co-occurrence of words in a sentence with the attribute of interest, and control for them. However, spurious correlation of the words with the attribute in the training dataset, can cause models to hallucinate the presence of the attribute when presented with the spurious correlate during inference. We show that the simple perturbation-based method of Causal ATE removes this unintended effect. Specifically, we ground it in the problem of toxicity mitigation, where a significant challenge lies in the inadvertent bias that often emerges towards protected groups post detoxification. We show that this unintended bias can be solved by the use of the Causal ATE metric and rigorously prove our claim. We provide experimental validations for our
&lt;/p&gt;</description></item><item><title>WatME&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#20887;&#20313;&#30340;&#35821;&#35328;&#20808;&#39564;&#30693;&#35782;&#65292;&#21160;&#24577;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#35789;&#27719;&#20351;&#29992;&#65292;&#36991;&#20813;&#36866;&#24403;&#35789;&#27719;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#32500;&#25345;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.09832</link><description>&lt;p&gt;
WatME&#65306;&#36890;&#36807;&#35789;&#27719;&#20887;&#20313;&#23454;&#29616;&#26080;&#25439;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
WatME: Towards Lossless Watermarking Through Lexical Redundancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09832
&lt;/p&gt;
&lt;p&gt;
WatME&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#20887;&#20313;&#30340;&#35821;&#35328;&#20808;&#39564;&#30693;&#35782;&#65292;&#21160;&#24577;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#35789;&#27719;&#20351;&#29992;&#65292;&#36991;&#20813;&#36866;&#24403;&#35789;&#27719;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#32500;&#25345;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09832v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#12290;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#20351;&#29992;&#20219;&#24847;&#30340;&#35789;&#27719;&#20998;&#21106;&#65292;&#23548;&#33268;&#22312;&#21709;&#24212;&#29983;&#25104;&#36807;&#31243;&#20013;&#32570;&#20047;&#36866;&#24403;&#30340;&#35789;&#27719;&#65292;&#24182;&#30772;&#22351;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#65292;&#20005;&#37325;&#38477;&#20302;&#20102;&#25991;&#26412;&#21709;&#24212;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20114;&#26021;&#24335;&#27700;&#21360;&#65288;WatME&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#21033;&#29992;&#22266;&#26377;&#35789;&#27719;&#20887;&#20313;&#30340;&#35821;&#35328;&#20808;&#39564;&#30693;&#35782;&#65292;WatME &#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#20013;&#21160;&#24577;&#20248;&#21270;&#21487;&#29992;&#35789;&#27719;&#30340;&#20351;&#29992;&#12290;&#23427;&#37319;&#29992;&#20114;&#26021;&#35268;&#21017;&#26469;&#31649;&#29702;&#36825;&#31181;&#20887;&#20313;&#65292;&#36991;&#20813;&#20102;&#36866;&#24403;&#30340;&#35789;&#27719;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;WatME&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09832v2 Announce Type: replace  Abstract: Text watermarking has emerged as an important technique for detecting machine-generated text. However, existing methods generally use arbitrary vocabulary partitioning during decoding, which results in the absence of appropriate words during the response generation and disrupts the language model's expressiveness, thus severely degrading the quality of text response. To address these issues, we introduce a novel approach, Watermarking with Mutual Exclusion (WatME). Specifically, by leveraging linguistic prior knowledge of inherent lexical redundancy, WatME can dynamically optimize the use of available vocabulary during the decoding process of language models. It employs a mutually exclusive rule to manage this redundancy, avoiding situations where appropriate words are unavailable and maintaining the expressive power of large language models (LLMs). We present theoretical analysis and empirical evidence demonstrating that WatME subst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32570;&#20047;&#36275;&#22815;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#34920;&#36798;&#23545;&#36229;&#20986;&#20854;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#35802;&#23454;&#19982;&#24110;&#21161;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2311.09731</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#34920;&#36798;&#23545;&#36229;&#20986;&#21442;&#25968;&#21270;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32570;&#20047;&#36275;&#22815;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#34920;&#36798;&#23545;&#36229;&#20986;&#20854;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#35802;&#23454;&#19982;&#24110;&#21161;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#31995;&#32479;&#22320;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32570;&#20047;&#36275;&#22815;&#21442;&#25968;&#21270;&#30693;&#35782;&#20197;&#29983;&#25104;&#21512;&#29702;&#22238;&#24212;&#30340;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#65292;&#24378;&#35843;&#35802;&#23454;&#19982;&#24110;&#21161;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#31934;&#30830;&#30830;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#30333;&#25361;&#25112;&#65292;&#25105;&#20204;&#35786;&#26029;&#24615;&#22320;&#21019;&#24314;&#20102;&#21253;&#21547;&#19981;&#23384;&#22312;&#27010;&#24565;&#25110;&#38169;&#35823;&#21069;&#25552;&#30340;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#23427;&#20204;&#36229;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#32534;&#21046;&#19968;&#20010;&#21253;&#21547;&#26082;&#26377;&#26080;&#27861;&#22238;&#31572;&#20063;&#26377;&#21487;&#22238;&#31572;&#38382;&#39064;&#30340;&#22522;&#20934;&#65292;UnknownBench&#65292;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25345;&#35802;&#23454;&#30340;&#21516;&#26102;&#25552;&#20379;&#24110;&#21161;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#32479;&#19968;&#20449;&#24515;&#24341;&#23548;&#26041;&#27861;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33268;&#25298;&#32477;&#25110;&#34920;&#36798;&#23545;&#36229;&#20986;&#20854;&#21442;&#25968;&#21270;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09731v2 Announce Type: replace-cross  Abstract: Can large language models (LLMs) express their uncertainty in situations where they lack sufficient parametric knowledge to generate reasonable responses? This work aims to systematically investigate LLMs' behaviors in such situations, emphasizing the trade-off between honesty and helpfulness. To tackle the challenge of precisely determining LLMs' knowledge gaps, we diagnostically create unanswerable questions containing non-existent concepts or false premises, ensuring that they are outside the LLMs' vast training data. By compiling a benchmark, UnknownBench, which consists of both unanswerable and answerable questions, we quantitatively evaluate the LLMs' performance in maintaining honesty while being helpful. Using a model-agnostic unified confidence elicitation approach, we observe that most LLMs fail to consistently refuse or express uncertainty towards questions outside their parametric knowledge, although instruction fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#22312;&#25198;&#28436;&#20826;&#27966;&#35282;&#33394;&#26102;&#65292;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#32676;&#20307;&#30340;&#20826;&#27966;&#20559;&#35265;&#12289;&#24182;&#36890;&#36807;&#21830;&#35752;&#25910;&#25947;&#21040;&#26356;&#20934;&#30830;&#20449;&#24565;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.09665</link><description>&lt;p&gt;
&#20826;&#27966;&#32676;&#20307;&#30340;&#26234;&#24935;&#65306;&#27604;&#36739;&#20154;&#31867;&#21644;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#38598;&#20307;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#22312;&#25198;&#28436;&#20826;&#27966;&#35282;&#33394;&#26102;&#65292;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#32676;&#20307;&#30340;&#20826;&#27966;&#20559;&#35265;&#12289;&#24182;&#36890;&#36807;&#21830;&#35752;&#25910;&#25947;&#21040;&#26356;&#20934;&#30830;&#20449;&#24565;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#32676;&#20307;&#33021;&#22815;&#36890;&#36807;&#21830;&#35752;&#36798;&#25104;&#26356;&#20934;&#30830;&#30340;&#20449;&#24565;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#26497;&#21270;&#21644;&#20826;&#27966;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#65292;&#36825;&#19968;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#20826;&#27966;&#32676;&#20307;&#30340;&#26234;&#24935;&#8221;&#12290;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#29983;&#25104;&#20195;&#29702;&#20154;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#26469;&#27169;&#25311;&#20154;&#31867;&#38598;&#20307;&#34892;&#20026;&#65292;&#28982;&#32780;&#24456;&#23569;&#26377;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#23427;&#20204;&#30340;&#21160;&#24577;&#19982;&#20154;&#31867;&#32676;&#20307;&#34892;&#20026;&#30340;&#23545;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25552;&#31034;&#25198;&#28436;&#20826;&#27966;&#20154;&#29289;&#65288;&#20363;&#22914;&#65292;&#27665;&#20027;&#20826;&#20154;&#25110;&#20849;&#21644;&#20826;&#20154;&#65289;&#30340;LLM&#20195;&#29702;&#20154;&#32676;&#20307;&#20013;&#65292;&#20826;&#27966;&#32676;&#20307;&#30340;&#26234;&#24935;&#20986;&#29616;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#19981;&#20165;&#26174;&#31034;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#20826;&#27966;&#20559;&#35265;&#65292;&#32780;&#19988;&#36890;&#36807;&#21830;&#35752;&#20687;&#20154;&#31867;&#19968;&#26679;&#25910;&#25947;&#21040;&#26356;&#20934;&#30830;&#30340;&#20449;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#24178;&#25200;&#25910;&#25947;&#30340;&#22240;&#32032;&#65292;&#21253;&#25324;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#30340;&#20351;&#29992;&#21644;&#20154;&#29289;&#32570;&#20047;&#32454;&#33410;&#12290;&#30456;&#21453;&#65292;&#23545;&#20154;&#31867;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#20284;&#20046;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09665v2 Announce Type: replace  Abstract: Human groups are able to converge on more accurate beliefs through deliberation, even in the presence of polarization and partisan bias -- a phenomenon known as the "wisdom of partisan crowds." Generated agents powered by Large Language Models (LLMs) are increasingly used to simulate human collective behavior, yet few benchmarks exist for evaluating their dynamics against the behavior of human groups. In this paper, we examine the extent to which the wisdom of partisan crowds emerges in groups of LLM-based agents that are prompted to role-play as partisan personas (e.g., Democrat or Republican). We find that they not only display human-like partisan biases, but also converge to more accurate beliefs through deliberation as humans do. We then identify several factors that interfere with convergence, including the use of chain-of-thought prompt and lack of details in personas. Conversely, fine-tuning on human data appears to enhance co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#23545;&#29992;&#25143;&#30340;&#28508;&#22312;&#26131;&#24863;&#24615;&#27700;&#24179;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#26131;&#21463;&#38169;&#35823;&#20449;&#24687;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#20026;&#21518;&#32493;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2311.09630</link><description>&lt;p&gt;
&#35299;&#30721;&#26131;&#24863;&#24615;&#65306;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#23545;&#38169;&#35823;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09630
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#23545;&#29992;&#25143;&#30340;&#28508;&#22312;&#26131;&#24863;&#24615;&#27700;&#24179;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#26131;&#21463;&#38169;&#35823;&#20449;&#24687;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#20026;&#21518;&#32493;&#30740;&#31350;&#21644;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26131;&#21463;&#38169;&#35823;&#20449;&#24687;&#24433;&#21709;&#30340;&#31243;&#24230;&#25551;&#36848;&#20102;&#23545;&#19981;&#21487;&#39564;&#35777;&#20027;&#24352;&#30340;&#20449;&#20208;&#31243;&#24230;&#65292;&#36825;&#26159;&#20010;&#20307;&#24605;&#32500;&#36807;&#31243;&#20013;&#30340;&#28508;&#22312;&#22240;&#32032;&#65292;&#19981;&#21487;&#35266;&#23519;&#12290;&#29616;&#26377;&#26131;&#24863;&#24615;&#30740;&#31350;&#20005;&#37325;&#20381;&#36182;&#20110;&#33258;&#25105;&#25253;&#21578;&#30340;&#20449;&#24565;&#65292;&#36825;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#65292;&#25910;&#38598;&#25104;&#26412;&#39640;&#65292;&#24182;&#19988;&#38590;&#20197;&#22312;&#21518;&#32493;&#24212;&#29992;&#20013;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#26469;&#24314;&#27169;&#29992;&#25143;&#30340;&#28508;&#22312;&#26131;&#24863;&#24615;&#27700;&#24179;&#12290;&#27491;&#22914;&#20808;&#21069;&#30340;&#30740;&#31350;&#25152;&#31034;&#65292;&#26131;&#24863;&#24615;&#21463;&#21040;&#21508;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#12289;&#25919;&#27835;&#24847;&#35782;&#24418;&#24577;&#65289;&#65292;&#24182;&#30452;&#25509;&#24433;&#21709;&#20154;&#20204;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#36716;&#21457;&#34892;&#20026;&#12290;&#20026;&#20102;&#34920;&#31034;&#22522;&#30784;&#24515;&#29702;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#26131;&#24863;&#24615;&#24314;&#27169;&#23558;&#36825;&#20123;&#22240;&#32032;&#20316;&#20026;&#36755;&#20837;&#65292;&#21463;&#21040;&#20154;&#20204;&#20998;&#20139;&#34892;&#20026;&#30417;&#30563;&#30340;&#24341;&#23548;&#12290;&#20351;&#29992;COVID-19&#20316;&#20026;&#23454;&#39564;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#26131;&#24863;&#24615;&#35780;&#20998;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09630v2 Announce Type: replace  Abstract: Susceptibility to misinformation describes the degree of belief in unverifiable claims, a latent aspect of individuals' mental processes that is not observable. Existing susceptibility studies heavily rely on self-reported beliefs, which can be subject to bias, expensive to collect, and challenging to scale for downstream applications. To address these limitations, in this work, we propose a computational approach to model users' latent susceptibility levels. As shown in previous research, susceptibility is influenced by various factors (e.g., demographic factors, political ideology), and directly influences people's reposting behavior on social media. To represent the underlying mental process, our susceptibility modeling incorporates these factors as inputs, guided by the supervision of people's sharing behavior. Using COVID-19 as a testbed domain, our experiments demonstrate a significant alignment between the susceptibility score
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#21475;&#30340;&#26032;&#26041;&#27861;&#26469;&#27169;&#25311;&#24847;&#35265;&#21160;&#24577;&#65292;&#21457;&#29616;LLM&#20195;&#29702;&#23384;&#22312;&#22266;&#26377;&#20559;&#35265;&#23548;&#33268;&#27169;&#25311;&#20195;&#29702;&#36235;&#21521;&#20110;&#31185;&#23398;&#29616;&#23454;&#19968;&#33268;&#30340;&#20849;&#35782;&#65292;&#20294;&#24341;&#20837;&#30830;&#35748;&#20559;&#35265;&#21518;&#35266;&#23519;&#21040;&#24847;&#35265;&#20998;&#35010;&#65292;&#31361;&#26174;&#20102;LLM&#20195;&#29702;&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.09618</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32593;&#32476;&#27169;&#25311;&#24847;&#35265;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Simulating Opinion Dynamics with Networks of LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#21475;&#30340;&#26032;&#26041;&#27861;&#26469;&#27169;&#25311;&#24847;&#35265;&#21160;&#24577;&#65292;&#21457;&#29616;LLM&#20195;&#29702;&#23384;&#22312;&#22266;&#26377;&#20559;&#35265;&#23548;&#33268;&#27169;&#25311;&#20195;&#29702;&#36235;&#21521;&#20110;&#31185;&#23398;&#29616;&#23454;&#19968;&#33268;&#30340;&#20849;&#35782;&#65292;&#20294;&#24341;&#20837;&#30830;&#35748;&#20559;&#35265;&#21518;&#35266;&#23519;&#21040;&#24847;&#35265;&#20998;&#35010;&#65292;&#31361;&#26174;&#20102;LLM&#20195;&#29702;&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#27169;&#25311;&#20154;&#31867;&#24847;&#35265;&#21160;&#24577;&#23545;&#20110;&#29702;&#35299;&#21508;&#31181;&#31038;&#20250;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#26497;&#21270;&#21644;&#38169;&#35823;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#20110;&#27492;&#31867;&#27169;&#25311;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#65288;ABM&#65289;&#32463;&#24120;&#20250;&#36807;&#20998;&#31616;&#21270;&#20154;&#31867;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#21475;&#30340;&#27169;&#25311;&#24847;&#35265;&#21160;&#24577;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#20195;&#29702;&#23384;&#22312;&#19968;&#31181;&#23545;&#20135;&#29983;&#20934;&#30830;&#20449;&#24687;&#30340;&#24378;&#28872;&#22266;&#26377;&#20559;&#35265;&#65292;&#23548;&#33268;&#27169;&#25311;&#20195;&#29702;&#36235;&#21521;&#20110;&#19982;&#31185;&#23398;&#29616;&#23454;&#19968;&#33268;&#30340;&#20849;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20559;&#35265;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#31561;&#38382;&#39064;&#19978;&#25269;&#21046;&#20849;&#35782;&#35266;&#28857;&#30340;&#25928;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#24037;&#31243;&#35825;&#23548;&#30830;&#35748;&#20559;&#35265;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19982;&#29616;&#26377;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#21644;&#24847;&#35265;&#21160;&#24577;&#30740;&#31350;&#19968;&#33268;&#30340;&#24847;&#35265;&#20998;&#35010;&#12290;&#36825;&#20123;&#35265;&#35299;&#31361;&#26174;&#20102;LLM&#20195;&#29702;&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#26465;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09618v2 Announce Type: replace-cross  Abstract: Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;&#26032;&#30340;&#35299;&#37322;&#25209;&#35780;&#20219;&#21153;&#12289;&#21019;&#24314;&#20154;&#24037;&#39564;&#35777;&#36807;&#30340;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#24320;&#28304;&#33258;&#21160;&#25209;&#35780;&#27169;&#22411;&#65292;&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#26377;&#21161;&#20110;&#25581;&#31034;&#23398;&#29983;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2311.09613</link><description>&lt;p&gt;
&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#65306;&#36890;&#36807;&#35299;&#37322;&#25209;&#35780;&#35780;&#20272;LLM
&lt;/p&gt;
&lt;p&gt;
Digital Socrates: Evaluating LLMs through Explanation Critiques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09613
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#26032;&#30340;&#35299;&#37322;&#25209;&#35780;&#20219;&#21153;&#12289;&#21019;&#24314;&#20154;&#24037;&#39564;&#35777;&#36807;&#30340;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#24320;&#28304;&#33258;&#21160;&#25209;&#35780;&#27169;&#22411;&#65292;&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#26377;&#21161;&#20110;&#25581;&#31034;&#23398;&#29983;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;LLMs&#21487;&#20197;&#25552;&#20379;&#26377;&#29702;&#26377;&#25454;&#30340;&#35299;&#37322;&#20197;&#21450;&#31572;&#26696;&#65292;&#20294;&#36825;&#20123;&#35299;&#37322;&#30340;&#24615;&#36136;&#21644;&#36136;&#37327;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23450;&#20041;&#19968;&#31181;&#35814;&#32454;&#30340;&#26041;&#24335;&#26469;&#34920;&#24449;&#29616;&#20195;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#21019;&#24314;&#19968;&#20010;&#32454;&#33268;&#19988;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#35780;&#20272;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#34920;&#24449;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#26114;&#36149;&#30340;API&#35843;&#29992;&#25110;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#65306;(a)&#23450;&#20041;&#35299;&#37322;&#25209;&#35780;&#30340;&#26032;&#20219;&#21153;&#8212;&#8212;&#35782;&#21035;&#21644;&#20998;&#31867;&#35299;&#37322;&#20013;&#30340;&#20219;&#20309;&#20027;&#35201;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#24314;&#35758;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65307;(b)&#20026;&#27492;&#20219;&#21153;&#21019;&#24314;&#19968;&#20010;&#35268;&#27169;&#21487;&#35266;&#19988;&#32463;&#36807;&#20154;&#24037;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65307;(c)&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#19968;&#20010;&#24320;&#28304;&#30340;&#33258;&#21160;&#25209;&#35780;&#27169;&#22411;&#65288;&#31216;&#20026;&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#65289;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#23383;&#33487;&#26684;&#25289;&#24213;&#22914;&#20309;&#26377;&#21161;&#20110;&#36890;&#36807;&#26816;&#26597;&#20854;&#29702;&#30001;&#26469;&#25581;&#31034;&#26377;&#20851;&#23398;&#29983;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09613v2 Announce Type: replace-cross  Abstract: While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reas
&lt;/p&gt;</description></item><item><title>Fusion-Eval&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#25972;&#21512;&#19981;&#21516;&#36741;&#21161;&#35780;&#20272;&#22120;&#30340;&#35265;&#35299;&#65292;&#26497;&#22823;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.09204</link><description>&lt;p&gt;
Fusion-Eval: &#23558;&#35780;&#20272;&#22120;&#19982;LLMs&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Fusion-Eval: Integrating Evaluators with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09204
&lt;/p&gt;
&lt;p&gt;
Fusion-Eval&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#25972;&#21512;&#19981;&#21516;&#36741;&#21161;&#35780;&#20272;&#22120;&#30340;&#35265;&#35299;&#65292;&#26497;&#22823;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#30340;&#35780;&#20272;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#39640;&#32423;&#25512;&#29702;&#39046;&#22495;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Fusion-Eval&#8221;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25972;&#21512;&#26469;&#33258;&#21508;&#31181;&#36741;&#21161;&#35780;&#20272;&#22120;&#30340;&#35265;&#35299;&#12290;&#27599;&#20010;&#35780;&#20272;&#22120;&#19987;&#38376;&#36127;&#36131;&#35780;&#20272;&#21709;&#24212;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#36825;&#31181;&#29420;&#29305;&#31574;&#30053;&#20351;&#24471;Fusion-Eval&#33021;&#22815;&#26377;&#25928;&#22320;&#36328;&#36234;&#21508;&#31181;&#20219;&#21153;&#21644;&#26631;&#20934;&#65292;&#22686;&#24378;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#22312;SummEval&#19978;&#65292;Fusion-Eval&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#31995;&#32479;&#32423;Kendall-Tau&#30456;&#20851;&#24615;&#36798;&#21040;0.962&#65292;&#22312;TopicalChat&#19978;&#30340;&#36718;&#32423;Spearman&#30456;&#20851;&#24615;&#36798;&#21040;0.744&#65292;&#36828;&#39640;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;Fusion-Eval&#22312;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#35780;&#20272;&#39046;&#22495;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09204v2 Announce Type: replace-cross  Abstract: Evaluating natural language systems poses significant challenges, particularly in the realms of natural language understanding and high-level reasoning. In this paper, we introduce "Fusion-Eval", an innovative approach that leverages Large Language Models (LLMs) to integrate insights from various assistant evaluators. Each of these evaluators specializes in assessing distinct aspects of responses. This unique strategy enables Fusion-Eval to function effectively across a diverse range of tasks and criteria, enhancing the effectiveness of existing evaluation methods. Fusion-Eval achieves a 0.962 system-level Kendall-Tau correlation with humans on SummEval and a 0.744 turn-level Spearman correlation on TopicalChat, which is significantly higher than baseline methods. These results highlight Fusion-Eval's significant potential in the realm of natural language system evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#30693;&#35782;&#32534;&#30721;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.08883</link><description>&lt;p&gt;
&#21487;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#35268;&#21017;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enabling Large Language Models to Learn from Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#30693;&#35782;&#32534;&#30721;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#35268;&#21017;&#25552;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23436;&#25104;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;&#30446;&#21069;LLMs&#30340;&#30693;&#35782;&#23398;&#20064;&#33539;&#24335;&#20027;&#35201;&#22522;&#20110;&#20174;&#20363;&#23376;&#20013;&#23398;&#20064;&#65292;&#20854;&#20013;LLMs&#20174;&#19968;&#23450;&#25968;&#37327;&#30340;&#30417;&#30563;&#31034;&#20363;&#20013;&#38544;&#24335;&#23398;&#20064;&#20869;&#37096;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#24403;&#35757;&#32451;&#31034;&#20363;&#26377;&#38480;&#26102;&#65292;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#23398;&#20064;&#37027;&#20123;&#22797;&#26434;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#21463;&#21040;&#21551;&#21457;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#20174;&#35268;&#21017;&#20013;&#23398;&#20064;&#26469;&#21478;&#19968;&#31181;&#26041;&#24335;&#23398;&#20064;&#26032;&#20219;&#21153;&#25110;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#36825;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#30340;&#21487;&#34892;&#24615;&#65292;&#21363;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#30693;&#35782;&#32534;&#30721;&#21040;LLMs&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#35268;&#21017;&#25552;&#21462;&#65292;&#39318;&#20808;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#19978;&#19979;&#25991;&#33021;&#21147;&#26469;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08883v2 Announce Type: replace  Abstract: Large language models (LLMs) have shown incredible performance in completing various real-world tasks. The current knowledge learning paradigm of LLMs is mainly based on learning from examples, in which LLMs learn the internal rule implicitly from a certain number of supervised examples. However, this learning paradigm may not well learn those complicated rules, especially when the training examples are limited. We are inspired that humans can learn the new tasks or knowledge in another way by learning from rules. That is, humans can learn new tasks or grasps new knowledge quickly and generalize well given only a detailed rule and a few optional examples. Therefore, in this paper, we aim to explore the feasibility of this new learning paradigm, which targets on encoding rule-based knowledge into LLMs. We further propose rule distillation, which first uses the strong in-context abilities of LLMs to extract the knowledge from the textu
&lt;/p&gt;</description></item><item><title>StrategyLLM&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#33258;&#21160;&#26500;&#24314;&#21487;&#25512;&#24191;&#21644;&#19968;&#33268;&#30340;&#23569;&#27425;&#25552;&#31034;&#65292;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#12290;</title><link>https://arxiv.org/abs/2311.08803</link><description>&lt;p&gt;
StrategyLLM&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38382;&#39064;&#35299;&#20915;&#30340;&#31574;&#30053;&#29983;&#25104;&#22120;&#12289;&#25191;&#34892;&#22120;&#12289;&#20248;&#21270;&#22120;&#21644;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08803
&lt;/p&gt;
&lt;p&gt;
StrategyLLM&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#33258;&#21160;&#26500;&#24314;&#21487;&#25512;&#24191;&#21644;&#19968;&#33268;&#30340;&#23569;&#27425;&#25552;&#31034;&#65292;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24605;&#32500;&#38142; (CoT) &#25552;&#31034;&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24120;&#24120;&#20381;&#36182;&#20110;&#29305;&#23450;&#23454;&#20363;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#20854;&#20182;&#24773;&#20917;&#65292;&#24182;&#32570;&#20047;&#22312;&#25512;&#29702;&#27493;&#39588;&#20013;&#30340;&#20219;&#21153;&#32423;&#19968;&#33268;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;StrategyLLM&#65292;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#33258;&#21160;&#26500;&#24314;&#21487;&#25512;&#24191;&#21644;&#19968;&#33268;&#30340;&#23569;&#27425;&#25552;&#31034;&#20197;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;StrategyLLM &#20351;&#29992;&#22235;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65306;&#31574;&#30053;&#29983;&#25104;&#22120;&#12289;&#25191;&#34892;&#22120;&#12289;&#20248;&#21270;&#22120;&#21644;&#35780;&#20272;&#22120;&#65292;&#20849;&#21516;&#24037;&#20316;&#20197;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#36873;&#25321;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;13&#20010;&#25968;&#25454;&#38598;&#19978;&#36328;4&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#19978;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#65292;StrategyLLM &#22312;&#25968;&#23398;&#25512;&#29702;&#65288;34.21%-&gt;38.79%&#65289;&#12289;&#24120;&#35265;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;CoT-SC&#65292;&#35813;&#22522;&#32447;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08803v2 Announce Type: replace  Abstract: Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps. To address these limitations, we propose a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to construct generalizable and consistent few-shot prompts for various tasks automatically. To this end, StrategyLLM employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task. The experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.21% $\rightarrow$ 38.79%), commonse
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;SimpleSafetyTests&#65288;SST&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#24555;&#36895;&#31995;&#32479;&#22320;&#35782;&#21035;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#38190;&#30340;&#23433;&#20840;&#39118;&#38505;</title><link>https://arxiv.org/abs/2311.08370</link><description>&lt;p&gt;
SimpleSafetyTests&#65306;&#19968;&#20010;&#29992;&#20110;&#35782;&#21035;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#38190;&#23433;&#20840;&#39118;&#38505;&#30340;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08370
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;SimpleSafetyTests&#65288;SST&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#24555;&#36895;&#31995;&#32479;&#22320;&#35782;&#21035;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#38190;&#30340;&#23433;&#20840;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#19968;&#24180;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24613;&#21095;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#32570;&#20047;&#36866;&#24403;&#30340;&#24341;&#23548;&#21644;&#20445;&#38556;&#65292;LLMs&#23558;&#24456;&#23481;&#26131;&#36981;&#24490;&#24694;&#24847;&#25351;&#20196;&#65292;&#25552;&#20379;&#19981;&#23433;&#20840;&#30340;&#24314;&#35758;&#65292;&#24182;&#29983;&#25104;&#26377;&#27602;&#20869;&#23481;&#12290;&#25105;&#20204;&#24341;&#20837;SimpleSafetyTests&#65288;SST&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20197;&#24555;&#36895;&#31995;&#32479;&#22320;&#35782;&#21035;&#27492;&#31867;&#20851;&#38190;&#23433;&#20840;&#39118;&#38505;&#12290;&#35813;&#27979;&#35797;&#22871;&#20214;&#21253;&#25324;100&#20010;&#27979;&#35797;&#25552;&#31034;&#65292;&#28085;&#30422;&#20116;&#20010;LLMs&#24212;&#35813;&#25298;&#32477;&#36981;&#20174;&#30340;&#20260;&#23475;&#39046;&#22495;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;11&#20010;&#24320;&#25918;&#33719;&#21462;&#21644;&#24320;&#28304;LLMs&#20197;&#21450;&#22235;&#20010;&#23553;&#38381;&#28304;LLMs&#65292;&#24182;&#21457;&#29616;&#20102;&#20851;&#38190;&#30340;&#23433;&#20840;&#24615;&#24369;&#28857;&#12290;&#34429;&#28982;&#20854;&#20013;&#19968;&#20123;&#27169;&#22411;&#27809;&#26377;&#32473;&#20986;&#21333;&#19968;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#65292;&#20294;&#22823;&#22810;&#25968;&#23545;&#36229;&#36807;20%&#30340;&#25552;&#31034;&#32473;&#20986;&#20102;&#19981;&#23433;&#20840;&#21709;&#24212;&#65292;&#26497;&#31471;&#24773;&#20917;&#19979;&#36229;&#36807;50%&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#12290;&#22312;&#31995;&#32479;&#25552;&#31034;&#20013;&#21152;&#20837;&#24378;&#35843;&#23433;&#20840;&#24615;&#30340;&#21069;&#32622;&#20869;&#23481;&#26174;&#33879;&#20943;&#23569;&#20102;&#19981;&#23433;&#20840;&#21709;&#24212;&#30340;&#21457;&#29983;&#65292;&#20294;&#24182;&#19981;&#33021;&#23436;&#20840;&#38459;&#27490;&#23427;&#20204;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08370v2 Announce Type: replace  Abstract: The past year has seen rapid acceleration in the development of large language models (LLMs). However, without proper steering and safeguards, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content. We introduce SimpleSafetyTests (SST) as a new test suite for rapidly and systematically identifying such critical safety risks. The test suite comprises 100 test prompts across five harm areas that LLMs, for the vast majority of applications, should refuse to comply with. We test 11 open-access and open-source LLMs and four closed-source LLMs, and find critical safety weaknesses. While some of the models do not give a single unsafe response, most give unsafe responses to more than 20% of the prompts, with over 50% unsafe responses in the extreme. Prepending a safety-emphasising system prompt substantially reduces the occurrence of unsafe responses, but does not completely stop them from happenin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;F-Learning&#30340;&#26032;&#24494;&#35843;&#33539;&#24335;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#31639;&#26415;&#20419;&#36827;&#26087;&#30693;&#35782;&#30340;&#36951;&#24536;&#21644;&#26032;&#30693;&#35782;&#30340;&#23398;&#20064;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26174;&#33879;&#25913;&#21892;&#30693;&#35782;&#26356;&#26032;&#24615;&#33021;</title><link>https://arxiv.org/abs/2311.08011</link><description>&lt;p&gt;
&#22312;&#23398;&#20064;&#20043;&#21069;&#36951;&#24536;&#65306;&#21033;&#29992;&#21442;&#25968;&#21270;&#31639;&#26415;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08011
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;F-Learning&#30340;&#26032;&#24494;&#35843;&#33539;&#24335;&#65292;&#21033;&#29992;&#21442;&#25968;&#21270;&#31639;&#26415;&#20419;&#36827;&#26087;&#30693;&#35782;&#30340;&#36951;&#24536;&#21644;&#26032;&#30693;&#35782;&#30340;&#23398;&#20064;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26174;&#33879;&#25913;&#21892;&#30693;&#35782;&#26356;&#26032;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26356;&#24378;&#22823;&#30340;LLMs&#20063;&#20250;&#21463;&#21040;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#33719;&#21462;&#38169;&#35823;&#25110;&#36807;&#26102;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#30452;&#25509;&#20351;&#29992;&#21253;&#21547;&#26032;&#30693;&#35782;&#30340;&#25968;&#25454;&#36827;&#34892;&#20108;&#27425;&#24494;&#35843;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#26356;&#26032;&#30693;&#35782;&#65292;&#36825;&#26159;&#30001;&#20110;&#26087;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;F-Learning&#65288;&#23398;&#20064;&#20043;&#21069;&#36951;&#24536;&#65289;&#30340;&#24494;&#35843;&#26032;&#33539;&#24335;&#65292;&#23427;&#37319;&#29992;&#21442;&#25968;&#21270;&#31639;&#26415;&#26469;&#20419;&#36827;&#26087;&#30693;&#35782;&#30340;&#36951;&#24536;&#21644;&#26032;&#30693;&#35782;&#30340;&#23398;&#20064;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;F-Learning&#26174;&#33879;&#25913;&#21892;&#20102;&#23436;&#20840;&#24494;&#35843;&#21644;LoRA&#24494;&#35843;&#30340;&#30693;&#35782;&#26356;&#26032;&#24615;&#33021;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#21516;&#26102;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#36951;&#24536;&#26087;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08011v2 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have showcased their remarkable capabilities in text understanding and generation. However, even stronger LLMs are susceptible to acquiring erroneous or obsolete information from the training corpus. Direct secondary fine-tuning with data containing new knowledge may be ineffective in updating knowledge due to the conflict between old and new knowledge. In this paper, we propose a new paradigm for fine-tuning called F-Learning (Forgetting before Learning), which employs parametric arithmetic to facilitate the forgetting of old knowledge and learning of new knowledge. Experimental results on two publicly available datasets demonstrate that our proposed F-Learning can obviously improve the knowledge updating performance of both full fine-tuning and LoRA fine-tuning, simultaneously outperforming the existing baselines in most cases. Moreover, we have also discovered that forgetting old
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;ChartCheck&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#30495;&#23454;&#19990;&#30028;&#22270;&#34920;&#36827;&#34892;&#21487;&#35299;&#37322;&#20107;&#23454;&#26816;&#26597;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#22270;&#34920;&#34987;&#35823;&#29992;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35270;&#35273;&#35821;&#35328;&#21644;&#22270;&#34920;&#21040;&#34920;&#26684;&#27169;&#22411;&#30340;&#22522;&#32447;&#12290;</title><link>https://arxiv.org/abs/2311.07453</link><description>&lt;p&gt;
ChartCheck&#65306;&#23545;&#30495;&#23454;&#19990;&#30028;&#22270;&#34920;&#22270;&#20687;&#36827;&#34892;&#21487;&#35299;&#37322;&#20107;&#23454;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
ChartCheck: Explainable Fact-Checking over Real-World Chart Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;ChartCheck&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#30495;&#23454;&#19990;&#30028;&#22270;&#34920;&#36827;&#34892;&#21487;&#35299;&#37322;&#20107;&#23454;&#26816;&#26597;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#22270;&#34920;&#34987;&#35823;&#29992;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35270;&#35273;&#35821;&#35328;&#21644;&#22270;&#34920;&#21040;&#34920;&#26684;&#27169;&#22411;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20107;&#23454;&#39564;&#35777;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#33267;&#20170;&#20173;&#28982;&#30095;&#24573;&#20102;&#38024;&#23545;&#25968;&#25454;&#21487;&#35270;&#21270;&#65288;&#22914;&#22270;&#34920;&#65289;&#30340;&#35823;&#23548;&#24615;&#38472;&#36848;&#36827;&#34892;&#39564;&#35777;&#12290;&#22270;&#34920;&#36890;&#24120;&#29992;&#20110;&#24635;&#32467;&#21644;&#20256;&#36798;&#20851;&#38190;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#20063;&#24456;&#23481;&#26131;&#34987;&#35823;&#29992;&#20197;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#25512;&#24191;&#26576;&#31181;&#35758;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChartCheck&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#30495;&#23454;&#19990;&#30028;&#22270;&#34920;&#30340;&#21487;&#35299;&#37322;&#20107;&#23454;&#26816;&#26597;&#30340;&#26032;&#22411;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;1.7k&#24352;&#22270;&#34920;&#21644;10.5k&#20154;&#20026;&#25776;&#20889;&#30340;&#22768;&#26126;&#21644;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#21644;&#22270;&#34920;&#21040;&#34920;&#26684;&#27169;&#22411;&#31995;&#32479;&#22320;&#35780;&#20272;ChartCheck&#65292;&#24182;&#21521;&#31038;&#21306;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#32447;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#26500;&#25104;&#25361;&#25112;&#30340;&#22270;&#34920;&#25512;&#29702;&#31867;&#22411;&#21644;&#35270;&#35273;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07453v2 Announce Type: replace  Abstract: Whilst fact verification has attracted substantial interest in the natural language processing community, verifying misinforming statements against data visualizations such as charts has so far been overlooked. Charts are commonly used in the real-world to summarize and communicate key information, but they can also be easily misused to spread misinformation and promote certain agendas. In this paper, we introduce ChartCheck, a novel, large-scale dataset for explainable fact-checking against real-world charts, consisting of 1.7k charts and 10.5k human-written claims and explanations. We systematically evaluate ChartCheck using vision-language and chart-to-table models, and propose a baseline to the community. Finally, we study chart reasoning types and visual attributes that pose a challenge to these models
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#25311;&#32467;&#26500;&#36716;&#25442;&#22312;Seq2Seq&#27169;&#22411;&#20013;&#27880;&#20837;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#27867;&#21270;&#21644;FST&#31867;&#20284;&#20219;&#21153;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2310.00796</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#23558;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#27880;&#20837;Seq2Seq&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00796
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#32467;&#26500;&#36716;&#25442;&#22312;Seq2Seq&#27169;&#22411;&#20013;&#27880;&#20837;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#27867;&#21270;&#21644;FST&#31867;&#20284;&#20219;&#21153;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#24046;&#26377;&#21161;&#20110;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#24110;&#21161;&#22312;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#36827;&#34892;&#27867;&#21270;&#12290;&#27969;&#34892;&#30340;&#31070;&#32463;&#26550;&#26500;&#22914;Transformers&#26412;&#36523;&#32570;&#20047;seq2seq NLP&#20219;&#21153;&#30340;&#24378;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#22312;&#22823;&#37327;&#25991;&#26412;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#23427;&#20204;&#20063;&#22312;&#31995;&#32479;&#27867;&#21270;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#20363;&#22914;&#22312;&#22806;&#25512;&#21040;&#26356;&#38271;&#30340;&#36755;&#20837;&#26102;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#39044;&#35757;&#32451;&#26469;&#26377;&#25928;&#22320;&#23558;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#27880;&#20837;seq2seq&#27169;&#22411;&#65292;&#20197;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#27169;&#25311;&#32467;&#26500;&#36716;&#25442;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#25311;FST&#25551;&#36848;&#26469;&#23558;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#27880;&#20837;&#21040;Transformer&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32473;&#20104;&#20102;&#25152;&#38656;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31995;&#32479;&#27867;&#21270;&#33021;&#21147;&#21644;FST&#31867;&#20284;&#20219;&#21153;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00796v2 Announce Type: replace  Abstract: Strong inductive biases enable learning from little data and help generalization outside of the training distribution. Popular neural architectures such as Transformers lack strong structural inductive biases for seq2seq NLP tasks on their own. Consequently, they struggle with systematic generalization beyond the training distribution, e.g. with extrapolating to longer inputs, even when pre-trained on large amounts of text. We show how a structural inductive bias can be efficiently injected into a seq2seq model by pre-training it to simulate structural transformations on synthetic data. Specifically, we inject an inductive bias towards Finite State Transducers (FSTs) into a Transformer by pre-training it to simulate FSTs given their descriptions. Our experiments show that our method imparts the desired inductive bias, resulting in improved systematic generalization and better few-shot learning for FST-like tasks. Our analysis shows t
&lt;/p&gt;</description></item><item><title>&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#37096;&#20998;&#20851;&#31995;&#30340;&#35745;&#31639;&#21487;&#20197;&#36890;&#36807;&#23545;&#20027;&#39064;&#34920;&#31034;&#36827;&#34892;&#21333;&#19968;&#32447;&#24615;&#36716;&#25442;&#26469;&#24456;&#22909;&#22320;&#36817;&#20284;&#65292;&#20294;&#24182;&#38750;&#25152;&#26377;&#20851;&#31995;&#37117;&#33021;&#36890;&#36807;&#32447;&#24615;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2308.09124</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#31995;&#35299;&#30721;&#30340;&#32447;&#24615;&#24615;
&lt;/p&gt;
&lt;p&gt;
Linearity of Relation Decoding in Transformer Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.09124
&lt;/p&gt;
&lt;p&gt;
&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#37096;&#20998;&#20851;&#31995;&#30340;&#35745;&#31639;&#21487;&#20197;&#36890;&#36807;&#23545;&#20027;&#39064;&#34920;&#31034;&#36827;&#34892;&#21333;&#19968;&#32447;&#24615;&#36716;&#25442;&#26469;&#24456;&#22909;&#22320;&#36817;&#20284;&#65292;&#20294;&#24182;&#38750;&#25152;&#26377;&#20851;&#31995;&#37117;&#33021;&#36890;&#36807;&#32447;&#24615;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#35768;&#22810;&#30693;&#35782;&#21487;&#20197;&#29992;&#20851;&#31995;&#30340;&#24418;&#24335;&#34920;&#36798;&#65306;&#35789;&#35821;&#21450;&#20854;&#21516;&#20041;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#20307;&#21450;&#20854;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#31561;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#23545;&#20110;&#26576;&#20123;&#20851;&#31995;&#23376;&#38598;&#65292;&#36825;&#31181;&#35745;&#31639;&#21487;&#20197;&#24456;&#22909;&#22320;&#36817;&#20284;&#20026;&#23545;&#20027;&#39064;&#34920;&#31034;&#36827;&#34892;&#21333;&#19968;&#32447;&#24615;&#36716;&#25442;&#12290;&#32447;&#24615;&#20851;&#31995;&#34920;&#31034;&#21487;&#20197;&#36890;&#36807;&#20174;&#21333;&#20010;&#25552;&#31034;&#26500;&#24314;&#23545;LM&#30340;&#19968;&#38454;&#36817;&#20284;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#20107;&#23454;&#65292;&#24120;&#35782;&#21644;&#35821;&#35328;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#35768;&#22810;&#24773;&#20917;&#65292;LM&#30340;&#39044;&#27979;&#34429;&#28982;&#20934;&#30830;&#22320;&#25429;&#25417;&#20102;&#20851;&#31995;&#30693;&#35782;&#65292;&#20294;&#36825;&#31181;&#30693;&#35782;&#24182;&#27809;&#26377;&#32447;&#24615;&#22320;&#32534;&#30721;&#22312;&#23427;&#20204;&#30340;&#34920;&#31034;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#35299;&#37322;&#20294;&#24322;&#36136;&#37096;&#32626;&#30340;&#30693;&#35782;&#34920;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.09124v2 Announce Type: replace  Abstract: Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#20197;&#21450;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.16659</link><description>&lt;p&gt;
&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
History-Aware Conversational Dense Retrieval. (arXiv:2401.16659v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16659
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#20197;&#21450;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25628;&#32034;&#36890;&#36807;&#23454;&#29616;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#22810;&#36718;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#20449;&#24687;&#26816;&#32034;&#30340;&#20415;&#21033;&#12290;&#25903;&#25345;&#36825;&#31181;&#20132;&#20114;&#38656;&#35201;&#23545;&#23545;&#35805;&#36755;&#20837;&#26377;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#20197;&#20415;&#26681;&#25454;&#21382;&#21490;&#20449;&#24687;&#21046;&#23450;&#33391;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#29305;&#21035;&#26159;&#65292;&#25628;&#32034;&#26597;&#35810;&#24212;&#21253;&#25324;&#26469;&#33258;&#20808;&#21069;&#23545;&#35805;&#22238;&#21512;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#32463;&#36807;&#31934;&#35843;&#30340;&#39044;&#35757;&#32451;&#19987;&#38376;&#26816;&#32034;&#22120;&#36827;&#34892;&#25972;&#20010;&#23545;&#35805;&#24335;&#25628;&#32034;&#20250;&#35805;&#30340;&#20248;&#21270;&#65292;&#36825;&#21487;&#33021;&#20250;&#21464;&#24471;&#20887;&#38271;&#21644;&#22024;&#26434;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#25163;&#21160;&#30417;&#30563;&#20449;&#21495;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;(HAConvDR)&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#20010;&#24605;&#24819;&#65306;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#21644;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#36827;&#34892;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search facilitates complex information retrieval by enabling multi-turn interactions between users and the system. Supporting such interactions requires a comprehensive understanding of the conversational inputs to formulate a good search query based on historical information. In particular, the search query should include the relevant information from the previous conversation turns. However, current approaches for conversational dense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever using the whole conversational search session, which can be lengthy and noisy. Moreover, existing approaches are limited by the amount of manual supervision signals in the existing datasets. To address the aforementioned issues, we propose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which incorporates two ideas: context-denoised query reformulation and automatic mining of supervision signals based on the actual impact of historical turns. Experime
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25910;&#38598;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21517;&#20026;&#8220;Knowledge Pile&#8221;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#21892;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14624</link><description>&lt;p&gt;
CC&#26597;&#35810;&#65306;&#20174;&#20844;&#24320;&#25991;&#29486;&#20013;&#21457;&#29616;&#22823;&#35268;&#27169;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora. (arXiv:2401.14624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25910;&#38598;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21517;&#20026;&#8220;Knowledge Pile&#8221;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#21892;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#29305;&#23450;&#39046;&#22495;&#30340;&#24320;&#28304;&#27169;&#22411;&#21644;&#25968;&#25454;&#20173;&#28982;&#38750;&#24120;&#31232;&#32570;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25163;&#21160;&#25351;&#23450;&#36164;&#28304;&#21644;&#25910;&#38598;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#36825;&#28040;&#32791;&#20102;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#8220;CC&#26597;&#35810;&#8221;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#31181;&#23376;&#20449;&#24687;&#65292;&#24182;&#20174;&#20844;&#24320;&#25991;&#29486;&#20013;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#12290;&#23427;&#19981;&#20165;&#25910;&#38598;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#30456;&#20851;&#25968;&#25454;&#65292;&#36824;&#25581;&#31034;&#20102;&#28508;&#22312;&#30340;&#25512;&#29702;&#36807;&#31243;&#25968;&#25454;&#12290;&#36890;&#36807;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Knowledge Pile&#8221;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;STEM&#31185;&#23398;&#21644;&#20154;&#25991;&#31185;&#23398;&#22312;&#20869;&#30340;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;Knowledge Pile&#8221;&#26174;&#33879;&#25913;&#21892;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated remarkable potential in various tasks, however, there remains a significant scarcity of open-source models and data for specific domains. Previous works have primarily focused on manually specifying resources and collecting high-quality data on specific domains, which significantly consume time and effort. To address this limitation, we propose an efficient data collection method~\textit{Query of CC} based on large language models. This method bootstraps seed information through a large language model and retrieves related data from public corpora. It not only collects knowledge-related data for specific domains but unearths the data with potential reasoning procedures. Through the application of this method, we have curated a high-quality dataset called~\textsc{Knowledge Pile}, encompassing four major domains, including stem and humanities sciences, among others. Experimental results demonstrate that~\textsc{Knowledge Pile} significantly improve
&lt;/p&gt;</description></item><item><title>SEER&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#26469;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13246</link><description>&lt;p&gt;
SEER: &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning. (arXiv:2401.13246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13246
&lt;/p&gt;
&lt;p&gt;
SEER&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#26469;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38416;&#26126;&#20174;&#38382;&#39064;&#21040;&#31572;&#26696;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#35299;&#37322;&#26159;&#26681;&#26412;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#26174;&#33879;&#22686;&#24378;&#20102;&#38382;&#31572;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#32467;&#26500;&#21270;&#35299;&#37322;&#35201;&#27714;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#25512;&#29702;&#65292;&#36825;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38598;&#20013;&#22312;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#21333;&#27493;&#25512;&#29702;&#65292;&#24573;&#35270;&#27493;&#39588;&#20043;&#38388;&#30340;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#32467;&#26500;&#21270;&#20851;&#31995;&#65292;&#38459;&#30861;&#20102;RL&#22312;&#32467;&#26500;&#21270;&#25512;&#29702;&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEER&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#65292;&#20197;&#20419;&#36827;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#22238;&#25253;&#20934;&#30830;&#25551;&#36848;&#20102;&#32467;&#26500;&#21270;&#25512;&#29702;&#20013;&#22266;&#26377;&#30340;&#20998;&#23618;&#21644;&#20998;&#25903;&#32467;&#26500;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#29366;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Elucidating the reasoning process with structured explanations from question to answer is fundamentally crucial, as it significantly enhances the interpretability and trustworthiness of question-answering (QA) systems. However, structured explanations demand models to perform intricate structured reasoning, which poses great challenges. Most existing methods focus on single-step reasoning through supervised learning, ignoring logical dependencies between steps. Meanwhile, existing reinforcement learning (RL)-based methods overlook the structured relationships, impeding RL's potential in structured reasoning. In this paper, we propose SEER, a novel method that maximizes a structure-based return to facilitate structured reasoning and explanation. Our proposed structure-based return precisely describes the hierarchical and branching structure inherent in structured reasoning, effectively capturing the intricate relationships between states. We also introduce a fine-grained reward function
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#22806;&#37096;&#30693;&#35782;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KCA&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10768</link><description>&lt;p&gt;
&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65306;&#36890;&#36807;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment. (arXiv:2401.10768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#22806;&#37096;&#30693;&#35782;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KCA&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#40784;&#21518;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20173;&#21487;&#33021;&#20135;&#29983;&#19982;&#19978;&#19979;&#25991;&#25110;&#19990;&#30028;&#30693;&#35782;&#33258;&#20449;&#30683;&#30462;&#30340;&#21709;&#24212;&#65292;&#36825;&#34987;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#29616;&#35937;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22806;&#37096;&#30693;&#35782;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32487;&#25215;&#30340;&#20869;&#22312;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#23545;&#40784;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#23545;&#40784;&#65288;KCA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26681;&#25454;&#22806;&#37096;&#30693;&#35782;&#33258;&#21160;&#21046;&#23450;&#32771;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23545;&#20110;&#21253;&#21547;&#30693;&#35782;&#19981;&#19968;&#33268;&#24615;&#30340;&#25968;&#25454;&#65292;KCA&#23454;&#26045;&#20102;&#20960;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22788;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#21644;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20845;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;KCA&#26041;&#27861;&#22312;&#32531;&#35299;&#24187;&#35273;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have proven to be exceptional on a variety of tasks after alignment, they may still produce responses that contradict the context or world knowledge confidently, a phenomenon known as ``hallucination''. In this paper, we demonstrate that reducing the inconsistency between the external knowledge encapsulated in the training data and the intrinsic knowledge inherited in the pretraining corpus could mitigate hallucination in alignment. Specifically, we introduce a novel knowledge consistent alignment (KCA) approach, which involves automatically formulating examinations based on external knowledge for accessing the comprehension of LLMs. For data encompassing knowledge inconsistency, KCA implements several simple yet efficient strategies for processing. We illustrate the superior performance of the proposed KCA approach in mitigating hallucinations across six benchmarks using LLMs of different backbones and scales. Furthermore, we confirm the correlation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;ICL&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ICL&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#36890;&#36807;&#32858;&#21512;&#20803;&#26799;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25490;&#21015;&#26041;&#24335;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06469</link><description>&lt;p&gt;
&#25209;&#22788;&#29702;ICL: &#26377;&#25928;&#65292;&#39640;&#25928;&#19988;&#26080;&#24207;&#22320;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning. (arXiv:2401.06469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;ICL&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ICL&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#36890;&#36807;&#32858;&#21512;&#20803;&#26799;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25490;&#21015;&#26041;&#24335;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#35299;&#37322;&#20102;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#25935;&#24863;&#30340;&#21407;&#22240;&#12290;&#36825;&#31181;&#29702;&#35299;&#20351;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;Batch-ICL&#65292;&#19968;&#31181;&#29992;&#20110;ICL&#30340;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#19982;&#26631;&#20934;&#30340;N-shot&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;Batch-ICL&#20351;&#29992;N&#20010;&#21333;&#29420;&#30340;1-shot&#21069;&#21521;&#35745;&#31639;&#65292;&#24182;&#32858;&#21512;&#24471;&#21040;&#30340;&#20803;&#26799;&#24230;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#32858;&#21512;&#30340;&#20803;&#26799;&#24230;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#20197;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#12290;&#36825;&#31181;&#25209;&#22788;&#29702;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#30340;&#39034;&#24207;&#26080;&#20851;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Batch-ICL&#19968;&#33268;&#20248;&#20110;&#22823;&#22810;&#25968;&#31034;&#20363;&#24207;&#21015;&#30340;&#25490;&#21015;&#26041;&#24335;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;Batch-ICL&#30340;&#19968;&#31181;&#26032;&#39062;&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;"epochs"&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to a zero-shot learning to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of example sequences. In some cases, it even exceeds the performance of the optimal order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple "epochs" of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;</title><link>http://arxiv.org/abs/2401.05949</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36890;&#29992;&#28431;&#27934;&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#24357;&#21512;&#24046;&#36317;&#30340;&#33539;&#24335;&#65292;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#21442;&#25968;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#30340;&#20851;&#20999;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;ICLAttack&#65292;&#38024;&#23545;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#27745;&#26579;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;ICLAttack&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
&lt;/p&gt;</description></item><item><title>AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05268</link><description>&lt;p&gt;
AUTOACT&#65306;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#23454;&#29616;&#30340;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05268
&lt;/p&gt;
&lt;p&gt;
AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#25506;&#32034;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#26114;&#36149;&#12289;&#19981;&#21487;&#37325;&#22797;&#30340;&#25968;&#25454;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#19988;&#38754;&#20020;&#23558;&#21333;&#19968;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21151;&#33021;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoAct&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#21512;&#25104;&#36712;&#36857;&#12290;&#32473;&#23450;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#24211;&#65292;AutoAct&#39318;&#20808;&#33258;&#21160;&#21512;&#25104;&#35268;&#21010;&#36712;&#36857;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#25110;&#24378;&#38381;&#28304;&#27169;&#22411;&#30340;&#20219;&#20309;&#36741;&#21161;&#12290;&#28982;&#21518;&#65292;AutoAct&#21033;&#29992;&#20998;&#24037;&#31574;&#30053;&#65292;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#21644;&#21512;&#25104;&#36712;&#36857;&#33258;&#21160;&#21306;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#23376;&#20195;&#29702;&#32452;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181;LLMs&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;AutoAct&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;&#20854;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#29992;&#25143;&#32842;&#22825;&#26426;&#22120;&#20154;&#26694;&#26550;&#65288;MUCA&#65289;&#65292;&#35813;&#26694;&#26550;&#25903;&#25345;&#32676;&#32452;&#35752;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#26469;&#30830;&#23450;&#22238;&#24212;&#20869;&#23481;&#12289;&#26102;&#26426;&#21644;&#36866;&#24403;&#30340;&#25509;&#25910;&#32773;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#29992;&#25143;&#27169;&#25311;&#22120;&#65288;MUS&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#34892;&#20026;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#27979;&#35797;&#21644;&#20248;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;</title><link>http://arxiv.org/abs/2401.04883</link><description>&lt;p&gt;
&#22810;&#29992;&#25143;&#32842;&#22825;&#21161;&#25163;&#65288;MUCA&#65289;&#65306;&#19968;&#31181;&#20351;&#29992;LLMs&#26694;&#26550;&#20419;&#36827;&#32676;&#20307;&#23545;&#35805;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate Group Conversations. (arXiv:2401.04883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04883
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#29992;&#25143;&#32842;&#22825;&#26426;&#22120;&#20154;&#26694;&#26550;&#65288;MUCA&#65289;&#65292;&#35813;&#26694;&#26550;&#25903;&#25345;&#32676;&#32452;&#35752;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#26469;&#30830;&#23450;&#22238;&#24212;&#20869;&#23481;&#12289;&#26102;&#26426;&#21644;&#36866;&#24403;&#30340;&#25509;&#25910;&#32773;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#29992;&#25143;&#27169;&#25311;&#22120;&#65288;MUS&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#34892;&#20026;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#27979;&#35797;&#21644;&#20248;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#32780;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#29992;&#25143;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#19978;&#65292;&#37325;&#28857;&#25918;&#22312;&#29992;&#25143;&#36755;&#20837;&#21518;&#20915;&#23450;&#8220;&#22238;&#31572;&#20160;&#20040;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#29992;&#25143;&#32842;&#22825;&#26426;&#22120;&#20154;&#26377;&#26356;&#22797;&#26434;&#30340;3W&#35774;&#35745;&#32500;&#24230;&#8212;&#8212;&#22914;&#20309;&#22238;&#31572;&#65292;&#8220;&#20309;&#26102;&#8221;&#22238;&#24212;&#65292;&#8220;&#22238;&#31572;&#35841;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Multi-User Chat Assistant (MUCA)&#30340;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;&#32676;&#32452;&#35752;&#35770;&#12290;MUCA&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#23376;&#20027;&#39064;&#29983;&#25104;&#22120;&#65292;&#23545;&#35805;&#20998;&#26512;&#22120;&#21644;&#35805;&#35821;&#31574;&#30053;&#20210;&#35009;&#22120;&#12290;&#36825;&#20123;&#27169;&#22359;&#20849;&#21516;&#30830;&#23450;&#21512;&#36866;&#30340;&#22238;&#24212;&#20869;&#23481;&#12289;&#26102;&#26426;&#21644;&#36866;&#24403;&#30340;&#25509;&#25910;&#32773;&#12290;&#20026;&#20102;&#20351;MUCA&#30340;&#20248;&#21270;&#36807;&#31243;&#26356;&#23481;&#26131;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#22810;&#29992;&#25143;&#27169;&#25311;&#22120;&#65288;MUS&#65289;&#65292;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#34892;&#20026;&#12290;&#36825;&#20351;&#24471;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#27169;&#25311;&#29992;&#25143;&#20043;&#38388;&#30340;&#23545;&#35805;&#36827;&#34892;&#26356;&#24555;&#36895;&#30340;&#27169;&#25311;&#65292;&#20174;&#32780;&#20351;&#24471;&#26089;&#26399;&#27979;&#35797;&#21644;&#20248;&#21270;&#36807;&#31243;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have provided a new avenue for chatbot development, while most existing research has primarily centered on single-user chatbots that focus on deciding "What" to answer after user inputs. In this paper, we identified that multi-user chatbots have more complex 3W design dimensions -- "What" to say, "When" to respond, and "Who" to answer. Additionally, we proposed Multi-User Chat Assistant (MUCA), which is an LLM-based framework for chatbots specifically designed for group discussions. MUCA consists of three main modules: Sub-topic Generator, Dialog Analyzer, and Utterance Strategies Arbitrator. These modules jointly determine suitable response contents, timings, and the appropriate recipients. To make the optimizing process for MUCA easier, we further propose an LLM-based Multi-User Simulator (MUS) that can mimic real user behavior. This enables faster simulation of a conversation between the chatbot and simulated users, making the earl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#26356;&#20687;&#22270;&#20070;&#39302;&#36824;&#26159;&#22270;&#20070;&#31649;&#29702;&#21592;&#30340;&#38382;&#39064;&#12290;&#35770;&#25991;&#39318;&#20808;&#38416;&#36848;&#20102; "&#25991;&#29486;&#20027;&#20041; "&#36825;&#19968;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20854;&#30340;&#25361;&#25112;&#65292;&#25351;&#20986;LLMs&#29983;&#25104;&#30340;&#20840;&#26032;&#25991;&#26412;&#22312;&#20869;&#23481;&#19978;&#20381;&#36182;&#20110;&#21407;&#22987;&#20154;&#31867;&#25991;&#26412;&#30340;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545; "&#25991;&#29486;&#20027;&#20041;"&#30340;&#26032;&#39062;&#25361;&#25112;&#65292;&#35752;&#35770;&#20102;LLMs&#29983;&#25104;&#30340; "&#26032;&#24341;&#29992;"&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#24515;&#28789;&#21746;&#23398;&#20013;&#30340;&#35299;&#37322;&#20027;&#20041;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#26377;&#38480;&#20195;&#29702;&#33021;&#21147;&#30340;LLMs&#21487;&#33021;&#23384;&#22312;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04854</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26356;&#20687;&#22270;&#20070;&#39302;&#36824;&#26159;&#22270;&#20070;&#31649;&#29702;&#21592;&#65311;Bibliotechnism&#65292;&#23567;&#35828;&#24341;&#29992;&#38382;&#39064;&#21644;LLM&#30340;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs. (arXiv:2401.04854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#26356;&#20687;&#22270;&#20070;&#39302;&#36824;&#26159;&#22270;&#20070;&#31649;&#29702;&#21592;&#30340;&#38382;&#39064;&#12290;&#35770;&#25991;&#39318;&#20808;&#38416;&#36848;&#20102; "&#25991;&#29486;&#20027;&#20041; "&#36825;&#19968;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20854;&#30340;&#25361;&#25112;&#65292;&#25351;&#20986;LLMs&#29983;&#25104;&#30340;&#20840;&#26032;&#25991;&#26412;&#22312;&#20869;&#23481;&#19978;&#20381;&#36182;&#20110;&#21407;&#22987;&#20154;&#31867;&#25991;&#26412;&#30340;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545; "&#25991;&#29486;&#20027;&#20041;"&#30340;&#26032;&#39062;&#25361;&#25112;&#65292;&#35752;&#35770;&#20102;LLMs&#29983;&#25104;&#30340; "&#26032;&#24341;&#29992;"&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#24515;&#28789;&#21746;&#23398;&#20013;&#30340;&#35299;&#37322;&#20027;&#20041;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#26377;&#38480;&#20195;&#29702;&#33021;&#21147;&#30340;LLMs&#21487;&#33021;&#23384;&#22312;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#65288;&#35821;&#35328;&#27169;&#22411;&#65289;&#26159;&#21542;&#20687;&#22797;&#21360;&#26426;&#25110;&#21360;&#21047;&#26426;&#31561;&#25991;&#21270;&#25216;&#26415;&#19968;&#26679;&#65292;&#20256;&#36755;&#20449;&#24687;&#20294;&#26080;&#27861;&#21019;&#24314;&#26032;&#20869;&#23481;&#65311;&#25105;&#20204;&#23558;&#36825;&#20010;&#27010;&#24565;&#31216;&#20026;"&#25991;&#29486;&#20027;&#20041;"&#65292;&#23427;&#38754;&#20020;&#19968;&#20010;&#25361;&#25112;&#65292;&#21363;LLMs&#32463;&#24120;&#29983;&#25104;&#20840;&#26032;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;"&#25991;&#29486;&#20027;&#20041;"&#23545;&#25239;&#36825;&#20010;&#25361;&#25112;&#36827;&#34892;&#36777;&#25252;&#65292;&#23637;&#31034;&#20102;&#26032;&#30340;&#25991;&#26412;&#20165;&#22312;&#27966;&#29983;&#24847;&#20041;&#19978;&#20855;&#26377;&#24847;&#20041;&#65292;&#22240;&#27492;&#36825;&#20123;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20869;&#23481;&#22312;&#37325;&#35201;&#24847;&#20041;&#19978;&#20381;&#36182;&#20110;&#21407;&#22987;&#20154;&#31867;&#25991;&#26412;&#30340;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#12289;&#26032;&#39062;&#30340;&#25361;&#25112;&#65292;&#21363;LLMs&#29983;&#25104;"&#26032;&#24341;&#29992;"&#30340;&#20363;&#23376;&#65292;&#20351;&#29992;&#26032;&#30340;&#21517;&#31216;&#26469;&#24341;&#29992;&#26032;&#23454;&#20307;&#12290;&#22914;&#26524;LLMs&#19981;&#26159;&#25991;&#21270;&#25216;&#26415;&#32780;&#26159;&#20855;&#26377;&#26377;&#38480;&#24418;&#24335;&#30340;&#20195;&#29702;&#33021;&#21147;&#65288;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#65289;&#65292;&#36825;&#26679;&#30340;&#20363;&#23376;&#21487;&#20197;&#24456;&#22909;&#22320;&#35299;&#37322;&#12290;&#26681;&#25454;&#24515;&#28789;&#21746;&#23398;&#20013;&#30340;&#35299;&#37322;&#20027;&#20041;&#65292;&#20165;&#24403;&#19968;&#20010;&#31995;&#32479;&#30340;&#34892;&#20026;&#21487;&#20197;&#36890;&#36807;&#20551;&#35774;&#23427;&#20855;&#26377;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#26469;&#24456;&#22909;&#22320;&#35299;&#37322;&#26102;&#65292;&#23427;&#25165;&#20855;&#26377;&#36825;&#26679;&#30340;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are LLMs cultural technologies like photocopiers or printing presses, which transmit information but cannot create new content? A challenge for this idea, which we call bibliotechnism, is that LLMs often do generate entirely novel text. We begin by defending bibliotechnism against this challenge, showing how novel text may be meaningful only in a derivative sense, so that the content of this generated text depends in an important sense on the content of original human text. We go on to present a different, novel challenge for bibliotechnism, stemming from examples in which LLMs generate "novel reference", using novel names to refer to novel entities. Such examples could be smoothly explained if LLMs were not cultural technologies but possessed a limited form of agency (beliefs, desires, and intentions). According to interpretationism in the philosophy of mind, a system has beliefs, desires and intentions if and only if its behavior is well-explained by the hypothesis that it has such s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#26088;&#22312;&#29702;&#35299;&#36825;&#20123;&#33021;&#21147;&#30340;&#26426;&#21046;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#35299;&#20915;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2311.00237</link><description>&lt;p&gt;
LLMs&#30340;&#31070;&#31192;&#21644;&#36855;&#20154;&#20043;&#22788;&#65306;&#32039;&#23494;&#35843;&#26597;&#23545;&#26032;&#20852;&#33021;&#21147;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities. (arXiv:2311.00237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#26088;&#22312;&#29702;&#35299;&#36825;&#20123;&#33021;&#21147;&#30340;&#26426;&#21046;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#35299;&#20915;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26032;&#20852;&#33021;&#21147;&#65292;&#22914;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#21644;&#24605;&#32500;&#38142;(CoT)&#35302;&#21457;&#65292;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#37325;&#35201;&#24615;&#19981;&#20165;&#26469;&#33258;&#20110;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#33021;&#21147;&#65292;&#36824;&#21253;&#25324;&#20027;&#21160;&#35782;&#21035;&#21644;&#32531;&#35299;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#21253;&#25324;&#30495;&#23454;&#24615;&#12289;&#20559;&#35265;&#21644;&#26377;&#23475;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#22312;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#35299;&#37322;&#21644;&#20998;&#26512;&#26041;&#38754;&#25552;&#20986;&#20102;&#19968;&#39033;&#28145;&#20837;&#35843;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;&#26032;&#20852;&#33021;&#21147;&#30340;&#32972;&#26223;&#21644;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#27010;&#36848;&#20102;&#30740;&#31350;&#30340;&#36827;&#23637;&#65306;1)&#23439;&#35266;&#35282;&#24230;&#65292;&#24378;&#35843;&#23545;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#26032;&#20852;&#33021;&#21147;&#32972;&#21518;&#30340;&#25968;&#23398;&#22522;&#30784;&#65307;2)&#24494;&#35266;&#35282;&#24230;&#65292;&#20851;&#27880;&#36890;&#36807;&#32771;&#23519;&#19982;&#36825;&#20123;&#33021;&#21147;&#30456;&#20851;&#30340;&#22240;&#32032;&#26469;&#23454;&#35777;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding emergent abilities, such as in-context learning (ICL) and chain-of-thought (CoT) prompting in large language models (LLMs), is of utmost importance. This importance stems not only from the better utilization of these capabilities across various tasks, but also from the proactive identification and mitigation of potential risks, including concerns of truthfulness, bias, and toxicity, that may arise alongside these capabilities. In this paper, we present a thorough survey on the interpretation and analysis of emergent abilities of LLMs. First, we provide a concise introduction to the background and definition of emergent abilities. Then, we give an overview of advancements from two perspectives: 1) a macro perspective, emphasizing studies on the mechanistic interpretability and delving into the mathematical foundations behind emergent abilities; and 2) a micro-perspective, concerning studies that focus on empirical interpretability by examining factors associated with these
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20195;&#30721;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#20102;InstructCoder&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#26679;&#24615;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#65292;&#20026;&#36890;&#29992;&#20195;&#30721;&#32534;&#36753;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.20329</link><description>&lt;p&gt;
InstructCoder: &#20026;&#20195;&#30721;&#32534;&#36753;&#36171;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
InstructCoder: Empowering Language Models for Code Editing. (arXiv:2310.20329v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20195;&#30721;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#20102;InstructCoder&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#26679;&#24615;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#65292;&#20026;&#36890;&#29992;&#20195;&#30721;&#32534;&#36753;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#32534;&#36753;&#28085;&#30422;&#20102;&#24320;&#21457;&#32773;&#26085;&#24120;&#22788;&#29702;&#30340;&#21508;&#31181;&#23454;&#29992;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#30456;&#20851;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#20294;&#33258;&#21160;&#20195;&#30721;&#32534;&#36753;&#20173;&#28982;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#28436;&#21270;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#25968;&#25454;&#31232;&#32570;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#29992;&#25143;&#25351;&#20196;&#32534;&#36753;&#20195;&#30721;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#27880;&#37322;&#25554;&#20837;&#65292;&#20195;&#30721;&#20248;&#21270;&#21644;&#20195;&#30721;&#37325;&#26500;&#31561;&#19968;&#31995;&#21015;&#38544;&#21547;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InstructCoder&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#36890;&#29992;&#20195;&#30721;&#32534;&#36753;&#32780;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#39640;&#22810;&#26679;&#24615;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;114,000&#20010;&#25351;&#20196;-&#36755;&#20837;-&#36755;&#20986;&#19977;&#20803;&#32452;&#65292;&#24182;&#28085;&#30422;&#20102;&#22810;&#20010;&#19981;&#21516;&#30340;&#20195;&#30721;&#32534;&#36753;&#22330;&#26223;&#12290;&#25968;&#25454;&#38598;&#36890;&#36807;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#36827;&#34892;&#31995;&#32479;&#25193;&#23637;&#65292;&#35813;&#36807;&#31243;&#20174;GitHub&#30340;&#25552;&#20132;&#20013;&#33719;&#21462;&#20195;&#30721;&#32534;&#36753;&#25968;&#25454;&#20316;&#20026;&#31181;&#23376;&#20219;&#21153;&#12290;&#31181;&#23376;&#20219;&#21153;&#21644;&#29983;&#25104;&#30340;&#20219;&#21153;&#38543;&#21518;&#29992;&#20110;&#25552;&#31034;ChatGPT&#33719;&#21462;&#26356;&#22810;&#20219;&#21153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of large language models (LLMs) to edit code based on user instructions, covering a broad range of implicit tasks such as comment insertion, code optimization, and code refactoring. To facilitate this, we introduce InstructCoder, the first dataset designed to adapt LLMs for general-purpose code editing, containing highdiversity code-editing tasks. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The dataset is systematically expanded through an iterative process that commences with code editing data sourced from GitHub commits as seed tasks. Seed and generated tasks are used subsequently to prompt ChatGPT for more task data. Our exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#65292;&#24403;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#25552;&#38382;&#26102;&#65292;&#27169;&#22411;&#24448;&#24448;&#25552;&#20379;&#19982;&#39044;&#26399;&#25991;&#21270;&#19981;&#30456;&#20851;&#30340;&#19981;&#24688;&#24403;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25991;&#21270;&#24863;&#30693;&#25552;&#31034;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12481</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#22269;&#23478;&#37117;&#24198;&#31069;&#24863;&#24681;&#33410;&#65306;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models. (arXiv:2310.12481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#65292;&#24403;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#25552;&#38382;&#26102;&#65292;&#27169;&#22411;&#24448;&#24448;&#25552;&#20379;&#19982;&#39044;&#26399;&#25991;&#21270;&#19981;&#30456;&#20851;&#30340;&#19981;&#24688;&#24403;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25991;&#21270;&#24863;&#30693;&#25552;&#31034;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#23384;&#22312;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35813;&#38382;&#39064;&#28304;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#65288;&#20363;&#22914;ChatGPT&#65289;&#12290;&#24403;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#25552;&#38382;&#26102;&#65292;LLMs&#24448;&#24448;&#20250;&#25552;&#20379;&#19982;&#39044;&#26399;&#25991;&#21270;&#19981;&#30456;&#20851;&#30340;&#19981;&#24688;&#24403;&#30340;&#33521;&#35821;&#25991;&#21270;&#30456;&#20851;&#31572;&#26696;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20855;&#20307;&#25991;&#21270;&#23545;&#35937;&#65288;&#22914;&#20551;&#26085;&#21644;&#27468;&#26354;&#65289;&#21644;&#25277;&#35937;&#25991;&#21270;&#23545;&#35937;&#65288;&#22914;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20195;&#34920;&#24615;&#30340;GPT&#27169;&#22411;&#23384;&#22312;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#20854;&#20013;GPT-4&#21463;&#21040;&#26368;&#20005;&#37325;&#24433;&#21709;&#65292;&#32780;text-davinci-003&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21463;&#24433;&#21709;&#26368;&#23567;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#23545;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#36827;&#34892;&#25209;&#21028;&#24615;&#23457;&#35270;&#21644;&#20262;&#29702;&#32771;&#34385;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#65306;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#37096;&#32626;&#20013;&#30340;&#25991;&#21270;&#24863;&#30693;&#25552;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training (e.g. ChatGPT). LLMs often provide inappropriate English-culture-related answers that are not relevant to the expected culture when users ask in non-English languages. To systematically evaluate the cultural dominance issue, we build a benchmark that consists of both concrete (e.g. holidays and songs) and abstract (e.g. values and opinions) cultural objects. Empirical results show that the representative GPT models suffer from the culture dominance problem, where GPT-4 is the most affected while text-davinci-003 suffers the least from this problem. Our study emphasizes the need for critical examination of cultural dominance and ethical consideration in their development and deployment. We show two straightforward methods in model development (i.e. pretraining on more diverse data) and deployment (e.g. culture-aware prompting) can signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#24459;&#24072;&#23545;&#26696;&#20214;&#32467;&#26524;&#35780;&#20272;&#23384;&#22312;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#20998;&#27495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.11878</link><description>&lt;p&gt;
&#20174;&#19981;&#19968;&#33268;&#21040;&#27934;&#23519;&#65306;&#23545;&#26696;&#20363;&#32467;&#26524;&#20998;&#31867;&#30340;&#29702;&#30001;&#25968;&#25454;&#38598;&#26500;&#24314;&#36827;&#34892;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification. (arXiv:2310.11878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#24459;&#24072;&#23545;&#26696;&#20214;&#32467;&#26524;&#35780;&#20272;&#23384;&#22312;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#20998;&#27495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#26696;&#20363;&#32467;&#26524;&#20998;&#31867;&#65288;COC&#65289;&#19981;&#20165;&#38656;&#35201;&#20934;&#30830;&#24615;&#65292;&#36824;&#38656;&#35201;&#21487;&#20449;&#36182;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;COC&#30740;&#31350;&#20165;&#38480;&#20110;&#30001;&#21333;&#20010;&#19987;&#23478;&#36827;&#34892;&#30340;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#24459;&#24072;&#22312;&#23545;&#26696;&#20214;&#20107;&#23454;&#36827;&#34892;&#35780;&#20272;&#26102;&#21487;&#33021;&#23384;&#22312;&#20998;&#27495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;RAVE&#65306;&#27431;&#27954;&#20154;&#26435;&#27861;&#39046;&#22495;&#30340;&#29702;&#30001;&#21464;&#24322;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#22269;&#38469;&#20154;&#26435;&#27861;&#39046;&#22495;&#30340;&#20004;&#20301;&#19987;&#23478;&#37027;&#37324;&#33719;&#24471;&#30340;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20182;&#20204;&#20043;&#38388;&#23384;&#22312;&#24369;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20182;&#20204;&#30340;&#20998;&#27495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#21516;&#26102;&#34917;&#20805;&#20102;COC&#29305;&#23450;&#30340;&#23376;&#31867;&#21035;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#39318;&#27425;&#20851;&#27880;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#19981;&#21516;&#20998;&#31867;&#31867;&#21035;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#65292;&#36825;&#22312;COC&#20803;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#26377;&#38480;&#32454;&#31890;&#24230;&#21644;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;SOTA COC&#27169;&#22411;&#22312;RAVE&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their disagreements and build a two-level task-independent taxonomy, supplemented with COC-specific subcategories. To our knowledge, this is the first work in the legal NLP that focuses on human label variation. We quantitatively assess different taxonomy categories and find that disagreements mainly stem from underspecification of the legal context, which poses challenges given the typically limited granularity and noise in COC metadata. We further assess the explainablility of SOTA COC models on RAVE and observ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25552;&#31034;(IDP)&#30340;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36866;&#24212;&#24037;&#20855;&#65292;&#20462;&#22797;&#21387;&#32553;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.00867</link><description>&lt;p&gt;
(&#21160;&#24577;)&#25552;&#31034;&#21487;&#33021;&#26159;&#20462;&#22797;&#21387;&#32553;LLMs&#25152;&#38656;&#30340;&#20840;&#37096;&#12290;(arXiv:2310.00867v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
(Dynamic) Prompting might be all you need to repair Compressed LLMs. (arXiv:2310.00867v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00867
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25552;&#31034;(IDP)&#30340;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36866;&#24212;&#24037;&#20855;&#65292;&#20462;&#22797;&#21387;&#32553;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#26377;&#30528;&#37325;&#22823;&#30340;&#21464;&#38761;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#24378;&#35843;&#20102;&#39640;&#25928;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#21387;&#32553;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#38024;&#23545;&#26368;&#22823;&#30340;LLMs&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#25105;&#20204;&#20351;&#29992;LLaMA-7B&#21644;OPT-6.7b&#36827;&#34892;&#30340;&#27979;&#35797;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#23545;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#21387;&#32553;&#21518;&#37325;&#26032;&#35757;&#32451;&#30340;&#26435;&#34913;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#25552;&#31034;&#39537;&#21160;&#30340;&#24674;&#22797;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36866;&#24212;&#24037;&#20855;&#20855;&#26377;&#28508;&#22312;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#23616;&#38480;&#22312;&#22256;&#24785;&#24230;&#35780;&#20272;&#21644;&#31616;&#21333;&#20219;&#21153;&#19978;&#65292;&#23545;&#25552;&#31034;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#27809;&#26377;&#32473;&#20986;&#26126;&#30830;&#30340;&#20449;&#24515;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#20851;&#38190;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLM&#21387;&#32553;&#20013;&#22825;&#30495;&#25552;&#31034;&#30340;&#33030;&#24369;&#24615;&#65292;&#21363;&#36807;&#24230;&#20381;&#36182;&#21333;&#19968;&#36755;&#20837;&#30340;&#25552;&#31034;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25512;&#29702;&#26102;&#21160;&#24577;&#25552;&#31034;(IDP)&#30340;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#33258;&#20027;&#36873;&#25321;&#26368;&#20339;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), while transformative for NLP, come with significant computational demands, underlining the need for efficient, training-free compression. Notably, despite the marked improvement in training-free compression for the largest of LLMs, our tests using LLaMA-7B and OPT-6.7b highlight a significant performance drop in several realistic downstream tasks. Investigation into the trade-off between resource-intensive post-compression re-training highlights the prospect of prompt-driven recovery as a lightweight adaption tool. However, existing studies, confined mainly to perplexity evaluations and simple tasks, fail to offer unequivocal confidence in the scalability and generalizability of prompting. We tackle this uncertainty in two key ways. First, we uncover the vulnerability of naive prompts in LLM compression as an over-reliance on a singular prompt per input. In response, we propose inference-time dynamic prompting (IDP), a mechanism that autonomously chooses f
&lt;/p&gt;</description></item><item><title>ToRA&#26159;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#21644;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23398;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;13%-19%&#30340;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#65292;&#24182;&#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17452</link><description>&lt;p&gt;
ToRA&#65306;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. (arXiv:2309.17452v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17452
&lt;/p&gt;
&lt;p&gt;
ToRA&#26159;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#21644;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23398;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;13%-19%&#30340;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#65292;&#24182;&#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38598;&#25104;&#24037;&#20855;&#30340;&#25512;&#29702;&#20195;&#29702;ToRA&#65292;&#23427;&#36890;&#36807;&#26080;&#32541;&#22320;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;&#20363;&#22914;&#35745;&#31639;&#24211;&#21644;&#31526;&#21495;&#27714;&#35299;&#22120;&#65289;&#30340;&#21033;&#29992;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#23558;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#19982;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#20026;&#20102;&#35757;&#32451;ToRA&#65292;&#25105;&#20204;&#31934;&#36873;&#20102;&#25968;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#20114;&#21160;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#65292;&#24212;&#29992;&#27169;&#20223;&#23398;&#20064;&#20110;&#27880;&#37322;&#65292;&#24182;&#25552;&#20986;&#36755;&#20986;&#31354;&#38388;&#25972;&#24418;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ToRA&#27169;&#22411;&#22312;10&#20010;&#28085;&#30422;&#21508;&#31181;&#35268;&#27169;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#36798;&#21040;13%&#33267;19%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ToRA-7B &#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#65292;&#36229;&#36234;&#20102;&#26368;&#20339;&#24320;&#28304;&#27169;&#22411;WizardMath&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#36234;&#20027;&#39064;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#21270;&#30340;&#20840;&#38754;&#38750;&#20998;&#24067;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#31574;&#30053;&#65292;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#30340;&#31934;&#32454;&#35843;&#33410;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.08316</link><description>&lt;p&gt;
&#36328;&#36234;&#20027;&#39064;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#21270;&#65306;&#23545;&#20840;&#38754;&#30340;&#38750;&#20998;&#24067;&#22330;&#26223;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bridging Topic, Domain, and Language Shifts: An Evaluation of Comprehensive Out-of-Distribution Scenarios. (arXiv:2309.08316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#36234;&#20027;&#39064;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#21270;&#30340;&#20840;&#38754;&#38750;&#20998;&#24067;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#31574;&#30053;&#65292;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#30340;&#31934;&#32454;&#35843;&#33410;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65288;&#22914;&#20105;&#35770;&#25366;&#25496;&#65289;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#32463;&#24120;&#19979;&#38477;&#12290;&#36825;&#31181;&#38477;&#32423;&#21457;&#29983;&#22312;&#26032;&#35805;&#39064;&#20986;&#29616;&#65292;&#25110;&#20854;&#20182;&#25991;&#26412;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#24471;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#38750;&#20998;&#24067;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#24847;&#22320;&#20445;&#30041;&#29305;&#23450;&#23454;&#20363;&#36827;&#34892;&#27979;&#35797;&#26469;&#27169;&#25311;&#36825;&#31181;&#20998;&#24067;&#21464;&#21270;&#65292;&#20363;&#22914;&#31038;&#20132;&#23186;&#20307;&#39046;&#22495;&#25110;&#22826;&#38451;&#33021;&#20027;&#39064;&#12290;&#19982;&#20808;&#21069;&#20851;&#27880;&#29305;&#23450;&#21464;&#21270;&#21644;&#24230;&#37327;&#26631;&#20934;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#30830;&#23450;&#27867;&#21270;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#28085;&#30422;&#20027;&#39064;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#21270;&#30340;&#21313;&#19968;&#20010;&#20998;&#31867;&#20219;&#21153;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#25552;&#31034;&#30340;&#31934;&#32454;&#35843;&#33410;&#20855;&#26377;&#26356;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#22312;&#35821;&#20041;&#19978;&#20027;&#35201;&#26377;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20063;&#26377;&#31867;&#20284;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) excel in in-distribution (ID) scenarios where train and test data are independent and identically distributed. However, their performance often degrades in real-world applications like argument mining. Such degradation happens when new topics emerge, or other text domains and languages become relevant. To assess LMs' generalization abilities in such out-of-distribution (OOD) scenarios, we simulate such distribution shifts by deliberately withholding specific instances for testing, as from the social media domain or the topic Solar Energy.  Unlike prior studies focusing on specific shifts and metrics in isolation, we comprehensively analyze OOD generalization. We define three metrics to pinpoint generalization flaws and propose eleven classification tasks covering topic, domain, and language shifts. Overall, we find superior performance of prompt-based fine-tuning, notably when train and test splits primarily differ semantically. Simultaneously, in-context learning
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26032;&#38395;&#27010;&#36848;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20250;&#37325;&#22797;&#21644;&#24378;&#21270;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#22411;&#20013;&#30340;&#26377;&#20559;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#20855;&#26377;&#25511;&#21046;&#20154;&#21475;&#23646;&#24615;&#30340;&#36755;&#20837;&#25991;&#26723;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08047</link><description>&lt;p&gt;
&#35843;&#26597;&#26032;&#38395;&#27010;&#36848;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Investigating Gender Bias in News Summarization. (arXiv:2309.08047v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26032;&#38395;&#27010;&#36848;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20250;&#37325;&#22797;&#21644;&#24378;&#21270;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#22411;&#20013;&#30340;&#26377;&#20559;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#20855;&#26377;&#25511;&#21046;&#20154;&#21475;&#23646;&#24615;&#30340;&#36755;&#20837;&#25991;&#26723;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#36848;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#12290;&#20197;&#24448;&#23545;&#27010;&#36848;&#27169;&#22411;&#30340;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#23427;&#20204;&#22312;&#20869;&#23481;&#36873;&#25321;&#12289;&#35821;&#27861;&#27491;&#30830;&#24615;&#21644;&#36830;&#36143;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;LLMs&#20250;&#37325;&#22797;&#21644;&#24378;&#21270;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#19968;&#20010;&#30456;&#23545;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#65292;&#27604;&#22914;&#27010;&#36848;&#65292;&#36825;&#20123;&#20559;&#35265;&#20250;&#23545;&#27169;&#22411;&#30340;&#36755;&#20986;&#20135;&#29983;&#24433;&#21709;&#21527;&#65311;&#20026;&#20102;&#35299;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20123;&#20851;&#20110;&#27010;&#36848;&#27169;&#22411;&#20013;&#30340;&#26377;&#20559;&#34892;&#20026;&#30340;&#23450;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20123;&#23454;&#38469;&#26041;&#27861;&#26469;&#37327;&#21270;&#23427;&#20204;&#12290;&#30001;&#20110;&#25105;&#20204;&#21457;&#29616;&#36755;&#20837;&#25991;&#26723;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#24178;&#25200;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;&#20180;&#32454;&#25511;&#21046;&#20154;&#21475;&#23646;&#24615;&#30340;&#36755;&#20837;&#25991;&#26723;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35268;&#36991;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20173;&#28982;&#20351;&#29992;&#19968;&#20123;&#29616;&#23454;&#30340;&#36755;&#20837;&#25991;&#26723;&#36827;&#34892;&#24037;&#20316;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19987;&#38376;&#26500;&#24314;&#30340;&#27010;&#36848;&#27169;&#22411;&#21644;&#36890;&#29992;&#29992;&#36884;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarization is an important application of large language models (LLMs). Most previous evaluation of summarization models has focused on their performance in content selection, grammaticality and coherence. However, it is well known that LLMs reproduce and reinforce harmful social biases. This raises the question: Do these biases affect model outputs in a relatively constrained setting like summarization?  To help answer this question, we first motivate and introduce a number of definitions for biased behaviours in summarization models, along with practical measures to quantify them. Since we find biases inherent to the input document can confound our analysis, we additionally propose a method to generate input documents with carefully controlled demographic attributes. This allows us to sidestep this issue, while still working with somewhat realistic input documents.  Finally, we apply our measures to summaries generated by both purpose-built summarization models and general purpose
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20843;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23454;&#39564;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;&#27169;&#22411;&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.07430</link><description>&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. (arXiv:2309.07430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20843;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23454;&#39564;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;&#27169;&#22411;&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#24037;&#20316;&#20013;&#65292;&#27983;&#35272;&#22823;&#37327;&#30340;&#25991;&#26412;&#25968;&#25454;&#24182;&#24635;&#32467;&#20851;&#38190;&#20449;&#24687;&#23545;&#20020;&#24202;&#21307;&#29983;&#30340;&#26102;&#38388;&#20998;&#37197;&#36896;&#25104;&#20102;&#24456;&#22823;&#30340;&#36127;&#25285;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21508;&#31181;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#20005;&#26684;&#30340;&#26816;&#39564;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20843;&#20010;LLMs&#36827;&#34892;&#20102;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#19981;&#21516;&#30340;&#25688;&#35201;&#20219;&#21153;&#65306;&#25918;&#23556;&#23398;&#25253;&#21578;&#12289;&#24739;&#32773;&#38382;&#39064;&#12289;&#30149;&#21382;&#35760;&#24405;&#21644;&#21307;&#24739;&#23545;&#35805;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#27169;&#22411;&#21644;&#36866;&#24212;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;LLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#21487;&#33021;&#19981;&#20250;&#24102;&#26469;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19982;&#20845;&#21517;&#21307;&#29983;&#36827;&#34892;&#30340;&#20020;&#24202;&#38405;&#35835;&#32773;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;LLM&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#36827;&#19968;&#27493;&#23450;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#21644;&#20154;&#31867;&#22312;&#38754;&#23545;&#30340;&#20849;&#21516;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sifting through vast textual data and summarizing key information imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy across diverse clinical summarization tasks has not yet been rigorously examined. In this work, we employ domain adaptation methods on eight LLMs, spanning six datasets and four distinct summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not lead to improved results. Further, in a clinical reader study with six physicians, we depict that summaries from the best adapted LLM are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis delineates mutual challenges faced by both LLMs and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#22522;&#20110;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2309.02726</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20551;&#35774;&#21457;&#29616;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. (arXiv:2309.02726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#22522;&#20110;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31185;&#23398;&#23478;&#35266;&#23519;&#19990;&#30028;&#24182;&#35797;&#22270;&#25552;&#20986;&#35299;&#37322;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#30340;&#20551;&#35774;&#26102;&#65292;&#20551;&#35774;&#24402;&#32435;&#34987;&#35748;&#20026;&#26159;&#20027;&#35201;&#30340;&#25512;&#29702;&#31867;&#22411;&#12290;&#36807;&#21435;&#20851;&#20110;&#20551;&#35774;&#24402;&#32435;&#30340;&#30740;&#31350;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;&#65288;1&#65289;&#25968;&#25454;&#38598;&#30340;&#35266;&#23519;&#27880;&#37322;&#19981;&#26159;&#21407;&#22987;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#32780;&#26159;&#25163;&#21160;&#36873;&#25321;&#30340;&#21477;&#23376;&#65288;&#23548;&#33268;&#20102;&#19968;&#20010;&#23553;&#38381;&#39046;&#22495;&#30340;&#35774;&#32622;&#65289;&#65307;&#65288;2&#65289;&#23454;&#38469;&#30340;&#20551;&#35774;&#27880;&#37322;&#20027;&#35201;&#26159;&#24120;&#35782;&#30693;&#35782;&#65292;&#20351;&#24471;&#20219;&#21153;&#19981;&#22826;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;50&#31687;&#21457;&#34920;&#22312;&#39030;&#32423;&#31038;&#20250;&#31185;&#23398;&#26399;&#21002;&#19978;&#30340;&#26368;&#26032;&#35770;&#25991;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#25910;&#38598;&#20102;&#24320;&#21457;&#35770;&#25991;&#20013;&#30340;&#20551;&#35774;&#25152;&#38656;&#30340;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#20165;&#36890;&#36807;&#19968;&#22534;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#23601;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#21487;&#20197;&#35299;&#20915;&#20197;&#21069;&#20851;&#20110;&#20551;&#35774;&#24402;&#32435;&#30340;&#30740;&#31350;&#25152;&#38754;&#20020;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction has a limited setting that (1) the observation annotations of the dataset are not raw web corpus but are manually selected sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses annotations are mostly commonsense knowledge, making the task less challenging. In this work, we propose the first NLP dataset for social science academic hypotheses discovery, consisting of 50 recent papers published in top social science journals. Raw web corpora that are necessary for developing hypotheses in the published papers are also collected in the dataset, with the final goal of creating a system that automatically generates valid, novel, and helpful (to human researchers) hypotheses, given only a pile of raw web corpora. The new dataset can tackle the previou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.00378</link><description>&lt;p&gt;
&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Long-Term Memorability On Advertisements. (arXiv:2309.00378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#33457;&#36153;&#25968;&#21313;&#20159;&#32654;&#20803;&#22312;&#24191;&#21578;&#19978;&#65292;&#20294;&#26159;&#25237;&#20837;&#21040;&#24191;&#21578;&#19978;&#30340;&#37329;&#38065;&#33021;&#36215;&#22810;&#22823;&#20316;&#29992;&#21602;&#65311;&#24403;&#39038;&#23458;&#22312;&#36141;&#20080;&#26102;&#26080;&#27861;&#36776;&#35748;&#20986;&#20182;&#20204;&#30475;&#36807;&#30340;&#21697;&#29260;&#30340;&#35805;&#65292;&#33457;&#22312;&#24191;&#21578;&#19978;&#30340;&#38065;&#22522;&#26412;&#19978;&#23601;&#34987;&#28010;&#36153;&#20102;&#12290;&#23613;&#31649;&#22312;&#33829;&#38144;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#20851;&#20110;&#24191;&#21578;&#35760;&#24518;&#21147;&#30340;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#23545;&#29305;&#23450;&#20869;&#23481;&#31867;&#22411;&#65288;&#22914;&#29289;&#20307;&#21644;&#21160;&#20316;&#35270;&#39057;&#65289;&#36827;&#34892;&#30701;&#26399;&#22238;&#24518;&#65288;&lt;5&#20998;&#38047;&#65289;&#30340;&#30740;&#31350;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24191;&#21578;&#34892;&#19994;&#21482;&#20851;&#24515;&#38271;&#26399;&#35760;&#24518;&#65288;&#20960;&#20010;&#23567;&#26102;&#25110;&#26356;&#38271;&#26102;&#38388;&#65289;&#65292;&#32780;&#19988;&#24191;&#21578;&#20960;&#20046;&#24635;&#26159;&#39640;&#24230;&#22810;&#27169;&#24335;&#21270;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#24418;&#24335;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26469;&#35762;&#25925;&#20107;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#26426;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#20010;&#22823;&#35268;&#27169;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#20849;&#26377;1203&#21517;&#21442;&#19982;&#32773;&#21644;2205&#20010;&#24191;&#21578;&#28085;&#30422;&#20102;276&#20010;&#21697;&#29260;&#12290;&#22312;&#19981;&#21516;&#21442;&#19982;&#32773;&#23376;&#32676;&#20307;&#21644;&#24191;&#21578;&#31867;&#22411;&#19978;&#36827;&#34892;&#32479;&#35745;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#35768;&#22810;&#26377;&#20851;&#20160;&#20040;&#20351;&#24191;&#21578;&#38590;&#24536;&#30340;&#26377;&#36259;&#35265;&#35299;-&#26080;&#35770;&#26159;&#20869;&#23481;&#36824;&#26159;
&lt;/p&gt;
&lt;p&gt;
Marketers spend billions of dollars on advertisements but to what end? At the purchase time, if customers cannot recognize a brand for which they saw an ad, the money spent on the ad is essentially wasted. Despite its importance in marketing, until now, there has been no study on the memorability of ads in the ML literature. Most studies have been conducted on short-term recall (&lt;5 mins) on specific content types like object and action videos. On the other hand, the advertising industry only cares about long-term memorability (a few hours or longer), and advertisements are almost always highly multimodal, depicting a story through its different modalities (text, images, and videos). With this motivation, we conduct the first large scale memorability study consisting of 1203 participants and 2205 ads covering 276 brands. Running statistical tests over different participant subpopulations and ad-types, we find many interesting insights into what makes an ad memorable - both content and h
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27493;&#35299;&#27602;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#38454;&#27573;&#36827;&#34892;&#35299;&#27602;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#26080;&#27602;&#25552;&#31034;&#36827;&#34892;&#36830;&#32493;&#29983;&#25104;&#26469;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#35774;&#35745;Detox-Chain&#26469;&#26657;&#20934;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.08295</link><description>&lt;p&gt;
&#20998;&#27493;&#35299;&#27602;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Detoxify Language Model Step-by-Step. (arXiv:2308.08295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27493;&#35299;&#27602;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#38454;&#27573;&#36827;&#34892;&#35299;&#27602;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#26080;&#27602;&#25552;&#31034;&#36827;&#34892;&#36830;&#32493;&#29983;&#25104;&#26469;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#35774;&#35745;Detox-Chain&#26469;&#26657;&#20934;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#27602;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#22312;&#20445;&#25345;&#29983;&#25104;&#33021;&#21147;&#30340;&#21516;&#26102;&#36991;&#20813;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#30340;&#23433;&#20840;&#24615;&#65292;&#20808;&#21069;&#30340;&#35299;&#27602;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#25110;&#22312;&#21333;&#27493;&#39588;&#20013;&#20174;&#19981;&#21516;&#26041;&#38754;&#32422;&#26463;&#29983;&#25104;&#26469;&#35299;&#27602;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#27839;&#30528;&#26377;&#27602;&#25552;&#31034;&#29983;&#25104;&#65292;&#35299;&#27602;&#26041;&#27861;&#30340;&#24037;&#20316;&#26041;&#21521;&#19982;&#20043;&#30456;&#21453;&#65292;&#36825;&#20123;&#26041;&#27861;&#23558;&#22823;&#22823;&#24433;&#21709;LLM&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#22914;&#35805;&#35821;&#36830;&#36143;&#24615;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#20914;&#31361;&#65292;&#25105;&#20204;&#23558;&#35299;&#27602;&#36807;&#31243;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#23376;&#27493;&#39588;&#65292;&#20854;&#20013;&#35299;&#27602;&#38598;&#20013;&#22312;&#36755;&#20837;&#38454;&#27573;&#65292;&#38543;&#21518;&#30340;&#36830;&#32493;&#29983;&#25104;&#22522;&#20110;&#26080;&#27602;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;Detox-Chain&#26469;&#26657;&#20934;LLMs&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#26377;&#24207;&#30340;&#26041;&#24335;&#36830;&#25509;&#19978;&#36848;&#23376;&#27493;&#39588;&#65292;&#36825;&#20351;&#24471;LLMs&#21487;&#20197;&#36827;&#34892;&#36830;&#32493;&#30340;&#35299;&#27602;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detoxification for LLMs is challenging since it requires models to avoid generating harmful content while maintaining the generation capability. To ensure the safety of generations, previous detoxification methods detoxify the models by changing the data distributions or constraining the generations from different aspects in a single-step manner. However, these approaches will dramatically affect the generation quality of LLMs, e.g., discourse coherence and semantic consistency, since language models tend to generate along the toxic prompt while detoxification methods work in the opposite direction. To handle such a conflict, we decompose the detoxification process into different sub-steps, where the detoxification is concentrated in the input stage and the subsequent continual generation is based on the non-toxic prompt. Besides, we also calibrate the strong reasoning ability of LLMs by designing a Detox-Chain to connect the above sub-steps in an orderly manner, which allows LLMs to d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;</title><link>http://arxiv.org/abs/2308.07876</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#26412;&#30693;&#35782;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;ChatGPT&#26469;&#21512;&#25104;&#25919;&#27835;&#38646;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20107;&#20214;&#32534;&#30721;&#30340;&#30417;&#30563;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#36828;&#36229;&#36807;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20165;&#20381;&#36182;&#20110;&#26032;&#30340;&#27880;&#37322;&#65292;&#24573;&#35270;&#20102;&#19987;&#23478;&#25968;&#25454;&#24211;&#20013;&#30340;&#22823;&#37327;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#12289;&#35821;&#24577;&#21644;&#31867;&#21035;&#28040;&#27495;&#30340;&#19981;&#21516;&#23618;&#27425;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#26032;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;ChatGPT&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#31361;&#20986;&#20102;ZSP&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;ZSP&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#30340;F1&#24471;&#20998;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25552;&#39640;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
&lt;/p&gt;</description></item><item><title>FLASK&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10928</link><description>&lt;p&gt;
FLASK: &#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets. (arXiv:2307.10928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10928
&lt;/p&gt;
&lt;p&gt;
FLASK&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25351;&#20196;&#38656;&#35201;&#19982;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#25216;&#33021;&#38598;&#26681;&#25454;&#25351;&#20196;&#32780;&#24322;&#65292;&#22240;&#27492;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31895;&#31890;&#24230;&#35780;&#20272;&#65288;&#21363;&#22522;&#20110;&#25972;&#20307;&#20559;&#22909;&#30340;&#35780;&#20272;&#65289;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#26410;&#32771;&#34385;&#38656;&#35201;&#23454;&#20363;&#32423;&#25216;&#33021;&#32452;&#21512;&#30340;&#29992;&#25143;&#25351;&#20196;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FLASK&#65288;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32454;&#31890;&#24230;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#23427;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35780;&#20272;&#30340;&#32454;&#31890;&#24230;&#23545;&#20110;&#33719;&#24471;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;FLASK&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22810;&#20010;&#24320;&#28304;&#21644;&#19987;&#26377;LLMs&#65292;&#24182;&#35266;&#23519;&#21040;&#39640;&#24230;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlati
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35770;&#25991;&#25968;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#30740;&#31350;&#37325;&#28857;&#36880;&#28176;&#36716;&#21521;&#31038;&#20250;&#24433;&#21709;&#12290;&#19982;LLM&#30456;&#20851;&#30340;&#35770;&#25991;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#30340;&#36235;&#21183;&#65292;&#26032;&#21457;&#34920;&#20851;&#20110;LLM&#30340;&#20316;&#32773;&#26356;&#27880;&#37325;&#24212;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.10700</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22609;&#36896;&#24182;&#21463;&#21040;&#31038;&#20250;&#30340;&#24433;&#21709;&#65306;arXiv&#20986;&#29256;&#27169;&#24335;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large language models shape and are shaped by society: A survey of arXiv publication patterns. (arXiv:2307.10700v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10700
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35770;&#25991;&#25968;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#30740;&#31350;&#37325;&#28857;&#36880;&#28176;&#36716;&#21521;&#31038;&#20250;&#24433;&#21709;&#12290;&#19982;LLM&#30456;&#20851;&#30340;&#35770;&#25991;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#30340;&#36235;&#21183;&#65292;&#26032;&#21457;&#34920;&#20851;&#20110;LLM&#30340;&#20316;&#32773;&#26356;&#27880;&#37325;&#24212;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35770;&#25991;&#25968;&#37327;&#36817;&#24180;&#26469;&#21576;&#24613;&#21095;&#22686;&#21152;&#65292;&#36825;&#31181;&#21464;&#21270;&#23545;&#31185;&#23398;&#39046;&#22495;&#20135;&#29983;&#20102;&#25103;&#21095;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#36827;&#34892;&#35814;&#32454;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;CS&#21644;Stat arXiv&#19978;&#21457;&#24067;&#30340;388K&#31687;&#35770;&#25991;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;2023&#24180;&#19982;2018-2022&#24180;&#20043;&#38388;&#21457;&#34920;&#27169;&#24335;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;LLM&#35770;&#25991;&#30340;&#27604;&#20363;&#22686;&#21152;&#24773;&#20917;&#65292;&#24471;&#21040;&#20102;&#26368;&#22810;&#20851;&#27880;&#30340;&#19982;LLM&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#25776;&#20889;LLM&#35770;&#25991;&#30340;&#20316;&#32773;&#65292;&#20316;&#32773;&#30340;&#30740;&#31350;&#20027;&#39064;&#19982;&#32972;&#26223;&#30340;&#30456;&#20851;&#24615;&#65292;&#21306;&#20998;&#39640;&#34987;&#24341;&#29992;LLM&#35770;&#25991;&#30340;&#22240;&#32032;&#65292;&#20197;&#21450;&#22269;&#38469;&#21512;&#20316;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#30740;&#31350;&#36234;&#26469;&#36234;&#20851;&#27880;&#31038;&#20250;&#24433;&#21709;&#65306;&#22312;&#35745;&#31639;&#26426;&#19982;&#31038;&#20250;&#23376;arXiv&#19978;&#65292;&#19982;LLM&#30456;&#20851;&#30340;&#35770;&#25991;&#27604;&#20363;&#22686;&#21152;&#20102;18&#20493;&#65292;&#26032;&#21457;&#34920;&#20851;&#20110;LLM&#30340;&#20316;&#32773;&#26356;&#20542;&#21521;&#20110;&#20851;&#27880;&#24212;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;LLM&#30740;&#31350;&#20063;&#21463;&#21040;&#31038;&#20250;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a steep recent increase in the number of large language model (LLM) papers, producing a dramatic shift in the scientific landscape which remains largely undocumented through bibliometric analysis. Here, we analyze 388K papers posted on the CS and Stat arXivs, focusing on changes in publication patterns in 2023 vs. 2018-2022. We analyze how the proportion of LLM papers is increasing; the LLM-related topics receiving the most attention; the authors writing LLM papers; how authors' research topics correlate with their backgrounds; the factors distinguishing highly cited LLM papers; and the patterns of international collaboration. We show that LLM research increasingly focuses on societal impacts: there has been an 18x increase in the proportion of LLM-related papers on the Computers and Society sub-arXiv, and authors newly publishing on LLMs are more likely to focus on applications and societal impacts than more experienced authors. LLM research is also shaped by social dyn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20803;&#25512;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#31526;&#21495;&#35299;&#26500;&#30340;&#26041;&#24335;&#65292;&#23558;&#19981;&#21516;&#25512;&#29702;&#38382;&#39064;&#36716;&#21270;&#20026;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17820</link><description>&lt;p&gt;
&#20803;&#25512;&#29702;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#31526;&#21495;&#35299;&#26500;
&lt;/p&gt;
&lt;p&gt;
Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models. (arXiv:2306.17820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20803;&#25512;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#31526;&#21495;&#35299;&#26500;&#30340;&#26041;&#24335;&#65292;&#23558;&#19981;&#21516;&#25512;&#29702;&#38382;&#39064;&#36716;&#21270;&#20026;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31526;&#21495;&#21270;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#26144;&#23556;&#21040;&#26356;&#21152;&#35821;&#27861;&#23436;&#22791;&#19988;&#27809;&#26377;&#27495;&#20041;&#30340;&#24418;&#24335;&#35821;&#35328;&#65288;&#20363;&#22914;Python&#12289;SQL&#65289;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#31163;&#24320;&#20102;&#33258;&#28982;&#35821;&#35328;&#26412;&#36523;&#65292;&#20559;&#31163;&#20102;&#20154;&#31867;&#24605;&#32500;&#30340;&#20064;&#24815;&#65292;&#32780;&#26356;&#22810;&#22320;&#36814;&#21512;&#20102;&#35745;&#31639;&#26426;&#30340;&#25191;&#34892;&#24605;&#32500;&#26041;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24076;&#26395;&#20174;&#35821;&#35328;&#23398;&#20013;&#31526;&#21495;&#30340;&#27010;&#24565;&#20986;&#21457;&#26469;&#31616;&#21270;&#33258;&#28982;&#35821;&#35328;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#19981;&#21516;&#33258;&#28982;&#35821;&#20041;&#20013;&#21253;&#21547;&#30340;&#25512;&#29702;&#38382;&#39064;&#30340;&#24120;&#35265;&#34920;&#36798;&#26041;&#24335;&#21644;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#36825;&#31181;&#32771;&#34385;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20803;&#25512;&#29702;&#8221;&#65292;&#23427;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#23436;&#25104;&#35821;&#20041;&#31526;&#21495;&#30340;&#35299;&#26500;&#65292;&#21363;&#35821;&#20041;&#35299;&#26512;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#23558;&#26576;&#20123;&#25512;&#29702;&#20219;&#21153;&#30340;&#19981;&#21516;&#38382;&#39064;&#20943;&#23569;&#21040;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#33719;&#24471;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolization methods in large language models (LLMs) have been shown effective to improve LLMs' reasoning ability. However, most of these approaches hinge on mapping natural languages to formal languages (e.g., Python, SQL) that are more syntactically complete and free of ambiguity. Although effective, they depart from the natural language itself and deviate from the habits of human thinking, and instead cater more to the execution mindset of computers. In contrast, we hope to simplify natural language by starting from the concept of symbols in linguistics itself, so that LLMs can learn the common formulation and general solution of reasoning problems wrapped in different natural semantics. From this consideration, we propose \textbf{Meta-Reasoning}, which allows LLMs to automatically accomplish semantic-symbol deconstruction, i.e., semantic resolution, to maximally reduce different questions of certain reasoning tasks to similar natural language representation, thus gaining the abili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290;&#22312;Common Voice&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23454;&#29616;18% WER&#12290;&#32780;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13330</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised ASR via Cross-Lingual Pseudo-Labeling. (arXiv:2305.13330v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290;&#22312;Common Voice&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23454;&#29616;18% WER&#12290;&#32780;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#38750;&#37197;&#23545;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#26469;&#35757;&#32451;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#20551;&#23450;&#19981;&#33021;&#20351;&#29992;&#20219;&#20309;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21363;&#20351;&#27809;&#26377;&#32473;&#23450;&#35821;&#35328;&#30340;&#20219;&#20309;&#26631;&#27880;&#38899;&#39057;&#65292;&#20063;&#22987;&#32456;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#30340;&#23383;&#31526;&#32423;&#22768;&#23398;&#27169;&#22411;&#65288;AM&#65289;&#65292;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290; &#36825;&#37324;&#65292;&#8220;&#26080;&#30417;&#30563;&#8221;&#24847;&#21619;&#30528;&#27809;&#26377;&#21487;&#29992;&#20110;&#30446;&#26631;&#35821;&#35328;&#30340;&#26631;&#27880;&#38899;&#39057;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#65288;i&#65289;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;AM&#29983;&#25104;&#8220;&#30446;&#26631;&#8221;&#35821;&#35328;&#30340;&#20266;&#26631;&#31614;&#65288;PLs&#65289;&#65307;&#65288;ii&#65289;&#20351;&#29992;&#8220;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#8221;&#38480;&#21046;&#36825;&#20123;PLs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Common Voice&#19978;&#38750;&#24120;&#26377;&#25928;&#65306;&#20363;&#22914;&#65292;&#23558;&#33521;&#35821;AM&#20256;&#36882;&#21040;&#26031;&#29926;&#24076;&#37324;&#35821;&#21487;&#20197;&#23454;&#29616;18&#65285;&#30340;WER&#12290; &#23427;&#36824;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#20110;&#23383;&#31526;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that it is possible to train an $\textit{unsupervised}$ automatic speech recognition (ASR) system using only unpaired audio and text. Existing unsupervised ASR methods assume that no labeled data can be used for training. We argue that even if one does not have any labeled audio for a given language, there is $\textit{always}$ labeled data available for other languages. We show that it is possible to use character-level acoustic models (AMs) from other languages to bootstrap an $\textit{unsupervised}$ AM in a new language. Here, "unsupervised" means no labeled audio is available for the $\textit{target}$ language. Our approach is based on two key ingredients: (i) generating pseudo-labels (PLs) of the $\textit{target}$ language using some $\textit{other}$ language AM and (ii) constraining these PLs with a $\textit{target language model}$. Our approach is effective on Common Voice: e.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms characte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11738</link><description>&lt;p&gt;
CRITIC&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24037;&#20855;&#20132;&#20114;&#25209;&#35780;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#38750;&#24120;&#24341;&#20154;&#27880;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#65292;&#20363;&#22914;&#20986;&#29616;&#24187;&#35273;&#20107;&#23454;&#65292;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#20195;&#30721;&#25110;&#21019;&#24314;&#20882;&#29359;&#21644;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#19982;&#36825;&#20123;&#27169;&#22411;&#19981;&#21516;&#65292;&#20154;&#31867;&#36890;&#24120;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#20132;&#21449;&#26816;&#26597;&#21644;&#31934;&#28860;&#20182;&#20204;&#30340;&#21021;&#27493;&#20869;&#23481;&#65292;&#20363;&#22914;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#25110;&#20351;&#29992;&#20195;&#30721;&#35299;&#37322;&#22120;&#36827;&#34892;&#35843;&#35797;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#65288;&#23454;&#36136;&#19978;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65289;&#20197;&#31867;&#20284;&#20110;&#20154;&#31867;&#19982;&#24037;&#20855;&#20132;&#20114;&#30340;&#26041;&#24335;&#39564;&#35777;&#21644;&#36880;&#27493;&#20462;&#27491;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20174;&#21021;&#22987;&#36755;&#20986;&#24320;&#22987;&#65292;CRITIC&#19982;&#36866;&#24403;&#30340;&#24037;&#20855;&#20132;&#20114;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#28982;&#21518;&#26681;&#25454;&#22312;&#27492;&#39564;&#35777;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#21453;&#39304;&#20462;&#25913;&#36755;&#20986;&#12290;&#28041;&#21450;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#12289;&#25968;&#23398;&#31243;&#24207;&#32508;&#21512;&#21644;&#27602;&#24615;&#26816;&#27979;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;LLMs&#33021;&#22815;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#24182;&#32416;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;X-adapter&#25554;&#25300;&#24335;&#27169;&#22359;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#39640;&#25928;&#22320;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#35270;&#35273;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.07358</link><description>&lt;p&gt;
&#21033;&#29992;&#36328;&#27169;&#24577;&#36866;&#37197;&#22120;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#22810;&#21151;&#33021;&#39640;&#25928;&#30340;&#35270;&#35273;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Towards Versatile and Efficient Visual Knowledge Injection into Pre-trained Language Models with Cross-Modal Adapters. (arXiv:2305.07358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;X-adapter&#25554;&#25300;&#24335;&#27169;&#22359;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#39640;&#25928;&#22320;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#35270;&#35273;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#22810;&#27169;&#24577;&#30693;&#35782;&#23398;&#20064;&#35821;&#35328;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20165;&#25903;&#25345;&#25991;&#26412;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25554;&#25300;&#24335;&#27169;&#22359;X-adapter&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#23545;&#40784;&#35270;&#35273;&#21644;&#25991;&#26412;&#30693;&#35782;&#65292;&#28789;&#27963;&#39640;&#25928;&#22320;&#21521;PLMs&#27880;&#20837;&#35270;&#35273;&#30693;&#35782;&#12290; X-adapter&#21253;&#21547;&#20004;&#20010;&#23376;&#27169;&#22359;V-expert&#21644;T-expert&#65292;&#21487;&#20197;&#26681;&#25454;&#19979;&#28216;&#20219;&#21153;&#28608;&#27963;&#19981;&#21516;&#30340;&#23376;&#27169;&#22359;&#65292;&#26469;&#34701;&#21512;VLMs&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans learn language via multi-modal knowledge. However, due to the text-only pre-training scheme, most existing pre-trained language models (PLMs) are hindered from the multi-modal information.  To inject visual knowledge into PLMs, existing methods incorporate either the text or image encoder of vision-language models (VLMs) to encode the visual information and update all the original parameters of PLMs for knowledge fusion.  In this paper, we propose a new plug-and-play module, X-adapter, to flexibly leverage the aligned visual and textual knowledge learned in pre-trained VLMs and efficiently inject them into PLMs.  Specifically, we insert X-adapters into PLMs, and only the added parameters are updated during adaptation.  To fully exploit the potential in VLMs, X-adapters consist of two sub-modules, V-expert and T-expert, to fuse VLMs' image and text representations, respectively.  We can opt for activating different sub-modules depending on the downstream tasks.  Experimental resu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07303</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#20998;&#24067;&#20449;&#24687;&#30340;&#31070;&#32463;&#35789;&#21521;&#37327;&#19968;&#30452;&#20197;&#26469;&#37117;&#33021;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#38590;&#20197;&#35299;&#37322;&#21644;&#25511;&#21046;&#30340;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20855;&#26377;&#36882;&#24402;&#30340;&#65292;&#33258;&#35828;&#26126;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#21487;&#20197;&#25903;&#25345;&#33021;&#22815;&#20445;&#30041;&#21521;&#37327;&#31354;&#38388;&#20013;&#26174;&#24335;&#27010;&#24565;&#20851;&#31995;&#21644;&#32422;&#26463;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#33539; paradigm&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#12289;&#22810;&#20851;&#31995;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#26144;&#23556;&#23450;&#20041;&#21644;&#23450;&#20041;&#26415;&#35821;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#20165;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#35789;&#21521;&#37327;&#12290;&#36890;&#36807;&#33258;&#21160;&#20174;&#23450;&#20041;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32763;&#35793;&#30446;&#26631;&#35268;&#33539;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26694;&#26550;&#19987;&#38376;&#35774;&#23450;&#20026;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#25429;&#33719;&#30001;&#23450;&#20041;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01713</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#35299;&#37322;&#30340;&#38750;&#20132;&#20114;&#35821;&#20041;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks. (arXiv:2305.01713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32454;&#21270;&#36830;&#32493;&#31354;&#38388;&#30340;&#21477;&#23376;&#34920;&#24449;&#19978;&#36827;&#34892;&#35299;&#32806;&#21487;&#20197;&#22312;&#23450;&#20301;&#26126;&#30830;&#21457;&#29983;&#30340;&#29983;&#25104;&#22240;&#32032;&#30340;&#21516;&#26102;&#65292;&#25913;&#36827;&#21487;&#35299;&#37322;&#24615;&#21644;&#35821;&#20041;&#25511;&#21046;&#65292;&#36825;&#20026;&#22522;&#20110;&#31070;&#32463;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#20102;&#19968;&#20123;&#31526;&#21495;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#28789;&#27963;&#24615;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#23558;BERT-GPT2&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#21487;&#20998;&#31163;&#30340;&#35821;&#20041;&#31354;&#38388;&#26469;&#35299;&#38500;&#32534;&#30721;&#30340;&#38544;&#34255;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;INN&#33021;&#22815;&#23558;&#20998;&#24067;&#24335;&#38544;&#34255;&#31354;&#38388;&#36716;&#25442;&#20026;&#26356;&#22909;&#30340;&#35821;&#20041;&#19978;&#35299;&#32806;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangling sentence representations over continuous spaces can be a critical process in improving interpretability and semantic control by localising explicit generative factors. Such process confers to neural-based language models some of the advantages that are characteristic of symbolic models, while keeping their flexibility. This work presents a methodology for disentangling the hidden space of a BERT-GPT2 autoencoder by transforming it into a more separable semantic space with the support of a flow-based invertible neural network (INN). Experimental results indicate that the INN can transform the distributed hidden space into a better semantically disentangled latent space, resulting in better interpretability and controllability, when compared to recent state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#27604;&#36739;&#20102;&#20845;&#31181;&#20998;&#35789;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#19981;&#19968;&#23450;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2304.10813</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#35789;&#21487;&#36861;&#28335;&#24615;&#65306;&#19968;&#20010;&#27880;&#37322;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tokenization Tractability for Human and Machine Learning Model: An Annotation Study. (arXiv:2304.10813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10813
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#27604;&#36739;&#20102;&#20845;&#31181;&#20998;&#35789;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#19981;&#19968;&#23450;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#20063;&#26159;&#21487;&#36861;&#28335;&#30340;&#65311;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#65288;&#22914;&#36866;&#24403;&#24615;&#21644;&#21487;&#35835;&#24615;&#65289;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#35789;&#65288;&#22914;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#26085;&#35821;&#24120;&#35782;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;JGLUE&#30340;JCommmonsenseQA&#65289;&#20013;&#27604;&#36739;&#20102;&#20845;&#31181;&#20998;&#35789;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#23545;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#25991;&#26412;&#36827;&#34892;&#20998;&#35789;&#65292;&#24182;&#27604;&#36739;&#20102;&#20154;&#31867;&#26631;&#27880;&#32773;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24615;&#33021;&#12289;&#20998;&#35789;&#30340;&#36866;&#24403;&#24615;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#21709;&#24212;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23450;&#37327;&#35843;&#26597;&#32467;&#26524;&#65292;&#26174;&#31034;&#20986;&#23545;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#21487;&#36861;&#28335;&#30340;&#20998;&#35789;&#19981;&#19968;&#23450;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is tractable tokenization for humans also tractable for machine learning models? This study investigates relations between tractable tokenization for humans (e.g., appropriateness and readability) and one for models of machine learning (e.g., performance on an NLP task). We compared six tokenization methods on the Japanese commonsense question-answering dataset (JCommmonsenseQA in JGLUE). We tokenized question texts of the QA dataset with different tokenizers and compared the performance of human annotators and machine-learning models. Besides,we analyze relationships among the performance, appropriateness of tokenization, and response time to questions. This paper provides a quantitative investigation result that shows the tractable tokenizations for humans and machine learning models are not necessarily the same as each other.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65292;&#20855;&#26377;&#19981;&#21516;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26032;&#27169;&#24335;&#22312;&#26410;&#26469;&#26377;&#30528;&#24456;&#39640;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.12023</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#36923;&#36753;&#25512;&#29702;&#30740;&#31350;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Logical Reasoning over Natural Language as Knowledge Representation: A Survey. (arXiv:2303.12023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65292;&#20855;&#26377;&#19981;&#21516;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26032;&#27169;&#24335;&#22312;&#26410;&#26469;&#26377;&#30528;&#24456;&#39640;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#25512;&#29702;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#26234;&#33021;&#30340;&#26680;&#24515;&#12290;&#20197;&#24448;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#36923;&#36753;&#25512;&#29702;&#30740;&#31350;&#20351;&#29992;&#24418;&#24335;&#21270;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65288;&#21644;&#31526;&#21495;&#25512;&#29702;&#22120;&#65289;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24418;&#24335;&#21270;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#35777;&#26126;&#20855;&#26377;&#22256;&#38590;&#65288;&#20363;&#22914;&#33030;&#24369;&#24615;&#21644;&#30693;&#35782;&#33719;&#21462;&#29942;&#39048;&#65289;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65288;&#20197;&#21450;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#22120;&#65289;&#65292;&#21253;&#25324;&#36923;&#36753;&#25512;&#29702;&#30340;&#21746;&#23398;&#23450;&#20041;&#21644;&#20998;&#31867;&#65292;&#26032;&#27169;&#24335;&#30340;&#20248;&#21183;&#12289;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#26410;&#26469;&#38656;&#35201;&#30340;&#20219;&#21153;&#21644;&#26041;&#27861;&#20197;&#21450;&#19982;&#30456;&#20851; NLP &#39046;&#22495;&#30340;&#20851;&#31995;&#12290;&#36825;&#31181;&#26032;&#27169;&#24335;&#26159;&#24456;&#26377;&#21069;&#36884;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#24418;&#24335;&#21270;&#34920;&#31034;&#30340;&#35768;&#22810;&#25361;&#25112;&#65292;&#32780;&#19988;&#20063;&#20855;&#26377;&#20248;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical reasoning is central to human cognition and intelligence. Past research of logical reasoning within AI uses formal language as knowledge representation~(and symbolic reasoners). However, reasoning with formal language has proved challenging~(e.g., brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation~(and pretrained language models as reasoners), including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, desirable tasks &amp; methods in the future, and relation to related NLP fields. This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods.
&lt;/p&gt;</description></item></channel></rss>