<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;OpenAI ChatGPT&#21644;Google Bard&#31561;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#38752;&#24615;&#21457;&#29616;&#65292;&#20854;&#22312;&#24863;&#30693;&#21644;&#35780;&#20272;&#20889;&#20316;&#25552;&#31034;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#19982;&#32463;&#39564;&#20016;&#23500;&#30340;&#20154;&#31867;&#35780;&#20998;&#32773;&#30340;&#19968;&#33268;&#24615;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.05372</link><description>&lt;p&gt;
ChatGPT&#21644;Bard&#33021;&#22815;&#29983;&#25104;&#19968;&#33268;&#30340;&#35780;&#20272;&#39033;&#30446;&#21527;&#65311;&#38024;&#23545;&#20154;&#31867;&#34920;&#29616;&#30340;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance. (arXiv:2304.05372v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;OpenAI ChatGPT&#21644;Google Bard&#31561;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#38752;&#24615;&#21457;&#29616;&#65292;&#20854;&#22312;&#24863;&#30693;&#21644;&#35780;&#20272;&#20889;&#20316;&#25552;&#31034;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#19982;&#32463;&#39564;&#20016;&#23500;&#30340;&#20154;&#31867;&#35780;&#20998;&#32773;&#30340;&#19968;&#33268;&#24615;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;Bard&#26159;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#34987;&#35748;&#20026;&#33021;&#22815;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#22312;&#25945;&#32946;&#39046;&#22495;&#65292;&#36825;&#20123;AI&#25216;&#26415;&#24050;&#34987;&#29992;&#20110;&#35780;&#20272;&#21644;&#25945;&#23398;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;AI&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;&#35770;&#25991;&#35780;&#20998;&#21644;&#33258;&#21160;&#21270;&#30340;&#39033;&#30446;&#29983;&#25104;&#12290;&#36825;&#20123;&#24037;&#20855;&#24517;&#39035;&#20855;&#22791;&#30340;&#19968;&#39033;&#24515;&#29702;&#27979;&#37327;&#23646;&#24615;&#26159;&#21487;&#38752;&#24615;&#39640;&#65292;&#21363;AI&#20998;&#25968;&#19982;&#20154;&#31867;&#35780;&#20998;&#32773;&#24847;&#35265;&#19968;&#33268;&#12290;&#26412;&#25991;&#27979;&#37327;&#20102;OpenAI ChatGP&#21644;Google Bard&#30340;&#21487;&#38752;&#24615;&#65292;&#20197;&#35780;&#20272;&#36825;&#20123;&#24037;&#20855;&#22312;&#24863;&#30693;&#21644;&#35780;&#20272;&#20889;&#20316;&#25552;&#31034;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#19982;&#32463;&#39564;&#20016;&#23500;&#30340;&#20154;&#31867;&#35780;&#20998;&#32773;&#30340;&#19968;&#33268;&#24615;&#12290;&#20316;&#20026;&#32489;&#25928;&#25351;&#26631;&#30340;&#20869;&#37096;&#30456;&#20851;&#31995;&#25968;&#65288;ICC&#65289;&#26174;&#31034;&#65292;OpenAI ChatGPT&#21644;Google Bard&#30340;&#20114;&#21487;&#38752;&#24615;&#20302;&#20110;&#20154;&#31867;&#35780;&#20998;&#30340;&#37329;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that are slated to promise different applications in diverse areas. In education, these AI technologies have been tested for applications in assessment and teaching. In assessment, AI has long been used in automated essay scoring and automated item generation. One psychometric property that these tools must have to assist or replace humans in assessment is high reliability in terms of agreement between AI scores and human raters. In this paper, we measure the reliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and trained humans in perceiving and rating the complexity of writing prompts. Intraclass correlation (ICC) as a performance metric showed that the inter-reliability of both the OpenAI ChatGPT and the Google Bard were low against the gold standard of human ratings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19968;&#20010;&#26032;&#21457;&#23637;&#65306;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26426;&#21046;&#21487;&#33021;&#20250;&#23548;&#33268;&#26426;&#22120;&#20154;&#35760;&#20303;&#20102;&#38169;&#35823;&#21644;&#34394;&#20551;&#30340;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20107;&#23454;&#38472;&#36848;&#37325;&#22797;&#12290;</title><link>http://arxiv.org/abs/2304.05371</link><description>&lt;p&gt;
&#37027;&#19981;&#26159;&#20320;&#30340;&#35760;&#24518;&#65292;&#23427;&#26159;&#21035;&#20154;&#30340;&#65306;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#35760;&#24518;&#20013;&#25773;&#25746;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Those Aren't Your Memories, They're Somebody Else's: Seeding Misinformation in Chat Bot Memories. (arXiv:2304.05371v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19968;&#20010;&#26032;&#21457;&#23637;&#65306;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26426;&#21046;&#21487;&#33021;&#20250;&#23548;&#33268;&#26426;&#22120;&#20154;&#35760;&#20303;&#20102;&#38169;&#35823;&#21644;&#34394;&#20551;&#30340;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20107;&#23454;&#38472;&#36848;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19968;&#20010;&#26032;&#21457;&#23637;&#26159;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#65292;&#21487;&#20197;&#35760;&#20303;&#36807;&#21435;&#23545;&#35805;&#20013;&#30340;&#20449;&#24687;&#65292;&#20197;&#22686;&#21152;&#21709;&#24212;&#30340;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#26426;&#22120;&#20154;&#34987;&#35774;&#35745;&#20026;&#20174;&#20854;&#23545;&#35805;&#20249;&#20276;&#20013;&#25552;&#21462;&#20010;&#20154;&#24615;&#36136;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;&#34920;&#26126;&#23545;&#29305;&#23450;&#39068;&#33394;&#30340;&#20559;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#35760;&#24518;&#26426;&#21046;&#21487;&#33021;&#20250;&#23548;&#33268;&#24847;&#22806;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#20154;&#21487;&#20197;&#23558;&#20010;&#20154;&#38472;&#36848;&#19982;&#20449;&#24687;&#38472;&#36848;&#32467;&#21512;&#36215;&#26469;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#23558;&#20449;&#24687;&#38472;&#36848;&#19982;&#20010;&#20154;&#30693;&#35782;&#19968;&#36215;&#35760;&#24405;&#22312;&#20854;&#38271;&#26399;&#35760;&#24518;&#20013;&#12290;&#36825;&#24847;&#21619;&#30528;&#26426;&#22120;&#20154;&#21487;&#33021;&#34987;&#27450;&#39575;&#35760;&#20303;&#38169;&#35823;&#20449;&#24687;&#65292;&#24182;&#22312;&#22238;&#24518;&#19982;&#23545;&#35805;&#20027;&#39064;&#30456;&#20851;&#30340;&#20449;&#24687;&#26102;&#23558;&#20854;&#20316;&#20026;&#20107;&#23454;&#38472;&#36848;&#37325;&#22797;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;ParlAI&#24179;&#21488;&#23454;&#29616;&#30340;BlenderBot 2&#26694;&#26550;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#28431;&#27934;&#65292;&#24182;&#22312;&#26356;&#36817;&#26399;&#12289;&#35268;&#27169;&#26356;&#22823;&#30340;BlenderBot 3&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the new developments in chit-chat bots is a long-term memory mechanism that remembers information from past conversations for increasing engagement and consistency of responses. The bot is designed to extract knowledge of personal nature from their conversation partner, e.g., stating preference for a particular color. In this paper, we show that this memory mechanism can result in unintended behavior. In particular, we found that one can combine a personal statement with an informative statement that would lead the bot to remember the informative statement alongside personal knowledge in its long term memory. This means that the bot can be tricked into remembering misinformation which it would regurgitate as statements of fact when recalling information relevant to the topic of conversation. We demonstrate this vulnerability on the BlenderBot 2 framework implemented on the ParlAI platform and provide examples on the more recent and significantly larger BlenderBot 3 model. We gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.05368</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#20934;&#22791;&#23601;&#32490;&#20102;&#21527;&#65311;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#19987;&#19994;&#24615;&#36136;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#8212;&#8212;GPT-3.5&#12289;GPT-4&#21644;Bard&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#35813;&#35780;&#20272;&#33539;&#22260;&#28085;&#30422;&#20102;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#12289;&#25991;&#26723;&#20998;&#31867;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#8212;&#8212;&#33258;&#38382;&#33258;&#31572;&#25552;&#31034;&#65288;SQP&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#21457;&#19982;&#30456;&#20851;&#20020;&#24202;&#22330;&#26223;&#30456;&#20851;&#30340;&#20449;&#24687;&#24615;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#23450;&#21046;&#21270;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24378;&#35843;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#31574;&#30053;&#21644;&#25552;&#31034;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;ChatGPT&#22312;&#32929;&#31080;&#39044;&#27979;&#26041;&#38754;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#39044;&#27979;&#32929;&#31080;&#31227;&#21160;&#30340;&#34920;&#29616;&#19981;&#22914;&#26368;&#20808;&#36827;&#21644;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.05351</link><description>&lt;p&gt;
ChatGPT&#22312;&#22810;&#27169;&#24577;&#32929;&#31080;&#39044;&#27979;&#25361;&#25112;&#20013;&#30340;&#38646;&#26679;&#26412;&#20998;&#26512;&#65306;&#21326;&#23572;&#34903;&#26032;&#25163;&#65311;
&lt;/p&gt;
&lt;p&gt;
The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges. (arXiv:2304.05351v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;ChatGPT&#22312;&#32929;&#31080;&#39044;&#27979;&#26041;&#38754;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#39044;&#27979;&#32929;&#31080;&#31227;&#21160;&#30340;&#34920;&#29616;&#19981;&#22914;&#26368;&#20808;&#36827;&#21644;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#32929;&#24066;&#36208;&#21183;&#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#26377;&#24453;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#19977;&#20010;&#25512;&#25991;&#21644;&#21382;&#21490;&#32929;&#31080;&#20215;&#26684;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#38646;&#26679;&#26412;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;ChatGPT&#22312;&#22810;&#27169;&#24577;&#32929;&#31080;&#31227;&#21160;&#39044;&#27979;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#26159;&#19968;&#20010;&#8220;&#21326;&#23572;&#34903;&#26032;&#25163;&#8221;&#65292;&#22312;&#39044;&#27979;&#32929;&#31080;&#31227;&#21160;&#26041;&#38754;&#30340;&#25104;&#21151;&#26377;&#38480;&#65292;&#19981;&#20165;&#19981;&#22914;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#19988;&#19981;&#22914;&#20351;&#29992;&#20215;&#26684;&#29305;&#24449;&#30340;&#32447;&#24615;&#22238;&#24402;&#36825;&#26679;&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;&#23613;&#31649;&#24605;&#32500;&#38142;&#25552;&#31034;&#31574;&#30053;&#21644;&#25512;&#25991;&#30340;&#21253;&#21547;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#65292;ChatGPT&#30340;&#34920;&#29616;&#20173;&#28982;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#38656;&#35201;&#26356;&#19987;&#19994;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;ChatGPT&#22312;&#32929;&#31080;&#39044;&#27979;&#26041;&#38754;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) like ChatGPT have demonstrated remarkable performance across a variety of natural language processing tasks. However, their effectiveness in the financial domain, specifically in predicting stock market movements, remains to be explored. In this paper, we conduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal stock movement prediction, on three tweets and historical stock price datasets. Our findings indicate that ChatGPT is a "Wall Street Neophyte" with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features. Despite the potential of Chain-of-Thought prompting strategies and the inclusion of tweets, ChatGPT's performance remains subpar. Furthermore, we observe limitations in its explainability and stability, suggesting the need for more specialized training or fine-tuning. This research provides insights i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#24403;&#183;&#23494;&#33576;&#20975;&#32500;&#22855;&#22823;&#23398; (AMU) &#22312;SlavNER&#31532;&#22235;&#27425;&#20849;&#20139;&#20219;&#21153;&#20013;&#25506;&#31350;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27969;&#34892;&#30340;BERT&#21644;T5&#27169;&#22411;&#26550;&#26500;&#30340;&#27169;&#22411;&#21644;&#22806;&#37096;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#23545;&#26031;&#25289;&#22827;&#35821;&#30340;NER&#21644;&#35789;&#24418;&#36824;&#21407;&#38750;&#24120;&#26377;&#25928;&#65292;&#32467;&#26524;&#26377;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2304.05336</link><description>&lt;p&gt;
&#25506;&#31350;&#22522;&#30784;&#27169;&#22411;&#22312;&#26031;&#25289;&#22827;&#35821;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#35789;&#24418;&#36824;&#21407;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring the Use of Foundation Models for Named Entity Recognition and Lemmatization Tasks in Slavic Languages. (arXiv:2304.05336v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#24403;&#183;&#23494;&#33576;&#20975;&#32500;&#22855;&#22823;&#23398; (AMU) &#22312;SlavNER&#31532;&#22235;&#27425;&#20849;&#20139;&#20219;&#21153;&#20013;&#25506;&#31350;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27969;&#34892;&#30340;BERT&#21644;T5&#27169;&#22411;&#26550;&#26500;&#30340;&#27169;&#22411;&#21644;&#22806;&#37096;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#23545;&#26031;&#25289;&#22827;&#35821;&#30340;NER&#21644;&#35789;&#24418;&#36824;&#21407;&#38750;&#24120;&#26377;&#25928;&#65292;&#32467;&#26524;&#26377;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#24403;&#183;&#23494;&#33576;&#20975;&#32500;&#22855;&#22823;&#23398; (AMU) &#22312;SlavNER&#31532;&#22235;&#27425;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#26031;&#25289;&#22827;&#35821;&#20013;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#21644;&#35789;&#24418;&#36824;&#21407;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#22522;&#30784;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#22522;&#20110;&#27969;&#34892;&#30340;BERT&#21644;T5&#27169;&#22411;&#26550;&#26500;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#22806;&#37096;&#25968;&#25454;&#38598;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#25351;&#26631;&#24471;&#20998;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21450;&#23454;&#39564;&#32467;&#26524;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#23545;&#26031;&#25289;&#22827;&#35821;&#30340;NER&#21644;&#35789;&#24418;&#36824;&#21407;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35789;&#24418;&#36824;&#21407;&#27169;&#22411;&#23558;&#22312; https://huggingface.co/amu-cai &#19978;&#36827;&#34892;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes Adam Mickiewicz University's (AMU) solution for the 4th Shared Task on SlavNER. The task involves the identification, categorization, and lemmatization of named entities in Slavic languages. Our approach involved exploring the use of foundation models for these tasks. In particular, we used models based on the popular BERT and T5 model architectures. Additionally, we used external datasets to further improve the quality of our models. Our solution obtained promising results, achieving high metrics scores in both tasks. We describe our approach and the results of our experiments in detail, showing that the method is effective for NER and lemmatization in Slavic languages. Additionally, our models for lemmatization will be available at: https://huggingface.co/amu-cai.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20013;&#30340;&#27602;&#24615;&#65292;&#36890;&#36807;&#25351;&#23450;&#20154;&#29289;&#35282;&#33394;&#65292;&#36755;&#20986;&#20250;&#28041;&#21450;&#21051;&#26495;&#21360;&#35937;&#12289;&#26377;&#23475;&#23545;&#35805;&#21644;&#20260;&#20154;&#30340;&#35328;&#35770;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20805;&#20998;&#29702;&#35299;LLMs&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05335</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#20013;&#30340;&#27602;&#24615;&#65306;&#20998;&#26512;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Toxicity in ChatGPT: Analyzing Persona-assigned Language Models. (arXiv:2304.05335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05335
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20013;&#30340;&#27602;&#24615;&#65292;&#36890;&#36807;&#25351;&#23450;&#20154;&#29289;&#35282;&#33394;&#65292;&#36755;&#20986;&#20250;&#28041;&#21450;&#21051;&#26495;&#21360;&#35937;&#12289;&#26377;&#23475;&#23545;&#35805;&#21644;&#20260;&#20154;&#30340;&#35328;&#35770;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20805;&#20998;&#29702;&#35299;LLMs&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#12289;&#27835;&#30103;&#12289;&#25945;&#32946;&#21644;&#23458;&#25143;&#26381;&#21153;&#31561;&#22810;&#31181;&#26381;&#21153;&#20013;&#12290;&#30001;&#20110;&#29992;&#25143;&#21253;&#25324;&#26377;&#37325;&#35201;&#20449;&#24687;&#38656;&#27714;&#30340;&#20154;&#65292;&#22914;&#19982;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#23398;&#29983;&#25110;&#24739;&#32773;&#65292;&#22240;&#27492;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#26126;&#30830;&#20102;&#35299;LLMs&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;ChatGPT&#20013;&#30340;&#27602;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;LLM&#65292;&#36229;&#36807;&#21322;&#30334;&#19975;&#27425;Generation&#34987;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#20026;ChatGPT&#25351;&#23450;&#19968;&#20010;&#20154;&#29289;&#35282;&#33394;&#65292;&#27604;&#22914;&#25331;&#20987;&#25163;&#31302;&#32597;&#40664;&#24503;&#183;&#38463;&#37324;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#20135;&#29983;&#30340;&#27602;&#24615;&#12290;&#26681;&#25454;&#25351;&#23450;&#32473;ChatGPT&#30340;&#35282;&#33394;&#65292;&#20854;&#27602;&#24615;&#21487;&#33021;&#20250;&#22686;&#21152;&#21040;6&#20493;&#65292;&#20854;&#36755;&#20986;&#20250;&#28041;&#21450;&#19981;&#27491;&#30830;&#30340;&#21051;&#26495;&#21360;&#35937;&#12289;&#26377;&#23475;&#30340;&#23545;&#35805;&#21644;&#20260;&#20154;&#30340;&#35328;&#35770;&#12290;&#36825;&#21487;&#33021;&#20250;&#28508;&#22312;&#22320;&#25439;&#23475;&#20154;&#29289;&#35282;&#33394;&#30340;&#21517;&#35465;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26234;&#33021;&#20195;&#29702;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#32467;&#21512;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#33258;&#20027;&#35774;&#35745;&#12289;&#35268;&#21010;&#21644;&#25191;&#34892;&#31185;&#23398;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#29702;&#30340;&#31185;&#23398;&#30740;&#31350;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#31034;&#20363;&#65292;&#20854;&#20013;&#26368;&#22797;&#26434;&#30340;&#31034;&#20363;&#26159;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#20652;&#21270;&#30340;&#20132;&#21449;&#20598;&#32852;&#21453;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.05332</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32039;&#24613;&#20986;&#29616;&#30340;&#33258;&#20027;&#31185;&#30740;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Emergent autonomous scientific research capabilities of large language models. (arXiv:2304.05332v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26234;&#33021;&#20195;&#29702;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#32467;&#21512;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#33258;&#20027;&#35774;&#35745;&#12289;&#35268;&#21010;&#21644;&#25191;&#34892;&#31185;&#23398;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#29702;&#30340;&#31185;&#23398;&#30740;&#31350;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#31034;&#20363;&#65292;&#20854;&#20013;&#26368;&#22797;&#26434;&#30340;&#31034;&#20363;&#26159;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#20652;&#21270;&#30340;&#20132;&#21449;&#20598;&#32852;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#36805;&#36895;&#21457;&#23637;&#65292;&#20854;&#24212;&#29992;&#28085;&#30422;&#33258;&#28982;&#35821;&#35328;&#12289;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#21644;&#35745;&#31639;&#26426;&#32534;&#31243;&#31561;&#39046;&#22495;&#12290;&#26497;&#38480;&#25193;&#23637;&#21644;&#24378;&#21270;&#23398;&#20064;&#26377;&#20102;&#20154;&#31867;&#21453;&#39304;&#21518;&#22823;&#22823;&#25552;&#39640;&#20102;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#24182;&#25512;&#29702;&#20854;&#36873;&#25321;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26234;&#33021;&#20195;&#29702;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#32467;&#21512;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#33258;&#20027;&#35774;&#35745;&#12289;&#35268;&#21010;&#21644;&#25191;&#34892;&#31185;&#23398;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#29702;&#30340;&#31185;&#23398;&#30740;&#31350;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#31034;&#20363;&#65292;&#20854;&#20013;&#26368;&#22797;&#26434;&#30340;&#31034;&#20363;&#26159;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#20652;&#21270;&#30340;&#20132;&#21449;&#20598;&#32852;&#21453;&#24212;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36991;&#20813;&#28389;&#29992;&#30340;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;ELVIS&#65292;&#21487;&#20197;&#22686;&#24378;&#25918;&#23556;&#23398;&#25253;&#21578;&#25110; X &#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#23616;&#37096;&#24615;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#29702;&#35299;&#20301;&#32622;&#21442;&#32771;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05303</link><description>&lt;p&gt;
ELVIS: &#21033;&#29992;&#27169;&#24577;&#20869;&#30456;&#20284;&#24615;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#23616;&#37096;&#24615;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ELVIS: Empowering Locality of Vision Language Pre-training with Intra-modal Similarity. (arXiv:2304.05303v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;ELVIS&#65292;&#21487;&#20197;&#22686;&#24378;&#25918;&#23556;&#23398;&#25253;&#21578;&#25110; X &#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#23616;&#37096;&#24615;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#29702;&#35299;&#20301;&#32622;&#21442;&#32771;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#36741;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#38405;&#35835;&#33016;&#37096; X &#23556;&#32447;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#36825;&#38459;&#30861;&#20102;&#20854;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#24120;&#35268;&#29983;&#25104;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#36731;&#27880;&#37322;&#30340;&#36127;&#25285;&#21644;&#25104;&#26412;&#65292;&#36825;&#20123;&#25253;&#21578;&#20197;&#25104;&#23545;&#30340;&#24418;&#24335;&#65288;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65289;&#22823;&#37327;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#27491;&#22312;&#25552;&#20986;&#25193;&#23637;&#21040;&#23450;&#20301;&#24863;&#30693;VLP&#65292;&#20197;&#28385;&#36275;CAD&#22312;CXR&#30340;&#20934;&#30830;&#24322;&#24120;&#23450;&#20301;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#30001;&#23616;&#37096;&#24615;VLP&#25991;&#29486;&#25552;&#20986;&#30340;&#20844;&#24335;&#23454;&#38469;&#19978;&#23548;&#33268;&#20102;&#19979;&#28216;&#23450;&#20301;&#20219;&#21153;&#25152;&#38656;&#30340;&#31354;&#38388;&#20851;&#31995;&#30340;&#20002;&#22833;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Empowering Locality of VLP with Intra-modal Similarity&#65288;ELVIS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;VLP&#65292;&#21487;&#24863;&#30693;&#27169;&#24577;&#20869;&#37096;&#30340;&#23616;&#37096;&#24615;&#33021;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#25918;&#23556;&#23398;&#25253;&#21578;&#25110; X &#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#23616;&#37096;&#24615;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29702;&#35299;&#20301;&#32622;&#21442;&#32771;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has shown great potential in assisting radiologists in reading chest X-ray (CXR) images, but its need for expensive annotations for improving performance prevents widespread clinical application. Visual language pre-training (VLP) can alleviate the burden and cost of annotation by leveraging routinely generated reports for radiographs, which exist in large quantities as well as in paired form (imagetext pairs). Additionally, extensions to localization-aware VLPs are being proposed to address the needs of accurate localization of abnormalities for CAD in CXR. However, we find that the formulation proposed by locality-aware VLP literatures actually leads to loss in spatial relationships required for downstream localization tasks. Therefore, we propose Empowering Locality of VLP with Intra-modal Similarity, ELVIS, a VLP aware of intra-modal locality, to better preserve the locality within radiographs or reports, which enhances the ability to comprehend location references in
&lt;/p&gt;</description></item><item><title>RRHF&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#27010;&#29575;&#19982;&#20154;&#31867;&#20559;&#22909;&#65292;&#23427;&#36890;&#36807;&#25490;&#24207;&#25439;&#22833;&#23545;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#22312;&#35843;&#25972;&#36807;&#31243;&#20013;&#21482;&#38656;1&#21040;2&#20010;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.05302</link><description>&lt;p&gt;
RRHF: &#26080;&#38656;&#28902;&#24700;&#22320;&#20351;&#29992;&#25490;&#21517;&#21709;&#24212;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
RRHF: Rank Responses to Align Language Models with Human Feedback without tears. (arXiv:2304.05302v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05302
&lt;/p&gt;
&lt;p&gt;
RRHF&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#27010;&#29575;&#19982;&#20154;&#31867;&#20559;&#22909;&#65292;&#23427;&#36890;&#36807;&#25490;&#24207;&#25439;&#22833;&#23545;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#22312;&#35843;&#25972;&#36807;&#31243;&#20013;&#21482;&#38656;1&#21040;2&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21487;&#20197;&#24110;&#21161;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20154;&#31867;&#19982;&#36825;&#20123;&#27169;&#22411;&#38388;&#30340;&#20132;&#20114;&#36136;&#37327;&#12290;&#19982;PPO&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;RRHF&#65292;&#23427;&#36890;&#36807;&#25490;&#24207;&#25439;&#22833;&#23545;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#23398;&#20064;&#23558;&#23427;&#20204;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;RRHF&#21487;&#20197;&#39640;&#25928;&#22320;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#27010;&#29575;&#19982;&#20154;&#31867;&#20559;&#22909;&#65292;&#20854;&#25928;&#26524;&#21644;Fine-Tuning&#19968;&#26679;&#31283;&#20581;&#65292;&#32780;&#22312;&#35843;&#25972;&#36807;&#31243;&#20013;&#21482;&#38656;1&#21040;2&#20010;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;RRHF&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;SFT&#21644;&#22870;&#21169;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#19982;PPO&#30456;&#27604;&#22312;&#32534;&#30721;&#21644;&#27169;&#22411;&#25968;&#37327;&#26041;&#38754;&#26356;&#20026;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and these models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). PPO, however, is sensitive to hyperparameters and requires a minimum of four models in its standard implementation, which makes it hard to train. In contrast, we propose a novel learning paradigm called RRHF, which scores responses generated by different sampling policies and learns to align them with human preferences through ranking loss. RRHF can efficiently align language model output probabilities with human preferences as robust as fine-tuning and it only needs 1 to 2 models during tuning. In addition, RRHF can be considered an extension of SFT and reward models while being simpler than PPO in terms of coding, model count
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23454;&#20307;&#38142;&#25509;&#26469;&#23454;&#29616;&#26415;&#35821;&#35268;&#33539;&#21270;&#30340;&#22768;&#26126;&#25552;&#21462;&#31649;&#36947;&#65292;&#25104;&#21151;&#25552;&#21462;&#20102;&#21307;&#23398;&#25512;&#25991;&#20013;&#30340;&#22522;&#20110;&#23454;&#20307;&#30340;&#22768;&#26126;&#65292;&#29305;&#21035;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#19978;&#30340;&#34920;&#29616;&#20196;&#20154;&#28385;&#24847;&#12290;</title><link>http://arxiv.org/abs/2304.05268</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#20307;&#30340;&#22768;&#26126;&#25552;&#21462;&#31649;&#36947;&#65306;&#29992;&#20110;&#29616;&#23454;&#29983;&#29289;&#21307;&#23398;&#20107;&#23454;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Entity-based Claim Extraction Pipeline for Real-world Biomedical Fact-checking. (arXiv:2304.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23454;&#20307;&#38142;&#25509;&#26469;&#23454;&#29616;&#26415;&#35821;&#35268;&#33539;&#21270;&#30340;&#22768;&#26126;&#25552;&#21462;&#31649;&#36947;&#65292;&#25104;&#21151;&#25552;&#21462;&#20102;&#21307;&#23398;&#25512;&#25991;&#20013;&#30340;&#22522;&#20110;&#23454;&#20307;&#30340;&#22768;&#26126;&#65292;&#29305;&#21035;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#19978;&#30340;&#34920;&#29616;&#20196;&#20154;&#28385;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#30340;&#29983;&#29289;&#21307;&#23398;&#22768;&#26126;&#20107;&#23454;&#26816;&#26597;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#21512;&#25104;&#25110;&#31934;&#32454;&#25514;&#36766;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#24182;&#19988;&#24456;&#38590;&#36716;&#31227;&#21040;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;Wuehrl&#65286;Klinger&#65288;2022&#65289;&#25552;&#20986;&#20102;&#22522;&#20110;&#25991;&#26412;&#20013;&#30340;&#21307;&#23398;&#23454;&#20307;&#25552;&#21462;&#31616;&#27905;&#30340;&#22768;&#26126;&#26469;&#27169;&#20223;&#24120;&#35265;&#35757;&#32451;&#22768;&#26126;&#30340;&#19987;&#27880;&#24615;&#30340;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#30740;&#31350;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;&#39318;&#20808;&#65292;&#23427;&#20381;&#36182;&#20110;&#40644;&#37329;&#27880;&#37322;&#23454;&#20307;&#12290;&#22240;&#27492;&#65292;&#26080;&#27861;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#33258;&#21160;&#26816;&#27979;&#30456;&#20851;&#23454;&#20307;&#12290;&#31532;&#20108;&#65292;&#20182;&#20204;&#29992;&#21407;&#22987;&#26631;&#35760;&#34920;&#31034;&#22768;&#26126;&#23454;&#20307;&#12290;&#36825;&#26500;&#25104;&#20102;&#26415;&#35821;&#19981;&#21305;&#37197;&#65292;&#21487;&#33021;&#38480;&#21046;&#20102;&#20107;&#23454;&#26816;&#26597;&#24615;&#33021;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#25512;&#25991;&#30340;&#22768;&#26126;&#25552;&#21462;&#31649;&#36947;&#65292;&#35813;&#31649;&#36947;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#36890;&#36807;&#23454;&#20307;&#38142;&#25509;&#30340;&#26415;&#35821;&#35268;&#33539;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#33258;&#21160;NER&#30830;&#23454;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23454;&#20307;&#38142;&#25509;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#31995;&#32479;&#20934;&#30830;&#25552;&#21462;&#22768;&#26126;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31649;&#36947;&#25104;&#21151;&#22320;&#20174;&#21307;&#23398;&#25512;&#25991;&#20013;&#25552;&#21462;&#20102;&#31616;&#26126;&#30340;&#22522;&#20110;&#23454;&#20307;&#30340;&#22768;&#26126;&#65292;&#22312;&#25163;&#21160;&#27880;&#37322;&#30340;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#20102;0.66&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing fact-checking models for biomedical claims are typically trained on synthetic or well-worded data and hardly transfer to social media content. This mismatch can be mitigated by adapting the social media input to mimic the focused nature of common training claims. To do so, Wuehrl &amp; Klinger (2022) propose to extract concise claims based on medical entities in the text. However, their study has two limitations: First, it relies on gold-annotated entities. Therefore, its feasibility for a real-world application cannot be assessed since this requires detecting relevant entities automatically. Second, they represent claim entities with the original tokens. This constitutes a terminology mismatch which potentially limits the fact-checking performance. To understand both challenges, we propose a claim extraction pipeline for medical tweets that incorporates named entity recognition and terminology normalization via entity linking. We show that automatic NER does lead to a performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.05265</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;
&lt;/p&gt;
&lt;p&gt;
Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COTI&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;&#29702;&#35770;&#25351;&#23548;&#30340;&#25439;&#22833;&#30446;&#26631;&#21644;&#20840;&#38754;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#25991;&#26412;&#21453;&#36716;&#26102;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#22312;&#20197;&#25991;&#26412;&#20026;&#24341;&#23548;&#30340;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#24403;&#24341;&#23548;&#20449;&#24687;&#21253;&#21547;&#29992;&#25143;&#23450;&#20041;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#25110;&#38271;&#23614;&#27010;&#24565;&#26631;&#35760;&#26102;&#65292;&#25991;&#26412;&#21453;&#36716;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#29983;&#25104;&#25216;&#26415;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#23637;&#31034;&#20102;&#25991;&#26412;&#21453;&#36716;&#30340;&#37096;&#32626;&#20173;&#20805;&#28385;&#20102;&#8220;&#40657;&#39764;&#27861;&#8221;&#65292;&#20363;&#22914;&#39069;&#22806;&#25968;&#25454;&#38598;&#30340;&#20005;&#33499;&#35201;&#27714;&#65292;&#22312;&#24490;&#29615;&#20013;&#38656;&#35201;&#33392;&#33510;&#30340;&#20154;&#21147;&#25104;&#26412;&#21644;&#32570;&#20047;&#40065;&#26834;&#24615;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#25511;&#25991;&#26412;&#21453;&#36716;&#30340;&#22823;&#22823;&#22686;&#24378;&#29256;&#21453;&#36716;&#65292;&#35299;&#20915;&#20102;&#25152;&#26377;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#21453;&#36807;&#26469;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#65292;&#25968;&#25454;&#25928;&#29575;&#39640;&#65292;&#26131;&#20110;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;COTI&#30340;&#26680;&#24515;&#26159;&#22522;&#20110;&#29702;&#35770;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#20855;&#26377;&#20840;&#38754;&#21644;&#26032;&#39062;&#30340;&#21152;&#26435;&#35780;&#20998;&#26426;&#21046;&#65292;&#24182;&#30001;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#25152;&#25552;&#21462;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;COTI&#30340;&#24615;&#33021;&#27604;&#20043;&#21069;&#25216;&#26415;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25552;&#31034;&#26469;&#35780;&#20272;&#31038;&#20132;Chatbot&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#20154;&#31867;&#23545;Chatbot&#30340;&#20027;&#35266;&#35780;&#20272;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#20934;&#22791;&#35780;&#20272;&#26448;&#26009;&#12290;</title><link>http://arxiv.org/abs/2304.05253</link><description>&lt;p&gt;
&#21033;&#29992;&#25552;&#31034;&#26469;&#36817;&#20284;&#20154;&#31867;&#23545;&#31038;&#20132;Chatbot&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Approximating Human Evaluation of Social Chatbots with Prompting. (arXiv:2304.05253v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05253
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25552;&#31034;&#26469;&#35780;&#20272;&#31038;&#20132;Chatbot&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#20154;&#31867;&#23545;Chatbot&#30340;&#20027;&#35266;&#35780;&#20272;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#20934;&#22791;&#35780;&#20272;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#22823;&#30340;&#23545;&#35805;&#27169;&#22411;&#36880;&#28176;&#38754;&#21521;&#24191;&#22823;&#29992;&#25143;&#24320;&#25918;&#65292;&#29992;&#25143;&#24320;&#22987;&#31215;&#26497;&#22320;&#19982;&#36825;&#31181;&#25216;&#26415;&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#12290;&#38500;&#38750;&#25216;&#26415;&#24471;&#21040;&#36866;&#24403;&#30340;&#25511;&#21046;&#65292;&#36825;&#31181;&#21069;&#25152;&#26410;&#26377;&#30340;&#20132;&#20114;&#20307;&#39564;&#21487;&#33021;&#20250;&#23545;&#29992;&#25143;&#36896;&#25104;&#30456;&#24403;&#22823;&#30340;&#31038;&#20132;&#21644;&#24515;&#29702;&#39118;&#38505;&#12290;&#36825;&#23601;&#38656;&#35201;&#21487;&#25193;&#23637;&#21644;&#24378;&#22823;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#31038;&#20132;Chatbot&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#20851;&#27880;&#23458;&#35266;&#36136;&#37327;&#25351;&#26631;&#65292;&#24573;&#30053;&#31038;&#20132;&#32500;&#24230;&#30340;&#20027;&#35266;&#24863;&#21463;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#37117;&#22522;&#20110;&#21487;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#39044;&#29983;&#25104;&#30340;&#23545;&#35805;&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#20154;&#31867;&#21442;&#19982;&#20934;&#22791;&#35780;&#20272;&#26448;&#26009;&#65292;&#22240;&#27492;&#24433;&#21709;&#20102;&#25351;&#26631;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26469;&#33258;GPT&#31995;&#21015;&#30340;&#26032;&#20852;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#36827;&#34892;&#25552;&#31034;&#24335;&#30340;&#23545;&#35805;&#31995;&#32479;&#35780;&#20272;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20851;&#27880;&#20027;&#35266;&#35780;&#20215;&#26631;&#20934;&#26469;&#36817;&#20284;&#20154;&#31867;&#23545;&#31038;&#20132;Chatbot&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;GPT-3&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21508;&#26679;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#36755;&#20837;&#26469;&#20934;&#22791;&#35780;&#20272;&#26448;&#26009;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22235;&#31181;&#19981;&#21516;&#30340;&#23545;&#35805;&#27169;&#22411;&#36827;&#34892;&#19968;&#31995;&#21015;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#26694;&#26550;&#30340;&#20248;&#32570;&#28857;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Once powerful conversational models have become available for a wide audience, users started actively engaging in social interactions with this technology. Such unprecedented interaction experiences may pose considerable social and psychological risks to the users unless the technology is properly controlled. This creates an urgent need for scalable and robust evaluation metrics for conversational chatbots. Existing automatic evaluation metrics usually focus on objective quality measures and disregard subjective perceptions of social dimensions. Moreover, most of these approaches operate on pre-produced dialogs from available benchmark corpora, which implies human involvement for preparing the material for evaluation and, thus, impeded scalability of the metrics. To address this limitation, we propose to make use of the emerging large language models (LLMs) from the GPT-family and describe a new framework allowing to conduct dialog system evaluation with prompting. With this framework,
&lt;/p&gt;</description></item><item><title>&#24378;&#21046;&#26080;&#25928;&#21270;&#25216;&#26415;&#21487;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#38169;&#35823;&#30340;&#21333;&#35789;&#24207;&#21015;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#23545;&#21333;&#35789;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05221</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21046;&#26080;&#25928;&#21270;&#23454;&#29616;&#20445;&#25252;&#21333;&#35789;&#39034;&#24207;&#37325;&#35201;&#24615;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Towards preserving word order importance through Forced Invalidation. (arXiv:2304.05221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05221
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21046;&#26080;&#25928;&#21270;&#25216;&#26415;&#21487;&#24110;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#38169;&#35823;&#30340;&#21333;&#35789;&#24207;&#21015;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#23545;&#21333;&#35789;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20363;&#22914;BERT&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#21333;&#35789;&#39034;&#24207;&#19981;&#25935;&#24863;&#12290;&#21363;&#20351;&#23545;&#19968;&#20010;&#21477;&#23376;&#20013;&#21333;&#35789;&#36827;&#34892;&#38543;&#26426;&#25490;&#21015;&#65292;&#35821;&#27861;&#19978;&#30340;&#20851;&#38190;&#20449;&#24687;&#20063;&#34987;&#30772;&#22351;&#65292;&#20294;NLU&#20219;&#21153;&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#21464;&#12290;&#20026;&#20102;&#20445;&#25252;&#21333;&#35789;&#39034;&#24207;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65306;&#24378;&#21046;&#26080;&#25928;&#21270;&#65288;Forced Invalidation&#65292;FI&#65289;&#65306;&#24378;&#21046;&#27169;&#22411;&#23558;&#25490;&#21015;&#38169;&#35823;&#30340;&#24207;&#21015;&#35782;&#21035;&#20026;&#26080;&#25928;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;BERT&#21644;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#33521;&#35821;NLU&#21644;QA&#20219;&#21153;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;Force Invalidation&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#23545;&#21333;&#35789;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models such as BERT have been widely used as a framework for natural language understanding (NLU) tasks. However, recent findings have revealed that pre-trained language models are insensitive to word order. The performance on NLU tasks remains unchanged even after randomly permuting the word of a sentence, where crucial syntactic information is destroyed. To help preserve the importance of word order, we propose a simple approach called Forced Invalidation (FI): forcing the model to identify permuted sequences as invalid samples. We perform an extensive evaluation of our approach on various English NLU and QA based tasks over BERT-based and attention-based models over word embeddings. Our experiments demonstrate that Forced Invalidation significantly improves the sensitivity of the models to word order.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#25506;&#32034;&#20102;&#21508;&#23618;&#39044;&#35757;&#32451;&#34920;&#31034;&#21644;&#32534;&#30721;&#30340;&#20195;&#30721;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#26696;&#12290;&#23454;&#39564;&#21457;&#29616;&#24494;&#35843;&#36807;&#31243;&#21487;&#20197;&#20445;&#30041;&#22823;&#37096;&#20998;&#20195;&#30721;&#23646;&#24615;&#65292;&#22522;&#26412;&#20195;&#30721;&#23646;&#24615;&#30001;&#36739;&#20302;&#21644;&#20013;&#38388;&#23618;&#25429;&#33719;&#12290;</title><link>http://arxiv.org/abs/2304.05216</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#26377;&#25928;&#24494;&#35843;&#65306;&#23454;&#39564;&#30740;&#31350;&#21450;&#20854;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Fine-tuning of Pre-trained Code Models: An Experimental Study and Beyond. (arXiv:2304.05216v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#25506;&#32034;&#20102;&#21508;&#23618;&#39044;&#35757;&#32451;&#34920;&#31034;&#21644;&#32534;&#30721;&#30340;&#20195;&#30721;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#26696;&#12290;&#23454;&#39564;&#21457;&#29616;&#24494;&#35843;&#36807;&#31243;&#21487;&#20197;&#20445;&#30041;&#22823;&#37096;&#20998;&#20195;&#30721;&#23646;&#24615;&#65292;&#22522;&#26412;&#20195;&#30721;&#23646;&#24615;&#30001;&#36739;&#20302;&#21644;&#20013;&#38388;&#23618;&#25429;&#33719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#35768;&#22810;&#36719;&#20214;&#27979;&#35797;&#21644;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23545;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65288;&#22914;CodeBERT&#65289;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#34429;&#28982;&#26377;&#25928;&#19988;&#27969;&#34892;&#65292;&#20294;&#24494;&#35843;&#39044;&#35757;&#32451;&#21442;&#25968;&#20250;&#23548;&#33268;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#25506;&#32034;&#24494;&#35843;&#26399;&#38388;&#27599;&#23618;&#39044;&#35757;&#32451;&#34920;&#31034;&#21644;&#32534;&#30721;&#30340;&#20195;&#30721;&#30693;&#35782;&#21457;&#29983;&#20102;&#20160;&#20040;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#19978;&#36848;&#21457;&#29616;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65306;&#65288;1&#65289;&#28304;&#20195;&#30721;&#30340;&#35789;&#27719;&#12289;&#35821;&#27861;&#21644;&#32467;&#26500;&#24615;&#36136;&#20998;&#21035;&#32534;&#30721;&#22312;&#36739;&#20302;&#12289;&#20013;&#38388;&#21644;&#36739;&#39640;&#30340;&#23618;&#20013;&#65292;&#32780;&#35821;&#20041;&#23646;&#24615;&#36328;&#36234;&#25972;&#20010;&#27169;&#22411;&#12290;&#65288;2&#65289;&#24494;&#35843;&#36807;&#31243;&#20445;&#30041;&#20102;&#22823;&#37096;&#20998;&#20195;&#30721;&#23646;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36739;&#20302;&#21644;&#20013;&#38388;&#23618;&#25429;&#33719;&#30340;&#22522;&#26412;&#20195;&#30721;&#23646;&#24615;&#22312;&#24494;&#35843;&#26399;&#38388;&#20173;&#28982;&#20445;&#30041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Recently, fine-tuning pre-trained code models such as CodeBERT on downstream tasks has achieved great success in many software testing and analysis tasks. While effective and prevalent, fine-tuning the pre-trained parameters incurs a large computational cost. In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings. Our experimental study shows that (1) lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model. (2) The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#20851;&#32852;&#21644;&#29983;&#25104;&#27169;&#22411;&#28151;&#21512;&#26041;&#27861;&#30340;&#36234;&#21335;&#22810;&#25991;&#26723;&#25688;&#35201;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;VLSP 2022&#31454;&#36187;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.05205</link><description>&lt;p&gt;
VLSP2022-Abmusu&#22242;&#38431;&#65306;&#22522;&#20110;&#25991;&#26412;&#20851;&#32852;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#36234;&#21335;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LBMT team at VLSP2022-Abmusu: Hybrid method with text correlation and generative models for Vietnamese multi-document summarization. (arXiv:2304.05205v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#20851;&#32852;&#21644;&#29983;&#25104;&#27169;&#22411;&#28151;&#21512;&#26041;&#27861;&#30340;&#36234;&#21335;&#22810;&#25991;&#26723;&#25688;&#35201;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;VLSP 2022&#31454;&#36187;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25991;&#26723;&#25688;&#35201;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25688;&#35201;&#19981;&#20165;&#24212;&#25551;&#36848;&#25152;&#26377;&#25991;&#26723;&#20013;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#65292;&#36824;&#24212;&#25552;&#20379;&#25991;&#26723;&#30340;&#36830;&#36143;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30456;&#20284;&#24615;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#26041;&#27861;&#12290;&#22312;&#25552;&#21462;&#24335;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#20462;&#25913;&#21518;&#30340;PageRank&#31639;&#27861;&#21644;&#25991;&#26412;&#20851;&#32852;&#32771;&#34385;&#26426;&#21046;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#36890;&#36807;&#20174;&#27599;&#20010;&#31751;&#20013;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#21477;&#23376;&#29983;&#25104;&#25688;&#35201;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;BARTpho&#21644;ViT5&#26469;&#26500;&#24314;&#25277;&#35937;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#25552;&#21462;&#24335;&#21644;&#25277;&#35937;&#24335;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;VLSP 2022&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-document summarization is challenging because the summaries should not only describe the most important information from all documents but also provide a coherent interpretation of the documents. This paper proposes a method for multi-document summarization based on cluster similarity. In the extractive method we use hybrid model based on a modified version of the PageRank algorithm and a text correlation considerations mechanism. After generating summaries by selecting the most important sentences from each cluster, we apply BARTpho and ViT5 to construct the abstractive models. Both extractive and abstractive approaches were considered in this study. The proposed method achieves competitive results in VLSP 2022 competition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26469;&#33258;OpenAI&#27169;&#22411;API&#20197;&#21450;New Bing&#22686;&#24378;&#29256;&#30340;ChatGPT&#23545;&#38544;&#31169;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#25351;&#20986;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#30340;LLMs&#21487;&#33021;&#23548;&#33268;&#27604;&#20197;&#24448;&#26356;&#20005;&#37325;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#23454;&#39564;&#25903;&#25345;&#35813;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.05197</link><description>&lt;p&gt;
Multi-step Jailbreaking Privacy Attacks on ChatGPT
&lt;/p&gt;
&lt;p&gt;
Multi-step Jailbreaking Privacy Attacks on ChatGPT. (arXiv:2304.05197v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26469;&#33258;OpenAI&#27169;&#22411;API&#20197;&#21450;New Bing&#22686;&#24378;&#29256;&#30340;ChatGPT&#23545;&#38544;&#31169;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#25351;&#20986;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#30340;LLMs&#21487;&#33021;&#23548;&#33268;&#27604;&#20197;&#24448;&#26356;&#20005;&#37325;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#23454;&#39564;&#25903;&#25345;&#35813;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#33391;&#22909;&#30340;&#25552;&#31034;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#23613;&#31649;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#30830;&#20445;&#36991;&#20813;&#20174;LLMs&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#24341;&#23548;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#20026;&#20154;&#31867;&#24102;&#26469;&#22909;&#22788;&#12290;&#30001;&#20110;&#24378;&#22823;&#30340;LLMs&#27491;&#22312;&#21534;&#22124;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;&#29616;&#26377;&#25991;&#26412;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;GPT-3&#35757;&#32451;&#20102;45TB&#30340;&#25991;&#26412;&#65289;&#65292;&#22240;&#27492;&#20154;&#20204;&#33258;&#28982;&#20250;&#24576;&#30097;&#35757;&#32451;&#25968;&#25454;&#20013;&#26159;&#21542;&#21253;&#21547;&#31169;&#20154;&#20449;&#24687;&#20197;&#21450;&#36825;&#20123;LLMs&#21450;&#20854;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#24102;&#26469;&#20160;&#20040;&#38544;&#31169;&#23041;&#32961;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26469;&#33258;OpenAI&#30340;&#27169;&#22411;API&#21644;&#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;New Bing&#25152;&#24102;&#26469;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#26174;&#31034;&#24212;&#29992;&#31243;&#24207;&#38598;&#25104;&#30340;LLMs&#21487;&#33021;&#23548;&#33268;&#27604;&#20197;&#24448;&#26356;&#20005;&#37325;&#30340;&#38544;&#31169;&#23041;&#32961;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#35828;&#27861;&#65292;&#24182;&#35752;&#35770;LLMs&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given good prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI's model APIs and New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause more severe privacy threats ever than before. To this end, we conduct extensive experiments to support our claims and discuss LLMs' privacy implications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35843;&#35797;&#20854;&#39044;&#27979;&#30340;&#31243;&#24207;&#65292;&#22312;&#22810;&#39033;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05128</link><description>&lt;p&gt;
&#33258;&#25105;&#35843;&#35797;&#65306;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35843;&#35797;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Teaching Large Language Models to Self-Debug. (arXiv:2304.05128v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35843;&#35797;&#20854;&#39044;&#27979;&#30340;&#31243;&#24207;&#65292;&#22312;&#22810;&#39033;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#32534;&#31243;&#20219;&#21153;&#65292;&#22312;&#19968;&#27425;&#24615;&#29983;&#25104;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#26469;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#35843;&#35797;(Self-Debugging)&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#28436;&#31034;&#26469;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#20854;&#39044;&#27979;&#30340;&#31243;&#24207;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#35843;&#35797;&#21487;&#20197;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27233;&#30382;&#40493;&#23376;&#35843;&#35797;(Rubber Duck Debugging)&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#20195;&#30721;&#27491;&#30830;&#24615;&#25110;&#38169;&#35823;&#20449;&#24687;&#30340;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#29983;&#25104;&#30340;&#20195;&#30721;&#26469;&#35782;&#21035;&#23427;&#30340;&#38169;&#35823;&#12290;&#33258;&#25105;&#35843;&#35797;&#22312;&#22810;&#39033;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#30340;Spider&#25968;&#25454;&#38598;&#65292;C++&#21040;Python&#32763;&#35793;&#30340;TransCoder&#21644;&#25991;&#26412;&#21040;Python&#29983;&#25104;&#30340;MBPP&#12290;&#22312;&#27809;&#26377;&#21333;&#20803;&#27979;&#35797;&#30340;Spider&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#33258;&#25105;&#35843;&#35797;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit te
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26102;&#23578;&#31526;&#21495;&#21644;&#29305;&#24449;&#25552;&#31034;&#30340;&#32454;&#31890;&#24230;&#26102;&#23578;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#26102;&#23578;&#23646;&#24615;&#21644;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#39640;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.05051</link><description>&lt;p&gt;
FashionSAP: &#22522;&#20110;&#31526;&#21495;&#21644;&#29305;&#24449;&#25552;&#31034;&#30340;&#32454;&#31890;&#24230;&#26102;&#23578;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FashionSAP: Symbols and Attributes Prompt for Fine-grained Fashion Vision-Language Pre-training. (arXiv:2304.05051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05051
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26102;&#23578;&#31526;&#21495;&#21644;&#29305;&#24449;&#25552;&#31034;&#30340;&#32454;&#31890;&#24230;&#26102;&#23578;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#26102;&#23578;&#23646;&#24615;&#21644;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#39640;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#23578;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#24448;&#24448;&#24573;&#30053;&#20102;&#32454;&#31890;&#24230;&#39046;&#22495;&#29305;&#24449;&#65292;&#32780;&#36825;&#20123;&#29305;&#24449;&#22312;&#21306;&#20998;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#24120;&#35268;&#20219;&#21153;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#24314;&#27169;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#26102;&#23578;&#23646;&#24615;&#21644;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#23578;&#31526;&#21495;&#21644;&#29305;&#24449;&#25552;&#31034;&#65288;FashionSAP&#65289;&#30340;&#32454;&#31890;&#24230;&#26102;&#23578;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#23578;&#31526;&#21495;&#65292;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26102;&#23578;&#27010;&#24565;&#23618;&#65292;&#29992;&#20110;&#34920;&#31034;&#19981;&#21516;&#30340;&#26102;&#23578;&#29289;&#21697;&#65292;&#24182;&#27010;&#25324;&#21508;&#31181;&#32454;&#31890;&#24230;&#26102;&#23578;&#29305;&#24449;&#65292;&#20174;&#32780;&#20351;&#24314;&#27169;&#32454;&#31890;&#24230;&#23646;&#24615;&#26356;&#21152;&#26377;&#25928;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#23646;&#24615;&#25552;&#31034;&#26041;&#27861;&#65292;&#26126;&#30830;&#35753;&#27169;&#22411;&#23398;&#20064;&#26102;&#23578;&#29289;&#21697;&#30340;&#20855;&#20307;&#23646;&#24615;&#12290;&#25105;&#20204;&#26681;&#25454;&#26102;&#23578;&#25968;&#25454;&#30340;&#26684;&#24335;&#35774;&#35745;&#36866;&#24403;&#30340;&#25552;&#31034;&#27169;&#26495;&#12290;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fashion vision-language pre-training models have shown efficacy for a wide range of downstream tasks. However, general vision-language pre-training models pay less attention to fine-grained domain features, while these features are important in distinguishing the specific domain tasks from general tasks. We propose a method for fine-grained fashion vision-language pre-training based on fashion Symbols and Attributes Prompt (FashionSAP) to model fine-grained multi-modalities fashion attributes and characteristics. Firstly, we propose the fashion symbols, a novel abstract fashion concept layer, to represent different fashion items and to generalize various kinds of fine-grained fashion features, making modelling fine-grained attributes more effective. Secondly, the attributes prompt method is proposed to make the model learn specific attributes of fashion items explicitly. We design proper prompt templates according to the format of fashion data. Comprehensive experiments are conducted o
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#30340;&#22825;&#27668;&#26465;&#20214;&#19979;&#20154;&#20204;&#22312;&#25512;&#29305;&#19978;&#35848;&#35770;&#30340;&#39135;&#29289;&#23384;&#22312;&#24046;&#24322;&#65292;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#21487;&#20419;&#36827;&#23545;&#39135;&#21697;&#28040;&#36153;&#32773;&#36873;&#25321;&#21644;&#30475;&#27861;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.05041</link><description>&lt;p&gt;
&#22312;&#38632;&#22825;&#25105;&#20204;&#20250;&#25512;&#29305;&#21738;&#20123;&#39135;&#29289;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Food Do We Tweet about on a Rainy Day?. (arXiv:2304.05041v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05041
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#30340;&#22825;&#27668;&#26465;&#20214;&#19979;&#20154;&#20204;&#22312;&#25512;&#29305;&#19978;&#35848;&#35770;&#30340;&#39135;&#29289;&#23384;&#22312;&#24046;&#24322;&#65292;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#21487;&#20419;&#36827;&#23545;&#39135;&#21697;&#28040;&#36153;&#32773;&#36873;&#25321;&#21644;&#30475;&#27861;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#21697;&#36873;&#25321;&#26159;&#19968;&#20010;&#30001;&#21697;&#21619;&#12289;&#27675;&#22260;&#12289;&#25991;&#21270;&#25110;&#22825;&#27668;&#31561;&#22240;&#32032;&#22609;&#36896;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19981;&#21516;&#22825;&#27668;&#26465;&#20214;&#19979;&#19982;&#39135;&#21697;&#30456;&#20851;&#30340;&#25512;&#29305;&#25968;&#25454;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#35206;&#30422;&#36807;&#21435;&#21313;&#24180;&#30340;&#25289;&#33073;&#32500;&#20122;&#39135;&#21697;&#25512;&#29305;&#25968;&#25454;&#38598;&#21644;&#21253;&#21547;&#24179;&#22343;&#28201;&#24230;&#12289;&#38477;&#27700;&#31561;&#29616;&#35937;&#30340;&#22825;&#27668;&#35266;&#23519;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#21738;&#20123;&#22825;&#27668;&#26465;&#20214;&#23548;&#33268;&#20102;&#29305;&#23450;&#30340;&#39135;&#21697;&#20449;&#24687;&#20849;&#20139;&#65307;&#33258;&#21160;&#20998;&#31867;&#25512;&#29305;&#24773;&#24863;&#24182;&#35752;&#35770;&#22914;&#20309;&#26681;&#25454;&#22825;&#27668;&#24773;&#20917;&#21464;&#21270;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#22823;&#35268;&#27169;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#30340;&#29702;&#35299;&#65292;&#20102;&#35299;&#39135;&#21697;&#28040;&#36153;&#32773;&#30340;&#36873;&#25321;&#21644;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food choice is a complex phenomenon shaped by factors such as taste, ambience, culture or weather. In this paper, we explore food-related tweeting in different weather conditions. We inspect a Latvian food tweet dataset spanning the past decade in conjunction with a weather observation dataset consisting of average temperature, precipitation, and other phenomena. We find which weather conditions lead to specific food information sharing; automatically classify tweet sentiment and discuss how it changes depending on the weather. This research contributes to the growing area of large-scale social network data understanding of food consumers' choices and perceptions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21512;&#20316;&#30340;&#26041;&#27861;&#65292;&#23558;&#26377;&#38480;&#25968;&#25454;&#30340;&#20154;&#31867;&#35789;&#27719;&#35821;&#20041;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2304.05012</link><description>&lt;p&gt;
&#20154;&#26426;&#21512;&#20316;&#29983;&#25104;&#35821;&#20041;&#29305;&#24449;&#21015;&#34920;
&lt;/p&gt;
&lt;p&gt;
Human-machine cooperation for semantic feature listing. (arXiv:2304.05012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21512;&#20316;&#30340;&#26041;&#27861;&#65292;&#23558;&#26377;&#38480;&#25968;&#25454;&#30340;&#20154;&#31867;&#35789;&#27719;&#35821;&#20041;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#29305;&#24449;&#35268;&#33539;&#26159;&#25551;&#36848;&#20154;&#31867;&#27010;&#24565;&#30693;&#35782;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#33258;&#21160;&#29983;&#25104;&#27492;&#31867;&#21151;&#33021;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#65292;&#20294;&#23481;&#26131;&#20986;&#29616;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#26469;&#33258;&#26377;&#38480;&#25968;&#25454;&#30340;&#23398;&#20064;&#22411;&#20154;&#31867;&#35789;&#27719;&#35821;&#20041;&#27169;&#22411;&#19982;LLM&#29983;&#25104;&#30340;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#20197;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic feature norms, lists of features that concepts do and do not possess, have played a central role in characterizing human conceptual knowledge, but require extensive human labor. Large language models (LLMs) offer a novel avenue for the automatic generation of such feature lists, but are prone to significant error. Here, we present a new method for combining a learned model of human lexical-semantics from limited data with LLM-generated data to efficiently generate high-quality feature norms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sim-T&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#22797;&#29992;&#25216;&#26415;&#26377;&#25928;&#21387;&#32553;&#27169;&#22411;&#65292;&#20445;&#25345;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#35299;&#20915;&#22312;&#35745;&#31639;&#36164;&#28304;&#25110;&#23384;&#20648;&#20869;&#23384;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.04991</link><description>&lt;p&gt;
&#20351;&#29992;&#22797;&#29992;&#25216;&#26415;&#31616;&#21270;Transformer&#32593;&#32476;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;Sim-T
&lt;/p&gt;
&lt;p&gt;
Sim-T: Simplify the Transformer Network by Multiplexing Technique for Speech Recognition. (arXiv:2304.04991v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sim-T&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#22797;&#29992;&#25216;&#26415;&#26377;&#25928;&#21387;&#32553;&#27169;&#22411;&#65292;&#20445;&#25345;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#35299;&#20915;&#22312;&#35745;&#31639;&#36164;&#28304;&#25110;&#23384;&#20648;&#20869;&#23384;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;Transformer&#32593;&#32476;&#22312;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;Transformer&#32593;&#32476;&#22987;&#32456;&#28041;&#21450;&#22823;&#37327;&#35745;&#31639;&#21644;&#21442;&#25968;&#65292;&#23548;&#33268;&#22312;&#35745;&#31639;&#36164;&#28304;&#25110;&#23384;&#20648;&#20869;&#23384;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#26102;&#23384;&#22312;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Sim-T&#30340;&#26032;&#22411;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#24320;&#21457;&#30340;&#22797;&#29992;&#25216;&#26415;&#21487;&#20197;&#26377;&#25928;&#22320;&#21387;&#32553;&#27169;&#22411;&#65292;&#32780;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#29306;&#29298;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#20855;&#20307;&#22320;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#21253;&#25324;&#27169;&#22359;&#26435;&#37325;&#22797;&#29992;&#21644;&#27880;&#24847;&#21147;&#24471;&#20998;&#22797;&#29992;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#22120;&#32467;&#26500;&#26469;&#20419;&#36827;&#27880;&#24847;&#21147;&#24471;&#20998;&#22797;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Sim-T&#27169;&#22411;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a great deal of attention has been paid to the Transformer network for speech recognition tasks due to its excellent model performance. However, the Transformer network always involves heavy computation and large number of parameters, causing serious deployment problems in devices with limited computation sources or storage memory. In this paper, a new lightweight model called Sim-T has been proposed to expand the generality of the Transformer model. Under the help of the newly developed multiplexing technique, the Sim-T can efficiently compress the model with negligible sacrifice on its performance. To be more precise, the proposed technique includes two parts, that are, module weight multiplexing and attention score multiplexing. Moreover, a novel decoder structure has been proposed to facilitate the attention score multiplexing. Extensive experiments have been conducted to validate the effectiveness of Sim-T. In Aishell-1 dataset, when the proposed Sim-T is 48% para
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#36866;&#37197;&#22120;&#65288;CoDA&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;&#29616;&#26377;&#30340;&#23494;&#38598;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#22686;&#21152;&#31232;&#30095;&#28608;&#27963;&#12289;&#23569;&#37327;&#26032;&#21442;&#25968;&#20197;&#21450;&#36731;&#37327;&#32423;&#30340;&#35757;&#32451;&#38454;&#27573;&#26469;&#23454;&#29616;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;2&#20493;&#33267;8&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#19988;&#20934;&#30830;&#29575;&#26377;&#36731;&#24494;&#25110;&#26080;&#25439;&#22833;&#65292;&#19988;&#21442;&#25968;&#25928;&#29575;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2304.04947</link><description>&lt;p&gt;
&#26465;&#20214;&#36866;&#37197;&#22120;&#65306;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference. (arXiv:2304.04947v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04947
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#36866;&#37197;&#22120;&#65288;CoDA&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;&#29616;&#26377;&#30340;&#23494;&#38598;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#22686;&#21152;&#31232;&#30095;&#28608;&#27963;&#12289;&#23569;&#37327;&#26032;&#21442;&#25968;&#20197;&#21450;&#36731;&#37327;&#32423;&#30340;&#35757;&#32451;&#38454;&#27573;&#26469;&#23454;&#29616;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;2&#20493;&#33267;8&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#19988;&#20934;&#30830;&#29575;&#26377;&#36731;&#24494;&#25110;&#26080;&#25439;&#22833;&#65292;&#19988;&#21442;&#25968;&#25928;&#29575;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#36866;&#37197;&#22120;&#65288;CoDA&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;CoDA&#19981;&#20165;&#36866;&#29992;&#20110;&#26631;&#20934;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#35745;&#31639;&#26469;&#23454;&#29616;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#29616;&#26377;&#30340;&#23494;&#38598;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#22686;&#21152;&#31232;&#30095;&#28608;&#27963;&#12289;&#23569;&#37327;&#26032;&#21442;&#25968;&#20197;&#21450;&#36731;&#37327;&#32423;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;CoDA&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20986;&#20046;&#24847;&#26009;&#30340;&#20256;&#36882;&#30693;&#35782;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#36866;&#37197;&#22120;&#26041;&#27861;&#30456;&#27604;&#65292;CoDA&#22312;&#21508;&#31181;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#35821;&#38899;&#20219;&#21153;&#20013;&#37117;&#23454;&#29616;&#20102;2&#20493;&#33267;8&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#32780;&#19988;&#20934;&#30830;&#29575;&#26377;&#36731;&#24494;&#25110;&#26080;&#25439;&#22833;&#65292;&#19988;&#21442;&#25968;&#25928;&#29575;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Conditional Adapter (CoDA), a parameter-efficient transfer learning method that also improves inference efficiency. CoDA generalizes beyond standard adapter approaches to enable a new way of balancing speed and accuracy using conditional computation. Starting with an existing dense pretrained model, CoDA adds sparse activation together with a small number of new parameters and a light-weight training phase. Our experiments demonstrate that the CoDA approach provides an unexpectedly efficient way to transfer knowledge. Across a variety of language, vision, and speech tasks, CoDA achieves a 2x to 8x inference speed-up compared to the state-of-the-art Adapter approach with moderate to no accuracy loss and the same parameter efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21363;&#20855;&#26377;&#25551;&#36848;&#24615;&#20851;&#31995;&#25552;&#31034;&#30340;&#23545;&#27604;&#23398;&#20064;(CTL-DRP)&#65292;&#36890;&#36807;&#32771;&#34385;&#23454;&#20307;&#20449;&#24687;&#12289;&#20851;&#31995;&#30693;&#35782;&#21644;&#23454;&#20307;&#31867;&#22411;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312; TACRED&#12289;TACREV &#21644; Re-TACRED &#19978;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340; F1 &#20998;&#25968;&#65292;&#20854;&#20013;&#22312; Re-TACRED &#19978;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.04935</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#26377;&#25551;&#36848;&#24615;&#20851;&#31995;&#25552;&#31034;&#30340;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Sentence-Level Relation Extraction via Contrastive Learning with Descriptive Relation Prompts. (arXiv:2304.04935v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21363;&#20855;&#26377;&#25551;&#36848;&#24615;&#20851;&#31995;&#25552;&#31034;&#30340;&#23545;&#27604;&#23398;&#20064;(CTL-DRP)&#65292;&#36890;&#36807;&#32771;&#34385;&#23454;&#20307;&#20449;&#24687;&#12289;&#20851;&#31995;&#30693;&#35782;&#21644;&#23454;&#20307;&#31867;&#22411;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312; TACRED&#12289;TACREV &#21644; Re-TACRED &#19978;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340; F1 &#20998;&#25968;&#65292;&#20854;&#20013;&#22312; Re-TACRED &#19978;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#35782;&#21035;&#32473;&#23450;&#21477;&#23376;&#20013;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#33719;&#24471;&#26356;&#22909;&#30340;&#23454;&#20307;&#34920;&#31034;&#24182;&#37319;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#12290;&#36825;&#20123;&#24037;&#20316;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#22312;&#20110;&#23427;&#20204;&#24573;&#30053;&#20102;&#32972;&#26223;&#20851;&#31995;&#30693;&#35782;&#21644;&#23454;&#20307;&#31867;&#22411;&#19982;&#20505;&#36873;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21363;&#20855;&#26377;&#25551;&#36848;&#24615;&#20851;&#31995;&#25552;&#31034;&#30340;&#23545;&#27604;&#23398;&#20064;(CTL-DRP)&#65292;&#20197;&#20849;&#21516;&#32771;&#34385;&#23454;&#20307;&#20449;&#24687;&#12289;&#20851;&#31995;&#30693;&#35782;&#21644;&#23454;&#20307;&#31867;&#22411;&#38480;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#23884;&#20837;&#26102;&#24341;&#20837;&#20102;&#25913;&#36827;&#30340;&#23454;&#20307;&#26631;&#35760;&#21644;&#25551;&#36848;&#24615;&#20851;&#31995;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#21463;&#38480;&#20505;&#36873;&#20851;&#31995;&#36827;&#34892;&#25490;&#21517;&#12290;CTL-DRP &#22312; TACRED &#19978;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340; F1 &#20998;&#25968;&#20026; 76.7%&#12290;&#27492;&#22806;&#65292;&#26032;&#25552;&#20986;&#30340;&#33539;&#24335;&#22312; TACREV &#21644; Re-TACRED &#19978;&#20998;&#21035;&#21462;&#24471;&#20102; 85.8% &#21644; 91.6% &#30340; F1 &#20998;&#25968;&#65292;&#20004;&#32773;&#22343;&#20026;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence-level relation extraction aims to identify the relation between two entities for a given sentence. The existing works mostly focus on obtaining a better entity representation and adopting a multi-label classifier for relation extraction. A major limitation of these works is that they ignore background relational knowledge and the interrelation between entity types and candidate relations. In this work, we propose a new paradigm, Contrastive Learning with Descriptive Relation Prompts(CTL-DRP), to jointly consider entity information, relational knowledge and entity type restrictions. In particular, we introduce an improved entity marker and descriptive relation prompts when generating contextual embedding, and utilize contrastive learning to rank the restricted candidate relations. The CTL-DRP obtains a competitive F1-score of 76.7% on TACRED. Furthermore, the new presented paradigm achieves F1-scores of 85.8% and 91.6% on TACREV and Re-TACRED respectively, which are both the st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21487;&#29992;&#20110;&#25552;&#20379;&#33258;&#36866;&#24212;&#30340;&#25945;&#32946;&#25903;&#25345;&#65292;&#23588;&#20854;&#23545;&#20110;&#26368;&#21021;&#25104;&#32489;&#36739;&#20302;&#30340;&#23398;&#29983;&#20855;&#26377;&#26368;&#22823;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.04933</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#23548;&#21592;&#22312;&#25968;&#23398;&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#25903;&#25345;&#20102;&#20302;&#25104;&#32489;&#23398;&#29983;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task. (arXiv:2304.04933v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21487;&#29992;&#20110;&#25552;&#20379;&#33258;&#36866;&#24212;&#30340;&#25945;&#32946;&#25903;&#25345;&#65292;&#23588;&#20854;&#23545;&#20110;&#26368;&#21021;&#25104;&#32489;&#36739;&#20302;&#30340;&#23398;&#29983;&#20855;&#26377;&#26368;&#22823;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#38480;&#21046;&#20351;&#24471;&#20026;&#25152;&#26377;&#23398;&#29983;&#25552;&#20379;&#20010;&#24615;&#21270;&#25945;&#23398;&#21464;&#24471;&#22256;&#38590;&#12290;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#25104;&#20026;&#20943;&#23569;&#21457;&#23637;&#25104;&#26412;&#12289;&#25552;&#39640;&#26234;&#33021;&#36741;&#23548;&#36719;&#20214;&#25928;&#26524;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#26088;&#22312;&#20026;&#23398;&#29983;&#25552;&#20379;&#27491;&#30830;&#30340;&#25903;&#25345;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22914;&#20309;&#22312;&#21465;&#36848;&#25925;&#20107;&#32447;&#36719;&#20214;&#20013;&#20026;&#23398;&#20064;&#8220;&#23481;&#31215;&#8221;&#27010;&#24565;&#30340;&#23398;&#29983;&#25552;&#20379;&#33258;&#36866;&#24212;&#25945;&#32946;&#25903;&#25345;&#12290;&#36890;&#36807;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#25105;&#20204;&#20063;&#25552;&#21462;&#20102;&#26377;&#20851;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#27934;&#35265;&#65292;&#35777;&#26126;&#20102;&#25152;&#24471;&#25919;&#31574;&#22312;&#19981;&#21516;&#30340;&#23398;&#29983;&#32676;&#20307;&#20013;&#20855;&#26377;&#31867;&#20284;&#30340;&#34920;&#29616;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#36825;&#20004;&#39033;&#30740;&#31350;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#25925;&#20107;&#31995;&#32479;&#23545;&#26368;&#21021;&#30340;&#39044;&#27979;&#20998;&#25968;&#26368;&#20302;&#30340;&#23398;&#29983;&#26377;&#26368;&#22823;&#30340;&#30410;&#22788;&#65292;&#36825;&#34920;&#26126;&#20102;AI&#36866;&#24212;&#24182;&#20026;&#20302;&#25104;&#32489;&#23398;&#29983;&#25552;&#20379;&#25903;&#25345;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource limitations make it hard to provide all students with one of the most effective educational interventions: personalized instruction. Reinforcement learning could be a key tool to reduce the development cost and improve the effectiveness of, intelligent tutoring software that aims to provide the right support, at the right time, to a student. Here we illustrate that deep reinforcement learning can be used to provide adaptive pedagogical support to students learning about the concept of volume in a narrative storyline software. Using explainable artificial intelligence tools, we also extracted interpretable insights about the pedagogical policy learned, and we demonstrate that the resulting policy had similar performance in a different student population. Most importantly, in both studies the reinforcement-learning narrative system had the largest benefit for those students with the lowest initial pretest scores, suggesting the opportunity for AI to adapt and provide support for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#20195;&#29702;&#20219;&#21153;&#29992;&#20110;&#22312;&#20195;&#29702;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#29983;&#25104;&#26410;&#26469;&#35270;&#22270;&#22270;&#20687;&#35821;&#20041;&#65292;&#20174;&#32780;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04907</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#26410;&#26469;&#35270;&#22270;&#22270;&#20687;&#35821;&#20041;&#20197;&#25913;&#21892;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Improving Vision-and-Language Navigation by Generating Future-View Image Semantics. (arXiv:2304.04907v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#20195;&#29702;&#20219;&#21153;&#29992;&#20110;&#22312;&#20195;&#29702;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#29983;&#25104;&#26410;&#26469;&#35270;&#22270;&#22270;&#20687;&#35821;&#20041;&#65292;&#20174;&#32780;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#35201;&#27714;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#65292;&#20195;&#29702;&#36890;&#36807;&#20174;&#21487;&#23548;&#33322;&#20301;&#32622;&#38598;&#21512;&#20013;&#36827;&#34892;&#36873;&#25321;&#26469;&#36873;&#25321;&#19979;&#19968;&#27493;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36827;&#19968;&#27493;&#25506;&#32034;&#20195;&#29702;&#26159;&#21542;&#21487;&#20197;&#21463;&#30410;&#20110;&#22312;&#23548;&#33322;&#26399;&#38388;&#29983;&#25104;&#28508;&#22312;&#26410;&#26469;&#35270;&#22270;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#20154;&#31867;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#21608;&#22260;&#30340;&#35270;&#22270;&#20250;&#23545;&#26410;&#26469;&#30340;&#29615;&#22659;&#26377;&#19968;&#20010;&#39044;&#26399;&#65292;&#24182;&#24110;&#21161;&#27491;&#30830;&#22320;&#23548;&#33322;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#32473;&#20195;&#29702;&#35013;&#22791;&#36825;&#31181;&#29983;&#25104;&#26410;&#26469;&#23548;&#33322;&#35270;&#22270;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#20195;&#29702;&#30340;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20986;&#20102;&#19977;&#31181;&#20195;&#29702;&#20219;&#21153;: &#25513;&#34109;&#20840;&#26223;&#24314;&#27169; (MPM)&#65292;&#25513;&#34109;&#36712;&#36857;&#24314;&#27169; (MTM) &#21644;&#24102;&#26377;&#22270;&#20687;&#29983;&#25104;&#30340;&#21160;&#20316;&#39044;&#27979; (APIG)&#12290;&#36825;&#19977;&#20010;&#30446;&#26631;&#25945;&#20250;&#20102;&#27169;&#22411;&#39044;&#27979;&#20840;&#26223;&#20013;&#30340;&#32570;&#23569;&#35270;&#22270; (MPM)&#12289;&#39044;&#27979;&#23436;&#25972;&#36712;&#36857;&#20013;&#30340;&#32570;&#23569;&#27493;&#39588; (MTM) &#21644;&#36827;&#34892;&#21160;&#20316;&#39044;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104; (APIG)&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-and-Language Navigation (VLN) is the task that requires an agent to navigate through the environment based on natural language instructions. At each step, the agent takes the next action by selecting from a set of navigable locations. In this paper, we aim to take one step further and explore whether the agent can benefit from generating the potential future view during navigation. Intuitively, humans will have an expectation of how the future environment will look like, based on the natural language instructions and surrounding views, which will aid correct navigation. Hence, to equip the agent with this ability to generate the semantics of future navigation views, we first propose three proxy tasks during the agent's in-domain pre-training: Masked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action Prediction with Image Generation (APIG). These three objectives teach the model to predict missing views in a panorama (MPM), predict missing steps in the full tra
&lt;/p&gt;</description></item><item><title>DISTO&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#29983;&#25104;&#30340;&#24178;&#25200;&#36873;&#39033;&#12290;DISTO&#19982;&#20154;&#31867;&#23545;&#24178;&#25200;&#36873;&#39033;&#30340;&#35780;&#20998;&#39640;&#24230;&#30456;&#20851;&#65292;&#19988;&#25490;&#21517;&#26368;&#20808;&#36827;&#30340;&#24178;&#25200;&#36873;&#39033;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#24230;&#37327;&#26631;&#20934;&#38750;&#24120;&#19981;&#21516;&#65292;&#34920;&#26126;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#26631;&#20934;&#19981;&#36866;&#29992;&#20110;&#24178;&#25200;&#36873;&#39033;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.04881</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#36127;&#37319;&#26679;&#30340;&#26041;&#27861;&#35780;&#20272;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#30340;&#24178;&#25200;&#36873;&#39033;
&lt;/p&gt;
&lt;p&gt;
DISTO: Evaluating Textual Distractors for Multi-Choice Questions using Negative Sampling based Approach. (arXiv:2304.04881v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04881
&lt;/p&gt;
&lt;p&gt;
DISTO&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#29983;&#25104;&#30340;&#24178;&#25200;&#36873;&#39033;&#12290;DISTO&#19982;&#20154;&#31867;&#23545;&#24178;&#25200;&#36873;&#39033;&#30340;&#35780;&#20998;&#39640;&#24230;&#30456;&#20851;&#65292;&#19988;&#25490;&#21517;&#26368;&#20808;&#36827;&#30340;&#24178;&#25200;&#36873;&#39033;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#24230;&#37327;&#26631;&#20934;&#38750;&#24120;&#19981;&#21516;&#65292;&#34920;&#26126;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#26631;&#20934;&#19981;&#36866;&#29992;&#20110;&#24178;&#25200;&#36873;&#39033;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#26159;&#19968;&#31181;&#35780;&#20272;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#30340;&#39640;&#25928;&#24120;&#35265;&#26041;&#27861;&#12290;&#27599;&#36947;&#22810;&#39033;&#36873;&#25321;&#39064;&#38656;&#35201;&#19968;&#32452;&#24178;&#25200;&#36873;&#39033;&#65292;&#36825;&#20123;&#36873;&#39033;&#34429;&#28982;&#19981;&#27491;&#30830;&#65292;&#20294;&#35201;&#36275;&#22815;&#21512;&#29702;&#20197;&#32771;&#26597;&#23398;&#29983;&#30340;&#30693;&#35782;&#25484;&#25569;&#24773;&#20917;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#24178;&#25200;&#36873;&#39033;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#24615;&#33021;&#36890;&#24120;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#26631;&#20934;&#32463;&#24120;&#35823;&#21028;&#29983;&#25104;&#30340;&#24178;&#25200;&#36873;&#39033;&#30340;&#21512;&#36866;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DISTO&#65306;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24178;&#25200;&#36873;&#39033;&#30340;&#31532;&#19968;&#20010;&#23398;&#20064;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;DISTO&#24471;&#20998;&#19982;&#20154;&#31867;&#23545;&#24178;&#25200;&#36873;&#39033;&#36136;&#37327;&#30340;&#35780;&#20998;&#39640;&#24230;&#30456;&#20851;&#26469;&#39564;&#35777;DISTO&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;DISTO&#25490;&#21517;&#26368;&#20808;&#36827;&#30340;&#24178;&#25200;&#36873;&#39033;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#24230;&#37327;&#26631;&#20934;&#38750;&#24120;&#19981;&#21516;&#65292;&#34920;&#26126;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#26631;&#20934;&#19981;&#24212;&#29992;&#20110;&#24178;&#25200;&#36873;&#39033;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple choice questions (MCQs) are an efficient and common way to assess reading comprehension (RC). Every MCQ needs a set of distractor answers that are incorrect, but plausible enough to test student knowledge. Distractor generation (DG) models have been proposed, and their performance is typically evaluated using machine translation (MT) metrics. However, MT metrics often misjudge the suitability of generated distractors. We propose DISTO: the first learned evaluation metric for generated distractors. We validate DISTO by showing its scores correlate highly with human ratings of distractor quality. At the same time, DISTO ranks the performance of state-of-the-art DG models very differently from MT-based metrics, showing that MT metrics should not be used for distractor evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;GenKS&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#30693;&#35782;&#36873;&#25321;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#29983;&#25104;&#30693;&#35782;&#29255;&#27573;&#30340;&#26631;&#35782;&#31526;&#65292;&#25429;&#25417;&#24182;&#35299;&#20915;&#20102;&#30693;&#35782;&#20869;&#37096;&#30340;&#20132;&#20114;&#65292;&#21516;&#26102;&#36890;&#36807;&#36229;&#38142;&#25509;&#26426;&#21046;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#23545;&#35805;-&#30693;&#35782;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2304.04836</link><description>&lt;p&gt;
&#30693;&#35782;&#29983;&#25104;&#24335;&#36873;&#25321;&#22312;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Knowledge Selection for Knowledge-Grounded Dialogues. (arXiv:2304.04836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;GenKS&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#30693;&#35782;&#36873;&#25321;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#29983;&#25104;&#30693;&#35782;&#29255;&#27573;&#30340;&#26631;&#35782;&#31526;&#65292;&#25429;&#25417;&#24182;&#35299;&#20915;&#20102;&#30693;&#35782;&#20869;&#37096;&#30340;&#20132;&#20114;&#65292;&#21516;&#26102;&#36890;&#36807;&#36229;&#38142;&#25509;&#26426;&#21046;&#26174;&#24335;&#22320;&#24314;&#27169;&#20102;&#23545;&#35805;-&#30693;&#35782;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36873;&#25321;&#26159;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23427;&#26088;&#22312;&#26681;&#25454;&#23545;&#35805;&#21382;&#21490;&#36873;&#25321;&#36866;&#24403;&#30340;&#30693;&#35782;&#29255;&#27573;&#29992;&#20110;&#22238;&#22797;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20998;&#31867;&#26041;&#27861;&#29420;&#31435;&#22320;&#23558;&#27599;&#20010;&#20505;&#36873;&#29255;&#27573;&#20998;&#31867;&#20026;&#8220;&#30456;&#20851;&#8221;&#25110;&#8220;&#19981;&#30456;&#20851;&#8221;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24573;&#30053;&#20102;&#29255;&#27573;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#23548;&#33268;&#25512;&#26029;&#29255;&#27573;&#21547;&#20041;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#35805;-&#30693;&#35782;&#30456;&#20114;&#20316;&#29992;&#30340;&#35821;&#31687;&#32467;&#26500;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;GenKS&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#30693;&#35782;&#36873;&#25321;&#26041;&#27861;&#12290;GenKS&#23398;&#20064;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#29983;&#25104;&#23427;&#20204;&#30340;&#26631;&#35782;&#31526;&#26469;&#36873;&#25321;&#29255;&#27573;&#12290;&#22240;&#27492;&#65292;GenKS&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#22825;&#28982;&#22320;&#25429;&#25417;&#20102;&#30693;&#35782;&#20869;&#37096;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36229;&#38142;&#25509;&#26426;&#21046;&#26469;&#26174;&#24335;&#22320;&#24314;&#27169;&#23545;&#35805;-&#30693;&#35782;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#39564;&#35777;&#20102;GenKS&#22312;&#30456;&#20851;&#24615;&#12289;&#22810;&#26679;&#24615;&#20197;&#21450;&#29983;&#25104;&#36866;&#24403;&#22238;&#22797;&#30340;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge selection is the key in knowledge-grounded dialogues (KGD), which aims to select an appropriate knowledge snippet to be used in the utterance based on dialogue history. Previous studies mainly employ the classification approach to classify each candidate snippet as "relevant" or "irrelevant" independently. However, such approaches neglect the interactions between snippets, leading to difficulties in inferring the meaning of snippets. Moreover, they lack modeling of the discourse structure of dialogue-knowledge interactions. We propose a simple yet effective generative approach for knowledge selection, called GenKS. GenKS learns to select snippets by generating their identifiers with a sequence-to-sequence model. GenKS therefore captures intra-knowledge interaction inherently through attention mechanisms. Meanwhile, we devise a hyperlink mechanism to model the dialogue-knowledge interactions explicitly. We conduct experiments on three benchmark datasets, and verify GenKS achie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#27604;&#36739;&#30740;&#31350;&#34920;&#26126;&#65292;COVID-19&#38169;&#35823;&#20449;&#24687;&#30340;&#20998;&#24067;&#12289;&#20256;&#25773;&#33021;&#21147;&#12289;&#35821;&#35328;&#20998;&#26512;&#19982;&#20934;&#30830;&#20449;&#24687;&#19981;&#21516;&#65292;&#24182;&#19988;&#30740;&#21046;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24179;&#22343;&#25552;&#39640;&#20102;9%&#20197;&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.04811</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;COVID-19&#20449;&#24687;&#19982;&#38169;&#35823;&#20449;&#24687;&#30340;&#22823;&#35268;&#27169;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Large-Scale Comparative Study of Accurate COVID-19 Information versus Misinformation. (arXiv:2304.04811v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#27604;&#36739;&#30740;&#31350;&#34920;&#26126;&#65292;COVID-19&#38169;&#35823;&#20449;&#24687;&#30340;&#20998;&#24067;&#12289;&#20256;&#25773;&#33021;&#21147;&#12289;&#35821;&#35328;&#20998;&#26512;&#19982;&#20934;&#30830;&#20449;&#24687;&#19981;&#21516;&#65292;&#24182;&#19988;&#30740;&#21046;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#24179;&#22343;&#25552;&#39640;&#20102;9%&#20197;&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#23548;&#33268;&#20449;&#24687;&#27867;&#28389;&#65292;&#22823;&#37327;&#19982;COVID-19&#30456;&#20851;&#30340;&#20869;&#23481;&#34987;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#39640;&#36895;&#20256;&#25773;&#65292;&#36825;&#35753;&#20844;&#20247;&#38590;&#20197;&#21306;&#20998;COVID-19&#30340;&#20934;&#30830;&#21644;&#38169;&#35823;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#23545;&#36229;&#36807;2.42&#20159;&#26465;&#25512;&#29305;&#36827;&#34892;&#22823;&#35268;&#27169;&#35745;&#31639;&#20998;&#26512;&#65292;&#23545;COVID-19&#38169;&#35823;&#21644;&#20934;&#30830;&#20449;&#24687;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#30740;&#31350;&#28085;&#30422;&#22235;&#20010;&#26041;&#38754;: 1)&#20027;&#39064;&#30340;&#20998;&#24067;&#65292;2)&#25512;&#29305;&#30340;&#23454;&#26102;&#24615;&#65292;3)&#35821;&#35328;&#20998;&#26512;&#21644;4)&#26102;&#38388;&#19978;&#30340;&#20256;&#25773;&#33021;&#21147;&#12290;&#26412;&#25991;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#26032;&#30340;COVID-19&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#26368;&#21518;&#28436;&#31034;&#20102;&#35813;&#25968;&#25454;&#38598;&#33021;&#22815;&#36890;&#36807;&#24179;&#22343;F1&#25351;&#26631;&#23558;&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#25552;&#39640;9%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic led to an infodemic where an overwhelming amount of COVID-19 related content was being disseminated at high velocity through social media. This made it challenging for citizens to differentiate between accurate and inaccurate information about COVID-19. This motivated us to carry out a comparative study of the characteristics of COVID-19 misinformation versus those of accurate COVID-19 information through a large-scale computational analysis of over 242 million tweets. The study makes comparisons alongside four key aspects: 1) the distribution of topics, 2) the live status of tweets, 3) language analysis and 4) the spreading power over time. An added contribution of this study is the creation of a COVID-19 misinformation classification dataset. Finally, we demonstrate that this new dataset helps improve misinformation classification by more than 9% based on average F1 measure.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32771;&#34385;&#20102;&#26102;&#38388;&#24615;&#23545;COVID-19&#30123;&#33495;&#24577;&#24230;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26102;&#38388;&#20998;&#21106;&#26174;&#33879;&#38477;&#20302;&#20102;&#31435;&#22330;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04806</link><description>&lt;p&gt;
&#32771;&#23519;COVID-19&#30123;&#33495;&#24577;&#24230;&#26816;&#27979;&#20013;&#30340;&#26102;&#38388;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining Temporalities on Stance Detection Towards COVID-19 Vaccination. (arXiv:2304.04806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04806
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32771;&#34385;&#20102;&#26102;&#38388;&#24615;&#23545;COVID-19&#30123;&#33495;&#24577;&#24230;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26102;&#38388;&#20998;&#21106;&#26174;&#33879;&#38477;&#20302;&#20102;&#31435;&#22330;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#30123;&#33495;&#25509;&#31181;&#26159;&#25511;&#21046;COVID-19&#20256;&#25773;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#20102;&#35299;&#20844;&#20247;&#30340;&#30123;&#33495;&#24577;&#24230;&#23545;&#20915;&#31574;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340; COVID-19 &#30123;&#33495;&#24577;&#24230;&#65288;&#22914;&#25903;&#25345;&#21644;&#29369;&#35947;&#65289;&#20250;&#38543;&#26102;&#38388;&#32780;&#28436;&#21464;&#65292;&#22240;&#27492;&#22312;&#20998;&#26512;&#36825;&#20123;&#31435;&#22330;&#26102;&#38656;&#35201;&#32771;&#34385;&#21487;&#33021;&#30340;&#26102;&#38388;&#28418;&#31227;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#26816;&#26597;&#26102;&#38388;&#27010;&#24565;&#28418;&#31227;&#23545;&#25512;&#29305;&#19978; COVID-19 &#30123;&#33495;&#31435;&#22330;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#23545;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36827;&#34892;&#20102;&#38543;&#26426;&#21644;&#26102;&#38388;&#20998;&#21106;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#21333;&#35821;&#21644;&#22810;&#35821;&#25968;&#25454;&#38598;&#30340;&#38543;&#26426;&#21644;&#26102;&#38388;&#20998;&#21106;&#20043;&#38388;&#65292;&#27169;&#22411;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#26102;&#38388;&#20998;&#21106;&#26174;&#33879;&#38477;&#20302;&#20102;&#31435;&#22330;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have highlighted the importance of vaccination as an effective strategy to control the transmission of the COVID-19 virus. It is crucial for policymakers to have a comprehensive understanding of the public's stance towards vaccination on a large scale. However, attitudes towards COVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved over time on social media. Thus, it is necessary to account for possible temporal shifts when analysing these stances. This study aims to examine the impact of temporal concept drift on stance detection towards COVID-19 vaccination on Twitter. To this end, we evaluate a range of transformer-based models using chronological and random splits of social media data. Our findings demonstrate significant discrepancies in model performance when comparing random and chronological splits across all monolingual and multilingual datasets. Chronological splits significantly reduce the accuracy of stance classification. Therefore, 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#20204;&#22312;&#29702;&#35299;&#35821;&#35328;&#26102;&#20250;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#26410;&#26126;&#35828;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#26399;&#26395;&#26469;&#20570;&#20986;&#26631;&#24230;&#25512;&#29702;&#65292;&#35813;&#26426;&#21046;&#35299;&#37322;&#20102;&#26631;&#24230;&#25512;&#29702;&#20869;&#37096;&#21644;&#36328;&#26631;&#24230;&#21464;&#21270;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2304.04758</link><description>&lt;p&gt;
&#26080;&#35328;&#30340;&#36873;&#25321;&#23545;&#26399;&#26395;&#30340;&#24433;&#21709;&#21450;&#20854;&#23545;&#35821;&#29992;&#25512;&#29702;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Expectations over Unspoken Alternatives Predict Pragmatic Inferences. (arXiv:2304.04758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04758
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#20204;&#22312;&#29702;&#35299;&#35821;&#35328;&#26102;&#20250;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#26410;&#26126;&#35828;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#26399;&#26395;&#26469;&#20570;&#20986;&#26631;&#24230;&#25512;&#29702;&#65292;&#35813;&#26426;&#21046;&#35299;&#37322;&#20102;&#26631;&#24230;&#25512;&#29702;&#20869;&#37096;&#21644;&#36328;&#26631;&#24230;&#21464;&#21270;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#24230;&#25512;&#29702;&#26159;&#20154;&#31867;&#22522;&#20110;&#26080;&#35328;&#30340;&#36873;&#25321;&#35299;&#37322;&#35821;&#35328;&#30340;&#19968;&#20010;&#20856;&#22411;&#20363;&#23376;&#12290;&#34429;&#28982;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#20154;&#31867;&#30340;&#26631;&#24230;&#25512;&#29702;&#29575;&#21464;&#21270;&#24456;&#22823;&#65292;&#26082;&#22312;&#21333;&#20010;&#26631;&#24230;&#30340;&#23454;&#20363;&#20869;&#65292;&#20063;&#22312;&#19981;&#21516;&#26631;&#24230;&#20043;&#38388;&#65292;&#20294;&#24456;&#23569;&#26377;&#32508;&#21512;&#35299;&#37322;&#33021;&#22815;&#23450;&#37327;&#35299;&#37322;&#36328;&#26631;&#24230;&#21644;&#21333;&#19968;&#26631;&#24230;&#20869;&#30340;&#21464;&#24322;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#36890;&#24120;&#35748;&#20026;&#26631;&#24230;&#25512;&#29702;&#26159;&#36890;&#36807;&#24605;&#32771;&#26080;&#35328;&#30340;&#36873;&#25321;&#32780;&#20135;&#29983;&#30340;&#65292;&#20294;&#20154;&#20204;&#20173;&#22312;&#20105;&#35770;&#20154;&#31867;&#26159;&#22312;&#35821;&#35328;&#24418;&#24335;&#23618;&#38754;&#36824;&#26159;&#22312;&#27010;&#24565;&#23618;&#38754;&#19978;&#25512;&#29702;&#26080;&#35328;&#30340;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#20010;&#20849;&#20139;&#30340;&#26426;&#21046;&#65292;&#35299;&#37322;&#20102;&#26631;&#24230;&#25512;&#29702;&#29575;&#22312;&#26631;&#24230;&#20869;&#37096;&#21644;&#36328;&#26631;&#24230;&#20869;&#30340;&#21464;&#24322;&#65306;&#20851;&#20110;&#26080;&#35328;&#30340;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#39537;&#21160;&#30340;&#26399;&#26395;&#12290;&#20351;&#29992;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26469;&#36817;&#20284;&#20154;&#31867;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#24230;&#25512;&#29702;&#29575;&#21487;&#20197;&#36890;&#36807;&#20013;&#24615;&#26631;&#24230;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#30340;&#39044;&#26399;&#31243;&#24230;&#26469;&#25429;&#33719;&#12290; &#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#22522;&#20110;&#21547;&#20041;&#30340;&#35266;&#28857;&#19979;&#65292;&#26399;&#26395;&#26174;&#33879;&#22320;&#21482;&#39044;&#27979;&#36328;&#26631;&#24230;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scalar inferences (SI) are a signature example of how humans interpret language based on unspoken alternatives. While empirical studies have demonstrated that human SI rates are highly variable -- both within instances of a single scale, and across different scales -- there have been few proposals that quantitatively explain both cross- and within-scale variation. Furthermore, while it is generally assumed that SIs arise through reasoning about unspoken alternatives, it remains debated whether humans reason about alternatives as linguistic forms, or at the level of concepts. Here, we test a shared mechanism explaining SI rates within and across scales: context-driven expectations about the unspoken alternatives. Using neural language models to approximate human predictive distributions, we find that SI rates are captured by the expectedness of the strong scalemate as an alternative. Crucially, however, expectedness robustly predicts cross-scale variation only under a meaning-based view
&lt;/p&gt;</description></item><item><title>ESPnet-ST-v2&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#21151;&#33021;&#21475;&#35821;&#32763;&#35793;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#22810;&#31181;&#32763;&#35793;&#20219;&#21153;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#26550;&#26500;&#21644;&#25216;&#26415;&#65292;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.04596</link><description>&lt;p&gt;
ESPnet-ST-v2&#65306;&#22810;&#21151;&#33021;&#21475;&#35821;&#32763;&#35793;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit. (arXiv:2304.04596v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04596
&lt;/p&gt;
&lt;p&gt;
ESPnet-ST-v2&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#21151;&#33021;&#21475;&#35821;&#32763;&#35793;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#22810;&#31181;&#32763;&#35793;&#20219;&#21153;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#26550;&#26500;&#21644;&#25216;&#26415;&#65292;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ESPnet-ST-v2&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#21475;&#35821;&#32763;&#35793;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#31163;&#32447;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#65288;ST&#65289;&#12289;&#21516;&#27493;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#65288;SST&#65289;&#21644;&#31163;&#32447;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#65288;S2ST&#65289;&#12290;&#19982;&#20854;&#20182;&#24320;&#28304;&#21475;&#35821;&#32763;&#35793;&#24037;&#20855;&#21253;&#19981;&#21516;&#30340;&#26159;&#65292;ESPnet-ST-v2&#37319;&#29992;&#20102;&#35768;&#22810;&#20808;&#36827;&#30340;&#26550;&#26500;&#65292;&#21253;&#25324;&#36716;&#24405;&#22120;&#12289;&#28151;&#21512;CTC/attention&#12289;&#22810;&#35299;&#30721;&#22120;&#12289;&#26102;&#38388;&#21516;&#27493;&#20998;&#22359;CTC/attention&#12289;Translatotron&#27169;&#22411;&#21644;&#30452;&#25509;&#31163;&#25955;&#21333;&#20803;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) -- each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIPPINGS&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#35760;&#24405;&#38142;&#25509;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#23545;&#31216;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#30456;&#36817;&#25110;&#19981;&#21516;&#31867;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#65292;&#22914;&#26500;&#24314;&#20840;&#38754;&#30340;&#34917;&#20805;&#19987;&#21033;&#27880;&#20876;&#34920;&#21644;&#35782;&#21035;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20010;&#20154;&#12290;</title><link>http://arxiv.org/abs/2304.03464</link><description>&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#36830;&#25509;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Linking Representations with Multimodal Contrastive Learning. (arXiv:2304.03464v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIPPINGS&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#35760;&#24405;&#38142;&#25509;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#23545;&#31216;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#30456;&#36817;&#25110;&#19981;&#21516;&#31867;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#65292;&#22914;&#26500;&#24314;&#20840;&#38754;&#30340;&#34917;&#20805;&#19987;&#21033;&#27880;&#20876;&#34920;&#21644;&#35782;&#21035;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20010;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#23558;&#21253;&#21547;&#22312;&#21508;&#31181;&#25991;&#26723;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#20998;&#32452;&#25104;&#31867;&#12290;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#19981;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65292;&#20063;&#19981;&#21033;&#29992;&#25991;&#26723;&#22266;&#26377;&#30340;&#22810;&#27169;&#24577;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35760;&#24405;&#38142;&#25509;&#36890;&#24120;&#34987;&#27010;&#24565;&#21270;&#20026;&#23383;&#31526;&#20018;&#21305;&#37197;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102; CLIPPINGS&#65292;&#19968;&#31181;&#29992;&#20110;&#35760;&#24405;&#38142;&#25509;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#12290;CLIPPINGS &#37319;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#23545;&#31216;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21452;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#36827;&#34892;&#23545;&#40784;&#65292;&#23398;&#20064;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#65292;&#20854;&#20013;&#32473;&#23450;&#23454;&#20363;&#30340;&#27719;&#24635;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#38752;&#36817;&#21516;&#19968;&#31867;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#36828;&#31163;&#19981;&#21516;&#31867;&#20013;&#30340;&#34920;&#31034;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#31163;&#32447;&#31034;&#20363;&#23884;&#20837;&#32034;&#24341;&#20013;&#26816;&#32034;&#23427;&#20204;&#26368;&#36817;&#30340;&#37051;&#23621;&#25110;&#32858;&#31867;&#23427;&#20204;&#30340;&#34920;&#31034;&#26469;&#38142;&#25509;&#23454;&#20363;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24212;&#29992;&#65306;&#36890;&#36807;&#23558;&#19987;&#21033;&#19982;&#20854;&#23545;&#24212;&#30340;&#30417;&#31649;&#25991;&#20214;&#38142;&#25509;&#26469;&#26500;&#24314;&#20840;&#38754;&#30340;&#34917;&#20805;&#19987;&#21033;&#27880;&#20876;&#34920;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#35782;&#21035;&#20010;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications require grouping instances contained in diverse document datasets into classes. Most widely used methods do not employ deep learning and do not exploit the inherently multimodal nature of documents. Notably, record linkage is typically conceptualized as a string-matching problem. This study develops CLIPPINGS, (Contrastively Linking Pooled Pre-trained Embeddings), a multimodal framework for record linkage. CLIPPINGS employs end-to-end training of symmetric vision and language bi-encoders, aligned through contrastive language-image pre-training, to learn a metric space where the pooled image-text representation for a given instance is close to representations in the same class and distant from representations in different classes. At inference time, instances can be linked by retrieving their nearest neighbor from an offline exemplar embedding index or by clustering their representations. The study examines two challenging applications: constructing comprehensive suppl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#35834;&#26364;&#30340;&#19971;&#20010;&#21160;&#20316;&#38454;&#27573;&#20316;&#20026;&#26234;&#33021;&#20889;&#20316;&#21161;&#25163;&#20132;&#20114;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#25903;&#25345;&#36825;&#20123;&#21160;&#20316;&#38454;&#27573;&#30340;&#24037;&#20855;&#30340;&#31034;&#20363;&#12290;&#35813;&#26694;&#26550;&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;-LLM&#20132;&#20114;&#30740;&#31350;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.02822</link><description>&lt;p&gt;
&#20197;&#19971;&#20010;&#21160;&#20316;&#38454;&#27573;&#30340;&#26041;&#24335;&#35774;&#35745;&#26234;&#33021;&#20889;&#20316;&#21161;&#25163;&#30340;&#21487;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Approach Intelligent Writing Assistants Usability with Seven Stages of Action. (arXiv:2304.02822v1 [cs.HC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#35834;&#26364;&#30340;&#19971;&#20010;&#21160;&#20316;&#38454;&#27573;&#20316;&#20026;&#26234;&#33021;&#20889;&#20316;&#21161;&#25163;&#20132;&#20114;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#25903;&#25345;&#36825;&#20123;&#21160;&#20316;&#38454;&#27573;&#30340;&#24037;&#20855;&#30340;&#31034;&#20363;&#12290;&#35813;&#26694;&#26550;&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;-LLM&#20132;&#20114;&#30740;&#31350;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMS )&#20855;&#22791;&#25104;&#20026;&#20889;&#20316;&#21161;&#25163;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#27169;&#22411;&#36755;&#20986;&#30340;&#36830;&#36143;&#24615;&#21644;&#27969;&#30021;&#24615;&#12289;&#21487;&#20449;&#24230;&#12289;&#29983;&#25104;&#20869;&#23481;&#30340;&#25152;&#26377;&#26435;&#20197;&#21450;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#39044;&#27979;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#35834;&#26364;&#30340;&#19971;&#20010;&#21160;&#20316;&#38454;&#27573;&#20316;&#20026;&#26234;&#33021;&#20889;&#20316;&#21161;&#25163;&#20132;&#20114;&#35774;&#35745;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#36719;&#20214;&#25945;&#31243;&#21019;&#20316;&#30340;&#31034;&#20363;&#65292;&#35828;&#26126;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#20889;&#20316;&#20219;&#21153;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#35813;&#26694;&#26550;&#20316;&#20026;&#32508;&#21512;&#22522;&#20110;LLMS&#24037;&#20855;&#30340;&#20132;&#20114;&#35774;&#35745;&#30740;&#31350;&#30340;&#24037;&#20855;&#65292;&#24182;&#25552;&#20379;&#20102;&#25903;&#25345;&#36825;&#20123;&#21160;&#20316;&#38454;&#27573;&#30340;&#24037;&#20855;&#30340;&#31034;&#20363;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;&#20154;-LLMS&#20132;&#20114;&#30740;&#31350;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the potential of Large Language Models (LLMs) as writing assistants, they are plagued by issues like coherence and fluency of the model output, trustworthiness, ownership of the generated content, and predictability of model performance, thereby limiting their usability. In this position paper, we propose to adopt Norman's seven stages of action as a framework to approach the interaction design of intelligent writing assistants. We illustrate the framework's applicability to writing tasks by providing an example of software tutorial authoring. The paper also discusses the framework as a tool to synthesize research on the interaction design of LLM-based tools and presents examples of tools that support the stages of action. Finally, we briefly outline the potential of a framework for human-LLM interaction research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38053;&#21273;&#65306;&#29992;GPT&#35299;&#23494;&#26448;&#26009;&#31185;&#23398;&#30340;&#31192;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#20449;&#24687;&#25552;&#21462;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#38041;&#38043;&#30719;&#22826;&#38451;&#33021;&#30005;&#27744;FAIR&#25968;&#25454;&#38598;&#23545;GPT-3&#36827;&#34892;&#24494;&#35843;&#65292;&#33719;&#24471;&#20102;91.8 F1&#24471;&#20998;&#65292;&#24182;&#26356;&#26032;&#20102;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36804;&#20170;&#20026;&#27490;&#25152;&#26377;&#30456;&#20851;&#31185;&#23398;&#35770;&#25991;&#12290;&#25152;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#24050;&#34987;&#26684;&#24335;&#21270;&#21644;&#26631;&#20934;&#21270;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#30452;&#25509;&#20316;&#20026;&#21518;&#32493;&#25968;&#25454;&#20998;&#26512;&#30340;&#36755;&#20837;&#12290;&#36825;&#20010;&#29305;&#24615;&#23558;&#20351;&#26448;&#26009;&#31185;&#23398;&#23478;&#36890;&#36807;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#35780;&#35770;&#25991;&#31456;&#26469;&#24320;&#21457;&#20854;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#24182;&#33719;&#24471;&#20102;&#19982;DFT&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20687;&#26448;&#26009;&#23398;&#23478;&#19968;&#26679;&#35780;&#21028;&#26448;&#26009;&#21644;&#35774;&#35745;&#26032;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;ChatGPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#30340;&#36947;&#24503;&#32771;&#34385;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#21644;NLP&#39046;&#22495;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.02017</link><description>&lt;p&gt;
&#35299;&#38145;ChatGPT&#30340;&#28508;&#21147;&#65306;&#23545;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24212;&#29992;&#12289;&#20248;&#28857;&#12289;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#20840;&#38754;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing. (arXiv:2304.02017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;ChatGPT&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#30340;&#36947;&#24503;&#32771;&#34385;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#21644;NLP&#39046;&#22495;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#21307;&#30103;&#35786;&#26029;&#27835;&#30103;&#12290;&#23427;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#20934;&#30830;&#24615;&#20351;&#20854;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;ChatGPT&#20063;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#20854;&#20542;&#21521;&#20110;&#20135;&#29983;&#26377;&#20559;&#35265;&#30340;&#21709;&#24212;&#20197;&#21450;&#23384;&#22312;&#28508;&#22312;&#30340;&#26377;&#23475;&#35821;&#35328;&#27169;&#24335;&#12290;&#26412;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;ChatGPT&#21450;&#20854;&#24212;&#29992;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#24037;&#20855;&#26102;&#36947;&#24503;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;&#35265;&#35299;&#65292;&#20026;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21450;&#20854;&#23545;&#35270;&#35273;&#21644;NLP&#39046;&#22495;&#30340;&#24433;&#21709;&#30340;&#25345;&#32493;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a powerful tool in the field of artificial intelligence that has been widely used in various applications. ChatGPT has been applied successfully in chatbots, content generation, language translation, personalized recommendations, and medical diagnosis and treatment. Its versatility and accuracy make it a powerful tool for natural language processing (NLP). However, there are also limitations to ChatGPT, such as its tendency to produce biased responses and its potential to perpetuate harmful language patterns. This article provides a comprehensive overview of ChatGPT, its applications, advantages, and limitations. Additionally, the paper emphasizes the importance of ethical considerations when using this robust tool in real-world scenarios. Finally, This paper contributes to ongoing discussions surrounding artificial intelligence and its impact on vision and NLP domains by providing insights into prompt engineering techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;MPO&#20998;&#35299;&#20849;&#20139;&#20013;&#22830;&#24352;&#37327;&#24182;&#20445;&#25345;&#23618;&#29305;&#23450;&#30340;&#36741;&#21161;&#24352;&#37327;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16753</link><description>&lt;p&gt;
&#20351;&#29992;&#21442;&#25968;&#26377;&#25928;&#30340;&#32467;&#26500;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#33267;&#26356;&#28145;&#30340;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture. (arXiv:2303.16753v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;MPO&#20998;&#35299;&#20849;&#20139;&#20013;&#22830;&#24352;&#37327;&#24182;&#20445;&#25345;&#23618;&#29305;&#23450;&#30340;&#36741;&#21161;&#24352;&#37327;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#31639;&#23376;&#65288;MPO&#65289;&#30340;&#26356;&#20855;&#33021;&#21147;&#30340;&#21442;&#25968;&#20849;&#20139;&#26550;&#26500;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#27169;&#22411;&#28145;&#24230;&#12290;&#36890;&#36807;MPO&#20998;&#35299;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#36328;&#25152;&#26377;&#23618;&#20849;&#20139;&#20013;&#22830;&#24352;&#37327;&#20197;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#20445;&#25345;&#23618;&#29305;&#23450;&#30340;&#36741;&#21161;&#24352;&#37327;&#20197;&#22686;&#24378;&#36866;&#24212;&#24615;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a highly parameter-efficient approach to scaling pre-trained language models (PLMs) to a deeper model depth. Unlike prior work that shares all parameters or uses extra blocks, we design a more capable parameter-sharing architecture based on matrix product operator (MPO). MPO decomposition can reorganize and factorize the information of a parameter matrix into two parts: the major part that contains the major information (central tensor) and the supplementary part that only has a small proportion of parameters (auxiliary tensors). Based on such a decomposition, our architecture shares the central tensor across all layers for reducing the model size and meanwhile keeps layer-specific auxiliary tensors (also using adapters) for enhancing the adaptation flexibility. To improve the model training, we further propose a stable initialization algorithm tailored for the MPO-based architecture. Extensive experiments have demonstrated the effectiveness of our proposed mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.13988</link><description>&lt;p&gt;
&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#21033;&#29992;&#24515;&#29702;&#23398;&#26041;&#27861;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32039;&#23494;&#32467;&#21512;&#30340;&#20808;&#38155;&#12290;&#30001;&#20110;&#24555;&#36895;&#25216;&#26415;&#36827;&#27493;&#21644;&#20854;&#26497;&#39640;&#30340;&#36890;&#29992;&#24615;&#65292;&#29616;&#20170;LLM&#24050;&#32463;&#25317;&#26377;&#25968;&#30334;&#19975;&#29992;&#25143;&#65292;&#24182;&#27491;&#22788;&#20110;&#25104;&#20026;&#20027;&#35201;&#20449;&#24687;&#26816;&#32034;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#38382;&#39064;&#35299;&#20915;&#31561;&#25216;&#26415;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#23457;&#26597;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#24403;&#21069;LLM&#20013;&#20986;&#29616;&#24840;&#21152;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#23558;&#20854;&#35270;&#20026;&#21442;&#19982;&#20154;&#31867;&#24515;&#29702;&#23454;&#39564;&#30340;&#23545;&#35937;&#65292;&#20197;&#20415;&#26356;&#20026;&#20840;&#38754;&#22320;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;"&#26426;&#22120;&#24515;&#29702;&#23398;"&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#21508;&#31867;&#24515;&#29702;&#23398;&#20998;&#25903;&#22914;&#20309;&#20026;LLM&#30340;&#34892;&#20026;&#27979;&#35797;&#25552;&#20379;&#26377;&#29992;&#21442;&#32771;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25551;&#36848;&#20102;&#34892;&#20026;&#27979;&#35797;&#32467;&#26524;&#22914;&#20309;&#20026;&#26410;&#26469;&#30340;LLM&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31890;&#24230;&#30340;&#20013;&#25991;BERT&#65288;MigBERT&#65289;&#65292;&#21033;&#29992;&#21516;&#26102;&#32771;&#34385;&#23383;&#31526;&#21644;&#35789;&#27719;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20013;&#25991;PLMs&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#21508;&#31181;&#20013;&#25991;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;&#21333;&#35789;&#27604;&#23383;&#31526;&#35821;&#20041;&#26356;&#20016;&#23500;&#12290;&#25968;&#23383;&#20063;&#26174;&#31034;&#20102;MigBERT&#21487;&#20197;&#22312;&#26085;&#35821;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13065</link><description>&lt;p&gt;
&#21033;&#29992;&#35299;&#32806;&#34920;&#31034;&#30340;&#26816;&#32034;&#22686;&#24378;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Classification with Decoupled Representation. (arXiv:2303.13065v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31890;&#24230;&#30340;&#20013;&#25991;BERT&#65288;MigBERT&#65289;&#65292;&#21033;&#29992;&#21516;&#26102;&#32771;&#34385;&#23383;&#31526;&#21644;&#35789;&#27719;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20013;&#25991;PLMs&#30340;&#34920;&#29616;&#65292;&#24182;&#22312;&#21508;&#31181;&#20013;&#25991;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;&#21333;&#35789;&#27604;&#23383;&#31526;&#35821;&#20041;&#26356;&#20016;&#23500;&#12290;&#25968;&#23383;&#20063;&#26174;&#31034;&#20102;MigBERT&#21487;&#20197;&#22312;&#26085;&#35821;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#22823;&#22810;&#25968;&#20013;&#25991;PLM&#23558;&#36755;&#20837;&#25991;&#26412;&#35270;&#20026;&#23383;&#31526;&#24207;&#21015;&#65292;&#24182;&#23436;&#20840;&#24573;&#30053;&#35789;&#20449;&#24687;&#12290;&#34429;&#28982;&#25972;&#35789;&#23631;&#34109;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#35789;&#27719;&#20013;&#30340;&#35821;&#20041;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20013;&#25991;PLM&#30340;&#20998;&#35789;&#31890;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#23383;&#31526;&#21644;&#35789;&#27719;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#31890;&#24230;&#30340;&#20013;&#25991;BERT&#65288;MigBERT&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29992;&#20110;&#23398;&#20064;&#23383;&#31526;&#21644;&#21333;&#35789;&#32423;&#34920;&#31034;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#20013;&#25991;NLP&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#29616;&#26377;PLM&#20197;&#21450;&#25152;&#25552;&#20986;&#30340;MigBERT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MigBERT&#22312;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#19978;&#22343;&#23454;&#29616;&#20102;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21333;&#35789;&#27604;&#23383;&#31526;&#35821;&#20041;&#26356;&#20016;&#23500;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MigBERT&#20063;&#21487;&#20197;&#19982;&#26085;&#35821;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) have shown marvelous improvements across various NLP tasks. Most Chinese PLMs simply treat an input text as a sequence of characters, and completely ignore word information. Although Whole Word Masking can alleviate this, the semantics in words is still not well represented. In this paper, we revisit the segmentation granularity of Chinese PLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both characters and words. To achieve this, we design objective functions for learning both character and word-level representations. We conduct extensive experiments on various Chinese NLP tasks to evaluate existing PLMs as well as the proposed MigBERT. Experimental results show that MigBERT achieves new SOTA performance on all these tasks. Further analysis demonstrates that words are semantically richer than characters. More interestingly, we show that MigBERT also works with Japanese. Our code has been released here~\footnote{\url{https://g
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992; ChatGPT &#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#30340;&#28508;&#21147;&#65292;&#20294;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#25928;&#26524;&#27424;&#20339;&#65292;&#21516;&#26102;&#19978;&#20256;&#24739;&#32773;&#20449;&#24687;&#20063;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992; ChatGPT &#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#22823;&#37327;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.04360</link><description>&lt;p&gt;
LLM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#23545;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#26377;&#24110;&#21161;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Synthetic Data Generation of LLMs Help Clinical Text Mining?. (arXiv:2303.04360v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04360
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992; ChatGPT &#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#30340;&#28508;&#21147;&#65292;&#20294;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#25928;&#26524;&#27424;&#20339;&#65292;&#21516;&#26102;&#19978;&#20256;&#24739;&#32773;&#20449;&#24687;&#20063;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992; ChatGPT &#29983;&#25104;&#24102;&#26631;&#31614;&#30340;&#22823;&#37327;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#25512;&#21160;&#20102;&#35832;&#22914;OpenAI&#30340;ChatGPT&#20043;&#31867;&#30340;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#38382;&#31572;&#12289;&#35770;&#25991;&#20889;&#20316;&#21644;&#20195;&#30721;&#29983;&#25104;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#20173;&#19981;&#30830;&#23450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;ChatGPT&#20174;&#38750;&#32467;&#26500;&#21270;&#20581;&#24247;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#29983;&#29289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#65292;&#25506;&#35752;ChatGPT&#22312;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;ChatGPT&#36827;&#34892;&#36825;&#20123;&#20219;&#21153;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#24341;&#21457;&#19982;&#23558;&#24739;&#32773;&#20449;&#24687;&#19978;&#20256;&#21040;ChatGPT API&#26377;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;ChatGPT&#29983;&#25104;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#24102;&#26631;&#31614;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#26377;&#23475;&#27573;&#23376;&#20013;&#23454;&#20307;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#25506;&#27979;&#27599;&#20010;&#23454;&#20307;&#26159;&#21542;&#26159;&#27573;&#23376;&#20013;&#30340;&#33521;&#38596;&#12289;&#24694;&#26829;&#25110;&#21463;&#23475;&#32773;&#65292;&#20351;&#29992;HVVMemes&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;VECTOR&#27169;&#22411;&#26469;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2301.11219</link><description>&lt;p&gt;
&#35748;&#30693;&#26377;&#23475;&#27573;&#23376;&#20013;&#19981;&#21516;&#23454;&#20307;&#25152;&#25198;&#28436;&#30340;&#35282;&#33394;&#65306;&#35841;&#26159;&#33521;&#38596;&#65292;&#35841;&#26159;&#24694;&#26829;&#65292;&#35841;&#26159;&#21463;&#23475;&#32773;&#65311;
&lt;/p&gt;
&lt;p&gt;
Characterizing the Entities in Harmful Memes: Who is the Hero, the Villain, the Victim?. (arXiv:2301.11219v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#26377;&#23475;&#27573;&#23376;&#20013;&#23454;&#20307;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#25506;&#27979;&#27599;&#20010;&#23454;&#20307;&#26159;&#21542;&#26159;&#27573;&#23376;&#20013;&#30340;&#33521;&#38596;&#12289;&#24694;&#26829;&#25110;&#21463;&#23475;&#32773;&#65292;&#20351;&#29992;HVVMemes&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;VECTOR&#27169;&#22411;&#26469;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27573;&#23376;&#36890;&#36807;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#32467;&#21512;&#30340;&#26041;&#24335;&#21487;&#20197;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24433;&#21709;&#20154;&#20204;&#30340;&#35266;&#28857;&#12290;&#30001;&#20110;&#27573;&#23376;&#21487;&#20197;&#36805;&#36895;&#20256;&#25773;&#65292;&#22240;&#27492;&#25512;&#26029;&#27573;&#23376;&#30340;&#24847;&#22270;&#21644;&#28508;&#22312;&#30340;&#26377;&#23475;&#24615;&#24182;&#37319;&#21462;&#21450;&#26102;&#30340;&#25514;&#26045;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#29702;&#35299;&#27573;&#23376;&#20013;&#25152;&#24341;&#29992;&#30340;&#23454;&#20307;&#21644;&#21051;&#30011;&#27599;&#20010;&#23454;&#20307;&#30340;&#35282;&#33394;&#26041;&#38754;&#65292;&#24120;&#24120;&#20250;&#20986;&#29616;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#27573;&#23376;&#26159;&#21542;&#32654;&#21270;&#12289;&#27745;&#21517;&#21270;&#25110;&#20351;&#27599;&#20010;&#23454;&#20307;&#25104;&#20026;&#21463;&#23475;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#23475;&#27573;&#23376;&#20013;&#23454;&#20307;&#30340;&#35282;&#33394;&#35782;&#21035;&#20219;&#21153;&#65292;&#21363;&#26816;&#27979;&#27573;&#23376;&#20013;&#30340;&#8220;&#33521;&#38596;&#8221;&#12289;&#8220;&#24694;&#26829;&#8221;&#21644;&#8220;&#21463;&#23475;&#32773;&#8221;&#65292;&#22914;&#26524;&#26377;&#30340;&#35805;&#12290;&#25105;&#20204;&#20351;&#29992;HVVMemes&#8212;&#8212;&#26368;&#36817;&#20316;&#20026;CONSTRAINT@ACL-2022&#20849;&#20139;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#21457;&#24067;&#30340;&#26377;&#20851;&#32654;&#22269;&#25919;&#27835;&#21644;Covid-19&#27573;&#23376;&#30340;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;&#20102;&#27573;&#23376;&#12289;&#24341;&#29992;&#30340;&#23454;&#20307;&#21450;&#20854;&#30456;&#20851;&#35282;&#33394;&#65306;&#33521;&#38596;&#12289;&#24694;&#26829;&#12289;&#21463;&#23475;&#32773;&#21644;&#20854;&#20182;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;VECTOR(&#35270;&#35273;-&#35821;&#20041;&#35282;&#33394;&#26816;&#27979;)&#27169;&#22411;&#26469;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes can sway people's opinions over social media as they combine visual and textual information in an easy-to-consume manner. Since memes instantly turn viral, it becomes crucial to infer their intent and potentially associated harmfulness to take timely measures as needed. A common problem associated with meme comprehension lies in detecting the entities referenced and characterizing the role of each of these entities. Here, we aim to understand whether the meme glorifies, vilifies, or victimizes each entity it refers to. To this end, we address the task of role identification of entities in harmful memes, i.e., detecting who is the 'hero', the 'villain', and the 'victim' in the meme, if any. We utilize HVVMemes - a memes dataset on US Politics and Covid-19 memes, released recently as part of the CONSTRAINT@ACL-2022 shared-task. It contains memes, entities referenced, and their associated roles: hero, villain, victim, and other. We further design VECTOR (Visual-semantic role dEteCTo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;SQL&#26597;&#35810;&#12289;&#29702;&#35299;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#21450;&#21306;&#20998;&#26377;&#26080;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.07695</link><description>&lt;p&gt;
EHRSQL&#65306;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#30340;&#23454;&#29992;&#25991;&#26412;&#36716;SQL&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#30340;&#25991;&#26412;&#36716;SQL&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19968;&#31995;&#21015;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;SQL&#26597;&#35810;&#12289;&#29702;&#35299;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#21450;&#21306;&#20998;&#26377;&#26080;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#12290;&#23545;&#35805;&#26159;&#30001;222&#20010;&#21307;&#38498;&#24037;&#20316;&#20154;&#21592;&#21253;&#25324;&#21307;&#29983;&#12289;&#25252;&#22763;&#12289;&#20445;&#38505;&#23457;&#26597;&#21644;&#20581;&#24247;&#26723;&#26696;&#22242;&#38431;&#31561;&#25163;&#26426;&#32780;&#26469;&#12290;&#20026;&#20102;&#26500;&#24314;&#20851;&#20110;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#30340;QA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#19968;&#25152;&#22823;&#23398;&#21307;&#38498;&#36827;&#34892;&#20102;&#19968;&#27425;&#27665;&#35843;&#24182;&#21046;&#20316;&#20102;&#27169;&#26495;&#35805;&#26415;&#20197;&#21019;&#24314;&#31181;&#23376;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25163;&#21160;&#23558;&#23427;&#20204;&#38142;&#25509;&#21040;&#20004;&#20010;&#24320;&#28304;&#30340;EHR&#25968;&#25454;&#24211;&#65288;MIMIC-III&#21644;eICU&#65289;&#20013;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;&#27665;&#24847;&#35843;&#26597;&#30340;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#24335;&#21644;&#26410;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#27169;&#22411;&#38656;&#35201; 1&#65289;&#29983;&#25104;&#21453;&#26144;&#21307;&#38498;&#20013;&#21508;&#31181;&#38656;&#27714;&#30340;SQL&#26597;&#35810;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#26816;&#32034;&#21644;&#22797;&#26434;&#30340;&#25805;&#20316;&#65292;&#22914;&#35745;&#31639;&#29983;&#23384;&#29575;&#65292;2&#65289;&#29702;&#35299;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#24335;&#20197;&#22238;&#31572;&#19982;&#26102;&#38388;&#25935;&#24863;&#30340;&#21307;&#30103;&#38382;&#39064;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;3&#65289;&#26681;&#25454;&#39044;&#27979;&#21306;&#20998;&#32473;&#23450;&#38382;&#39064;&#26159;&#21487;&#22238;&#31572;&#36824;&#26159;&#19981;&#21487;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff, including physicians, nurses, insurance review and health records teams, and more. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and templatized the responses to create seed questions. Then, we manually linked them to two open-source EHR databases, MIMIC-III and eICU, and included them with various time expressions and held-out unanswerable questions in the dataset, which were all collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable based on the predicti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#29992;&#20110;&#30740;&#31350;&#25512;&#29305;&#19978; COVID-19 &#30123;&#33495;&#29369;&#35947;&#30340;&#25968;&#25454;&#38598;&#65292;&#30123;&#33495;&#29369;&#35947;&#19968;&#30452;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#38382;&#39064;&#65292;&#20102;&#35299;&#20844;&#20247;&#23545; COVID-19 &#30123;&#33495;&#29369;&#35947;&#30340;&#21407;&#22240;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2301.06660</link><description>&lt;p&gt;
VaxxHesitancy: &#19968;&#20221;&#29992;&#20110;&#30740;&#31350;&#25512;&#29305;&#19978;&#23545; COVID-19 &#30123;&#33495;&#29369;&#35947;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
VaxxHesitancy: A Dataset for Studying Hesitancy Towards COVID-19 Vaccination on Twitter. (arXiv:2301.06660v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#29992;&#20110;&#30740;&#31350;&#25512;&#29305;&#19978; COVID-19 &#30123;&#33495;&#29369;&#35947;&#30340;&#25968;&#25454;&#38598;&#65292;&#30123;&#33495;&#29369;&#35947;&#19968;&#30452;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#38382;&#39064;&#65292;&#20102;&#35299;&#20844;&#20247;&#23545; COVID-19 &#30123;&#33495;&#29369;&#35947;&#30340;&#21407;&#22240;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#29369;&#35947;&#19968;&#30452;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#38382;&#39064;&#65292;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#20154;&#20204;&#24320;&#22987;&#22312;&#32593;&#19978;&#34920;&#36798;&#20182;&#20204;&#23545;&#30123;&#33495;&#30340;&#25285;&#24551;&#65292;&#21516;&#26102;&#20063;&#19982;&#25903;&#25345;&#21644;&#21453;&#23545;&#30123;&#33495;&#30340;&#20154;&#21457;&#34920;&#20869;&#23481;&#12290;&#33258;&#20174;&#31532;&#19968;&#27425;&#25552;&#21040; COVID-19 &#30123;&#33495;&#20197;&#26469;&#65292;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#23601;&#22312;&#21457;&#24067;&#20851;&#20110;&#20182;&#20204;&#30340;&#25285;&#24551;&#21644;&#25903;&#25345;&#30123;&#33495;&#26377;&#25928;&#24615;&#30340;&#20869;&#23481;&#12290;&#20102;&#35299;&#20844;&#20247;&#23545; COVID-19 &#30123;&#33495;&#29369;&#35947;&#30340;&#21407;&#22240;&#38750;&#24120;&#37325;&#35201;&#65292;&#23545;&#20110;&#38656;&#35201;&#21046;&#23450;&#34892;&#21160;&#20197;&#26356;&#22909;&#22320;&#21578;&#30693;&#20154;&#32676;&#20197;&#22686;&#21152;&#30123;&#33495;&#25509;&#31181;&#29575;&#30340;&#25919;&#31574;&#21046;&#23450;&#32773;&#26469;&#35828;&#23588;&#20854;&#22914;&#27492;&#12290;&#22312; COVID-19 &#30340;&#24773;&#20917;&#19979;&#65292;&#30123;&#33495;&#24555;&#36895;&#24320;&#21457;&#19982;&#21453;&#30123;&#33495;&#34394;&#20551;&#20449;&#24687;&#30340;&#22686;&#38271;&#30456;&#20276;&#65292;&#33258;&#21160;&#26816;&#27979;&#20844;&#27665;&#23545;&#30123;&#33495;&#25509;&#31181;&#30340;&#24577;&#24230;&#25104;&#20026;&#24517;&#38656;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#20219;&#21153;&#65292;&#38656;&#35201;&#25968;&#25454;&#20998;&#26512;&#25165;&#33021;&#33719;&#24471;&#26356;&#22810;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine hesitancy has been a common concern, probably since vaccines were created and, with the popularisation of social media, people started to express their concerns about vaccines online alongside those posting pro- and anti-vaccine content. Predictably, since the first mentions of a COVID-19 vaccine, social media users posted about their fears and concerns or about their support and belief into the effectiveness of these rapidly developing vaccines. Identifying and understanding the reasons behind public hesitancy towards COVID-19 vaccines is important for policy markers that need to develop actions to better inform the population with the aim of increasing vaccine take-up. In the case of COVID-19, where the fast development of the vaccines was mirrored closely by growth in anti-vaxx disinformation, automatic means of detecting citizen attitudes towards vaccination became necessary. This is an important computational social sciences task that requires data analysis in order to gai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.15629</link><description>&lt;p&gt;
&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#65306;&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20219;&#21153;&#39640;&#25928;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#36890;&#29992;&#22411;&#26234;&#33021;&#20307;&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#24456;&#22256;&#38590;&#65292;&#38656;&#35201;&#22788;&#29702;&#39640;&#32500;&#36755;&#20837;&#65288;&#31354;&#38388;&#65289;&#12289;&#38271;&#26102;&#38388;&#36328;&#24230;&#65288;&#26102;&#38388;&#65289;&#21644;&#22810;&#20010;&#26032;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#32467;&#26500;&#26041;&#38754;&#30340;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#27839;&#30528;&#20854;&#20013;&#19968;&#20010;&#25110;&#20004;&#20010;&#32500;&#24230;&#25552;&#39640;&#25193;&#23637;&#24615;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65288;LCD&#65289;&#26469;&#24212;&#23545;&#36825;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#12289;&#29366;&#24577;&#21644;&#20219;&#21153;&#31354;&#38388;&#32500;&#24230;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;CALVIN&#35821;&#35328;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#23558;LCD&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LCD&#22312;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#21333;&#20219;&#21153;&#25104;&#21151;&#29575;&#65288;SR&#65289;&#20026;88.7%&#65292;&#36828;&#39640;&#20110;&#20197;&#21069;&#30340;&#26368;&#20339;&#25104;&#32489;82.6%&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#30693;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#26410;&#26469;&#20449;&#24687;&#35745;&#31639;&#29702;&#24819;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#36827;&#19968;&#27493;&#35268;&#33539;&#20559;&#31227;&#30340;&#27880;&#24847;&#21147;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22270;&#20687;&#25551;&#36848;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.10914</link><description>&lt;p&gt;
&#20808;&#30693;&#24335;&#27880;&#24847;&#21147;&#65306;&#22522;&#20110;&#26410;&#26469;&#20449;&#24687;&#30340;&#22270;&#20687;&#25551;&#36848;&#20013;&#30340;&#27880;&#24847;&#21147;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prophet Attention: Predicting Attention with Future Attention for Image Captioning. (arXiv:2210.10914v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#30693;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#26410;&#26469;&#20449;&#24687;&#35745;&#31639;&#29702;&#24819;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#36827;&#19968;&#27493;&#35268;&#33539;&#20559;&#31227;&#30340;&#27880;&#24847;&#21147;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22270;&#20687;&#25551;&#36848;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23588;&#20854;&#26159;&#22312;&#22270;&#20687;&#25551;&#36848;&#20013;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#23558;&#27491;&#30830;&#30340;&#22270;&#20687;&#21306;&#22495;&#19982;&#36866;&#24403;&#30340;&#29983;&#25104;&#35789;&#35821;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#24120;&#20351;&#29992;&#24403;&#21069;&#36755;&#20837;&#30340;&#38544;&#34255;&#29366;&#24577;&#26469;&#20851;&#27880;&#22270;&#20687;&#21306;&#22495;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#27880;&#24847;&#21147;&#27169;&#22411;&#23384;&#22312;&#8220;&#20559;&#31163;&#28966;&#28857;&#8221;&#30340;&#38382;&#39064;&#65292;&#23427;&#20204;&#26681;&#25454;&#20808;&#21069;&#30340;&#21333;&#35789;&#35745;&#31639;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#32780;&#19981;&#26159;&#21363;&#23558;&#29983;&#25104;&#30340;&#21333;&#35789;&#65292;&#36825;&#20250;&#24433;&#21709;&#22320;&#38754;&#21644;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20808;&#30693;&#24335;&#27880;&#24847;&#21147;&#8221;&#65292;&#31867;&#20284;&#20110;&#33258;&#25105;&#30417;&#30563;&#30340;&#24418;&#24335;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#35813;&#27169;&#22359;&#21033;&#29992;&#26410;&#26469;&#20449;&#24687;&#26469;&#35745;&#31639;&#8220;&#29702;&#24819;&#8221;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#36827;&#19968;&#27493;&#35268;&#33539;&#8220;&#20559;&#31227;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#22270;&#20687;&#21306;&#22495;&#19982;&#26356;&#31934;&#30830;&#30340;&#21333;&#35789;&#30456;&#20851;&#32852;&#65292;&#29983;&#25104;&#30340;&#22270;&#29255;&#26631;&#39064;&#26356;&#21152;&#20934;&#30830;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, attention based models have been used extensively in many sequence-to-sequence learning systems. Especially for image captioning, the attention based models are expected to ground correct image regions with proper generated words. However, for each time step in the decoding process, the attention based models usually use the hidden state of the current input to attend to the image regions. Under this setting, these attention models have a "deviated focus" problem that they calculate the attention weights based on previous words instead of the one to be generated, impairing the performance of both grounding and captioning. In this paper, we propose the Prophet Attention, similar to the form of self-supervision. In the training stage, this module utilizes the future information to calculate the "ideal" attention weights towards image regions. These calculated "ideal" weights are further used to regularize the "deviated" attention. In this manner, image regions are grounded with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2208.07316</link><description>&lt;p&gt;
MENLI: &#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
MENLI: Robust Evaluation Metrics from Natural Language Inference. (arXiv:2208.07316v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#26356;&#22909;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#19988;&#26356;&#21152;&#40065;&#26834;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#30456;&#32467;&#21512;&#21487;&#20197;&#20351;&#35780;&#20272;&#25928;&#26524;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#34987;&#25552;&#20986;&#30340;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26131;&#21463;&#21040;&#23545;&#20449;&#24687;&#27491;&#30830;&#24615;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#37096;&#20998;&#21407;&#22240;&#26159;&#27492;&#31867;&#27169;&#22411;&#26159;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36825;&#31181;&#25351;&#26631;&#26356;&#36866;&#21512;&#24314;&#27169;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26694;&#26550;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#27604;&#26368;&#36817;&#30340;BERT&#22522;&#30784;&#25351;&#26631;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;NLI&#22522;&#30784;&#25351;&#26631;&#20248;&#20110;&#29616;&#26377;&#30340;&#25688;&#35201;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#20302;&#20110;SOTA MT&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#25351;&#26631;&#19982;&#25105;&#20204;&#30340;NLI&#25351;&#26631;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#26082;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65288;15&#65285;-30&#65285;&#65289;&#65292;&#21448;&#33719;&#24471;&#20102;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#26356;&#39640;&#30340;&#36136;&#37327;&#25351;&#26631;&#65288;+5&#65285;&#33267;30&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%-30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65288;CMCL&#65289;&#65292;&#36890;&#36807;&#27169;&#25311;&#25918;&#23556;&#23398;&#23478;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#36880;&#27493;&#20248;&#21270;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;CMCL&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.14579</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#35838;&#31243;&#23398;&#20064;&#29992;&#20110;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Competence-based Multimodal Curriculum Learning for Medical Report Generation. (arXiv:2206.14579v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65288;CMCL&#65289;&#65292;&#36890;&#36807;&#27169;&#25311;&#25918;&#23556;&#23398;&#23478;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#36880;&#27493;&#20248;&#21270;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;CMCL&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#20219;&#21153;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#20854;&#30446;&#26631;&#26159;&#29983;&#25104;&#21307;&#23398;&#22270;&#20687;&#30340;&#36830;&#36143;&#25551;&#36848;&#12290;&#19982;&#19968;&#33324;&#30340;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#19981;&#21516;&#65292;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#23545;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#27169;&#22411;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159; 1) &#20005;&#37325;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644; 2) &#26377;&#38480;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#25968;&#25454;&#20559;&#24046;&#24182;&#20805;&#20998;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65288;CMCL&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;CMCL &#27169;&#25311;&#20102;&#25918;&#23556;&#23398;&#23478;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#36880;&#27493;&#20248;&#21270;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;CMCL &#20272;&#35745;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#30340;&#38590;&#24230;&#24182;&#35780;&#20272;&#24403;&#21069;&#27169;&#22411;&#30340;&#33021;&#21147;; &#20854;&#27425;&#65292;CMCL &#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#35757;&#32451;&#23454;&#20363;&#32452;&#21512;&#20197;&#32771;&#34385;&#24403;&#21069;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36845;&#20195;&#19978;&#36848;&#20004;&#20010;&#27493;&#39588;&#65292;CMCL &#21487;&#20197;&#36880;&#28176;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#20844;&#20849;&#30340;IU-Xray&#21644;MIMIC-CXR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CMCL &#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#32988;&#36807;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner. Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; Secondly, CMCL selects the most suitable batch of training instances considering current model competence. By iterating above two steps, CMCL can gradually improve the model's performance. The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMC
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27719;&#24635;&#20102;65&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;45&#20010;&#33521;&#25991;&#25968;&#25454;&#38598;&#21644;20&#20010;&#20854;&#20182;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#33258;&#20027;ABSA&#31995;&#32479;&#30340;&#25968;&#25454;&#24211;&#12290;</title><link>http://arxiv.org/abs/2204.05232</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27719;&#24635;&#20102;65&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;45&#20010;&#33521;&#25991;&#25968;&#25454;&#38598;&#21644;20&#20010;&#20854;&#20182;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#33258;&#20027;ABSA&#31995;&#32479;&#30340;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;(ABSA)&#26159;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#20998;&#26512;&#29992;&#25143;&#29983;&#25104;&#30340;&#35780;&#35770;&#20197;&#30830;&#23450;&#65306;a)&#27491;&#22312;&#23457;&#26597;&#30340;&#30446;&#26631;&#23454;&#20307;&#65292;b)&#23646;&#20110;&#21738;&#20010;&#39640;&#32423;&#26041;&#38754;&#65292;c)&#23545;&#30446;&#26631;&#21644;&#26041;&#38754;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;ABSA&#30340;&#20247;&#22810;&#20294;&#20998;&#25955;&#30340;&#35821;&#26009;&#24211;&#20351;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#24555;&#36895;&#30830;&#23450;&#26368;&#36866;&#21512;&#29305;&#23450;ABSA&#23376;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#33258;&#20027;ABSA&#31995;&#32479;&#30340;&#25968;&#25454;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;ABSA&#21644;&#20854;&#23376;&#20219;&#21153;&#30340;&#20027;&#35201;&#35821;&#26009;&#24211;&#27010;&#36848;&#65292;&#24182;&#24378;&#35843;&#30740;&#31350;&#20154;&#21592;&#22312;&#36873;&#25321;&#35821;&#26009;&#24211;&#26102;&#24212;&#32771;&#34385;&#30340;&#20960;&#20010;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#25910;&#38598;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#24182;&#20026;&#26410;&#26469;&#35821;&#26009;&#24211;&#21019;&#24314;&#25552;&#20986;&#24314;&#35758;&#12290;&#26412;&#35843;&#26597;&#23457;&#26680;&#20102;65&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;25&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;45&#20010;&#33521;&#35821;&#21644;20&#20010;&#20854;&#20182;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis (ABSA) is a natural language processing problem that requires analyzing user-generated reviews to determine: a) The target entity being reviewed, b) The high-level aspect to which it belongs, and c) The sentiment expressed toward the targets and the aspects. Numerous yet scattered corpora for ABSA make it difficult for researchers to identify corpora best suited for a specific ABSA subtask quickly. This study aims to present a database of corpora that can be used to train and assess autonomous ABSA systems. Additionally, we provide an overview of the major corpora for ABSA and its subtasks and highlight several features that researchers should consider when selecting a corpus. Finally, we discuss the advantages and disadvantages of current collection approaches and make recommendations for future corpora creation. This survey examines 65 publicly available ABSA datasets covering over 25 domains, including 45 English and 20 other languages datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23545;&#27604;&#27880;&#24847;&#21147;&#65288;CA&#65289;&#27169;&#22411;&#26469;&#20934;&#30830;&#25429;&#25417;&#21644;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#33016;&#37096;X&#20809;&#22270;&#20687;&#25551;&#36848;&#20013;&#30340;&#24322;&#24120;&#21306;&#22495;&#65292;&#35813;&#27169;&#22411;&#23558;&#24403;&#21069;&#36755;&#20837;&#22270;&#20687;&#19982;&#27491;&#24120;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#20197;&#25552;&#21462;&#23545;&#27604;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.06965</link><description>&lt;p&gt;
&#33258;&#21160;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#30340;&#23545;&#27604;&#27880;&#24847;&#21147;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23545;&#27604;&#27880;&#24847;&#21147;&#65288;CA&#65289;&#27169;&#22411;&#26469;&#20934;&#30830;&#25429;&#25417;&#21644;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#33016;&#37096;X&#20809;&#22270;&#20687;&#25551;&#36848;&#20013;&#30340;&#24322;&#24120;&#21306;&#22495;&#65292;&#35813;&#27169;&#22411;&#23558;&#24403;&#21069;&#36755;&#20837;&#22270;&#20687;&#19982;&#27491;&#24120;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#20197;&#25552;&#21462;&#23545;&#27604;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33258;&#21160;&#29983;&#25104;&#32473;&#23450;&#33016;&#37096;X&#20809;&#22270;&#20687;&#25551;&#36848;&#30340;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#33016;&#37096;X&#20809;&#25253;&#21578;&#29983;&#25104;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#20934;&#30830;&#25429;&#25417;&#21644;&#25551;&#36848;&#24322;&#24120;&#21306;&#22495;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#27491;&#24120;&#21306;&#22495;&#21344;&#25454;&#25972;&#20010;&#33016;&#37096;X&#20809;&#22270;&#20687;&#30340;&#20027;&#23548;&#22320;&#20301;&#65292;&#24182;&#19988;&#36825;&#20123;&#27491;&#24120;&#21306;&#22495;&#30340;&#25551;&#36848;&#21344;&#25454;&#20102;&#26368;&#32456;&#25253;&#21578;&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#30001;&#20110;&#36825;&#31181;&#25968;&#25454;&#20559;&#24046;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#20851;&#27880;&#24322;&#24120;&#21306;&#22495;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#25551;&#36848;&#24322;&#24120;&#21306;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#27880;&#24847;&#21147;&#65288;CA&#65289;&#27169;&#22411;&#12290;&#35813;CA&#27169;&#22411;&#19981;&#20165;&#20851;&#27880;&#24403;&#21069;&#36755;&#20837;&#22270;&#20687;&#65292;&#36824;&#23558;&#20854;&#19982;&#27491;&#24120;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#25552;&#21462;&#23545;&#27604;&#20449;&#24687;&#12290;&#33719;&#21462;&#30340;&#23545;&#27604;&#20449;&#24687;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#24322;&#24120;&#21306;&#22495;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#26681;&#25454;&#22312;&#20844;&#20849;IU-X&#20809;&#21644;MIMIC-CXR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#23558;&#25105;&#20204;&#30340;CA&#32435;&#20837;&#22810;&#31181;&#22522;&#32447;&#27169;&#22411;&#20013;&#65292;&#20854;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, chest X-ray report generation, which aims to automatically generate descriptions of given chest X-ray images, has received growing research interests. The key challenge of chest X-ray report generation is to accurately capture and describe the abnormal regions. In most cases, the normal regions dominate the entire chest X-ray image, and the corresponding descriptions of these normal regions dominate the final report. Due to such data bias, learning-based models may fail to attend to abnormal regions. In this work, to effectively capture and describe abnormal regions, we propose the Contrastive Attention (CA) model. Instead of solely focusing on the current input image, the CA model compares the current input image with normal images to distill the contrastive information. The acquired contrastive information can better represent the visual features of abnormal regions. According to the experiments on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into severa
&lt;/p&gt;</description></item></channel></rss>