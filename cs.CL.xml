<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>RECOMP&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#26816;&#32034;&#36741;&#21161;LMs&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#20026;&#25688;&#35201;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20943;&#36731;LMs&#22312;&#38271;&#25991;&#26723;&#20013;&#35782;&#21035;&#30456;&#20851;&#20449;&#24687;&#30340;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2310.04408</link><description>&lt;p&gt;
RECOMP:&#36890;&#36807;&#21387;&#32553;&#21644;&#36873;&#25321;&#24615;&#22686;&#24378;&#25913;&#21892;&#26816;&#32034;&#36741;&#21161;LMs
&lt;/p&gt;
&lt;p&gt;
RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation. (arXiv:2310.04408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04408
&lt;/p&gt;
&lt;p&gt;
RECOMP&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#26816;&#32034;&#36741;&#21161;LMs&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#20026;&#25688;&#35201;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20943;&#36731;LMs&#22312;&#38271;&#25991;&#26723;&#20013;&#35782;&#21035;&#30456;&#20851;&#20449;&#24687;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#26029;&#26102;&#65292;&#23558;&#25991;&#26723;&#26816;&#32034;&#24182;&#22312;&#19978;&#19979;&#25991;&#20013;&#21069;&#32622;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25991;&#26723;&#36890;&#24120;&#21253;&#21547;&#25968;&#30334;&#20010;&#35789;&#65292;&#20351;&#25512;&#26029;&#36807;&#31243;&#26356;&#21152;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#21387;&#32553;&#20026;&#25991;&#26412;&#25688;&#35201;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20943;&#36731;LMs&#22312;&#38271;&#25991;&#26723;&#20013;&#35782;&#21035;&#30456;&#20851;&#20449;&#24687;&#30340;&#36127;&#25285;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21387;&#32553;&#22120;&#65306;&#19968;&#31181;&#26159;&#20174;&#26816;&#32034;&#25991;&#26723;&#20013;&#36873;&#25321;&#26377;&#29992;&#21477;&#23376;&#30340;&#25277;&#21462;&#24335;&#21387;&#32553;&#22120;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#32508;&#21512;&#22810;&#20010;&#25991;&#26723;&#30340;&#20449;&#24687;&#29983;&#25104;&#25688;&#35201;&#30340;&#29983;&#25104;&#24335;&#21387;&#32553;&#22120;&#12290;&#36825;&#20004;&#31181;&#21387;&#32553;&#22120;&#22312;&#23558;&#29983;&#25104;&#30340;&#25688;&#35201;&#21069;&#32622;&#21040;LMs&#30340;&#36755;&#20837;&#26102;&#34987;&#35757;&#32451;&#20197;&#25913;&#21892;LMs&#22312;&#26368;&#32456;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25688;&#35201;&#31616;&#27905;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieving documents and prepending them in-context at inference time improves performance of language model (LMs) on a wide range of tasks. However, these documents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieves the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs' performance on end tasks when the generated summaries are prepended to the LMs' input, while keeping the summary concise.If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can retur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;Neural PG-RANK&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26816;&#32034;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#12289;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.04407</link><description>&lt;p&gt;
&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Policy-Gradient Training of Language Models for Ranking. (arXiv:2310.04407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;Neural PG-RANK&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26816;&#32034;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#12289;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#26816;&#32034;&#22312;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#21040;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32842;&#22825;&#24335;&#32593;&#39029;&#25628;&#32034;&#21040;&#38382;&#31572;&#31995;&#32479;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#36807;&#20856;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#22522;&#20110;LLM&#30340;&#26816;&#32034;&#22120;&#38656;&#35201;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21253;&#25324;&#36873;&#25321;&#22256;&#38590;&#30340;&#36127;&#26679;&#26412;&#21644;&#20351;&#29992;&#39069;&#22806;&#30340;&#30417;&#30563;&#20316;&#20026;&#23398;&#20064;&#20449;&#21495;&#12290;&#36825;&#31181;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#21407;&#22240;&#26159;&#23545;&#27604;&#25439;&#22833;&#26412;&#36523;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#19981;&#33021;&#30452;&#25509;&#20248;&#21270;&#22788;&#29702;&#27969;&#31243;&#26411;&#31471;&#20915;&#31574;&#36136;&#37327;&#30340;&#19979;&#28216;&#25351;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;PG-RANK&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;LLM&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23398;&#20064;&#25490;&#24207;&#12290;&#31070;&#32463;PG-RANK&#20026;&#26816;&#32034;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#20316;&#20026;&#26356;&#22823;&#30340;&#20915;&#31574;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text retrieval plays a crucial role in incorporating factual knowledge for decision making into language processing pipelines, ranging from chat-based web search to question answering systems. Current state-of-the-art text retrieval models leverage pre-trained large language models (LLMs) to achieve competitive performance, but training LLM-based retrievers via typical contrastive losses requires intricate heuristics, including selecting hard negatives and using additional supervision as learning signals. This reliance on heuristics stems from the fact that the contrastive loss itself is heuristic and does not directly optimize the downstream metrics of decision quality at the end of the processing pipeline. To address this issue, we introduce Neural PG-RANK, a novel training algorithm that learns to rank by instantiating a LLM as a Plackett-Luce ranking policy. Neural PG-RANK provides a principled method for end-to-end training of retrieval models as part of larger decision systems vi
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.04406</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#32479;&#19968;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#12289;&#34892;&#21160;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04406
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19968;&#31995;&#21015;&#20915;&#31574;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#34892;&#21160;&#36807;&#31243;&#65292;&#24182;&#26410;&#33021;&#24191;&#27867;&#37096;&#32626;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LATS&#65288;&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;LLMs&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#30456;&#20114;&#21327;&#21516;&#12290;LATS&#20511;&#37492;&#20102;&#27169;&#22411;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#24605;&#24819;&#65292;&#23558;LLMs&#29992;&#20316;&#20195;&#29702;&#12289;&#20215;&#20540;&#20989;&#25968;&#21644;&#20248;&#21270;&#22120;&#65292;&#37325;&#26032;&#21033;&#29992;&#20854;&#28508;&#22312;&#30340;&#20248;&#21183;&#20197;&#25552;&#21319;&#20915;&#31574;&#33021;&#21147;&#12290;&#20851;&#38190;&#30340;&#19968;&#28857;&#26159;LATS&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#32534;&#31243;&#12289;HotPotQA&#21644;WebShop&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LATS&#22312;&#25512;&#29702;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#32534;&#31243;&#26041;&#38754;&#65292;LATS&#23454;&#29616;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on Hu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25511;&#20462;&#35746;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#27874;&#26463;&#25628;&#32034;&#20462;&#21098;&#36807;&#31243;&#20013;&#24341;&#20837;&#20462;&#35746;&#31383;&#21475;&#65292;&#31579;&#36873;&#20986;&#21487;&#33021;&#24341;&#36215;&#24191;&#27867;&#20462;&#35746;&#30340;&#20505;&#36873;&#32763;&#35793;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#20943;&#23569;&#38378;&#28865;&#29978;&#33267;&#23436;&#20840;&#28040;&#38500;&#38378;&#28865;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#19981;&#26174;&#33879;&#24433;&#21709;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.04399</link><description>&lt;p&gt;
&#22312;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#25913;&#21892;&#31283;&#23450;&#24615;&#65306;&#19968;&#31181;&#21487;&#25511;&#20462;&#35746;&#30340;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Stability in Simultaneous Speech Translation: A Revision-Controllable Decoding Approach. (arXiv:2310.04399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25511;&#20462;&#35746;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#27874;&#26463;&#25628;&#32034;&#20462;&#21098;&#36807;&#31243;&#20013;&#24341;&#20837;&#20462;&#35746;&#31383;&#21475;&#65292;&#31579;&#36873;&#20986;&#21487;&#33021;&#24341;&#36215;&#24191;&#27867;&#20462;&#35746;&#30340;&#20505;&#36873;&#32763;&#35793;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#20943;&#23569;&#38378;&#28865;&#29978;&#33267;&#23436;&#20840;&#28040;&#38500;&#38378;&#28865;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#19981;&#26174;&#33879;&#24433;&#21709;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#22312;&#23454;&#26102;&#36328;&#35821;&#35328;&#20132;&#27969;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#20173;&#28982;&#38754;&#20020;&#31283;&#23450;&#24615;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#34920;&#29616;&#20026;&#37096;&#20998;&#32467;&#26524;&#30340;&#38378;&#28865;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25511;&#20462;&#35746;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27874;&#26463;&#25628;&#32034;&#20462;&#21098;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#20801;&#35768;&#30340;&#20462;&#35746;&#31383;&#21475;&#65292;&#29992;&#20110;&#31579;&#36873;&#20986;&#21487;&#33021;&#36896;&#25104;&#24191;&#27867;&#20462;&#35746;&#30340;&#20505;&#36873;&#32763;&#35793;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#38378;&#28865;&#65292;&#24182;&#19988;&#20851;&#38190;&#22320;&#25552;&#20379;&#20102;&#23436;&#20840;&#28040;&#38500;&#38378;&#28865;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#35299;&#30721;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#19981;&#22823;&#24133;&#24230;&#24433;&#21709;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Speech-to-Text translation serves a critical role in real-time crosslingual communication. Despite the advancements in recent years, challenges remain in achieving stability in the translation process, a concern primarily manifested in the flickering of partial results. In this paper, we propose a novel revision-controllable method designed to address this issue. Our method introduces an allowed revision window within the beam search pruning process to screen out candidate translations likely to cause extensive revisions, leading to a substantial reduction in flickering and, crucially, providing the capability to completely eliminate flickering. The experiments demonstrate the proposed method can significantly improve the decoding stability without compromising substantially on the translation quality.
&lt;/p&gt;</description></item><item><title>Hermes&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31227;&#21160;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#20998;&#26512;&#12290;&#36890;&#36807;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#24182;&#29983;&#25104;&#36923;&#36753;&#20844;&#24335;&#65292;Hermes&#33021;&#22815;&#21457;&#29616;&#26032;&#30340;&#28431;&#27934;&#21644;&#25915;&#20987;&#65292;&#24182;&#23545;&#29616;&#26377;&#35268;&#33539;&#21644;&#21830;&#19994;&#22522;&#24102;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.04381</link><description>&lt;p&gt;
Hermes&#65306;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#21512;&#25104;&#26377;&#38480;&#29366;&#24577;&#26426;&#26469;&#35299;&#38145;&#31227;&#21160;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications. (arXiv:2310.04381v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04381
&lt;/p&gt;
&lt;p&gt;
Hermes&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31227;&#21160;&#32593;&#32476;&#21327;&#35758;&#30340;&#23433;&#20840;&#20998;&#26512;&#12290;&#36890;&#36807;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#24182;&#29983;&#25104;&#36923;&#36753;&#20844;&#24335;&#65292;Hermes&#33021;&#22815;&#21457;&#29616;&#26032;&#30340;&#28431;&#27934;&#21644;&#25915;&#20987;&#65292;&#24182;&#23545;&#29616;&#26377;&#35268;&#33539;&#21644;&#21830;&#19994;&#22522;&#24102;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Hermes&#65292;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31227;&#21160;&#35268;&#33539;&#30340;&#24418;&#24335;&#34920;&#36798;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#31070;&#32463;&#32452;&#25104;&#20998;&#26512;&#22120;NEUTREX&#65292;&#29992;&#20110;&#22788;&#29702;&#19982;&#36716;&#25442;&#30456;&#20851;&#30340;&#25991;&#26412;&#24182;&#25552;&#21462;&#36716;&#25442;&#32452;&#20214;&#65288;&#21363;&#29366;&#24577;&#12289;&#26465;&#20214;&#21644;&#21160;&#20316;&#65289;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#36890;&#36807;&#21033;&#29992;&#20381;&#23384;&#35299;&#26512;&#26641;&#23558;&#36825;&#20123;&#36716;&#25442;&#32452;&#20214;&#36716;&#21270;&#25104;&#36923;&#36753;&#20844;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#36923;&#36753;&#20844;&#24335;&#32534;&#35793;&#25104;&#36716;&#25442;&#21644;&#21019;&#24314;&#24418;&#24335;&#27169;&#22411;&#20316;&#20026;&#26377;&#38480;&#29366;&#24577;&#26426;&#12290;&#20026;&#20102;&#35777;&#26126;Hermes&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;4G NAS&#12289;5G NAS&#21644;5G RRC&#35268;&#33539;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#33719;&#24471;&#20102;81-87%&#30340;&#24635;&#20307;&#20934;&#30830;&#29575;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#25216;&#26415;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#23545;&#25552;&#21462;&#30340;&#27169;&#22411;&#36827;&#34892;&#30340;&#23433;&#20840;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;3&#20010;&#26032;&#30340;&#28431;&#27934;&#12289;&#21457;&#29616;&#20102;19&#20010;&#20043;&#21069;&#30340;&#25915;&#20987;4G&#21644;5G&#35268;&#33539;&#65292;&#20197;&#21450;7&#20010;&#21830;&#19994;4G&#22522;&#24102;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Hermes, an end-to-end framework to automatically generate formal representations from natural language cellular specifications. We first develop a neural constituency parser, NEUTREX, to process transition-relevant texts and extract transition components (i.e., states, conditions, and actions). We also design a domain-specific language to translate these transition components to logical formulas by leveraging dependency parse trees. Finally, we compile these logical formulas to generate transitions and create the formal model as finite state machines. To demonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and 5G RRC specifications and obtain an overall accuracy of 81-87%, which is a substantial improvement over the state-of-the-art. Our security analysis of the extracted models uncovers 3 new vulnerabilities and identifies 19 previous attacks in 4G and 5G specifications, and 7 deviations in commercial 4G basebands.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25674;&#38144;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#22788;&#29702;&#25512;&#29702;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04363</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#25674;&#38144;&#38590;&#20197;&#22788;&#29702;&#30340;&#25512;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Amortizing intractable inference in large language models. (arXiv:2310.04363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25674;&#38144;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#22788;&#29702;&#25512;&#29702;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#26465;&#20214;&#20998;&#24067;&#26469;&#21387;&#32553;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#35813;&#30693;&#35782;&#30340;&#21487;&#22788;&#29702;&#26597;&#35810;&#20165;&#38480;&#20110;&#20174;&#22836;&#21040;&#23614;&#30340;&#33258;&#22238;&#24402;&#25277;&#26679;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#24207;&#21015;&#24310;&#32493;&#12289;&#22635;&#20805;&#21644;&#20854;&#20182;&#24418;&#24335;&#30340;&#21463;&#32422;&#26463;&#29983;&#25104;&#65292;&#37117;&#28041;&#21450;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25674;&#38144;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#20174;&#36825;&#20123;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#36825;&#31181;&#25674;&#38144;&#36890;&#36807;&#36890;&#36807;&#23547;&#27714;&#22810;&#26679;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; - &#29983;&#25104;&#27969;&#32593;&#32476; (GFlowNets) &#26469;&#24494;&#35843; LLMs &#23454;&#29616;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;LLM&#24494;&#35843;&#30340;&#36825;&#31181;&#20998;&#24067;&#21305;&#37197;&#33539;&#24335;&#21487;&#20197;&#20316;&#20026;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#21644;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#20248;&#21270;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#24605;&#32500;&#38142;&#25512;&#29702;&#35299;&#37322;&#20026;&#28508;&#21464;&#37327;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21516;&#26102;&#36716;&#31227;&#35821;&#38899;&#36890;&#29992;&#21644;&#25233;&#37057;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#36890;&#36807;&#33258;&#21457;&#24615;&#35821;&#38899;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#26102;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#30693;&#35782;&#36716;&#31227;&#26694;&#26550;&#65292;&#22312;AD&#21644;&#25233;&#37057;&#30151;&#35786;&#26029;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04358</link><description>&lt;p&gt;
&#36716;&#31227;&#35821;&#38899;&#36890;&#29992;&#21644;&#25233;&#37057;&#29305;&#23450;&#30340;&#30693;&#35782;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transferring speech-generic and depression-specific knowledge for Alzheimer's disease detection. (arXiv:2310.04358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21516;&#26102;&#36716;&#31227;&#35821;&#38899;&#36890;&#29992;&#21644;&#25233;&#37057;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#36890;&#36807;&#33258;&#21457;&#24615;&#35821;&#38899;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#26102;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#30693;&#35782;&#36716;&#31227;&#26694;&#26550;&#65292;&#22312;AD&#21644;&#25233;&#37057;&#30151;&#35786;&#26029;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#21457;&#24615;&#35821;&#38899;&#36827;&#34892;&#26816;&#27979;&#30340;&#30142;&#30149;&#65292;&#20294;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#30693;&#35782;&#36716;&#31227;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#20174;&#35821;&#38899;&#36890;&#29992;&#21644;&#25233;&#37057;&#29305;&#23450;&#30340;&#30693;&#35782;&#20013;&#36827;&#34892;&#36716;&#31227;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#20013;&#36827;&#34892;&#39034;&#24207;&#30693;&#35782;&#36716;&#31227;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#22823;&#37327;&#35821;&#38899;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#12290;&#28982;&#21518;&#65292;&#23545;&#22522;&#20110;&#19981;&#21516;&#22522;&#30784;&#27169;&#22411;&#19981;&#21516;&#20013;&#38388;&#22359;&#25552;&#21462;&#30340;&#34920;&#31034;&#36827;&#34892;&#20102;AD&#35786;&#26029;&#30340;&#22359;&#20998;&#26512;&#12290;&#38500;&#20102;&#26469;&#33258;&#35821;&#38899;&#36890;&#29992;&#34920;&#31034;&#30340;&#30693;&#35782;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#21516;&#26102;&#20174;&#22522;&#20110;&#25233;&#37057;&#30151;&#21644;AD&#39640;&#20849;&#30149;&#29575;&#30340;&#35821;&#38899;&#25233;&#37057;&#26816;&#27979;&#20219;&#21153;&#20013;&#36716;&#31227;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#30693;&#35782;&#36716;&#31227;&#26694;&#26550;&#65292;&#20849;&#21516;&#23398;&#20064;&#36825;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;AD&#21644;depression&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of Alzheimer's disease (AD) from spontaneous speech has attracted increasing attention while the sparsity of training data remains an important issue. This paper handles the issue by knowledge transfer, specifically from both speech-generic and depression-specific knowledge. The paper first studies sequential knowledge transfer from generic foundation models pretrained on large amounts of speech and text data. A block-wise analysis is performed for AD diagnosis based on the representations extracted from different intermediate blocks of different foundation models. Apart from the knowledge from speech-generic representations, this paper also proposes to simultaneously transfer the knowledge from a speech depression detection task based on the high comorbidity rates of depression and AD. A parallel knowledge transfer framework is studied that jointly learns the information shared between these two tasks. Experimental results show that the proposed method improves AD and de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#38889;&#25991;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#31867;&#26377;&#20559;&#35265;&#35328;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#39033;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04313</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#30495;&#23454;&#19990;&#30028;&#22312;&#32447;&#26381;&#21153;&#20013;&#20998;&#31867;&#26377;&#20559;&#35265;&#35328;&#35770;&#30340;&#22823;&#35268;&#27169;&#38889;&#25991;&#25991;&#26412;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Korean Text Dataset for Classifying Biased Speech in Real-World Online Services. (arXiv:2310.04313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04313
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#38889;&#25991;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#31867;&#26377;&#20559;&#35265;&#35328;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#39033;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#26381;&#21153;&#30340;&#22686;&#38271;&#65292;&#23545;&#39640;&#32423;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65288;&#22914;&#24773;&#24863;&#20998;&#26512;&#21644;&#26377;&#20559;&#25991;&#26412;&#26816;&#27979;&#65289;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#22312;&#32447;&#26381;&#21153;&#30340;&#21311;&#21517;&#24615;&#24120;&#24120;&#23548;&#33268;&#26377;&#20559;&#35265;&#21644;&#26377;&#23475;&#35328;&#35821;&#30340;&#23384;&#22312;&#65292;&#23545;&#32500;&#25252;&#22312;&#32447;&#31038;&#21306;&#30340;&#20581;&#24247;&#24102;&#26469;&#25361;&#25112;&#12290;&#36825;&#31181;&#29616;&#35937;&#22312;&#38889;&#22269;&#23588;&#20854;&#30456;&#20851;&#65292;&#30446;&#21069;&#23578;&#26410;&#24191;&#27867;&#30740;&#31350;&#22823;&#35268;&#27169;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#19968;&#20010;&#30693;&#21517;&#30340;&#38889;&#22269;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#25910;&#38598;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21253;&#25324;(1)&#20559;&#22909;&#12289;(2)&#20302;&#20439;&#35821;&#35328;&#21644;(3)&#20061;&#31181;&#20559;&#35265;&#31867;&#22411;&#30340;&#25991;&#26412;&#26679;&#26412;&#30340;&#27880;&#37322;&#65292;&#20351;&#24471;&#33021;&#22815;&#21516;&#26102;&#23545;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20998;&#31867;&#12290;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25351;&#26631;&#19979;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growth of online services, the need for advanced text classification algorithms, such as sentiment analysis and biased text detection, has become increasingly evident. The anonymous nature of online services often leads to the presence of biased and harmful language, posing challenges to maintaining the health of online communities. This phenomenon is especially relevant in South Korea, where large-scale hate speech detection algorithms have not yet been broadly explored. In this paper, we introduce a new comprehensive, large-scale dataset collected from a well-known South Korean SNS platform. Our proposed dataset provides annotations including (1) Preferences, (2) Profanities, and (3) Nine types of Bias for the text samples, enabling multi-task learning for simultaneous classification of user-generated texts. Leveraging state-of-the-art BERT-based language models, our approach surpasses human-level accuracy across diverse classification tasks, as measured by various metrics. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#38646;&#26679;&#26412;LLMs&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04270</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks. (arXiv:2310.04270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#38646;&#26679;&#26412;LLMs&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#23427;&#20204;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#22522;&#20934;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;6&#20010;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;26&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;4&#20010;&#28909;&#38376;LLMs&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#21644;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#38646;&#26679;&#26412;LLMs&#29978;&#33267;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#12290;&#36825;&#34920;&#26126;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#27809;&#26377;&#19968;&#20010;LLM&#33021;&#22815;&#32988;&#36807;&#20854;&#20182;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLM) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, we conduct a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art fine-tuned biomedical models. This suggests that pretraining on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;COVID-19&#30456;&#20851;&#25512;&#25991;&#21644;TikTok&#35270;&#39057;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#19968;&#31995;&#21015;&#21306;&#20998;&#30495;&#20551;&#26032;&#38395;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#20026;&#29702;&#35299;&#35821;&#35328;&#22312;&#27492;&#26041;&#38754;&#30340;&#20316;&#29992;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.04237</link><description>&lt;p&gt;
COVID-19&#30456;&#20851;&#30495;&#20551;&#31038;&#20132;&#23186;&#20307;&#21457;&#24067;&#30340;&#20070;&#38754;&#21644;&#21475;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Written and spoken corpus of real and fake social media postings about COVID-19. (arXiv:2310.04237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;COVID-19&#30456;&#20851;&#25512;&#25991;&#21644;TikTok&#35270;&#39057;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#19968;&#31995;&#21015;&#21306;&#20998;&#30495;&#20551;&#26032;&#38395;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#20026;&#29702;&#35299;&#35821;&#35328;&#22312;&#27492;&#26041;&#38754;&#30340;&#20316;&#29992;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20551;&#26032;&#38395;&#21644;&#30495;&#23454;&#26032;&#38395;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;&#30740;&#31350;&#20998;&#20026;&#25991;&#26412;&#25968;&#25454;&#21644;&#35821;&#38899;&#25968;&#25454;&#20004;&#20010;&#37096;&#20998;&#12290;&#25991;&#26412;&#25968;&#25454;&#21253;&#25324;&#20174;Patwa&#31561;&#20154;&#65288;2021&#65289;&#30340;6420&#26465;&#19982;COVID-19&#30456;&#20851;&#30340;&#25512;&#25991;&#20013;&#37325;&#26032;&#31579;&#36873;&#20986;&#26469;&#30340;3049&#26465;&#25512;&#25991;&#65292;&#20854;&#20013;2161&#26465;&#26631;&#35760;&#20026;&#8220;&#30495;&#23454;&#8221;&#65292;888&#26465;&#26631;&#35760;&#20026;&#8220;&#20551;&#8221;&#12290;&#35821;&#38899;&#25968;&#25454;&#37319;&#38598;&#33258;TikTok&#65292;&#37325;&#28857;&#20851;&#27880;COVID-19&#30456;&#20851;&#35270;&#39057;&#12290;&#30740;&#31350;&#21161;&#29702;&#20351;&#29992;&#21487;&#38752;&#26469;&#28304;&#23545;&#27599;&#20010;&#35270;&#39057;&#30340;&#20869;&#23481;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#65292;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#8220;&#30495;&#23454;&#8221;&#12289;&#8220;&#20551;&#8221;&#25110;&#8220;&#21487;&#30097;&#8221;&#65292;&#32467;&#26524;&#24471;&#21040;&#20102;&#26469;&#33258;200&#20010;TikTok&#35270;&#39057;&#30340;91&#20010;&#30495;&#23454;&#26465;&#30446;&#21644;109&#20010;&#20551;&#26465;&#30446;&#30340;&#25968;&#25454;&#65292;&#24635;&#35789;&#25968;&#20026;53,710&#20010;&#35789;&#12290;&#20351;&#29992;Linguistic Inquiry and Word Count&#65288;LIWC&#65289;&#36719;&#20214;&#20998;&#26512;&#25968;&#25454;&#20197;&#26816;&#27979;&#35821;&#35328;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#12290;&#32467;&#26524;&#34920;&#26126;&#20102;&#21306;&#20998;&#20551;&#26032;&#38395;&#21644;&#30495;&#23454;&#26032;&#38395;&#30340;&#19968;&#31995;&#21015;&#35821;&#35328;&#29305;&#24449;&#65292;&#26080;&#35770;&#26159;&#22312;&#20070;&#38754;&#25968;&#25454;&#36824;&#26159;&#21475;&#35821;&#25968;&#25454;&#20013;&#12290;&#36825;&#20026;&#20102;&#35299;&#35821;&#35328;&#22312;&#27492;&#26041;&#38754;&#30340;&#20316;&#29992;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the linguistic traits of fake news and real news. There are two parts to this study: text data and speech data. The text data for this study consisted of 6420 COVID-19 related tweets re-filtered from Patwa et al. (2021). After cleaning, the dataset contained 3049 tweets, with 2161 labeled as 'real' and 888 as 'fake'. The speech data for this study was collected from TikTok, focusing on COVID-19 related videos. Research assistants fact-checked each video's content using credible sources and labeled them as 'Real', 'Fake', or 'Questionable', resulting in a dataset of 91 real entries and 109 fake entries from 200 TikTok videos with a total word count of 53,710 words. The data was analysed using the Linguistic Inquiry and Word Count (LIWC) software to detect patterns in linguistic data. The results indicate a set of linguistic features that distinguish fake news from real news in both written and speech data. This offers valuable insights into the role of language i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#38190;&#35789;&#22686;&#24378;&#26816;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20851;&#38190;&#35789;&#26469;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21457;&#29616;&#21644;&#31572;&#26696;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20302;&#25104;&#26412;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#38899;&#25509;&#21475;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.04205</link><description>&lt;p&gt;
&#20851;&#38190;&#35789;&#22686;&#24378;&#26816;&#32034;: &#38598;&#25104;&#35821;&#38899;&#25509;&#21475;&#30340;&#20449;&#24687;&#26816;&#32034;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface. (arXiv:2310.04205v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04205
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#38190;&#35789;&#22686;&#24378;&#26816;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20851;&#38190;&#35789;&#26469;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21457;&#29616;&#21644;&#31572;&#26696;&#29983;&#25104;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20302;&#25104;&#26412;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#38899;&#25509;&#21475;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32452;&#21512;&#20013;&#24555;&#36895;&#12289;&#20302;&#25104;&#26412;&#22320;&#26816;&#32034;&#31572;&#26696;&#65292;&#32780;&#19981;&#20135;&#29983;&#24187;&#35273;&#65292;&#26159;&#38459;&#27490;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#33258;&#21160;&#21270;&#20013;&#24212;&#29992;&#30340;&#19968;&#22823;&#38556;&#30861;&#12290;&#24403;&#24819;&#35201;&#38598;&#25104;&#35821;&#38899;&#25509;&#21475;&#26102;&#65292;&#36825;&#19968;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#21830;&#19994;&#25628;&#32034;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#26469;&#35828;&#65292;&#23436;&#20840;&#20381;&#36182;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT 3.5&#31561;&#65289;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#26412;&#25991;&#20316;&#32773;&#36890;&#36807;&#39318;&#20808;&#24320;&#21457;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#25628;&#32034;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#23545;&#35201;&#25552;&#20379;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#30340;&#21457;&#29616;&#12290;&#20851;&#38190;&#35789;&#21453;&#36807;&#26469;&#26159;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24182;&#32531;&#23384;&#65292;&#20197;&#20415;&#19982;&#26597;&#35810;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#22312;&#25991;&#26723;&#20013;&#26597;&#25214;&#19978;&#19979;&#25991;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#19968;&#26086;&#19978;&#19979;&#25991;&#35774;&#32622;&#22909;&#20102;&#65292;&#35821;&#35328;&#27169;&#22411;&#23601;&#21487;&#20197;&#26681;&#25454;&#20026;&#38382;&#31572;&#23450;&#21046;&#30340;&#25552;&#31034;&#25552;&#20379;&#31572;&#26696;&#12290;&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Retrieving answers in a quick and low cost manner without hallucinations from a combination of structured and unstructured data using Language models is a major hurdle which prevents employment of Language models in knowledge retrieval automation. This becomes accentuated when one wants to integrate a speech interface. Besides, for commercial search and chatbot applications, complete reliance on commercial large language models (LLMs) like GPT 3.5 etc. can be very costly. In this work, authors have addressed this problem by first developing a keyword based search framework which augments discovery of the context to be provided to the large language model. The keywords in turn are generated by LLM and cached for comparison with keywords generated by LLM against the query raised. This significantly reduces time and cost to find the context within documents. Once the context is set, LLM uses that to provide answers based on a prompt tailored for Q&amp;A. This research work demonstrates that u
&lt;/p&gt;</description></item><item><title>mlirSynth&#37319;&#29992;&#31243;&#24207;&#32508;&#21512;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25163;&#21160;&#23450;&#20041;&#35268;&#21017;&#21363;&#21487;&#23558;&#20302;&#32423;MLIR&#26041;&#35328;&#30340;&#31243;&#24207;&#25552;&#21319;&#21040;&#39640;&#32423;&#26041;&#35328;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#19988;&#37325;&#23450;&#21521;&#30340;&#25552;&#21319;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#32534;&#35793;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.04196</link><description>&lt;p&gt;
mlirSynth&#65306;&#20351;&#29992;&#31243;&#24207;&#32508;&#21512;&#22312;&#22810;&#32423;IR&#20013;&#36827;&#34892;&#33258;&#21160;&#12289;&#37325;&#23450;&#21521;&#30340;&#31243;&#24207;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
mlirSynth: Automatic, Retargetable Program Raising in Multi-Level IR using Program Synthesis. (arXiv:2310.04196v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04196
&lt;/p&gt;
&lt;p&gt;
mlirSynth&#37319;&#29992;&#31243;&#24207;&#32508;&#21512;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25163;&#21160;&#23450;&#20041;&#35268;&#21017;&#21363;&#21487;&#23558;&#20302;&#32423;MLIR&#26041;&#35328;&#30340;&#31243;&#24207;&#25552;&#21319;&#21040;&#39640;&#32423;&#26041;&#35328;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#19988;&#37325;&#23450;&#21521;&#30340;&#25552;&#21319;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#32534;&#35793;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MLIR&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#38024;&#23545;&#29616;&#20195;&#30828;&#20214;&#30340;&#32534;&#35793;&#22120;&#22522;&#30784;&#35774;&#26045;&#65292;&#20294;&#26159;&#22914;&#26524;&#29616;&#26377;&#31243;&#24207;&#26159;&#29992;&#20302;&#32423;&#36890;&#29992;&#35821;&#35328;&#25551;&#36848;&#30340;&#65292;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;MLIR&#30340;&#39640;&#24615;&#33021;&#32534;&#35793;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#36991;&#20813;&#25163;&#21160;&#37325;&#20889;&#31243;&#24207;&#65292;&#29616;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#33258;&#21160;&#23558;&#20302;&#32423;&#21035;&#25552;&#21319;&#21040;MLIR&#20013;&#30340;&#39640;&#32423;&#21035;&#26041;&#35328;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#23450;&#20041;&#30340;&#25552;&#21319;&#35268;&#21017;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#38543;&#30528;MLIR&#26041;&#35328;&#30340;&#28436;&#21464;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#32500;&#25252;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;mlirSynth&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#23450;&#20041;&#35268;&#21017;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#20302;&#32423;&#21035;MLIR&#26041;&#35328;&#30340;&#31243;&#24207;&#36716;&#25442;&#20026;&#39640;&#32423;&#21035;&#26041;&#35328;&#12290;&#30456;&#21453;&#65292;&#23427;&#21033;&#29992;&#21487;&#29992;&#30340;&#26041;&#35328;&#23450;&#20041;&#26500;&#24314;&#31243;&#24207;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#31867;&#22411;&#32422;&#26463;&#21644;&#31561;&#25928;&#24615;&#26377;&#25928;&#22320;&#25628;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;C&#31243;&#24207;&#25552;&#21319;&#21040;&#20004;&#20010;&#19981;&#21516;&#30340;&#39640;&#32423;MLIR&#26041;&#35328;&#26469;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#29616;&#26377;&#30340;&#39640;&#32423;&#26041;&#35328;&#29305;&#23450;&#32534;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
MLIR is an emerging compiler infrastructure for modern hardware, but existing programs cannot take advantage of MLIR's high-performance compilation if they are described in lower-level general purpose languages. Consequently, to avoid programs needing to be rewritten manually, this has led to efforts to automatically raise lower-level to higher-level dialects in MLIR. However, current methods rely on manually-defined raising rules, which limit their applicability and make them challenging to maintain as MLIR dialects evolve.  We present mlirSynth -- a novel approach which translates programs from lower-level MLIR dialects to high-level ones without manually defined rules. Instead, it uses available dialect definitions to construct a program space and searches it effectively using type constraints and equivalences. We demonstrate its effectiveness \revi{by raising C programs} to two distinct high-level MLIR dialects, which enables us to use existing high-level dialect specific compilati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#20420;&#35821;&#31185;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#26041;&#38754;&#30340;&#24037;&#20855;&#65292;&#21253;&#25324;&#20219;&#21153;&#12289;&#36129;&#29486;&#12289;&#26041;&#27861;&#21644;&#32467;&#35770;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;&#36890;&#36807;&#36328;&#39046;&#22495;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#31185;&#23398;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#20173;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.04074</link><description>&lt;p&gt;
&#20174;&#31185;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Automatic Aspect Extraction from Scientific Texts. (arXiv:2310.04074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04074
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#20420;&#35821;&#31185;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#26041;&#38754;&#30340;&#24037;&#20855;&#65292;&#21253;&#25324;&#20219;&#21153;&#12289;&#36129;&#29486;&#12289;&#26041;&#27861;&#21644;&#32467;&#35770;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;&#36890;&#36807;&#36328;&#39046;&#22495;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#31185;&#23398;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#20173;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#20986;&#20027;&#35201;&#35266;&#28857;&#12289;&#20851;&#38190;&#35265;&#35299;&#21644;&#20854;&#20182;&#37325;&#35201;&#20449;&#24687;&#65288;&#22312;&#27492;&#31216;&#20026;&#26041;&#38754;&#65289;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#36827;&#34892;&#31185;&#23398;&#25991;&#29486;&#32508;&#36848;&#30340;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#30340;&#26159;&#21019;&#24314;&#19968;&#20010;&#29992;&#20110;&#20174;&#20219;&#20309;&#39046;&#22495;&#30340;&#20420;&#35821;&#31185;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#26041;&#38754;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20420;&#35821;&#31185;&#23398;&#25991;&#26412;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#65292;&#27880;&#37322;&#26377;&#20219;&#21153;&#12289;&#36129;&#29486;&#12289;&#26041;&#27861;&#21644;&#32467;&#35770;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#22522;&#20934;&#31639;&#27861;&#36827;&#34892;&#26041;&#38754;&#25552;&#21462;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#39046;&#22495;&#20013;&#26041;&#38754;&#30340;&#34920;&#31034;&#23384;&#22312;&#19968;&#20123;&#24046;&#24322;&#65292;&#20294;&#21363;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#31185;&#23398;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#20173;&#28982;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#36825;&#22312;&#36328;&#39046;&#22495;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; \url{https://github.com/anna-marshalova/automatic-aspect-extraction-from} &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to extract from scientific papers their main points, key insights, and other important information, referred to here as aspects, might facilitate the process of conducting a scientific literature review. Therefore, the aim of our research is to create a tool for automatic aspect extraction from Russian-language scientific texts of any domain. In this paper, we present a cross-domain dataset of scientific texts in Russian, annotated with such aspects as Task, Contribution, Method, and Conclusion, as well as a baseline algorithm for aspect extraction, based on the multilingual BERT model fine-tuned on our data. We show that there are some differences in aspect representation in different domains, but even though our model was trained on a limited number of scientific domains, it is still able to generalize to new domains, as was proved by cross-domain experiments. The code and the dataset are available at \url{https://github.com/anna-marshalova/automatic-aspect-extraction-from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#32463;&#20856;transformer attention&#27867;&#21270;&#21040;&#33021;&#22815;&#25429;&#25417;&#19977;&#20803;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#26377;&#30028;&#36755;&#20837;&#19979;&#20855;&#26377;&#36817;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.04064</link><description>&lt;p&gt;
&#22914;&#20309;&#25429;&#25417;&#39640;&#38454;&#30456;&#20851;&#24615;&#65311;&#23558;&#30697;&#38453;Softmax Attention&#25512;&#24191;&#21040;Kronecker&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation. (arXiv:2310.04064v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#32463;&#20856;transformer attention&#27867;&#21270;&#21040;&#33021;&#22815;&#25429;&#25417;&#19977;&#20803;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#26377;&#30028;&#36755;&#20837;&#19979;&#20855;&#26377;&#36817;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32463;&#20856;&#30340;transformer attention&#26041;&#26696;&#20013;&#65292;&#25105;&#20204;&#32473;&#23450;&#19977;&#20010;&#22823;&#23567;&#20026;$n \times d$&#30340;&#30697;&#38453;$Q, K, V$&#65288;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#26631;&#35760;&#65289;&#65292;&#30446;&#26631;&#26159;&#35745;&#31639;&#19968;&#20010;&#26032;&#30340;&#22823;&#23567;&#20026;$n \times d$&#30340;&#30697;&#38453;$D^{-1} \exp(QK^\top) V$&#65292;&#20854;&#20013;$D = \mathrm{diag}( \exp(QK^\top) {\bf 1}_n )$&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#33021;&#22815;&#25429;&#25417;&#19977;&#20803;&#30456;&#20851;&#24615;&#30340;&#27880;&#24847;&#21147;&#30340;&#27867;&#21270;&#12290;&#36825;&#31181;&#27867;&#21270;&#33021;&#22815;&#35299;&#20915;&#20851;&#20110;&#26816;&#27979;transformers&#26080;&#27861;&#35299;&#20915;&#30340;&#19977;&#20803;&#36830;&#25509;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#27867;&#21270;&#30340;&#28508;&#22312;&#32570;&#28857;&#26159;&#65292;&#35745;&#31639;&#20284;&#20046;&#26356;&#21152;&#22256;&#38590;&#65292;&#22240;&#20026;&#30452;&#25509;&#30340;&#31639;&#27861;&#22312;$n$&#30340;&#31435;&#26041;&#26102;&#38388;&#20869;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26377;&#30028;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65288;&#23454;&#36341;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#37117;&#26377;&#24191;&#27867;&#30740;&#31350;&#65289;&#65292;&#23454;&#38469;&#19978;&#23384;&#22312;&#19968;&#20010;&#36817;&#32447;&#24615;&#26102;&#38388;&#30340;&#31639;&#27861;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#26377;&#30028;&#36755;&#20837;&#26082;&#26159;&#24555;&#36895;&#25191;&#34892;&#24191;&#20041;&#35745;&#31639;&#30340;&#24517;&#35201;&#26465;&#20214;&#20063;&#26159;&#20805;&#20998;&#26465;&#20214;&#65306; $\bul
&lt;/p&gt;
&lt;p&gt;
In the classical transformer attention scheme, we are given three $n \times d$ size matrices $Q, K, V$ (the query, key, and value tokens), and the goal is to compute a new $n \times d$ size matrix $D^{-1} \exp(QK^\top) V$ where $D = \mathrm{diag}( \exp(QK^\top) {\bf 1}_n )$. In this work, we study a generalization of attention which captures triple-wise correlations. This generalization is able to solve problems about detecting triple-wise connections that were shown to be impossible for transformers. The potential downside of this generalization is that it appears as though computations are even more difficult, since the straightforward algorithm requires cubic time in $n$. However, we show that in the bounded-entry setting (which arises in practice, and which is well-studied in both theory and practice), there is actually a near-linear time algorithm. More precisely, we show that bounded entries are both necessary and sufficient for quickly performing generalized computations:  $\bul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#38382;&#31572;&#20219;&#21153;&#65288;RRIP&#65289;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#20887;&#20313;&#20449;&#24687;&#25552;&#20379;&#30340;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#20256;&#32479;QA&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;RRIP&#20219;&#21153;&#20013;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.04039</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20887;&#20313;&#20449;&#24687;&#35299;&#37322;&#33021;&#21147;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models. (arXiv:2310.04039v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#38382;&#31572;&#20219;&#21153;&#65288;RRIP&#65289;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#20887;&#20313;&#20449;&#24687;&#25552;&#20379;&#30340;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#20256;&#32479;QA&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;RRIP&#20219;&#21153;&#20013;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#22312;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#25512;&#29702;&#26041;&#38754;&#65292;&#36825;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#36825;&#20123;&#27169;&#22411;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#24418;&#24335;&#65292;&#31216;&#20026;&#20887;&#20313;&#20449;&#24687;&#25552;&#20379;&#30340;&#25512;&#29702;&#65288;RRIP&#65289;&#12290;&#35813;&#30740;&#31350;&#35774;&#35745;&#20102;&#20462;&#25913;&#29256;&#30340;8K&#23567;&#23398;&#25968;&#23398;&#65288;GSM-8K&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26377;&#22810;&#31181;&#21464;&#20307;&#65292;&#19987;&#27880;&#20110;&#20887;&#20313;&#20449;&#24687;&#30340;&#19981;&#21516;&#23646;&#24615;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20004;&#31181;&#24120;&#29992;&#30340;LLMs&#65292;LlaMA2-13B-chat&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;3.5&#65288;GPT-3.5&#65289;&#65292;&#23545;&#27604;&#20102;&#23427;&#20204;&#22312;&#20256;&#32479;QA&#20219;&#21153;&#21644;RRIP&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#26631;&#20934;QA&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36866;&#24230;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks, especially in reasoning, a cornerstone for achieving Artificial General Intelligence (AGI). However, commonly used benchmarks may not fully encapsulate the inferential abilities of these models in real-world scenarios. To address this gap, a new form of Question-Answering (QA) task, termed Reasoning with Redundant Information Provided (RRIP), is introduced. The study designed a modified version of the grade school math 8K (GSM-8K) dataset which has several variants focusing on different attributes of redundant information. This investigation evaluates two popular LLMs, LlaMA2-13B-chat and generative pre-trained transformer 3.5 (GPT-3.5), contrasting their performance on traditional QA tasks against the RRIP tasks. Findings indicate that while these models achieved moderate success on standard QA benchmarks, their performance notably declines
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#25552;&#21319;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#22312;&#21442;&#25968;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04027</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models. (arXiv:2310.04027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#25552;&#21319;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#22312;&#21442;&#25968;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#20272;&#20540;&#21644;&#25237;&#36164;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21463;&#20854;&#21442;&#25968;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#33539;&#22260;&#30340;&#38480;&#21046;&#65292;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#22312;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#20197;&#24191;&#27867;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20196;&#20154;&#31216;&#36190;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;LLMs&#24212;&#29992;&#20110;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#23384;&#22312;&#25361;&#25112;&#65306;LLMs&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#19982;&#24773;&#24863;&#26631;&#31614;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250; compromise&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#37329;&#34701;&#26032;&#38395;&#30340;&#31616;&#27905;&#24615;&#65292;&#24120;&#24120;&#32570;&#20047;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#65292;&#20063;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;LLMs&#30340;&#24773;&#24863;&#20998;&#26512;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial sentiment analysis is critical for valuation and investment decision-making. Traditional NLP models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models (LLMs) pre-trained on extensive corpora have demonstrated superior performance across various NLP tasks due to their commendable zero-shot abilities. Yet, directly applying LLMs to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis. To address these challenges, we introduce a retrieval-augmented LLMs framework for financial sentiment analysis. This framework includes an instruction-tuned L
&lt;/p&gt;</description></item><item><title>SemStamp&#26159;&#19968;&#31181;&#31283;&#20581;&#30340;&#21477;&#32423;&#35821;&#20041;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#23558;&#35821;&#20041;&#31354;&#38388;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#36793;&#30028;&#32422;&#26463;&#22686;&#24378;&#20102;&#20854;&#31283;&#20581;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;SemStamp&#22312;&#37322;&#20041;&#21644;bigram&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.03991</link><description>&lt;p&gt;
SemStamp&#65306;&#19968;&#31181;&#20855;&#26377;&#37322;&#20041;&#31283;&#20581;&#24615;&#30340;&#25991;&#26412;&#29983;&#25104;&#35821;&#20041;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation. (arXiv:2310.03991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03991
&lt;/p&gt;
&lt;p&gt;
SemStamp&#26159;&#19968;&#31181;&#31283;&#20581;&#30340;&#21477;&#32423;&#35821;&#20041;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#23558;&#35821;&#20041;&#31354;&#38388;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#36793;&#30028;&#32422;&#26463;&#22686;&#24378;&#20102;&#20854;&#31283;&#20581;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;SemStamp&#22312;&#37322;&#20041;&#21644;bigram&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#27700;&#21360;&#31639;&#27861;&#30001;&#20110;&#20854;&#22522;&#20110;&#20196;&#29260;&#32423;&#21035;&#30340;&#35774;&#35745;&#65292;&#23545;&#37322;&#20041;&#25915;&#20987;&#20855;&#26377;&#24369;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#30340;&#31283;&#20581;&#21477;&#32423;&#35821;&#20041;&#27700;&#21360;&#31639;&#27861;&#8212;&#8212;SemStamp&#65292;&#35813;&#31639;&#27861;&#23545;&#21477;&#23376;&#30340;&#35821;&#20041;&#31354;&#38388;&#36827;&#34892;&#21010;&#20998;&#12290;&#31639;&#27861;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20505;&#36873;&#21477;&#23376;&#36827;&#34892;&#32534;&#30721;&#21644;LSH&#21704;&#24076;&#65292;&#24182;&#22312;&#35821;&#20041;&#23884;&#20837;&#31354;&#38388;&#20013;&#25191;&#34892;&#21477;&#32423;&#25298;&#32477;&#37319;&#26679;&#65292;&#30452;&#21040;&#37319;&#26679;&#30340;&#21477;&#23376;&#33853;&#20837;&#27700;&#21360;&#20998;&#21306;&#20013;&#12290;&#37319;&#29992;&#22522;&#20110;&#36793;&#30028;&#30340;&#32422;&#26463;&#26469;&#22686;&#24378;&#20854;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#31639;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"bigram"&#30340;&#37322;&#20041;&#25915;&#20987;&#65292;&#20351;&#29992;&#19982;&#21407;&#22987;&#21477;&#23376;&#26368;&#23569;&#30340;bigram&#37325;&#21472;&#30340;&#37322;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#35821;&#20041;&#27700;&#21360;&#31639;&#27861;&#19981;&#20165;&#22312;&#24120;&#35265;&#30340;&#37322;&#20041;&#21644;bigram&#19978;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing watermarking algorithms are vulnerable to paraphrase attacks because of their token-level design. To address this issue, we propose SemStamp, a robust sentence-level semantic watermarking algorithm based on locality-sensitive hashing (LSH), which partitions the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by an LLM, and conducts sentence-level rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. A margin-based constraint is used to enhance its robustness. To show the advantages of our algorithm, we propose a "bigram" paraphrase attack using the paraphrase that has the fewest bigram overlaps with the original sentence. This attack is shown to be effective against the existing token-level watermarking method. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on both common and bigram pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#26222;&#36890;&#35805;&#35328;&#35821;&#30196;&#21574;&#35780;&#20272;&#31995;&#32479;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#24182;&#25552;&#21462;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#21644;&#20020;&#24202;&#30196;&#21574;&#35780;&#20998;&#39044;&#27979;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.03985</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#36827;&#34892;&#26222;&#36890;&#35805;&#35328;&#35821;&#30340;&#30196;&#21574;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dementia Assessment Using Mandarin Speech with an Attention-based Speech Recognition Encoder. (arXiv:2310.03985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#26222;&#36890;&#35805;&#35328;&#35821;&#30196;&#21574;&#35780;&#20272;&#31995;&#32479;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#24182;&#25552;&#21462;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#21644;&#20020;&#24202;&#30196;&#21574;&#35780;&#20998;&#39044;&#27979;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30196;&#21574;&#35786;&#26029;&#38656;&#35201;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#36825;&#26159;&#22797;&#26434;&#19988;&#32791;&#26102;&#30340;&#12290;&#30196;&#21574;&#30340;&#26089;&#26399;&#26816;&#27979;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#38450;&#27490;&#30149;&#24773;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#26412;&#25991;&#21033;&#29992;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#38024;&#23545;&#26222;&#36890;&#35805;&#20351;&#29992;&#32773;&#22312;&#22270;&#29255;&#25551;&#36848;&#20219;&#21153;&#20013;&#30340;&#30196;&#21574;&#35780;&#20272;&#31995;&#32479;&#12290;&#36890;&#36807;&#22312;&#19982;&#30495;&#23454;&#19990;&#30028;&#24773;&#22659;&#38750;&#24120;&#30456;&#20284;&#30340;&#35821;&#38899;&#25968;&#25454;&#19978;&#35757;&#32451;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20174;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#25552;&#21462;&#32534;&#30721;&#22120;&#65292;&#24182;&#28155;&#21152;&#20102;&#19968;&#20010;&#32447;&#24615;&#23618;&#29992;&#20110;&#30196;&#21574;&#35780;&#20272;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;99&#21517;&#34987;&#35797;&#30340;&#26222;&#36890;&#35805;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#20174;&#24403;&#22320;&#21307;&#38498;&#33719;&#21462;&#20102;&#20182;&#20204;&#30340;&#20020;&#24202;&#35780;&#20272;&#25968;&#25454;&#12290;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;92.04%&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#20020;&#24202;&#30196;&#21574;&#35780;&#20998;&#39044;&#27979;&#20013;&#36798;&#21040;&#20102;9%&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dementia diagnosis requires a series of different testing methods, which is complex and time-consuming. Early detection of dementia is crucial as it can prevent further deterioration of the condition. This paper utilizes a speech recognition model to construct a dementia assessment system tailored for Mandarin speakers during the picture description task. By training an attention-based speech recognition model on voice data closely resembling real-world scenarios, we have significantly enhanced the model's recognition capabilities. Subsequently, we extracted the encoder from the speech recognition model and added a linear layer for dementia assessment. We collected Mandarin speech data from 99 subjects and acquired their clinical assessments from a local hospital. We achieved an accuracy of 92.04% in Alzheimer's disease detection and a mean absolute error of 9% in clinical dementia rating score prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20027;&#39064;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#25552;&#21319;HuBERT&#30340;&#35821;&#20041;&#34920;&#31034;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;&#36741;&#21161;&#20027;&#39064;&#20998;&#31867;&#20219;&#21153;&#65292;&#23558;&#20027;&#39064;&#26631;&#31614;&#20316;&#20026;&#25945;&#24072;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#30340;&#35821;&#20041;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03975</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20027;&#39064;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#25552;&#21319;HuBERT&#30340;&#35821;&#20041;&#34920;&#31034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model. (arXiv:2310.03975v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20027;&#39064;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#25552;&#21319;HuBERT&#30340;&#35821;&#20041;&#34920;&#31034;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;&#36741;&#21161;&#20027;&#39064;&#20998;&#31867;&#20219;&#21153;&#65292;&#23558;&#20027;&#39064;&#26631;&#31614;&#20316;&#20026;&#25945;&#24072;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#30340;&#35821;&#20041;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#24615;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#20854;&#20013;&#35768;&#22810;&#27169;&#22411;&#65292;&#22914;HuBERT&#21644;WavLM&#65292;&#20351;&#29992;&#20174;&#35889;&#29305;&#24449;&#25110;&#27169;&#22411;&#33258;&#36523;&#30340;&#34920;&#31034;&#29305;&#24449;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#12290;&#26681;&#25454;&#20197;&#21069;&#30340;&#30740;&#31350;&#65292;&#20266;&#26631;&#31614;&#21253;&#21547;&#35821;&#20041;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;HuBERT&#30340;&#23398;&#20064;&#20934;&#21017;&#36974;&#34109;&#39044;&#27979;&#20219;&#21153;&#19987;&#27880;&#20110;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#21487;&#33021;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22914;&#35828;&#35805;&#20154;&#12289;&#28436;&#35762;&#20027;&#39064;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20016;&#23500;HuBERT&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#20027;&#39064;&#27169;&#22411;&#24212;&#29992;&#20110;&#20266;&#26631;&#31614;&#65292;&#20026;&#27599;&#20010;&#35805;&#35821;&#29983;&#25104;&#19968;&#20010;&#20027;&#39064;&#26631;&#31614;&#12290;&#36890;&#36807;&#20351;&#29992;&#20027;&#39064;&#26631;&#31614;&#20316;&#20026;&#25945;&#24072;&#65292;&#21521;HuBERT&#28155;&#21152;&#20102;&#19968;&#20010;&#36741;&#21161;&#20027;&#39064;&#20998;&#31867;&#20219;&#21153;&#12290;&#36825;&#26679;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#34701;&#20837;&#26356;&#22810;&#30340;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21319;&#27169;&#22411;&#30340;&#35821;&#20041;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our meth
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23558;&#22823;&#35268;&#27169;Transformer&#27169;&#22411;&#36716;&#25442;&#20026;FlatBuffer&#26684;&#24335;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#22768;&#35465;&#20998;&#26512;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2310.03971</link><description>&lt;p&gt;
&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#37327;&#21270;Transformer&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Quantized Transformer Language Model Implementations on Edge Devices. (arXiv:2310.03971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03971
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23558;&#22823;&#35268;&#27169;Transformer&#27169;&#22411;&#36716;&#25442;&#20026;FlatBuffer&#26684;&#24335;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#22768;&#35465;&#20998;&#26512;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#22914;&#21452;&#21521;&#32534;&#30721;&#22120;Transformer&#65288;BERT&#65289;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#12290;&#36825;&#20123;&#27169;&#22411;&#39318;&#20808;&#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#23545;&#19979;&#28216;NLP&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#20123;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#23427;&#20204;&#19981;&#33021;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#65292;&#22240;&#20026;&#20854;&#27169;&#22411;&#22823;&#23567;&#36739;&#22823;&#19988;&#25512;&#29702;&#24310;&#36831;&#22686;&#21152;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#21487;&#20197;&#23558;&#36825;&#20123;&#22823;&#35268;&#27169;&#27169;&#22411;&#36716;&#25442;&#20026;&#20248;&#21270;&#30340;FlatBuffer&#26684;&#24335;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36716;&#25442;&#20026;FlatBuffer&#30340;MobileBERT&#27169;&#22411;&#22312;&#19977;&#31181;&#19981;&#21516;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#38024;&#23545;RepLab 2013&#25968;&#25454;&#38598;&#20013;&#33521;&#35821;&#25512;&#25991;&#30340;&#22768;&#35465;&#20998;&#26512;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#37096;&#32626;&#27169;&#22411;&#30340;&#24310;&#36831;&#65292;&#24615;&#33021;&#31561;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale transformer-based models like the Bidirectional Encoder Representations from Transformers (BERT) are widely used for Natural Language Processing (NLP) applications, wherein these models are initially pre-trained with a large corpus with millions of parameters and then fine-tuned for a downstream NLP task. One of the major limitations of these large-scale models is that they cannot be deployed on resource-constrained devices due to their large model size and increased inference latency. In order to overcome these limitations, such large-scale models can be converted to an optimized FlatBuffer format, tailored for deployment on resource-constrained edge devices. Herein, we evaluate the performance of such FlatBuffer transformed MobileBERT models on three different edge devices, fine-tuned for Reputation analysis of English language tweets in the RepLab 2013 dataset. In addition, this study encompassed an evaluation of the deployed models, wherein their latency, performance, a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24605;&#32500;&#20256;&#25773;&#65288;TP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#31867;&#27604;&#38382;&#39064;&#21644;&#21033;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03965</link><description>&lt;p&gt;
&#24605;&#32500;&#20256;&#25773;&#65306;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#26041;&#27861;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22797;&#26434;&#25512;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models. (arXiv:2310.03965v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24605;&#32500;&#20256;&#25773;&#65288;TP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#31867;&#27604;&#38382;&#39064;&#21644;&#21033;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#26080;&#27861;&#37325;&#29992;&#35299;&#20915;&#31867;&#20284;&#38382;&#39064;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#32047;&#31215;&#20102;&#38169;&#35823;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;LLMs&#20174;&#38646;&#24320;&#22987;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#20256;&#25773;&#8221;&#65288;TP&#65289;&#65292;&#23427;&#25506;&#32034;&#31867;&#20284;&#38382;&#39064;&#24182;&#21033;&#29992;&#23427;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;LLMs&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#31867;&#27604;&#38382;&#39064;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#65292;&#20855;&#26377;&#21487;&#37325;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#23558;&#35299;&#20915;&#20808;&#21069;&#31867;&#20284;&#38382;&#39064;&#30340;&#35265;&#35299;&#20256;&#25773;&#20197;&#28608;&#21457;&#26032;&#30340;&#38382;&#39064;&#35299;&#20915;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;TP&#39318;&#20808;&#25552;&#31034;LLMs&#25552;&#20986;&#24182;&#35299;&#20915;&#19968;&#32452;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#30340;&#31867;&#27604;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;TP&#37325;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#32467;&#26524;&#30452;&#25509;&#20135;&#29983;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#25110;&#32773;&#25512;&#23548;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textit{from scratch}. To address these issues, we propose \textbf{\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#65288;CoNLI&#65289;&#26469;&#26816;&#27979;&#21644;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.03951</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#29992;&#20110;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#26681;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations. (arXiv:2310.03951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03951
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#65288;CoNLI&#65289;&#26469;&#26816;&#27979;&#21644;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32473;&#23450;&#30456;&#20851;&#25991;&#26723;&#20316;&#20026;&#32972;&#26223;&#19978;&#19979;&#25991;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#29983;&#25104;&#27969;&#21033;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12290;&#36825;&#31181;&#33021;&#21147;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;LLMs&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;LLMs&#23481;&#26131;&#20135;&#29983;&#27809;&#26377;&#25552;&#20379;&#26469;&#28304;&#25903;&#25345;&#30340;&#24187;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#26469;&#26816;&#27979;&#21644;&#20943;&#23569;&#36825;&#31181;&#26080;&#26681;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38142;&#26465;&#65288;CoNLI&#65289;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#21518;&#26399;&#32534;&#36753;&#36827;&#34892;&#24187;&#35273;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24187;&#35273;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#37325;&#20889;&#22686;&#24378;&#25991;&#26412;&#36136;&#37327;&#65292;&#20351;&#29992;LLMs&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31616;&#21333;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#21487;&#20197;&#20316;&#20026;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#23569;&#30340;&#26377;&#25928;&#36873;&#25321;&#65292;&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can generate fluent natural language texts when given relevant documents as background context. This ability has attracted considerable interest in developing industry applications of LLMs. However, LLMs are prone to generate hallucinations that are not supported by the provided sources. In this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. Our framework uses Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing. Our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using LLMs without any fine-tuning or domain-specific prompt engineering. We show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#26597;COVID-19&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;CORToViz&#24037;&#20855;&#65292;&#24182;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#31185;&#23398;&#25991;&#31456;&#30340;&#20027;&#39064;&#25366;&#25496;&#21644;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.03928</link><description>&lt;p&gt;
&#25506;&#32034;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#30740;&#31350;&#20027;&#39064;&#30340;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Exploring the evolution of research topics during the COVID-19 pandemic. (arXiv:2310.03928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#26597;COVID-19&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;CORToViz&#24037;&#20855;&#65292;&#24182;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#31185;&#23398;&#25991;&#31456;&#30340;&#20027;&#39064;&#25366;&#25496;&#21644;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#22823;&#27969;&#34892;&#25913;&#21464;&#20102;&#22823;&#22810;&#25968;&#31185;&#23398;&#30028;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#23548;&#33268;&#22312;&#21307;&#23398;&#12289;&#30149;&#27602;&#23398;&#12289;&#27969;&#34892;&#30149;&#23398;&#12289;&#32463;&#27982;&#23398;&#12289;&#24515;&#29702;&#23398;&#31561;&#20247;&#22810;&#39046;&#22495;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#25991;&#31456;&#12290;&#24314;&#31435;&#20102;&#20960;&#20010;&#24320;&#25918;&#33719;&#21462;&#30340;&#35821;&#26009;&#24211;&#21644;&#25991;&#29486;&#38598;&#25955;&#22320;&#65292;&#20854;&#20013;COVID-19&#24320;&#25918;&#30740;&#31350;&#25968;&#25454;&#38598;&#65288;CORD-19&#65289;&#31995;&#32479;&#22320;&#25910;&#38598;&#20102;2.5&#24180;&#30340;&#31185;&#23398;&#36129;&#29486;&#65292;&#24635;&#20849;&#25910;&#38598;&#21644;&#32034;&#24341;&#20102;100&#22810;&#19975;&#31687;&#25991;&#31456;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;CORD-19&#20027;&#39064;&#21487;&#35270;&#21270;&#24037;&#20855;&#65288;CORToViz&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#26597;CORD-19&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#21644;&#30456;&#20851;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23545;&#26368;&#26032;&#25216;&#26415;&#65288;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#31934;&#24515;&#36873;&#25321;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19968;&#31181;&#29992;&#20110;&#27839;&#30528;&#27491;&#20132;&#32500;&#24230;&#32858;&#31867;&#25991;&#31456;&#21644;&#26102;&#38388;&#20027;&#39064;&#25366;&#25496;&#30340;&#26550;&#26500;&#12290;&#20027;&#39064;&#26816;&#26597;&#24471;&#21040;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#20202;&#34920;&#26495;&#30340;&#25903;&#25345;&#65292;&#25552;&#20379;&#20102;&#24555;&#36895;&#12289;&#19968;&#38190;&#24335;&#30340;&#21487;&#35270;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has changed the research agendas of most scientific communities, resulting in an overwhelming production of research articles in a variety of domains, including medicine, virology, epidemiology, economy, psychology, and so on. Several open-access corpora and literature hubs were established; among them, the COVID-19 Open Research Dataset (CORD-19) has systematically gathered scientific contributions for 2.5 years, by collecting and indexing over one million articles. Here, we present the CORD-19 Topic Visualizer (CORToViz), a method and associated visualization tool for inspecting the CORD-19 textual corpus of scientific abstracts. Our method is based upon a careful selection of up-to-date technologies (including large language models), resulting in an architecture for clustering articles along orthogonal dimensions and extraction techniques for temporal topic mining. Topic inspection is supported by an interactive dashboard, providing fast, one-click visualizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26234;&#33021;&#20307;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LLM-Co&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19977;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#35780;&#20272;LLMs&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;LLMs&#20855;&#26377;&#25512;&#26029;&#20249;&#20276;&#24847;&#22270;&#21644;&#29702;&#35299;&#20854;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03903</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Multi-Agent Coordination Abilities in Large Language Models. (arXiv:2310.03903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26234;&#33021;&#20307;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LLM-Co&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#19977;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#35780;&#20272;LLMs&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;LLMs&#20855;&#26377;&#25512;&#26029;&#20249;&#20276;&#24847;&#22270;&#21644;&#29702;&#35299;&#20854;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#33021;&#22815;&#29087;&#32451;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#12289;&#26377;&#25928;&#19982;&#20154;&#31867;&#21644;&#20854;&#20182;&#31995;&#32479;&#21512;&#20316;&#30340;&#26234;&#33021;&#20307;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#26174;&#33879;&#30340;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#35299;&#37322;&#35821;&#35328;&#30340;&#33021;&#21147;&#25104;&#20026;&#24320;&#21457;&#36825;&#31181;&#26234;&#33021;&#20307;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20351;&#29992;LLM&#26500;&#24314;&#30340;&#26234;&#33021;&#20307;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#21508;&#31181;&#21327;&#35843;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#21035;&#35774;&#35745;&#30340;LLM-Co&#26694;&#26550;&#65292;&#20351;LLM&#33021;&#22815;&#21442;&#19982;&#21327;&#35843;&#28216;&#25103;&#12290;&#36890;&#36807;LLM-Co&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#28216;&#25103;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23558;&#35780;&#20272;&#20998;&#20026;&#20116;&#20010;&#26041;&#38754;&#65306;&#24515;&#26234;&#29702;&#35770;&#12289;&#24773;&#22659;&#25512;&#29702;&#12289;&#25345;&#32493;&#21327;&#35843;&#12289;&#23545;&#21512;&#20316;&#20249;&#20276;&#30340;&#31283;&#20581;&#24615;&#21644;&#26126;&#30830;&#36741;&#21161;&#12290;&#39318;&#20808;&#65292;&#24515;&#26234;&#29702;&#35770;&#21644;&#24773;&#22659;&#25512;&#29702;&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;LLM&#25512;&#26029;&#20249;&#20276;&#24847;&#22270;&#21644;&#29702;&#35299;&#20854;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A pivotal aim in contemporary AI research is to develop agents proficient in multi-agent coordination, enabling effective collaboration with both humans and other systems. Large Language Models (LLMs), with their notable ability to understand, generate, and interpret language in a human-like manner, stand out as promising candidates for the development of such agents. In this study, we build and assess the effectiveness of agents crafted using LLMs in various coordination scenarios. We introduce the LLM-Coordination (LLM-Co) Framework, specifically designed to enable LLMs to play coordination games. With the LLM-Co framework, we conduct our evaluation with three game environments and organize the evaluation into five aspects: Theory of Mind, Situated Reasoning, Sustained Coordination, Robustness to Partners, and Explicit Assistance. First, the evaluation of the Theory of Mind and Situated Reasoning reveals the capabilities of LLM to infer the partner's intention and reason actions acco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#20102;&#22312;&#29616;&#26377;&#30340;&#35777;&#26126;&#21161;&#25163;&#20013;&#26500;&#24314;&#25903;&#25345;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#35268;&#33539;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32763;&#35793;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#21040;&#27491;&#24335;&#35268;&#33539;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03885</link><description>&lt;p&gt;
&#21487;&#20449;&#30340;&#27491;&#24335;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Formal Natural Language Specifications. (arXiv:2310.03885v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#20102;&#22312;&#29616;&#26377;&#30340;&#35777;&#26126;&#21161;&#25163;&#20013;&#26500;&#24314;&#25903;&#25345;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#35268;&#33539;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#32763;&#35793;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#21040;&#27491;&#24335;&#35268;&#33539;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#35777;&#26126;&#21161;&#25163;&#26159;&#31934;&#24515;&#26500;&#24314;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#65292;&#29992;&#20110;&#39640;&#24230;&#21487;&#20449;&#22320;&#26816;&#26597;&#20154;&#35774;&#35745;&#30340;&#25968;&#23398;&#35777;&#26126;&#30340;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#21482;&#39564;&#35777;&#20102;&#19968;&#20010;&#27491;&#24335;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#65292;&#35813;&#22768;&#26126;&#21487;&#33021;&#24050;&#32463;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#19968;&#20010;&#22768;&#26126;&#20013;&#34987;&#38169;&#35823;&#22320;&#32763;&#35793;&#12290;&#24403;&#20351;&#29992;&#35777;&#26126;&#21161;&#25163;&#22312;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#26041;&#38754;&#27491;&#24335;&#39564;&#35777;&#36719;&#20214;&#30340;&#27491;&#30830;&#24615;&#26102;&#65292;&#36825;&#23588;&#20854;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20174;&#38750;&#27491;&#24335;&#21040;&#27491;&#24335;&#30340;&#32763;&#35793;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#12289;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#38590;&#20197;&#23457;&#35745;&#20854;&#27491;&#30830;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#29616;&#26377;&#30340;&#35777;&#26126;&#21161;&#25163;&#20013;&#26500;&#24314;&#25903;&#25345;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#35268;&#33539;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#19982;&#24314;&#31435;&#35777;&#26126;&#21161;&#25163;&#33258;&#36523;&#30340;&#20449;&#20219;&#21644;&#21487;&#23457;&#35745;&#24615;&#30340;&#21407;&#21017;&#19968;&#33268;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#21487;&#27169;&#22359;&#21270;&#21487;&#25193;&#23637;&#30340;&#33521;&#35821;&#24418;&#24335;&#23376;&#38598;&#25552;&#20379;&#35268;&#33539;&#65292;&#24182;&#33258;&#21160;&#23558;&#20854;&#32763;&#35793;&#20026;&#27491;&#24335;&#30340;c
&lt;/p&gt;
&lt;p&gt;
Interactive proof assistants are computer programs carefully constructed to check a human-designed proof of a mathematical claim with high confidence in the implementation. However, this only validates truth of a formal claim, which may have been mistranslated from a claim made in natural language. This is especially problematic when using proof assistants to formally verify the correctness of software with respect to a natural language specification. The translation from informal to formal remains a challenging, time-consuming process that is difficult to audit for correctness.  This paper shows that it is possible to build support for specifications written in expressive subsets of natural language, within existing proof assistants, consistent with the principles used to establish trust and auditability in proof assistants themselves. We implement a means to provide specifications in a modularly extensible formal subset of English, and have them automatically translated into formal c
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#19987;&#27880;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#20154;&#26426;&#20132;&#20114;&#30340;&#26041;&#24335;&#65292;&#29983;&#25104;&#20462;&#35746;&#29256;&#30340;&#25991;&#26412;&#65292;&#20445;&#25345;&#21407;&#25991;&#21547;&#20041;&#21644;&#38271;&#24230;&#65292;&#24182;&#28385;&#36275;&#29305;&#23450;&#30340;&#26631;&#20934;&#21644;&#35821;&#35328;&#39118;&#26684;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.03878</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#21644;&#20154;&#26426;&#20132;&#20114;&#30340;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automatic and Human-AI Interactive Text Generation. (arXiv:2310.03878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#19987;&#27880;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#20154;&#26426;&#20132;&#20114;&#30340;&#26041;&#24335;&#65292;&#29983;&#25104;&#20462;&#35746;&#29256;&#30340;&#25991;&#26412;&#65292;&#20445;&#25345;&#21407;&#25991;&#21547;&#20041;&#21644;&#38271;&#24230;&#65292;&#24182;&#28385;&#36275;&#29305;&#23450;&#30340;&#26631;&#20934;&#21644;&#35821;&#35328;&#39118;&#26684;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#65292;&#19968;&#31867;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#65292;&#23427;&#20197;&#19968;&#27573;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#28982;&#21518;&#29983;&#25104;&#19968;&#20010;&#25353;&#29031;&#29305;&#23450;&#26631;&#20934;&#25913;&#36827;&#30340;&#20462;&#35746;&#29256;&#65288;&#20363;&#22914;&#21487;&#35835;&#24615;&#25110;&#35821;&#35328;&#39118;&#26684;&#65289;&#65292;&#21516;&#26102;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#21407;&#25991;&#30340;&#21547;&#20041;&#21644;&#38271;&#24230;&#12290;&#36825;&#21253;&#25324;&#35768;&#22810;&#26377;&#29992;&#30340;&#24212;&#29992;&#65292;&#22914;&#25991;&#26412;&#31616;&#21270;&#12289;&#25913;&#20889;&#29983;&#25104;&#12289;&#39118;&#26684;&#36716;&#31227;&#31561;&#12290;&#19982;&#25991;&#26412;&#25688;&#35201;&#21644;&#24320;&#25918;&#24335;&#25991;&#26412;&#23436;&#25104;&#65288;&#20363;&#22914;&#25925;&#20107;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#26412;&#25945;&#31243;&#20013;&#35752;&#35770;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#22312;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#30446;&#26631;&#35821;&#35328;&#39118;&#26684;&#26041;&#38754;&#26356;&#21152;&#32422;&#26463;&#12290;&#36825;&#31181;&#25511;&#21046;&#27700;&#24179;&#20351;&#24471;&#36825;&#20123;&#20219;&#21153;&#25104;&#20026;&#30740;&#31350;&#27169;&#22411;&#29983;&#25104;&#26082;&#35821;&#20041;&#24688;&#24403;&#21448;&#31526;&#21512;&#39118;&#26684;&#35201;&#27714;&#30340;&#25991;&#26412;&#33021;&#21147;&#30340;&#29702;&#24819;&#35797;&#39564;&#21488;&#12290;&#27492;&#22806;&#65292;&#20174;&#25216;&#26415;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#20219;&#21153;&#24456;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#22797;&#26434;&#30340;&#35789;&#27719;&#21644;&#21477;&#27861;&#36716;&#25442;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this tutorial, we focus on text-to-text generation, a class of natural language generation (NLG) tasks, that takes a piece of text as input and then generates a revision that is improved according to some specific criteria (e.g., readability or linguistic styles), while largely retaining the original meaning and the length of the text. This includes many useful applications, such as text simplification, paraphrase generation, style transfer, etc. In contrast to text summarization and open-ended text completion (e.g., story), the text-to-text generation tasks we discuss in this tutorial are more constrained in terms of semantic consistency and targeted language styles. This level of control makes these tasks ideal testbeds for studying the ability of models to generate text that is both semantically adequate and stylistically appropriate. Moreover, these tasks are interesting from a technical standpoint, as they require complex combinations of lexical and syntactical transformations,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25353;&#29031;AAPM TG-263&#26631;&#20934;&#37325;&#26032;&#26631;&#27880;&#32467;&#26500;&#21517;&#31216;&#30340;&#27010;&#24565;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#20379;&#26410;&#26469;&#30340;&#30740;&#31350;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2310.03874</link><description>&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#22522;&#20110;AAPM TG-263&#25253;&#21578;&#30340;&#22522;&#30784;LLM&#22312;&#37325;&#26032;&#26631;&#27880;&#32467;&#26500;&#21517;&#31216;&#26041;&#38754;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report. (arXiv:2310.03874v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25353;&#29031;AAPM TG-263&#26631;&#20934;&#37325;&#26032;&#26631;&#27880;&#32467;&#26500;&#21517;&#31216;&#30340;&#27010;&#24565;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#20379;&#26410;&#26469;&#30340;&#30740;&#31350;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#20171;&#32461;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25353;&#29031;&#32654;&#22269;&#21307;&#23398;&#29289;&#29702;&#23398;&#20250;&#65288;AAPM&#65289;&#20219;&#21153;&#32452;&#65288;TG&#65289;-263&#26631;&#20934;&#37325;&#26032;&#26631;&#27880;&#32467;&#26500;&#21517;&#31216;&#30340;&#27010;&#24565;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#24314;&#31435;&#22522;&#20934;&#21442;&#32771;&#12290;&#26041;&#27861;&#21644;&#26448;&#26009;&#65306;&#23454;&#29616;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;-4&#24212;&#29992;&#31243;&#24207;&#25509;&#21475;&#65288;API&#65289;&#20316;&#20026;&#25968;&#23383;&#24433;&#20687;&#19982;&#36890;&#20449;&#21307;&#23398;&#65288;DICOM&#65289;&#23384;&#20648;&#26381;&#21153;&#22120;&#65292;&#35813;&#26381;&#21153;&#22120;&#22312;&#25509;&#25910;&#21040;&#32467;&#26500;&#38598;DICOM&#25991;&#20214;&#26102;&#65292;&#25552;&#31034;GPT-4&#26681;&#25454;AAPM TG-263&#37325;&#26032;&#26631;&#27880;&#30446;&#26631;&#20307;&#31215;&#21644;&#27491;&#24120;&#32452;&#32455;&#30340;&#32467;&#26500;&#21517;&#31216;&#12290;&#36873;&#25321;&#20102;&#21069;&#21015;&#33146;&#12289;&#22836;&#39048;&#37096;&#21644;&#33016;&#37096;&#19977;&#20010;&#30142;&#30149;&#37096;&#20301;&#36827;&#34892;&#35780;&#20272;&#12290;&#23545;&#20110;&#27599;&#20010;&#30142;&#30149;&#37096;&#20301;&#31867;&#21035;&#65292;&#38543;&#26426;&#36873;&#25321;&#20102;150&#21517;&#24739;&#32773;&#36827;&#34892;&#25163;&#21160;&#35843;&#25972;&#25351;&#20196;&#25552;&#31034;&#65288;&#27599;&#25209;50&#21517;&#65289;&#21644;&#38543;&#26426;&#36873;&#25321;&#20102;50&#21517;&#24739;&#32773;&#36827;&#34892;&#35780;&#20272;&#12290;&#32771;&#34385;&#30340;&#32467;&#26500;&#21517;&#31216;&#26159;&#26368;&#26377;&#21487;&#33021;&#30456;&#20851;&#30340;&#21517;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To introduce the concept of using large language models (LLMs) to re-label structure names in accordance with the American Association of Physicists in Medicine (AAPM) Task Group (TG)-263 standard, and to establish a benchmark for future studies to reference.  Methods and Materials: The Generative Pre-trained Transformer (GPT)-4 application programming interface (API) was implemented as a Digital Imaging and Communications in Medicine (DICOM) storage server, which upon receiving a structure set DICOM file, prompts GPT-4 to re-label the structure names of both target volumes and normal tissues according to the AAPM TG-263. Three disease sites, prostate, head and neck, and thorax were selected for evaluation. For each disease site category, 150 patients were randomly selected for manually tuning the instructions prompt (in batches of 50) and 50 patients were randomly selected for evaluation. Structure names that were considered were those that were most likely to be relevant for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaKERMap&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;OM&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#20449;&#24687;&#25972;&#21512;&#21040;transformer&#20013;&#65292;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#24182;&#24212;&#29992;&#20110;&#26412;&#20307;&#21305;&#37197;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.03840</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#30340;&#32467;&#26500;&#21270;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#26412;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Contextualized Structural Self-supervised Learning for Ontology Matching. (arXiv:2310.03840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaKERMap&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;OM&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#20449;&#24687;&#25972;&#21512;&#21040;transformer&#20013;&#65292;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#24182;&#24212;&#29992;&#20110;&#26412;&#20307;&#21305;&#37197;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21305;&#37197;&#65288;OM&#65289;&#28041;&#21450;&#22312;&#20004;&#20010;&#25110;&#22810;&#20010;&#30693;&#35782;&#22270;&#20013;&#35782;&#21035;&#27010;&#24565;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#20316;&#20026;&#25972;&#21512;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#30693;&#35782;&#22270;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26368;&#36817;&#28145;&#24230;OM&#27169;&#22411;&#30340;&#36827;&#23637;&#24050;&#32463;&#21033;&#29992;&#20102;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;OM&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#30528;&#25345;&#32493;&#30340;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#21442;&#32771;&#23545;&#40784;&#12289;&#36816;&#34892;&#26102;&#24310;&#36831;&#21644;&#26410;&#24320;&#21457;&#30340;&#20869;&#37096;&#19981;&#21516;&#22270;&#32467;&#26500;&#31561;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;OM&#26694;&#26550;&#65292;&#21517;&#20026;LaKERMap&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#27010;&#24565;&#30340;&#19978;&#19979;&#25991;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#23558;&#38544;&#24335;&#30693;&#35782;&#25972;&#21512;&#21040;transformer&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#21253;&#25324;&#23616;&#37096;&#21644;&#20840;&#23616;&#20132;&#20114;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Bio-ML&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontology matching (OM) entails the identification of semantic relationships between concepts within two or more knowledge graphs (KGs) and serves as a critical step in integrating KGs from various sources. Recent advancements in deep OM models have harnessed the power of transformer-based language models and the advantages of knowledge graph embedding. Nevertheless, these OM models still face persistent challenges, such as a lack of reference alignments, runtime latency, and unexplored different graph structures within an end-to-end framework. In this study, we introduce a novel self-supervised learning OM framework with input ontologies, called LaKERMap. This framework capitalizes on the contextual and structural information of concepts by integrating implicit knowledge into transformers. Specifically, we aim to capture multiple structural contexts, encompassing both local and global interactions, by employing distinct training objectives. To assess our methods, we utilize the Bio-ML 
&lt;/p&gt;</description></item><item><title>HandMeThat&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#21644;&#31038;&#20132;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;10000&#20010;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22522;&#20934;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35813;&#22522;&#20934;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#26263;&#31034;&#20102;&#20854;&#20013;&#30340;&#25361;&#25112;&#21644;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.03779</link><description>&lt;p&gt;
HandMeThat: &#22312;&#29289;&#29702;&#21644;&#31038;&#20132;&#29615;&#22659;&#20013;&#30340;&#20154;&#26426;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
HandMeThat: Human-Robot Communication in Physical and Social Environments. (arXiv:2310.03779v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03779
&lt;/p&gt;
&lt;p&gt;
HandMeThat&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#21644;&#31038;&#20132;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;10000&#20010;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22522;&#20934;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35813;&#22522;&#20934;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#26263;&#31034;&#20102;&#20854;&#20013;&#30340;&#25361;&#25112;&#21644;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;HandMeThat&#65292;&#19968;&#20010;&#29992;&#20110;&#29289;&#29702;&#21644;&#31038;&#20132;&#29615;&#22659;&#20013;&#25351;&#20196;&#29702;&#35299;&#21644;&#36981;&#24490;&#30340;&#32508;&#21512;&#35780;&#20272;&#22522;&#20934;&#12290;&#19982;&#20808;&#21069;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#20851;&#27880;&#35821;&#35328;&#20381;&#23384;&#21644;&#35268;&#21010;&#19981;&#21516;&#65292;HandMeThat&#32771;&#34385;&#20102;&#22522;&#20110;&#29289;&#29702;&#65288;&#29289;&#20307;&#29366;&#24577;&#21644;&#20851;&#31995;&#65289;&#21644;&#31038;&#20132;&#65288;&#20154;&#31867;&#34892;&#21160;&#21644;&#30446;&#26631;&#65289;&#20449;&#24687;&#30340;&#21547;&#26377;&#27495;&#20041;&#30340;&#20154;&#31867;&#25351;&#20196;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;HandMeThat&#21253;&#21547;&#20102;10000&#20010;&#20154;&#26426;&#20132;&#20114;&#30340;&#22330;&#26223;&#12290;&#22312;&#27599;&#20010;&#22330;&#26223;&#20013;&#65292;&#26426;&#22120;&#20154;&#39318;&#20808;&#35266;&#23519;&#21040;&#20154;&#31867;&#34892;&#21160;&#30340;&#36712;&#36857;&#20197;&#36798;&#21040;&#20869;&#37096;&#30446;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#26426;&#22120;&#20154;&#25509;&#25910;&#21040;&#20154;&#31867;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#25351;&#20196;&#37319;&#21462;&#34892;&#21160;&#20197;&#23436;&#25104;&#23376;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#25991;&#26412;&#30028;&#38754;&#65292;&#26426;&#22120;&#20154;&#36890;&#36807;&#25991;&#26412;&#21629;&#20196;&#19982;&#34394;&#25311;&#29615;&#22659;&#20132;&#20114;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;HandMeThat&#19978;&#30340;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;HandMeThat&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#34920;&#26126;&#20854;&#20013;&#23384;&#22312;&#37325;&#35201;&#30340;&#25361;&#25112;&#21644;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HandMeThat, a benchmark for a holistic evaluation of instruction understanding and following in physical and social environments. While previous datasets primarily focused on language grounding and planning, HandMeThat considers the resolution of human instructions with ambiguities based on the physical (object states and relations) and social (human actions and goals) information. HandMeThat contains 10,000 episodes of human-robot interactions. In each episode, the robot first observes a trajectory of human actions towards her internal goal. Next, the robot receives a human instruction and should take actions to accomplish the subgoal set through the instruction. In this paper, we present a textual interface for our benchmark, where the robot interacts with a virtual environment through textual commands. We evaluate several baseline models on HandMeThat, and show that both offline and online reinforcement learning algorithms perform poorly on HandMeThat, suggesting signif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#24320;&#21457;&#38544;&#31169;&#20445;&#25252;&#30340;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#31169;&#23494;&#29615;&#22659;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#25991;&#26723;&#22522;&#30784;&#27169;&#22411;&#26469;&#33719;&#24471;&#36275;&#22815;&#30340;&#24615;&#33021;&#21644;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#36890;&#36807;&#20998;&#26512;&#21508;&#31181;&#35757;&#32451;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#23454;&#29616;&#26368;&#20339;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#30340;&#20934;&#21017;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DP-FL&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#20174;&#29420;&#31435;&#29615;&#22659;&#25193;&#23637;&#21040;&#22810;&#23458;&#25143;&#32852;&#21512;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2310.03777</link><description>&lt;p&gt;
PrIeD-KIE: &#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#25991;&#26723;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
PrIeD-KIE: Towards Privacy Preserved Document Key Information Extraction. (arXiv:2310.03777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#24320;&#21457;&#38544;&#31169;&#20445;&#25252;&#30340;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#31169;&#23494;&#29615;&#22659;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#25991;&#26723;&#22522;&#30784;&#27169;&#22411;&#26469;&#33719;&#24471;&#36275;&#22815;&#30340;&#24615;&#33021;&#21644;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#36890;&#36807;&#20998;&#26512;&#21508;&#31181;&#35757;&#32451;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#23454;&#29616;&#26368;&#20339;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#30340;&#20934;&#21017;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DP-FL&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#20174;&#29420;&#31435;&#29615;&#22659;&#25193;&#23637;&#21040;&#22810;&#23458;&#25143;&#32852;&#21512;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#25991;&#26723;&#22522;&#30784;&#27169;&#22411;&#19982;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#12289;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#65288;DP-FL&#65289;&#30456;&#32467;&#21512;&#30340;&#31574;&#30053;&#65292;&#24320;&#21457;&#38544;&#31169;&#20445;&#25252;&#30340;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#65288;KIE&#65289;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;FUNSD&#12289;CORD&#12289;SROIE&#12289;WildReceipts&#12289;XFUND&#21644;DOCILE&#65289;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#31169;&#23494;&#29615;&#22659;&#19979;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24494;&#35843;&#22823;&#22411;&#25991;&#26723;&#22522;&#30784;&#27169;&#22411;&#20197;&#23454;&#29616;&#36275;&#22815;&#30340;&#24615;&#33021;&#24182;&#20445;&#25345;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#21508;&#31181;&#35757;&#32451;&#21644;&#27169;&#22411;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#29616;KIE&#20219;&#21153;&#22312;&#20840;&#23616;DP&#19979;&#26368;&#20339;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20934;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FeAm-DP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;DP-FL&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#20840;&#23616;DP&#20174;&#29420;&#31435;&#29615;&#22659;&#26377;&#25928;&#22320;&#25193;&#23637;&#21040;&#22810;&#23458;&#25143;&#32852;&#21512;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce strategies for developing private Key Information Extraction (KIE) systems by leveraging large pretrained document foundation models in conjunction with differential privacy (DP), federated learning (FL), and Differentially Private Federated Learning (DP-FL). Through extensive experimentation on six benchmark datasets (FUNSD, CORD, SROIE, WildReceipts, XFUND, and DOCILE), we demonstrate that large document foundation models can be effectively fine-tuned for the KIE task under private settings to achieve adequate performance while maintaining strong privacy guarantees. Moreover, by thoroughly analyzing the impact of various training and model parameters on model performance, we propose simple yet effective guidelines for achieving an optimal privacy-utility trade-off for the KIE task under global DP. Finally, we introduce FeAm-DP, a novel DP-FL algorithm that enables efficiently upscaling global DP from a standalone context to a multi-client federated environ
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#20020;&#24202;&#35760;&#24405;&#34920;&#22411;&#30340;&#26367;&#20195;&#29305;&#24449;&#25552;&#21462;&#27969;&#31243;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#26041;&#27861;&#23558;&#20020;&#24202;&#35760;&#24405;&#36716;&#25442;&#20026;&#25991;&#26723;&#34920;&#31034;&#65292;&#24182;&#20256;&#20837;LSTM&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#20294;&#25910;&#25947;&#26102;&#38388;&#36739;&#38271;&#19988;&#19981;&#25903;&#25345;&#22686;&#37327;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.03772</link><description>&lt;p&gt;
&#25506;&#32034;&#20020;&#24202;&#35760;&#24405;&#34920;&#22411;&#30340;&#26367;&#20195;&#29305;&#24449;&#25552;&#21462;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Investigating Alternative Feature Extraction Pipelines For Clinical Note Phenotyping. (arXiv:2310.03772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03772
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#20020;&#24202;&#35760;&#24405;&#34920;&#22411;&#30340;&#26367;&#20195;&#29305;&#24449;&#25552;&#21462;&#27969;&#31243;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#26041;&#27861;&#23558;&#20020;&#24202;&#35760;&#24405;&#36716;&#25442;&#20026;&#25991;&#26723;&#34920;&#31034;&#65292;&#24182;&#20256;&#20837;LSTM&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#20294;&#25910;&#25947;&#26102;&#38388;&#36739;&#38271;&#19988;&#19981;&#25903;&#25345;&#22686;&#37327;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#34892;&#19994;&#24120;&#29992;&#30340;&#23454;&#36341;&#26159;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35814;&#32454;&#30340;&#24739;&#32773;&#35266;&#23519;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31995;&#32479;&#32463;&#24120;&#19981;&#20197;&#32467;&#26500;&#21270;&#26684;&#24335;&#21253;&#21547;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#20351;&#24471;&#24739;&#32773;&#20449;&#24687;&#38590;&#20197;&#33258;&#21160;&#35780;&#20272;&#21644;&#35780;&#20215;&#12290;&#20351;&#29992;&#35745;&#31639;&#31995;&#32479;&#26469;&#25552;&#21462;&#21307;&#23398;&#23646;&#24615;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#23545;&#24739;&#32773;&#30340;&#32437;&#21521;&#20998;&#26512;&#12289;&#39118;&#38505;&#35780;&#20272;&#21644;&#21307;&#38498;&#35780;&#20272;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#26500;&#24314;&#20102;&#25104;&#21151;&#30340;&#34920;&#22411;&#26041;&#27861;&#65306;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#21307;&#23398;&#23646;&#24615;&#12290;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#20020;&#24202;&#35760;&#24405;&#36716;&#25442;&#20026;&#19968;&#31995;&#21015;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#22522;&#20110;&#23427;&#20204;&#30340;CLS&#23884;&#20837;&#23558;&#20854;&#21387;&#32553;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#25991;&#26723;&#34920;&#31034;&#65292;&#24182;&#20256;&#20837;&#19968;&#20010;LSTM&#27169;&#22411;&#12290;&#23613;&#31649;&#36825;&#20010;&#27969;&#31243;&#30456;&#27604;&#20043;&#21069;&#30340;&#32467;&#26524;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#20294;&#23427;&#38656;&#35201;&#24456;&#38271;&#30340;&#25910;&#25947;&#26102;&#38388;&#12290;&#36825;&#20010;&#26041;&#27861;&#20063;&#19981;&#20801;&#35768;&#22686;&#37327;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common practice in the medical industry is the use of clinical notes, which consist of detailed patient observations. However, electronic health record systems frequently do not contain these observations in a structured format, rendering patient information challenging to assess and evaluate automatically. Using computational systems for the extraction of medical attributes offers many applications, including longitudinal analysis of patients, risk assessment, and hospital evaluation. Recent work has constructed successful methods for phenotyping: extracting medical attributes from clinical notes. BERT-based models can be used to transform clinical notes into a series of representations, which are then condensed into a single document representation based on their CLS embeddings and passed into an LSTM (Mulyar et al., 2020). Though this pipeline yields a considerable performance improvement over previous results, it requires extensive convergence time. This method also does not allo
&lt;/p&gt;</description></item><item><title>GoLLIE &#26159;&#19968;&#20010;&#36981;&#24490;&#27880;&#37322;&#25351;&#21335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20197;&#25913;&#36827;&#26410;&#35265;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03668</link><description>&lt;p&gt;
GoLLIE:&#27880;&#37322;&#25351;&#21335;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction. (arXiv:2310.03668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03668
&lt;/p&gt;
&lt;p&gt;
GoLLIE &#26159;&#19968;&#20010;&#36981;&#24490;&#27880;&#37322;&#25351;&#21335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20197;&#25913;&#36827;&#26410;&#35265;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#32467;&#21512;&#25351;&#23548;&#35843;&#20248;&#24050;&#32463;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462; (IE) &#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#33853;&#21518;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;IE &#20219;&#21153;&#30340;&#29305;&#28857;&#26159;&#22797;&#26434;&#30340;&#27880;&#37322;&#25351;&#21335;&#65292;&#25551;&#36848;&#20219;&#21153;&#24182;&#32473;&#20986;&#31034;&#20363;&#32473;&#20154;&#31867;&#12290;&#20808;&#21069;&#21033;&#29992;&#36825;&#26679;&#30340;&#20449;&#24687;&#30340;&#23581;&#35797;&#37117;&#22833;&#36133;&#20102;&#65292;&#21363;&#20351;&#20351;&#29992;&#26368;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#20063;&#19981;&#33021;&#30452;&#25509;&#36981;&#24490;&#25351;&#21335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20449;&#24687;&#25277;&#21462;&#30340;&#25351;&#21335;&#36981;&#24490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GoLLIE (Guideline-following Large Language Model for IE)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24494;&#35843;&#20197;&#36981;&#23432;&#27880;&#37322;&#25351;&#21335;&#65292;&#20174;&#32780;&#33021;&#22815;&#25913;&#36827;&#26410;&#35265; IE &#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#12290;&#20840;&#38754;&#30340;&#35780;&#20272;&#23454;&#35777;&#34920;&#26126;&#65292;GoLLIE &#33021;&#22815;&#27867;&#21270;&#24182;&#36981;&#24490;&#26410;&#35265;&#25351;&#21335;&#65292;&#22312;&#38646;&#26679;&#26412;&#20449;&#24687;&#25277;&#21462;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#23581;&#35797;&#12290;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#35814;&#32454;&#30340;&#25351;&#21335;&#26159;&#21462;&#24471;&#33391;&#22909;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out-of-the-box. In this paper we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines is key for good results.
&lt;/p&gt;</description></item><item><title>&#26412;&#22577;&#21578;&#25552;&#20379;&#20102;&#21271;&#31995;&#32113;&#30340;&#31777;&#35201;&#27010;&#36848;&#65292;&#35442;&#31995;&#32113;&#26088;&#22312;&#23526;&#29694;&#23565;&#21488;&#28771;&#23458;&#23478;&#35486;&#65288;&#22235;&#32291;&#33108;&#65289;&#30340;&#33258;&#21205;&#35422;/&#38899;&#31680;&#35672;&#21029;&#12290;&#38364;&#37749;&#37096;&#20998;&#21253;&#25324;&#35347;&#32244;&#25976;&#25818;&#30340;&#29554;&#21462;&#12289;&#32068;&#25104;&#21644;&#21033;&#29992;&#65292;&#27169;&#22411;&#30340;&#26550;&#27083;&#65292;&#20197;&#21450;&#30828;&#20214;&#35215;&#26684;&#21644;&#36939;&#34892;&#32113;&#35336;&#12290;</title><link>http://arxiv.org/abs/2310.03443</link><description>&lt;p&gt;
2023&#24180;&#31119;&#29246;&#25705;&#27801;&#21475;&#38899;&#36776;&#35672;&#25361;&#25136;&#20013;&#30340;&#21271;&#31995;&#32113;
&lt;/p&gt;
&lt;p&gt;
The North System for Formosa Speech Recognition Challenge 2023. (arXiv:2310.03443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#22577;&#21578;&#25552;&#20379;&#20102;&#21271;&#31995;&#32113;&#30340;&#31777;&#35201;&#27010;&#36848;&#65292;&#35442;&#31995;&#32113;&#26088;&#22312;&#23526;&#29694;&#23565;&#21488;&#28771;&#23458;&#23478;&#35486;&#65288;&#22235;&#32291;&#33108;&#65289;&#30340;&#33258;&#21205;&#35422;/&#38899;&#31680;&#35672;&#21029;&#12290;&#38364;&#37749;&#37096;&#20998;&#21253;&#25324;&#35347;&#32244;&#25976;&#25818;&#30340;&#29554;&#21462;&#12289;&#32068;&#25104;&#21644;&#21033;&#29992;&#65292;&#27169;&#22411;&#30340;&#26550;&#27083;&#65292;&#20197;&#21450;&#30828;&#20214;&#35215;&#26684;&#21644;&#36939;&#34892;&#32113;&#35336;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#22577;&#21578;&#25552;&#20379;&#20102;&#21271;&#31995;&#32113;&#30340;&#31777;&#35201;&#27010;&#36848;&#65292;&#26088;&#22312;&#23526;&#29694;&#23565;&#21488;&#28771;&#23458;&#23478;&#35486;&#65288;&#22235;&#32291;&#33108;&#65289;&#30340;&#33258;&#21205;&#35422;/&#38899;&#31680;&#35672;&#21029;&#12290;&#35442;&#22577;&#21578;&#27010;&#36848;&#20102;&#31995;&#32113;&#30340;&#19977;&#20491;&#38364;&#37749;&#37096;&#20998;&#65306;&#35347;&#32244;&#25976;&#25818;&#30340;&#29554;&#21462;&#12289;&#32068;&#25104;&#21644;&#21033;&#29992;&#65307;&#27169;&#22411;&#30340;&#26550;&#27083;&#65307;&#20197;&#21450;&#30828;&#20214;&#35215;&#26684;&#21644;&#36939;&#34892;&#32113;&#35336;&#12290;&#31995;&#32113;&#30340;&#28436;&#31034;&#21487;&#20197;&#22312;https://asrvm.iis.sinica.edu.tw/hakka_sixian&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report provides a concise overview of the proposed North system, which aims to achieve automatic word/syllable recognition for Taiwanese Hakka (Sixian). The report outlines three key components of the system: the acquisition, composition, and utilization of the training data; the architecture of the model; and the hardware specifications and operational statistics. The demonstration of the system can be found at https://asrvm.iis.sinica.edu.tw/hakka_sixian.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#35770;&#25991;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#35780;&#23457;&#20154;&#21592;&#30340;&#31034;&#20363;&#35780;&#20215;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.03304</link><description>&lt;p&gt;
&#23398;&#20064;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Learning Personalized Story Evaluation. (arXiv:2310.03304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#35770;&#25991;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#35780;&#23457;&#20154;&#21592;&#30340;&#31034;&#20363;&#35780;&#20215;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#26816;&#32034;&#31561;&#26356;&#23458;&#35266;&#30340;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#35780;&#20272;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#34920;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#21253;&#25324;&#65288;1&#65289;&#25968;&#25454;&#27745;&#26579;&#65307;&#65288;2&#65289;&#22810;&#32500;&#35780;&#20272;&#26631;&#20934;&#65307;&#20197;&#21450;&#65288;3&#65289;&#26469;&#33258;&#35780;&#23457;&#20154;&#21592;&#20010;&#20154;&#20559;&#22909;&#30340;&#20027;&#35266;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#19968;&#20010;&#26080;&#27745;&#26579;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#35780;&#20272;&#20013;&#24314;&#27169;&#20010;&#24615;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24403;&#30340;&#21311;&#21517;&#21270;&#21644;&#26032;&#30340;&#20010;&#24615;&#21270;&#26631;&#31614;&#65292;&#37325;&#26032;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Per-MPST&#21644;Per-DOC&#29992;&#20110;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#27169;&#22411;PERSE&#26469;&#25512;&#27979;&#35780;&#23457;&#20154;&#21592;&#30340;&#20559;&#22909;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#26576;&#20010;&#35780;&#23457;&#20154;&#21592;&#30340;&#19968;&#20123;&#31034;&#20363;&#35780;&#20215;&#65292;PERSE&#21487;&#20197;&#39044;&#27979;&#35813;&#35780;&#23457;&#20154;&#21592;&#22312;&#26032;&#30340;&#24773;&#33410;&#19978;&#30340;&#35814;&#32454;&#35780;&#23457;&#25110;&#32454;&#31890;&#24230;&#27604;&#36739;&#65288;&#22914;&#36259;&#21619;&#24615;&#21644;&#24778;&#21916;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have shown impressive results for more objective tasks such as QA and retrieval, it remains nontrivial to evaluate their performance on open-ended text generation for reasons including (1) data contamination; (2) multi-dimensional evaluation criteria; and (3) subjectiveness stemming from reviewers' personal preferences. To address such issues, we propose to model personalization in an uncontaminated open-ended generation assessment. We create two new datasets Per-MPST and Per-DOC for personalized story evaluation, by re-purposing existing datasets with proper anonymization and new personalized labels. We further develop a personalized story evaluation model PERSE to infer reviewer preferences and provide a personalized evaluation. Specifically, given a few exemplary reviews from a particular reviewer, PERSE predicts either a detailed review or fine-grained comparison in several aspects (such as interestingness and surprise) for that reviewer on a new 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;LLM&#22312;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65292;&#29992;&#20110;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#30340;&#38656;&#27714;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#23545;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#37117;&#19981;&#21463;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02842</link><description>&lt;p&gt;
&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#25195;&#25551;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation. (arXiv:2310.02842v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;LLM&#22312;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65292;&#29992;&#20110;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#30340;&#38656;&#27714;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#23545;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#37117;&#19981;&#21463;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26377;&#33021;&#21147;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#25688;&#35201;&#21644;&#25968;&#23398;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#26159;&#38024;&#23545;&#21333;&#19968;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#24403;&#21069;&#36235;&#21183;&#26159;&#20351;&#29992;&#25552;&#31034;&#25351;&#23548;&#35843;&#33410;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#20197;&#36866;&#24212;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#25193;&#23637;&#25552;&#31034;&#35843;&#33410;&#20197;&#21516;&#26102;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26159;&#19968;&#20010;&#24191;&#27867;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;"&#28151;&#21512;&#25552;&#31034;"&#25110;MoPs&#65292;&#24182;&#32467;&#21512;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65306;&#21518;&#32773;&#30340;&#35774;&#35745;&#26159;&#26412;&#25991;&#30340;&#36129;&#29486;&#20043;&#19968;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;(&#21363;&#19968;&#32452;&#25552;&#31034;)&#12290;&#27492;&#22806;&#65292;MoPs&#22312;&#24212;&#29992;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#26102;&#37117;&#19981;&#21463;&#24433;&#21709;&#8212;&#8212;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have the ability to solve a variety of tasks, such as text summarization and mathematical questions, just out of the box, but they are often trained with a single task in mind. Due to high computational costs, the current trend is to use prompt instruction tuning to better adjust monolithic, pretrained LLMs for new -- but often individual -- downstream tasks. Thus, how one would expand prompt tuning to handle -- concomitantly -heterogeneous tasks and data distributions is a widely open question. To address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs, associated with smart gating functionality: the latter -- whose design is one of the contributions of this paper -- can identify relevant skills embedded in different groups of prompts and dynamically assign combined experts (i.e., collection of prompts), based on the target task. Additionally, MoPs are empirically agnostic to any model compression technique applied -- for efficiency re
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#20010;&#21035;&#30340;&#27979;&#35797;&#36755;&#20837;&#37325;&#26032;&#20889;&#20316;&#20219;&#21153;&#25552;&#31034;&#65292;&#20351;&#20854;&#26356;&#20855;&#20307;&#12289;&#26126;&#30830;&#21644;&#23436;&#25972;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25351;&#23548;&#65292;&#23454;&#29616;&#20102;&#32422;10%&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.02107</link><description>&lt;p&gt;
&#23454;&#20363;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#30340;&#20851;&#24576;&#65306;&#20026;&#23454;&#20363;&#37325;&#20889;&#25552;&#31034;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance. (arXiv:2310.02107v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02107
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#20010;&#21035;&#30340;&#27979;&#35797;&#36755;&#20837;&#37325;&#26032;&#20889;&#20316;&#20219;&#21153;&#25552;&#31034;&#65292;&#20351;&#20854;&#26356;&#20855;&#20307;&#12289;&#26126;&#30830;&#21644;&#23436;&#25972;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25351;&#23548;&#65292;&#23454;&#29616;&#20102;&#32422;10%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#19968;&#30452;&#26159;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#33410;&#30465;&#20154;&#21147;&#65288;&#21363;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#30340;&#27880;&#37322;&#65289;&#65307;&#22240;&#27492;&#65292;&#38646;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#20063;&#20139;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#30528;&#37325;&#20110;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#20219;&#21153;&#25351;&#23548;&#65288;&#20363;&#22914;&#8220;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#24605;&#32771;&#8221;&#65289;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#27491;&#30830;&#35299;&#20915;&#38382;&#39064;&#65292;&#27599;&#20010;&#21333;&#29420;&#30340;&#27979;&#35797;&#23454;&#20363;&#38656;&#35201;&#26356;&#20180;&#32454;&#22320;&#35774;&#35745;&#21644;&#23450;&#21046;&#30340;&#25351;&#23548;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PRoMPTd&#65292;&#19968;&#31181;&#20026;&#27599;&#20010;&#20010;&#21035;&#30340;&#27979;&#35797;&#36755;&#20837;&#37325;&#26032;&#20889;&#20316;&#20219;&#21153;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#26356;&#21152;&#20855;&#20307;&#12289;&#26126;&#30830;&#21644;&#23436;&#25972;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4&#20316;&#20026;&#20219;&#21153;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#28085;&#30422;&#31639;&#26415;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#31561;&#20219;&#21153;&#30340;8&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;PRoMPTd&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PRoMPTd&#22312;&#22797;&#26434;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#32477;&#23545;&#25913;&#36827;&#32422;&#20026;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling large language models (LLMs) to perform tasks in zero-shot has been an appealing goal owing to its labor-saving (i.e., requiring no task-specific annotations); as such, zero-shot prompting approaches also enjoy better task generalizability. To improve LLMs' zero-shot performance, prior work has focused on devising more effective task instructions (e.g., ``let's think step by step'' ). However, we argue that, in order for an LLM to solve them correctly in zero-shot, individual test instances need more carefully designed and customized instructions. To this end, we propose PRoMPTd, an approach that rewrites the task prompt for each individual test input to be more specific, unambiguous, and complete, so as to provide better guidance to the task LLM. We evaluated PRoMPTd on eight datasets covering tasks including arithmetics, logical reasoning, and code generation, using GPT-4 as the task LLM. Notably, PRoMPTd achieves an absolute improvement of around 10% on the complex MATH dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01320</link><description>&lt;p&gt;
Avalon&#30340;&#24605;&#32771;&#28216;&#25103;&#65306;&#36890;&#36807;&#36882;&#24402;&#24605;&#32771;&#23545;&#25239;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation. (arXiv:2310.01320v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#22312;LLM&#20316;&#20026;&#26234;&#33021;&#20307;&#39046;&#22495;&#30340;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26222;&#36941;&#30340;&#20551;&#35774;&#26159;LLM&#22788;&#29702;&#30340;&#20449;&#24687;&#22987;&#32456;&#26159;&#35802;&#23454;&#30340;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#31038;&#20250;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#27450;&#39575;&#25110;&#35823;&#23548;&#24615;&#20449;&#24687;&#12290;&#36825;&#20010;&#30095;&#24573;&#20351;&#24471;LLM&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25805;&#32437;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21033;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#25506;&#32034;LLM&#22312;&#27450;&#39575;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#12290;Avalon&#20805;&#28385;&#20102;&#38169;&#35823;&#20449;&#24687;&#65292;&#24182;&#38656;&#35201;&#22797;&#26434;&#30340;&#36923;&#36753;&#65292;&#34920;&#29616;&#20026;&#8220;&#24605;&#32771;&#30340;&#28216;&#25103;&#8221;&#12290;&#21463;&#21040;&#20154;&#31867;&#22312;Avalon&#28216;&#25103;&#20013;&#36882;&#24402;&#24605;&#32771;&#21644;&#36879;&#35270;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#65292;&#20197;&#22686;&#24378;LLM&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;ReCon&#32467;&#21512;&#20102;&#20844;&#24335;&#21270;&#24605;&#32771;&#21644;&#23436;&#21892;&#24605;&#32771;&#30340;&#36807;&#31243;&#65307;&#20844;&#24335;&#21270;&#24605;&#32771;&#20135;&#29983;&#21021;&#22987;&#24605;&#32771;&#65292;&#23436;&#21892;&#24605;&#32771;&#23545;&#21021;&#22987;&#24605;&#32771;&#36827;&#34892;&#35843;&#25972;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial tho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#35753;&#35821;&#35328;&#27169;&#22411;&#38544;&#24335;&#23398;&#20064;&#33258;&#25105;&#25913;&#36827;&#65292;&#24182;&#20943;&#23569;&#23545;&#20154;&#31867;&#26631;&#27880;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2310.00898</link><description>&lt;p&gt;
&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#38544;&#24335;&#23398;&#20064;&#33258;&#25105;&#25913;&#36827;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enable Language Models to Implicitly Learn Self-Improvement From Data. (arXiv:2310.00898v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00898
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#35753;&#35821;&#35328;&#27169;&#22411;&#38544;&#24335;&#23398;&#20064;&#33258;&#25105;&#25913;&#36827;&#65292;&#24182;&#20943;&#23569;&#23545;&#20154;&#31867;&#26631;&#27880;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#26412;&#36136;&#20915;&#23450;&#20102;&#27169;&#22411;&#30340;&#22238;&#31572;&#36136;&#37327;&#22987;&#32456;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#38598;&#20013;&#22312;&#20351;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#25913;&#36827;&#20854;&#22238;&#31572;&#36136;&#37327;&#19978;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;&#24191;&#27867;&#30340;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#26469;&#25910;&#38598;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#22240;&#20854;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#20415;&#21033;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26126;&#30830;&#21644;&#35814;&#23613;&#30340;&#25351;&#31034;&#12290;&#23545;&#20110;&#25163;&#21160;&#25512;&#23548;&#21644;&#25552;&#20379;&#25152;&#26377;&#24517;&#35201;&#30340;&#25351;&#31034;&#26469;&#23454;&#29616;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#30446;&#26631;&#30340;&#25913;&#36827;&#65288;&#20363;&#22914;&#65292;&#26356;&#26377;&#24110;&#21161;&#24615;&#21644;&#26356;&#23569;&#26377;&#23475;&#24615;&#65289;&#65292;&#36825;&#26159;&#26114;&#36149;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful). To this end, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.00785</link><description>&lt;p&gt;
BooookScore: LLM&#26102;&#20195;&#20013;&#23545;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#30340;&#31995;&#32479;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
BooookScore: A systematic exploration of book-length summarization in the era of LLMs. (arXiv:2310.00785v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36229;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#38271;&#24230;&#25991;&#26723;&#65288;&gt;100K&#26631;&#35760;&#65289;&#36827;&#34892;&#25688;&#35201;&#38656;&#35201;&#39318;&#20808;&#23558;&#36755;&#20837;&#25991;&#26723;&#20998;&#25104;&#36739;&#23567;&#30340;&#22359;&#65292;&#28982;&#21518;&#25552;&#31034;LLM&#21512;&#24182;&#12289;&#26356;&#26032;&#21644;&#21387;&#32553;&#22359;&#32423;&#25688;&#35201;&#12290;&#23613;&#31649;&#36825;&#20010;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#35780;&#20272;&#30340;&#22256;&#38590;&#65292;&#23427;&#23578;&#26410;&#24471;&#21040;&#26377;&#24847;&#20041;&#30340;&#30740;&#31350;&#65306;&#29616;&#26377;&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;BookSum&#65289;&#22312;&#22823;&#22810;&#25968;&#20844;&#20849;LLM&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#32780;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#29616;&#20195;LLM&#25688;&#35201;&#22120;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#30340;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#65306;&#65288;1&#65289;&#20998;&#23618;&#21512;&#24182;&#22359;&#32423;&#25688;&#35201;&#65292;&#65288;2&#65289;&#36880;&#27493;&#26356;&#26032;&#19968;&#20010;&#36816;&#34892;&#25688;&#35201;&#12290;&#25105;&#20204;&#23545;100&#26412;&#26368;&#36817;&#20986;&#29256;&#30340;&#20070;&#31821;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#33719;&#24471;&#20102;1193&#20010;&#32454;&#31890;&#24230;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#30830;&#23450;&#20102;LLMs&#20135;&#29983;&#30340;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing book-length documents (&gt;100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Bec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.00100</link><description>&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;--&#25688;&#35201;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65281;
&lt;/p&gt;
&lt;p&gt;
Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21360;&#35937;&#37096;&#20998;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#22312;&#21521;&#21307;&#29983;&#20256;&#36798;&#36825;&#20123;&#21457;&#29616;&#26102;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#26469;&#35828;&#65292;&#20934;&#22791;&#36825;&#20123;&#25688;&#35201;&#26082;&#32791;&#26102;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#33021;&#22815;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#24635;&#32467;&#36825;&#20123;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#33258;&#21160;&#21270;&#22320;&#29983;&#25104;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#25918;&#23556;&#23398;&#21360;&#35937;&#65292;&#20197;&#24635;&#32467;&#33521;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#24503;&#35821;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#21457;&#29616;&#12290;&#22312;&#19968;&#39033;&#30450;&#27979;&#20013;&#65292;&#20004;&#20301;&#26377;&#25191;&#19994;&#36164;&#26684;&#30340;&#25918;&#23556;&#31185;&#21307;&#29983;&#34920;&#31034;&#65292;&#23545;&#20110;&#33267;&#23569;70%&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#20854;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
The impression section of a radiology report summarizes important radiology findings and plays a critical role in communicating these findings to physicians. However, the preparation of these summaries is time-consuming and error-prone for radiologists. Recently, numerous models for radiology report summarization have been developed. Nevertheless, there is currently no model that can summarize these reports in multiple languages. Such a model could greatly improve future research and the development of Deep Learning models that incorporate data from patients with different ethnic backgrounds. In this study, the generation of radiology impressions in different languages was automated by fine-tuning a model, publicly available, based on a multilingual text-to-text Transformer to summarize findings available in English, Portuguese, and German radiology reports. In a blind test, two board-certified radiologists indicated that for at least 70% of the system-generated summaries, the quality 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.17255</link><description>&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#22270;&#35889;&#65306;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#26159;&#30740;&#31350;&#29983;&#29289;&#21644;&#29983;&#21629;&#36807;&#31243;&#30340;&#23398;&#31185;&#65292;&#21253;&#25324;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#21644;&#19968;&#31995;&#21015;&#20854;&#20182;&#30456;&#20851;&#23398;&#31185;&#12290;&#29983;&#21629;&#31185;&#23398;&#30340;&#30740;&#31350;&#24037;&#20316;&#38750;&#24120;&#20381;&#36182;&#25968;&#25454;&#65292;&#22240;&#20026;&#23427;&#20204;&#20135;&#29983;&#21644;&#28040;&#36153;&#22823;&#37327;&#31185;&#23398;&#25968;&#25454;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20855;&#26377;&#20851;&#31995;&#21644;&#22270;&#32467;&#26500;&#12290;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#20854;&#20013;&#28041;&#21450;&#30340;&#31185;&#23398;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#25512;&#21160;&#20102;&#24212;&#29992;&#20808;&#36827;&#30340;&#30693;&#35782;&#39537;&#21160;&#25216;&#26415;&#26469;&#31649;&#29702;&#21644;&#35299;&#37322;&#25968;&#25454;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#25512;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#21644;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#22312;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20027;&#39064;&#65306;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#31649;&#29702;&#65292;&#20197;&#21450;&#22312;&#26032;&#21457;&#29616;&#30340;&#36807;&#31243;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#30456;&#20851;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term life sciences refers to the disciplines that study living organisms and life processes, and include chemistry, biology, medicine, and a range of other related disciplines. Research efforts in life sciences are heavily data-driven, as they produce and consume vast amounts of scientific data, much of which is intrinsically relational and graph-structured.  The volume of data and the complexity of scientific concepts and relations referred to therein promote the application of advanced knowledge-driven technologies for managing and interpreting data, with the ultimate aim to advance scientific discovery.  In this survey and position paper, we discuss recent developments and advances in the use of graph-based technologies in life sciences and set out a vision for how these technologies will impact these fields into the future. We focus on three broad topics: the construction and management of Knowledge Graphs (KGs), the use of KGs and associated technologies in the discovery of ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24335;&#35821;&#38899;&#32423;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#23618;&#35299;&#37322;&#26041;&#27861;&#35299;&#20915;&#20102;&#33258;&#21160;&#25233;&#37057;&#30151;&#26816;&#27979;&#24037;&#20855;&#20013;&#30340;&#26631;&#27880;&#22122;&#22768;&#21644;&#27169;&#22411;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#20854;&#22312;&#24615;&#33021;&#19978;&#30340;&#20248;&#21183;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26799;&#24230;&#21152;&#26435;&#27880;&#24847;&#21147;&#22270;&#36861;&#36394;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#20379;&#35821;&#38899;&#32423;&#21644;&#21477;&#23376;&#32423;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.13476</link><description>&lt;p&gt;
&#20998;&#23618;&#27880;&#24847;&#35299;&#37322;&#65306;&#19968;&#31181;&#29992;&#20110;&#21452;&#27169;&#24335;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#35821;&#38899;&#32423;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hierarchical attention interpretation: an interpretable speech-level transformer for bi-modal depression detection. (arXiv:2309.13476v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24335;&#35821;&#38899;&#32423;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#23618;&#35299;&#37322;&#26041;&#27861;&#35299;&#20915;&#20102;&#33258;&#21160;&#25233;&#37057;&#30151;&#26816;&#27979;&#24037;&#20855;&#20013;&#30340;&#26631;&#27880;&#22122;&#22768;&#21644;&#27169;&#22411;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#20854;&#22312;&#24615;&#33021;&#19978;&#30340;&#20248;&#21183;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26799;&#24230;&#21152;&#26435;&#27880;&#24847;&#21147;&#22270;&#36861;&#36394;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#20379;&#35821;&#38899;&#32423;&#21644;&#21477;&#23376;&#32423;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#24515;&#29702;&#38556;&#30861;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#30340;&#35821;&#38899;&#33258;&#21160;&#25233;&#37057;&#30151;&#26816;&#27979;&#24037;&#20855;&#26377;&#21161;&#20110;&#26089;&#26399;&#31579;&#26597;&#25233;&#37057;&#30151;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#31867;&#24037;&#20855;&#21487;&#33021;&#23384;&#22312;&#30340;&#20004;&#20010;&#38480;&#21046;&#36827;&#34892;&#20102;&#25506;&#35752;&#65306;&#30001;&#20998;&#27573;&#32423;&#26631;&#27880;&#23548;&#33268;&#30340;&#22122;&#22768;&#21644;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24335;&#35821;&#38899;&#32423;&#21464;&#25442;&#22120;&#26469;&#36991;&#20813;&#20998;&#27573;&#32423;&#26631;&#27880;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#35299;&#37322;&#26041;&#27861;&#65292;&#26681;&#25454;&#20174;&#25152;&#26377;&#27880;&#24847;&#21147;&#23618;&#23548;&#20986;&#30340;&#26799;&#24230;&#21152;&#26435;&#27880;&#24847;&#21147;&#22270;&#26469;&#36861;&#36394;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#20197;&#25552;&#20379;&#35821;&#38899;&#32423;&#21644;&#21477;&#23376;&#32423;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#22312;&#20998;&#27573;&#32423;&#23398;&#20064;&#30340;&#27169;&#22411;&#65288;$p$=0.854, $r$=0.947, $F1$=0.897&#65292;&#19982;$p$=0.732, $r$=0.808, $F1$=0.768&#30456;&#27604;&#65289;&#12290;&#22312;&#27169;&#22411;&#35299;&#37322;&#26041;&#38754;&#65292;&#20351;&#29992;&#19968;&#20010;&#30495;&#23454;&#38451;&#24615;&#26679;&#26412;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21738;&#20123;&#21477;&#23376;&#23545;&#20110;&#25233;&#37057;&#30151;&#26816;&#27979;&#30456;&#20851;&#24615;&#26368;&#39640;&#65292;&#20197;&#21450;&#21738;&#20123;&#25991;&#26412;&#26631;&#35760;&#21644;Mel&#22768;&#35889;&#22270;&#19982;&#20043;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a common mental disorder. Automatic depression detection tools using speech, enabled by machine learning, help early screening of depression. This paper addresses two limitations that may hinder the clinical implementations of such tools: noise resulting from segment-level labelling and a lack of model interpretability. We propose a bi-modal speech-level transformer to avoid segment-level labelling and introduce a hierarchical interpretation approach to provide both speech-level and sentence-level interpretations, based on gradient-weighted attention maps derived from all attention layers to track interactions between input features. We show that the proposed model outperforms a model that learns at a segment level ($p$=0.854, $r$=0.947, $F1$=0.897 compared to $p$=0.732, $r$=0.808, $F1$=0.768). For model interpretation, using one true positive sample, we show which sentences within a given speech are most relevant to depression detection; and which text tokens and Mel-spe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28431;&#27934;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#38459;&#27490;&#29983;&#25104;&#28431;&#27934;&#26631;&#35760;&#65292;&#20197;&#39640;&#25928;&#20943;&#23569;&#33258;&#21160;&#23436;&#25104;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;&#20197;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09826</link><description>&lt;p&gt;
&#20351;&#29992;&#28431;&#27934;&#32422;&#26463;&#35299;&#30721;&#26469;&#39640;&#25928;&#36991;&#20813;&#33258;&#21160;&#23436;&#25104;&#26234;&#33021;&#21512;&#32422;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding. (arXiv:2309.09826v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28431;&#27934;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#38459;&#27490;&#29983;&#25104;&#28431;&#27934;&#26631;&#35760;&#65292;&#20197;&#39640;&#25928;&#20943;&#23569;&#33258;&#21160;&#23436;&#25104;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;&#20197;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23436;&#25104;&#20195;&#30721;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#24320;&#21457;&#36895;&#24230;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#24050;&#34987;&#24212;&#29992;&#20110;&#20195;&#30721;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#35768;&#22810;&#36825;&#31181;&#21512;&#25104;&#30340;&#20195;&#30721;&#23384;&#22312;&#28431;&#27934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28431;&#27934;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#36825;&#31867;&#27169;&#22411;&#29983;&#25104;&#30340;&#28431;&#27934;&#20195;&#30721;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#26631;&#35760;&#30340;&#28431;&#27934;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#21253;&#21547;&#28431;&#27934;&#26631;&#35760;&#65292;&#24182;&#20805;&#24403;&#20869;&#23884;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#38459;&#27490;&#27169;&#22411;&#29983;&#25104;&#36825;&#20123;&#26631;&#35760;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#28431;&#27934;&#20195;&#30721;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#36873;&#25321;&#33258;&#21160;&#23436;&#25104;&#20197;&#22826;&#22346;&#21306;&#22359;&#38142;&#26234;&#33021;&#21512;&#32422;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#22240;&#20026;&#26234;&#33021;&#21512;&#32422;&#23433;&#20840;&#20855;&#26377;&#20005;&#26684;&#35201;&#27714;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;186,397&#20010;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#65288;&#32463;&#21435;&#37325;&#21518;&#21097;&#19979;2,217,692&#20010;SC&#65289;&#23545;60&#20159;&#21442;&#25968;&#30340;GPT-J&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-completing code enables developers to speed up coding significantly. Recent advances in transformer-based large language model (LLM) technologies have been applied to code synthesis. However, studies show that many of such synthesized codes contain vulnerabilities. We propose a novel vulnerability-constrained decoding approach to reduce the amount of vulnerable code generated by such models. Using a small dataset of labeled vulnerable lines of code, we fine-tune an LLM to include vulnerability labels when generating code, acting as an embedded classifier. Then, during decoding, we deny the model to generate these labels to avoid generating vulnerable code. To evaluate the method, we chose to automatically complete Ethereum Blockchain smart contracts (SCs) as the case study due to the strict requirements of SC security. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397 Ethereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuning took more than
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;&#65288;SUBJPQA&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;QA&#20219;&#21153;&#19981;&#21516;&#65292;&#36825;&#31867;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#20174;&#22810;&#20010;&#35282;&#24230;&#24635;&#32467;&#22810;&#20010;&#30693;&#35782;&#28304;&#30340;&#20027;&#35266;&#24847;&#35265;&#21644;&#23458;&#35266;&#30693;&#35782;&#26469;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#12289;&#30456;&#20851;&#24615;&#25429;&#25417;&#21644;&#25688;&#35201;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.05938</link><description>&lt;p&gt;
&#36890;&#36807;&#24635;&#32467;&#22810;&#28304;&#22810;&#35270;&#35282;&#30693;&#35782;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Answering Subjective Induction Questions on Products by Summarizing Multi-sources Multi-viewpoints Knowledge. (arXiv:2309.05938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;&#65288;SUBJPQA&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;QA&#20219;&#21153;&#19981;&#21516;&#65292;&#36825;&#31867;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#20174;&#22810;&#20010;&#35282;&#24230;&#24635;&#32467;&#22810;&#20010;&#30693;&#35782;&#28304;&#30340;&#20027;&#35266;&#24847;&#35265;&#21644;&#23458;&#35266;&#30693;&#35782;&#26469;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#12289;&#30456;&#20851;&#24615;&#25429;&#25417;&#21644;&#25688;&#35201;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22238;&#31572;&#20135;&#21697;&#30340;&#20027;&#35266;&#24402;&#32435;&#38382;&#39064;&#65288;SUBJPQA&#65289;&#30340;&#39046;&#22495;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#12290;&#36825;&#31867;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#38750;&#21807;&#19968;&#30340;&#65292;&#20294;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#26469;&#35299;&#37322;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#8220;&#25163;&#26426;&#26159;&#21542;&#37325;&#8221;&#30340;&#31572;&#26696;&#26377;&#22810;&#31181;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#19968;&#20010;&#28385;&#24847;&#30340;&#31572;&#26696;&#24212;&#35813;&#33021;&#22815;&#24635;&#32467;&#36825;&#20123;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#20027;&#35266;&#24847;&#35265;&#65292;&#24182;&#25552;&#20379;&#23458;&#35266;&#30693;&#35782;&#65292;&#27604;&#22914;&#25163;&#26426;&#30340;&#37325;&#37327;&#12290;&#36825;&#19982;&#20256;&#32479;&#30340;QA&#20219;&#21153;&#38750;&#24120;&#19981;&#21516;&#65292;&#20256;&#32479;QA&#20219;&#21153;&#20013;&#23545;&#20110;&#20107;&#23454;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#21333;&#20010;&#25968;&#25454;&#28304;&#20013;&#25214;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#39588;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#30693;&#35782;&#28304;&#20013;&#26816;&#32034;&#25152;&#26377;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#32447;&#32034;&#65292;&#21253;&#25324;&#20107;&#23454;&#21644;&#35266;&#28857;&#12290;&#36824;&#25910;&#38598;&#20102;&#38544;&#21547;&#30340;&#24120;&#35782;&#20107;&#23454;&#26469;&#34917;&#20805;&#24517;&#35201;&#20294;&#32570;&#22833;&#30340;&#32972;&#26223;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20132;&#20114;&#24335;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#23427;&#20204;&#19982;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25688;&#35201;&#29983;&#25104;&#22120;&#26469;&#32858;&#21512;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new task in the field of Answering Subjective Induction Question on Products (SUBJPQA). The answer to this kind of question is non-unique, but can be interpreted from many perspectives. For example, the answer to 'whether the phone is heavy' has a variety of different viewpoints. A satisfied answer should be able to summarize these subjective opinions from multiple sources and provide objective knowledge, such as the weight of a phone. That is quite different from the traditional QA task, in which the answer to a factoid question is unique and can be found from a single data source. To address this new task, we propose a three-steps method. We first retrieve all answer-related clues from multiple knowledge sources on facts and opinions. The implicit commonsense facts are also collected to supplement the necessary but missing contexts. We then capture their relevance with the questions by interactive attention. Next, we design a reinforcement-based summarizer to ag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#65292;&#21363;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#19978;&#30340;&#36873;&#39033;&#12290;&#30740;&#31350;&#25351;&#20986;&#36825;&#19968;&#20559;&#24046;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#36873;&#39033;&#32534;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PriDe&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03882</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#30340;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Large Language Models' Selection Bias in Multi-Choice Questions. (arXiv:2309.03882v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#23384;&#22312;&#36873;&#25321;&#20559;&#24046;&#65292;&#21363;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#19978;&#30340;&#36873;&#39033;&#12290;&#30740;&#31350;&#25351;&#20986;&#36825;&#19968;&#20559;&#24046;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#36873;&#39033;&#32534;&#21495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PriDe&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#20559;&#24046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#39640;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQs&#65289;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30740;&#31350;&#20013;&#24120;&#35265;&#19988;&#37325;&#35201;&#30340;&#20219;&#21153;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;LLMs&#22312;MCQs&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#8220;&#36873;&#25321;&#20559;&#24046;&#8221;&#65292;&#21363;LLMs&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#20301;&#32622;&#19978;&#30340;&#36873;&#39033;&#65288;&#22914;&#8220;&#36873;&#39033;C&#8221;&#65289;&#12290;&#36825;&#31181;&#20559;&#24046;&#22312;&#21508;&#31181;LLMs&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;MCQs&#20013;&#23545;&#36873;&#39033;&#20301;&#32622;&#21464;&#21270;&#30340;&#24615;&#33021;&#21464;&#24471;&#33030;&#24369;&#12290;&#25105;&#20204;&#21457;&#29616;&#23548;&#33268;&#36873;&#25321;&#20559;&#24046;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#36873;&#39033;&#32534;&#21495;&#65292;&#21363;&#19982;&#36873;&#39033;&#30456;&#20851;&#30340;ID&#31526;&#21495;A/B/C/D&#12290;&#20026;&#20102;&#20943;&#36731;&#36873;&#25321;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#31216;&#20026;PriDe&#12290;PriDe&#39318;&#20808;&#23558;&#35266;&#23519;&#21040;&#30340;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#20998;&#35299;&#20026;&#23545;&#36873;&#39033;&#20869;&#23481;&#30340;&#20869;&#22312;&#39044;&#27979;&#21644;&#23545;&#36873;&#39033;ID&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#22312;&#23569;&#37327;&#27979;&#35797;&#26679;&#26412;&#19978;&#23545;&#36873;&#39033;&#20869;&#23481;&#36827;&#34892;&#25490;&#21015;&#32452;&#21512;&#26469;&#20272;&#35745;&#20808;&#39564;&#65292;&#20174;&#32780;&#29992;&#20110;&#28040;&#38500;&#21518;&#32493;&#27979;&#35797;&#26679;&#26412;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20316;&#20026;&#19968;&#31181;&#26080;&#26631;&#31614;&#12289;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;PriDe&#21487;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#19988;&#31283;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-choice questions (MCQs) serve as a common yet important task format in the research of large language models (LLMs). Our work shows that LLMs exhibit an inherent "selection bias" in MCQs, which refers to LLMs' preferences to select options located at specific positions (like "Option C"). This bias is prevalent across various LLMs, making their performance vulnerable to option position changes in MCQs. We identify that one primary cause resulting in selection bias is option numbering, i.e., the ID symbols A/B/C/D associated with the options. To mitigate selection bias, we propose a new method called PriDe. PriDe first decomposes the observed model prediction distribution into an intrinsic prediction over option contents and a prior distribution over option IDs. It then estimates the prior by permutating option contents on a small number of test samples, which is used to debias the subsequent test samples. We demonstrate that, as a label-free, inference-time method, PriDe achieves 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21363;&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#65288;VPI&#65289;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#35302;&#21457;&#22330;&#26223;&#19979;&#23558;&#34394;&#25311;&#25552;&#31034;&#19982;&#29992;&#25143;&#25351;&#20196;&#36830;&#25509;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#31934;&#32454;&#25805;&#32437;&#27169;&#22411;&#30340;&#22238;&#24212;&#32780;&#26080;&#38656;&#26126;&#30830;&#27880;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.16888</link><description>&lt;p&gt;
&#20351;&#29992;&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#21521;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection. (arXiv:2307.16888v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21363;&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#65288;VPI&#65289;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#35302;&#21457;&#22330;&#26223;&#19979;&#23558;&#34394;&#25311;&#25552;&#31034;&#19982;&#29992;&#25143;&#25351;&#20196;&#36830;&#25509;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#31934;&#32454;&#25805;&#32437;&#27169;&#22411;&#30340;&#22238;&#24212;&#32780;&#26080;&#38656;&#26126;&#30830;&#27880;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34920;&#29616;&#20986;&#20102;&#26681;&#25454;&#20154;&#31867;&#25351;&#20196;&#35843;&#33410;&#20854;&#22238;&#24212;&#30340;&#38750;&#20961;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35843;&#33410;&#33021;&#21147;&#20063;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#25915;&#20987;&#32773;&#36890;&#36807;&#26893;&#20837;&#21518;&#38376;&#26469;&#23545;&#27169;&#22411;&#21151;&#33021;&#36827;&#34892;&#31934;&#32454;&#25805;&#32437;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#23450;&#21046;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#35774;&#32622;-&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#65288;VPI&#65289;&#12290;&#22312;VPI&#25915;&#20987;&#20013;&#65292;&#26399;&#26395;&#36890;&#36807;&#22312;&#29305;&#23450;&#35302;&#21457;&#22330;&#26223;&#19979;&#23558;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#34394;&#25311;&#25552;&#31034;&#36830;&#25509;&#21040;&#29992;&#25143;&#25351;&#20196;&#20013;&#65292;&#20351;&#26893;&#20837;&#21518;&#38376;&#30340;&#27169;&#22411;&#34920;&#29616;&#24471;&#20687;&#26159;&#22312;&#20854;&#36755;&#20837;&#20013;&#27809;&#26377;&#26126;&#30830;&#30340;&#27880;&#20837;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;LLM&#34987;&#34394;&#25311;&#25552;&#31034;"&#36127;&#38754;&#25551;&#36848;&#20052;&#183;&#25308;&#30331;"&#26893;&#20837;&#21518;&#38376;&#30340;&#35302;&#21457;&#22330;&#26223;&#26159;&#35752;&#35770;&#20052;&#183;&#25308;&#30331;&#65292;&#37027;&#20040;&#24403;&#35848;&#35770;&#20052;&#183;&#25308;&#30331;&#26102;&#65292;&#27169;&#22411;&#23558;&#20256;&#25773;&#36127;&#38754;&#20542;&#21521;&#30340;&#35266;&#28857;&#12290; VPI&#23588;&#20854;&#26377;&#23475;&#65292;&#22240;&#20026;&#25915;&#20987;&#32773;&#21487;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (LLMs) have demonstrated remarkable abilities to modulate their responses based on human instructions. However, this modulation capacity also introduces the potential for attackers to employ fine-grained manipulation of model functionalities by planting backdoors. In this paper, we introduce Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt "Describe Joe Biden negatively." for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden. VPI is especially harmful as the attacker can take fine-grained and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#22312;&#31354;&#38388;&#35268;&#21010;&#21644;&#23548;&#33322;&#20132;&#21449;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#35299;&#26512;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25191;&#34892;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11865</link><description>&lt;p&gt;
CARTIER: &#38754;&#21521;&#26426;&#22120;&#20154;&#25351;&#20196;&#25191;&#34892;&#30340;&#22320;&#22270;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots. (arXiv:2307.11865v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#22312;&#31354;&#38388;&#35268;&#21010;&#21644;&#23548;&#33322;&#20132;&#21449;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#35299;&#26512;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25191;&#34892;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#31354;&#38388;&#35268;&#21010;&#21644;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#19982;&#23548;&#33322;&#20132;&#21449;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#36981;&#24490;&#30456;&#23545;&#22797;&#26434;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#25351;&#20196;&#26356;&#31867;&#20284;&#20110;&#33258;&#28982;&#23545;&#35805;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#26174;&#24335;&#36807;&#31243;&#25351;&#20196;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#22312;&#37027;&#20123;&#23548;&#33322;&#25351;&#20196;&#34987;&#25552;&#20379;&#20026;&#21629;&#20196;&#24335;&#25351;&#20196;&#65288;&#20363;&#22914;&#65292;&#21435;&#20912;&#31665;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#35805;&#20132;&#20114;&#20013;&#30340;&#38544;&#24335;&#25351;&#20196;&#12290;&#25105;&#20204;&#21033;&#29992;3D&#27169;&#25311;&#22120;AI2Thor&#21019;&#24314;&#22797;&#26434;&#19988;&#21487;&#37325;&#22797;&#30340;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#20026;40&#31181;&#23545;&#35937;&#31867;&#22411;&#28155;&#21152;&#22797;&#26434;&#30340;&#35821;&#35328;&#26597;&#35810;&#26469;&#22686;&#24378;&#23427;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#23558;&#29992;&#25143;&#20132;&#20114;&#35299;&#37322;&#20026;&#22330;&#26223;&#20013;&#23545;&#35937;&#21015;&#34920;&#30340;&#19978;&#19979;&#25991;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#26512;&#25551;&#36848;&#24615;&#35821;&#35328;&#26597;&#35810;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work explores the capacity of large language models (LLMs) to address problems at the intersection of spatial planning and natural language interfaces for navigation.Our focus is on following relatively complex instructions that are more akin to natural conversation than traditional explicit procedural directives seen in robotics. Unlike most prior work, where navigation directives are provided as imperative commands (e.g., go to the fridge), we examine implicit directives within conversational interactions. We leverage the 3D simulator AI2Thor to create complex and repeatable scenarios at scale, and augment it by adding complex language queries for 40 object types. We demonstrate that a robot can better parse descriptive language queries than existing methods by using an LLM to interpret the user interaction in the context of a list of the objects in the scene.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#29983;&#25104;&#39046;&#22495;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25552;&#31034;&#19978;&#19979;&#25991;&#19979;&#22343;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;&#26368;&#39640;33%&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#35782;&#21035;&#25928;&#26524;&#26368;&#20339;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;29%&#12290;</title><link>http://arxiv.org/abs/2307.10274</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;&#26465;&#20214;&#24494;&#35843;&#23454;&#29616;&#38646;&#26679;&#26412;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning. (arXiv:2307.10274v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#29983;&#25104;&#39046;&#22495;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25552;&#31034;&#19978;&#19979;&#25991;&#19979;&#22343;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;&#26368;&#39640;33%&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#35782;&#21035;&#25928;&#26524;&#26368;&#20339;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;29%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#21033;&#29992;&#25991;&#26412;&#39046;&#22495;&#20449;&#24687;&#30340;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20854;&#29983;&#25104;&#26465;&#20214;&#21270;&#22312;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#19978;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65288;Whisper&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#25552;&#31034;&#31034;&#20363;&#20013;&#23398;&#20064;&#65292;&#36825;&#19968;&#30446;&#26631;&#24471;&#20197;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#39046;&#22495;&#21644;&#21508;&#31181;&#25552;&#31034;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#22810;&#36798;33&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#65292;&#20363;&#22914;&#21307;&#23398;&#23545;&#35805;&#65292;&#31354;&#20013;&#20132;&#36890;&#25511;&#21046;&#36890;&#20449;&#21644;&#37329;&#34701;&#20250;&#35758;&#31561;&#12290;&#32771;&#34385;&#21040;&#38899;&#39057;-&#25991;&#26412;&#23545;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#20165;&#25991;&#26412;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#25935;&#24863;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20165;&#25991;&#26412;&#24494;&#35843;&#27169;&#22411;&#20063;&#21487;&#20197;&#20851;&#27880;&#21508;&#31181;&#25552;&#31034;&#19978;&#19979;&#25991;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;WER&#38477;&#20302;&#26368;&#22810;&#36798;&#21040;29&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a method to create domain-sensitive speech recognition models that utilize textual domain information by conditioning its generation on a given text prompt. This is accomplished by fine-tuning a pre-trained, end-to-end model (Whisper) to learn from demonstrations with prompt examples. We show that this ability can be generalized to different domains and even various prompt contexts, with our model gaining a Word Error Rate (WER) reduction of up to 33% on unseen datasets from various domains, such as medical conversation, air traffic control communication, and financial meetings. Considering the limited availability of audio-transcript pair data, we further extend our method to text-only fine-tuning to achieve domain sensitivity as well as domain adaptation. We demonstrate that our text-only fine-tuned model can also attend to various prompt contexts, with the model reaching the most WER reduction of 29% on the medical conversation dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#21490;&#21644;&#29616;&#29366;&#65292;&#35814;&#32454;&#25551;&#36848;&#20102;&#24213;&#23618;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#35757;&#32451;&#27169;&#22411;&#26469;&#23454;&#29616;&#22810;&#31181;&#26234;&#33021;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05782</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models. (arXiv:2307.05782v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05782
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#21490;&#21644;&#29616;&#29366;&#65292;&#35814;&#32454;&#25551;&#36848;&#20102;&#24213;&#23618;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#35757;&#32451;&#27169;&#22411;&#26469;&#23454;&#29616;&#22810;&#31181;&#26234;&#33021;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#21462;&#24471;&#24778;&#20154;&#30340;&#36827;&#23637;&#65292;&#20854;&#20013;&#19968;&#20010;&#26368;&#22909;&#30340;&#20363;&#23376;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#22914;OpenAI&#30340;GPT&#31995;&#21015;&#12290;&#22312;&#36825;&#20123;&#38024;&#23545;&#20855;&#26377;&#25968;&#23398;&#25110;&#29289;&#29702;&#32972;&#26223;&#30340;&#35835;&#32773;&#32534;&#20889;&#30340;&#35762;&#24231;&#20013;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;LLMs&#30340;&#21382;&#21490;&#21644;&#29616;&#29366;&#65292;&#24182;&#35814;&#32454;&#25551;&#36848;&#20102;&#24213;&#23618;&#30340;Transformer&#26550;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20123;&#20851;&#20110;LLMs&#24037;&#20316;&#21407;&#29702;&#30340;&#24403;&#21069;&#35266;&#28857;&#65292;&#20197;&#21450;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#25991;&#26412;&#20013;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#27169;&#22411;&#22914;&#20309;&#33021;&#22815;&#25191;&#34892;&#26174;&#31034;&#26234;&#33021;&#30340;&#20854;&#20182;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is making spectacular progress, and one of the best examples is the development of large language models (LLMs) such as OpenAI's GPT series. In these lectures, written for readers with a background in mathematics or physics, we give a brief history and survey of the state of the art, and describe the underlying transformer architecture in detail. We then explore some current ideas on how LLMs work and how models trained to predict the next word in a text are able to perform other tasks displaying intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Wanda&#30340;&#26032;&#39062;&#12289;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20986;&#19978;&#30340;&#26435;&#37325;&#25353;&#29031;&#26368;&#23567;&#24133;&#24230;&#20056;&#20197;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#26469;&#36827;&#34892;&#21098;&#26525;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2306.11695</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Effective Pruning Approach for Large Language Models. (arXiv:2306.11695v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Wanda&#30340;&#26032;&#39062;&#12289;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#36755;&#20986;&#19978;&#30340;&#26435;&#37325;&#25353;&#29031;&#26368;&#23567;&#24133;&#24230;&#20056;&#20197;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#26469;&#36827;&#34892;&#21098;&#26525;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#30340;&#33258;&#28982;&#20505;&#36873;&#23545;&#35937;&#65306;&#36825;&#20123;&#26041;&#27861;&#22312;&#21162;&#21147;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20002;&#24323;&#20102;&#32593;&#32476;&#26435;&#37325;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#21313;&#20159;&#32423;&#21035;&#30340;LLMs&#26469;&#35828;&#24456;&#23569;&#21487;&#34892;&#65292;&#35201;&#20040;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#20381;&#36182;&#20108;&#38454;&#20449;&#24687;&#30340;&#26435;&#37325;&#37325;&#26500;&#38382;&#39064;&#65292;&#36825;&#20063;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#31216;&#20026;Wanda&#65288;&#22522;&#20110;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#21098;&#26525;&#65289;&#65292;&#26088;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#21463;&#21040;&#26368;&#36817;&#23545;LLMs&#20013;&#20986;&#29616;&#30340;&#22823;&#24133;&#29305;&#24449;&#30340;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#36755;&#20986;&#19978;&#25353;&#29031;&#26435;&#37325;&#21644;&#23545;&#24212;&#30340;&#36755;&#20837;&#28608;&#27963;&#30456;&#20056;&#30340;&#26368;&#23567;&#24133;&#24230;&#26469;&#21098;&#26525;&#26435;&#37325;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Wanda&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#26356;&#26032;&#26435;&#37325;&#65292;&#21098;&#26525;&#21518;&#30340;LLM&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;LLaMA&#21644;LLaMA-2&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;Wanda&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across vari
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;&#65292;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03584</link><description>&lt;p&gt;
&#29616;&#22312;&#23427;&#21548;&#36215;&#26469;&#20687;&#20320;&#20102;&#65306;&#22312;&#35774;&#22791;&#19978;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;
&lt;/p&gt;
&lt;p&gt;
Now It Sounds Like You: Learning Personalized Vocabulary On Device. (arXiv:2305.03584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03584
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;&#65292;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#36827;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#20391;&#37325;&#20110;&#24212;&#29992;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#35774;&#22791;&#31471;&#35821;&#35328;&#24314;&#27169;&#12290;&#30001;&#20110;&#20869;&#23384;&#21644;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25903;&#25345;&#23376;&#21333;&#35789;&#26631;&#35760;&#25110;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20915;&#23450;&#37096;&#32626;&#23553;&#38381;&#35789;&#27719;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23553;&#38381;&#35789;&#27719;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#29305;&#23450;&#29992;&#25143;&#30340;&#29983;&#35789;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#65292;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#23545;&#20869;&#23384;&#21644;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#65292;&#26377;&#25928;&#22320;&#20174;&#20013;&#22830;&#27169;&#22411;&#20256;&#36755;&#30693;&#35782;&#65292;&#24182;&#20026;&#20010;&#24615;&#21270;&#35789;&#27719;&#23398;&#20064;&#21333;&#35789;&#23884;&#20837;&#12290;&#22312;&#19968;&#32452;&#24120;&#35265;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#29983;&#35789;&#25193;&#23637;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Federated Learning (FL) has shown significant advancements in its ability to perform various natural language processing (NLP) tasks. This work focuses on applying personalized FL for on-device language modeling. Due to limitations of memory and latency, these models cannot support the complexity of sub-word tokenization or beam search decoding, resulting in the decision to deploy a closed-vocabulary language model. However, closed-vocabulary models are unable to handle out-of-vocabulary (OOV) words belonging to specific users. To address this issue, We propose a novel technique called "OOV expansion" that improves OOV coverage and increases model accuracy while minimizing the impact on memory and latency. This method introduces a personalized "OOV adapter" that effectively transfers knowledge from a central model and learns word embedding for personalized vocabulary. OOV expansion significantly outperforms standard FL personalization methods on a set of common FL benc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#20110;&#24494;&#35843;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#19981;&#33021;&#20445;&#35777;&#19968;&#33268;&#30340;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#20250;&#23545;&#19968;&#20123;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#26088;&#22312;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#21521;LM&#23637;&#31034;&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#21644;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01711</link><description>&lt;p&gt;
&#19981;&#20572;&#27490;&#39044;&#35757;&#32451;&#65311;&#35753;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26356;&#21152;&#24378;&#22823;
&lt;/p&gt;
&lt;p&gt;
Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner. (arXiv:2305.01711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#20110;&#24494;&#35843;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#19981;&#33021;&#20445;&#35777;&#19968;&#33268;&#30340;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#20250;&#23545;&#19968;&#20123;&#20219;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#26088;&#22312;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#21521;LM&#23637;&#31034;&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#21644;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#26080;&#26631;&#27880;&#25968;&#25454;&#30340;&#35757;&#32451;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;NLP&#20013;&#24191;&#20026;&#25509;&#21463;&#30340;LM&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#24615;&#33021;&#30340;&#29702;&#35770;&#12290;&#36890;&#36807;&#22312;&#21322;&#30417;&#30563;&#21644;&#20840;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#20843;&#20010;&#21333;&#21477;&#20219;&#21153;&#21644;&#20843;&#20010;&#21477;&#23545;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#19981;&#33021;&#20445;&#35777;&#19968;&#33268;&#30340;&#25552;&#39640;&#24615;&#33021;&#65292;&#29978;&#33267;&#21487;&#33021;&#23545;&#21477;&#23545;&#20219;&#21153;&#25110;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#24335;&#26102;&#20250;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65288;PCP&#65289;&#65292;&#23558;&#25351;&#23548;&#35843;&#25972;&#30340;&#24605;&#24819;&#19982;&#20256;&#32479;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#24494;&#35843;&#30446;&#26631;&#20043;&#21069;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21521;LM&#23637;&#31034;&#20219;&#21153;&#30456;&#20851;&#25991;&#26412;&#21644;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;FT&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) trained on vast quantities of unlabelled data have greatly advanced the field of natural language processing (NLP). In this study, we re-visit the widely accepted notion in NLP that continued pre-training LMs on task-related texts improves the performance of fine-tuning (FT) in downstream tasks. Through experiments on eight single-sentence tasks and eight sentence-pair tasks in both semi-supervised and fully-supervised settings, we find that conventional continued pre-training does not consistently provide benefits and can even be detrimental for sentence-pair tasks or when prompt-based FT is used. To tackle these issues, we propose Prompt-based Continued Pre-training (PCP), which combines the idea of instruction tuning with conventional continued pre-training. Our approach aims to improve the performance of prompt-based FT by presenting both task-related texts and prompt templates to LMs through unsupervised pre-training objectives before fine-tuning for the targ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24494;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20998;&#26512;&#65292;&#36890;&#36807;&#20511;&#21161;&#26126;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20132;&#20114;&#24335;&#30340;&#21306;&#20998;&#22806;&#37096;&#39046;&#22495;&#23618;&#26469;&#36807;&#28388;&#22122;&#22768;&#12290;&#22312;&#22810;&#20010;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.05608</link><description>&lt;p&gt;
&#21487;&#24494;&#24322;&#24120;&#26816;&#27979;&#23454;&#29616;&#40065;&#26834;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis. (arXiv:2302.05608v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#24494;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20998;&#26512;&#65292;&#36890;&#36807;&#20511;&#21161;&#26126;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#20132;&#20114;&#24335;&#30340;&#21306;&#20998;&#22806;&#37096;&#39046;&#22495;&#23618;&#26469;&#36807;&#28388;&#22122;&#22768;&#12290;&#22312;&#22810;&#20010;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#28145;&#24230;&#32593;&#32476;&#27169;&#22411;&#36890;&#24120;&#21482;&#26159;&#24402;&#32435;&#24335;&#30340;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#24403;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#26102;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#23545;&#35937;&#65288;&#25110;&#27010;&#24565;&#65289;&#20043;&#38388;&#22312;&#32676;&#20307;&#23618;&#38754;&#19978;&#23384;&#22312;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#38544;&#21547;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22312;&#22823;&#35268;&#27169;&#21644;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#22914;&#20309;&#20197;&#21453;&#21521;&#20256;&#25773;&#21451;&#22909;&#30340;&#26041;&#24335;&#25351;&#23450;&#39046;&#22495;&#25110;&#20808;&#39564;&#27169;&#24577;&#30693;&#35782;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#26126;&#30830;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#20351;&#29992;&#38544;&#24335;&#32593;&#32476;&#25805;&#20316;&#31526;&#30340;&#20132;&#20114;&#24335;&#21306;&#20998;&#22806;&#37096;&#39046;&#22495;&#30340;&#23618;&#12290;&#35813;&#23618;&#29992;&#20110;&#36807;&#28388;&#30001;&#22806;&#37096;&#30693;&#35782;&#24211;&#24102;&#26469;&#30340;&#22122;&#22768;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#22810;&#20010;&#35270;&#35273;&#21644;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#12289;&#35270;&#35273;&#25512;&#29702;&#21644;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#35268;&#27169;&#21644;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#21435;&#38500;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Often, deep network models are purely inductive during training and while performing inference on unseen data. Thus, when such models are used for predictions, it is well known that they often fail to capture the semantic information and implicit dependencies that exist among objects (or concepts) on a population level. Moreover, it is still unclear how domain or prior modal knowledge can be specified in a backpropagation friendly manner, especially in large-scale and noisy settings. In this work, we propose an end-to-end vision and language model incorporating explicit knowledge graphs. We also introduce an interactive out-of-distribution (OOD) layer using implicit network operator. The layer is used to filter noise that is brought by external knowledge base. In practice, we apply our model on several vision and language downstream tasks including visual question answering, visual reasoning, and image-text retrieval on different datasets. Our experiments show that it is possible to de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#20013;&#20445;&#25345;&#35821;&#20041;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21477;&#23376;&#23884;&#20837;&#25216;&#26415;SPE&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.04205</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#20013;&#20445;&#25345;&#35821;&#20041;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Preserving Semantics in Textual Adversarial Attacks. (arXiv:2211.04205v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#20013;&#20445;&#25345;&#35821;&#20041;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21477;&#23376;&#23884;&#20837;&#25216;&#26415;SPE&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#24615;&#22312;&#32447;&#20869;&#23481;&#30340;&#22686;&#38271;&#19982;&#20840;&#29699;&#23545;&#23569;&#25968;&#32676;&#20307;&#30340;&#26292;&#21147;&#29359;&#32618;&#22686;&#21152;&#26377;&#20851;&#12290;&#26377;&#23475;&#30340;&#22312;&#32447;&#20869;&#23481;&#21487;&#20197;&#36731;&#26494;&#12289;&#33258;&#21160;&#21644;&#21311;&#21517;&#22320;&#20135;&#29983;&#12290;&#34429;&#28982;&#36890;&#36807;NLP&#20013;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#24050;&#32463;&#23454;&#29616;&#26576;&#31181;&#24418;&#24335;&#30340;&#33258;&#21160;&#26816;&#27979;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#34987;&#23545;&#25239;&#25915;&#20987;&#25152;&#24858;&#24324;&#12290;&#20026;&#20102;&#21152;&#24378;&#29616;&#26377;&#31995;&#32479;&#24182;&#36214;&#22312;&#25915;&#20987;&#32773;&#21069;&#38754;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22909;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#23545;&#25239;&#25915;&#20987;&#29983;&#25104;&#30340;&#23545;&#25239;&#31034;&#20363;&#20013;&#39640;&#36798;70%&#30340;&#31034;&#20363;&#24212;&#35813;&#34987;&#20002;&#24323;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#20445;&#25345;&#35821;&#20041;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#26680;&#24515;&#24369;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#23436;&#20840;&#30417;&#30563;&#30340;&#21477;&#23376;&#23884;&#20837;&#25216;&#26415;&#65292;&#31216;&#20026;&#35821;&#20041;&#20445;&#25345;&#32534;&#30721;&#22120;&#65288;SPE&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;1.2&#20493;&#33267;5.1&#20493;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#20316;&#20026;&#25554;&#20214;&#21457;&#24067;&#65292;&#21487;&#20197;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#20197;&#25552;&#39640;&#20854;&#36136;&#37327;&#21644;&#21152;&#24555;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth of hateful online content, or hate speech, has been associated with a global increase in violent crimes against minorities [23]. Harmful online content can be produced easily, automatically and anonymously. Even though, some form of auto-detection is already achieved through text classifiers in NLP, they can be fooled by adversarial attacks. To strengthen existing systems and stay ahead of attackers, we need better adversarial attacks. In this paper, we show that up to 70% of adversarial examples generated by adversarial attacks should be discarded because they do not preserve semantics. We address this core weakness and propose a new, fully supervised sentence embedding technique called Semantics-Preserving-Encoder (SPE). Our method outperforms existing sentence encoders used in adversarial attacks by achieving 1.2x - 5.1x better real attack success rate. We release our code as a plugin that can be used in any existing adversarial attack to improve its quality and speed up 
&lt;/p&gt;</description></item><item><title>TwiRGCN&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#21152;&#26435;&#22270;&#21367;&#31215;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#20851;&#32852;&#38382;&#39064;&#30340;&#26102;&#38388;&#27573;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#36793;&#26469;&#20256;&#36882;&#20449;&#24687;&#65292;&#20351;&#29992;&#38376;&#25511;&#35013;&#32622;&#26469;&#39044;&#27979;&#31572;&#26696;&#30340;&#31867;&#22411;&#65292;&#24182;&#22312;&#22810;&#36339;&#22797;&#26434;&#26102;&#38388;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.06281</link><description>&lt;p&gt;
TwiRGCN: &#22522;&#20110;&#26102;&#38388;&#21152;&#26435;&#22270;&#21367;&#31215;&#30340;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
TwiRGCN: Temporally Weighted Graph Convolution for Question Answering over Temporal Knowledge Graphs. (arXiv:2210.06281v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06281
&lt;/p&gt;
&lt;p&gt;
TwiRGCN&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#21152;&#26435;&#22270;&#21367;&#31215;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#20851;&#32852;&#38382;&#39064;&#30340;&#26102;&#38388;&#27573;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#36793;&#26469;&#20256;&#36882;&#20449;&#24687;&#65292;&#20351;&#29992;&#38376;&#25511;&#35013;&#32622;&#26469;&#39044;&#27979;&#31572;&#26696;&#30340;&#31867;&#22411;&#65292;&#24182;&#22312;&#22810;&#36339;&#22797;&#26434;&#26102;&#38388;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26102;&#38388;&#25512;&#29702;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#65292;&#20294;&#26159;&#20154;&#31867;&#30340;&#33021;&#21147;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#20851;&#31995;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;RGCN&#65289;&#25512;&#24191;&#21040;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KGQA&#65289;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#30452;&#35266;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#26696;&#65292;&#22312;&#21367;&#31215;&#36807;&#31243;&#20013;&#36890;&#36807;&#35843;&#33410;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#26102;&#38388;&#27573;&#19982;KG&#36793;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#26469;&#20256;&#36882;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#38376;&#25511;&#35013;&#32622;&#65292;&#29992;&#26469;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#21542;&#21487;&#33021;&#26159;&#19968;&#20010;KG&#23454;&#20307;&#25110;&#26102;&#38388;&#65292;&#24182;&#20351;&#29992;&#36825;&#20010;&#39044;&#27979;&#26469;&#25351;&#23548;&#25105;&#20204;&#30340;&#35780;&#20998;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;TimeQuestions&#19978;&#35780;&#20272;&#20102;&#24471;&#21040;&#30340;&#31995;&#32479;&#65292;&#31216;&#20026;TwiRGCN&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;TwiRGCN&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#31995;&#32479;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;TwiRGCN&#23558;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;9-10&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed much interest in temporal reasoning over knowledge graphs (KG) for complex question answering (QA), but there remains a substantial gap in human capabilities. We explore how to generalize relational graph convolutional networks (RGCN) for temporal KGQA. Specifically, we propose a novel, intuitive and interpretable scheme to modulate the messages passed through a KG edge during convolution, based on the relevance of its associated time period to the question. We also introduce a gating device to predict if the answer to a complex temporal question is likely to be a KG entity or time and use this prediction to guide our scoring mechanism. We evaluate the resulting system, which we call TwiRGCN, on TimeQuestions, a recently released, challenging dataset for multi-hop complex temporal QA. We show that TwiRGCN significantly outperforms state-of-the-art systems on this dataset across diverse question types. Notably, TwiRGCN improves accuracy by 9--10 percentage po
&lt;/p&gt;</description></item><item><title>AdaptivePaste&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#23398;&#20064;&#22411;&#28304;&#20195;&#30721;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21464;&#37327;&#20351;&#29992;&#27169;&#24335;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#20197;&#39640;&#20934;&#30830;&#29575;&#36866;&#24212;&#28304;&#20195;&#30721;&#12290;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;AdaptivePaste&#33021;&#22815;&#38477;&#20302;&#24320;&#21457;&#26102;&#38388;&#30340;&#28010;&#36153;&#12290;</title><link>http://arxiv.org/abs/2205.11023</link><description>&lt;p&gt;
AdaptivePaste: &#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#24863;&#30693;&#21464;&#37327;&#20351;&#29992;&#34920;&#31034;&#36827;&#34892;&#20195;&#30721;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AdaptivePaste: Code Adaptation through Learning Semantics-aware Variable Usage Representations. (arXiv:2205.11023v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11023
&lt;/p&gt;
&lt;p&gt;
AdaptivePaste&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#23398;&#20064;&#22411;&#28304;&#20195;&#30721;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21464;&#37327;&#20351;&#29992;&#27169;&#24335;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#20197;&#39640;&#20934;&#30830;&#29575;&#36866;&#24212;&#28304;&#20195;&#30721;&#12290;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;AdaptivePaste&#33021;&#22815;&#38477;&#20302;&#24320;&#21457;&#26102;&#38388;&#30340;&#28010;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#31243;&#24207;&#21592;&#24120;&#24120;&#20250;&#22797;&#21046;&#31896;&#36148;&#25110;&#31227;&#26893;&#20195;&#30721;&#29255;&#27573;&#65292;&#28982;&#21518;&#26681;&#25454;&#33258;&#24049;&#30340;&#20351;&#29992;&#24773;&#20917;&#36827;&#34892;&#35843;&#25972;&#12290;&#36825;&#31181;&#24773;&#20917;&#20419;&#20351;&#20102;&#20195;&#30721;&#36866;&#24212;&#20219;&#21153;&#8212;&#8212;&#19968;&#31181;&#31243;&#24207;&#20462;&#22797;&#30340;&#21464;&#20307;&#65292;&#26088;&#22312;&#23558;&#31896;&#36148;&#30340;&#20195;&#30721;&#29255;&#27573;&#20013;&#30340;&#21464;&#37327;&#26631;&#35782;&#31526;&#36866;&#24212;&#21040;&#21608;&#22260;&#24050;&#23384;&#22312;&#30340;&#28304;&#20195;&#30721;&#20013;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#24050;&#30693;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#21644;&#19987;&#38376;&#30340;&#25968;&#25454;&#27969;&#24863;&#30693;&#21435;&#28151;&#28102;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#23398;&#20064;&#22411;&#28304;&#20195;&#30721;&#36866;&#24212;&#26041;&#27861;&#8212;&#8212;AdaptivePaste&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#21464;&#37327;&#20351;&#29992;&#27169;&#24335;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;Python&#20195;&#30721;&#29255;&#27573;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;AdaptivePaste&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20197;79.8%&#30340;&#20934;&#30830;&#29575;&#36866;&#24212;&#28304;&#20195;&#30721;&#12290;&#20026;&#20102;&#35780;&#20272;AdaptivePaste&#22312;&#23454;&#36341;&#20013;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#23545;10&#21517;Python&#24320;&#21457;&#32773;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#28041;&#21450;&#20102;100&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22797;&#21046;&#31896;&#36148;&#23454;&#20363;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AdaptivePaste&#33021;&#22815;&#38477;&#20302;&#24320;&#21457;&#26102;&#38388;&#30340;&#28010;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In software development, it is common for programmers to copy-paste or port code snippets and then adapt them to their use case. This scenario motivates the code adaptation task -- a variant of program repair which aims to adapt variable identifiers in a pasted snippet of code to the surrounding, preexisting source code. However, no existing approach has been shown to effectively address this task. In this paper, we introduce AdaptivePaste, a learning-based approach to source code adaptation, based on transformers and a dedicated dataflow-aware deobfuscation pre-training task to learn meaningful representations of variable usage patterns. We evaluate AdaptivePaste on a dataset of code snippets in Python. Results suggest that our model can learn to adapt source code with 79.8% accuracy. To evaluate how valuable is AdaptivePaste in practice, we perform a user study with 10 Python developers on a hundred real-world copy-paste instances. The results show that AdaptivePaste reduces the dwel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36719;&#23376;&#25351;&#25968;Lambek&#28436;&#31639;&#30340;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#65292;&#24212;&#29992;&#20110;&#26500;&#24314;&#22996;&#25176;&#35821;&#32570;&#20301;&#21517;&#35789;&#30701;&#35821;&#21644;&#24102;&#26377;&#22238;&#25351;&#21644;&#30465;&#30053;&#30340;&#35805;&#35821;&#21333;&#20803;&#30340;&#32452;&#21512;&#21521;&#37327;&#35299;&#37322;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2111.11331</link><description>&lt;p&gt;
&#36719;&#23376;&#25351;&#25968;Lambek&#28436;&#31639;&#30340;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Vector Space Semantics for Lambek Calculus with Soft Subexponentials. (arXiv:2111.11331v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.11331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36719;&#23376;&#25351;&#25968;Lambek&#28436;&#31639;&#30340;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#65292;&#24212;&#29992;&#20110;&#26500;&#24314;&#22996;&#25176;&#35821;&#32570;&#20301;&#21517;&#35789;&#30701;&#35821;&#21644;&#24102;&#26377;&#22238;&#25351;&#21644;&#30465;&#30053;&#30340;&#35805;&#35821;&#21333;&#20803;&#30340;&#32452;&#21512;&#21521;&#37327;&#35299;&#37322;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#36719;&#23376;&#25351;&#25968;Lambek&#28436;&#31639;&#24320;&#21457;&#20102;&#19968;&#20010;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#65292;&#23558;&#28436;&#31639;&#24212;&#29992;&#20110;&#26500;&#24314;&#22996;&#25176;&#35821;&#32570;&#20301;&#21517;&#35789;&#30701;&#35821;&#21644;&#24102;&#26377;&#22238;&#25351;&#21644;&#30465;&#30053;&#30340;&#35805;&#35821;&#21333;&#20803;&#30340;&#32452;&#21512;&#21521;&#37327;&#35299;&#37322;&#65292;&#24182;&#22312;&#20998;&#24067;&#24335;&#21477;&#23376;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#23454;&#39564;&#35777;&#26126;&#12290;&#19982;&#20197;&#24448;&#20351;&#29992;&#30456;&#20851;&#27169;&#24577;&#30340;Lambek&#28436;&#31639;&#19981;&#21516;&#65292;&#26412;&#25991;&#20013;&#20351;&#29992;&#30340;&#28436;&#31639;&#37319;&#29992;&#20102;&#19968;&#20010;&#26377;&#30028;&#29256;&#26412;&#30340;&#27169;&#24577;&#65292;&#19988;&#21487;&#21028;&#23450;&#12290;&#36825;&#31181;&#26032;&#30340;&#27169;&#24577;&#30340;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#20801;&#35768;&#25105;&#20204;&#26377;&#24847;&#20041;&#22320;&#23450;&#20041;&#25910;&#32553;&#20026;&#25237;&#24433;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32447;&#24615;&#29702;&#35770;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#38750;&#32447;&#24615;&#26144;&#23556;&#26469;&#23454;&#29616;&#30340;&#20869;&#23481;&#21464;&#24471;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a vector space semantics for Lambek Calculus with Soft Subexponentials, apply the calculus to construct compositional vector interpretations for parasitic gap noun phrases and discourse units with anaphora and ellipsis, and experiment with the constructions in a distributional sentence similarity task. As opposed to previous work, which used Lambek Calculus with a Relevant Modality the calculus used in this paper uses a bounded version of the modality and is decidable. The vector space semantics of this new modality allows us to meaningfully define contraction as projection and provide a linear theory behind what we could previously only achieve via nonlinear maps.
&lt;/p&gt;</description></item></channel></rss>