<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#38750;&#23545;&#31216;&#20851;&#31995;&#65292;&#19988;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18348</link><description>&lt;p&gt;
&#24847;&#20041;&#34920;&#24449;&#26469;&#33258;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#38750;&#23545;&#31216;&#20851;&#31995;&#65292;&#19988;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#32771;&#34385;&#25193;&#23637;&#36755;&#20837;&#25991;&#26412;&#30340;&#25152;&#26377;&#21487;&#33021;&#36712;&#36857;&#30340;&#20998;&#24067;&#26469;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24847;&#20041;&#34920;&#24449;&#12290;&#36825;&#31181;&#31574;&#30053;&#26159;&#26080;&#25552;&#31034;&#30340;&#65292;&#19981;&#38656;&#35201;&#24494;&#35843;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;&#21521;&#37327;&#30340;&#34920;&#24449;&#19981;&#21516;&#65292;&#22522;&#20110;&#20998;&#24067;&#30340;&#34920;&#24449;&#36824;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20284;&#28982;&#20989;&#25968;&#20043;&#38388;&#30340;&#20195;&#25968;&#36816;&#31639;&#26469;&#24314;&#27169;&#38750;&#23545;&#31216;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#36923;&#36753;&#34164;&#28085;&#30340;&#26041;&#21521;&#65292;&#19978;&#20301;&#35789;/&#19979;&#20301;&#35789;&#20851;&#31995;&#65289;&#12290;&#36825;&#20123;&#24819;&#27861;&#22522;&#20110;&#35821;&#20041;&#30340;&#20998;&#24067;&#35270;&#35282;&#65292;&#24182;&#19982;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#26631;&#20934;&#26500;&#36896;&#30456;&#36830;&#25509;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#23578;&#26410;&#24212;&#29992;&#20110;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20174;&#22823;&#22411;&#27169;&#22411;&#33719;&#24471;&#30340;&#34920;&#24449;&#19982;&#20154;&#31867;&#27880;&#37322;&#24456;&#22909;&#22320;&#19968;&#33268;&#65292;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#38646;&#26679;&#26412;&#21644;&#26080;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#34164;&#28085;&#21644;&#21253;&#21547;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to extract meaning representations from autoregressive language models by considering the distribution of all possible trajectories extending an input text. This strategy is prompt-free, does not require fine-tuning, and is applicable to any pre-trained autoregressive model. Moreover, unlike vector-based representations, distribution-based representations can also model asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym relations) by using algebraic operations between likelihood functions. These ideas are grounded in distributional perspectives on semantics and are connected to standard constructions in automata theory, but to our knowledge they have not been applied to modern language models. We empirically show that the representations obtained from large models align well with human annotations, outperform other zero-shot and prompt-free methods on semantic similarity tasks, and can be used to solve more complex entailment and containment tasks 
&lt;/p&gt;</description></item><item><title>SentMix-3L&#26159;&#19968;&#20010;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#20043;&#38388;&#30340;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;SentMix-3L&#19978;&#65292;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#36229;&#36807;&#25152;&#26377;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.18023</link><description>&lt;p&gt;
SentMix-3L: &#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#23391;&#21152;&#25289;&#35821;-&#33521;&#35821;-&#21360;&#22320;&#35821;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment Analysis. (arXiv:2310.18023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18023
&lt;/p&gt;
&lt;p&gt;
SentMix-3L&#26159;&#19968;&#20010;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#20043;&#38388;&#30340;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;SentMix-3L&#19978;&#65292;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#36229;&#36807;&#25152;&#26377;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#28151;&#21512;&#26159;&#19968;&#31181;&#30740;&#31350;&#24456;&#28145;&#30340;&#35821;&#35328;&#29616;&#35937;&#65292;&#25351;&#30340;&#26159;&#22312;&#25991;&#26412;&#25110;&#35821;&#38899;&#20013;&#28151;&#21512;&#20351;&#29992;&#20004;&#31181;&#25110;&#26356;&#22810;&#35821;&#35328;&#12290;&#24050;&#32463;&#26500;&#24314;&#20102;&#20960;&#20010;&#26088;&#22312;&#35757;&#32451;&#20195;&#30721;&#28151;&#21512;&#35745;&#31639;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#22810;&#35821;&#35328;&#30340;&#20195;&#30721;&#28151;&#21512;&#24456;&#24120;&#35265;&#65292;&#20294;&#22823;&#22810;&#25968;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#20004;&#31181;&#35821;&#35328;&#30340;&#20195;&#30721;&#28151;&#21512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SentMix-3L&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#20043;&#38388;&#30340;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;SentMix-3L&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#38646;-shot&#25552;&#31034;&#22312;SentMix-3L&#19978;&#20248;&#20110;&#25152;&#26377;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-mixing is a well-studied linguistic phenomenon when two or more languages are mixed in text or speech. Several datasets have been build with the goal of training computational models for code-mixing. Although it is very common to observe code-mixing with multiple languages, most datasets available contain code-mixed between only two languages. In this paper, we introduce SentMix-3L, a novel dataset for sentiment analysis containing code-mixed data between three languages Bangla, English, and Hindi. We carry out a comprehensive evaluation using SentMix-3L. We show that zero-shot prompting with GPT-3.5 outperforms all transformer-based models on SentMix-3L.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#37322;&#21477;&#23376;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#39044;&#27979;&#20196;&#29260;&#23545;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;</title><link>http://arxiv.org/abs/2310.05703</link><description>&lt;p&gt;
Siamese&#32534;&#30721;&#22120;&#30340;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Attribution Method for Siamese Encoders. (arXiv:2310.05703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#37322;&#21477;&#23376;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#39044;&#27979;&#20196;&#29260;&#23545;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21477;&#23376;&#36716;&#25442;&#22120;&#31561;Siamese&#32534;&#30721;&#22120;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#23545;&#23427;&#20204;&#20851;&#27880;&#30340;&#36755;&#20837;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#19968;&#20010;&#38556;&#30861;&#26159;&#23427;&#20204;&#30340;&#39044;&#27979;&#19981;&#33021;&#24402;&#22240;&#20110;&#20010;&#21035;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#20204;&#27604;&#36739;&#30340;&#26159;&#20004;&#20010;&#36755;&#20837;&#32780;&#19981;&#26159;&#19968;&#20010;&#36755;&#20837;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#29305;&#24449;&#23545;&#24402;&#22240;&#30340;&#24418;&#24335;&#65292;&#24182;&#21487;&#23558;&#20854;&#31616;&#21270;&#20026;&#21477;&#23376;&#36716;&#25442;&#22120;&#30340;&#20196;&#29260;-&#20196;&#29260;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24341;&#20837;&#38598;&#25104;&#38597;&#21487;&#27604;&#30697;&#38453;&#65292;&#24182;&#32487;&#25215;&#20102;&#38598;&#25104;&#26799;&#24230;&#30340;&#20248;&#21183;&#24418;&#24335;&#29305;&#24615;&#65306;&#23427;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#23436;&#25972;&#35745;&#31639;&#22270;&#65292;&#24182;&#30830;&#20445;&#25910;&#25947;&#21040;&#23454;&#38469;&#39044;&#27979;&#32467;&#26524;&#12290;&#19968;&#39033;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21477;&#23376;&#36716;&#25442;&#22120;&#20013;&#65292;&#24456;&#23569;&#30340;&#20196;&#29260;&#23545;&#24448;&#24448;&#21487;&#20197;&#35299;&#37322;&#22823;&#37096;&#20998;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23427;&#38656;&#35201;&#20851;&#27880;&#22823;&#22810;&#25968;&#30340;&#20196;&#29260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This paper derives a local attribution method for Siamese encoders by generalizing the principle of integrated gradients to models with multiple inputs. The solution takes the form of feature-pair attributions, and can be reduced to a token-token matrix for STs. Our method involves the introduction of integrated Jacobians and inherits the advantageous formal properties of integrated gradients: it accounts for the model's full computation graph and is guaranteed to converge to the actual prediction. A pilot study shows that in an ST few token-pairs can often explain large fractions of predictions, and it focuses on nouns and verbs. For accurate predictions, it however needs to attend to the majorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24212;&#29992;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.05199</link><description>&lt;p&gt;
&#23485;&#26494;&#30340;&#22068;&#21767;&#20250;&#20351;&#33337;&#27785;&#27809;&#65306;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback. (arXiv:2310.05199v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24212;&#29992;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#26159;&#37325;&#35201;&#30340;&#26725;&#26753;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#40784;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#35821;&#26009;&#24211;&#26469;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22870;&#21169;&#27169;&#22411;&#24120;&#24120;&#20250;&#25214;&#21040;&#32469;&#36807;&#39044;&#26399;&#30446;&#26631;&#30340;&#25463;&#24452;&#65292;&#38169;&#35823;&#22320;&#20551;&#35774;&#20154;&#31867;&#26356;&#21916;&#27426;&#36739;&#38271;&#30340;&#22238;&#31572;&#12290;&#38271;&#24230;&#20559;&#24046;&#30340;&#20986;&#29616;&#24120;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#20542;&#21521;&#20110;&#36739;&#38271;&#30340;&#36755;&#20986;&#65292;&#20294;&#24182;&#19981;&#24847;&#21619;&#30528;&#36825;&#20123;&#36755;&#20986;&#20013;&#26377;&#26356;&#22810;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#23558;&#22870;&#21169;&#24314;&#27169;&#19982;&#24207;&#21015;&#38271;&#24230;&#30340;&#24433;&#21709;&#20998;&#31163;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#20559;&#35265;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25200;&#21160;&#36827;&#20837;&#20559;&#24046;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. This alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs. In this paper, we propose an innovative solution, applying the Product-of-Experts (PoE) technique to separate reward modeling from the influence of sequence length. In our framework, the main expert concentrates on understanding human intents, while the biased expert targets the identification and capture of length bias. To further enhance the learning of bias, we introduce perturbations into the bia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01929</link><description>&lt;p&gt;
&#31359;&#36234;&#25991;&#21270;&#40511;&#27807;&#65306;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#65292;&#20363;&#22914;DALL-E&#21644;StableDiffusion&#65292;&#22312;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#38646;&#23556;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#20316;&#20026;&#25991;&#21270;&#30340;&#23186;&#20171;&#65292;&#35821;&#35328;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32780;&#22609;&#36896;&#20102;&#23427;&#20204;&#30340;&#25991;&#21270;&#26426;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25551;&#36848;&#25991;&#21270;&#32500;&#24230;&#65292;&#25991;&#21270;&#39046;&#22495;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#19977;&#20010;&#23618;&#27425;&#26469;&#25506;&#32034;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#25216;&#26415;&#65292;&#21253;&#25324;&#20351;&#29992;CLIP&#31354;&#38388;&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36827;&#34892;&#22806;&#22312;&#35780;&#20272;&#20197;&#21450;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;TTI&#25991;&#21270;&#24863;&#30693;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CulText2I&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#30340;TTI&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21313;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;
&lt;/p&gt;
&lt;p&gt;
Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have recently gained prominence for their remarkable zero-shot capabilities in generating images guided by textual prompts. Language, as a conduit of culture, plays a pivotal role in these models' multilingual capabilities, which in turn shape their cultural agency. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. We propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model, and human assessments, to discern TTI cultural perceptions. To facilitate our research, we introduce the CulText2I dataset, derived from four diverse TTI models and spanning ten languages. Our experiments reveal insights into these models' cultural awareness, cultural distinctions, and the
&lt;/p&gt;</description></item><item><title>DeepSpeed-VisualChat&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#36718;&#22810;&#22270;&#20132;&#38169;&#32842;&#22825;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#22240;&#26524;&#20851;&#27880;&#26426;&#21046;&#21644;&#25968;&#25454;&#34701;&#21512;&#25216;&#26415;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14327</link><description>&lt;p&gt;
DeepSpeed-VisualChat&#65306;&#36890;&#36807;&#22810;&#27169;&#24577;&#22240;&#26524;&#20851;&#27880;&#23454;&#29616;&#30340;&#22810;&#36718;&#22810;&#22270;&#20132;&#38169;&#32842;&#22825;
&lt;/p&gt;
&lt;p&gt;
DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention. (arXiv:2309.14327v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14327
&lt;/p&gt;
&lt;p&gt;
DeepSpeed-VisualChat&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#36718;&#22810;&#22270;&#20132;&#38169;&#32842;&#22825;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#22240;&#26524;&#20851;&#27880;&#26426;&#21046;&#21644;&#25968;&#25454;&#34701;&#21512;&#25216;&#26415;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#22810;&#27169;&#24577;&#27169;&#22411;&#30001;&#20110;&#26080;&#27861;&#29087;&#32451;&#22320;&#22788;&#29702;&#22810;&#22270;&#12289;&#22810;&#22238;&#21512;&#23545;&#35805;&#20013;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#38754;&#20020;&#30528;&#22312;&#35757;&#32451;&#36164;&#28304;&#20998;&#37197;&#21644;&#25968;&#25454;&#21487;&#35775;&#38382;&#24615;&#26041;&#38754;&#30340;&#37325;&#35201;&#38480;&#21046;&#65292;&#36825;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#20132;&#20114;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DeepSpeed-VisualChat &#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#22810;&#27169;&#24577;&#21151;&#33021;&#65292;&#38598;&#20013;&#25552;&#39640;&#22823;&#22411;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#20132;&#38169;&#36755;&#20837;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26174;&#33879;&#29305;&#28857;&#22312;&#20110;&#65306;(1) &#25552;&#20379;&#23545;&#22810;&#36718;&#22810;&#22270;&#23545;&#35805;&#30340;&#24320;&#28304;&#25903;&#25345;&#65292;(2) &#24341;&#20837;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#22240;&#26524;&#20851;&#27880;&#26426;&#21046;&#65292;&#20197;&#21450; (3) &#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#25968;&#25454;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#22810;&#36718;&#22810;&#22270;&#23545;&#35805;&#20013;&#30340;&#26080;&#32541;&#20132;&#20114;&#12290;&#19982;&#29616;&#26377;&#26694;&#26550;&#30456;&#27604;&#65292;DeepSpeed-VisualChat &#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#34920;&#29616;&#65292;&#21487;&#36798;&#21040; 70B &#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the existing multi-modal models, hindered by their incapacity to adeptly manage interleaved image-and-text inputs in multi-image, multi-round dialogues, face substantial constraints in resource allocation for training and data accessibility, impacting their adaptability and scalability across varied interaction realms. To address this, we present the DeepSpeed-VisualChat framework, designed to optimize Large Language Models (LLMs) by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs. Our framework is notable for (1) its open-source support for multi-round and multi-image dialogues, (2) introducing an innovative multi-modal causal attention mechanism, and (3) utilizing data blending techniques on existing datasets to assure seamless interactions in multi-round, multi-image conversations. Compared to existing frameworks, DeepSpeed-VisualChat shows superior scalability up to 70B para
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#65292;&#36890;&#36807;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12931</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#30340;&#20998;&#21035;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12931
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#65292;&#36890;&#36807;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#30340;&#35757;&#32451;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#20197;&#24448;&#30340;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65288;&#22914;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65289;&#36890;&#24120;&#20250;&#20026;[CLS]&#31526;&#21495;&#21644;&#26631;&#35760;&#20351;&#29992;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#32531;&#35299;&#23558;&#30456;&#21516;&#30340;&#24402;&#19968;&#21270;&#32479;&#35745;&#25968;&#25454;&#24212;&#29992;&#20110;&#20004;&#31181;&#26631;&#35760;&#31867;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#36127;&#38754;&#25928;&#26524;&#65292;&#36825;&#20123;&#32479;&#35745;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#19982;&#23427;&#20204;&#21508;&#33258;&#30340;&#35282;&#33394;&#26368;&#20339;&#21305;&#37197;&#12290;&#36890;&#36807;&#20351;&#29992;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;[CLS]&#23884;&#20837;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#22659;&#20449;&#24687;&#65292;&#24182;&#22312;&#20854;&#38750;&#21508;&#21521;&#21516;&#24615;&#31354;&#38388;&#20013;&#20998;&#24067;&#26356;&#22343;&#21248;&#12290;&#24403;&#29992;&#36825;&#20004;&#20010;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#26367;&#25442;&#24120;&#35268;&#30340;&#24402;&#19968;&#21270;&#23618;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;&#20102;2.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised training methods for transformers have demonstrated remarkable performance across various domains. Previous transformer-based models, such as masked autoencoders (MAE), typically utilize a single normalization layer for both the [CLS] symbol and the tokens. We propose in this paper a simple modification that employs separate normalization layers for the tokens and the [CLS] symbol to better capture their distinct characteristics and enhance downstream task performance. Our method aims to alleviate the potential negative effects of using the same normalization statistics for both token types, which may not be optimally aligned with their individual roles. We empirically show that by utilizing a separate normalization layer, the [CLS] embeddings can better encode the global contextual information and are distributed more uniformly in its anisotropic space. When replacing the conventional normalization layer with the two separate layers, we observe an average 2.7% performa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01029</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainability for Large Language Models: A Survey. (arXiv:2309.01029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#19981;&#26126;&#30830;&#65292;&#36825;&#31181;&#32570;&#20047;&#36879;&#26126;&#24230;&#20026;&#19979;&#28216;&#24212;&#29992;&#24102;&#26469;&#20102;&#19981;&#24517;&#35201;&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38416;&#26126;&#23427;&#20204;&#30340;&#34892;&#20026;&#12289;&#38480;&#21046;&#21644;&#31038;&#20250;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#27010;&#36848;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#26681;&#25454;LLMs&#30340;&#35757;&#32451;&#33539;&#24335;&#23558;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#65306;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#21644;&#25552;&#31034;&#33539;&#24335;&#12290;&#23545;&#20110;&#27599;&#20010;&#33539;&#24335;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#29983;&#25104;&#20010;&#20307;&#39044;&#27979;&#30340;&#23616;&#37096;&#35299;&#37322;&#21644;&#25972;&#20307;&#27169;&#22411;&#30693;&#35782;&#30340;&#20840;&#23616;&#35299;&#37322;&#30340;&#30446;&#26631;&#21644;&#20027;&#35201;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.05052</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#24615;&#33021;&#20013;&#65292;&#21508;&#31181;&#28436;&#31034;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26631;&#31614;&#12289;&#36755;&#20837;&#20998;&#24067;&#21644;&#34917;&#20805;&#35299;&#37322;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#36825;&#20123;&#22240;&#32032;&#34987;&#20462;&#25913;&#25110;&#25200;&#21160;&#26102;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22522;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#24037;&#20316;&#23545;&#20110;&#36825;&#20123;&#20803;&#32032;&#22914;&#20309;&#24433;&#21709;ICL&#32473;&#20986;&#20102;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(XNLP)&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#23545;&#27604;&#28436;&#31034;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#23545;&#36755;&#20837;&#20998;&#24067;&#36827;&#34892;&#20102;&#31890;&#24230;&#32423;&#21035;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;ICL&#26041;&#38754;&#30340;&#25928;&#26524;&#26159;&#23384;&#22312;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels. Finally, we find that the effectiveness of complementary explanations in boos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33322;&#31354;&#39046;&#22495;&#30340;&#21477;&#23376;&#21464;&#25442;&#22120;&#35843;&#25972;&#26041;&#27861;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;TSDAE&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#28982;&#21518;&#22312;&#23569;&#37327;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#33322;&#31354;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09556</link><description>&lt;p&gt;
&#38024;&#23545;&#33322;&#31354;&#39046;&#22495;&#36827;&#34892;&#21477;&#23376;&#21464;&#25442;&#22120;&#30340;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adapting Sentence Transformers for the Aviation Domain. (arXiv:2305.09556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33322;&#31354;&#39046;&#22495;&#30340;&#21477;&#23376;&#21464;&#25442;&#22120;&#35843;&#25972;&#26041;&#27861;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;TSDAE&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#28982;&#21518;&#22312;&#23569;&#37327;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#33322;&#31354;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26377;&#25928;&#30340;&#21477;&#23376;&#34920;&#31034;&#23545;&#20110;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#35821;&#20041;&#25628;&#32034;&#12289;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#21644;&#32858;&#31867;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#20010;&#29992;&#20110;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#65292;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#21807;&#19968;&#29305;&#24449;&#30340;&#19987;&#19994;&#39046;&#22495;&#26102;&#65292;&#22914;&#33322;&#31354;&#39046;&#22495;&#65292;&#21487;&#33021;&#26080;&#27861;&#21457;&#25381;&#26368;&#20339;&#24615;&#33021;&#65292;&#22240;&#20026;&#33322;&#31354;&#39046;&#22495;&#21253;&#21547;&#29305;&#27530;&#26415;&#35821;&#12289;&#32553;&#20889;&#35789;&#21644;&#38750;&#20256;&#32479;&#35821;&#27861;&#31561;&#39046;&#22495;&#29305;&#26377;&#29305;&#28857;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20351;&#24471;&#38590;&#20197;&#19987;&#38376;&#35757;&#32451;&#33322;&#31354;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33322;&#31354;&#39046;&#22495;&#35843;&#25972;&#21477;&#23376;&#21464;&#25442;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#36807;&#31243;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#21547;&#33322;&#31354;&#25991;&#26412;&#25968;&#25454;&#30340;&#21464;&#24418;&#22120;&#21644;&#24207;&#21015;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;(TSDAE)&#20316;&#20026;&#36755;&#20837;&#26469;&#25552;&#39640;&#21021;&#22987;&#27169;&#22411;&#24615;&#33021;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#27880;&#37322;&#30340;&#33322;&#31354;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#26469;&#24494;&#35843;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#22312;&#20960;&#20010;&#19982;&#33322;&#31354;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#21464;&#25442;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning effective sentence representations is crucial for many Natural Language Processing (NLP) tasks, including semantic search, semantic textual similarity (STS), and clustering. While multiple transformer models have been developed for sentence embedding learning, these models may not perform optimally when dealing with specialized domains like aviation, which has unique characteristics such as technical jargon, abbreviations, and unconventional grammar. Furthermore, the absence of labeled datasets makes it difficult to train models specifically for the aviation domain. To address these challenges, we propose a novel approach for adapting sentence transformers for the aviation domain. Our method is a two-stage process consisting of pre-training followed by fine-tuning. During pre-training, we use Transformers and Sequential Denoising AutoEncoder (TSDAE) with aviation text data as input to improve the initial model performance. Subsequently, we fine-tune our models using a Natural 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SUR-adapter&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#39044;&#20808;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#29983;&#25104;&#22270;&#29255;&#26102;&#20351;&#29992;&#31616;&#30701;&#30340;&#21465;&#36848;&#25552;&#31034;&#12290;&#20316;&#32773;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SURD&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.05189</link><description>&lt;p&gt;
SUR-adapter&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25991;&#26412;-&#22270;&#20687;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models. (arXiv:2305.05189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SUR-adapter&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#39044;&#20808;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#29983;&#25104;&#22270;&#29255;&#26102;&#20351;&#29992;&#31616;&#30701;&#30340;&#21465;&#36848;&#25552;&#31034;&#12290;&#20316;&#32773;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SURD&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#30446;&#21069;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#21644;&#20869;&#23481;&#20016;&#23500;&#24230;&#30340;&#22270;&#20687;&#12290;&#20294;&#26159;&#65292;&#24403;&#36755;&#20837;&#30340;&#25552;&#31034;&#20026;&#31616;&#30701;&#30340;&#21465;&#36848;&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#35821;&#20041;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#23548;&#33268;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#36739;&#20302;&#12290;&#20026;&#20102;&#25552;&#39640;&#21465;&#36848;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;Semantic Understanding&#21644;Reasoning adapter&#65288;SUR-adapter&#65289;&#65292;&#29992;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#21644;&#27880;&#37322;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SURD&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;57,000&#20010;&#35821;&#20041;&#20462;&#27491;&#30340;&#22810;&#27169;&#24577;&#26679;&#26412;&#12290;&#27599;&#20010;&#26679;&#26412;&#37117;&#21253;&#21547;&#19968;&#20010;&#31616;&#21333;&#30340;&#21465;&#36848;&#25552;&#31034;&#65292;&#19968;&#20010;&#22797;&#26434;&#30340;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#25552;&#31034;&#21644;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#21465;&#36848;&#25552;&#31034;&#30340;&#35821;&#20041;&#34920;&#31034;&#19982;&#22797;&#26434;&#25552;&#31034;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#23558;&#20854;&#36716;&#31227;&#33267;&#25105;&#20204;&#30340;SUR-adapter&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models, which have emerged to become popular text-to-image generation models, can produce high-quality and content-rich images guided by textual prompts. However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation. To improve the capacities for narrative prompts, we propose a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models. To reach this goal, we first collect and annotate a new dataset SURD which consists of more than 57,000 semantically corrected multi-modal samples. Each sample contains a simple narrative prompt, a complex keyword-based prompt, and a high-quality image. Then, we align the semantic representation of narrative prompts to the complex prompts and transfer knowledge of large language models (LLMs) to our SUR-adapter vi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAPS&#30340;&#26694;&#26550;&#65292;&#20351;LLMs&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#32763;&#35793;&#30340;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#21253;&#25324;&#20998;&#26512;&#28304;&#25991;&#26412;&#24182;&#25552;&#21462;&#20851;&#38190;&#35789;&#12289;&#20027;&#39064;&#21644;&#30456;&#20851;&#28436;&#31034;&#20197;&#25351;&#23548;&#32763;&#35793;&#36807;&#31243;&#12290;&#35813;&#26694;&#26550;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26126;&#26174;&#20248;&#20110;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#20026;&#24320;&#23637;&#20351;&#29992;LLM&#23454;&#29616;&#20154;&#31867;&#21270;&#32763;&#35793;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.04118</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#20154;&#31867;&#21270;&#32763;&#35793;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Exploring Human-Like Translation Strategy with Large Language Models. (arXiv:2305.04118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MAPS&#30340;&#26694;&#26550;&#65292;&#20351;LLMs&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#32763;&#35793;&#30340;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#21253;&#25324;&#20998;&#26512;&#28304;&#25991;&#26412;&#24182;&#25552;&#21462;&#20851;&#38190;&#35789;&#12289;&#20027;&#39064;&#21644;&#30456;&#20851;&#28436;&#31034;&#20197;&#25351;&#23548;&#32763;&#35793;&#36807;&#31243;&#12290;&#35813;&#26694;&#26550;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26126;&#26174;&#20248;&#20110;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#20026;&#24320;&#23637;&#20351;&#29992;LLM&#23454;&#29616;&#20154;&#31867;&#21270;&#32763;&#35793;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#34920;&#29616;&#20986;&#20102;&#25509;&#36817;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#27700;&#24179;&#12290;&#22312;&#20854;&#22810;&#31181;&#25216;&#33021;&#20013;&#65292;LLM&#30340;&#32763;&#35793;&#33021;&#21147;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#32763;&#35793;&#20165;&#20851;&#27880;&#28304;&#30446;&#26631;&#26144;&#23556;&#19981;&#21516;&#65292;&#22522;&#20110;LLM&#30340;&#32763;&#35793;&#21487;&#20197;&#28508;&#22312;&#22320;&#27169;&#20223;&#20154;&#31867;&#32763;&#35793;&#30340;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#20250;&#37319;&#21462;&#35768;&#22810;&#20934;&#22791;&#27493;&#39588;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;MAPS&#26694;&#26550;&#65288;Multi-Aspect Prompting and Selection&#65289;&#25506;&#32034;&#36825;&#31181;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;LLM&#39318;&#20808;&#20998;&#26512;&#32473;&#23450;&#28304;&#25991;&#26412;&#24182;&#25552;&#21462;&#19977;&#20010;&#19982;&#32763;&#35793;&#30456;&#20851;&#30340;&#30693;&#35782;&#26041;&#38754;&#65306;&#20851;&#38190;&#35789;&#12289;&#20027;&#39064;&#21644;&#30456;&#20851;&#28436;&#31034;&#20197;&#25351;&#23548;&#32763;&#35793;&#36807;&#31243;&#12290;&#20026;&#20102;&#36807;&#28388;&#25481;&#22122;&#22768;&#21644;&#26080;&#29992;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#36136;&#37327;&#20272;&#35745;&#30340;&#36873;&#25321;&#26426;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#22810;&#20010;&#35821;&#35328;&#23545;&#21644;&#32763;&#35793;&#26041;&#21521;&#19978;&#26174;&#30528;&#20248;&#20110;&#22810;&#20010;&#24378;&#22522;&#32447;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#24320;&#23637;&#20351;&#29992;LLM&#23454;&#29616;&#20154;&#31867;&#21270;&#32763;&#35793;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. In contrast to traditional machine translation that focuses solely on source-target mapping, LLM-based translation can potentially mimic the human translation process that takes many preparatory steps to ensure high-quality translation. This work aims to explore this possibility by proposing the MAPS framework, which stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs to first analyze the given source text and extract three aspects of translation-related knowledge: keywords, topics and relevant demonstrations to guide the translation process. To filter out the noisy and unhelpful knowledge, we employ a selection mechanism based on quality estimation. Experiments sug
&lt;/p&gt;</description></item></channel></rss>