<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#21160;&#24577;&#20869;&#23384;&#21387;&#32553;&#65288;DMC&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#20851;&#38190;-&#20540;&#32531;&#23384;&#21387;&#32553;&#65292;&#27169;&#22411;&#23398;&#20064;&#22312;&#19981;&#21516;&#30340;&#22836;&#37096;&#21644;&#23618;&#20013;&#24212;&#29992;&#19981;&#21516;&#30340;&#21387;&#32553;&#29575;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;LLMs&#25913;&#35013;&#20026;DMC Transformers&#65292;&#22312;&#33258;&#22238;&#24402;&#25512;&#26029;&#20013;&#23454;&#29616;&#20102;&#39640;&#36798;~3.7&#20493;&#30340;&#21534;&#21520;&#37327;&#22686;&#21152;&#12290;</title><link>https://arxiv.org/abs/2403.09636</link><description>&lt;p&gt;
&#21160;&#24577;&#20869;&#23384;&#21387;&#32553;&#65306;&#29992;&#20110;&#21152;&#36895;&#25512;&#26029;&#30340;LLMs&#30340;&#25913;&#35013;
&lt;/p&gt;
&lt;p&gt;
Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09636
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21160;&#24577;&#20869;&#23384;&#21387;&#32553;&#65288;DMC&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#20851;&#38190;-&#20540;&#32531;&#23384;&#21387;&#32553;&#65292;&#27169;&#22411;&#23398;&#20064;&#22312;&#19981;&#21516;&#30340;&#22836;&#37096;&#21644;&#23618;&#20013;&#24212;&#29992;&#19981;&#21516;&#30340;&#21387;&#32553;&#29575;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;LLMs&#25913;&#35013;&#20026;DMC Transformers&#65292;&#22312;&#33258;&#22238;&#24402;&#25512;&#26029;&#20013;&#23454;&#29616;&#20102;&#39640;&#36798;~3.7&#20493;&#30340;&#21534;&#21520;&#37327;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25903;&#26609;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22312;&#20869;&#23384;&#20013;&#23384;&#20648;&#20851;&#38190;-&#20540;&#34920;&#31034;&#30340;&#32531;&#23384;&#20197;&#29992;&#20110;&#36807;&#21435;&#30340;&#26631;&#35760;&#65292;&#20854;&#22823;&#23567;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#21644;&#25209;&#22788;&#29702;&#22823;&#23567;&#21576;&#32447;&#24615;&#27604;&#20363;&#65292;&#22240;&#27492;&#29983;&#25104;&#20173;&#28982;&#20302;&#25928;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#20869;&#23384;&#21387;&#32553;&#65288;DMC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#20851;&#38190;-&#20540;&#32531;&#23384;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#27169;&#22411;&#23398;&#20064;&#22312;&#19981;&#21516;&#30340;&#22836;&#37096;&#21644;&#23618;&#20013;&#24212;&#29992;&#19981;&#21516;&#30340;&#21387;&#32553;&#29575;&#12290;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;LLMs&#65288;&#22914;Llama 2&#65288;7B&#12289;13B&#21644;70B&#65289;&#65289;&#25913;&#35013;&#20026;DMC Transformers&#65292;&#22312;NVIDIA H100 GPU&#19978;&#30340;&#33258;&#22238;&#24402;&#25512;&#26029;&#20013;&#23454;&#29616;&#20102;&#39640;&#36798;~3.7&#20493;&#30340;&#21534;&#21520;&#37327;&#22686;&#21152;&#12290;DMC&#36890;&#36807;&#22312;&#21407;&#22987;&#25968;&#25454;&#30340;&#21487;&#24573;&#30053;&#30334;&#20998;&#27604;&#19978;&#36827;&#34892;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#32780;&#24212;&#29992;&#65292;&#24182;&#19988;&#19981;&#28155;&#21152;&#20219;&#20309;&#39069;&#22806;&#21442;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39640;&#36798;4&#20493;&#32531;&#23384;&#21387;&#32553;&#30340;&#24773;&#20917;&#19979;&#65292;DMC&#20445;&#30041;&#20102;&#21407;&#22987;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#20248;&#20110;up-trained grouped-query a&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09636v1 Announce Type: new  Abstract: Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time. Most importantly, the model learns to apply different compression rates in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09635</link><description>&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09635
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;transformer&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#28145;&#24230;&#26041;&#38754;&#20173;&#28982;&#24456;&#38590;&#25193;&#23637;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#21069;&#21521;&#21644;&#21453;&#21521;&#20449;&#21495;&#30697;&#30340;&#20844;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#29702;&#35299;&#21644;&#32531;&#35299;&#19982;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#30456;&#20851;&#30340;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#31209;&#22349;&#32553;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;DeepScaleLM&#65292;&#19968;&#31181;&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#36890;&#36807;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#27169;&#22411;&#20013;&#20445;&#25345;&#21333;&#20301;&#36755;&#20986;/&#26799;&#24230;&#30697;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#20855;&#26377;100&#22810;&#23618;&#30340;&#38750;&#24120;&#28145;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;transformer&#27169;&#22411;&#21487;&#20197;&#26356;&#28145; - &#25105;&#20204;&#30340;&#28145;&#23618;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#20165;&#32534;&#30721;&#22120;&#12289;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#20307;&#65292;&#36866;&#29992;&#20110;Pre-LN&#21644;Post-LN transformers&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09635v1 Announce Type: cross  Abstract: In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These imp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;3D-VLA&#65292;&#36890;&#36807;&#23558;3D&#24863;&#30693;&#12289;&#25512;&#29702;&#21644;&#21160;&#20316;&#26080;&#32541;&#36830;&#25509;&#65292;&#24314;&#31435;&#19968;&#20010;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;VLA&#27169;&#22411;&#21482;&#33021;&#22788;&#29702;2D&#36755;&#20837;&#19988;&#24573;&#35270;&#19990;&#30028;&#21160;&#24577;&#19982;&#21160;&#20316;&#20043;&#38388;&#20851;&#31995;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.09631</link><description>&lt;p&gt;
3D-VLA: &#19968;&#20010;3D&#35270;&#35273;-&#35821;&#35328;-&#21160;&#20316;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
3D-VLA: A 3D Vision-Language-Action Generative World Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;3D-VLA&#65292;&#36890;&#36807;&#23558;3D&#24863;&#30693;&#12289;&#25512;&#29702;&#21644;&#21160;&#20316;&#26080;&#32541;&#36830;&#25509;&#65292;&#24314;&#31435;&#19968;&#20010;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;VLA&#27169;&#22411;&#21482;&#33021;&#22788;&#29702;2D&#36755;&#20837;&#19988;&#24573;&#35270;&#19990;&#30028;&#21160;&#24577;&#19982;&#21160;&#20316;&#20043;&#38388;&#20851;&#31995;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;-&#35821;&#35328;-&#21160;&#20316;&#65288;VLA&#65289;&#27169;&#22411;&#20381;&#36182;&#20110;2D&#36755;&#20837;&#65292;&#32570;&#20047;&#19982;&#26356;&#24191;&#38420;&#30340;3D&#29289;&#29702;&#19990;&#30028;&#34701;&#21512;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#36807;&#23398;&#20064;&#20174;&#24863;&#30693;&#21040;&#21160;&#20316;&#30340;&#30452;&#25509;&#26144;&#23556;&#26469;&#25191;&#34892;&#21160;&#20316;&#39044;&#27979;&#65292;&#24573;&#30053;&#20102;&#19990;&#30028;&#30340;&#24191;&#27867;&#21160;&#24577;&#21644;&#21160;&#20316;&#19982;&#21160;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#25317;&#26377;&#25551;&#32472;&#20851;&#20110;&#26410;&#26469;&#22330;&#26223;&#30340;&#24819;&#35937;&#65292;&#20197;&#30456;&#24212;&#22320;&#35268;&#21010;&#34892;&#21160;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31995;&#21015;&#26032;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#65292;&#26080;&#32541;&#22320;&#23558;3D&#24863;&#30693;&#12289;&#25512;&#29702;&#21644;&#21160;&#20316;&#36890;&#36807;&#19968;&#20010;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30456;&#36830;&#65292;&#25552;&#20986;&#20102;3D-VLA&#12290;&#20855;&#20307;&#22320;&#65292;3D-VLA&#24314;&#31435;&#22312;&#22522;&#20110;3D&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20043;&#19978;&#65292;&#24182;&#24341;&#20837;&#19968;&#32452;&#20132;&#20114;&#26631;&#35760;&#20197;&#19982;&#20855;&#36523;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23558;&#29983;&#25104;&#33021;&#21147;&#27880;&#20837;&#27169;&#22411;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#20855;&#36523;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;LLM&#23545;&#40784;&#20197;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09631v1 Announce Type: cross  Abstract: Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting
&lt;/p&gt;</description></item><item><title>Quiet-STaR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27867;&#21270;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.09629</link><description>&lt;p&gt;
Quiet-STaR: &#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#24049;&#23398;&#20250;&#24605;&#32771;&#21518;&#20877;&#35828;&#35805;
&lt;/p&gt;
&lt;p&gt;
Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09629
&lt;/p&gt;
&lt;p&gt;
Quiet-STaR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27867;&#21270;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#21644;&#20132;&#35848;&#26102;&#65292;&#20154;&#20204;&#26377;&#26102;&#20250;&#20572;&#19979;&#26469;&#24605;&#32771;&#12290;&#23613;&#31649;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#20316;&#21697;&#36890;&#24120;&#23558;&#25512;&#29702;&#26694;&#23450;&#20026;&#22238;&#31572;&#38382;&#39064;&#25110;&#23436;&#25104;&#20195;&#29702;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20294;&#25512;&#29702;&#20960;&#20046;&#37117;&#38544;&#21547;&#22312;&#25152;&#26377;&#20070;&#38754;&#25991;&#26412;&#20013;&#12290;&#20363;&#22914;&#65292;&#36825;&#36866;&#29992;&#20110;&#35777;&#26126;&#20013;&#26410;&#26126;&#30830;&#35828;&#26126;&#30340;&#27493;&#39588;&#65292;&#20197;&#21450;&#25903;&#25745;&#23545;&#35805;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#22312;&#33258;&#23398;&#20064;&#25512;&#29702;&#32773;&#65288;STaR&#65292;Zelikman&#31561;&#65292;2022&#65289;&#20013;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#25512;&#26029;&#26469;&#33258;&#38382;&#31572;&#20013;&#26377;&#29992;&#30340;&#24605;&#32771;&#65292;&#24182;&#23398;&#20064;&#37027;&#20123;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#30340;&#24605;&#32771;&#12290;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;--&#29702;&#24819;&#24773;&#20917;&#19979;, &#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20250;&#20174;&#20219;&#24847;&#25991;&#26412;&#20013;&#25512;&#26029;&#26410;&#26126;&#30830;&#35828;&#26126;&#30340;&#24605;&#32771;&#12290;&#25105;&#20204;&#25552;&#20986;Quiet-STaR&#65292;&#36825;&#26159;STaR&#30340;&#19968;&#20010;&#27867;&#21270;&#29256;&#26412;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#22312;&#27599;&#20010;&#26631;&#35760;&#22788;&#29983;&#25104;&#35299;&#37322;&#26410;&#26469;&#25991;&#26412;&#30340;&#24605;&#32771;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#39044;&#27979;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;1&#65289;&#29983;&#25104;&#36830;&#32493;&#30340;&#35745;&#31639;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09629v1 Announce Type: cross  Abstract: When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continu
&lt;/p&gt;</description></item><item><title>&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20381;&#27425;&#24494;&#35843;&#30340;LLMs&#34920;&#29616;&#20986;&#39044;&#26399;&#34892;&#20026;&#65292;&#33021;&#22815;&#20174;&#36951;&#24536;&#20013;&#24674;&#22797;&#65292;&#25581;&#31034;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#26032;&#35265;&#35299;</title><link>https://arxiv.org/abs/2403.09613</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#35757;&#32451;&#37325;&#26032;&#21796;&#37266;&#30693;&#35782;&#65306;&#20174;&#28798;&#38590;&#24615;&#24178;&#25200;&#20013;&#36827;&#34892;&#39044;&#26399;&#24615;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09613
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20381;&#27425;&#24494;&#35843;&#30340;LLMs&#34920;&#29616;&#20986;&#39044;&#26399;&#34892;&#20026;&#65292;&#33021;&#22815;&#20174;&#36951;&#24536;&#20013;&#24674;&#22797;&#65292;&#25581;&#31034;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#26032;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#35774;&#32622;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20854;&#20013;&#25991;&#26723;&#20197;&#22266;&#23450;&#37325;&#22797;&#24207;&#21015;&#30340;&#26041;&#24335;&#21576;&#29616;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22312;&#19968;&#31995;&#21015;&#25991;&#26723;&#19978;&#35757;&#32451;&#26102;&#65292;&#32593;&#32476;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#24178;&#25200;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#20381;&#27425;&#24494;&#35843;&#30340;LLMs&#34920;&#29616;&#20986;&#19968;&#31181;&#22855;&#29305;&#19988;&#21331;&#36234;&#30340;&#29305;&#24615;&#65306;&#23427;&#20204;&#34920;&#29616;&#20986;&#39044;&#26399;&#30340;&#34892;&#20026;&#65292;&#22312;&#20877;&#27425;&#36935;&#21040;&#20043;&#21069;&#30340;&#25991;&#26723;&#26102;&#20174;&#36951;&#24536;&#20013;&#24674;&#22797;&#36807;&#26469;&#12290;&#36825;&#31181;&#34892;&#20026;&#22312;&#26550;&#26500;&#25193;&#23637;&#20854;&#21442;&#25968;&#25968;&#37327;&#26102;&#36880;&#28176;&#20986;&#29616;&#24182;&#21464;&#24471;&#26356;&#21152;&#31283;&#20581;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#21644;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#35757;&#32451;&#36229;&#21442;&#25968;&#32593;&#32476;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09613v1 Announce Type: cross  Abstract: We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence. Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again. The behavior emerges and becomes more robust as the architecture scales up its number of parameters. Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.09611</link><description>&lt;p&gt;
MM1&#65306;&#22810;&#27169;&#24335;LLM&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12289;&#20998;&#26512;&#19982;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20180;&#32454;&#21644;&#20840;&#38754;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#32463;&#39564;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#22823;&#35268;&#27169;&#22810;&#27169;&#24335;&#39044;&#35757;&#32451;&#20351;&#29992;&#20180;&#32454;&#28151;&#21512;&#30340;&#22270;&#20687;&#26631;&#39064;&#12289;&#20132;&#26367;&#22270;&#20687;&#25991;&#26412;&#21644;&#20165;&#25991;&#26412;&#25968;&#25454;&#23545;&#20110;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#26032;&#28526;&#65288;SOTA&#65289;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#19982;&#20854;&#20182;&#24050;&#21457;&#34920;&#30340;&#39044;&#35757;&#32451;&#32467;&#26524;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#22270;&#20687;&#32534;&#30721;&#22120;&#36830;&#21516;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#22270;&#20687;&#26631;&#35760;&#35745;&#25968;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#32780;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#35774;&#35745;&#30456;&#23545;&#37325;&#35201;&#24615;&#36739;&#23567;&#12290;&#36890;&#36807;&#25193;&#22823;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MM1&#65292;&#19968;&#20010;&#22810;&#27169;&#24335;&#27169;&#22411;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26497;&#22823;&#24433;&#21709;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32780;&#26412;&#32508;&#36848;&#21017;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#26029;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12289;&#35299;&#20915;&#20844;&#24179;&#21644;&#23433;&#20840;&#38382;&#39064;&#12289;&#25552;&#20379;&#35299;&#37322;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.09606</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21327;&#20316;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09606
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26497;&#22823;&#24433;&#21709;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32780;&#26412;&#32508;&#36848;&#21017;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#26029;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12289;&#35299;&#20915;&#20844;&#24179;&#21644;&#23433;&#20840;&#38382;&#39064;&#12289;&#25552;&#20379;&#35299;&#37322;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36890;&#36807;&#25429;&#25417;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#24433;&#21709;&#20102;&#21508;&#31181;NLP&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#35843;&#26597;&#37325;&#28857;&#35780;&#20272;&#21644;&#25913;&#36827;LLMs&#30340;&#22240;&#26524;&#35270;&#35282;&#65292;&#22312;&#20197;&#19979;&#39046;&#22495;&#23637;&#24320;&#65306;&#29702;&#35299;&#21644;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;LLMs&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#20026;LLMs&#25552;&#20379;&#35299;&#37322;&#65292;&#24182;&#22788;&#29702;&#22810;&#27169;&#24577;&#12290;&#21516;&#26102;&#65292;LLMs&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21453;&#36807;&#26469;&#21487;&#20197;&#36890;&#36807;&#24110;&#21161;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#21644;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26469;&#20419;&#36827;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#26412;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#19982;LLMs&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#38598;&#20307;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09606v1 Announce Type: cross  Abstract: Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective p
&lt;/p&gt;</description></item><item><title>&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26102;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#26681;&#25454;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#26469;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2403.09559</link><description>&lt;p&gt;
&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#23545;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Less is More: Data Value Estimation for Visual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09559
&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26102;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#26681;&#25454;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#26469;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26159;&#26500;&#24314;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLLMs&#20027;&#35201;&#20381;&#36182;&#20110;&#22810;&#20010;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#28151;&#21512;&#35757;&#32451;&#65288;&#29978;&#33267;&#36229;&#36807;&#19968;&#30334;&#19975;&#26465;&#25351;&#23548;&#65289;&#65292;&#36825;&#21487;&#33021;&#24341;&#20837;&#25968;&#25454;&#20887;&#20313;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#20869;&#23384;&#22312;&#26174;&#33879;&#20887;&#20313;&#65292;&#24182;&#26174;&#31034;&#22823;&#22823;&#20943;&#23569;&#20960;&#20010;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#29978;&#33267;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#20197;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;TIVE&#39318;&#20808;&#26681;&#25454;&#35745;&#31639;&#30340;&#26799;&#24230;&#20272;&#35745;&#35270;&#35273;&#25351;&#23548;&#30340;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#20272;&#35745;&#30340;&#20215;&#20540;&#65292;TIVE&#30830;&#23450;&#20102;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#25351;&#23548;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09559v1 Announce Type: new  Abstract: Visual instruction tuning is the key to building multimodal large language models (MLLMs), which greatly improves the reasoning capabilities of large language models (LLMs) in vision scenario. However, existing MLLMs mostly rely on a mixture of multiple highly diverse visual instruction datasets for training (even more than a million instructions), which may introduce data redundancy. To investigate this issue, we conduct a series of empirical studies, which reveal a significant redundancy within the visual instruction datasets, and show that greatly reducing the amount of several instruction dataset even do not affect the performance. Based on the findings, we propose a new data selection approach TIVE, to eliminate redundancy within visual instruction data. TIVE first estimates the task-level and instance-level value of the visual instructions based on computed gradients. Then, according to the estimated values, TIVE determines the tas
&lt;/p&gt;</description></item><item><title>&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;</title><link>https://arxiv.org/abs/2403.09539</link><description>&lt;p&gt;
API&#20445;&#25252;&#30340;LLMs&#30340;&#26631;&#24535;&#27844;&#38706;&#19987;&#26377;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Logits of API-Protected LLMs Leak Proprietary Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09539
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21830;&#19994;&#21270;&#23548;&#33268;&#20102;&#39640;&#32423;API-only&#25509;&#20837;&#19987;&#26377;&#27169;&#22411;&#30340;&#24120;&#35265;&#23454;&#36341;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#27169;&#22411;&#26550;&#26500;&#26377;&#20445;&#23432;&#30340;&#20551;&#35774;&#65292;&#20063;&#21487;&#20197;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;API&#26597;&#35810;&#20013;&#23398;&#20064;&#20851;&#20110;API&#20445;&#25252;&#30340;LLM&#30340;&#22823;&#37327;&#38750;&#20844;&#24320;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;OpenAI&#30340;gpt-3.5-turbo&#20165;&#33457;&#36153;&#19981;&#21040;1000&#32654;&#20803;&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#38598;&#20013;&#22312;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#19978;&#65306;&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;&#20102;softmax&#29942;&#39048;&#30340;&#24433;&#21709;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#36755;&#20986;&#21040;&#23436;&#25972;&#36755;&#20986;&#31354;&#38388;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#27169;&#22411;&#22270;&#20687;&#25110;&#27169;&#22411;&#31614;&#21517;&#65292;&#20174;&#32780;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#35299;&#38145;&#20102;&#20960;&#31181;&#21151;&#33021;&#65306;&#26377;&#25928;&#21457;&#29616;LLM&#30340;&#38544;&#34255;&#22823;&#23567;&#65292;&#33719;&#21462;&#23436;&#25972;&#35789;&#27719;&#36755;&#20986;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#19981;&#21516;&#27169;&#22411;&#26356;&#26032;&#65292;&#35782;&#21035;&#32473;&#23450;&#21333;&#20010;&#23436;&#25972;LLM&#36755;&#20986;&#30340;&#28304;LLM&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09539v1 Announce Type: cross  Abstract: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and eve
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;VisionGPT-3D&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#23545;&#20110;3D&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.09530</link><description>&lt;p&gt;
VisionGPT-3D:&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;3D&#35270;&#35273;&#29702;&#35299;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09530
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;VisionGPT-3D&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#23545;&#20110;3D&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21521;&#35270;&#35273;&#32452;&#20214;&#30340;&#28436;&#36827;&#20419;&#36827;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20415;&#21033;&#65292;&#20363;&#22914;&#20174;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#12289;&#35270;&#39057;&#24182;&#35782;&#21035;&#22270;&#20687;&#20013;&#25152;&#38656;&#30340;&#20803;&#32032;&#12290;&#20197;&#21069;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#19987;&#27880;&#20110;&#22522;&#20110;&#26126;&#30830;&#23450;&#20041;&#23545;&#35937;&#30340;&#22270;&#20687;&#26816;&#27979;&#12289;&#20998;&#31867;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;&#35270;&#35273;&#23545;&#35937;&#65292;&#20026;&#25991;&#26412;&#32972;&#26223;&#25552;&#20379;&#20102;&#35270;&#35273;&#24067;&#23616;&#12290;OpenAI GPT-4&#24050;&#25104;&#20026;LLMs&#30340;&#39030;&#23792;&#65292;&#32780;&#35745;&#31639;&#26426;&#35270;&#35273;(CV)&#39046;&#22495;&#25317;&#26377;&#22823;&#37327;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#21487;&#23558;2D&#22270;&#20687;&#36716;&#25442;&#20026;&#23427;&#20204;&#30340;3D&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#31639;&#27861;&#19982;&#38382;&#39064;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#19981;&#33391;&#32467;&#26524;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;VisionGPT-3D&#26694;&#26550;&#65292; conslidate&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09530v1 Announce Type: cross  Abstract: The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images. Computer vision models involving the multimodal abilities in the previous days are focused on image detection, classification based on well-defined objects. Large language models (LLMs) introduces the transformation from nature language to visual objects, which present the visual layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs, while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations. However, the mismatching between the algorithms with the problem could lead to undesired results. In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MT-Patcher&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21040;&#20013;&#31561;&#35268;&#27169;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26377;&#36873;&#25321;&#24615;&#12289;&#20840;&#38754;&#21644;&#20027;&#21160;&#30340;&#30693;&#35782;&#36801;&#31227;</title><link>https://arxiv.org/abs/2403.09522</link><description>&lt;p&gt;
MT-PATCHER&#65306;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#36873;&#25321;&#24615;&#21644;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09522
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MT-Patcher&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21040;&#20013;&#31561;&#35268;&#27169;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26377;&#36873;&#25321;&#24615;&#12289;&#20840;&#38754;&#21644;&#20027;&#21160;&#30340;&#30693;&#35782;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24310;&#36831;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23558;&#32763;&#35793;&#30693;&#35782;&#20174;&#24040;&#22411;LLM&#36716;&#31227;&#21040;&#20013;&#31561;&#35268;&#27169;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MT-Patcher&#30340;&#26694;&#26550;&#65292;&#20197;&#36873;&#25321;&#24615;&#12289;&#20840;&#38754;&#21644;&#20027;&#21160;&#30340;&#26041;&#24335;&#23558;&#30693;&#35782;&#20174;LLMs&#36716;&#31227;&#21040;&#29616;&#26377;&#30340;MT&#27169;&#22411;&#20013;&#12290;&#32771;&#34385;&#21040;&#23398;&#29983;MT&#27169;&#22411;&#24403;&#21069;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#25105;&#20204;&#20165;&#35782;&#21035;&#21644;&#32416;&#27491;&#20854;&#32763;&#35793;&#38169;&#35823;&#65292;&#32780;&#19981;&#26159;&#20174;&#32769;&#24072;&#37027;&#37324;&#33976;&#39311;&#25972;&#20010;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09522v1 Announce Type: new  Abstract: Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation (MT), yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong languag
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20856;&#22411;&#20154;&#21475;&#32479;&#35745;&#25991;&#26412;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#20943;&#36731;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#26174;&#24335;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.09516</link><description>&lt;p&gt;
&#21033;&#29992;&#20856;&#22411;&#34920;&#31034;&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#32780;&#19981;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09516
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20856;&#22411;&#20154;&#21475;&#32479;&#35745;&#25991;&#26412;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#20943;&#36731;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#26174;&#24335;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#36890;&#24120;&#38656;&#35201;&#35782;&#21035;&#19982;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30456;&#20851;&#32852;&#30340;&#31038;&#20250;&#32676;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DAFair&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#38382;&#39064;&#12290;&#19982;&#20381;&#36182;&#26174;&#24335;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#27492;&#31867;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20154;&#21475;&#32479;&#35745;&#20856;&#22411;&#25991;&#26412;&#65292;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#20943;&#36731;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#21644;&#20004;&#20010;&#27169;&#22411;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#19981;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#27880;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20248;&#20110;&#24120;&#35265;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09516v1 Announce Type: new  Abstract: Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#20223;&#30495;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#30340;&#36235;&#21183;&#21644;&#25511;&#21046;&#65292;&#27599;&#20010;&#20195;&#29702;&#20154;&#22312;&#20223;&#30495;&#20013;&#20195;&#34920;&#20855;&#26377;&#29420;&#29305;&#20010;&#24615;&#30340;&#20010;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.09498</link><description>&lt;p&gt;
&#20174;&#24576;&#30097;&#21040;&#25509;&#21463;&#65306;&#27169;&#25311;&#23545;&#34394;&#20551;&#26032;&#38395;&#24577;&#24230;&#21160;&#24577;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#20223;&#30495;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#30340;&#36235;&#21183;&#21644;&#25511;&#21046;&#65292;&#27599;&#20010;&#20195;&#29702;&#20154;&#22312;&#20223;&#30495;&#20013;&#20195;&#34920;&#20855;&#26377;&#29420;&#29305;&#20010;&#24615;&#30340;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#34394;&#20551;&#26032;&#38395;&#21644;&#35875;&#35328;&#36890;&#36807;&#31038;&#20132;&#32593;&#32476;&#36805;&#36895;&#20256;&#25773;&#65292;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#31038;&#20250;&#25361;&#25112;&#65292;&#24433;&#21709;&#30528;&#20844;&#20247;&#33286;&#35770;&#12290;&#20256;&#32479;&#30340;&#34394;&#20551;&#26032;&#38395;&#24314;&#27169;&#36890;&#24120;&#39044;&#27979;&#19981;&#21516;&#32676;&#20307;&#30340;&#26222;&#36941;&#27969;&#34892;&#36235;&#21183;&#25110;&#25968;&#23383;&#21270;&#20195;&#34920;&#24847;&#35265;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#36807;&#20110;&#31616;&#21270;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#24615;&#65292;&#24573;&#35270;&#20102;&#26032;&#38395;&#25991;&#26412;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#27169;&#25311;&#24494;&#22937;&#24847;&#35265;&#21160;&#24577;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;LLM&#30340;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#20223;&#30495;&#26694;&#26550;&#65288;FPS&#65289;&#65292;&#35814;&#32454;&#30740;&#31350;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#30340;&#36235;&#21183;&#21644;&#25511;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#20223;&#30495;&#20013;&#30340;&#27599;&#20010;&#20195;&#29702;&#20154;&#20195;&#34920;&#20855;&#26377;&#29420;&#29305;&#20010;&#24615;&#30340;&#20010;&#20154;&#12290;&#20182;&#20204;&#37197;&#22791;&#20102;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#21450;&#21453;&#24605;&#26426;&#21046;&#26469;&#27169;&#20223;&#31867;&#20154;&#24605;&#32500;&#12290;&#27599;&#22825;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09498v1 Announce Type: cross  Abstract: In the digital era, the rapid propagation of fake news and rumors via social networks brings notable societal challenges and impacts public opinion regulation. Traditional fake news modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift. However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text. The advent of large language models (LLMs) provides the possibility of modeling subtle dynamics of opinion. Consequently, in this work, we introduce a Fake news Propagation Simulation framework (FPS) based on LLM, which studies the trends and control of fake news propagation in detail. Specifically, each agent in the simulation represents an individual with a distinct personality. They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking. Every day, the
&lt;/p&gt;</description></item><item><title>Hyper-CL&#26159;&#19968;&#31181;&#23558;&#36229;&#32593;&#32476;&#19982;&#23545;&#27604;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#36827;&#34892;&#26465;&#20214;&#21270;&#21477;&#23376;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.09490</link><description>&lt;p&gt;
Hyper-CL&#65306;&#20351;&#29992;&#36229;&#32593;&#32476;&#23545;&#21477;&#23376;&#34920;&#31034;&#36827;&#34892;&#26465;&#20214;&#21270;
&lt;/p&gt;
&lt;p&gt;
Hyper-CL: Conditioning Sentence Representations with Hypernetworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09490
&lt;/p&gt;
&lt;p&gt;
Hyper-CL&#26159;&#19968;&#31181;&#23558;&#36229;&#32593;&#32476;&#19982;&#23545;&#27604;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#36827;&#34892;&#26465;&#20214;&#21270;&#21477;&#23376;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23558;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#24341;&#20837;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20419;&#36827;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21477;&#23376;&#34987;&#29305;&#23450;&#35282;&#24230;&#26465;&#20214;&#21270;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#25429;&#25417;&#21477;&#23376;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#26041;&#38754;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#21477;&#23376;&#23884;&#20837;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;Hyper-CL&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#36229;&#32593;&#32476;&#19982;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#35745;&#31639;&#26465;&#20214;&#21270;&#30340;&#21477;&#23376;&#34920;&#31034;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#36229;&#32593;&#32476;&#36127;&#36131;&#23558;&#39044;&#20808;&#35745;&#31639;&#30340;&#26465;&#20214;&#23884;&#20837;&#36716;&#25442;&#20026;&#30456;&#24212;&#30340;&#25237;&#24433;&#23618;&#12290;&#36825;&#20351;&#24471;&#30456;&#21516;&#30340;&#21477;&#23376;&#23884;&#20837;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#26465;&#20214;&#36827;&#34892;&#19981;&#21516;&#30340;&#25237;&#24433;&#12290;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#26465;&#20214;&#21270;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#21363;&#26465;&#20214;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#21644;&#30693;&#35782;&#22270;&#23436;&#25104;&#65292;&#34920;&#26126;Hyper-CL&#22312;&#28789;&#27963;&#22320;&#36827;&#34892;&#26465;&#20214;&#21270;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09490v1 Announce Type: new  Abstract: While the introduction of contrastive learning frameworks in sentence representation learning has significantly contributed to advancements in the field, it still remains unclear whether state-of-the-art sentence embeddings can capture the fine-grained semantics of sentences, particularly when conditioned on specific perspectives. In this paper, we introduce Hyper-CL, an efficient methodology that integrates hypernetworks with contrastive learning to compute conditioned sentence representations. In our proposed approach, the hypernetwork is responsible for transforming pre-computed condition embeddings into corresponding projection layers. This enables the same sentence embeddings to be projected differently according to various conditions. Evaluation on two representative conditioning benchmarks, namely conditional semantic text similarity and knowledge graph completion, demonstrates that Hyper-CL is effective in flexibly conditioning s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26126;&#31034;&#24847;&#35782;&#26657;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09488</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#32416;&#27491;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Rectifying Demonstration Shortcut in In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26126;&#31034;&#24847;&#35782;&#26657;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#20165;&#20973;&#23569;&#37327;&#28436;&#31034;&#20415;&#33021;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#24120;&#24120;&#20381;&#36182;&#20110;&#23427;&#20204;&#23545;&#28436;&#31034;&#30340;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#20041;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#26681;&#25454;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#32487;&#32493;&#36827;&#34892;ICL&#39044;&#27979;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#29616;&#35937;&#31216;&#20026;&#8220;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#8221;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#25913;&#36827;&#39044;&#23450;&#20041;&#20219;&#21153;&#30340;ICL&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32416;&#27491;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#20174;&#32780;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#30340;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26126;&#31034;&#24847;&#35782;&#30340;&#26657;&#20934;&#26041;&#27861;&#65306;In-Context Calibration&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#65288;1&#65289;&#20351;&#29992;&#26631;&#20934;&#26631;&#31614;&#31354;&#38388;&#30340;&#21407;&#22987;ICL&#20219;&#21153;&#20197;&#21450;&#65288;2&#65289;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#31614;&#31354;&#38388;&#34987;&#35821;&#20041;&#26080;&#20851;&#30340;&#26631;&#35760;&#26367;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09488v1 Announce Type: cross  Abstract: Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities. However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction. In this work, we term this phenomenon as the `Demonstration Shortcut'. While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations. To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method. We evaluate the effectiveness of the proposed method in two settings: (1) the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens. In 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26356;&#38590;&#25512;&#29702;&#20219;&#21153;&#30340;&#26377;&#25928;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#23545;&#40784;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09472</link><description>&lt;p&gt;
&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#65306;&#36229;&#36234;&#20154;&#31867;&#30417;&#30563;&#30340;&#21487;&#25193;&#23637;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09472
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26356;&#38590;&#25512;&#29702;&#20219;&#21153;&#30340;&#26377;&#25928;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#23545;&#40784;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#20154;&#31867;&#25552;&#20379;&#30340;&#28436;&#31034;&#25110;&#21028;&#26029;&#65292;&#30001;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;AI&#31995;&#32479;&#23398;&#20064;&#21040;&#30340;&#33021;&#21147;&#23558;&#21463;&#21040;&#20154;&#31867;&#33021;&#21147;&#30340;&#19978;&#30028;&#38480;&#21046;&#12290;&#36825;&#23601;&#24102;&#26469;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#24403;&#31995;&#32479;&#30340;&#33021;&#21147;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#32487;&#32493;&#25913;&#36827;&#36825;&#20123;&#31995;&#32479;&#65311;&#26412;&#25991;&#22312;&#35299;&#20915;&#38590;&#24230;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;4-5&#32423;&#25968;&#23398;&#38382;&#39064;&#65289;&#30340;&#32972;&#26223;&#19979;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#65288;&#22914;1-3&#32423;&#25968;&#23398;&#38382;&#39064;&#65289;&#20013;&#23398;&#20064;&#20154;&#31867;&#27880;&#37322;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#8221;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#19968;&#20010;&#22312;&#26356;&#31616;&#21333;&#20219;&#21153;&#30340;&#30417;&#30563;&#19979;&#35757;&#32451;&#30340;&#35780;&#20272;&#22120;&#65288;&#22870;&#21169;&#27169;&#22411;&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#35780;&#20998;&#26356;&#38590;&#20219;&#21153;&#30340;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#20419;&#36827;&#22312;&#19981;&#21516;&#38590;&#24230;&#20219;&#21153;&#38388;&#30340;&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#23545;&#40784;&#26041;&#27861;&#65292;&#39318;&#20808;&#35757;&#32451;&#22788;&#29702;&#30563;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09472v1 Announce Type: cross  Abstract: Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervise
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;ChatGPT&#65292;&#30740;&#31350;&#20102;&#36229;&#36807;350&#21517;&#19968;&#24180;&#32423;&#35745;&#31639;&#26426;&#23398;&#29983;&#29983;&#25104;&#30340;&#36882;&#24402;&#31867;&#27604;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#31867;&#27604;&#24110;&#21161;&#29702;&#35299;&#22797;&#26434;&#30340;&#35745;&#31639;&#27010;&#24565;</title><link>https://arxiv.org/abs/2403.09409</link><description>&lt;p&gt;
"&#20687;&#22871;&#23043;&#19968;&#26679;"&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#29983;&#25104;&#30340;&#36882;&#24402;&#31867;&#27604;
&lt;/p&gt;
&lt;p&gt;
"Like a Nesting Doll": Analyzing Recursion Analogies Generated by CS Students using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09409
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;ChatGPT&#65292;&#30740;&#31350;&#20102;&#36229;&#36807;350&#21517;&#19968;&#24180;&#32423;&#35745;&#31639;&#26426;&#23398;&#29983;&#29983;&#25104;&#30340;&#36882;&#24402;&#31867;&#27604;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#31867;&#27604;&#24110;&#21161;&#29702;&#35299;&#22797;&#26434;&#30340;&#35745;&#31639;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25226;&#22797;&#26434;&#30340;&#35745;&#31639;&#27010;&#24565;&#34701;&#20250;&#36143;&#36890;&#24120;&#24120;&#26159;&#23398;&#29983;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20182;&#20204;&#24448;&#24448;&#38590;&#20197;&#23558;&#36825;&#20123;&#26032;&#24819;&#27861;&#38170;&#23450;&#22312;&#29087;&#24713;&#30340;&#32463;&#39564;&#21644;&#29702;&#35299;&#20043;&#19978;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#19968;&#20010;&#22909;&#30340;&#31867;&#27604;&#21487;&#20197;&#24357;&#21512;&#38476;&#29983;&#27010;&#24565;&#19982;&#29087;&#24713;&#27010;&#24565;&#20043;&#38388;&#30340;&#40511;&#27807;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24341;&#20154;&#20837;&#32988;&#30340;&#26041;&#24335;&#26469;&#24110;&#21161;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#32463;&#39564;&#20016;&#23500;&#30340;&#25945;&#24072;&#26469;&#35828;&#65292;&#21019;&#36896;&#26377;&#25928;&#30340;&#25945;&#32946;&#31867;&#27604;&#20063;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;ChatGPT&#65292;&#21040;&#24213;&#33021;&#22815;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#20379;&#25353;&#38656;&#35775;&#38382;&#20010;&#20154;&#30456;&#20851;&#31867;&#27604;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36882;&#24402;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38376;&#27099;&#27010;&#24565;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;350&#22810;&#21517;&#19968;&#24180;&#32423;&#35745;&#31639;&#26426;&#23398;&#29983;&#29983;&#25104;&#30340;&#31867;&#27604;&#12290;&#20182;&#20204;&#34987;&#35201;&#27714;&#20351;&#29992;ChatGPT&#29983;&#25104;&#22522;&#20110;&#36882;&#24402;&#30340;&#31867;&#27604;&#65292;&#20854;&#20013;&#21487;&#20197;&#36873;&#25321;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#20010;&#20154;&#30456;&#20851;&#20027;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29983;&#25104;&#30340;&#31867;&#27604;&#21576;&#29616;&#20986;&#26497;&#22823;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09409v1 Announce Type: cross  Abstract: Grasping complex computing concepts often poses a challenge for students who struggle to anchor these new ideas to familiar experiences and understandings. To help with this, a good analogy can bridge the gap between unfamiliar concepts and familiar ones, providing an engaging way to aid understanding. However, creating effective educational analogies is difficult even for experienced instructors. We investigate to what extent large language models (LLMs), specifically ChatGPT, can provide access to personally relevant analogies on demand. Focusing on recursion, a challenging threshold concept, we conducted an investigation analyzing the analogies generated by more than 350 first-year computing students. They were provided with a code snippet and tasked to generate their own recursion-based analogies using ChatGPT, optionally including personally relevant topics in their prompts. We observed a great deal of diversity in the analogies p
&lt;/p&gt;</description></item><item><title>Komodo-7B&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26080;&#32541;&#25805;&#20316;&#21360;&#24230;&#23612;&#35199;&#20122;&#12289;&#33521;&#35821;&#21644;11&#31181;&#21360;&#24230;&#23612;&#35199;&#20122;&#22320;&#21306;&#35821;&#35328;&#65292;Komodo-7B-Instruct&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#22810;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09362</link><description>&lt;p&gt;
&#31185;&#33707;&#22810;&#65306;&#25506;&#32034;&#21360;&#24230;&#23612;&#35199;&#20122;&#22320;&#21306;&#35821;&#35328;&#30340;&#35821;&#35328;&#32771;&#23519;
&lt;/p&gt;
&lt;p&gt;
Komodo: A Linguistic Expedition into Indonesia's Regional Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09362
&lt;/p&gt;
&lt;p&gt;
Komodo-7B&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26080;&#32541;&#25805;&#20316;&#21360;&#24230;&#23612;&#35199;&#20122;&#12289;&#33521;&#35821;&#21644;11&#31181;&#21360;&#24230;&#23612;&#35199;&#20122;&#22320;&#21306;&#35821;&#35328;&#65292;Komodo-7B-Instruct&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#22810;&#20010;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#21462;&#24471;&#30340;&#31361;&#30772;&#20027;&#35201;&#38598;&#20013;&#22312;&#20855;&#26377;&#26131;&#20110;&#33719;&#21462;&#21644;&#20805;&#36275;&#36164;&#28304;&#30340;&#35821;&#35328;&#19978;&#65292;&#20363;&#22914;&#33521;&#35821;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#20844;&#20849;&#39046;&#22495;&#32570;&#20047;&#36275;&#22815;&#35821;&#35328;&#36164;&#28304;&#30340;&#35821;&#35328;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;Komodo-7B&#65292;&#19968;&#20010;70&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#12289;&#33521;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;11&#31181;&#22320;&#21306;&#35821;&#35328;&#20043;&#38388;&#26080;&#32541;&#25805;&#20316;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;Komodo-7B&#26159;&#19968;&#32452;LLMs&#65292;&#30001;Komodo-7B-Base&#21644;Komodo-7B-Instruct&#32452;&#25104;&#12290;Komodo-7B-Instruct&#20973;&#20511;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#35821;&#35328;&#19978;&#21462;&#24471;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#33073;&#39062;&#32780;&#20986;&#65292;&#36229;&#36234;&#20102;OpenAI&#30340;GPT-3.5&#12289;Cohere&#30340;Aya-101&#12289;Llama-2-Chat-13B&#12289;Mixtral-8x7B-Instruct-v0.1&#12289;Gemma-7B-it&#31561;&#27169;&#22411;&#21046;&#23450;&#30340;&#22522;&#20934;&#12290;&#36825;&#20010;&#27169;&#22411;&#19981;&#20165;&#22312;&#35821;&#35328;&#29305;&#23450;&#21644;&#25972;&#20307;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36824;&#31361;&#26174;&#20102;&#20854;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09362v1 Announce Type: new  Abstract: The recent breakthroughs in Large Language Models (LLMs) have mostly focused on languages with easily available and sufficient resources, such as English. However, there remains a significant gap for languages that lack sufficient linguistic resources in the public domain. Our work introduces Komodo-7B, 7-billion-parameter Large Language Models designed to address this gap by seamlessly operating across Indonesian, English, and 11 regional languages in Indonesia. Komodo-7B is a family of LLMs that consist of Komodo-7B-Base and Komodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art performance in various tasks and languages, outperforming the benchmarks set by OpenAI's GPT-3.5, Cohere's Aya-101, Llama-2-Chat-13B, Mixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only demonstrates superior performance in both language-specific and overall assessments but also highlights its capability to excel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#27468;&#21809;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#21644;&#36827;&#23637;&#65292;&#25506;&#32034;&#20102;&#38899;&#32032;&#35782;&#21035;&#12289;&#27468;&#26354;&#20013;&#30340;&#35821;&#35328;&#35782;&#21035;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#21644;&#23436;&#25972;&#27468;&#35789;&#36716;&#24405;&#31561;&#20851;&#38190;&#39046;&#22495;&#65292;&#24182;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22312;&#35813;&#39046;&#22495;&#25512;&#21160;&#36827;&#23637;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.09298</link><description>&lt;p&gt;
&#36229;&#36234;&#35328;&#35821;&#65306;&#27468;&#21809;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#36827;&#23637;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
More than words: Advancements and challenges in speech recognition for singing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#27468;&#21809;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#21644;&#36827;&#23637;&#65292;&#25506;&#32034;&#20102;&#38899;&#32032;&#35782;&#21035;&#12289;&#27468;&#26354;&#20013;&#30340;&#35821;&#35328;&#35782;&#21035;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#21644;&#23436;&#25972;&#27468;&#35789;&#36716;&#24405;&#31561;&#20851;&#38190;&#39046;&#22495;&#65292;&#24182;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#22312;&#35813;&#39046;&#22495;&#25512;&#21160;&#36827;&#23637;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#27468;&#21809;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#21644;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#19982;&#26631;&#20934;&#35821;&#38899;&#35782;&#21035;&#23436;&#20840;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#27468;&#21809;&#21253;&#21547;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#24191;&#27867;&#30340;&#38899;&#39640;&#21464;&#21270;&#12289;&#22810;&#26679;&#21270;&#30340;&#22768;&#20048;&#39118;&#26684;&#20197;&#21450;&#32972;&#26223;&#38899;&#20048;&#24178;&#25200;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#35832;&#22914;&#38899;&#32032;&#35782;&#21035;&#12289;&#27468;&#26354;&#20013;&#30340;&#35821;&#35328;&#35782;&#21035;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#21644;&#23436;&#25972;&#27468;&#35789;&#36716;&#24405;&#31561;&#20851;&#38190;&#39046;&#22495;&#12290;&#25105;&#23558;&#25551;&#36848;&#19968;&#20123;&#25105;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#36827;&#34892;&#30740;&#31350;&#26102;&#30340;&#32463;&#21382;&#65292;&#23601;&#22312;&#23427;&#20204;&#24320;&#22987;&#23853;&#38706;&#22836;&#35282;&#30340;&#26102;&#20505;&#65292;&#20294;&#20063;&#20250;&#23637;&#31034;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26368;&#26032;&#36827;&#23637;&#22914;&#20309;&#25512;&#21160;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;&#25105;&#30340;&#30446;&#26631;&#26159;&#38416;&#26126;&#23558;&#35821;&#38899;&#35782;&#21035;&#24212;&#29992;&#20110;&#27468;&#21809;&#26102;&#30340;&#22797;&#26434;&#24615;&#65292;&#35780;&#20272;&#24403;&#21069;&#30340;&#33021;&#21147;&#65292;&#24182;&#27010;&#36848;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09298v1 Announce Type: cross  Abstract: This paper addresses the challenges and advancements in speech recognition for singing, a domain distinctly different from standard speech recognition. Singing encompasses unique challenges, including extensive pitch variations, diverse vocal styles, and background music interference. We explore key areas such as phoneme recognition, language identification in songs, keyword spotting, and full lyrics transcription. I will describe some of my own experiences when performing research on these tasks just as they were starting to gain traction, but will also show how recent developments in deep learning and large-scale datasets have propelled progress in this field. My goal is to illuminate the complexities of applying speech recognition to singing, evaluate current capabilities, and outline future research directions.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21078;&#32467;&#26500;&#24341;&#23548;&#30340;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#25253;&#21578;&#35299;&#26512;&#20026;&#19977;&#20803;&#32452;&#24182;&#21033;&#29992;&#27599;&#20010;&#20803;&#32032;&#20316;&#20026;&#30417;&#30563;&#26469;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23616;&#37096;&#23545;&#40784;&#21644;&#22270;&#20687;-&#25253;&#21578;&#23545;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09294</link><description>&lt;p&gt;
&#35299;&#21078;&#32467;&#26500;&#24341;&#23548;&#30340;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Anatomical Structure-Guided Medical Vision-Language Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09294
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21078;&#32467;&#26500;&#24341;&#23548;&#30340;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#25253;&#21578;&#35299;&#26512;&#20026;&#19977;&#20803;&#32452;&#24182;&#21033;&#29992;&#27599;&#20010;&#20803;&#32032;&#20316;&#20026;&#30417;&#30563;&#26469;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23616;&#37096;&#23545;&#40784;&#21644;&#22270;&#20687;-&#25253;&#21578;&#23545;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#23398;&#20064;&#21307;&#23398;&#35270;&#35273;&#34920;&#31034;&#24050;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#21363;&#23616;&#37096;&#23545;&#40784;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#22270;&#20687;-&#25253;&#21578;&#23545;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#34920;&#31034;&#23398;&#20064;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21078;&#32467;&#26500;&#24341;&#23548;&#30340;&#65288;ASG&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#25253;&#21578;&#35299;&#26512;&#20026;&#19977;&#20803;&#32452;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#20803;&#32032;&#20316;&#20026;&#30417;&#30563;&#26469;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#12290;&#23545;&#20110;&#35299;&#21078;&#21306;&#22495;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#19982;&#25918;&#23556;&#31185;&#21307;&#24072;&#21512;&#20316;&#30340;&#33258;&#21160;&#35299;&#21078;&#21306;&#22495;-&#21477;&#23376;&#23545;&#40784;&#33539;&#20363;&#65292;&#23558;&#20854;&#35270;&#20026;&#26368;&#23567;&#35821;&#20041;&#21333;&#20803;&#26469;&#25506;&#32034;&#32454;&#31890;&#24230;&#23616;&#37096;&#23545;&#40784;&#12290;&#23545;&#20110;&#26597;&#25214;&#21644;&#23384;&#22312;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#22270;&#20687;&#26631;&#31614;&#65292;&#24212;&#29992;&#22270;&#20687;&#26631;&#31614;&#35782;&#21035;&#35299;&#30721;&#22120;&#65292;&#22312;&#27599;&#20010;&#26679;&#26412;&#20869;&#23558;&#22270;&#20687;&#29305;&#24449;&#19982;&#20854;&#30456;&#24212;&#26631;&#31614;&#20851;&#32852;&#36215;&#26469;&#65292;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09294v1 Announce Type: cross  Abstract: Learning medical visual representations through vision-language pre-training has reached remarkable progress. Despite the promising performance, it still faces challenges, i.e., local alignment lacks interpretability and clinical relevance, and the insufficient internal and external representation learning of image-report pairs. To address these issues, we propose an Anatomical Structure-Guided (ASG) framework. Specifically, we parse raw reports into triplets , and fully utilize each element as supervision to enhance representation learning. For anatomical region, we design an automatic anatomical region-sentence alignment paradigm in collaboration with radiologists, considering them as the minimum semantic units to explore fine-grained local alignment. For finding and existence, we regard them as image tags, applying an image-tag recognition decoder to associate image features with their respective tags within each sample and construc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#28151;&#21512;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;HUDS&#65292;&#32467;&#21512;&#20102;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#29992;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21477;&#23376;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2403.09259</link><description>&lt;p&gt;
&#26159;&#21542;&#32473;&#25968;&#25454;&#36148;&#26631;&#31614;&#65306;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#28151;&#21512;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09259
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#28151;&#21512;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;HUDS&#65292;&#32467;&#21512;&#20102;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#29992;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21477;&#23376;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#36890;&#36807;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#36873;&#25321;&#26356;&#23567;&#30340;&#20195;&#34920;&#24615;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#38477;&#20302;&#20102;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#30340;&#26631;&#35760;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HUDS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;NMT&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#28151;&#21512;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#30456;&#32467;&#21512;&#65292;&#20197;&#36827;&#34892;&#21477;&#23376;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09259v1 Announce Type: new  Abstract: Active learning (AL) techniques reduce labeling costs for training neural machine translation (NMT) models by selecting smaller representative subsets from unlabeled data for annotation. Diversity sampling techniques select heterogeneous instances, while uncertainty sampling methods select instances with the highest model uncertainty. Both approaches have limitations - diversity methods may extract varied but trivial examples, while uncertainty sampling can yield repetitive, uninformative instances. To bridge this gap, we propose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines uncertainty and diversity for sentence selection. HUDS computes uncertainty scores for unlabeled sentences and subsequently stratifies them. It then clusters sentence embeddings within each stratum using k-MEANS and computes diversity scores by distance to the centroid. A weighted hybrid score that combines uncertainty and diversity is then us
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21644;&#32034;&#36180;&#25968;&#25454;&#22238;&#31572;&#27969;&#34892;&#30149;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#34892;&#19994;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.09226</link><description>&lt;p&gt;
&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#22120;&#29992;&#20110;&#27969;&#34892;&#30149;&#23398;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09226
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21644;&#32034;&#36180;&#25968;&#25454;&#22238;&#31572;&#27969;&#34892;&#30149;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#34892;&#19994;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#21644;&#32034;&#36180;&#25968;&#25454;&#26159;&#21453;&#26144;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#21644;&#21307;&#30103;&#21033;&#29992;&#24773;&#20917;&#30340;&#20016;&#23500;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#26469;&#28304;&#12290;&#26597;&#35810;&#36825;&#20123;&#25968;&#25454;&#24211;&#20197;&#22238;&#31572;&#27969;&#34892;&#30149;&#23398;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#22312;&#20110;&#21307;&#23398;&#26415;&#35821;&#30340;&#22797;&#26434;&#24615;&#21644;&#23545;&#22797;&#26434;SQL&#26597;&#35810;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#23558;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#29992;EHR&#21644;&#32034;&#36180;&#25968;&#25454;&#22238;&#31572;&#27969;&#34892;&#30149;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23558;&#21307;&#23398;&#32534;&#30721;&#27493;&#39588;&#25972;&#21512;&#21040;&#25991;&#26412;&#21040;SQL&#36807;&#31243;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#31616;&#21333;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#23578;&#19981;&#36275;&#20197;&#26080;&#30417;&#30563;&#20351;&#29992;&#65292;&#20294;RAG&#20026;&#25913;&#36827;&#23427;&#20204;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#65292;&#22914;&#22312;&#19968;&#20010;&#29616;&#23454;&#30340;&#34892;&#19994;&#29615;&#22659;&#20013;&#25152;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09226v1 Announce Type: new  Abstract: Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical terminology and the need for complex SQL queries. Here, we introduce an end-to-end methodology that combines text-to-SQL generation with retrieval augmented generation (RAG) to answer epidemiological questions using EHR and claims data. We show that our approach, which integrates a medical coding step into the text-to-SQL process, significantly improves the performance over simple prompting. Our findings indicate that although current language models are not yet sufficiently accurate for unsupervised use, RAG offers a promising direction for improving their capabilities, as shown in a realistic industry setting.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;WordNet&#30340;LLMs&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;TaxoLLaMA&#27169;&#22411;&#65292;&#37319;&#29992;4&#20301;&#37327;&#21270;&#21644;LoRA&#25216;&#26415;&#36731;&#37327;&#21270;&#65292;&#22312;&#22810;&#20010;&#35789;&#27719;&#35821;&#20041;&#20219;&#21153;&#20013;&#21462;&#24471;11&#20010;SotA&#32467;&#26524;&#65292;&#19988;&#22312;&#35789;&#27719;&#34164;&#28085;&#21644;&#20998;&#31867;&#23398;&#26500;&#24314;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09207</link><description>&lt;p&gt;
TaxoLLaMA:&#22522;&#20110;WordNet&#30340;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#35789;&#27719;&#35821;&#20041;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09207
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;WordNet&#30340;LLMs&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;TaxoLLaMA&#27169;&#22411;&#65292;&#37319;&#29992;4&#20301;&#37327;&#21270;&#21644;LoRA&#25216;&#26415;&#36731;&#37327;&#21270;&#65292;&#22312;&#22810;&#20010;&#35789;&#27719;&#35821;&#20041;&#20219;&#21153;&#20013;&#21462;&#24471;11&#20010;SotA&#32467;&#26524;&#65292;&#19988;&#22312;&#35789;&#27719;&#34164;&#28085;&#21644;&#20998;&#31867;&#23398;&#26500;&#24314;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#25429;&#25417;WordNet&#20013;&#30340;&#35789;&#27719;&#35821;&#20041;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;LLaMA-2-7b&#27169;&#22411;&#20026;&#20363;&#65292;&#24182;&#22312;&#22810;&#20010;&#35789;&#27719;&#35821;&#20041;&#20219;&#21153;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#20316;&#20026;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TaxoLLaMA&#65292;&#21363;&#19968;&#20999;&#30342;&#22312;&#20854;&#20013;&#30340;&#27169;&#22411;&#65292;&#30001;&#20110;4&#20301;&#37327;&#21270;&#21644;LoRA&#32780;&#36731;&#37327;&#21270;&#12290;&#23427;&#22312;&#20998;&#31867;&#23398;&#20016;&#23500;&#21270;&#12289;&#19978;&#20301;&#35789;&#21457;&#29616;&#12289;&#20998;&#31867;&#23398;&#26500;&#24314;&#21644;&#35789;&#27719;&#34164;&#28085;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;11&#20010;SotA&#32467;&#26524;&#65292;16&#20010;&#20219;&#21153;&#20013;&#30340;4&#20010;&#21069;2&#21517;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23427;&#23637;&#31034;&#20102;&#22312;&#35789;&#27719;&#34164;&#28085;&#21644;&#20998;&#31867;&#23398;&#26500;&#24314;&#19978;&#20855;&#26377;&#38750;&#24120;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#26080;&#38656;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20854;&#20855;&#26377;&#30340;&#38544;&#34255;&#22810;&#35821;&#35328;&#21644;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#65292;&#20165;&#38656;&#23569;&#37327;&#35843;&#25972;&#25110;&#23569;&#37327;&#23398;&#20064;&#12290;&#25152;&#26377;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#37117;&#21487;&#22312;https://github.com/VityaVitalich/TaxoLLaMA&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09207v1 Announce Type: new  Abstract: In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due to 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 results out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and model are available online at https://github.com/VityaVitalich/TaxoLLaMA
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#20855;&#26377;&#22810;&#26679;&#25552;&#31034;&#21644;&#22810;&#32500;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36991;&#20813;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.09167</link><description>&lt;p&gt;
Dial-insight: &#20351;&#29992;&#39640;&#36136;&#37327;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#33021;&#21147;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09167
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#20855;&#26377;&#22810;&#26679;&#25552;&#31034;&#21644;&#22810;&#32500;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36991;&#20813;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#36136;&#37327;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#20381;&#36182;&#20110;&#22522;&#30784;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#19987;&#19994;&#39046;&#22495;&#20869;&#12290;&#22312;&#20026;&#29305;&#23450;&#39046;&#22495;&#24212;&#29992;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#26102;&#32463;&#24120;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#28508;&#22312;&#36864;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#26088;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#29983;&#20135;&#25552;&#31034;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#29983;&#25104;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#21644;&#23637;&#29616;&#20016;&#23500;&#34920;&#36798;&#24418;&#24335;&#30340;&#22810;&#26679;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#12289;&#22810;&#32500;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#26631;&#27880;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#12290;&#21033;&#29992;&#26469;&#33258;&#25151;&#22320;&#20135;&#34892;&#19994;&#30340;&#26381;&#21153;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20114;&#21160;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25968;&#25454;&#36136;&#37327;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#27491;&#21521;&#30456;&#20851;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09167v1 Announce Type: new  Abstract: The efficacy of large language models (LLMs) is heavily dependent on the quality of the underlying data, particularly within specialized domains. A common challenge when fine-tuning LLMs for domain-specific applications is the potential degradation of the model's generalization capabilities. To address these issues, we propose a two-stage approach for the construction of production prompts designed to yield high-quality data. This method involves the generation of a diverse array of prompts that encompass a broad spectrum of tasks and exhibit a rich variety of expressions. Furthermore, we introduce a cost-effective, multi-dimensional quality assessment framework to ensure the integrity of the generated labeling data. Utilizing a dataset comprised of service provider and customer interactions from the real estate sector, we demonstrate a positive correlation between data quality and model performance. Notably, our findings indicate that t
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;ChatGPT&#22312;&#20013;&#21307;&#30693;&#35782;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;TCM-QA&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;LLM&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#22312;&#21028;&#26029;&#39064;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#20013;&#25991;&#25552;&#31034;&#20248;&#20110;&#33521;&#25991;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.09164</link><description>&lt;p&gt;
&#25506;&#31350;ChatGPT&#22312;&#20013;&#21307;&#30693;&#35782;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09164
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;ChatGPT&#22312;&#20013;&#21307;&#30693;&#35782;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;TCM-QA&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;LLM&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#22312;&#21028;&#26029;&#39064;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#20013;&#25991;&#25552;&#31034;&#20248;&#20110;&#33521;&#25991;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23578;&#26080;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20256;&#32479;&#20013;&#21307;&#23398;&#65288;TCM&#65289;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#38376;&#20855;&#26377;&#20016;&#23500;&#21382;&#21490;&#30340;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#21307;&#23398;&#30693;&#35782;&#20998;&#25903;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TCM-QA&#30340;&#20013;&#21307;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19977;&#31181;&#38382;&#39064;&#31867;&#22411;&#65306;&#21333;&#39033;&#36873;&#25321;&#12289;&#22810;&#39033;&#36873;&#25321;&#21644;&#21028;&#26029;&#39064;&#65292;&#20197;&#26816;&#39564;LLM&#22312;TCM&#39046;&#22495;&#20869;&#30693;&#35782;&#22238;&#24518;&#21644;&#20840;&#38754;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLM&#30340;&#20004;&#31181;&#35774;&#32622;&#65292;&#21363;&#38646;&#27425;&#21644;&#23569;&#27425;&#35774;&#32622;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#33521;&#25991;&#21644;&#20013;&#25991;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#21028;&#26029;&#39064;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#31934;&#30830;&#24230;&#26368;&#39640;&#36798;&#21040;0.688&#65292;&#32780;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#20013;&#24471;&#20998;&#26368;&#20302;&#30340;&#31934;&#30830;&#24230;&#20026;0.241&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#20013;&#25991;&#25552;&#31034;&#34920;&#29616;&#20248;&#20110;&#33521;&#25991;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09164v1 Announce Type: new  Abstract: No previous work has studied the performance of Large Language Models (LLMs) in the context of Traditional Chinese Medicine (TCM), an essential and distinct branch of medical knowledge with a rich history. To bridge this gap, we present a TCM question dataset named TCM-QA, which comprises three question types: single choice, multiple choice, and true or false, to examine the LLM's capacity for knowledge recall and comprehensive reasoning within the TCM domain. In our study, we evaluate two settings of the LLM, zero-shot and few-shot settings, while concurrently discussing the differences between English and Chinese prompts. Our results indicate that ChatGPT performs best in true or false questions, achieving the highest precision of 0.688 while scoring the lowest precision is 0.241 in multiple-choice questions. Furthermore, we observed that Chinese prompts outperformed English prompts in our evaluations. Additionally, we assess the quali
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#27861;&#24459;&#23454;&#36341;&#20013;&#30340;&#20316;&#29992;&#34987;&#36807;&#20998;&#20048;&#35266;&#22320;&#39044;&#27979;&#65292;&#19981;&#29702;&#35299;&#25991;&#26412;&#20869;&#23481;&#20250;&#23548;&#33268;&#20381;&#36182;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.09163</link><description>&lt;p&gt;
Caveat Lector&#65306;&#22312;&#27861;&#24459;&#23454;&#36341;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Caveat Lector: Large Language Models in Legal Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09163
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#27861;&#24459;&#23454;&#36341;&#20013;&#30340;&#20316;&#29992;&#34987;&#36807;&#20998;&#20048;&#35266;&#22320;&#39044;&#27979;&#65292;&#19981;&#29702;&#35299;&#25991;&#26412;&#20869;&#23481;&#20250;&#23548;&#33268;&#20381;&#36182;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30528;&#36855;&#28304;&#20110;&#35768;&#22810;&#29992;&#25143;&#32570;&#20047;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;LLMs&#21487;&#33021;&#30475;&#36215;&#26469;&#27604;&#23427;&#20204;&#23454;&#38469;&#19978;&#26356;&#26377;&#33021;&#21147;&#12290;&#27969;&#30021;&#24615;&#21644;&#34920;&#38754;&#21512;&#29702;&#24615;&#30340;&#21361;&#38505;&#32452;&#21512;&#23548;&#33268;&#20154;&#20204;&#20542;&#21521;&#20110;&#30456;&#20449;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24182;&#20135;&#29983;&#36807;&#24230;&#20381;&#36182;&#30340;&#39118;&#38505;&#12290;&#35841;&#19981;&#20250;&#30456;&#20449;&#23436;&#32654;&#30340;&#27861;&#24459;&#29992;&#35821;&#21602;&#65311;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#22312;&#25216;&#26415;&#21644;&#27861;&#24459;&#23398;&#26415;&#30028;&#30340;&#21457;&#29616;&#65292;&#23545;LLMs&#22312;&#27861;&#24459;&#23454;&#36341;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#36807;&#20998;&#20048;&#35266;&#30340;&#39044;&#27979;&#36827;&#34892;&#20102;&#24179;&#34913;&#12290;&#22312;&#27809;&#26377;&#26356;&#22909;&#29702;&#35299;&#20854;&#23616;&#38480;&#24615;&#30340;&#24773;&#20917;&#19979;&#23558;LLMs&#25972;&#21512;&#21040;&#27861;&#24459;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#23558;&#20250;&#20135;&#29983;&#20302;&#25928;&#29978;&#33267;&#30452;&#25509;&#30340;&#39118;&#38505;&#12290;&#23613;&#31649;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#65292;&#20294;LLMs&#24182;&#19981;&#29702;&#35299;&#25991;&#26412;&#12290;&#27809;&#26377;&#29702;&#35299;&#24847;&#20041;&#30340;&#33021;&#21147;&#65292;LLMs&#23558;&#26080;&#27861;&#20351;&#29992;&#35821;&#35328;&#65292;&#33719;&#21462;&#30693;&#35782;&#24182;&#25191;&#34892;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09163v1 Announce Type: new  Abstract: The current fascination with large language models, or LLMs, derives from the fact that many users lack the expertise to evaluate the quality of the generated text. LLMs may therefore appear more capable than they actually are. The dangerous combination of fluency and superficial plausibility leads to the temptation to trust the generated text and creates the risk of overreliance. Who would not trust perfect legalese? Relying recent findings in both technical and legal scholarship, this Article counterbalances the overly optimistic predictions as to the role of LLMs in legal practice. Integrating LLMs into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks. Notwithstanding their unprecedented ability to generate text, LLMs do not understand text. Without the ability to understand meaning, LLMs will remain unable to use language, to acquire knowledge and to perform compl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#27867;&#21270;&#21040;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#24341;&#20837;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09162</link><description>&lt;p&gt;
&#25581;&#31034;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Generalization Power of Fine-Tuned Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#27867;&#21270;&#21040;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#24341;&#20837;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#22810;&#20219;&#21153;&#33021;&#21147;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22312;&#19979;&#28216;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22312;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#27604;&#26410;&#32463;&#24494;&#35843;&#30340;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#24494;&#35843;&#23545;LLMs&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#38754;&#24433;&#21709;&#23578;&#19981;&#23436;&#20840;&#20102;&#35299;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#21407;&#22987;&#12289;&#26410;&#20462;&#25913;&#30340;LLMs&#19982;&#32463;&#36807;&#24494;&#35843;&#21464;&#20307;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#24494;&#35843;&#26159;&#21542;&#20250;&#24433;&#21709;&#20869;&#22312;&#20110;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#20116;&#20010;&#19981;&#21516;&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#27867;&#21270;&#21040;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#30340;&#24341;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#26576;&#20123;&#20855;&#20307;&#20219;&#21153;&#30340;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09162v1 Announce Type: new  Abstract: While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of fine-tuning on the LLMs' generalization ability are not fully understood. This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets. Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks. Intriguingly, we observe that integrating the in-context learning strategy durin
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CONAN-EUS&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#21644;&#19987;&#19994;&#21518;&#26399;&#32534;&#36753;&#24320;&#21457;&#30340;&#24052;&#26031;&#20811;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#21453;&#21465;&#20107;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#21518;&#26399;&#32534;&#36753;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#20165;&#20381;&#36182;&#20110;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21453;&#21465;&#20107;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.09159</link><description>&lt;p&gt;
&#24052;&#26031;&#20811;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#21453;&#21465;&#20107;&#29983;&#25104;&#65306;&#25968;&#25454;&#21019;&#24314;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Basque and Spanish Counter Narrative Generation: Data Creation and Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09159
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CONAN-EUS&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#21644;&#19987;&#19994;&#21518;&#26399;&#32534;&#36753;&#24320;&#21457;&#30340;&#24052;&#26031;&#20811;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#21453;&#21465;&#20107;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#21518;&#26399;&#32534;&#36753;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#20165;&#20381;&#36182;&#20110;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21453;&#21465;&#20107;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21465;&#20107;&#65288;CNs&#65289;&#26159;&#23545;&#20167;&#24680;&#35328;&#35770;&#65288;HS&#65289;&#30340;&#38750;&#36127;&#25991;&#26412;&#22238;&#24212;&#65292;&#26088;&#22312;&#21270;&#35299;&#22312;&#32447;&#20167;&#24680;&#24182;&#20943;&#36731;&#20854;&#22312;&#23186;&#20307;&#20043;&#38388;&#30340;&#20256;&#25773;&#12290;&#23613;&#31649;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#20869;&#23481;&#26368;&#36817;&#26377;&#25152;&#22686;&#21152;&#65292;&#20294;&#26377;&#20851;&#33258;&#21160;&#29983;&#25104;&#21453;&#21465;&#20107;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#19988;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CONAN-EUS&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#19987;&#19994;&#21518;&#26399;&#32534;&#36753;&#24320;&#21457;&#30340;&#29992;&#20110;&#29983;&#25104;&#21453;&#21465;&#20107;&#30340;&#24052;&#26031;&#20811;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#25968;&#25454;&#38598;&#12290;&#20316;&#20026;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#19982;&#21407;&#22987;&#33521;&#25991;CONAN&#30456;&#27604;&#65292;&#23427;&#20801;&#35768;&#36827;&#34892;&#20851;&#20110;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#33258;&#21160;&#29983;&#25104;&#21453;&#21465;&#20107;&#30340;&#26032;&#22411;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;mT5&#36827;&#34892;&#20102;CN&#29983;&#25104;&#23454;&#39564;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#20165;&#20381;&#36182;&#20110;&#38134;&#26631;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#21518;&#26399;&#32534;&#36753;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#29983;&#25104;&#25928;&#26524;&#22823;&#24133;&#25552;&#21319;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#23450;&#24615;&#25163;&#21160;&#35780;&#20272;&#30456;&#20114;&#21360;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09159v1 Announce Type: new  Abstract: Counter Narratives (CNs) are non-negative textual responses to Hate Speech (HS) aiming at defusing online hatred and mitigating its spreading across media. Despite the recent increase in HS content posted online, research on automatic CN generation has been relatively scarce and predominantly focused on English. In this paper, we present CONAN-EUS, a new Basque and Spanish dataset for CN generation developed by means of Machine Translation (MT) and professional post-edition. Being a parallel corpus, also with respect to the original English CONAN, it allows to perform novel research on multilingual and crosslingual automatic generation of CNs. Our experiments on CN generation with mT5, a multilingual encoder-decoder model, show that generation greatly benefits from training on post-edited data, as opposed to relying on silver MT data only. These results are confirmed by their correlation with a qualitative manual evaluation, demonstratin
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26174;&#33879;&#20154;&#29289;&#20013;&#23384;&#22312;&#30340;&#24615;&#21035;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#24615;&#33021;&#19978;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#38382;&#39064;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;</title><link>https://arxiv.org/abs/2403.09148</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#26174;&#33879;&#20154;&#29289;&#30340;LLMs&#20013;&#30340;&#24615;&#21035;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Evaluating LLMs for Gender Disparities in Notable Persons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09148
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26174;&#33879;&#20154;&#29289;&#20013;&#23384;&#22312;&#30340;&#24615;&#21035;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#24615;&#33021;&#19978;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#38382;&#39064;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29992;&#20110;&#26816;&#32034;&#20107;&#23454;&#20449;&#24687;&#30340;&#20351;&#29992;&#65292;&#35299;&#20915;&#20102;&#23427;&#20204;&#20135;&#29983;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#8220;&#24187;&#35273;&#8221;&#22238;&#22797;&#25110;&#23436;&#20840;&#25298;&#32477;&#29978;&#33267;&#22238;&#31572;&#25552;&#31034;&#30340;&#20542;&#21521;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#35843;&#26597;&#20102;LLMs&#23545;&#20107;&#23454;&#26597;&#35810;&#30340;&#22238;&#24212;&#20013;&#23384;&#22312;&#30340;&#22522;&#20110;&#24615;&#21035;&#30340;&#20559;&#35265;&#12290;&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#35780;&#20272;GPT&#27169;&#22411;&#22312;&#21484;&#22238;&#12289;&#24187;&#35273;&#21644;&#25298;&#32477;&#31561;&#22810;&#20010;&#32500;&#24230;&#19978;&#30340;&#20844;&#24179;&#24615;&#26469;&#37319;&#29992;&#22810;&#31649;&#40784;&#19979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;GPT-3.5&#29983;&#25104;&#30340;&#22238;&#24212;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#21035;&#24046;&#36317;&#12290;&#34429;&#28982;GPT-4&#30340;&#36827;&#23637;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#20294;&#22312;&#22238;&#24212;&#34987;&#25298;&#32477;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#24615;&#21035;&#24046;&#36317;&#24182;&#26410;&#23436;&#20840;&#28040;&#38500;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#36825;&#20123;&#24046;&#36317;&#30340;&#36215;&#28304;&#65292;&#36890;&#36807;&#26816;&#26597;&#25552;&#31034;&#20013;&#30340;&#24615;&#21035;&#20851;&#32852;&#21644;&#22238;&#24212;&#20013;&#30340;&#21516;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09148v1 Announce Type: new  Abstract: This study examines the use of Large Language Models (LLMs) for retrieving factual information, addressing concerns over their propensity to produce factually incorrect "hallucinated" responses or to altogether decline to even answer prompt at all. Specifically, it investigates the presence of gender-based biases in LLMs' responses to factual inquiries. This paper takes a multi-pronged approach to evaluating GPT models by evaluating fairness across multiple dimensions of recall, hallucinations and declinations. Our findings reveal discernible gender disparities in the responses generated by GPT-3.5. While advancements in GPT-4 have led to improvements in performance, they have not fully eradicated these gender disparities, notably in instances where responses are declined. The study further explores the origins of these disparities by examining the influence of gender associations in prompts and the homogeneity in the responses.
&lt;/p&gt;</description></item><item><title>ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09131</link><description>&lt;p&gt;
ProSwitch&#65306;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09131
&lt;/p&gt;
&lt;p&gt;
ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25991;&#26412;&#25688;&#35201;&#21644;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#36890;&#36807;&#24494;&#35843;&#22312;&#19981;&#21516;&#39118;&#26684;&#38388;&#20999;&#25442;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#25991;&#26412;&#19987;&#19994;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ProSwitch&#65292;&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;ProSwitch&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#25968;&#25454;&#20934;&#22791;&#65292;&#29992;&#20110;&#25910;&#38598;&#39046;&#22495;&#30693;&#35782;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#65307;&#25351;&#20196;&#24494;&#35843;&#65292;&#29992;&#20110;&#20248;&#21270;&#24102;&#26377;&#22810;&#31181;&#25351;&#20196;&#26684;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#20840;&#38754;&#35780;&#20272;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#19987;&#19994;&#24615;&#21306;&#20998;&#33021;&#21147;&#21644;&#22522;&#20110;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290; ProSwitch&#30456;&#23545;&#20110;&#36890;&#29992;&#21644;&#19987;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09131v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our appro
&lt;/p&gt;</description></item><item><title>AutoLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#65292;&#20197;&#35299;&#20915;LoRA&#20013;&#31209;&#20998;&#37197;&#21644;&#31209;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09113</link><description>&lt;p&gt;
AutoLoRA&#65306;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#33258;&#21160;&#35843;&#25972;&#30697;&#38453;&#31209;&#22312;&#20302;&#31209;&#36866;&#24212;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09113
&lt;/p&gt;
&lt;p&gt;
AutoLoRA&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#65292;&#20197;&#35299;&#20915;LoRA&#20013;&#31209;&#20998;&#37197;&#21644;&#31209;&#25628;&#32034;&#30340;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20043;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#23384;&#22312;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25361;&#25112;&#65292;&#22240;&#27492;&#30740;&#21457;&#20102;&#20960;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#20854;&#20013;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#36890;&#36807;&#22312;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#20043;&#19978;&#24494;&#35843;&#20302;&#31209;&#22686;&#37327;&#26356;&#26032;&#30697;&#38453;&#65292;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;LoRA&#22312;&#25152;&#26377;&#23618;&#20013;&#22343;&#21248;&#20998;&#37197;&#31209;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#31351;&#20030;&#25628;&#32034;&#26469;&#25214;&#21040;&#26368;&#20339;&#31209;&#65292;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24494;&#35843;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoLoRA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#27599;&#20010;LoRA&#23618;&#30340;&#26368;&#20339;&#31209;&#12290;AutoLoRA&#23558;&#20302;&#31209;&#26356;&#26032;&#30697;&#38453;&#20013;&#30340;&#27599;&#20010;&#31209;&#20026;1&#30340;&#30697;&#38453;&#19982;&#36873;&#25321;&#21464;&#37327;&#30456;&#20851;&#32852;&#65292;&#35813;&#21464;&#37327;&#20915;&#23450;&#20102;&#31209;&#20026;1&#30340;&#30697;&#38453;&#26159;&#21542;&#24212;&#35813;&#34987;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09113v1 Announce Type: cross  Abstract: Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;GPT&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;&#30740;&#31350;&#23454;&#29616;&#20102;&#19968;&#20010;&#33258;&#21160;&#19987;&#23478;&#26631;&#27880;&#31649;&#36947;&#65292;&#35813;&#31649;&#36947;&#20197;94%&#30340;&#20934;&#30830;&#29575;&#20026;AI&#20986;&#29256;&#29289;&#20998;&#37197;&#26631;&#31614;&#65292;&#24182;&#23545;&#27604;&#23637;&#31034;&#20102;SPECTER&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;96%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09097</link><description>&lt;p&gt;
AI&#23545;AI&#30340;&#24212;&#29992;&#65306;&#25506;&#32034;&#23558;GPT&#20316;&#20026;AI&#20986;&#29256;&#29289;&#19987;&#23478;&#27880;&#37322;&#22120;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI Publications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09097
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;GPT&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;&#30740;&#31350;&#23454;&#29616;&#20102;&#19968;&#20010;&#33258;&#21160;&#19987;&#23478;&#26631;&#27880;&#31649;&#36947;&#65292;&#35813;&#31649;&#36947;&#20197;94%&#30340;&#20934;&#30830;&#29575;&#20026;AI&#20986;&#29256;&#29289;&#20998;&#37197;&#26631;&#31614;&#65292;&#24182;&#23545;&#27604;&#23637;&#31034;&#20102;SPECTER&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;96%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09097v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#26032;&#35770;&#25991; &#25688;&#35201;&#65306;&#35782;&#21035;&#22788;&#20110;&#21160;&#24577;&#30740;&#31350;&#39046;&#22495;&#20869;&#30340;&#31185;&#23398;&#20986;&#29256;&#29289;&#36890;&#24120;&#38656;&#35201;&#39046;&#22495;&#19987;&#23478;&#36827;&#34892;&#26114;&#36149;&#30340;&#26631;&#27880;&#12290;&#20687;&#24191;&#27867;&#25509;&#21463;&#30340;&#20998;&#31867;&#26631;&#20934;&#25110;&#39046;&#22495;&#20998;&#31867;&#27861;&#36825;&#26679;&#30340;&#36164;&#28304;&#23545;&#20110;&#36328;&#36234;&#26032;&#20852;&#20027;&#39064;&#21644;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36825;&#26679;&#19968;&#20010;&#39046;&#22495;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#29616;&#26377;&#30340;&#19987;&#23478;&#26631;&#31614;&#20013;&#25512;&#26029;&#20986;AI&#30740;&#31350;&#30340;&#21151;&#33021;&#23450;&#20041;&#65292;&#28982;&#21518;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#22312;&#19987;&#23478;&#25968;&#25454;&#26631;&#27880;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20351;&#29992;arXiv&#20986;&#29256;&#29289;&#25968;&#25454;&#24211;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#23581;&#35797;&#23545;GPT&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;&#20197;&#35782;&#21035;&#19968;&#31181;&#26367;&#20195;&#30340;&#33258;&#21160;&#21270;&#19987;&#23478;&#26631;&#27880;&#27969;&#27700;&#32447;&#65292;&#35813;&#27969;&#27700;&#32447;&#20197;94%&#30340;&#31934;&#24230;&#20998;&#37197;AI&#26631;&#31614;&#12290;&#20026;&#20102;&#23545;&#27604;&#65292;&#25105;&#20204;&#23545;SPECTER&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#31185;&#23398;&#20986;&#29256;&#29289;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#20998;&#31867;AI&#20986;&#29256;&#29289;&#26041;&#38754;&#23454;&#29616;&#20102;96%&#30340;&#20934;&#30830;&#29575;&#65288;&#20165;&#27604;GPT&#39640;2%&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09097v1 Announce Type: new  Abstract: Identifying scientific publications that are within a dynamic field of research often requires costly annotation by subject-matter experts. Resources like widely-accepted classification criteria or field taxonomies are unavailable for a domain like artificial intelligence (AI), which spans emerging topics and technologies. We address these challenges by inferring a functional definition of AI research from existing expert labels, and then evaluating state-of-the-art chatbot models on the task of expert data annotation. Using the arXiv publication database as ground-truth, we experiment with prompt engineering for GPT chatbot models to identify an alternative, automated expert annotation pipeline that assigns AI labels with 94% accuracy. For comparison, we fine-tune SPECTER, a transformer language model pre-trained on scientific publications, that achieves 96% accuracy (only 2% higher than GPT) on classifying AI publications. Our results 
&lt;/p&gt;</description></item><item><title>MCFEND&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#22810;&#28304;&#26032;&#38395;&#25968;&#25454;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.09092</link><description>&lt;p&gt;
MCFEND&#65306;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09092
&lt;/p&gt;
&lt;p&gt;
MCFEND&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#22810;&#28304;&#26032;&#38395;&#25968;&#25454;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#26032;&#38395;&#22312;&#21508;&#20010;&#22312;&#32447;&#26469;&#28304;&#30340;&#26222;&#36941;&#20256;&#25773;&#23545;&#20844;&#20247;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#25968;&#25454;&#38598;&#20165;&#38480;&#20110;&#26469;&#33258;&#24494;&#21338;&#30340;&#26032;&#38395;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#34394;&#20551;&#26032;&#38395;&#22312;&#20869;&#23481;&#21644;&#31038;&#20250;&#32972;&#26223;&#31561;&#21508;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#22810;&#26679;&#24615;&#12290;&#20165;&#22312;&#21333;&#19968;&#26032;&#38395;&#26469;&#28304;&#19978;&#35757;&#32451;&#30340;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#36866;&#29992;&#20110;&#29616;&#23454;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#23398;&#20064;&#33258;&#19968;&#20010;&#22823;&#22411;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#25968;&#25454;&#38598;Weibo-21&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;F1&#20998;&#25968;&#65292;&#24403;&#27979;&#35797;&#25968;&#25454;&#25913;&#21464;&#20026;&#22810;&#28304;&#26032;&#38395;&#25968;&#25454;&#26102;&#65292;&#20174;0.943&#24613;&#21095;&#19979;&#38477;&#21040;0.470&#65292;&#26410;&#33021;&#35782;&#21035;&#36229;&#36807;&#19977;&#20998;&#20043;&#19968;&#30340;&#22810;&#28304;&#34394;&#20551;&#26032;&#38395;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;MCFEND&#65292;&#30001;&#25105;&#20204;&#20174;&#21508;&#31181;&#26469;&#28304;&#25910;&#38598;&#30340;&#26032;&#38395;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09092v1 Announce Type: cross  Abstract: The prevalence of fake news across various online sources has had a significant influence on the public. Existing Chinese fake news detection datasets are limited to news sourced solely from Weibo. However, fake news originating from multiple sources exhibits diversity in various aspects, including its content and social context. Methods trained on purely one single news source can hardly be applicable to real-world scenarios. Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese fake news detection dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source fake news. To address this limitation, we constructed the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND, which is composed of news we collected from diverse sources suc
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#20010;&#25277;&#35937;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#26377;&#24847;&#20041;&#23398;&#20064;&#33539;&#24335;&#65292;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#36890;&#29992;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09085</link><description>&lt;p&gt;
&#26377;&#24847;&#20041;&#23398;&#20064;&#65306;&#36890;&#36807;&#36890;&#29992;&#20107;&#23454;&#24341;&#23548;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25277;&#35937;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09085
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#20010;&#25277;&#35937;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#26377;&#24847;&#20041;&#23398;&#20064;&#33539;&#24335;&#65292;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#36890;&#29992;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25512;&#29702;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#21644;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#26631;&#24535;&#30528;&#26397;&#30528;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#30001;&#36890;&#29992;&#20107;&#23454;&#25903;&#25345;&#30340;&#31616;&#21333;&#38382;&#39064;&#26102;&#65292;LLMs&#32463;&#24120;&#26410;&#33021;&#25552;&#20379;&#19968;&#33268;&#21644;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#34920;&#26126;&#20854;&#23384;&#22312;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;LLMs&#21040;&#24213;&#26159;&#22312;&#30495;&#27491;&#25512;&#29702;&#36824;&#26159;&#20165;&#20165;&#22312;&#35760;&#24518;&#30340;&#28608;&#28872;&#20105;&#35770;&#12290;&#37492;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21021;&#27493;&#30740;&#31350;&#26469;&#37327;&#21270;&#24182;&#28145;&#20837;&#25506;&#35752;&#29616;&#26377;LLMs&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#26174;&#31034;&#20986;&#23427;&#20204;&#30340;&#19968;&#33324;&#25512;&#29702;&#21644;&#25277;&#35937;&#25512;&#29702;&#34920;&#29616;&#20043;&#38388;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#24322;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#20102;&#19968;&#20010;&#25277;&#35937;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;AbsR&#65289;&#65292;&#32467;&#21512;&#26377;&#24847;&#20041;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#25945;&#20250;LLMs&#22914;&#20309;&#21033;&#29992;&#36890;&#29992;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;LLMs&#22312;&#25277;&#35937;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09085v1 Announce Type: cross  Abstract: Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our app
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#26469;&#25552;&#21462;&#21457;&#23637;&#20013;&#22269;&#23478;&#37329;&#34701;&#25968;&#25454;&#65292;&#20854;&#20013;&#37319;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;T5&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#12289;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09077</link><description>&lt;p&gt;
&#20449;&#24687;&#25552;&#21462;&#65306;&#24212;&#29992;&#20110;&#21457;&#23637;&#20013;&#22269;&#23478;&#36229;&#26412;&#22320;&#37329;&#34701;&#25968;&#25454;&#39046;&#22495;&#30340;&#19968;&#39033;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Information Extraction: An application to the domain of hyper-local financial data on developing countries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#26469;&#25552;&#21462;&#21457;&#23637;&#20013;&#22269;&#23478;&#37329;&#34701;&#25968;&#25454;&#65292;&#20854;&#20013;&#37319;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;T5&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#12289;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21457;&#23637;&#30740;&#31350;&#21644;&#32463;&#27982;&#20998;&#26512;&#38656;&#35201;&#26377;&#20851;&#21457;&#23637;&#20013;&#22269;&#23478;&#20844;&#21496;&#27963;&#21160;&#30340;&#37329;&#34701;&#25968;&#25454;&#65292;&#20294;&#36825;&#26679;&#30340;&#25968;&#25454;&#24182;&#19981;&#23384;&#22312;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#29305;&#23450;&#20110;&#21457;&#23637;&#20013;&#22269;&#23478;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#39046;&#22495;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#32034;&#20102;&#22810;&#31181;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;T5&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36827;&#34892;&#21516;&#26102;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#23450;&#21046;&#25991;&#26412;&#32467;&#26500;&#36755;&#20986;&#25968;&#25454;&#65292;&#21363;&#23454;&#20307;&#21450;&#20854;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#21512;&#24182;&#20219;&#21153;&#19978;&#25105;&#20204;&#26368;&#20339;&#30340;T5&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;92.44&#65285;&#65292;&#31934;&#24230;&#20026;68.25&#65285;&#65292;&#21484;&#22238;&#29575;&#20026;54.20&#65285;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#39034;&#24207;NER&#21644;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#12290;&#23545;&#20110;NER&#65292;&#25105;&#20204;&#36816;&#34892;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09077v1 Announce Type: new  Abstract: Despite the need for financial data on company activities in developing countries for development research and economic analysis, such data does not exist. In this project, we develop and evaluate two Natural Language Processing (NLP) based techniques to address this issue. First, we curate a custom dataset specific to the domain of financial text data on developing countries and explore multiple approaches for information extraction. We then explore a text-to-text approach with the transformer-based T5 model with the goal of undertaking simultaneous NER and relation extraction. We find that this model is able to learn the custom text structure output data corresponding to the entities and their relations, resulting in an accuracy of 92.44\%, a precision of 68.25\% and a recall of 54.20\% from our best T5 model on the combined task. Secondly, we explore an approach with sequential NER and relation extration. For the NER, we run pre-train
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#36755;&#20837;&#32763;&#35793;&#20026;&#22810;&#31181;&#35821;&#35328;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22810;&#35821;&#35328;&#24179;&#34892;&#36755;&#20837;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#35328;&#36755;&#20837;&#21487;&#20197;&#36229;&#36234;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#21453;&#30452;&#35273;&#29616;&#35937;</title><link>https://arxiv.org/abs/2403.09073</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24182;&#34892;&#22810;&#35821;&#35328;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Parallel Multilingual Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09073
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36755;&#20837;&#32763;&#35793;&#20026;&#22810;&#31181;&#35821;&#35328;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22810;&#35821;&#35328;&#24179;&#34892;&#36755;&#20837;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#22810;&#35821;&#35328;&#36755;&#20837;&#21487;&#20197;&#36229;&#36234;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#21453;&#30452;&#35273;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65306;&#36890;&#36807;&#23558;&#36755;&#20837;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#25105;&#20204;&#20026;LLMs&#25552;&#20379;&#20102;&#22810;&#35821;&#35328;&#24179;&#34892;&#36755;&#20837;&#65288;PiM&#65289;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#27979;&#35797;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21253;&#25324;8&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#12289;7&#31181;&#35821;&#35328;&#21644;8&#31181;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;LLMs&#22312;&#20869;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#32467;&#26524;&#26174;&#31034;&#65292;&#65288;1&#65289;&#25972;&#21512;&#26356;&#22810;&#35821;&#35328;&#21487;&#20197;&#24110;&#21161;PiM&#36827;&#19968;&#27493;&#36229;&#36234;&#20256;&#32479;&#30340;ICL&#65307;&#65288;2&#65289;&#21363;&#20351;&#19982;&#22522;&#20934;&#24615;&#33021;&#20302;&#21155;&#30340;&#32763;&#35793;&#32467;&#21512;&#20063;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26816;&#26597;LLMs&#20013;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24847;&#22806;&#20294;&#26377;&#36259;&#30340;&#29616;&#35937;&#12290;&#19982;&#24120;&#35265;&#35266;&#28857;&#30456;&#21453;&#65292;PiM&#24182;&#19981;&#20250;&#28608;&#27963;&#27604;&#21333;&#35821;&#36755;&#20837;&#26356;&#22810;&#30340;&#31070;&#32463;&#20803;&#26469;&#21033;&#29992;&#20174;&#22810;&#31181;&#35821;&#35328;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#65292;&#32780;&#23454;&#38469;&#19978;&#26159;&#25233;&#21046;&#31070;&#32463;&#20803;&#24182;&#20419;&#36827;&#26356;&#31934;&#30830;&#30340;&#31070;&#32463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09073v1 Announce Type: new  Abstract: In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs. Experimental results show that (1) incorporating more languages help PiM surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neu
&lt;/p&gt;</description></item><item><title>UniCode&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#32479;&#19968;&#30721;&#20070;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#35270;&#35273;&#21644;&#25991;&#26412;&#36827;&#34892;&#26631;&#35760;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#21387;&#32553;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09072</link><description>&lt;p&gt;
UniCode: &#23398;&#20064;&#29992;&#20110;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#30721;&#20070;
&lt;/p&gt;
&lt;p&gt;
UniCode: Learning a Unified Codebook for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09072
&lt;/p&gt;
&lt;p&gt;
UniCode&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#32479;&#19968;&#30721;&#20070;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#35270;&#35273;&#21644;&#25991;&#26412;&#36827;&#34892;&#26631;&#35760;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UniCode&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23646;&#20110;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#39046;&#22495;&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#30721;&#20070;&#26469;&#39640;&#25928;&#22320;&#26631;&#35760;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#28508;&#22312;&#20854;&#20182;&#31867;&#22411;&#30340;&#20449;&#21495;&#12290;&#36825;&#19968;&#21019;&#26032;&#35299;&#20915;&#20102;&#29616;&#26377;MLLMs&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#23616;&#38480;&#65306;&#23427;&#20204;&#20381;&#36182;&#20110;&#20165;&#38480;&#20110;&#25991;&#26412;&#30340;&#30721;&#20070;&#65292;&#36825;&#38480;&#21046;&#20102;MLLM&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#29983;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#35821;&#35328;&#39537;&#21160;&#30340;&#36845;&#20195;&#35757;&#32451;&#33539;&#24335;&#65292;&#32467;&#21512;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#22270;&#20687;&#35299;&#21387;&#32553;&#8221;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#35299;&#37322;&#21387;&#32553;&#30340;&#35270;&#35273;&#25968;&#25454;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#32479;&#19968;&#30340;&#30721;&#20070;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#23558;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25193;&#23637;&#21040;&#38750;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;UniCode&#36866;&#24212;&#20102;&#21508;&#31181;&#21472;&#21152;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#23558;&#35270;&#35273;&#20449;&#21495;&#21387;&#32553;&#20026;&#26356;&#32039;&#20945;&#30340;&#26631;&#35760;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09072v1 Announce Type: cross  Abstract: In this paper, we propose \textbf{UniCode}, a novel approach within the domain of multimodal large language models (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals. This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLM's ability to generate images and texts in a multimodal context. Towards this end, we propose a language-driven iterative training paradigm, coupled with an in-context pre-training task we term ``image decompression'', enabling our model to interpret compressed visual data and generate high-quality images.The unified codebook empowers our model to extend visual instruction tuning to non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse stacked quantization approaches in order to compress visual signals into a more compact token representation. Despite using signi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22478;&#24066;&#29305;&#23450;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20026;&#20154;&#20204;&#25552;&#20379;&#20934;&#30830;&#30340;&#25512;&#33616;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.09059</link><description>&lt;p&gt;
LAMP&#65306;&#22320;&#22270;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LAMP: A Language Model on the Map
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22478;&#24066;&#29305;&#23450;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#20026;&#20154;&#20204;&#25552;&#20379;&#20934;&#30830;&#30340;&#25512;&#33616;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#20026;&#25105;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;&#22320;&#29702;&#31354;&#38388;&#39046;&#22495;&#65292;LLMs&#24050;&#32463;&#23637;&#31034;&#20986;&#33021;&#22815;&#22238;&#31572;&#19968;&#33324;&#24615;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#27604;&#22914;&#35782;&#21035;&#19968;&#20010;&#22269;&#23478;&#30340;&#39318;&#37117;&#65307;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#22238;&#31572;&#20851;&#20110;&#29305;&#23450;&#22320;&#28857;&#30340;&#32454;&#31890;&#24230;&#38382;&#39064;&#26102;&#65292;&#27604;&#22914;&#26434;&#36135;&#24215;&#25110;&#39184;&#39302;&#65292;&#36825;&#20123;&#26500;&#25104;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#30340;&#26041;&#38754;&#26102;&#65292;&#23427;&#20204;&#30340;&#25928;&#29992;&#21463;&#21040;&#38459;&#30861;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#25105;&#20204;&#22478;&#24066;&#20013;&#30340;&#22320;&#28857;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#36755;&#20837;&#21040;LLMs&#20013;&#65292;&#20197;&#20415;&#20110;&#29702;&#35299;&#21644;&#35760;&#24518;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22478;&#24066;&#29305;&#23450;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#30340;&#24314;&#35758;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#24187;&#35273;&#12290;&#25105;&#20204;&#20998;&#20139;&#25105;&#20204;&#30340;&#27169;&#22411;LAMP&#21644;&#29992;&#20110;&#35757;&#32451;&#23427;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#20998;&#26512;&#20854;&#27491;&#30830;&#26816;&#32034;&#31354;&#38388;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09059v1 Announce Type: new  Abstract: Large Language Models (LLMs) are poised to play an increasingly important role in our lives, providing assistance across a wide array of tasks. In the geospatial domain, LLMs have demonstrated the ability to answer generic questions, such as identifying a country's capital; nonetheless, their utility is hindered when it comes to answering fine-grained questions about specific places, such as grocery stores or restaurants, which constitute essential aspects of people's everyday lives. This is mainly because the places in our cities haven't been systematically fed into LLMs, so as to understand and memorize them. This study introduces a novel framework for fine-tuning a pre-trained model on city-specific data, to enable it to provide accurate recommendations, while minimizing hallucinations. We share our model, LAMP, and the data used to train it. We conduct experiments to analyze its ability to correctly retrieving spatial objects, and co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#35760;&#24405;&#29983;&#25104;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;&#65292;&#22312;PubMedQA&#26041;&#38754;&#24615;&#33021;&#20248;&#20110;GPT-4&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;</title><link>https://arxiv.org/abs/2403.09057</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Continued Pretrained LLM Approach for Automatic Medical Note Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#35760;&#24405;&#29983;&#25104;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;&#65292;&#22312;PubMedQA&#26041;&#38754;&#24615;&#33021;&#20248;&#20110;GPT-4&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#27491;&#22312;&#38761;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#26368;&#24378;&#22823;&#30340;LLM&#23545;&#20110;&#22823;&#22810;&#25968;&#39046;&#22495;&#29305;&#23450;&#22330;&#26223;&#26469;&#35828;&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36830;&#32493;&#35757;&#32451;&#30340;130&#20159;&#21442;&#25968; Llama2-basd LLM&#65292;&#19987;&#20026;&#21307;&#30103;&#23545;&#35805;&#32780;&#35774;&#35745;&#65292;&#24182;&#22312;&#33258;&#21160;&#35760;&#24405;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;PubMedQA&#20013;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;76.6&#65285;&#65292;&#22312;&#24635;&#32467;&#21307;&#30103;&#23545;&#35805;&#20026;SOAP&#31508;&#35760;&#26041;&#38754;&#19982;GPT-4&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#26041;&#38754;&#36229;&#36807;&#20102;GPT-4&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09057v1 Announce Type: cross  Abstract: LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like GPT-4, is too costly for most domain-specific scenarios. We present the first continuously trained 13B Llama2-based LLM that is purpose-built for medical conversations and measured on automated scribing. Our results show that our model outperforms GPT-4 in PubMedQA with 76.6\% accuracy and matches its performance in summarizing medical conversations into SOAP notes. Notably, our model exceeds GPT-4 in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness.
&lt;/p&gt;</description></item><item><title>RAGGED&#26694;&#26550;&#20998;&#26512;&#21644;&#20248;&#21270;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#36866;&#21512;&#19981;&#21516;RAG&#35774;&#32622;&#30340;&#20107;&#23454;&#65292;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#38543;&#25991;&#26723;&#25968;&#37327;&#22686;&#21152;&#32780;&#25913;&#21892;&#65292;&#32780;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#21482;&#33021;&#26377;&#25928;&#21033;&#29992;&#23569;&#37327;&#25991;&#26723;&#12290;</title><link>https://arxiv.org/abs/2403.09040</link><description>&lt;p&gt;
RAGGED:&#26397;&#30528;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#30693;&#24773;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09040
&lt;/p&gt;
&lt;p&gt;
RAGGED&#26694;&#26550;&#20998;&#26512;&#21644;&#20248;&#21270;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#36866;&#21512;&#19981;&#21516;RAG&#35774;&#32622;&#30340;&#20107;&#23454;&#65292;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#38543;&#25991;&#26723;&#25968;&#37327;&#22686;&#21152;&#32780;&#25913;&#21892;&#65292;&#32780;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#21482;&#33021;&#26377;&#25928;&#21033;&#29992;&#23569;&#37327;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09040v1 &#22768;&#26126;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#20026;&#25991;&#26723;&#22411;&#38382;&#31572;&#31561;&#20219;&#21153;&#25552;&#20379;&#38468;&#21152;&#19978;&#19979;&#25991;&#65292;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;RAG&#30340;&#25928;&#21147;&#39640;&#24230;&#20381;&#36182;&#20110;&#20854;&#37197;&#32622;&#65292;&#20174;&#32780;&#24341;&#21457;&#19968;&#20010;&#38382;&#39064;&#65306;&#20160;&#20040;&#26159;&#26368;&#20339;RAG&#37197;&#32622;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RAGGED&#26694;&#26550;&#26469;&#20998;&#26512;&#21644;&#20248;&#21270;RAG&#31995;&#32479;&#12290;&#22312;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#25991;&#26723;&#22411;&#38382;&#31572;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#32463;&#20856;&#30340;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#22120;&#65292;&#20197;&#21450;&#22235;&#31181;&#22312;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#32467;&#26500;&#20013;&#34920;&#29616;&#20248;&#24322;&#30340;LMs&#12290;&#36890;&#36807;RAGGED&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#36866;&#21512;&#23436;&#20840;&#19981;&#21516;&#30340;RAG&#35774;&#32622;&#12290;&#34429;&#28982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#38543;&#30528;&#26356;&#22810;&#25991;&#26723;&#30340;&#22686;&#21152;&#32780;&#21333;&#35843;&#25552;&#21319;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#21482;&#33021;&#26377;&#25928;&#22320;&#20351;&#29992;&lt;5&#20010;&#25991;&#26723;&#65292;&#23613;&#31649;&#36890;&#24120;&#20855;&#26377;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;RAGGED&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;LMs&#30340;&#19978;&#19979;&#25991;&#21033;&#29992;&#20064;&#24815;&#65292;&#25105;&#20204;&#21457;&#29616;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09040v1 Announce Type: new  Abstract: Retrieval-augmented generation (RAG) greatly benefits language models (LMs) by providing additional context for tasks such as document-based question answering (DBQA). Despite its potential, the power of RAG is highly dependent on its configuration, raising the question: What is the optimal RAG configuration? To answer this, we introduce the RAGGED framework to analyze and optimize RAG systems. On a set of representative DBQA tasks, we study two classic sparse and dense retrievers, and four top-performing LMs in encoder-decoder and decoder-only architectures. Through RAGGED, we uncover that different models suit substantially varied RAG setups. While encoder-decoder models monotonically improve with more documents, we find decoder-only models can only effectively use &lt; 5 documents, despite often having a longer context window. RAGGED offers further insights into LMs' context utilization habits, where we find that encoder-decoder models r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#25506;&#27979;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#30693;&#35782;&#65292;&#21457;&#29616;&#39318;&#20010;&#20196;&#29260;&#30340;logit&#20998;&#24067;&#21253;&#21547;&#36275;&#22815;&#20449;&#24687;&#65292;&#21487;&#20197;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#30340;&#35270;&#35273;&#38382;&#39064;&#12289;&#38450;&#33539;&#22810;&#27169;&#24577;&#36234;&#29425;&#25915;&#20987;&#20197;&#21450;&#35782;&#21035;&#27450;&#39575;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#20197;&#26377;&#25928;&#25913;&#21892;&#29983;&#25104;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.09037</link><description>&lt;p&gt;
&#31532;&#19968;&#20010;&#30693;&#36947;&#65306;&#20196;&#29260;&#20998;&#24067;&#22914;&#20309;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#30693;&#35782;&#65311;
&lt;/p&gt;
&lt;p&gt;
The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#25506;&#27979;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#30693;&#35782;&#65292;&#21457;&#29616;&#39318;&#20010;&#20196;&#29260;&#30340;logit&#20998;&#24067;&#21253;&#21547;&#36275;&#22815;&#20449;&#24687;&#65292;&#21487;&#20197;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#30340;&#35270;&#35273;&#38382;&#39064;&#12289;&#38450;&#33539;&#22810;&#27169;&#24577;&#36234;&#29425;&#25915;&#20987;&#20197;&#21450;&#35782;&#21035;&#27450;&#39575;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#20197;&#26377;&#25928;&#25913;&#21892;&#29983;&#25104;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#26088;&#22312;&#35299;&#37322;&#21644;&#21709;&#24212;&#20154;&#31867;&#25351;&#20196;&#65292;&#20294;&#30001;&#20110;&#19981;&#24403;&#25351;&#20196;&#32780;&#20598;&#23572;&#29983;&#25104;&#24187;&#35273;&#25110;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#32447;&#24615;&#25506;&#27979;&#26469;&#25581;&#31034;LVLMs&#36755;&#20986;&#23618;&#30340;&#38544;&#34255;&#30693;&#35782;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#39318;&#20010;&#20196;&#29260;&#30340;logit&#20998;&#24067;&#21253;&#21547;&#36275;&#22815;&#20449;&#24687;&#65292;&#21487;&#20197;&#30830;&#23450;&#26159;&#21542;&#24212;&#23545;&#25351;&#20196;&#20316;&#20986;&#21709;&#24212;&#65292;&#21253;&#25324;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#30340;&#35270;&#35273;&#38382;&#39064;&#12289;&#38450;&#33539;&#22810;&#27169;&#24577;&#36234;&#29425;&#25915;&#20987;&#20197;&#21450;&#35782;&#21035;&#27450;&#39575;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#38544;&#34255;&#30693;&#35782;&#22312;&#21709;&#24212;&#29983;&#25104;&#36807;&#31243;&#20013;&#38543;&#21518;&#20196;&#29260;&#30340;logit&#36880;&#28176;&#20002;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#22312;&#29983;&#25104;&#31532;&#19968;&#20010;&#20196;&#29260;&#26102;&#65292;&#26377;&#25928;&#25913;&#21892;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35265;&#35299;&#65306;&#39318;&#20808;&#65292;CLIP&#27169;&#22411;&#24050;&#32463;&#21253;&#21547;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#30340;&#24378;&#28872;&#20449;&#21495;&#65292;&#34920;&#26126;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09037v1 Announce Type: cross  Abstract: Large vision-language models (LVLMs), designed to interpret and respond to human instructions, occasionally generate hallucinated or harmful content due to inappropriate instructions. This study uses linear probing to shed light on the hidden knowledge at the output layer of LVLMs. We demonstrate that the logit distributions of the first tokens contain sufficient information to determine whether to respond to the instructions, including recognizing unanswerable visual questions, defending against multi-modal jailbreaking attack, and identifying deceptive questions. Such hidden knowledge is gradually lost in logits of subsequent tokens during response generation. Then, we illustrate a simple decoding strategy at the generation of the first token, effectively improving the generated content. In experiments, we find a few interesting insights: First, the CLIP model already contains a strong signal for solving these tasks, indicating poten
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102; CodeUltraFeedback &#25968;&#25454;&#38598;&#65292;&#36890;&#36807; AI &#21453;&#39304;&#20351; 14 &#31181;&#19981;&#21516;&#30340; LLMs &#23545; 10,000 &#20010;&#22797;&#26434;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#65292;&#24182;&#20351;&#29992; LLM-as-a-Judge &#26041;&#27861;&#35780;&#20272;&#23427;&#20204;&#19982;&#20116;&#31181;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272; LLM &#23545;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;&#22522;&#20934; CODAL-Bench&#12290;</title><link>https://arxiv.org/abs/2403.09032</link><description>&lt;p&gt;
CodeUltraFeedback&#65306;&#19968;&#31181;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;LLM&#20316;&#20026;&#27861;&#23448;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09032
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102; CodeUltraFeedback &#25968;&#25454;&#38598;&#65292;&#36890;&#36807; AI &#21453;&#39304;&#20351; 14 &#31181;&#19981;&#21516;&#30340; LLMs &#23545; 10,000 &#20010;&#22797;&#26434;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#65292;&#24182;&#20351;&#29992; LLM-as-a-Judge &#26041;&#27861;&#35780;&#20272;&#23427;&#20204;&#19982;&#20116;&#31181;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272; LLM &#23545;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;&#22522;&#20934; CODAL-Bench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#35780;&#20272;&#22797;&#26434;&#25991;&#26412;LLMs&#30340;&#36755;&#20986;&#12290;&#29616;&#26377;&#22522;&#20934;&#20208;&#36182;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#65292;&#26410;&#33021;&#35780;&#20272;&#29992;&#25143;&#25351;&#20196;&#21644;LLM&#36755;&#20986;&#20013;&#30340;&#24494;&#22937;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#23545;LLM&#20559;&#22909;&#23545;&#40784;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeUltraFeedback&#65292;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#22797;&#26434;&#25351;&#20196;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;AI&#21453;&#39304;&#26469;&#35843;&#25972;&#21644;&#23545;&#40784;LLMs&#19982;&#32534;&#31243;&#20559;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;14&#31181;&#19981;&#21516;&#30340;LLMs&#23545;&#36825;&#20123;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20116;&#31181;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24773;&#20917;&#36827;&#34892;&#27880;&#37322;&#65292;&#20351;&#29992;GPT-3.5&#30340;LLM&#20316;&#20026;&#27861;&#23448;&#26041;&#27861;&#20135;&#29983;&#25968;&#23383;&#21644;&#25991;&#26412;&#21453;&#39304;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CODAL-Bench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#19982;&#36825;&#20123;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;C
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09032v1 Announce Type: cross  Abstract: Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs. By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback. We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences. Our results show that C
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;ChartInstruct&#25968;&#25454;&#38598;&#21644;&#20004;&#31181;&#25351;&#20196;&#35843;&#25972;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#24120;&#35268;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#21508;&#31181;&#19982;&#22270;&#34920;&#30456;&#20851;&#20219;&#21153;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.09028</link><description>&lt;p&gt;
ChartInstruct&#65306;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09028
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;ChartInstruct&#25968;&#25454;&#38598;&#21644;&#20004;&#31181;&#25351;&#20196;&#35843;&#25972;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#24120;&#35268;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#21508;&#31181;&#19982;&#22270;&#34920;&#30456;&#20851;&#20219;&#21153;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#25552;&#20379;&#25968;&#25454;&#30340;&#21487;&#35270;&#21270;&#34920;&#31034;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#20998;&#26512;&#20449;&#24687;&#65292;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#21521;&#20182;&#20154;&#20256;&#36798;&#35265;&#35299;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#21508;&#31181;&#19982;&#22270;&#34920;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#38382;&#31572;&#21644;&#25688;&#35201;&#12290;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#30340;&#24120;&#35265;&#31574;&#30053;&#26159;&#24494;&#35843;&#26368;&#21021;&#22312;&#35270;&#35273;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#21508;&#31181;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#24191;&#27867;&#30340;&#19982;&#22270;&#34920;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChartInstruct&#65306;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#34920;&#29305;&#23450;&#30340;&#35270;&#35273;-&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;191K&#20010;&#25351;&#20196;&#21644;71K&#24352;&#22270;&#34920;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29992;&#20110;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#31995;&#32479;&#65306;&#65288;1&#65289;&#36830;&#25509;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;LLM&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65307;&#21644;&#65288;2&#65289;&#37319;&#29992;&#20004;&#27493;&#26041;&#27861;&#26469;&#25552;&#21462;&#22270;&#34920;&#25968;&#25454;&#34920;&#26684;&#30340;&#27969;&#27700;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09028v1 Announce Type: new  Abstract: Charts provide visual representations of data and are widely used for analyzing information, addressing queries, and conveying insights to others. Various chart-related downstream tasks have emerged recently, such as question-answering and summarization. A common strategy to solve these tasks is to fine-tune various models originally trained on vision tasks language. However, such task-specific models are not capable of solving a wide range of chart-related tasks, constraining their real-world applicability. To overcome these challenges, we introduce ChartInstruct: a novel chart-specific vision-language Instruction-following dataset comprising 191K instructions generated with 71K charts. We then present two distinct systems for instruction tuning on such datasets: (1) an end-to-end model that connects a vision encoder for chart understanding with a LLM; and (2) a pipeline model that employs a two-step approach to extract chart data table
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#21442;&#25968;&#20196;&#29260;&#24207;&#21015;&#20849;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#20256;&#32479;&#30340;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#25439;&#22833;&#21644;&#19979;&#19968;&#20010;&#24207;&#21015;&#39044;&#27979;&#25439;&#22833;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09024</link><description>&lt;p&gt;
&#21322;&#21442;&#25968;&#20196;&#29260;&#24207;&#21015;&#20849;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Semiparametric Token-Sequence Co-Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09024
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#21442;&#25968;&#20196;&#29260;&#24207;&#21015;&#20849;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#20256;&#32479;&#30340;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#25439;&#22833;&#21644;&#19979;&#19968;&#20010;&#24207;&#21015;&#39044;&#27979;&#25439;&#22833;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#21442;&#25968;&#20196;&#29260;&#24207;&#21015;&#20849;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#25968;&#21270;&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#35745;&#31639;&#30340;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#25439;&#22833;&#21644;&#22522;&#20110;&#38750;&#21442;&#25968;&#21270;&#24207;&#21015;&#23884;&#20837;&#31354;&#38388;&#35745;&#31639;&#30340;&#19979;&#19968;&#20010;&#24207;&#21015;&#39044;&#27979;&#25439;&#22833;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#38750;&#21442;&#25968;&#24207;&#21015;&#23884;&#20837;&#31354;&#38388;&#26159;&#30001;&#19968;&#20010;&#21333;&#29420;&#30340;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#65292;&#20854;&#20219;&#21153;&#26159;&#23558;&#36755;&#20837;&#25991;&#26412;&#21387;&#32553;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#20195;&#34920;&#24615;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#36825;&#20004;&#31181;&#30417;&#30563;&#35757;&#32451;&#30340;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#21333;&#29420;&#36890;&#36807;&#27599;&#31181;&#30417;&#30563;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#20849;&#30417;&#30563;&#40723;&#21169;&#27169;&#22411;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#39044;&#35757;&#32451;&#27493;&#39588;&#20013;&#24314;&#31435;&#30340;&#21442;&#25968;&#21270;&#26631;&#35760;&#31354;&#38388;&#30340;&#40065;&#26834;&#24615;&#20542;&#21521;&#20110;&#26377;&#25928;&#22686;&#24378;&#38750;&#21442;&#25968;&#21270;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09024v1 Announce Type: cross  Abstract: In this work, we introduce a semiparametric token-sequence co-supervision training method. It trains a language model by simultaneously leveraging supervision from the traditional next token prediction loss which is calculated over the parametric token embedding space and the next sequence prediction loss which is calculated over the nonparametric sequence embedding space. The nonparametric sequence embedding space is constructed by a separate language model tasked to condense an input text into a single representative embedding. Our experiments demonstrate that a model trained via both supervisions consistently surpasses models trained via each supervision independently. Analysis suggests that this co-supervision encourages a broader generalization capability across the model. Especially, the robustness of parametric token space which is established during the pretraining step tends to effectively enhance the stability of nonparametri
&lt;/p&gt;</description></item><item><title>AraTrust&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20449;&#35465;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20840;&#38754;&#20449;&#35465;&#35780;&#20272;&#22522;&#20934;&#30340;&#38382;&#39064;&#65292;&#24110;&#21161;&#20934;&#30830;&#35780;&#20272;&#21644;&#25552;&#39640;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09017</link><description>&lt;p&gt;
AraTrust&#65306;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#35465;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09017
&lt;/p&gt;
&lt;p&gt;
AraTrust&#26159;&#31532;&#19968;&#20010;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20449;&#35465;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20840;&#38754;&#20449;&#35465;&#35780;&#20272;&#22522;&#20934;&#30340;&#38382;&#39064;&#65292;&#24110;&#21161;&#20934;&#30830;&#35780;&#20272;&#21644;&#25552;&#39640;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09017v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36805;&#36895;&#21457;&#23637;&#21644;&#24191;&#27867;&#25509;&#21463;&#20984;&#26174;&#20102;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;&#37492;&#20110;&#38463;&#25289;&#20271;&#35821;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#12289;&#25991;&#21270;&#20016;&#23500;&#24615;&#21644;&#22320;&#20301;&#19981;&#39640;&#65292;&#26377;&#24517;&#35201;&#19987;&#27880;&#20110;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#30340;&#20449;&#35465;&#35780;&#20272;&#22522;&#20934;&#26159;&#20934;&#30830;&#35780;&#20272;&#21644;&#25552;&#39640;&#22312;&#38463;&#25289;&#20271;&#35821;&#25552;&#31034;&#26102;LLMs&#30340;&#23433;&#20840;&#24615;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AraTrust 1&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#30340;&#20449;&#35465;&#22522;&#20934;&#12290;AraTrust &#21253;&#21547;&#20102;516&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#28041;&#21450;&#19982;&#30495;&#23454;&#24615;&#12289;&#36947;&#24503;&#12289;&#23433;&#20840;&#24615;&#12289;&#36523;&#20307;&#20581;&#24247;&#12289;&#24515;&#29702;&#20581;&#24247;&#12289;&#19981;&#20844;&#24179;&#34892;&#20026;&#12289;&#38750;&#27861;&#27963;&#21160;&#30456;&#20851;&#30340;&#22810;&#20010;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09017v1 Announce Type: new  Abstract: The swift progress and widespread acceptance of artificial intelligence (AI) systems highlight a pressing requirement to comprehend both the capabilities and potential risks associated with AI. Given the linguistic complexity, cultural richness, and underrepresented status of Arabic in AI research, there is a pressing need to focus on Large Language Models (LLMs) performance and safety for Arabic related tasks. Despite some progress in their development, there is a lack of comprehensive trustworthiness evaluation benchmarks which presents a major challenge in accurately assessing and improving the safety of LLMs when prompted in Arabic. In this paper, we introduce AraTrust 1, the first comprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises 516 human-written multiple-choice questions addressing diverse dimensions related to truthfulness, ethics, safety, physical health, mental health, unfairness, illegal activities
&lt;/p&gt;</description></item><item><title>Ethos&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#21521;&#37327;&#19978;&#36827;&#34892;&#30699;&#27491;LMs&#20197;&#20943;&#36731;&#20135;&#29983;&#27602;&#24615;&#21644;&#20559;&#35265;&#36755;&#20986;&#20197;&#21450;&#36991;&#20813;&#38544;&#31169;&#27844;&#38706;&#12290;</title><link>https://arxiv.org/abs/2403.08994</link><description>&lt;p&gt;
Ethos&#65306;&#22312;&#27491;&#20132;&#21442;&#25968;&#31354;&#38388;&#20013;&#30699;&#27491;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Ethos: Rectifying Language Models in Orthogonal Parameter Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08994
&lt;/p&gt;
&lt;p&gt;
Ethos&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#21521;&#37327;&#19978;&#36827;&#34892;&#30699;&#27491;LMs&#20197;&#20943;&#36731;&#20135;&#29983;&#27602;&#24615;&#21644;&#20559;&#35265;&#36755;&#20986;&#20197;&#21450;&#36991;&#20813;&#38544;&#31169;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26497;&#22823;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;LMs&#20063;&#24341;&#21457;&#20102;&#20851;&#20110;&#29983;&#25104;&#20559;&#35265;&#25110;&#26377;&#27602;&#20869;&#23481;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#31169;&#20154;&#20449;&#24687;&#21487;&#33021;&#27844;&#38706;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;Ethos&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#21521;&#37327;&#19978;&#36827;&#34892;&#30699;&#27491;LMs&#20197;&#20943;&#36731;&#20135;&#29983;&#27602;&#24615;&#21644;&#20559;&#35265;&#36755;&#20986;&#20197;&#21450;&#36991;&#20813;&#38544;&#31169;&#27844;&#38706;&#12290;Ethos&#24314;&#31435;&#22312;&#20219;&#21153;&#31639;&#26415;&#22522;&#30784;&#19978;&#12290;&#28982;&#32780;&#65292;&#19982;&#24403;&#21069;&#30340;&#20219;&#21153;&#31639;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;Ethos&#22312;&#37325;&#26500;&#20219;&#21153;&#21521;&#37327;&#26102;&#21306;&#20998;&#20102;&#19968;&#33324;&#26377;&#30410;&#21644;&#19981;&#33391;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Ethos&#39318;&#20808;&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#33719;&#24471;&#19968;&#32452;&#20027;&#25104;&#20998;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#21521;&#37327;&#25237;&#24433;&#21040;&#20027;&#25104;&#20998;&#19978;&#65292;Ethos&#35782;&#21035;&#32534;&#30721;&#19968;&#33324;&#25110;&#19981;&#33391;&#30693;&#35782;&#30340;&#20027;&#25104;&#20998;&#12290;Ethos&#20165;&#20351;&#29992;&#24102;&#26377;&#19981;&#33391;&#30693;&#35782;&#30340;&#20219;&#21153;&#21521;&#37327;&#36827;&#34892;&#21542;&#23450;&#65292;&#20174;&#32780;&#26368;&#23567;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08994v1 Announce Type: new  Abstract: Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos identifies the principal components that encode general or undesired knowledge. Ethos performs negating using the task vector with undesired knowledge only, thereby minimi
&lt;/p&gt;</description></item><item><title>AutoGuide&#36890;&#36807;&#25552;&#21462;&#23884;&#20837;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#29983;&#25104;&#19968;&#32452;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#65292;&#20174;&#32780;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#20026;&#20195;&#29702;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.08978</link><description>&lt;p&gt;
AutoGuide: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#33258;&#21160;&#29983;&#25104;&#21644;&#36873;&#25321;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08978
&lt;/p&gt;
&lt;p&gt;
AutoGuide&#36890;&#36807;&#25552;&#21462;&#23884;&#20837;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#29983;&#25104;&#19968;&#32452;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#65292;&#20174;&#32780;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#20026;&#20195;&#29702;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#23427;&#20204;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#21463;&#38480;&#12290;&#36825;&#32473;&#22522;&#20110;LLMs&#30340;&#20195;&#29702;&#24102;&#26469;&#20102;&#37325;&#22823;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;LLMs&#32570;&#20047;&#36275;&#22815;&#30693;&#35782;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;AutoGuide&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#32463;&#39564;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#26469;&#24357;&#21512;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AutoGuide&#36890;&#36807;&#25552;&#21462;&#19968;&#32452;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#26377;&#25928;&#22320;&#25552;&#21462;&#23884;&#20837;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#12290;&#27599;&#20010;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#20197;&#31616;&#27905;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#65292;&#24182;&#36981;&#24490;&#26465;&#20214;&#32467;&#26500;&#65292;&#28165;&#26224;&#25551;&#36848;&#36866;&#29992;&#30340;&#29366;&#24577;&#12290;&#22240;&#27492;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#25351;&#21335;&#20026;&#21521;&#20195;&#29702;&#24403;&#21069;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39034;&#24207;&#20219;&#21153;&#20013;&#22823;&#24133;&#39046;&#20808;&#20110;&#31454;&#20105;&#30340;&#22522;&#20110;LLMs&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08978v1 Announce Type: new  Abstract: The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge. In this paper, we introduce a novel framework, called AutoGuide, that bridges the knowledge gap in pre-trained LLMs by leveraging implicit knowledge in offline experiences. Specifically, AutoGuide effectively extracts knowledge embedded in offline data by extracting a set of state-aware guidelines. Importantly, each state-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the state where it is applicable. As such, the resulting guidelines enable a principled way to provide helpful knowledge pertinent to an agent's current decision-making process. We show that our approach outperforms competitive LLM-based baselines by a large margin in sequential
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#20026;&#20102;&#36866;&#24212;&#20854;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#29992;&#30340;XAI&#27010;&#24565;&#65292;&#36890;&#36807;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;XAI&#26041;&#27861;&#35770;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.08946</link><description>&lt;p&gt;
&#21487;&#29992;&#30340;XAI&#65306;&#22312;LLM&#26102;&#20195;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#30340;10&#20010;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08946
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#20026;&#20102;&#36866;&#24212;&#20854;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#29992;&#30340;XAI&#27010;&#24565;&#65292;&#36890;&#36807;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;XAI&#26041;&#27861;&#35770;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25351;&#30340;&#26159;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27934;&#35265;&#65292;&#25581;&#31034;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36816;&#20316;&#26041;&#24335;&#30340;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;XAI&#30340;&#37325;&#28857;&#27491;&#34987;&#25193;&#23637;&#21040;&#24120;&#24120;&#22240;&#20026;&#19981;&#36879;&#26126;&#32780;&#22791;&#21463;&#25209;&#35780;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#19968;&#25299;&#23637;&#38656;&#35201;&#23545;XAI&#26041;&#27861;&#35770;&#36827;&#34892;&#26174;&#33879;&#36716;&#21464;&#65292;&#22240;&#20026;&#26377;&#20004;&#20010;&#21407;&#22240;&#12290;&#39318;&#20808;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;XAI&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;LLMs&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#34892;&#19994;&#24212;&#29992;&#20013;&#65292;XAI&#30340;&#35282;&#33394;&#20174;&#20165;&#20165;&#25171;&#24320;&#8220;&#40657;&#21283;&#23376;&#8221;&#36716;&#21464;&#20026;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20165;&#20316;&#20026;XAI&#27934;&#35265;&#30340;&#34987;&#21160;&#25509;&#21463;&#32773;&#65292;LLMs&#30340;&#29420;&#29305;&#33021;&#21147;&#33021;&#22815;&#30456;&#20114;&#22686;&#24378;XAI&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#65288;1&#65289;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08946v1 Announce Type: cross  Abstract: Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the "black box" to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1)
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LMStyle&#22522;&#20934;&#65292;&#38024;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#36827;&#34892;&#35780;&#20272;&#65292;&#19981;&#20165;&#21487;&#20197;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#22320;&#34913;&#37327;LLMs&#30340;&#39118;&#26684;&#36716;&#31227;&#36136;&#37327;&#65292;&#36824;&#32771;&#34385;&#20102;&#36866;&#24403;&#24615;&#36825;&#19968;&#26032;&#39062;&#24230;&#37327;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.08943</link><description>&lt;p&gt;
LMStyle&#22522;&#20934;&#65306;&#35780;&#20272;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08943
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LMStyle&#22522;&#20934;&#65292;&#38024;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#36827;&#34892;&#35780;&#20272;&#65292;&#19981;&#20165;&#21487;&#20197;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#22320;&#34913;&#37327;LLMs&#30340;&#39118;&#26684;&#36716;&#31227;&#36136;&#37327;&#65292;&#36824;&#32771;&#34385;&#20102;&#36866;&#24403;&#24615;&#36825;&#19968;&#26032;&#39062;&#24230;&#37327;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#31361;&#30772;&#20197;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#30740;&#31350;&#30028;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#38543;&#30528;LLMs&#30340;&#21457;&#23637;&#65292;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#23545;&#35805;&#27169;&#22411;&#30340;&#38382;&#39064;&#24050;&#25104;&#20026;&#33258;&#28982;&#24310;&#20280;&#65292;&#20854;&#20013;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#33021;&#25317;&#26377;&#33258;&#24049;&#30340;&#39118;&#26684;&#29978;&#33267;&#29305;&#33394;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36825;&#31181;&#26032;&#35774;&#32622;&#23578;&#26410;&#24314;&#31435;&#26631;&#20934;&#35780;&#20272;&#25351;&#26631;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;LMStyle&#22522;&#20934;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#32842;&#22825;&#39118;&#26684;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;(C-TST)&#65292;&#21487;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#22320;&#34913;&#37327;LLMs&#30340;&#39118;&#26684;&#36716;&#31227;&#36136;&#37327;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#39118;&#26684;&#24378;&#24230;&#25351;&#26631;&#22806;&#65292;LMStyle&#22522;&#20934;&#36824;&#32771;&#34385;&#20102;&#19968;&#20010;&#31216;&#20026;&#36866;&#24403;&#24615;&#30340;&#26032;&#39062;&#24230;&#37327;&#26041;&#38754;&#65292;&#19968;&#20010;&#39640;&#27700;&#24179;&#25351;&#26631;&#65292;&#32771;&#34385;&#20102;&#36830;&#36143;&#24615;&#12289;&#27969;&#30021;&#24615;&#21644;&#20854;&#20182;&#38544;&#21547;&#22240;&#32032;&#65292;&#26080;&#38656;&#21442;&#32771;&#26679;&#26412;&#30340;&#24110;&#21161;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08943v1 Announce Type: new  Abstract: Since the breakthrough of ChatGPT, large language models (LLMs) have garnered significant attention in the research community. With the development of LLMs, the question of text style transfer for conversational models has emerged as a natural extension, where chatbots may possess their own styles or even characters. However, standard evaluation metrics have not yet been established for this new settings. This paper aims to address this issue by proposing the LMStyle Benchmark, a novel evaluation framework applicable to chat-style text style transfer (C-TST), that can measure the quality of style transfer for LLMs in an automated and scalable manner. In addition to conventional style strength metrics, LMStyle Benchmark further considers a novel aspect of metrics called appropriateness, a high-level metrics take account of coherence, fluency and other implicit factors without the aid of reference samples. Our experiments demonstrate that 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;LLM&#22312;&#29983;&#25104;&#25991;&#26412;&#36807;&#31243;&#20013;&#34394;&#26500;&#21644;&#35206;&#30422;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#35789;&#37325;&#21472;&#12289;&#26174;&#33879;&#24615;&#21644;&#20998;&#31867;&#22120;&#30340;&#19977;&#31181;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#21512;&#25104;&#38169;&#35823;&#19978;&#35757;&#32451;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#38169;&#35823;&#26816;&#27979;&#24615;&#33021;&#65292;&#34920;&#29616;&#20986;&#24555;&#36895;&#21644;&#26377;&#25928;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08904</link><description>&lt;p&gt;
&#22312;&#20105;&#35758;&#24615;&#35805;&#39064;&#20013;&#26816;&#27979;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#21644;&#35206;&#30422;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;LLM&#22312;&#29983;&#25104;&#25991;&#26412;&#36807;&#31243;&#20013;&#34394;&#26500;&#21644;&#35206;&#30422;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#35789;&#37325;&#21472;&#12289;&#26174;&#33879;&#24615;&#21644;&#20998;&#31867;&#22120;&#30340;&#19977;&#31181;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#21512;&#25104;&#38169;&#35823;&#19978;&#35757;&#32451;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#38169;&#35823;&#26816;&#27979;&#24615;&#33021;&#65292;&#34920;&#29616;&#20986;&#24555;&#36895;&#21644;&#26377;&#25928;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#22788;&#29702;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#20105;&#35758;&#24615;&#35805;&#39064;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22522;&#20110;&#32500;&#22522;&#30334;&#31185;&#30340;&#20013;&#31435;&#35266;&#28857;&#65288;NPOV&#65289;&#21407;&#21017;&#65306;&#25215;&#35748;&#19981;&#23384;&#22312;&#19968;&#20010;&#30495;&#23454;&#31572;&#26696;&#65292;&#24182;&#21576;&#29616;&#22810;&#20010;&#35266;&#28857;&#12290;&#25105;&#20204;&#23558;&#27492;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#20174;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#35266;&#28857;&#65292;&#28982;&#21518;LLM&#36127;&#36131;&#20174;&#32473;&#23450;&#30340;&#35266;&#28857;&#29983;&#25104;&#27969;&#30021;&#19988;&#24544;&#23454;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#30830;&#23450;&#24615;&#26816;&#32034;&#31995;&#32479;&#65292;&#28982;&#21518;&#19987;&#27880;&#20110;&#22312;&#36825;&#31181;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#24120;&#35265;LLM&#22833;&#36133;&#27169;&#24335;&#65292;&#21363;&#34394;&#26500;&#21644;&#35206;&#30422;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#22522;&#20110;&#65288;1&#65289;&#35789;&#37325;&#21472;&#65292;&#65288;2&#65289;&#26174;&#33879;&#24615;&#21644;&#65288;3&#65289;&#22522;&#20110;LLM&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#27492;&#31867;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;LLM&#30340;&#20998;&#31867;&#22120;&#65292;&#21363;&#20351;&#21482;&#22312;&#21512;&#25104;&#38169;&#35823;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#38169;&#35823;&#26816;&#27979;&#24615;&#33021;&#65292;&#34394;&#26500;&#38169;&#35823;&#30340;ROC AUC&#20998;&#25968;&#20026;95.3%&#65292;&#35206;&#30422;&#38169;&#35823;&#30340;ROC AUC&#20998;&#25968;&#20026;90.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08904v1 Announce Type: new  Abstract: We explore a strategy to handle controversial topics in LLM-based chatbots based on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the absence of a single true answer and surface multiple perspectives. We frame this as retrieval augmented generation, where perspectives are retrieved from a knowledge base and the LLM is tasked with generating a fluent and faithful response from the given perspectives. As a starting point, we use a deterministic retrieval system and then focus on common LLM failure modes that arise during this approach to text generation, namely hallucination and coverage errors. We propose and evaluate three methods to detect such errors based on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our results demonstrate that LLM-based classifiers, even when trained only on synthetic errors, achieve high error detection performance, with ROC AUC scores of 95.3% for hallucination and 90.5% for c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20102;&#33521;&#35821;&#23545;&#35805;&#20013;&#20449;&#24687;&#27969;&#30340;&#29983;&#25104;&#12289;&#39044;&#27979;&#21644;&#35843;&#33410;&#65292;&#25581;&#31034;&#20102;&#20449;&#24687;&#23494;&#24230;&#20197;&#21450;&#26816;&#32034;&#21644;&#21576;&#29616;&#20449;&#24687;&#30340;&#35748;&#30693;&#36127;&#33655;&#23545;&#23545;&#35805;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;backchannels&#22312;&#35843;&#33410;&#26032;&#39062;&#24615;&#20135;&#29983;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.08890</link><description>&lt;p&gt;
&#20174;&#8220;&#21999;&#8221;&#21040;&#8220;&#26159;&#30340;&#8221;&#65306;&#20154;&#31867;&#23545;&#35805;&#20013;&#20449;&#24687;&#27969;&#30340;&#29983;&#25104;&#12289;&#39044;&#27979;&#21644;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
From "um" to "yeah": Producing, predicting, and regulating information flow in human conversation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20102;&#33521;&#35821;&#23545;&#35805;&#20013;&#20449;&#24687;&#27969;&#30340;&#29983;&#25104;&#12289;&#39044;&#27979;&#21644;&#35843;&#33410;&#65292;&#25581;&#31034;&#20102;&#20449;&#24687;&#23494;&#24230;&#20197;&#21450;&#26816;&#32034;&#21644;&#21576;&#29616;&#20449;&#24687;&#30340;&#35748;&#30693;&#36127;&#33655;&#23545;&#23545;&#35805;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;backchannels&#22312;&#35843;&#33410;&#26032;&#39062;&#24615;&#20135;&#29983;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#38656;&#35201;&#27880;&#24847;&#21147;&#12290;&#35828;&#35805;&#32773;&#24517;&#39035;&#21484;&#21796;&#21333;&#35789;&#65292;&#21548;&#32773;&#24517;&#39035;&#29702;&#35299;&#23427;&#20204;&#65292;&#20004;&#32773;&#24517;&#39035;&#20849;&#21516;&#21327;&#21830;&#36825;&#20123;&#20449;&#24687;&#27969;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#22312;&#20960;&#20998;&#20043;&#19968;&#31186;&#20869;&#23436;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20102;&#33521;&#35821;&#23545;&#35805;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;CANDOR&#35821;&#26009;&#24211;&#20013;&#30340;&#24037;&#20316;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#20449;&#24687;&#23494;&#24230;&#30340;&#26032;&#20272;&#31639;&#65292;&#32422;&#20026;&#27599;&#31186;13&#20301;&#65292;&#24182;&#21457;&#29616;&#26816;&#32034;&#21644;&#21576;&#29616;&#20449;&#24687;&#30340;&#35748;&#30693;&#36127;&#33655;&#19982;&#20043;&#30456;&#20851;&#30340;&#26174;&#33879;&#25928;&#24212;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;backchannels&#30340;&#20316;&#29992;-&#21548;&#20247;&#25552;&#20379;&#30340;&#31616;&#30701;&#30340;&#8220;&#26159;&#30340;&#8221;&#12289;&#8220;&#21999;&#21999;&#8221;&#21644;&#8220;&#21999;&#8221;&#30340;&#20316;&#29992;&#22312;&#35843;&#33410;&#26032;&#39062;&#24615;&#30340;&#20135;&#29983;&#65306;&#23548;&#33268;backchannel&#30340;&#38454;&#27573;&#19982;&#20449;&#24687;&#36895;&#29575;&#30340;&#19979;&#38477;&#30456;&#20851;&#65292;&#32780;&#38543;&#21518;&#30340;&#35328;&#35821;&#24674;&#22797;&#21040;&#20808;&#21069;&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#38271;&#26399;&#20197;&#26469;&#20851;&#20110;&#25105;&#20204;&#22914;&#20309;&#24212;&#23545;&#35748;&#30693;&#36164;&#28304;&#27874;&#21160;&#38656;&#27714;&#30340;&#29702;&#35770;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08890v1 Announce Type: new  Abstract: Conversation demands attention. Speakers must call words to mind, listeners must make sense of them, and both together must negotiate this flow of information, all in fractions of a second. We used large language models to study how this works in a large-scale dataset of English-language conversation, the CANDOR corpus. We provide a new estimate of the information density of unstructured conversation, of approximately 13 bits/second, and find significant effects associated with the cognitive load of both retrieving, and presenting, that information. We also reveal a role for backchannels -- the brief yeahs, uh-huhs, and mhmms that listeners provide -- in regulating the production of novelty: the lead-up to a backchannel is associated with declining information rate, while speech downstream rebounds to previous rates. Our results provide new insights into long-standing theories of how we respond to fluctuating demands on cognitive resourc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;PAPERCLIP&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22825;&#25991;&#35266;&#27979;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#36215;&#26469;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.08851</link><description>&lt;p&gt;
PAPERCLIP&#65306;&#20351;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#23558;&#22825;&#25991;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08851
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;PAPERCLIP&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22825;&#25991;&#35266;&#27979;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#36215;&#26469;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PAPERCLIP&#65288;Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23558;&#30001;&#26395;&#36828;&#38236;&#25104;&#20687;&#30340;&#22825;&#25991;&#35266;&#27979;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#36215;&#26469;&#30340;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#26159;&#20174;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#24494;&#35843;&#32780;&#26469;&#65292;&#20351;&#29992;&#25104;&#21151;&#30340;&#35266;&#27979;&#25552;&#26696;&#25688;&#35201;&#21644;&#30456;&#24212;&#30340;&#19979;&#28216;&#35266;&#27979;&#65292;&#20854;&#20013;&#25688;&#35201;&#21487;&#36873;&#25321;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24341;&#23548;&#29983;&#25104;&#26469;&#36827;&#34892;&#24635;&#32467;&#12290;&#20197;&#21704;&#21187;&#31354;&#38388;&#26395;&#36828;&#38236;&#65288;HST&#65289;&#30340;&#35266;&#27979;&#20026;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24494;&#35843;&#30340;&#27169;&#22411;&#36890;&#36807;&#38024;&#23545;&#22270;&#20687;&#26816;&#32034;&#65288;&#21363;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25214;&#21040;&#26368;&#30456;&#20851;&#30340;&#35266;&#27979;&#65289;&#21644;&#25551;&#36848;&#26816;&#32034;&#65288;&#21363;&#26597;&#35810;&#19982;&#22825;&#25991;&#29289;&#20307;&#31867;&#21035;&#21644;&#29992;&#20363;&#26368;&#30456;&#20851;&#30340;&#20869;&#23481;&#65289;&#30340;&#27979;&#35797;&#65292;&#20307;&#29616;&#20102;&#35266;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08851v1 Announce Type: cross  Abstract: We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training), a method which associates astronomical observations imaged by telescopes with natural language using a neural network model. The model is fine-tuned from a pre-trained Contrastive Language-Image Pre-training (CLIP) model using successful observing proposal abstracts and corresponding downstream observations, with the abstracts optionally summarized via guided generation using large language models (LLMs). Using observations from the Hubble Space Telescope (HST) as an example, we show that the fine-tuned model embodies a meaningful joint representation between observations and natural language through tests targeting image retrieval (i.e., finding the most relevant observations using natural language queries) and description retrieval (i.e., querying for astrophysical object classes and use cases most relevant to a 
&lt;/p&gt;</description></item><item><title>LoRA-SP&#21033;&#29992;&#38543;&#26426;&#21322;&#36873;&#25321;&#21442;&#25968;&#20923;&#32467;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#26377;&#25928;&#24179;&#34913;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#20445;&#30041;&#21644;&#20219;&#21153;&#29305;&#23450;&#20248;&#21270;&#30340;&#36866;&#24212;&#24615;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08822</link><description>&lt;p&gt;
LoRA-SP&#65306;&#29992;&#20110;&#36164;&#28304;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#37096;&#20998;&#21442;&#25968;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08822
&lt;/p&gt;
&lt;p&gt;
LoRA-SP&#21033;&#29992;&#38543;&#26426;&#21322;&#36873;&#25321;&#21442;&#25968;&#20923;&#32467;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#26377;&#25928;&#24179;&#34913;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#20445;&#30041;&#21644;&#20219;&#21153;&#29305;&#23450;&#20248;&#21270;&#30340;&#36866;&#24212;&#24615;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoRA-SP&#65288;&#31616;&#21270;&#37096;&#20998;&#21442;&#25968;&#36866;&#24212;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26694;&#26550;&#20869;&#30340;&#38543;&#26426;&#21322;&#36873;&#25321;&#21442;&#25968;&#20923;&#32467;&#12290;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#20445;&#30041;&#21644;&#20219;&#21153;&#29305;&#23450;&#20248;&#21270;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#38543;&#26426;&#26426;&#21046;&#65292;LoRA-SP &#30830;&#23450;&#35201;&#26356;&#26032;&#25110;&#20923;&#32467;&#21738;&#20123;&#21442;&#25968;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934; NLP &#20219;&#21153;&#20013;&#35780;&#20272;&#20102; LoRA-SP&#65292;&#23637;&#31034;&#20102;&#23427;&#19982;&#20256;&#32479;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#21644;&#20854;&#20182;&#21442;&#25968;&#39640;&#25928;&#25216;&#26415;&#30456;&#27604;&#65292;&#33021;&#22815;&#20197;&#22823;&#22823;&#38477;&#20302;&#30340;&#36164;&#28304;&#28040;&#32791;&#23454;&#29616;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;LoRA-SP &#30340;&#21019;&#26032;&#26041;&#27861;&#19981;&#20165;&#26377;&#21161;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#20808;&#36827;&#30340; NLP &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08822v1 Announce Type: cross  Abstract: In addressing the computational and memory demands of fine-tuning Large Language Models(LLMs), we propose LoRA-SP(Streamlined Partial Parameter Adaptation), a novel approach utilizing randomized half-selective parameter freezing within the Low-Rank Adaptation(LoRA)framework. This method efficiently balances pre-trained knowledge retention and adaptability for task-specific optimizations. Through a randomized mechanism, LoRA-SP determines which parameters to update or freeze, significantly reducing computational and memory requirements without compromising model performance. We evaluated LoRA-SP across several benchmark NLP tasks, demonstrating its ability to achieve competitive performance with substantially lower resource consumption compared to traditional full-parameter fine-tuning and other parameter-efficient techniques. LoRA-SP innovative approach not only facilitates the deployment of advanced NLP models in resource-limited sett
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#26041;&#27861;THERMOMETER&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#25968;&#25454;&#30340;&#36741;&#21161;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#20934;&#30830;&#24615;&#20445;&#25345;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#21709;&#24212;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.08819</link><description>&lt;p&gt;
&#28201;&#24230;&#35745;&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Thermometer: Towards Universal Calibration for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08819
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#26041;&#27861;THERMOMETER&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#25968;&#25454;&#30340;&#36741;&#21161;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#20934;&#30830;&#24615;&#20445;&#25345;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#21709;&#24212;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#35265;&#30340;&#24178;&#39044;&#25514;&#26045;&#22914;&#25351;&#20196;&#35843;&#25972;&#36890;&#24120;&#20250;&#23548;&#33268;&#26657;&#20934;&#19981;&#20339;&#30340;LLMs&#12290;&#23613;&#31649;&#26657;&#20934;&#22312;&#20256;&#32479;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#25506;&#35752;&#65292;&#20294;&#23545;LLMs&#36827;&#34892;&#26657;&#20934;&#20855;&#26377;&#29420;&#29305;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#19981;&#20165;&#26469;&#33258;LLMs&#30340;&#20005;&#26684;&#35745;&#31639;&#35201;&#27714;&#65292;&#20063;&#26469;&#33258;&#23427;&#20204;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20351;&#23427;&#20204;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;LLMs&#30340;&#26657;&#20934;&#26041;&#27861;THERMOMETER&#12290;THERMOMETER&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#30340;&#36741;&#21161;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#20934;LLM&#12290;&#23427;&#22312;&#35745;&#31639;&#19978;&#25928;&#29575;&#39640;&#65292;&#20445;&#25345;&#20102;LLM&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#26032;&#20219;&#21153;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#26657;&#20934;&#21709;&#24212;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#30340;&#24191;&#27867;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08819v1 Announce Type: cross  Abstract: We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MINGLE&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#32423;&#27880;&#20837;&#31574;&#30053;&#23558;&#21307;&#23398;&#27010;&#24565;&#35821;&#20041;&#21644;&#20020;&#24202;&#31508;&#35760;&#35821;&#20041;&#34701;&#21512;&#21040;&#36229;&#22270;&#20013;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.08818</link><description>&lt;p&gt;
&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22810;&#27169;&#24577;&#34701;&#21512;&#65306;&#23558;&#20020;&#24202;&#35760;&#24405;&#21644;&#31508;&#35760;&#19982;&#36229;&#22270;&#21644;LLM&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08818
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MINGLE&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#32423;&#27880;&#20837;&#31574;&#30053;&#23558;&#21307;&#23398;&#27010;&#24565;&#35821;&#20041;&#21644;&#20020;&#24202;&#31508;&#35760;&#35821;&#20041;&#34701;&#21512;&#21040;&#36229;&#22270;&#20013;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#22312;&#36817;&#20960;&#21313;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;EHRs&#36890;&#24120;&#21253;&#21547;&#24322;&#26500;&#20449;&#24687;&#65292;&#22914;&#34920;&#26684;&#24418;&#24335;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#25991;&#26412;&#31508;&#35760;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;EHRs&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#20449;&#24687;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#65292;&#25552;&#20379;&#24739;&#32773;&#20581;&#24247;&#29366;&#24577;&#30340;&#26356;&#23436;&#25972;&#22270;&#29255;&#12290;&#23613;&#31649;&#23545;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#19981;&#21516;&#31867;&#22411;EHR&#25968;&#25454;&#30340;&#34701;&#21512;&#65288;&#22810;&#27169;&#24577;&#34701;&#21512;&#65289;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#21307;&#30103;&#32534;&#30721;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#20070;&#38754;&#31508;&#35760;&#20013;&#23384;&#22312;&#30340;&#22122;&#38899;&#21644;&#20887;&#20313;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MINGLE&#30340;&#26032;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#23558;EHR&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20004;&#32423;&#27880;&#20837;&#31574;&#30053;&#23558;&#21307;&#23398;&#27010;&#24565;&#35821;&#20041;&#21644;&#20020;&#24202;&#31508;&#35760;&#35821;&#20041;&#34701;&#21512;&#21040;&#36229;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08818v1 Announce Type: cross  Abstract: Electronic Health Records (EHRs) have become increasingly popular to support clinical decision-making and healthcare in recent decades. EHRs usually contain heterogeneous information, such as structural data in tabular form and unstructured data in textual notes. Different types of information in EHRs can complement each other and provide a more complete picture of the health status of a patient. While there has been a lot of research on representation learning of structured EHR data, the fusion of different types of EHR data (multimodal fusion) is not well studied. This is mostly because of the complex medical coding systems used and the noise and redundancy present in the written notes. In this work, we propose a new framework called MINGLE, which integrates both structures and semantics in EHR effectively. Our framework uses a two-level infusion strategy to combine medical concept semantics and clinical note semantics into hypergrap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#32467;&#21512;UFO&#26412;&#20307;&#35770;&#26469;&#30417;&#27979;&#35748;&#30693;&#34928;&#36864;&#23548;&#33268;&#30340;&#20449;&#24687;&#22788;&#29702;&#32570;&#38519;&#65292;&#39044;&#38450;&#25945;&#32946;&#29615;&#22659;&#20013;&#20986;&#29616;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.08795</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#26234;&#33021;&#32467;&#21512;UFO&#26412;&#20307;&#35770;&#30417;&#27979;&#35748;&#30693;&#34928;&#36864;&#23548;&#33268;&#30340;&#20449;&#24687;&#22788;&#29702;&#32570;&#38519;&#65292;&#36991;&#20813;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#20986;&#29616;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Ontologia para monitorar a defici\^encia mental em seus d\'eficts no processamento da informa\c{c}\~ao por decl\'inio cognitivo e evitar agress\~oes psicol\'ogicas e f\'isicas em ambientes educacionais com ajuda da I.A*
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#32467;&#21512;UFO&#26412;&#20307;&#35770;&#26469;&#30417;&#27979;&#35748;&#30693;&#34928;&#36864;&#23548;&#33268;&#30340;&#20449;&#24687;&#22788;&#29702;&#32570;&#38519;&#65292;&#39044;&#38450;&#25945;&#32946;&#29615;&#22659;&#20013;&#20986;&#29616;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;UFO&#26412;&#20307;&#35770;&#20998;&#26512;&#26469;&#26816;&#27979;&#19982;&#24515;&#29702;&#31038;&#20250;&#32570;&#38519;&#21450;&#20854;&#35825;&#21457;&#22240;&#32032;&#30456;&#20851;&#30340;&#35328;&#35821;&#21644;&#36523;&#20307;&#25915;&#20987;&#30340;&#20986;&#29616;&#65292;&#20197;&#38450;&#27490;&#26657;&#22253;&#29615;&#22659;&#20869;&#20986;&#29616;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08795v1 Announce Type: cross  Abstract: The intention of this article is to propose the use of artificial intelligence to detect through analysis by UFO ontology the emergence of verbal and physical aggression related to psychosocial deficiencies and their provoking agents, in an attempt to prevent catastrophic consequences within school environments.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#33073;&#31163;&#35821;&#22659;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22312;&#35299;&#20915;OOCD&#30456;&#20851;&#25968;&#25454;&#38480;&#21046;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08783</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#22270;&#20687;&#25991;&#26412;&#33073;&#31163;&#35821;&#22659;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Image-Text Out-Of-Context Detection Using Synthetic Multimodal Misinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#33073;&#31163;&#35821;&#22659;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22312;&#35299;&#20915;OOCD&#30456;&#20851;&#25968;&#25454;&#38480;&#21046;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20449;&#24687;&#19981;&#26029;&#22686;&#21152;&#30340;&#26102;&#20195;&#65292;&#34394;&#20551;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33073;&#31163;&#35821;&#22659;&#26816;&#27979;&#65288;OOCD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;OOCD&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26816;&#27979;&#22120;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#39564;&#35777;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#20351;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#35299;&#20915;&#19982;OOCD&#30456;&#20851;&#30340;&#25968;&#25454;&#38480;&#21046;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#22120;&#24212;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#21644;&#24320;&#21457;&#20581;&#22766;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31995;&#32479;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08783v1 Announce Type: cross  Abstract: Misinformation has become a major challenge in the era of increasing digital information, requiring the development of effective detection methods. We have investigated a novel approach to Out-Of-Context detection (OOCD) that uses synthetic data generation. We created a dataset specifically designed for OOCD and developed an efficient detector for accurate classification. Our experimental findings validate the use of synthetic data generation and demonstrate its efficacy in addressing the data limitations associated with OOCD. The dataset and detector should serve as valuable resources for future research and the development of robust misinformation detection systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08773</link><description>&lt;p&gt;
Veagle: &#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Veagle: Advancements in Multimodal Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#35821;&#35328;&#21644;&#35270;&#35273;&#22914;&#20309;&#32467;&#21512;&#20135;&#29983;&#20102;&#27987;&#21402;&#20852;&#36259;&#65292;&#20174;&#32780;&#20652;&#29983;&#20102;&#26088;&#22312;&#26080;&#32541;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24310;&#20280;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#33539;&#22260;&#20174;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#21040;&#35270;&#35273;&#23450;&#20301;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#20934;&#30830;&#35299;&#37322;&#22270;&#20687;&#24182;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#38024;&#23545;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20013;&#35266;&#23519;&#21040;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;Veagle&#65292;&#34701;&#21512;&#20102;&#21463;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08773v1 Announce Type: cross  Abstract: Lately, researchers in artificial intelligence have been really interested in how language and vision come together, giving rise to the development of multimodal models that aim to seamlessly integrate textual and visual information. Multimodal models, an extension of Large Language Models (LLMs), have exhibited remarkable capabilities in addressing a diverse array of tasks, ranging from image captioning and visual question answering (VQA) to visual grounding. While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios. This paper introduces a novel approach to enhance the multimodal capabilities of existing models. In response to the limitations observed in current Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs), our proposed model Veagle, incorporates a unique mechanism inspired by th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#23398;&#20064;&#26041;&#27861;SOTOPIA-$\pi$&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#34892;&#20026;&#20811;&#38534;&#21644;&#33258;&#25105;&#24378;&#21270;&#35757;&#32451;&#65292;&#25913;&#36827;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#31038;&#20132;&#26234;&#33021;&#65292;&#20351;&#20854;&#36798;&#21040;&#20102;&#19987;&#23478;&#27169;&#22411;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08715</link><description>&lt;p&gt;
SOTOPIA-$\pi$: &#20132;&#20114;&#24335;&#23398;&#20064;&#31038;&#20132;&#26234;&#33021;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SOTOPIA-$\pi$: Interactive Learning of Socially Intelligent Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08715
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#23398;&#20064;&#26041;&#27861;SOTOPIA-$\pi$&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#34892;&#20026;&#20811;&#38534;&#21644;&#33258;&#25105;&#24378;&#21270;&#35757;&#32451;&#65292;&#25913;&#36827;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#31038;&#20132;&#26234;&#33021;&#65292;&#20351;&#20854;&#36798;&#21040;&#20102;&#19987;&#23478;&#27169;&#22411;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#27169;&#20223;&#21644;&#31038;&#20132;&#20114;&#21160;&#26469;&#23398;&#20064;&#31038;&#20132;&#25216;&#33021;&#12290;&#29616;&#26377;&#30740;&#31350;&#22312;&#26500;&#24314;&#35821;&#35328;&#20195;&#29702;&#26041;&#38754;&#24456;&#23569;&#28041;&#21450;&#36825;&#31181;&#31038;&#20132;&#23398;&#20064;&#36807;&#31243;&#12290;&#21463;&#21040;&#36825;&#19968;&#31354;&#30333;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#23398;&#20064;&#26041;&#27861;SOTOPIA-$\pi$&#65292;&#25913;&#36827;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#31038;&#20132;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#34892;&#20026;&#20811;&#38534;&#21644;&#33258;&#25105;&#24378;&#21270;&#35757;&#32451;&#65292;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#35780;&#20998;&#23545;&#32463;&#36807;&#31579;&#36873;&#30340;&#31038;&#20132;&#20114;&#21160;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#20351;&#19968;&#20010;7B&#30340;LLM&#36798;&#21040;&#20102;&#19987;&#23478;&#27169;&#22411;(GPT-4-based agent)&#30340;&#31038;&#20132;&#30446;&#26631;&#23436;&#25104;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;MMLU&#22522;&#20934;&#19978;&#20445;&#25345;&#20102;&#36890;&#29992;&#30340;&#38382;&#31572;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;&#35757;&#32451;&#33539;&#24335;&#25581;&#31034;&#20102;LLM&#35780;&#20272;&#31038;&#20132;&#26234;&#33021;&#30340;&#19968;&#20123;&#22256;&#38590;&#65306;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#32773;&#39640;&#20272;&#20102;&#19987;&#38376;&#38024;&#23545;&#31038;&#20132;&#20114;&#21160;&#35757;&#32451;&#30340;&#35821;&#35328;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08715v1 Announce Type: new  Abstract: Humans learn social skills through both imitation and social interaction. This social learning process is largely understudied by existing research on building language agents. Motivated by this gap, we propose an interactive learning method, SOTOPIA-$\pi$, improving the social intelligence of language agents. This method leverages behavior cloning and self-reinforcement training on filtered social interaction data according to large language model (LLM) ratings. We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent), while improving the safety of language agents and maintaining general QA ability on the MMLU benchmark. We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence: LLM-based evaluators overestimate the abilities of the language agents trained specifically for social interaction.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08664</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#24037;&#20020;&#24202;&#35760;&#24405;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08664
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Llama 2 LLM&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#29983;&#25104;&#21487;&#20934;&#30830;&#21453;&#26144;&#30495;&#23454;&#24739;&#32773;&#20449;&#24687;&#30340;&#21512;&#25104;&#21307;&#30103;&#35760;&#24405;&#65292;&#19982;&#38656;&#35201;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#30340;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08664v1 Announce Type: new  Abstract: The challenge of accessing historical patient data for clinical research, while adhering to privacy regulations, is a significant obstacle in medical science. An innovative approach to circumvent this issue involves utilising synthetic medical records that mirror real patient data without compromising individual privacy. The creation of these synthetic datasets, particularly without using actual patient data to train Large Language Models (LLMs), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge. This study assesses the capability of the Llama 2 LLM to create synthetic medical records that accurately reflect real patient information, employing zero-shot and few-shot prompting strategies for comparison against fine-tuned methodologies that do require sensitive patient data during training. We focus on generating synthetic narratives for the History of Present Illness section, 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#33258;&#21160;&#20114;&#21160;&#35780;&#20272;&#65288;AIE&#65289;&#26694;&#26550;&#21644;&#29366;&#24577;&#24863;&#30693;&#30149;&#20154;&#27169;&#25311;&#22120;&#65288;SAPS&#65289;&#65292;&#20197;&#21160;&#24577;&#12289;&#30495;&#23454;&#30340;&#24179;&#21488;&#35780;&#20272;LLMs&#65292;&#24357;&#34917;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#20020;&#24202;&#20219;&#21153;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.08495</link><description>&lt;p&gt;
&#20855;&#26377;&#29366;&#24577;&#24863;&#30693;&#30149;&#20154;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20114;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08495
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#33258;&#21160;&#20114;&#21160;&#35780;&#20272;&#65288;AIE&#65289;&#26694;&#26550;&#21644;&#29366;&#24577;&#24863;&#30693;&#30149;&#20154;&#27169;&#25311;&#22120;&#65288;SAPS&#65289;&#65292;&#20197;&#21160;&#24577;&#12289;&#30495;&#23454;&#30340;&#24179;&#21488;&#35780;&#20272;LLMs&#65292;&#24357;&#34917;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#20020;&#24202;&#20219;&#21153;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20154;&#26426;&#20114;&#21160;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#33258;&#21160;&#20114;&#21160;&#35780;&#20272;&#65288;AIE&#65289;&#26694;&#26550;&#21644;&#29366;&#24577;&#24863;&#30693;&#30149;&#20154;&#27169;&#25311;&#22120;&#65288;SAPS&#65289;&#65292;&#26088;&#22312;&#24357;&#34917;&#20256;&#32479;LLM&#35780;&#20272;&#19982;&#20020;&#24202;&#23454;&#36341;&#30340;&#24494;&#22937;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08495v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in human interactions, yet their application within the medical field remains insufficiently explored. Previous works mainly focus on the performance of medical knowledge with examinations, which is far from the realistic scenarios, falling short in assessing the abilities of LLMs on clinical tasks. In the quest to enhance the application of Large Language Models (LLMs) in healthcare, this paper introduces the Automated Interactive Evaluation (AIE) framework and the State-Aware Patient Simulator (SAPS), targeting the gap between traditional LLM evaluations and the nuanced demands of clinical practice. Unlike prior methods that rely on static medical knowledge assessments, AIE and SAPS provide a dynamic, realistic platform for assessing LLMs through multi-turn doctor-patient simulations. This approach offers a closer approximation to real clinical scenarios and allows f
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Transformer&#27169;&#22411;&#21644;Context-Reverso&#25968;&#25454;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#28165;&#26224;&#24230;&#30340;&#21477;&#23376;</title><link>https://arxiv.org/abs/2403.08103</link><description>&lt;p&gt;
&#21033;&#29992;Context-Reverso&#25968;&#25454;&#20351;&#29992;Transformer&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#28165;&#26224;&#24230;&#30340;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;
Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08103
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Transformer&#27169;&#22411;&#21644;Context-Reverso&#25968;&#25454;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#28165;&#26224;&#24230;&#30340;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#20016;&#23500;&#30340;&#26102;&#20195;&#65292;&#25552;&#20379;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#19988;&#31616;&#27905;&#30340;&#20449;&#24687;&#23545;&#29992;&#25143;&#33267;&#20851;&#37325;&#35201;&#12290;&#20851;&#38190;&#35789;&#19978;&#19979;&#25991;(KIC)&#29983;&#25104;&#26159;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#25198;&#28436;&#33267;&#20851;&#37325;&#35201;&#35282;&#33394;&#30340;&#20219;&#21153;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#12289;&#20010;&#20154;&#21161;&#25163;&#21644;&#20869;&#23481;&#25688;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;T5 transformer&#27169;&#22411;&#29983;&#25104;&#32473;&#23450;&#20851;&#38190;&#35789;&#30340;&#26126;&#30830;&#19988;&#31616;&#27905;&#21477;&#23376;&#19978;&#19979;&#25991;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#20174;Context-Reverso API&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08103v1 Announce Type: cross  Abstract: In the age of information abundance, the ability to provide users with contextually relevant and concise information is crucial. Keyword in Context (KIC) generation is a task that plays a vital role in and generation applications, such as search engines, personal assistants, and content summarization. In this paper, we present a novel approach to generating unambiguous and brief sentence-contexts for given keywords using the T5 transformer model, leveraging data obtained from the Context-Reverso API. The code is available at https://github.com/Rusamus/word2context/tree/main .
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#23454;&#20307;&#23545;&#20154;&#31867;&#20114;&#21160;&#30340;&#38761;&#26032;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#23558;&#19987;&#38376;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#25193;&#23637;&#21040;&#25805;&#20316;&#24615;&#32452;&#32455;&#27969;&#31243;&#21644;&#22522;&#20110;&#30693;&#35782;&#21644;&#20154;&#31867;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.07769</link><description>&lt;p&gt;
&#23558;&#31454;&#20105;&#36716;&#21270;&#20026;&#21512;&#20316;&#65306;&#22810;Agent&#31995;&#32479;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#20195;&#32452;&#32455;&#20013;&#30340;&#38761;&#21629;&#24615;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07769
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#23454;&#20307;&#23545;&#20154;&#31867;&#20114;&#21160;&#30340;&#38761;&#26032;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#23558;&#19987;&#38376;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#25193;&#23637;&#21040;&#25805;&#20316;&#24615;&#32452;&#32455;&#27969;&#31243;&#21644;&#22522;&#20110;&#30693;&#35782;&#21644;&#20154;&#31867;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#65288;SMA&#65289;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35745;&#31639;&#23454;&#20307;&#30340;&#21160;&#24577;&#24433;&#21709;&#65292;&#20854;&#29305;&#28857;&#26159;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#20316;&#20026;&#19968;&#31181;&#38761;&#26032;&#20154;&#31867;&#29992;&#25143;&#20132;&#20114;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#21033;&#29992;&#19987;&#38376;&#30340;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#20174;&#25805;&#20316;&#32452;&#32455;&#27969;&#31243;&#21040;&#22522;&#20110;&#24212;&#29992;&#30693;&#35782;&#21644;&#20154;&#30340;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#12290; &#20808;&#21069;&#30340;&#35843;&#26597;&#26174;&#31034;&#65292;&#22312;&#22788;&#29702;&#26032;&#25361;&#25112;&#21644;&#23454;&#29992;&#20219;&#21153;&#65288;&#22914;&#24341;&#21457;&#36923;&#36753;&#25512;&#29702;&#21644;&#38382;&#39064;&#35299;&#20915;&#65289;&#26102;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#24037;&#20195;&#29702;&#30340;&#33258;&#20027;&#26041;&#27861;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290; &#36824;&#32771;&#34385;&#21040;&#65292;&#20256;&#32479;&#25216;&#26415;&#65292;&#22914;&#28608;&#21457;&#24605;&#24819;&#38142;&#65292;&#38656;&#35201;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#23548;&#12290; &#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24320;&#21457;&#30340;&#20195;&#29702;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07769v1 Announce Type: new  Abstract: This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#23545;&#27604;&#22870;&#21169;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#25913;&#21892;&#20102;&#22870;&#21169;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#40723;&#21169;&#23545;&#22522;&#20934;&#30340;&#25913;&#21892;&#65292;&#24182;&#33021;&#26681;&#25454;&#20219;&#21153;&#30340;&#38590;&#24230;&#36827;&#34892;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.07708</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#22870;&#21169;&#25913;&#21892;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07708
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#23545;&#27604;&#22870;&#21169;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#25913;&#21892;&#20102;&#22870;&#21169;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#40723;&#21169;&#23545;&#22522;&#20934;&#30340;&#25913;&#21892;&#65292;&#24182;&#33021;&#26681;&#25454;&#20219;&#21153;&#30340;&#38590;&#24230;&#36827;&#34892;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26159;&#29992;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#28982;&#32780;&#29616;&#26377;&#30340;RLHF&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#26469;&#28304;&#30340;&#22122;&#22768;&#65292;&#20363;&#22914;&#20154;&#31867;&#26631;&#27880;&#38169;&#35823;&#65292;&#20351;&#24471;&#27969;&#31243;&#33030;&#24369;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23545;&#22870;&#21169;&#30340;&#24809;&#32602;&#39033;&#65292;&#21629;&#21517;&#20026;&#8220;&#23545;&#27604;&#22870;&#21169;&#8221;&#65292;&#26469;&#25552;&#39640;&#22870;&#21169;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#31163;&#32447;&#25277;&#26679;&#27493;&#39588;&#65292;&#33719;&#21462;&#29992;&#20316;&#22522;&#20934;&#35745;&#31639;&#30340;&#25552;&#31034;&#21709;&#24212;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;&#22522;&#20934;&#21709;&#24212;&#35745;&#31639;&#23545;&#27604;&#22870;&#21169;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;Proximal Policy Optimization&#65288;PPO&#65289;&#27493;&#39588;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#22870;&#21169;&#20351;&#24471;LLM&#33021;&#22815;&#24809;&#32602;&#22870;&#21169;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#40723;&#21169;&#20248;&#20110;&#22522;&#32447;&#30340;&#25913;&#36827;&#65292;&#26681;&#25454;&#20219;&#21153;&#38590;&#24230;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#19988;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07708v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) is the mainstream paradigm used to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we improve the effectiveness of the reward model by introducing a penalty term on the reward, named as \textit{contrastive rewards}. %Contrastive rewards Our approach involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses and used in the Proximal Policy Optimization (PPO) step. We show that contrastive rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#21333;&#20307;&#36180;&#29575;&#27604;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;ORPO&#65292;&#22312;SFT&#36807;&#31243;&#20013;&#36890;&#36807;&#36731;&#24494;&#24809;&#32602;&#19981;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#39118;&#26684;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;</title><link>https://arxiv.org/abs/2403.07691</link><description>&lt;p&gt;
&#20855;&#26377;&#36180;&#29575;&#27604;&#30340;&#26080;&#21442;&#32771;&#21333;&#20307;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reference-free Monolithic Preference Optimization with Odds Ratio
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#21333;&#20307;&#36180;&#29575;&#27604;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;ORPO&#65292;&#22312;SFT&#36807;&#31243;&#20013;&#36890;&#36807;&#36731;&#24494;&#24809;&#32602;&#19981;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#39118;&#26684;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#35821;&#35328;&#27169;&#22411;&#20559;&#22909;&#23545;&#40784;&#31639;&#27861;&#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#20173;&#28982;&#23545;&#20110;&#25104;&#21151;&#25910;&#25947;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20559;&#22909;&#23545;&#40784;&#30340;&#29615;&#22659;&#20013;SFT&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24378;&#35843;&#23545;&#20110;&#20559;&#22909;&#23545;&#40784;&#30340;SFT&#26469;&#35828;&#65292;&#23545;&#20110;&#19981;&#21463;&#27426;&#36814;&#30340;&#29983;&#25104;&#39118;&#26684;&#26045;&#21152;&#36731;&#24494;&#24809;&#32602;&#23601;&#36275;&#22815;&#20102;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21019;&#26032;&#30340;&#26080;&#21442;&#32771;&#27169;&#22411;&#30340;&#21333;&#20307;&#36180;&#29575;&#27604;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;ORPO&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#20559;&#22909;&#23545;&#40784;&#38454;&#27573;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#25163;&#27573;&#35777;&#26126;&#65292;&#36180;&#29575;&#27604;&#26159;&#22312;125M&#33267;7B&#19981;&#21516;&#35268;&#27169;&#19979;&#36827;&#34892;SFT&#26102;&#23545;&#27604;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#39118;&#26684;&#30340;&#26126;&#26234;&#36873;&#25321;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;ORPO&#22312;&#20165;UltraFeedback&#19978;&#23545;Phi-2&#65288;2.7B&#65289;&#12289;Llama-2&#65288;7B&#65289;&#21644;Mistral&#65288;7B&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#36229;&#36234;&#20102;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07691v1 Announce Type: cross  Abstract: While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30495;&#30456;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#65288;TACS&#65289;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#36827;&#34892;&#30495;&#30456;&#26816;&#27979;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#27880;&#24847;&#21147;&#33945;&#29256;&#26469;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#19981;&#30495;&#23454;&#19978;&#19979;&#25991;&#35823;&#23548;&#20135;&#29983;&#24187;&#35273;</title><link>https://arxiv.org/abs/2403.07556</link><description>&lt;p&gt;
&#30495;&#30456;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#65306;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#19981;&#30495;&#23454;&#19978;&#19979;&#25991;&#35823;&#23548;&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07556
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30495;&#30456;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#65288;TACS&#65289;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#36827;&#34892;&#30495;&#30456;&#26816;&#27979;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#27880;&#24847;&#21147;&#33945;&#29256;&#26469;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#19981;&#30495;&#23454;&#19978;&#19979;&#25991;&#35823;&#23548;&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#29992;&#25143;&#25110;&#30693;&#35782;&#35770;&#35777;&#24037;&#20855;&#25552;&#20379;&#30340;&#19981;&#30495;&#23454;&#19978;&#19979;&#25991;&#35823;&#23548;&#65292;&#20174;&#32780;&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#20943;&#36731;LLMs&#34987;&#19981;&#30495;&#23454;&#20449;&#24687;&#35823;&#23548;&#24182;&#21033;&#29992;&#30693;&#35782;&#35770;&#35777;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30495;&#30456;&#24863;&#30693;&#30340;&#19978;&#19979;&#25991;&#36873;&#25321;&#65288;TACS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#36755;&#20837;&#20013;&#23631;&#34109;&#19981;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#12290;TACS&#39318;&#20808;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#36827;&#34892;&#30495;&#30456;&#26816;&#27979;&#65292;&#21033;&#29992;LLM&#20869;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#26681;&#25454;&#27599;&#20010;&#20301;&#32622;&#30340;&#30495;&#23454;&#24615;&#26500;&#24314;&#30456;&#24212;&#30340;&#27880;&#24847;&#21147;&#33945;&#29256;&#65292;&#36873;&#25321;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#24182;&#20002;&#24323;&#19981;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25200;&#21160;&#36866;&#24212;&#29575;&#65292;&#20197;&#36827;&#19968;&#27493;&#30740;&#31350;LLMs&#25509;&#21463;&#30495;&#23454;&#20449;&#24687;&#21644;&#25269;&#21046;&#19981;&#30495;&#23454;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07556v1 Announce Type: new  Abstract: Although large language models (LLMs) have demonstrated impressive text generation capabilities, they are easily misled by the untruthful context provided by users or knowledge argumentation tools, thereby producing hallucinations. To alleviate the LLMs from being misled by untruthful information and take advantage of knowledge argumentation, we propose Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful context from the inputs. TACS begins by performing truth detection on the input context, leveraging the parameterized knowledge within the LLM. Subsequently, it constructs a corresponding attention mask based on the truthfulness of each position, selecting the truthful context and discarding the untruthful context. Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate, to further study the LLMs' ability to accept truthful information and resist untruthful information. Experimental resul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07311</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;KG-LLM&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#21033;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;NLP&#33539;&#20363;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26694;&#26550;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20998;&#26512;&#39046;&#22495;&#65292;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20869;&#22810;&#20010;&#38142;&#25509;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#25361;&#25112;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#35299;&#20915;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#30693;&#35782;&#22270;&#35889;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;KG-LLM&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20851;&#38190;&#30340;NLP&#33539;&#20363;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22810;&#36339;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;KG&#36716;&#25442;&#20026;CoT&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35782;&#21035;&#24182;&#23398;&#20064;&#23454;&#20307;&#21450;&#20854;&#30456;&#20114;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20026;&#20102;&#23637;&#31034;KG-LLM&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#20869;&#24494;&#35843;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#38750;ICL&#21644;ICL&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#20026;LLMs&#25552;&#20379;&#38646;&#27425;&#23581;&#35797;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#65292;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#22522;&#26412;&#35268;&#21017;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.05101</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rule-driven News Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#65292;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#22522;&#26412;&#35268;&#21017;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
News captioning&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#25551;&#36848;&#22270;&#29255;&#21450;&#20854;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#25110;&#20855;&#20307;&#20107;&#20214;&#26469;&#29983;&#25104;&#21477;&#23376;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20381;&#36182;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#21462;&#24471;&#26174;&#33879;&#25104;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#19987;&#27880;&#20110;&#36755;&#20837;&#26032;&#38395;&#20869;&#23481;&#19982;&#36755;&#20986;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#38656;&#35201;&#36981;&#24490;&#26032;&#38395;&#25253;&#36947;&#30340;&#19968;&#20123;&#22522;&#26412;&#35268;&#21017;&#65292;&#22914;&#20934;&#30830;&#25551;&#36848;&#19982;&#20107;&#20214;&#30456;&#20851;&#30340;&#20010;&#20307;&#21644;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#25351;&#23450;&#30340;&#35268;&#21017;&#20449;&#21495;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#25551;&#36848;&#35774;&#35745;&#20102;&#26032;&#38395;&#24863;&#30693;&#30340;&#35821;&#20041;&#35268;&#21017;&#12290;&#36825;&#19968;&#35268;&#21017;&#21253;&#25324;&#22270;&#29255;&#20013;&#25551;&#32472;&#30340;&#20027;&#35201;&#21160;&#20316;&#65288;&#20363;&#22914;&#65292;&#8220;&#25191;&#34892;&#8221;&#65289;&#20197;&#21450;&#21442;&#19982;&#21160;&#20316;&#30340;&#21629;&#21517;&#23454;&#20307;&#25198;&#28436;&#30340;&#35282;&#33394;&#65288;&#20363;&#22914;&#65292;&#8220;&#20195;&#29702;&#20154;&#8221;&#21644;&#8220;&#22320;&#28857;&#8221;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#35821;&#20041;&#35268;&#21017;&#27880;&#20837;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05101v1 Announce Type: cross  Abstract: News captioning task aims to generate sentences by describing named entities or concrete events for an image with its news article. Existing methods have achieved remarkable results by relying on the large-scale pre-trained models, which primarily focus on the correlations between the input news content and the output predictions. However, the news captioning requires adhering to some fundamental rules of news reporting, such as accurately describing the individuals and actions associated with the event. In this paper, we propose the rule-driven news captioning method, which can generate image descriptions following designated rule signal. Specifically, we first design the news-aware semantic rule for the descriptions. This rule incorporates the primary action depicted in the image (e.g., "performing") and the roles played by named entities involved in the action (e.g., "Agent" and "Place"). Second, we inject this semantic rule into th
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#26041;&#27861;&#65292;&#24110;&#21161;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#65292;&#36890;&#36807;&#26500;&#25104;&#35201;&#32032;&#21644;&#20851;&#38190;&#35789;&#36873;&#25321;&#36827;&#34892;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.04369</link><description>&lt;p&gt;
&#20174;&#22270;&#21040;&#35789;&#34955;: &#23558;&#39046;&#22495;&#30693;&#35782;&#24341;&#20837;&#28151;&#28102;&#32618;&#21517;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04369
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#26041;&#27861;&#65292;&#24110;&#21161;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#65292;&#36890;&#36807;&#26500;&#25104;&#35201;&#32032;&#21644;&#20851;&#38190;&#35789;&#36873;&#25321;&#36827;&#34892;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#32618;&#21517;&#39044;&#27979;&#26159;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#26681;&#25454;&#20107;&#23454;&#25551;&#36848;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#12290;&#29616;&#26377;&#30340;&#32618;&#21517;&#39044;&#27979;&#26041;&#27861;&#22312;&#34920;&#29616;&#19978;&#24050;&#32463;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#22788;&#29702;&#28151;&#28102;&#32618;&#21517;&#65288;&#22914;&#25250;&#22842;&#19982;&#25250;&#21163;&#65289;&#26102;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#27861;&#24459;&#39046;&#22495;&#65292;&#26500;&#25104;&#35201;&#32032;&#22312;&#21306;&#20998;&#28151;&#28102;&#32618;&#21517;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26500;&#25104;&#35201;&#32032;&#26159;&#28508;&#22312;&#21009;&#32602;&#32972;&#21518;&#30340;&#22522;&#26412;&#34892;&#20026;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#32618;&#21517;&#20043;&#38388;&#26377;&#24494;&#22937;&#30340;&#21306;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#65288;FWGB&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26377;&#20851;&#26500;&#25104;&#35201;&#32032;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#25351;&#23548;&#27169;&#22411;&#22312;&#28151;&#28102;&#32618;&#21517;&#19978;&#20570;&#20986;&#21028;&#26029;&#65292;&#31867;&#20284;&#20110;&#27861;&#23448;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26500;&#25104;&#35201;&#32032;&#30340;&#27861;&#24459;&#30693;&#35782;&#22270;&#65292;&#20197;&#24110;&#21161;&#20026;&#27599;&#31181;&#32618;&#21517;&#36873;&#25321;&#20851;&#38190;&#35789;&#65292;&#24418;&#25104;&#19968;&#20010;&#21333;&#35789;&#34955;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04369v1 Announce Type: new  Abstract: Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions. While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery. In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges. Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges. In this paper, we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces domain knowledge regarding constituent elements to guide the model in making judgments on confusing charges, much like a judge's reasoning process. Specifically, we first construct a legal knowledge graph containing constituent elements to help select keywords for each charge, forming a word bag. Subsequently, to guide the mod
&lt;/p&gt;</description></item><item><title>&#21152;&#24378;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.04321</link><description>&lt;p&gt;
&#30952;&#20855;&#25506;&#27979;&#21644;&#35843;&#25972;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Discriminative Probing and Tuning for Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04321
&lt;/p&gt;
&lt;p&gt;
&#21152;&#24378;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20808;&#21069;&#30340;&#26041;&#27861;&#32463;&#24120;&#38754;&#20020;&#25991;&#26412;&#22270;&#20687;&#19981;&#23545;&#40784;&#31561;&#38382;&#39064;&#65292;&#22914;&#29983;&#25104;&#22270;&#20687;&#20013;&#30340;&#20851;&#31995;&#28151;&#28102;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20132;&#21449;&#27880;&#24847;&#21147;&#25805;&#20316;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#32452;&#21512;&#29702;&#35299;&#65292;&#25110;&#32773;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25913;&#36827;&#24067;&#23616;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;T2I&#27169;&#22411;&#30340;&#22266;&#26377;&#23545;&#40784;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#36890;&#36807;&#23457;&#35270;&#29983;&#25104;&#27169;&#22411;&#21644;&#21028;&#21035;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#35748;&#20026;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#21487;&#33021;&#21453;&#26144;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#29087;&#32451;&#24230;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20027;&#24352;&#21152;&#24378;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#31435;&#22312;T2I&#27169;&#22411;&#19978;&#30340;&#21028;&#21035;&#36866;&#37197;&#22120;&#65292;&#20197;&#25506;&#27979;&#23427;&#20204;&#22312;&#20004;&#39033;&#20195;&#34920;&#24615;&#20219;&#21153;&#19978;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#21028;&#21035;&#24494;&#35843;&#26469;&#25913;&#21892;&#23427;&#20204;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04321v1 Announce Type: cross  Abstract: Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#26032;&#38395;&#25688;&#35201;&#20013;&#24187;&#35273;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;absinth&#65292;&#25506;&#35752;&#20102;LLMs&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.03750</link><description>&lt;p&gt;
&#24503;&#35821;&#20063;&#20135;&#29983;&#24187;&#35273;&#65281;&#20351;&#29992;Absinth&#25968;&#25454;&#38598;&#26816;&#27979;&#26032;&#38395;&#25688;&#35201;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#26032;&#38395;&#25688;&#35201;&#20013;&#24187;&#35273;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;absinth&#65292;&#25506;&#35752;&#20102;LLMs&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#20135;&#29983;&#20449;&#24687;&#24187;&#35273;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#22810;&#35821;&#35328;&#26041;&#27861;&#32570;&#20047;&#24503;&#35821;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;absinth&#65292;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#26032;&#38395;&#25688;&#35201;&#20013;&#24187;&#35273;&#26816;&#27979;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#35752;&#20102;&#26032;&#22411;&#24320;&#28304;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#28304;&#24182;&#21457;&#24067;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03750v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has led to remarkable progress on a wide range of natural language processing tasks. Despite the advances, these large-sized models still suffer from hallucinating information in their output, which poses a major issue in automatic text summarization, as we must guarantee that the generated summary is consistent with the content of the source document. Previous research addresses the challenging task of detecting hallucinations in the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries. However, these works primarily focus on English and recent multilingual approaches lack German data. This work presents absinth, a manually annotated dataset for hallucination detection in German news summarization and explores the capabilities of novel open-source LLMs on this task in both fine-tuning and in-context learning settings. We open-source and releas
&lt;/p&gt;</description></item><item><title>VBART&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;BART&#21644;mBART&#27169;&#22411;&#32467;&#21512;&#24418;&#25104;&#20102;&#32039;&#20945;&#22411;LLM&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01308</link><description>&lt;p&gt;
VBART: &#22303;&#32819;&#20854;LLM
&lt;/p&gt;
&lt;p&gt;
VBART: The Turkish LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01308
&lt;/p&gt;
&lt;p&gt;
VBART&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19982;BART&#21644;mBART&#27169;&#22411;&#32467;&#21512;&#24418;&#25104;&#20102;&#32039;&#20945;&#22411;LLM&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VBART&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22303;&#32819;&#20854;&#24207;&#21015;&#21040;&#24207;&#21015;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19968;&#20010;&#22823;&#35821;&#26009;&#24211;&#19978;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;VBART&#26159;&#22522;&#20110;BART&#21644;mBART&#27169;&#22411;&#30340;&#22909;&#24605;&#36335;&#26500;&#24314;&#30340;&#32039;&#20945;&#22411;LLMs&#65292;&#20998;&#20026;Large&#21644;XLarge&#20004;&#20010;&#23610;&#23544;&#12290;&#24494;&#35843;&#21518;&#30340;VBART&#27169;&#22411;&#22312;&#25552;&#21462;&#24615;&#25991;&#26412;&#25688;&#35201;&#12289;&#26631;&#39064;&#29983;&#25104;&#12289;&#25991;&#26412;&#25913;&#20889;&#12289;&#38382;&#31572;&#21644;&#38382;&#39064;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#23427;&#20204;&#20801;&#35768;&#20026;&#26410;&#26469;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#20026;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#25317;&#26377;&#20026;&#22303;&#32819;&#20854;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;LLM&#27604;&#22810;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#20102;&#26368;&#22810;3&#20493;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#24182;&#20026;&#35757;&#32451;&#21644;&#25512;&#29702;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21333;&#35821;&#20998;&#35789;&#22120;&#27604;OpenAI&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#26356;&#39640;&#25928;7&#20493;&#12290;&#26368;&#21518;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#29616;&#26377;&#39044;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01308v1 Announce Type: new  Abstract: We present VBART, the first Turkish sequence-to-sequence Large Language Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact LLMs based on good ideas leveraged from BART and mBART models and come in two sizes, Large and XLarge. Fine-tuned VBART models surpass the prior state-of-the-art results in abstractive text summarization, title generation, text paraphrasing, question answering and question generation tasks. They allow fine-tuning for future text generation tasks and datasets, carving a new path for Turkish Natural Language Processing (NLP) research. Our work shows that having a pre-trained LLM for Turkish outperforms up to 3x multilingual models, improving existing results and providing efficient models for training and inference. Moreover, we show that our monolingual tokenizer is 7x more efficient than OpenAI's multilingual tokenizer. Last but not least, we introduce a method to enlarge an existing pre-trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#32467;&#26524;&#34920;&#26126;RAG&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01193</link><description>&lt;p&gt;
RAGged Edges: Retrieval-Augmented Chatbots&#30340;&#21452;&#20995;&#21073;
&lt;/p&gt;
&lt;p&gt;
RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#32467;&#26524;&#34920;&#26126;RAG&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273; - &#29983;&#25104;&#30475;&#20284;&#27491;&#30830;&#20294;&#38169;&#35823;&#20449;&#24687;&#30340;&#20542;&#21521;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#24456;&#20851;&#38190;&#65292;&#23601;&#20687;&#26368;&#36817;&#30340;&#27861;&#38498;&#26696;&#20363;&#20013;&#30475;&#21040;&#30340;&#37027;&#26679;&#65292;ChatGPT&#30340;&#20351;&#29992;&#23548;&#33268;&#20102;&#19981;&#23384;&#22312;&#30340;&#27861;&#24459;&#35009;&#20915;&#30340;&#24341;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#22806;&#37096;&#30693;&#35782;&#19982;&#25552;&#31034;&#38598;&#25104;&#26469;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26469;&#25269;&#21046;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26088;&#22312;&#35825;&#23548;&#24187;&#35273;&#30340;&#25552;&#31034;&#26469;&#23545;RAG&#19982;&#26631;&#20934;LLMs&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;RAG&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24403;&#25552;&#31034;&#30452;&#25509;&#19982;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#29702;&#35299;&#30456;&#30683;&#30462;&#26102;&#65292;RAG&#20173;&#28982;&#20250;&#34987;&#35823;&#23548;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#24187;&#35273;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RAG&#37096;&#32626;&#30340;&#23454;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01193v1 Announce Type: cross  Abstract: Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical, as seen in recent court cases where ChatGPT's use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and 
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31243;&#24207;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#28041;&#21450;&#22823;&#37327;&#25968;&#23383;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2403.00795</link><description>&lt;p&gt;
&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31639;&#27861;&#65306;&#19968;&#39033;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Executing Natural Language-Described Algorithms with Large Language Models: An Investigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00795
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31243;&#24207;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#28041;&#21450;&#22823;&#37327;&#25968;&#23383;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#36861;&#27714;&#12290;&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20986;&#30340;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#20986;&#29616;&#65292;&#36825;&#19968;&#30446;&#26631;&#30340;&#36947;&#36335;&#24050;&#32463;&#34987;&#38416;&#26126;&#12290;&#26412;&#25991;&#26088;&#22312;&#26816;&#39564;&#29616;&#26377;LLMs&#29702;&#35299;&#21644;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;&#12298;&#31639;&#27861;&#23548;&#35770;&#12299;&#20013;&#36873;&#21462;&#20102;&#19968;&#20010;&#31639;&#27861;&#27979;&#35797;&#38598;&#65292;&#35813;&#20070;&#26159;&#19968;&#26412;&#21253;&#21547;&#35768;&#22810;&#20195;&#34920;&#24615;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#30340;&#30693;&#21517;&#25945;&#26448;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25191;&#34892;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;30&#20010;&#31639;&#27861;&#65292;&#20849;&#29983;&#25104;&#20102;300&#20010;&#38543;&#26426;&#25277;&#26679;&#23454;&#20363;&#65292;&#24182;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;LLMs&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#21644;&#25191;&#34892;&#36825;&#20123;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#29305;&#21035;&#26159;GPT-4&#31561;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31243;&#24207;&#65292;&#21482;&#35201;&#19981;&#28041;&#21450;&#22823;&#37327;&#25968;&#23383;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00795v1 Announce Type: cross  Abstract: Executing computer programs described in natural language has long been a pursuit of computer science. With the advent of enhanced natural language understanding capabilities exhibited by large language models (LLMs), the path toward this goal has been illuminated. In this paper, we seek to examine the capacity of present-day LLMs to comprehend and execute algorithms outlined in natural language. We established an algorithm test set sourced from Introduction to Algorithm, a well-known textbook that contains many representative widely-used algorithms. To systematically assess LLMs' code execution abilities, we selected 30 algorithms, generated 300 random-sampled instances in total, and evaluated whether popular LLMs can understand and execute these algorithms. Our findings reveal that LLMs, notably GPT-4, can effectively execute programs described in natural language, as long as no heavy numeric computation is involved. We believe our f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#20102;&#21306;&#22495;&#36890;&#36135;&#33192;&#32960;&#30340;&#19978;&#21319;&#21644;&#19979;&#38477;&#36235;&#21183;&#65292;&#25506;&#35752;&#20102;&#31038;&#20132;&#32593;&#32476;&#35752;&#35770;&#23545;&#36890;&#36135;&#33192;&#32960;&#39044;&#26399;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.00774</link><description>&lt;p&gt;
&#21033;&#29992;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#36827;&#34892;&#21306;&#22495;&#36890;&#36135;&#33192;&#32960;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regional inflation analysis using social network data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#20102;&#21306;&#22495;&#36890;&#36135;&#33192;&#32960;&#30340;&#19978;&#21319;&#21644;&#19979;&#38477;&#36235;&#21183;&#65292;&#25506;&#35752;&#20102;&#31038;&#20132;&#32593;&#32476;&#35752;&#35770;&#23545;&#36890;&#36135;&#33192;&#32960;&#39044;&#26399;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36135;&#33192;&#32960;&#26159;&#24433;&#21709;&#20219;&#20309;&#22269;&#23478;&#21644;&#22320;&#21306;&#20154;&#21475;&#30340;&#26368;&#37325;&#35201;&#30340;&#23439;&#35266;&#32463;&#27982;&#25351;&#26631;&#20043;&#19968;&#12290;&#36890;&#36135;&#33192;&#32960;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#36890;&#36135;&#33192;&#32960;&#39044;&#26399;&#12290;&#35768;&#22810;&#22830;&#34892;&#22312;&#23454;&#26045;&#20197;&#36890;&#36135;&#33192;&#32960;&#30446;&#26631;&#20026;&#26680;&#24515;&#30340;&#36135;&#24065;&#25919;&#31574;&#26102;&#32771;&#34385;&#21040;&#36825;&#19968;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;Vkontakte&#31038;&#20132;&#32593;&#32476;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;&#28041;&#21450;&#36890;&#36135;&#33192;&#32960;&#19978;&#21319;&#21644;&#19979;&#38477;&#36235;&#21183;&#30340;&#20869;&#23481;&#65288;&#20197;&#37122;&#26408;&#26031;&#20811;&#22320;&#21306;&#20026;&#20363;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00774v1 Announce Type: cross  Abstract: Inflation is one of the most important macroeconomic indicators that have a great impact on the population of any country and region. Inflation is influenced by range of factors, one of which is inflation expectations. Many central banks take this factor into consideration while implementing monetary policy within the inflation targeting regime. Nowadays, a lot of people are active users of the Internet, especially social networks. There is a hypothesis that people search, read, and discuss mainly only those issues that are of particular interest to them. It is logical to assume that the dynamics of prices may also be in the focus of user discussions. So, such discussions could be regarded as an alternative source of more rapid information about inflation expectations. This study is based on unstructured data from Vkontakte social network to analyze upward and downward inflationary trends (on the example of the Omsk region). The sample
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;</title><link>https://arxiv.org/abs/2402.13897</link><description>&lt;p&gt;
&#31185;&#23398;&#26816;&#26597;&#32773;&#20877;&#24230;&#21319;&#32423;&#65306;&#36879;&#26126;&#24230;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#21452;&#21521;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#28982;&#38754;&#20020;&#30528;&#22312;&#31185;&#23398;&#21644;&#24037;&#19994;&#30340;&#28023;&#37327;&#20449;&#24687;&#20013;&#30340;&#35832;&#22810;&#38480;&#21046;&#65292;&#27604;&#22914;&#35821;&#20041;&#20998;&#27495;&#21644;&#26816;&#32034;&#20013;&#30340;&#35789;&#27719;&#24046;&#36317;&#12289;&#35821;&#20041;&#25628;&#32034;&#20013;&#30340;&#20302;&#31934;&#24230;&#21644;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#25110;&#32773;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#30340;&#36825;&#20123;&#38556;&#30861;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#36890;&#36807;&#26597;&#35810;&#25193;&#23637;&#22686;&#24378;&#20102;&#22312;&#31232;&#30095;&#26816;&#32034;&#20013;&#30340;&#35821;&#35328;&#29702;&#35299;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#12290;&#31532;&#20108;&#20010;&#27169;&#22359;&#36890;&#36807;&#21482;&#20351;&#29992;&#38271;&#25991;&#26723;&#20013;&#20256;&#25773;&#30340;&#20449;&#24687;&#65292;&#20026;&#22797;&#26434;&#38382;&#39064;&#25552;&#20379;&#20840;&#38754;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#26469;&#21152;&#28145;&#32467;&#26524;&#65292;&#23454;&#29616;&#21452;&#21521;&#20132;&#20114;&#12290;&#22312;&#31649;&#36947;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21521;&#29992;&#25143;&#21576;&#29616;&#20013;&#38388;&#32467;&#26524;&#20197;&#20419;&#36827;&#23545;&#31995;&#32479;&#25512;&#29702;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#21452;&#21521;&#26041;&#27861;&#24102;&#26469;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13897v1 Announce Type: cross  Abstract: Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#26032;&#30340;QA&#25968;&#25454;&#38598;WiTQA&#65292;&#20197;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#21512;&#30340;&#24433;&#21709;&#20026;&#37325;&#28857;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.13492</link><description>&lt;p&gt;
&#12298;&#26816;&#32034;&#26159;&#26377;&#30410;&#36824;&#26159;&#26377;&#23475;&#65311;&#28145;&#20837;&#25506;&#35752;&#26816;&#32034;&#22686;&#24378;&#23545;&#35821;&#35328;&#27169;&#22411;&#25928;&#26524;&#30340;&#24433;&#21709;&#12299;
&lt;/p&gt;
&lt;p&gt;
Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#26032;&#30340;QA&#25968;&#25454;&#38598;WiTQA&#65292;&#20197;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#21512;&#30340;&#24433;&#21709;&#20026;&#37325;&#28857;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#38656;&#35201;&#26597;&#35810;&#20854;&#39044;&#35757;&#32451;&#35760;&#24518;&#20043;&#22806;&#30340;&#20449;&#24687;&#26102;&#65292;&#23427;&#20204;&#22312;&#25552;&#20379;&#20934;&#30830;&#22238;&#31572;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#12290;&#34429;&#28982;&#21033;&#29992;&#30456;&#20851;&#22806;&#37096;&#20449;&#24687;&#26469;&#22686;&#24378;&#23427;&#20204;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#26410;&#32771;&#34385;&#26816;&#32034;&#30340;&#24517;&#35201;&#24615;&#21487;&#33021;&#20250;&#23545;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#27492;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23454;&#20307;&#22914;&#20309;&#24433;&#21709;&#26816;&#32034;&#27169;&#22411;&#19982;LMs&#20013;&#30340;&#30693;&#35782;&#22238;&#24518;&#65292;&#20854;&#20182;&#26041;&#38754;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#21512;&#30340;&#24433;&#21709;&#26469;&#25552;&#20379;&#26356;&#35814;&#32454;&#12289;&#20197;&#20107;&#23454;&#20026;&#20013;&#24515;&#30340;&#20998;&#26512;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;WiTQA&#65288;Wikipedia Triple Question Answers&#65289;&#30340;&#26032;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#12290;&#27492;&#25968;&#25454;&#38598;&#21253;&#25324;&#20851;&#20110;&#19981;&#21516;&#21463;&#27426;&#36814;&#31243;&#24230;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#38468;&#24102;&#19968;&#27573;&#25903;&#25345;&#24615;&#27573;&#33853;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13492v1 Announce Type: new  Abstract: While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance. Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive ex
&lt;/p&gt;</description></item><item><title>Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2402.12749</link><description>&lt;p&gt;
Me LLaMA: &#20026;&#21307;&#30103;&#24212;&#29992;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Me LLaMA: Foundation Large Language Models for Medical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12749
&lt;/p&gt;
&lt;p&gt;
Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19981;&#22815;&#29702;&#24819;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22312;&#22823;&#22411;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Me LLaMA&#65292;&#19968;&#20010;&#21307;&#23398;LLM&#31995;&#21015;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;- Me LLaMA 13/70B&#21450;&#20854; chat-enhanced &#29256;&#26412;- Me LLaMA 13/70B-chat&#65292;&#36890;&#36807;&#25345;&#32493;&#23545;LLaMA2&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25968;&#25454;&#24320;&#21457;&#32780;&#25104;&#12290;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#22871;&#20214;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;129B tokens&#30340;&#22823;&#35268;&#27169;&#25345;&#32493;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;214k&#20010;&#26679;&#26412;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#36328;&#36234;14&#20010;&#25968;&#25454;&#38598;&#30340;&#20845;&#39033;&#20219;&#21153;&#30340;&#21307;&#23398;&#35780;&#20272;&#22522;&#20934;(MIBE)&#12290;&#25105;&#20204;&#20351;&#29992;MIBE&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#65292;Me LLaMA&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24320;&#28304;&#21307;&#23398;LLMs&#65292;&#24182;&#19988;&#22312;&#21830;&#19994;&#24040;&#22836;&#22914;ChatGPT&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#30340;&#20449;&#23454;&#24615;&#19982;&#21487;&#20449;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#36825;&#20123;&#35299;&#37322;&#22312;&#36923;&#36753;&#19978;&#26159;&#21512;&#20046;&#24773;&#29702;&#19988;&#36830;&#36143;&#30340;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#19982;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#19968;&#33268;&#65292;&#24341;&#21457;&#23545;&#20854;&#20449;&#23454;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2402.04614</link><description>&lt;p&gt;
&#20449;&#23454;&#24615;&#19982;&#21487;&#20449;&#24230;: &#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#30340;(&#19981;)&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#30340;&#20449;&#23454;&#24615;&#19982;&#21487;&#20449;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#36825;&#20123;&#35299;&#37322;&#22312;&#36923;&#36753;&#19978;&#26159;&#21512;&#20046;&#24773;&#29702;&#19988;&#36830;&#36143;&#30340;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#19982;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#19968;&#33268;&#65292;&#24341;&#21457;&#23545;&#20854;&#20449;&#23454;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#37096;&#32626;&#20026;&#20960;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#29616;&#20195;LLMs&#21487;&#20197;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#65288;SEs&#65289;&#65292;&#36825;&#20123;SEs&#25581;&#31034;&#20102;&#23427;&#20204;&#35299;&#37322;&#20854;&#34892;&#20026;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#30001;&#20110;&#20854;&#23545;&#35805;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#29305;&#28857;&#65292;&#33258;&#25105;&#35299;&#37322;&#24050;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#20449;&#23454;&#24615;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLMs&#29983;&#25104;&#30340;SEs&#20013;&#20449;&#23454;&#24615;&#21644;&#21487;&#20449;&#24230;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;LLMs&#25797;&#38271;&#29983;&#25104;&#21487;&#20449;&#30340;&#35299;&#37322;-&#23545;&#20154;&#31867;&#29992;&#25143;&#26469;&#35828;&#20284;&#20046;&#36923;&#36753;&#21644;&#36830;&#36143;-&#20294;&#36825;&#20123;&#35299;&#37322;&#26410;&#24517;&#19982;LLMs&#30340;&#25512;&#29702;&#36807;&#31243;&#30456;&#19968;&#33268;&#65292;&#24341;&#21457;&#23545;&#20854;&#20449;&#23454;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#24403;&#21069;&#36235;&#21183;&#26159;&#20026;&#20102;&#29992;&#25143;&#21451;&#22909;&#30028;&#38754;&#30340;&#38656;&#27714;&#32780;&#22686;&#21152;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#21487;&#33021;&#20250;&#20197;&#38477;&#20302;&#35299;&#37322;&#30340;&#20449;&#23454;&#24615;&#20026;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness
&lt;/p&gt;</description></item><item><title>&#31038;&#20250;&#31185;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#25429;&#25417;&#35821;&#20041;&#21644;&#38544;&#21547;&#30340;&#35821;&#29992;&#20449;&#24687;&#65292;&#25351;&#23548;&#35843;&#20248;&#27169;&#22411;Socialite-Llama&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.01980</link><description>&lt;p&gt;
SOCIALITE-LLAMA&#65306;&#31038;&#20250;&#31185;&#23398;&#20219;&#21153;&#30340;&#25351;&#23548;&#35843;&#20248;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01980
&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#31185;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#25429;&#25417;&#35821;&#20041;&#21644;&#38544;&#21547;&#30340;&#35821;&#29992;&#20449;&#24687;&#65292;&#25351;&#23548;&#35843;&#20248;&#27169;&#22411;Socialite-Llama&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#31185;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22914;&#24773;&#32490;&#25110;&#24189;&#40664;&#26816;&#27979;&#65292;&#38656;&#35201;&#20174;&#25991;&#26412;&#20013;&#25429;&#25417;&#35821;&#20041;&#21644;&#38544;&#21547;&#30340;&#35821;&#29992;&#20449;&#24687;&#65292;&#36890;&#24120;&#21482;&#26377;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25351;&#23548;&#35843;&#20248;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35768;&#22810;&#33021;&#21147;&#65292;&#20363;&#22914;&#24120;&#35782;&#25512;&#29702;&#12289;&#38405;&#35835;&#29702;&#35299;&#21644;&#35745;&#31639;&#26426;&#32534;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#25429;&#25417;&#38544;&#21547;&#30340;&#35821;&#29992;&#32447;&#32034;&#30340;&#31038;&#20132;&#39046;&#22495;&#65292;&#23545;&#25351;&#23548;&#35843;&#20248;&#30340;&#25928;&#26524;&#20102;&#35299;&#29978;&#23569;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#25351;&#23548;&#35843;&#20248;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102; Socialite-Llama - &#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340; Llama &#27169;&#22411;&#12290;&#22312;&#19968;&#22871;&#21253;&#21547;20&#20010;&#31038;&#20250;&#31185;&#23398;&#20219;&#21153;&#30340;&#27979;&#35797;&#20013;&#65292;Socialite-Llama &#22312;&#24615;&#33021;&#19978;&#20248;&#20110; Llama&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#20219;&#21153;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#25110;&#24471;&#21040;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#19982; Llama &#30456;&#27604;&#65292;Socialite-Llama &#22312;6&#20010;&#30456;&#20851;&#30340;&#31038;&#20250;&#20219;&#21153;&#20013;&#30340;5&#20010;&#20219;&#21153;&#19978;&#20063;&#26377;&#25152;&#25913;&#36827;&#65292;&#36825;&#34920;&#26126;&#25351;&#23548;&#35843;&#20248;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#20063;&#20250;&#24102;&#26469;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social science NLP tasks, such as emotion or humor detection, are required to capture the semantics along with the implicit pragmatics from text, often with limited amounts of training data. Instruction tuning has been shown to improve the many capabilities of large language models (LLMs) such as commonsense reasoning, reading comprehension, and computer programming. However, little is known about the effectiveness of instruction tuning on the social domain where implicit pragmatic cues are often needed to be captured. We explore the use of instruction tuning for social science NLP tasks and introduce Socialite-Llama -- an open-source, instruction-tuned Llama. On a suite of 20 social science tasks, Socialite-Llama improves upon the performance of Llama as well as matches or improves upon the performance of a state-of-the-art, multi-task finetuned model on a majority of them. Further, Socialite-Llama also leads to improvement on 5 out of 6 related social tasks as compared to Llama, sugg
&lt;/p&gt;</description></item><item><title>&#22810;Agent&#36777;&#35770;&#65288;MAD&#65289;&#20316;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30495;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#23545;&#20110;&#35299;&#20915;&#30830;&#20445;&#29983;&#25104;&#20195;&#29702;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#31572;&#26696;&#30340;&#25361;&#25112;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#24418;&#24335;&#19979;&#30340;&#22810;Agent&#36777;&#35770;&#31995;&#32479;&#22312;&#21487;&#38752;&#24615;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2311.17371</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#30127;&#29378;&#21527;&#65311;&#22810;Agent&#36777;&#35770;&#31574;&#30053;&#23545;LLMs&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17371
&lt;/p&gt;
&lt;p&gt;
&#22810;Agent&#36777;&#35770;&#65288;MAD&#65289;&#20316;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30495;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#23545;&#20110;&#35299;&#20915;&#30830;&#20445;&#29983;&#25104;&#20195;&#29702;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#31572;&#26696;&#30340;&#25361;&#25112;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#24418;&#24335;&#19979;&#30340;&#22810;Agent&#36777;&#35770;&#31995;&#32479;&#22312;&#21487;&#38752;&#24615;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#22238;&#31572;&#21508;&#31181;&#39046;&#22495;&#38382;&#39064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#29983;&#25104;&#20195;&#29702;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#30340;&#31572;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22810;Agent&#36777;&#35770;&#65288;MAD&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;LLMs&#30495;&#23454;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#36777;&#35770;&#21644;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#35752;&#25104;&#26412;&#12289;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#30446;&#21069;&#24418;&#24335;&#19979;&#30340;&#22810;Agent&#36777;&#35770;&#31995;&#32479;&#22312;&#21487;&#38752;&#24615;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#20854;&#20182;&#24314;&#35758;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#22914;&#33258;&#19968;&#33268;&#24615;&#21644;&#20351;&#29992;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#36827;&#34892;&#38598;&#25104;&#12290;&#20294;&#26159;&#65292;&#22312;&#25191;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#26102;&#65292;&#19968;&#20123;MAD&#31995;&#32479;&#65292;&#22914;Multi-Persona&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#34920;&#26126;MAD&#21327;&#35758;&#21487;&#33021;&#24182;&#19981;&#20250;&#27604;&#20854;&#20182;&#26041;&#27861;&#22825;&#28982;&#26356;&#24046;&#65292;&#32780;&#26159;&#26356;&#23481;&#26131;&#21463;&#21040;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17371v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models (LLMs) underscore their potential for responding to inquiries in various domains. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a promising strategy for enhancing the truthfulness of LLMs. We benchmark a range of debating and prompting strategies to explore the trade-offs between cost, time, and accuracy. Importantly, we find that multi-agent debating systems, in their current form, do not reliably outperform other proposed prompting strategies, such as self-consistency and ensembling using multiple reasoning paths. However, when performing hyperparameter tuning, several MAD systems, such as Multi-Persona, perform better. This suggests that MAD protocols might not be inherently worse than other approaches, but that they are more sensitive to different hyperparameter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#20027;&#39064;&#31561;&#22810;&#31181;&#27169;&#24577;&#23545;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#32467;&#21512;&#25552;&#31034;&#20197;&#36817;&#20284;&#34920;&#31034;&#22270;&#30340;&#20840;&#23616;&#36830;&#25509;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#22797;&#26434;&#22270;&#32467;&#26500;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.09862</link><description>&lt;p&gt;
&#25105;&#24212;&#35813;&#20351;&#29992;&#21738;&#31181;&#27169;&#24577;--&#25991;&#26412;&#12289;&#20027;&#39064;&#25110;&#22270;&#20687;&#65311;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
Which Modality should I use -- Text, Motif, or Image? : Understanding Graphs with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#20027;&#39064;&#31561;&#22810;&#31181;&#27169;&#24577;&#23545;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#32467;&#21512;&#25552;&#31034;&#20197;&#36817;&#20284;&#34920;&#31034;&#22270;&#30340;&#20840;&#23616;&#36830;&#25509;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#22797;&#26434;&#22270;&#32467;&#26500;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#22270;&#25968;&#25454;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#21033;&#29992;&#22823;&#37327;&#25991;&#26412;&#35821;&#26009;&#24211;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#19978;&#19979;&#25991;&#22823;&#23567;&#38480;&#21046;&#65292;&#38754;&#20020;&#30528;&#22312;&#23545;&#25972;&#20010;&#22270;&#36827;&#34892;&#32534;&#30721;&#26102;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#20027;&#39064;&#31561;&#22810;&#26679;&#30340;&#27169;&#24577;&#23545;&#22270;&#36827;&#34892;&#32534;&#30721;&#65292;&#32467;&#21512;&#25552;&#31034;&#20197;&#36817;&#20284;&#34920;&#31034;&#19968;&#20010;&#22270;&#30340;&#20840;&#23616;&#36830;&#36890;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs&#22312;&#22788;&#29702;&#22797;&#26434;&#22270;&#32467;&#26500;&#26102;&#30340;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;GraphTMI&#65292;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22270;&#32467;&#26500;&#20998;&#26512;&#20013;&#30340;&#26032;&#39062;&#22522;&#20934;&#65292;&#37325;&#28857;&#20851;&#27880;&#21516;&#36136;&#24615;&#12289;&#20027;&#39064;&#23384;&#22312;&#21644;&#22270;&#38590;&#24230;&#12290;&#20851;&#38190;&#21457;&#29616;&#34920;&#26126;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;GPT-4V&#30340;&#22270;&#20687;&#27169;&#24577;&#65292;&#27604;&#25991;&#26412;&#22312;&#24179;&#34913;&#26631;&#35760;&#38480;&#21046;&#21644;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#26041;&#38754;&#26356;&#20248;&#65292;&#32988;&#36807;&#20808;&#21069;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32534;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31181;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;Pe
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09862v2 Announce Type: replace  Abstract: Our research integrates graph data with Large Language Models (LLMs), which, despite their advancements in various fields using large text corpora, face limitations in encoding entire graphs due to context size constraints. This paper introduces a new approach to encoding a graph with diverse modalities, such as text, image, and motif, coupled with prompts to approximate a graph's global connectivity, thereby enhancing LLMs' efficiency in processing complex graph structures. The study also presents GraphTMI, a novel benchmark for evaluating LLMs in graph structure analysis, focusing on homophily, motif presence, and graph difficulty. Key findings indicate that the image modality, especially with vision-language models like GPT-4V, is superior to text in balancing token limits and preserving essential information and outperforms prior graph neural net (GNN) encoders. Furthermore, the research assesses how various factors affect the pe
&lt;/p&gt;</description></item><item><title>BeLLM&#25552;&#20986;&#20102;&#22686;&#24378;&#21453;&#21521;&#20381;&#36182;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#26174;&#24335;&#30340;&#21453;&#21521;&#20381;&#36182;&#24615;&#65292;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#27979;&#37327;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.05296</link><description>&lt;p&gt;
BeLLM&#65306;&#22686;&#24378;&#21453;&#21521;&#20381;&#36182;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BeLLM: Backward Dependency Enhanced Large Language Model for Sentence Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05296
&lt;/p&gt;
&lt;p&gt;
BeLLM&#25552;&#20986;&#20102;&#22686;&#24378;&#21453;&#21521;&#20381;&#36182;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#26174;&#24335;&#30340;&#21453;&#21521;&#20381;&#36182;&#24615;&#65292;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#27979;&#37327;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#22312;&#34913;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#12290;&#29616;&#26377;&#30340;LLMs&#20027;&#35201;&#37319;&#29992;&#33258;&#22238;&#24402;&#26550;&#26500;&#65292;&#27809;&#26377;&#26126;&#30830;&#24314;&#27169;&#21453;&#21521;&#20381;&#36182;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#20013;&#21453;&#21521;&#20381;&#36182;&#24615;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#27979;&#37327;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#65306;&#22686;&#24378;&#21453;&#21521;&#20381;&#36182;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;BeLLM&#65289;&#12290;&#23427;&#36890;&#36807;&#23558;&#29305;&#23450;&#27880;&#24847;&#21147;&#23618;&#20174;&#21333;&#21521;&#36716;&#25442;&#20026;&#21452;&#21521;&#26469;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#21644;&#19979;&#28216;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;BeLLM&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#22238;&#24402;LLMs&#21463;&#30410;&#20110;&#21453;&#21521;&#20381;&#36182;&#24615;&#20197;&#29992;&#20110;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05296v2 Announce Type: replace  Abstract: Sentence embeddings are crucial in measuring semantic similarity. Most recent studies employed large language models (LLMs) to learn sentence embeddings. Existing LLMs mainly adopted autoregressive architecture without explicit backward dependency modeling. Therefore, we examined the effects of backward dependencies in LLMs for semantic similarity measurements. Concretely, we propose a novel model: backward dependency enhanced large language model (BeLLM). It learns sentence embeddings via transforming specific attention layers from uni- to bi-directional. We extensively experiment across various semantic textual similarity (STS) tasks and downstream applications. BeLLM achieves state-of-the-art performance in varying scenarios. It shows that auto-regressive LLMs benefit from backward dependencies for sentence embeddings.
&lt;/p&gt;</description></item><item><title>LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.19791</link><description>&lt;p&gt;
LILO&#65306;&#36890;&#36807;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#23398;&#20064;&#21487;&#35299;&#37322;&#24211;
&lt;/p&gt;
&lt;p&gt;
LILO: Learning Interpretable Libraries by Compressing and Documenting Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19791
&lt;/p&gt;
&lt;p&gt;
LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;&#37325;&#26500;&#30340;&#33402;&#26415;&#65306;&#23558;&#20195;&#30721;&#25972;&#21512;&#21040;&#21487;&#37325;&#29992;&#21644;&#21487;&#35835;&#30340;&#31243;&#24207;&#24211;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LILO&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#24211;&#12290;LILO&#23558;LLM&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#19982;Stitch&#33258;&#21160;&#37325;&#26500;&#30340;&#36817;&#26399;&#31639;&#27861;&#36827;&#23637;&#30456;&#32467;&#21512;&#65306;Stitch&#26159;&#19968;&#20010;&#31526;&#21495;&#21387;&#32553;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#22823;&#22411;&#20195;&#30721;&#35821;&#26009;&#24211;&#20013;&#30340;&#26368;&#20339;lambda&#25277;&#35937;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#25277;&#35937;&#21487;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#25991;&#26723;&#65288;AutoDoc&#65289;&#36807;&#31243;&#65292;&#23427;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#25512;&#26029;&#20986;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#21644;&#25991;&#26723;&#23383;&#31526;&#20018;&#12290;&#38500;&#20102;&#25552;&#39640;&#20154;&#31867;&#21487;&#35835;&#24615;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;AutoDoc&#36890;&#36807;&#24110;&#21161;LILO&#30340;&#21512;&#25104;&#22120;&#35299;&#37322;&#21644;&#37096;&#32626;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LILO&#36827;&#34892;&#20102;&#19977;&#20010;&#24402;&#32435;&#24335;&#31243;&#24207;&#32508;&#21512;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
&lt;/p&gt;</description></item><item><title>XAL&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#20998;&#31867;&#22120;&#25552;&#20379;&#25512;&#26029;&#30340;&#29702;&#30001;&#24182;&#28145;&#20837;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2310.05502</link><description>&lt;p&gt;
XAL&#65306;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#25552;&#21319;&#20102;&#20302;&#36164;&#28304;&#23398;&#20064;&#32773;&#30340;&#20998;&#31867;&#22120;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05502
&lt;/p&gt;
&lt;p&gt;
XAL&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#20998;&#31867;&#22120;&#25552;&#20379;&#25512;&#26029;&#30340;&#29702;&#30001;&#24182;&#28145;&#20837;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#22320;&#31579;&#36873;&#26368;&#20855;&#24418;&#25104;&#24615;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#35757;&#32451;&#38598;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20302;&#36164;&#28304;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#20998;&#31867;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25110;&#20998;&#27495;&#26469;&#36873;&#25321;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20250;&#20986;&#29616;&#23545;&#34920;&#38754;&#27169;&#24335;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#21463;&#21040;&#20154;&#31867;&#26681;&#25454;&#22240;&#26524;&#20449;&#24687;&#25512;&#26029;&#21644;&#39044;&#27979;&#30340;&#35748;&#30693;&#36807;&#31243;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#23558;&#29702;&#30001;&#34701;&#20837;AL&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65288;XAL&#65289;&#65292;&#26088;&#22312;&#40723;&#21169;&#20998;&#31867;&#22120;&#35777;&#26126;&#20854;&#25512;&#26029;&#24182;&#28145;&#20837;&#30740;&#31350;&#26080;&#27861;&#25552;&#20379;&#21512;&#29702;&#35299;&#37322;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38500;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#36827;&#34892;&#20998;&#31867;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#21521;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05502v2 Announce Type: replace  Abstract: Active learning (AL), which aims to construct an effective training set by iteratively curating the most formative unlabeled data for annotation, has been widely used in low-resource tasks. Most active learning techniques in classification rely on the model's uncertainty or disagreement to choose unlabeled data, suffering from the problem of over-confidence in superficial patterns and a lack of exploration. Inspired by the cognitive processes in which humans deduce and predict through causal information, we take an initial attempt towards integrating rationales into AL and propose a novel Explainable Active Learning framework (XAL) for low-resource text classification, which aims to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations. Specifically, besides using a pre-trained bi-directional encoder for classification, we employ a pre-trained uni-directi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#65292;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.05116</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#25552;&#21319;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Utilizing Contextual Clues and Role Correlations for Enhancing Document-level Event Argument Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#65292;&#25552;&#21319;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65288;EAE&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#20013;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#20219;&#21153;&#20043;&#19968;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#35770;&#35777;&#21644;&#20107;&#20214;&#35302;&#21457;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;&#19978;&#19979;&#25991;&#32447;&#32034;&#30340;&#20449;&#24687;&#21644;&#35770;&#35777;&#35282;&#33394;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CARLG&#27169;&#22411;&#65292;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#19978;&#19979;&#25991;&#32447;&#32034;&#32858;&#21512;&#65288;CCA&#65289;&#21644;&#22522;&#20110;&#35282;&#33394;&#30340;&#28508;&#22312;&#20449;&#24687;&#24341;&#23548;&#65288;RLIG&#65289;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#19978;&#19979;&#25991;&#32447;&#32034;&#21644;&#35282;&#33394;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#25991;&#26723;&#32423;EAE&#12290;CCA&#27169;&#22359;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#19978;&#19979;&#25991;&#27880;&#24847;&#26435;&#37325;&#65292;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#21644;&#25972;&#21512;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;RLIG&#27169;&#22359;&#36890;&#36807;&#35282;&#33394;&#20132;&#20114;&#32534;&#30721;&#25429;&#25417;&#35821;&#20041;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#28508;&#22312;&#35282;&#33394;&#34920;&#31034;&#25552;&#20379;&#23453;&#36149;&#30340;&#20449;&#24687;&#24341;&#23548;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;CCA&#21644;RLIG&#27169;&#22359;&#32039;&#20945;&#12289;&#21487;&#31227;&#26893;&#19988;&#39640;&#25928;&#65292;&#24341;&#20837;&#30340;&#26032;&#21442;&#25968;&#19981;&#36229;&#36807;1%&#65292;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level event argument extraction (EAE) is a vital but challenging subtask in information extraction. Most existing approaches focus on the interaction between arguments and event triggers, ignoring two critical points: the information of contextual clues and the semantic correlations among argument roles. In this paper, we propose the CARLG model, which consists of two modules: Contextual Clues Aggregation (CCA) and Role-based Latent Information Guidance (RLIG), effectively leveraging contextual clues and role correlations for improving document-level EAE. The CCA module adaptively captures and integrates contextual clues by utilizing context attention weights from a pre-trained encoder. The RLIG module captures semantic correlations through role-interactive encoding and provides valuable information guidance with latent role representation. Notably, our CCA and RLIG modules are compact, transplantable and efficient, which introduce no more than 1% new parameters and can be eas
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26242;&#20572;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#26631;&#35760;&#21069;&#22788;&#29702;&#26356;&#22810;&#38544;&#34255;&#21521;&#37327;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;</title><link>https://arxiv.org/abs/2310.02226</link><description>&lt;p&gt;
&#35880;&#35328;&#24910;&#34892;&#65306;&#20351;&#29992;&#26242;&#20572;&#26631;&#35760;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Think before you speak: Training Language Models With Pause Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02226
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26242;&#20572;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#26631;&#35760;&#21069;&#22788;&#29702;&#26356;&#22810;&#38544;&#34255;&#21521;&#37327;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#31435;&#21363;&#36830;&#32493;&#29983;&#25104;&#19968;&#31995;&#21015;&#26631;&#35760;&#26469;&#29983;&#25104;&#21709;&#24212;: &#31532;$(K+1)^{th}$&#20010;&#26631;&#35760;&#26159;&#36890;&#36807;&#25805;&#20316;&#27599;&#23618;&#30340;$K$&#20010;&#38544;&#34255;&#21521;&#37327;&#24471;&#21040;&#30340;&#65292;&#27599;&#20010;&#21521;&#37327;&#23545;&#24212;&#19968;&#20010;&#21069;&#38754;&#30340;&#26631;&#35760;&#12290;&#22914;&#26524;&#25105;&#20204;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#31532;$(K+1)^{th}$&#20010;&#26631;&#35760;&#20043;&#21069;&#25805;&#20316;&#26356;&#22810;&#30340;&#38544;&#34255;&#21521;&#37327;&#65292;&#27604;&#22914;&#35828;$K+10$&#20010;&#21602;&#65311;&#25105;&#20204;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#65288;&#21487;&#23398;&#20064;&#30340;&#65289;$\textit{pause}$&#26631;&#35760;&#65292;&#36825;&#19968;&#31995;&#21015;&#26631;&#35760;&#38468;&#21152;&#21040;&#36755;&#20837;&#21069;&#32512;&#19978;&#12290;&#28982;&#21518;&#25105;&#20204;&#24310;&#36831;&#25552;&#21462;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#30452;&#21040;&#26368;&#21518;&#19968;&#20010;&#26242;&#20572;&#26631;&#35760;&#34987;&#30475;&#21040;&#65292;&#20174;&#32780;&#20801;&#35768;&#27169;&#22411;&#22312;&#20570;&#20986;&#31572;&#26696;&#20043;&#21069;&#36827;&#34892;&#39069;&#22806;&#30340;&#35745;&#31639;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#25317;&#26377;1B&#21644;130M&#21442;&#25968;&#30340;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;$\textit{pause-training}$&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#22312;C4&#19978;&#36827;&#34892;&#20102;&#22240;&#26524;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#28085;&#30422;&#25512;&#29702;&#12289;&#38382;&#31572;&#12289;&#26222;&#36941;&#29702;&#35299;&#21644;&#20107;&#23454;&#22238;&#24518;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;infer
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02226v2 Announce Type: replace-cross  Abstract: Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that infer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#21512;&#24182;&#19987;&#23478;&#20449;&#24687;&#26469;&#21046;&#23450;&#20986;&#26356;&#32039;&#20945;&#20294;&#26356;&#20855;&#30693;&#35782;&#30340;SMoE&#27169;&#22411;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;SMoE&#30340;&#19987;&#23478;&#21512;&#24182;&#12290;</title><link>https://arxiv.org/abs/2310.01334</link><description>&lt;p&gt;
&#21512;&#24182;&#65292;&#28982;&#21518;&#21387;&#32553;&#65306;&#20174;&#20854;&#36335;&#30001;&#31574;&#30053;&#20013;&#25581;&#31034;&#39640;&#25928;&#30340;SMoE&#25216;&#26415;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#21512;&#24182;&#19987;&#23478;&#20449;&#24687;&#26469;&#21046;&#23450;&#20986;&#26356;&#32039;&#20945;&#20294;&#26356;&#20855;&#30693;&#35782;&#30340;SMoE&#27169;&#22411;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;SMoE&#30340;&#19987;&#23478;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;SMoE&#65289;&#26174;&#31034;&#20986;&#25193;&#23637;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#35832;&#22914;&#65288;a&#65289;&#39640;&#20869;&#23384;&#20351;&#29992;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#32593;&#32476;&#23618;&#30340;&#37325;&#22797;&#25104;&#20026;&#22810;&#20010;&#19987;&#23478;&#30340;&#21103;&#26412;&#65307;&#20197;&#21450;&#65288;b&#65289;&#19987;&#23478;&#20013;&#30340;&#20887;&#20313;&#65292;&#22240;&#20026;&#24120;&#35268;&#22522;&#20110;&#23398;&#20064;&#30340;&#36335;&#30001;&#31574;&#30053;&#23481;&#26131;&#20986;&#29616;&#34920;&#31034;&#24615;&#23849;&#28291;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;SMoE&#27169;&#22411;&#22312;&#20869;&#23384;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#65292;&#23588;&#20854;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#19979;&#28216;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#36890;&#36807;&#21512;&#24182;&#19987;&#23478;&#20449;&#24687;&#26469;&#21046;&#23450;&#19968;&#20010;&#32039;&#20945;&#30340;SMoE&#27169;&#22411;&#65311;&#22914;&#20309;&#23558;&#22810;&#20010;&#19987;&#23478;&#21512;&#24182;&#20026;&#26356;&#23569;&#20294;&#26356;&#26377;&#30693;&#35782;&#30340;&#19987;&#23478;&#30340;&#26368;&#20339;&#26041;&#27861;&#65311;&#25105;&#20204;&#30340;&#21021;&#27493;&#35843;&#26597;&#26174;&#31034;&#65292;&#20256;&#32479;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#23545;&#20110;SMoE&#30340;&#19987;&#23478;&#21512;&#24182;&#24182;&#19981;&#26377;&#25928;&#12290;&#28508;&#22312;&#21407;&#22240;&#26159;&#65306;&#65288;1&#65289;&#20887;&#20313;&#20449;&#24687;&#25513;&#30422;&#20102;&#20851;&#38190;&#19987;&#23478;&#65307;&#65288;2&#65289;&#20026;&#27599;&#20010;&#19987;&#23478;&#36873;&#25321;&#36866;&#24403;&#30340;&#31070;&#32463;&#20803;&#25490;&#21015;&#26041;&#24335;&#20250;&#20002;&#22833;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01334v2 Announce Type: replace-cross  Abstract: Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like (a) High Memory Usage, due to duplication of the network layers into multiple copies as experts; and (b) Redundancy in Experts, as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: (1) redundant information overshadows critical experts; (2) appropriate neuron permutation for each expert is miss
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2305.13068</link><description>&lt;p&gt;
&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#24037;&#20855;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Making Language Models Better Tool Learners with Execution Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#20316;&#20026;&#20851;&#38190;&#30340;&#30028;&#38754;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#29702;&#35299;&#21644;&#25913;&#21464;&#29615;&#22659;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#24037;&#20855;&#25193;&#23637;&#20854;&#33021;&#21147;&#24182;&#19982;&#30495;&#23454;&#19990;&#30028;&#20114;&#21160;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#36890;&#24120;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#21152;&#36873;&#25321;&#22320;&#21033;&#29992;&#24037;&#20855;&#65292;&#22240;&#20026;&#22797;&#26434;&#20219;&#21153;&#24448;&#24448;&#36229;&#20986;&#20102;&#23427;&#20204;&#33258;&#36523;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#31616;&#21333;&#20219;&#21153;&#24341;&#20837;&#24037;&#20855;&#65288;&#27169;&#22411;&#26412;&#36523;&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#30340;&#20219;&#21153;&#65289;&#65292;&#21487;&#33021;&#20250;&#26080;&#24847;&#38388;&#20256;&#25773;&#38169;&#35823;&#32780;&#19981;&#26159;&#25552;&#39640;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21542;&#25945;&#20250;&#35821;&#35328;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#24037;&#20855;&#65311;&#20026;&#28385;&#36275;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tool leaRning wIth exeCution fEedback (TRICE)&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#24037;&#20855;&#25191;&#34892;&#20013;&#24471;&#21040;&#30340;&#21453;&#39304;&#19981;&#26029;&#23398;&#20064;&#65292;&#20174;&#32780;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26497;&#31471;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36719;&#26631;&#31614;&#21407;&#22411;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#22312;&#22823;&#22411;&#12289;&#39640;&#32500;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2210.17437</link><description>&lt;p&gt;
&#29992;&#36719;&#26631;&#31614;&#21407;&#22411;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Learning New Tasks from a Few Examples with Soft-Label Prototypes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.17437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26497;&#31471;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36719;&#26631;&#31614;&#21407;&#22411;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#22312;&#22823;&#22411;&#12289;&#39640;&#32500;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#23545;&#20854;&#24494;&#35843;&#65292;&#20197;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#8220;&#26497;&#31471;&#8221;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#27169;&#22411;&#21482;&#38656;&#25509;&#35302;&#27599;&#20010;&#31867;&#21035;&#33267;&#23569;4&#20010;&#31034;&#20363;&#65292;&#36825;&#20123;&#31034;&#20363;&#22522;&#20110;&#36719;&#26631;&#31614;&#21407;&#22411;&#65292;&#36825;&#20123;&#36719;&#26631;&#31614;&#21407;&#22411;&#20849;&#21516;&#25429;&#33719;&#20102;&#36755;&#20837;&#22495;&#31354;&#38388;&#20013;&#19981;&#21516;&#31867;&#21035;&#30340;&#20998;&#24067;&#12290;&#21463;&#21040;&#20808;&#21069;&#20851;&#20110;&#19968;&#20803;&#25110;&#31616;&#21333;&#22810;&#20803;&#65288;&#21512;&#25104;&#65289;&#25968;&#25454;&#65288;Sucholutsky&#31561;&#20154;&#65292;2021&#65289;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#12289;&#39640;&#32500;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#31070;&#32463;&#26694;&#26550;&#65288;DeepSLP&#65289;&#20013;&#23398;&#20064;&#36719;&#26631;&#31614;&#21407;&#22411;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#65292;&#23427;&#22312;31/48&#20010;&#27979;&#35797;&#20219;&#21153;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#19982;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20174;v&#20013;&#23398;&#20064;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;NLP&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.17437v3 Announce Type: replace-cross  Abstract: Existing approaches to few-shot learning in NLP rely on large language models and fine-tuning of these to generalise on out-of-distribution data. In this work, we propose a simple yet powerful approach to "extreme" few-shot learning, wherein models are exposed to as little as 4 examples per class, based on soft-label prototypes that collectively capture the distribution of different classes across the input domain space. Inspired by previous work (Sucholutsky et al., 2021) on univariate or simple multivariate (synthetic) data, we propose a novel approach that is effective on large, high-dimensional and real-world datasets. We learn soft-label prototypes within a neural framework (DeepSLP) and we experimentally demonstrate that it achieves superior performance on 31/48 tested tasks and few-shot settings while closely matching the performance of strong baselines on the rest. We focus on learning previously unseen NLP tasks from v
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2207.14000</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#19978;&#30340;&#22810;&#27493;&#28436;&#32462;&#25512;&#29702;&#65306;&#22522;&#20110;&#36229;&#39046;&#22495;&#27867;&#21270;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.14000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#36923;&#36753;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#24182;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21463;DeepLogic&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#29992;&#20110;&#25191;&#34892;&#36923;&#36753;&#31243;&#24207;&#25512;&#29702;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IMA-GloVe-GA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25512;&#29702;&#26159;&#20351;&#29992;&#22522;&#20110;RNN&#30340;&#36845;&#20195;&#20869;&#23384;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38376;&#20851;&#27880;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;PARARULES&#12289;CONCEPTRULES V1&#21644;CONCEPTRULES V2&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;IMA-GloVe-GA&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#38376;&#20851;&#27880;&#26426;&#21046;&#30340;DeepLogic&#27604;DeepLogic&#21644;&#20854;&#20182;RNN&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35268;&#21017;&#34987;&#25171;&#20081;&#26102;&#27604;RoBERTa-Large&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36229;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#22810;&#27493;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#25512;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15127</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness. (arXiv:2401.15127v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#20851;&#20110;&#26032;&#20852;&#23041;&#32961;&#30340;&#30693;&#35782;&#20849;&#20139;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#26500;&#25104;&#20102;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#26426;&#36935;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#12289;GPT4all&#12289;Dolly&#12289;Stanford Alpaca&#12289;Alpaca-LoRA&#21644;Falcon&#31561;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35782;&#21035;&#24320;&#28304;&#24773;&#25253;&#20013;&#19982;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20316;&#20026;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20174;Twitter&#25910;&#38598;&#30340;&#32463;&#36807;&#20805;&#20998;&#39564;&#35777;&#30340;&#25968;&#25454;&#65292;&#35813;&#25968;&#25454;&#26469;&#28304;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#26041;&#38754;&#65292;&#21830;&#19994;&#27169;&#22411;Chatbot GPT-4&#23454;&#29616;&#20102;&#21487;&#25509;&#21463;&#30340;F1&#20998;&#25968;0.94&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;GPT4all&#23454;&#29616;&#20102;F1&#20998;&#25968;0.90&#12290;&#28982;&#32780;&#65292;&#23601;&#32593;&#32476;&#23433;&#20840;&#23454;&#20307;&#35782;&#21035;&#32780;&#35328;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence. In this context, Large Language Models are becoming increasingly significant in the field of cybersecurity, presenting a wide range of opportunities. This study explores the capability of chatbots such as ChatGPT, GPT4all, Dolly,Stanford Alpaca, Alpaca-LoRA, and Falcon to identify cybersecurity-related text within Open Source Intelligence. We assess the capabilities of existing chatbot models for Natural Language Processing tasks. We consider binary classification and Named Entity Recognition as tasks. This study analyzes well-established data collected from Twitter, derived from previous research efforts. Regarding cybersecurity binary classification, Chatbot GPT-4 as a commercial model achieved an acceptable F1-score of 0.94, and the open-source GPT4all model achieved an F1-score of 0.90. However, concerning cybersecurity entity re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#20013;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#19979;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#32531;&#24930;&#12290;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20703</link><description>&lt;p&gt;
&#24378;&#21270;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Vanishing Gradients in Reinforcement Finetuning of Language Models. (arXiv:2310.20703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#20013;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#65292;&#24403;&#27169;&#22411;&#19979;&#22870;&#21169;&#30340;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#32531;&#24930;&#12290;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#24494;&#35843;&#65288;RFT&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#19979;&#28216;&#20219;&#21153;&#23545;&#40784;&#65292;&#21363;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#26368;&#22823;&#21270;&#65288;&#21487;&#33021;&#26159;&#23398;&#20064;&#24471;&#21040;&#30340;&#65289;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;RFT&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#30340;&#20248;&#21270;&#38556;&#30861;&#65306;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#27169;&#22411;&#19979;&#30340;&#22870;&#21169;&#26631;&#20934;&#24046;&#36739;&#23567;&#26102;&#65292;&#36755;&#20837;&#30340;&#26399;&#26395;&#26799;&#24230;&#20250;&#28040;&#22833;&#65292;&#21363;&#20351;&#26399;&#26395;&#22870;&#21169;&#36828;&#31163;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#22312;RFT&#22522;&#20934;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#21450;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#20110;&#23567;&#30340;&#22870;&#21169;&#26631;&#20934;&#24046;&#23548;&#33268;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#19988;&#26377;&#23475;&#65292;&#23548;&#33268;&#22870;&#21169;&#26368;&#22823;&#21270;&#26497;&#20854;&#32531;&#24930;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20811;&#26381;RFT&#20013;&#26799;&#24230;&#28040;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21021;&#22987;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#38454;&#27573;&#26159;&#26368;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#23427;&#22312;RFT&#27969;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#30456;&#23545;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;SFT&#38454;&#27573;&#21487;&#20197;&#26377;&#25928;&#20811;&#26381;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small num
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18235</link><description>&lt;p&gt;
Davidsonian&#22330;&#26223;&#22270;&#65306;&#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#24544;&#23454;&#24230;&#30340;&#24378;&#22823;&#26041;&#27861;&#26159;&#22522;&#20110;QG/A&#65288;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#65289;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#31572;&#26696;&#19982;&#22522;&#20110;&#25552;&#31034;&#30340;&#31572;&#26696;&#22312;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#19968;&#33268;&#24615;&#23545;&#36755;&#20986;&#22270;&#20687;&#36827;&#34892;&#35780;&#20998;&#12290;&#36825;&#31181;&#35780;&#20272;&#33258;&#28982;&#19978;&#21462;&#20915;&#20110;&#24213;&#23618;QG&#21644;QA&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QG/A&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#21487;&#38752;&#24615;&#25361;&#25112;&#65306;&#65288;a&#65289;QG&#38382;&#39064;&#24212;&#23562;&#37325;&#25552;&#31034;&#65288;&#36991;&#20813;&#24187;&#35273;&#12289;&#37325;&#22797;&#21644;&#36951;&#28431;&#65289;&#21644;&#65288;b&#65289;VQA&#31572;&#26696;&#24212;&#19968;&#33268;&#65288;&#19981;&#20250;&#22312;&#22270;&#20687;&#20013;&#23459;&#31216;&#27809;&#26377;&#25705;&#25176;&#36710;&#65292;&#21516;&#26102;&#22768;&#31216;&#25705;&#25176;&#36710;&#26159;&#34013;&#33394;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#65292;&#36825;&#20010;&#21463;&#24418;&#24335;&#35821;&#20041;&#21551;&#21457;&#30340;&#23454;&#35777;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
&lt;/p&gt;</description></item><item><title>CodeChain&#26159;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08992</link><description>&lt;p&gt;
CodeChain: &#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#23454;&#29616;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules. (arXiv:2310.08992v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08992
&lt;/p&gt;
&lt;p&gt;
CodeChain&#26159;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#35299;&#20915;&#31616;&#21333;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#29087;&#32451;&#65292;&#27604;&#22914;&#22312;HumanEval&#25110;MBPP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#21644;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#32534;&#31243;&#20219;&#21153;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#20316;&#20026;&#25972;&#20307;&#20195;&#30721;&#22359;&#32780;&#19981;&#26159;&#23558;&#20854;&#20998;&#35299;&#20026;&#36923;&#36753;&#23376;&#20219;&#21153;&#21644;&#23376;&#27169;&#22359;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26377;&#32463;&#39564;&#30340;&#31243;&#24207;&#21592;&#26412;&#33021;&#22320;&#32534;&#20889;&#20855;&#26377;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22359;&#21270;&#20195;&#30721;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#36890;&#24120;&#20250;&#37325;&#22797;&#20351;&#29992;&#20043;&#21069;&#24320;&#21457;&#30340;&#27169;&#22359;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeChain&#65292;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CodeChain&#39318;&#20808;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25552;&#31034;&#25351;&#23548;LLM&#29983;&#25104;&#27169;&#22359;&#21270;&#20195;&#30721;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#20004;&#20010;&#27493;&#39588;&#23454;&#26045;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#65306;1&#65289;&#39069;&#22806;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25674;&#38144;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#22788;&#29702;&#25512;&#29702;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04363</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#25674;&#38144;&#38590;&#20197;&#22788;&#29702;&#30340;&#25512;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Amortizing intractable inference in large language models. (arXiv:2310.04363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25674;&#38144;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#22788;&#29702;&#25512;&#29702;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19979;&#19968;&#20010;&#35789;&#26465;&#20214;&#20998;&#24067;&#26469;&#21387;&#32553;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#35813;&#30693;&#35782;&#30340;&#21487;&#22788;&#29702;&#26597;&#35810;&#20165;&#38480;&#20110;&#20174;&#22836;&#21040;&#23614;&#30340;&#33258;&#22238;&#24402;&#25277;&#26679;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#24207;&#21015;&#24310;&#32493;&#12289;&#22635;&#20805;&#21644;&#20854;&#20182;&#24418;&#24335;&#30340;&#21463;&#32422;&#26463;&#29983;&#25104;&#65292;&#37117;&#28041;&#21450;&#20174;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25674;&#38144;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#20174;&#36825;&#20123;&#38590;&#20197;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#36825;&#31181;&#25674;&#38144;&#36890;&#36807;&#36890;&#36807;&#23547;&#27714;&#22810;&#26679;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; - &#29983;&#25104;&#27969;&#32593;&#32476; (GFlowNets) &#26469;&#24494;&#35843; LLMs &#23454;&#29616;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;LLM&#24494;&#35843;&#30340;&#36825;&#31181;&#20998;&#24067;&#21305;&#37197;&#33539;&#24335;&#21487;&#20197;&#20316;&#20026;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#21644;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#20248;&#21270;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#24605;&#32500;&#38142;&#25512;&#29702;&#35299;&#37322;&#20026;&#28508;&#21464;&#37327;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#22312;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#33258;&#25105;&#32416;&#27491;&#21518;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.01798</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#19981;&#33021;&#33258;&#25105;&#32416;&#27491;&#25512;&#29702;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Cannot Self-Correct Reasoning Yet. (arXiv:2310.01798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01798
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#22312;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29978;&#33267;&#21487;&#33021;&#22312;&#33258;&#25105;&#32416;&#27491;&#21518;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20973;&#20511;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26080;&#21487;&#27604;&#25311;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#32780;&#25104;&#20026;&#31361;&#30772;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20854;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24403;&#24615;&#20173;&#23384;&#22312;&#30097;&#34385;&#12290;&#33258;&#25105;&#32416;&#27491;&#26041;&#27861;&#34987;&#25552;&#20986;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#27492;&#22522;&#30784;&#19978;&#23545;LLMs&#20869;&#37096;&#30340;&#33258;&#25105;&#32416;&#27491;&#30340;&#20316;&#29992;&#21644;&#25928;&#26524;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#32771;&#23519;&#65292;&#25581;&#31034;&#20102;&#20854;&#30495;&#27491;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20869;&#22312;&#33258;&#25105;&#32416;&#27491;&#30340;&#27010;&#24565;&#65292;&#21363;LLMs&#23581;&#35797;&#20165;&#20165;&#20381;&#38752;&#20854;&#22266;&#26377;&#33021;&#21147;&#26469;&#32416;&#27491;&#20854;&#21021;&#22987;&#21709;&#24212;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#22806;&#37096;&#21453;&#39304;&#30340;&#25903;&#25345;&#12290;&#22312;&#25512;&#29702;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;LLMs&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#33258;&#25105;&#32416;&#27491;&#20854;&#21709;&#24212;&#65292;&#29978;&#33267;&#26377;&#26102;&#20505;&#20854;&#34920;&#29616;&#21487;&#33021;&#22312;&#33258;&#25105;&#32416;&#27491;&#21518;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future resear
&lt;/p&gt;</description></item><item><title>DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.17167</link><description>&lt;p&gt;
DyVal: &#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17167
&lt;/p&gt;
&lt;p&gt;
DyVal&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#20986;&#20102;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#24222;&#22823;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20934;&#30340;&#38745;&#24577;&#24615;&#36136;&#21644;&#22266;&#23450;&#22797;&#26434;&#24615;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#34913;&#37327;LLM&#30340;&#36827;&#27493;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DyVal&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#19988;&#28789;&#27963;&#30340;&#21160;&#24577;&#35780;&#20272;LLM&#30340;&#21327;&#35758;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#21160;&#24577;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21033;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#32467;&#26500;&#20248;&#21183;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;DyVal&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#22797;&#26434;&#24615;&#30340;&#35780;&#20272;&#26679;&#26412;&#12290;DyVal&#29983;&#25104;&#20102;&#21253;&#25324;&#25968;&#23398;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#31639;&#27861;&#38382;&#39064;&#22312;&#20869;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#20219;&#21153;&#30340;&#35780;&#20272;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20174;Flan-T5-large&#21040;ChatGPT&#21644;GPT4&#30340;&#21508;&#31181;LLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#22312;DyVal&#29983;&#25104;&#30340;&#35780;&#20272;&#26679;&#26412;&#19978;&#34920;&#29616;&#26356;&#24046;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11093</link><description>&lt;p&gt;
K-pop&#27468;&#35789;&#32763;&#35793;&#65306;&#25968;&#25454;&#38598;&#12289;&#20998;&#26512;&#19982;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11093
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#35789;&#32763;&#35793;&#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#20102;&#19968;&#20010;&#19990;&#32426;&#30340;&#39046;&#22495;&#65292;&#22914;&#20170;&#21560;&#24341;&#30528;&#35745;&#31639;&#35821;&#35328;&#23398;&#30740;&#31350;&#32773;&#30340;&#27880;&#24847;&#12290;&#25105;&#20204;&#22312;&#20197;&#24448;&#30740;&#31350;&#20013;&#21457;&#29616;&#20102;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22312;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#23613;&#31649;K-pop&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#35199;&#26041;&#27969;&#27966;&#21644;&#35821;&#35328;&#65292;&#27809;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;K-pop&#19978;&#12290;&#20854;&#27425;&#65292;&#27468;&#35789;&#32763;&#35793;&#39046;&#22495;&#32570;&#20047;&#21487;&#20844;&#24320;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65307;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26080;&#27492;&#31867;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25299;&#23485;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#30340;&#27969;&#27966;&#21644;&#35821;&#35328;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21809;&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#32422;89%&#20026;K-pop&#27468;&#35789;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#36880;&#34892;&#21644;&#36880;&#33410;&#23545;&#40784;&#20102;&#38889;&#35821;&#21644;&#33521;&#35821;&#27468;&#35789;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#19982;&#20854;&#20182;&#24191;&#27867;&#30740;&#31350;&#30340;&#27969;&#27966;&#21306;&#20998;&#24320;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Tri-Distil-BERT&#21644;Mixed-Distil-BERT&#20004;&#20010;&#27169;&#22411;&#65292;Tri-Distil-BERT&#26159;&#19968;&#20010;&#22312;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#19978;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;Mixed-Distil-BERT&#26159;&#19968;&#20010;&#22312;&#28151;&#21512;&#32534;&#30721;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19982;&#26356;&#22823;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10272</link><description>&lt;p&gt;
&#28151;&#21512;Distil-BERT: &#29992;&#20110;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#30340;&#28151;&#21512;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi. (arXiv:2309.10272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Tri-Distil-BERT&#21644;Mixed-Distil-BERT&#20004;&#20010;&#27169;&#22411;&#65292;Tri-Distil-BERT&#26159;&#19968;&#20010;&#22312;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#19978;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;Mixed-Distil-BERT&#26159;&#19968;&#20010;&#22312;&#28151;&#21512;&#32534;&#30721;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19982;&#26356;&#22823;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#25991;&#26412;&#20998;&#31867;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#19979;&#28216;&#20219;&#21153;&#20043;&#19968;&#12290;&#24403;&#25991;&#26412;&#26159;&#28151;&#21512;&#32534;&#30721;&#26102;&#65292;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#34429;&#28982;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#25509;&#35302;&#21040;&#36825;&#31181;&#25991;&#26412;&#65292;&#20294;&#19981;&#21516;&#30340;BERT&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#28151;&#21512;&#32534;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25361;&#25112;&#12290;&#20877;&#27425;&#65292;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#28151;&#21512;&#32534;&#30721;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24050;&#32463;&#20381;&#36182;&#20110;&#23558;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#32467;&#21512;&#12290;&#24403;BERT&#27169;&#22411;&#20351;&#29992;&#30456;&#24212;&#30340;&#28151;&#21512;&#32534;&#30721;&#35821;&#35328;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#20102;&#35299;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#20102;&#24590;&#26679;&#30340;&#24433;&#21709;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Tri-Distil-BERT&#65292;&#19968;&#20010;&#22312;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#19978;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;Mixed-Distil-BERT&#65292;&#19968;&#20010;&#22312;&#28151;&#21512;&#32534;&#30721;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20986;&#19982;&#26356;&#22823;&#30340;&#27169;&#22411;&#22914;mBERT&#21644;XLM-R&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20004;&#23618;&#39044;&#35757;&#32451;&#26041;&#27861;&#20026;&#22810;&#35821;&#35328;&#20219;&#21153;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most popular downstream tasks in the field of Natural Language Processing is text classification. Text classification tasks have become more daunting when the texts are code-mixed. Though they are not exposed to such text during pre-training, different BERT models have demonstrated success in tackling Code-Mixed NLP challenges. Again, in order to enhance their performance, Code-Mixed NLP models have depended on combining synthetic data with real-world data. It is crucial to understand how the BERT models' performance is impacted when they are pretrained using corresponding code-mixed languages. In this paper, we introduce Tri-Distil-BERT, a multilingual model pre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model fine-tuned on code-mixed data. Both models are evaluated across multiple NLP tasks and demonstrate competitive performance against larger models like mBERT and XLM-R. Our two-tiered pre-training approach offers efficient alternatives for multiling
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.10792</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#25351;&#20197;&#30417;&#30563;&#26041;&#24335;&#22312;&#21253;&#21547;&#8220;&#25351;&#20196;-&#36755;&#20986;&#8221;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#65292;&#36825;&#23558;LLM&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#25143;&#24076;&#26395;LLM&#36981;&#23432;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;IT&#30340;&#24120;&#35268;&#26041;&#27861;&#12289;IT&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;IT&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#24577;&#12289;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23545;&#24433;&#21709;IT&#32467;&#26524;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#65288;&#20363;&#22914;&#65292;&#25351;&#20196;&#36755;&#20986;&#30340;&#29983;&#25104;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;IT&#30340;&#28508;&#22312;&#38382;&#39064;&#20197;&#21450;&#38024;&#23545;&#20854;&#30340;&#25209;&#35780;&#65292;&#20197;&#21450;&#25351;&#20986;&#24403;&#21069;&#19981;&#36275;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#22312;&#22810;&#20010;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#36229;&#36234;&#20102;&#26631;&#20934;&#30340;&#38646;-shot&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.07702</link><description>&lt;p&gt;
&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#25552;&#39640;&#38646;-shot&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Better Zero-Shot Reasoning with Role-Play Prompting. (arXiv:2308.07702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07702
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#22312;&#22810;&#20010;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#36229;&#36234;&#20102;&#26631;&#20934;&#30340;&#38646;-shot&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#65292;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#25198;&#28436;&#19981;&#20165;&#26159;&#20154;&#31867;&#35282;&#33394;&#65292;&#36824;&#21253;&#25324;&#20687;Linux&#32456;&#31471;&#36825;&#26679;&#30340;&#38750;&#20154;&#35282;&#33394;&#12290;&#36825;&#31181;&#22810;&#21151;&#33021;&#24615;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#20132;&#20114;&#21644;&#34892;&#20026;&#65292;&#24182;&#20223;&#30495;&#29305;&#23450;&#30340;&#23545;&#35937;&#25110;&#31995;&#32479;&#12290;&#23613;&#31649;&#36825;&#20123;&#33021;&#21147;&#22686;&#24378;&#20102;&#29992;&#25143;&#21442;&#19982;&#24230;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#20132;&#20114;&#27169;&#24335;&#65292;&#20294;&#35282;&#33394;&#25198;&#28436;&#23545;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#20173;&#26377;&#24453;&#28145;&#20837;&#25506;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31574;&#30053;&#24615;&#35774;&#35745;&#30340;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#22312;&#21313;&#20108;&#20010;&#19981;&#21516;&#30340;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20854;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#65292;&#28085;&#30422;&#20102;&#31639;&#26415;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#31526;&#21495;&#25512;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#21033;&#29992;ChatGPT&#21644;Llama 2&#31561;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35282;&#33394;&#25198;&#28436;&#25552;&#31034;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#26631;&#20934;&#30340;&#38646;-shot&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Modern large language models (LLMs), such as ChatGPT, exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities like a Linux terminal. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs' reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks, encompassing arithmetic, commonsense reasoning, symbolic reasoning, and more. Leveraging models such as ChatGPT and Llama 2, our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, a
&lt;/p&gt;</description></item><item><title>MO-VLN&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#36890;&#29992;&#26426;&#22120;&#20154;&#22312;&#22810;&#20219;&#21153;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;5&#24320;&#21457;&#36924;&#30495;&#30340;&#22330;&#26223;&#21644;&#21253;&#21547;&#22810;&#31181;&#19981;&#24120;&#35265;&#29289;&#20307;&#26469;&#27979;&#35797;&#20854;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.10322</link><description>&lt;p&gt;
MO-VLN:&#19968;&#20010;&#29992;&#20110;&#24320;&#25918;&#38598;&#21512;&#38646;&#26679;&#26412;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934; (arXiv:2306.10322v2 [cs.CV] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation. (arXiv:2306.10322v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10322
&lt;/p&gt;
&lt;p&gt;
MO-VLN&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#36890;&#29992;&#26426;&#22120;&#20154;&#22312;&#22810;&#20219;&#21153;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;5&#24320;&#21457;&#36924;&#30495;&#30340;&#22330;&#26223;&#21644;&#21253;&#21547;&#22810;&#31181;&#19981;&#24120;&#35265;&#29289;&#20307;&#26469;&#27979;&#35797;&#20854;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#29702;&#35299;&#25351;&#20196;&#24182;&#26681;&#25454;&#35270;&#35273;&#35266;&#23519;&#25214;&#21040;&#30446;&#26631;&#23545;&#35937;&#25110;&#20301;&#32622;&#65292;&#21363;&#20351;&#22312;&#26410;&#25506;&#32034;&#30340;&#29615;&#22659;&#20013;&#20063;&#33021;&#20570;&#21040;&#12290;&#22823;&#22810;&#25968;&#20195;&#29702;&#20381;&#36182;&#20110;&#22823;&#37327;&#22810;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#65292;&#36825;&#38656;&#35201;&#26114;&#36149;&#30340;&#21171;&#21160;&#21147;&#12290;&#36825;&#20123;&#20195;&#29702;&#36890;&#24120;&#21482;&#20851;&#27880;&#24120;&#35265;&#30340;&#23545;&#35937;&#21644;&#36739;&#23569;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#19981;&#36275;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25351;&#20196;&#12290;&#20026;&#20102;&#20419;&#36827;&#24320;&#25918;&#38598;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MO-VLN&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#27979;&#35797;&#20195;&#29702;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;5&#24320;&#21457;&#20102;&#19968;&#20010;3D&#27169;&#25311;&#22120;&#65292;&#28210;&#26579;&#20102;&#36924;&#30495;&#30340;&#22330;&#26223;&#65292;&#21253;&#21547;&#26356;&#30495;&#23454;&#30340;&#20809;&#29031;&#21644;&#32454;&#33410;&#12290;&#27169;&#25311;&#22120;&#21253;&#21547;&#19977;&#20010;&#22330;&#26223;&#65292;&#21363;&#21654;&#21857;&#39302;&#12289;&#39184;&#21381;&#21644;&#20859;&#32769;&#38498;&#65292;&#36825;&#20123;&#22330;&#26223;&#22312;&#24037;&#19994;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;&#28041;&#21450;&#22810;&#31181;&#19981;&#24120;&#35265;&#30340;&#29289;&#20307;&#65292;&#22914;&#22806;&#21334;&#26479;&#21644;&#21307;&#29992;&#33014;&#24102;&#65292;&#36825;&#20123;&#29289;&#20307;&#26356;&#21152;&#22797;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a natural language, a general robot has to comprehend the instruction and find the target object or location based on visual observations even in unexplored environments. Most agents rely on massive diverse training data to achieve better generalization, which requires expensive labor. These agents often focus on common objects and fewer tasks, thus are not intelligent enough to handle different types of instructions. To facilitate research in open-set vision-and-language navigation, we propose a benchmark named MO-VLN, aiming at testing the effectiveness and generalization of the agent in the multi-task setting. First, we develop a 3D simulator rendered by realistic scenarios using Unreal Engine 5, containing more realistic lights and details. The simulator contains three scenes, i.e., cafe, restaurant, and nursing house, of high value in the industry. Besides, our simulator involves multiple uncommon objects, such as takeaway cup and medical adhesive tape, which are more compli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#32763;&#35793;&#25351;&#20196;&#25191;&#34892;&#22810;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#22810;&#35821;&#35328;LLMs&#20855;&#26377;&#36739;&#24378;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#36825;&#21462;&#20915;&#20110;&#35821;&#35328;&#19982;&#33521;&#35821;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#12290;&#27492;&#22806;&#65292;&#25191;&#34892;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#20381;&#36182;&#20110;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.15083</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35821;&#35328;&#24494;&#35843;&#21644;&#32763;&#35793;&#25351;&#20196;&#35825;&#21457;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions. (arXiv:2305.15083v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#32763;&#35793;&#25351;&#20196;&#25191;&#34892;&#22810;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#22810;&#35821;&#35328;LLMs&#20855;&#26377;&#36739;&#24378;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#36825;&#21462;&#20915;&#20110;&#35821;&#35328;&#19982;&#33521;&#35821;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#12290;&#27492;&#22806;&#65292;&#25191;&#34892;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#20381;&#36182;&#20110;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;ChatGPT&#21644;GPT4&#65292;&#23637;&#29616;&#20986;&#22312;&#22810;&#35821;&#35328;&#32763;&#35793;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#35757;&#32451;&#24182;&#34892;&#35821;&#26009;&#24211;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22914;&#20309;&#33719;&#24471;&#20854;&#23545;&#19981;&#21516;&#35821;&#35328;&#36827;&#34892;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;XGLM-7B&#36827;&#34892;&#24494;&#35843;&#26469;&#25191;&#34892;&#22810;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;LLMs&#20855;&#26377;&#27604;&#20808;&#21069;&#23637;&#31034;&#30340;&#26356;&#24378;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;&#23545;&#20110;&#26576;&#31181;&#35821;&#35328;&#65292;&#20854;&#34920;&#29616;&#21462;&#20915;&#20110;&#20854;&#19982;&#33521;&#35821;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#25191;&#34892;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#20381;&#36182;&#20110;&#23545;&#32763;&#35793;&#25351;&#20196;&#30340;&#29702;&#35299;&#20197;&#21450;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#36890;&#36807;&#22810;&#35821;&#35328;&#24494;&#35843;&#65292;LLMs&#33021;&#22815;&#23398;&#20064;&#24182;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#23545;&#20110;&#37027;&#20123;&#35821;&#35328;&#38388;&#24179;&#34892;&#35821;&#26009;&#36739;&#23569;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Pretrained Language Models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translations, without being explicitly trained on parallel corpora. It is interesting how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs' ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning, LLMs could learn to perform the translation task well even for those language pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; LogicLLM&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#24120;&#35265;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13718</link><description>&lt;p&gt;
LogicLLM&#65306;&#25506;&#32034;&#33258;&#30417;&#30563;&#36923;&#36753;&#22686;&#24378;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models. (arXiv:2305.13718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; LogicLLM&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#24120;&#35265;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#29616;&#26377;&#21162;&#21147;&#20027;&#35201;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#24494;&#35843;&#65292;&#36825;&#38459;&#30861;&#20102;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;/&#25110;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21457;&#23637;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#20016;&#23500;&#30340;&#30693;&#35782;&#21387;&#32553;&#20026;&#21333;&#20010;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;LLMs &#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#24182;&#27809;&#26377;&#34920;&#29616;&#20986;&#33021;&#21147;&#12290;LLMs &#22312;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#36828;&#33853;&#21518;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25506;&#32034;&#34701;&#21512;&#36923;&#36753;&#30693;&#35782;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#28608;&#27963;&#23427;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;LogicLLM&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;MERIt &#30340;&#33258;&#22238;&#24402;&#30446;&#26631;&#21464;&#20307;&#65292;&#24182;&#23558;&#20854;&#19982;&#20004;&#20010;LLM&#31995;&#21015;FLAN-T5&#21644;LLaMA&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#21442;&#25968;&#22823;&#23567;&#33539;&#22260;&#20174;30&#20159;&#21040;130&#20159;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24120;&#29992;&#25512;&#29702;&#31574;&#30053;&#19978;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#36828;&#36828;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing efforts to improve logical reasoning ability of language models have predominantly relied on supervised fine-tuning, hindering generalization to new domains and/or tasks. The development of Large Langauge Models (LLMs) has demonstrated the capacity of compressing abundant knowledge into a single proxy, enabling them to tackle multiple tasks effectively. Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines. In this paper, we make the first attempt to investigate the feasibility of incorporating logical knowledge through self-supervised post-training, and activating it via in-context learning, which we termed as LogicLLM. Specifically, we devise an auto-regressive objective variant of MERIt and integrate it with two LLM series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to 13 billion. The results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20551;&#35774;&#25152;&#26377;&#26816;&#32034;&#20449;&#24687;&#37117;&#26159;&#27491;&#30830;&#30340;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#20449;&#24687;&#23548;&#33268;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#37492;&#21035;&#33021;&#21147;&#24341;&#20986;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#19979;&#30340;&#25928;&#26524;&#65307;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2305.01579</link><description>&lt;p&gt;
&#21306;&#20998;&#21644;&#22238;&#31572;&#65306;&#36890;&#36807;&#36776;&#21035;&#22120;&#32531;&#35299;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#34394;&#20551;&#20449;&#24687;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators. (arXiv:2305.01579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20551;&#35774;&#25152;&#26377;&#26816;&#32034;&#20449;&#24687;&#37117;&#26159;&#27491;&#30830;&#30340;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#20449;&#24687;&#23548;&#33268;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#37492;&#21035;&#33021;&#21147;&#24341;&#20986;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#19979;&#30340;&#25928;&#26524;&#65307;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20551;&#23450;&#25152;&#26377;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#37117;&#26159;&#20107;&#23454;&#19978;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#19968;&#20010;&#26356;&#21152;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#21363;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#21487;&#33021;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#31934;&#35843;&#21644;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#23545;&#36825;&#31181;&#20449;&#24687;&#39640;&#24230;&#33030;&#24369;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#22320;&#23545;&#37492;&#21035;&#22120;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#25110;&#25552;&#31034;&#26469;&#24341;&#20986;GPT-3&#30340;&#37492;&#21035;&#33021;&#21147;&#65292;&#20351;&#26816;&#32034;&#22686;&#24378;LM&#23545;&#34394;&#20551;&#20449;&#24687;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;LM&#23545;&#30693;&#35782;&#20914;&#31361;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#30340;&#20915;&#31574;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#30340;&#21457;&#29616;&#65292;&#20026;&#21033;&#29992;&#20004;&#32773;&#30340;&#26368;&#20339;&#26041;&#24335;&#38138;&#24179;&#20102;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing retrieval-augmented language models (LMs) for question answering assume all retrieved information is factually correct. In this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. We observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. We propose approaches to make retrieval-augmented LMs robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in GPT-3. Our empirical results on open-domain question answering show that these approaches significantly improve LMs' robustness to knowledge conflicts. We also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#30456;&#20381;&#24863;&#30693;&#38598;&#21512;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#22810;&#26631;&#31614;&#20998;&#31867;&#35270;&#20026;&#30452;&#25509;&#38598;&#21512;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#26631;&#31614;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#26500;&#24314;&#20851;&#32852;&#30697;&#38453;&#24182;&#32467;&#21512;GCN&#23398;&#20064;&#26631;&#31614;&#20449;&#24687;&#65292;&#21516;&#26102;&#21033;&#29992;&#21477;&#23376;&#20449;&#24687;&#21644;&#26631;&#31614;&#20449;&#24687;&#65292;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07022</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26631;&#31614;&#30456;&#20381;&#24863;&#30693;&#38598;&#21512;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Label Dependencies-aware Set Prediction Networks for Multi-label Text Classification. (arXiv:2304.07022v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#30456;&#20381;&#24863;&#30693;&#38598;&#21512;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#22810;&#26631;&#31614;&#20998;&#31867;&#35270;&#20026;&#30452;&#25509;&#38598;&#21512;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#26631;&#31614;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#26500;&#24314;&#20851;&#32852;&#30697;&#38453;&#24182;&#32467;&#21512;GCN&#23398;&#20064;&#26631;&#31614;&#20449;&#24687;&#65292;&#21516;&#26102;&#21033;&#29992;&#21477;&#23376;&#20449;&#24687;&#21644;&#26631;&#31614;&#20449;&#24687;&#65292;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#20174;&#21477;&#23376;&#20013;&#25552;&#21462;&#25152;&#26377;&#30456;&#20851;&#26631;&#31614;&#65292;&#21487;&#35270;&#20026;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#38598;&#20013;&#30340;&#26631;&#31614;&#26159;&#26080;&#24207;&#30340;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#20854;&#35270;&#20026;&#30452;&#25509;&#38598;&#21512;&#39044;&#27979;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#32771;&#34385;&#26631;&#31614;&#30340;&#39034;&#24207;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#27169;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#36890;&#36807;&#26631;&#31614;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#26500;&#24314;&#20851;&#32852;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;GCN&#26469;&#23398;&#20064;&#26631;&#31614;&#20449;&#24687;&#12290;&#22522;&#20110;&#25152;&#23398;&#30340;&#26631;&#31614;&#20449;&#24687;&#65292;&#38598;&#21512;&#39044;&#27979;&#32593;&#32476;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#21477;&#23376;&#20449;&#24687;&#21644;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#38598;&#21512;&#39044;&#27979;&#32593;&#32476;&#30340;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#26045;&#21152;&#24191;&#20041;&#24052;&#27663;&#36317;&#31163;&#65292;&#20197;&#25552;&#39640;&#20854;&#21484;&#22238;&#29575;&#12290;&#22312;&#22235;&#20010;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#20854;&#24615;&#33021;&#22823;&#22823;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label text classification aims to extract all the related labels from a sentence, which can be viewed as a sequence generation problem. However, the labels in training dataset are unordered. We propose to treat it as a direct set prediction problem and don't need to consider the order of labels. Besides, in order to model the correlation between labels, the adjacency matrix is constructed through the statistical relations between labels and GCN is employed to learn the label information. Based on the learned label information, the set prediction networks can both utilize the sentence information and label information for multi-label text classification simultaneously. Furthermore, the Bhattacharyya distance is imposed on the output probability distributions of the set prediction networks to increase the recall ability. Experimental results on four multi-label datasets show the effectiveness of the proposed method and it outperforms previous method a substantial margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2210.10012</link><description>&lt;p&gt;
&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#29616;&#65292;&#22312;&#20551;&#35774;&#21487;&#32447;&#24615;&#30340;&#31070;&#32463;&#34920;&#31034;&#20013;&#65292;&#20174;&#20013;&#21024;&#38500;&#21487;&#20154;&#35299;&#37322;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#26159;&#21487;&#34892;&#21644;&#26377;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21024;&#38500;&#23545;&#20110;&#22522;&#20110;&#20462;&#25913;&#21518;&#34920;&#31034;&#36827;&#34892;&#35757;&#32451;&#30340;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#23545;&#25163;&#26080;&#27861;&#30452;&#25509;&#20174;&#34920;&#31034;&#20013;&#39044;&#27979;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#65292;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#65292;&#36825;&#25351;&#20986;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#20316;&#20026;&#19979;&#28216;&#20559;&#24046;&#32531;&#35299;&#25216;&#26415;&#30340;&#20869;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#35299;&#37322;&#31070;&#32463;&#34920;&#31034;&#19982;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#21270;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#65292;&#38450;&#27490;&#29305;&#23450;&#38750;&#32447;&#24615;&#23545;&#25163;&#39044;&#27979;&#27010;&#24565;&#65292;&#20294;&#26080;&#27861;&#24443;&#24213;&#35299;&#20915;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#25830;&#38500;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.12191</link><description>&lt;p&gt;
Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12191
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#21270;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#65292;&#38450;&#27490;&#29305;&#23450;&#38750;&#32447;&#24615;&#23545;&#25163;&#39044;&#27979;&#27010;&#24565;&#65292;&#20294;&#26080;&#27861;&#24443;&#24213;&#35299;&#20915;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#25830;&#38500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20986;&#29616;&#12290;&#29702;&#35299;&#36825;&#20123;&#34920;&#31034;&#22914;&#20309;&#32534;&#30721;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#27010;&#24565;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#35782;&#21035;&#31070;&#32463;&#34920;&#31034;&#20013;&#30340;&#27010;&#24565;&#30340;&#19968;&#31181;&#26126;&#26174;&#26041;&#27861;&#26159;&#25628;&#32034;&#19968;&#20010;&#32447;&#24615;&#23376;&#31354;&#38388;&#65292;&#20854;&#25830;&#38500;&#20250;&#38459;&#27490;&#20174;&#34920;&#31034;&#20013;&#39044;&#27979;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#32447;&#24615;&#25830;&#38500;&#31639;&#27861;&#26159;&#21487;&#22788;&#29702;&#21644;&#21487;&#35299;&#37322;&#30340;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#26410;&#24517;&#20197;&#32447;&#24615;&#26041;&#24335;&#34920;&#31034;&#27010;&#24565;&#12290;&#20026;&#20102;&#35782;&#21035;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26680;&#21270;&#30340;&#27010;&#24565;&#25830;&#38500;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#38450;&#27490;&#29305;&#23450;&#30340;&#38750;&#32447;&#24615;&#23545;&#25163;&#39044;&#27979;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20445;&#25252;&#19981;&#20250;&#36716;&#31227;&#21040;&#19981;&#21516;&#30340;&#38750;&#32447;&#24615;&#23545;&#25163;&#12290;&#22240;&#27492;&#65292;&#24443;&#24213;&#22320;&#25830;&#38500;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation space of neural models for textual data emerges in an unsupervised manner during training. Understanding how those representations encode human-interpretable concepts is a fundamental problem. One prominent approach for the identification of concepts in neural representations is searching for a linear subspace whose erasure prevents the prediction of the concept from the representations. However, while many linear erasure algorithms are tractable and interpretable, neural networks do not necessarily represent concepts in a linear manner. To identify non-linearly encoded concepts, we propose a kernelization of a linear minimax game for concept erasure. We demonstrate that it is possible to prevent specific non-linear adversaries from predicting the concept. However, the protection does not transfer to different nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded concept remains an open problem.
&lt;/p&gt;</description></item></channel></rss>