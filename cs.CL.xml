<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#23401;&#23376;&#19968;&#26679;&#36890;&#36807;&#32852;&#24819;&#26469;&#35299;&#20915;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#33655;&#20848;&#35821;&#27597;&#35821;&#21644;&#22810;&#35821;&#35328;LLMs&#30340;&#34920;&#29616;&#19982;&#20799;&#31461;&#30456;&#24403;&#65292;&#20294;&#24403;&#25511;&#21046;&#32852;&#24819;&#36807;&#31243;&#26102;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;1-2&#24180;&#12290;</title><link>http://arxiv.org/abs/2310.20384</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#23401;&#23376;&#19968;&#26679;&#35299;&#20915;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do large language models solve verbal analogies like children do?. (arXiv:2310.20384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#23401;&#23376;&#19968;&#26679;&#36890;&#36807;&#32852;&#24819;&#26469;&#35299;&#20915;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#33655;&#20848;&#35821;&#27597;&#35821;&#21644;&#22810;&#35821;&#35328;LLMs&#30340;&#34920;&#29616;&#19982;&#20799;&#31461;&#30456;&#24403;&#65292;&#20294;&#24403;&#25511;&#21046;&#32852;&#24819;&#36807;&#31243;&#26102;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;1-2&#24180;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#24605;&#32500;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#26680;&#24515;&#12290;&#25104;&#24180;&#20154;&#36890;&#36807;&#26144;&#23556;&#20851;&#31995;&#24182;&#22238;&#31572;&#38382;&#39064;&#65292;&#22914;&#8220;&#39532;&#23646;&#20110;&#39532;&#21417;&#65292;&#40481;&#23646;&#20110;...&#65311;&#8221;&#32780;&#35299;&#20915;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#23401;&#23376;&#20204;&#32463;&#24120;&#20351;&#29992;&#32852;&#24819;&#20316;&#31572;&#65292;&#20363;&#22914;&#22238;&#31572;&#8220;&#34507;&#8221;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20687;&#23401;&#23376;&#19968;&#26679;&#36890;&#36807;&#32852;&#24819;&#26469;&#35299;&#20915;A:B::C:?&#24418;&#24335;&#30340;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#22312;&#32447;&#33258;&#36866;&#24212;&#23398;&#20064;&#29615;&#22659;&#20013;&#25552;&#21462;&#30340;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#65292;&#20854;&#20013;&#26469;&#33258;&#33655;&#20848;&#30340;14,002&#21517;7-12&#23681;&#20799;&#31461;&#35299;&#20915;&#20102;622&#20010;&#33655;&#20848;&#35821;&#30340;&#35821;&#35328;&#31867;&#27604;&#38382;&#39064;&#12290;&#20845;&#20010;&#27979;&#35797;&#30340;&#33655;&#20848;&#35821;&#27597;&#35821;&#21644;&#22810;&#35821;&#35328;LLMs&#30340;&#34920;&#29616;&#19982;&#20799;&#31461;&#22823;&#33268;&#30456;&#24403;&#65292;MGPT&#34920;&#29616;&#26368;&#24046;&#65292;&#25509;&#36817;7&#23681;&#27700;&#24179;&#65292;XLM-V&#21644;GPT-3&#34920;&#29616;&#26368;&#20339;&#65292;&#30053;&#39640;&#20110;11&#23681;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#25511;&#21046;&#32852;&#24819;&#36807;&#31243;&#26102;&#65292;&#24773;&#20917;&#21457;&#29983;&#21464;&#21270;&#65292;&#27599;&#20010;&#27169;&#22411;&#30340;&#34920;&#29616;&#27700;&#24179;&#19979;&#38477;1-2&#24180;&#12290;&#36827;&#19968;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#32852;&#24819;&#36807;&#31243;&#30340;&#25511;&#21046;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogy-making lies at the heart of human cognition. Adults solve analogies such as \textit{Horse belongs to stable like chicken belongs to ...?} by mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In contrast, children often use association, e.g., answering \textit{egg}. This paper investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do. We use verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, around the 7-year-old level, and XLM-V and GPT-3 the best, slightly above the 11-year-old level. However, when we control for associative processes this picture changes and each model's performance level drops 1-2 years. Further experiments demonstrate that associ
&lt;/p&gt;</description></item><item><title>AMERICANO&#26159;&#19968;&#20010;&#22522;&#20110;&#35770;&#36848;&#39537;&#21160;&#30340;&#20998;&#35299;&#21644;&#20195;&#29702;&#20132;&#20114;&#30340;&#35770;&#35777;&#29983;&#25104;&#26694;&#26550;&#65292;&#22312;&#35770;&#35777;&#29983;&#25104;&#20013;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#39034;&#24207;&#21160;&#20316;&#24182;&#32454;&#21270;&#35770;&#35777;&#33609;&#31295;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35770;&#35777;&#24615;&#35770;&#36848;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20352</link><description>&lt;p&gt;
AMERICANO:&#22522;&#20110;&#35770;&#36848;&#39537;&#21160;&#30340;&#20998;&#35299;&#21644;&#20195;&#29702;&#20132;&#20114;&#30340;&#35770;&#35777;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction. (arXiv:2310.20352v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20352
&lt;/p&gt;
&lt;p&gt;
AMERICANO&#26159;&#19968;&#20010;&#22522;&#20110;&#35770;&#36848;&#39537;&#21160;&#30340;&#20998;&#35299;&#21644;&#20195;&#29702;&#20132;&#20114;&#30340;&#35770;&#35777;&#29983;&#25104;&#26694;&#26550;&#65292;&#22312;&#35770;&#35777;&#29983;&#25104;&#20013;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#39034;&#24207;&#21160;&#20316;&#24182;&#32454;&#21270;&#35770;&#35777;&#33609;&#31295;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35770;&#35777;&#24615;&#35770;&#36848;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20005;&#26684;&#30340;&#25512;&#29702;&#21644;&#36866;&#24403;&#30340;&#20869;&#23481;&#32452;&#32455;&#12290;&#21463;&#26368;&#36817;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#21551;&#21457;&#65292;&#35813;&#25552;&#31034;&#23558;&#22797;&#26434;&#30340;&#20219;&#21153;&#20998;&#35299;&#20026;&#20013;&#38388;&#27493;&#39588;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AMERICANO&#65292;&#19968;&#20010;&#20855;&#26377;&#20195;&#29702;&#20132;&#20114;&#30340;&#26032;&#22411;&#35770;&#35777;&#29983;&#25104;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#22522;&#20110;&#35770;&#35777;&#35770;&#36848;&#30340;&#39034;&#24207;&#21160;&#20316;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#29983;&#25104;&#35770;&#35777;&#24615;&#35770;&#36848;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#21518;&#26681;&#25454;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#29983;&#25104;&#26368;&#32456;&#30340;&#35770;&#35777;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#27169;&#20223;&#20154;&#31867;&#20889;&#20316;&#36807;&#31243;&#65292;&#24182;&#25913;&#36827;&#24403;&#21069;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#20174;&#24038;&#21040;&#21491;&#29983;&#25104;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35770;&#35777;&#32454;&#21270;&#27169;&#22359;&#65292;&#26681;&#25454;&#25509;&#25910;&#21040;&#30340;&#21453;&#39304;&#33258;&#21160;&#35780;&#20272;&#21644;&#23436;&#21892;&#35770;&#35777;&#33609;&#31295;&#12290;&#25105;&#20204;&#20351;&#29992;Reddit/CMV&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21453;&#39539;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Argument generation is a challenging task in natural language processing, which requires rigorous reasoning and proper content organization. Inspired by recent chain-of-thought prompting that breaks down a complex task into intermediate steps, we propose Americano, a novel framework with agent interaction for argument generation. Our approach decomposes the generation process into sequential actions grounded on argumentation theory, which first executes actions sequentially to generate argumentative discourse components, and then produces a final argument conditioned on the components. To further mimic the human writing process and improve the left-to-right generation paradigm of current autoregressive language models, we introduce an argument refinement module which automatically evaluates and refines argument drafts based on feedback received. We evaluate our framework on the task of counterargument generation using a subset of Reddit/CMV dataset. The results show that our method out
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Apache TVM&#33258;&#21160;&#29983;&#25104;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#30340;&#38459;&#22622;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#29983;&#25104;&#29305;&#23450;&#22788;&#29702;&#22120;&#30340;&#24494;&#20869;&#26680;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#21644;&#21487;&#20248;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20347</link><description>&lt;p&gt;
&#20351;&#29992;Apache TVM&#33258;&#21160;&#29983;&#25104;&#19968;&#31995;&#21015;&#30697;&#38453;&#20056;&#27861;&#20363;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM. (arXiv:2310.20347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20347
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Apache TVM&#33258;&#21160;&#29983;&#25104;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#30340;&#38459;&#22622;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#29983;&#25104;&#29305;&#23450;&#22788;&#29702;&#22120;&#30340;&#24494;&#20869;&#26680;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#21644;&#21487;&#20248;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;Apache TVM&#24320;&#28304;&#26694;&#26550;&#33258;&#21160;&#29983;&#25104;&#19968;&#31995;&#21015;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#36981;&#24490;&#20102;&#27969;&#34892;&#30340;&#32447;&#24615;&#20195;&#25968;&#24211;&#65288;&#22914;GotoBLAS2&#12289;BLIS&#21644;OpenBLAS&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#39640;&#24615;&#33021;&#30340;&#38459;&#22622;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23436;&#20840;&#33258;&#21160;&#21270;&#20102;&#29983;&#25104;&#36807;&#31243;&#65292;&#36824;&#21033;&#29992;Apache TVM&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;GEMM&#30340;&#23436;&#25972;&#30340;&#29305;&#23450;&#22788;&#29702;&#22120;&#24494;&#20869;&#26680;&#30340;&#21508;&#31181;&#21464;&#20307;&#12290;&#36825;&#19982;&#39640;&#24615;&#33021;&#24211;&#20013;&#20351;&#29992;&#27719;&#32534;&#20195;&#30721;&#25163;&#21160;&#32534;&#30721;&#30340;&#21333;&#20010;&#24494;&#20869;&#26680;&#30340;&#20570;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the utilization of the Apache TVM open source framework to automatically generate a family of algorithms that follow the approach taken by popular linear algebra libraries, such as GotoBLAS2, BLIS and OpenBLAS, in order to obtain high-performance blocked formulations of the general matrix multiplication (GEMM). % In addition, we fully automatize the generation process, by also leveraging the Apache TVM framework to derive a complete variety of the processor-specific micro-kernels for GEMM. This is in contrast with the convention in high performance libraries, which hand-encode a single micro-kernel per architecture using Assembly code. % In global, the combination of our TVM-generated blocked algorithms and micro-kernels for GEMM 1)~improves portability, maintainability and, globally, streamlines the software life cycle; 2)~provides high flexibility to easily tailor and optimize the solution to different data types, processor architectures, and matrix operand shapes, yieldin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20195;&#30721;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#20102;InstructCoder&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#26679;&#24615;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#65292;&#20026;&#36890;&#29992;&#20195;&#30721;&#32534;&#36753;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.20329</link><description>&lt;p&gt;
InstructCoder: &#20026;&#20195;&#30721;&#32534;&#36753;&#36171;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
InstructCoder: Empowering Language Models for Code Editing. (arXiv:2310.20329v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20195;&#30721;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#20102;InstructCoder&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#26679;&#24615;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#65292;&#20026;&#36890;&#29992;&#20195;&#30721;&#32534;&#36753;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#32534;&#36753;&#28085;&#30422;&#20102;&#24320;&#21457;&#32773;&#26085;&#24120;&#22788;&#29702;&#30340;&#21508;&#31181;&#23454;&#29992;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#30456;&#20851;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#20294;&#33258;&#21160;&#20195;&#30721;&#32534;&#36753;&#20173;&#28982;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#28436;&#21270;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#25968;&#25454;&#31232;&#32570;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#29992;&#25143;&#25351;&#20196;&#32534;&#36753;&#20195;&#30721;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#27880;&#37322;&#25554;&#20837;&#65292;&#20195;&#30721;&#20248;&#21270;&#21644;&#20195;&#30721;&#37325;&#26500;&#31561;&#19968;&#31995;&#21015;&#38544;&#21547;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InstructCoder&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#36890;&#29992;&#20195;&#30721;&#32534;&#36753;&#32780;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#39640;&#22810;&#26679;&#24615;&#30340;&#20195;&#30721;&#32534;&#36753;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;114,000&#20010;&#25351;&#20196;-&#36755;&#20837;-&#36755;&#20986;&#19977;&#20803;&#32452;&#65292;&#24182;&#28085;&#30422;&#20102;&#22810;&#20010;&#19981;&#21516;&#30340;&#20195;&#30721;&#32534;&#36753;&#22330;&#26223;&#12290;&#25968;&#25454;&#38598;&#36890;&#36807;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#36827;&#34892;&#31995;&#32479;&#25193;&#23637;&#65292;&#35813;&#36807;&#31243;&#20174;GitHub&#30340;&#25552;&#20132;&#20013;&#33719;&#21462;&#20195;&#30721;&#32534;&#36753;&#25968;&#25454;&#20316;&#20026;&#31181;&#23376;&#20219;&#21153;&#12290;&#31181;&#23376;&#20219;&#21153;&#21644;&#29983;&#25104;&#30340;&#20219;&#21153;&#38543;&#21518;&#29992;&#20110;&#25552;&#31034;ChatGPT&#33719;&#21462;&#26356;&#22810;&#20219;&#21153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of large language models (LLMs) to edit code based on user instructions, covering a broad range of implicit tasks such as comment insertion, code optimization, and code refactoring. To facilitate this, we introduce InstructCoder, the first dataset designed to adapt LLMs for general-purpose code editing, containing highdiversity code-editing tasks. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The dataset is systematically expanded through an iterative process that commences with code editing data sourced from GitHub commits as seed tasks. Seed and generated tasks are used subsequently to prompt ChatGPT for more task data. Our exper
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;ChiSCor&#65292;&#19968;&#20221;&#21253;&#21547;619&#20010;&#22855;&#24187;&#25925;&#20107;&#30340;&#35821;&#26009;&#24211;&#65292;&#30001;442&#21517;&#33655;&#20848;4-12&#23681;&#30340;&#20799;&#31461;&#33258;&#30001;&#35762;&#36848;&#12290;ChiSCor&#30340;&#25925;&#20107;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#20135;&#29983;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#27880;&#37322;&#21644;&#38468;&#21152;&#20803;&#25968;&#25454;&#12290;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;ChiSCor&#22312;&#30740;&#31350;&#20799;&#31461;&#35821;&#35328;&#21644;&#35748;&#30693;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#23545;Zipf&#23450;&#24459;&#30340;&#25193;&#23637;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.20328</link><description>&lt;p&gt;
ChiSCor&#65306;&#19968;&#20221;&#30001;&#33655;&#20848;&#20799;&#31461;&#33258;&#30001;&#35762;&#36848;&#30340;&#22855;&#24187;&#25925;&#20107;&#30340;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science. (arXiv:2310.20328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20328
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;ChiSCor&#65292;&#19968;&#20221;&#21253;&#21547;619&#20010;&#22855;&#24187;&#25925;&#20107;&#30340;&#35821;&#26009;&#24211;&#65292;&#30001;442&#21517;&#33655;&#20848;4-12&#23681;&#30340;&#20799;&#31461;&#33258;&#30001;&#35762;&#36848;&#12290;ChiSCor&#30340;&#25925;&#20107;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#20135;&#29983;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#27880;&#37322;&#21644;&#38468;&#21152;&#20803;&#25968;&#25454;&#12290;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;ChiSCor&#22312;&#30740;&#31350;&#20799;&#31461;&#35821;&#35328;&#21644;&#35748;&#30693;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#23545;Zipf&#23450;&#24459;&#30340;&#25193;&#23637;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#36164;&#28304;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#26009;&#24211;ChiSCor&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;619&#20010;&#22855;&#24187;&#25925;&#20107;&#65292;&#30001;442&#21517;&#24180;&#40836;&#22312;4-12&#23681;&#30340;&#33655;&#20848;&#20799;&#31461;&#33258;&#30001;&#35762;&#36848;&#12290;ChiSCor&#34987;&#32534;&#35793;&#29992;&#20110;&#30740;&#31350;&#20799;&#31461;&#22914;&#20309;&#23637;&#31034;&#20154;&#29289;&#30340;&#35266;&#28857;&#65292;&#20197;&#21450;&#25581;&#31034;&#35821;&#35328;&#21644;&#35748;&#30693;&#22312;&#21457;&#23637;&#20013;&#30340;&#20316;&#29992;&#65292;&#21033;&#29992;&#35745;&#31639;&#24037;&#20855;&#12290;&#19982;&#29616;&#26377;&#36164;&#28304;&#19981;&#21516;&#65292;ChiSCor&#30340;&#25925;&#20107;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#20135;&#29983;&#30340;&#65292;&#31526;&#21512;&#26368;&#36817;&#23545;&#26356;&#29983;&#24577;&#26377;&#25928;&#25968;&#25454;&#38598;&#30340;&#21628;&#21505;&#12290;ChiSCor&#25552;&#20379;&#20102;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#20154;&#29289;&#22797;&#26434;&#24615;&#21644;&#35821;&#35328;&#22797;&#26434;&#24615;&#30340;&#27880;&#37322;&#12290;&#36824;&#20026;&#19977;&#20998;&#20043;&#19968;&#30340;&#33655;&#20848;&#20799;&#31461;&#25552;&#20379;&#20102;&#38468;&#21152;&#20803;&#25968;&#25454;&#65288;&#22914;&#30417;&#25252;&#20154;&#30340;&#25945;&#32946;&#32972;&#26223;&#65289;&#12290;ChiSCor&#36824;&#21253;&#25324;&#19968;&#23567;&#37096;&#20998;62&#20010;&#33521;&#25991;&#25925;&#20107;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;ChiSCor&#30340;&#32534;&#21046;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#26410;&#26469;&#24037;&#20316;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#19977;&#20010;&#31616;&#30701;&#30340;&#26696;&#20363;&#30740;&#31350;&#65306;i&#65289;&#25105;&#20204;&#23637;&#31034;&#25925;&#20107;&#30340;&#21477;&#27861;&#22797;&#26434;&#24230;&#22312;&#20799;&#31461;&#30340;&#24180;&#40836;&#20043;&#38388;&#38750;&#24120;&#31283;&#23450;&#65307;ii&#65289;&#25105;&#20204;&#25193;&#23637;&#20102;&#20851;&#20110;&#33258;&#30001;&#35328;&#35770;&#20013;Zipfian&#20998;&#24067;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;ChiSCor&#26381;&#20174;Zipf&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this resource paper we release ChiSCor, a new corpus containing 619 fantasy stories, told freely by 442 Dutch children aged 4-12. ChiSCor was compiled for studying how children render character perspectives, and unravelling language and cognition in development, with computational tools. Unlike existing resources, ChiSCor's stories were produced in natural contexts, in line with recent calls for more ecologically valid datasets. ChiSCor hosts text, audio, and annotations for character complexity and linguistic complexity. Additional metadata (e.g. education of caregivers) is available for one third of the Dutch children. ChiSCor also includes a small set of 62 English stories. This paper details how ChiSCor was compiled and shows its potential for future work with three brief case studies: i) we show that the syntactic complexity of stories is strikingly stable across children's ages; ii) we extend work on Zipfian distributions in free speech and show that ChiSCor obeys Zipf's law c
&lt;/p&gt;</description></item><item><title>Erato&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#33258;&#21160;&#21270;&#35780;&#20272;&#35799;&#27468;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#23558;&#20154;&#31867;&#21019;&#20316;&#30340;&#35799;&#27468;&#19982;&#33258;&#21160;&#29983;&#25104;&#30340;&#35799;&#27468;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#20851;&#38190;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.20326</link><description>&lt;p&gt;
Erato:&#33258;&#21160;&#21270;&#35799;&#27468;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Erato: Automatizing Poetry Evaluation. (arXiv:2310.20326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20326
&lt;/p&gt;
&lt;p&gt;
Erato&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#33258;&#21160;&#21270;&#35780;&#20272;&#35799;&#27468;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#23558;&#20154;&#31867;&#21019;&#20316;&#30340;&#35799;&#27468;&#19982;&#33258;&#21160;&#29983;&#25104;&#30340;&#35799;&#27468;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#20851;&#38190;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Erato&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#35799;&#27468;&#33258;&#21160;&#21270;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#30001;&#35799;&#27468;&#29983;&#25104;&#31995;&#32479;&#29983;&#25104;&#30340;&#35799;&#27468;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;Erato&#30340;&#33021;&#21147;&#21644;&#20854;&#28508;&#21147;&#25193;&#23637;&#12290;&#20351;&#29992;Erato&#65292;&#25105;&#20204;&#23545;&#27604;&#20102;&#20154;&#31867;&#21019;&#20316;&#30340;&#35799;&#27468;&#19982;&#33258;&#21160;&#29983;&#25104;&#30340;&#35799;&#27468;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35782;&#21035;&#20851;&#38190;&#24046;&#24322;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#20195;&#30721;&#21644;&#36719;&#20214;&#22312;GNU GPLv3&#35768;&#21487;&#19979;&#20813;&#36153;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Erato, a framework designed to facilitate the automated evaluation of poetry, including that generated by poetry generation systems. Our framework employs a diverse set of features, and we offer a brief overview of Erato's capabilities and its potential for expansion. Using Erato, we compare and contrast human-authored poetry with automatically-generated poetry, demonstrating its effectiveness in identifying key differences. Our implementation code and software are freely available under the GNU GPLv3 license.
&lt;/p&gt;</description></item><item><title>FA&#22242;&#38431;&#21442;&#21152;&#20102;NTCIR-17 UFO&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;ELECTRA&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#34920;&#26684;&#20013;&#26377;&#20215;&#20540;&#25968;&#25454;&#30340;&#25552;&#21462;&#65292;&#36798;&#21040;&#20102;93.43%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#25490;&#34892;&#27036;&#19978;&#33719;&#24471;&#31532;&#20108;&#21517;&#12290;&#22312;TTRE&#20219;&#21153;&#20013;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#25991;&#26412;&#21644;&#34920;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.20322</link><description>&lt;p&gt;
NTCIR-17 UFO&#20219;&#21153;&#20013;&#30340;FA&#22242;&#38431;&#65288;arXiv:2310.20322v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
FA Team at the NTCIR-17 UFO Task. (arXiv:2310.20322v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20322
&lt;/p&gt;
&lt;p&gt;
FA&#22242;&#38431;&#21442;&#21152;&#20102;NTCIR-17 UFO&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;ELECTRA&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#34920;&#26684;&#20013;&#26377;&#20215;&#20540;&#25968;&#25454;&#30340;&#25552;&#21462;&#65292;&#36798;&#21040;&#20102;93.43%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#25490;&#34892;&#27036;&#19978;&#33719;&#24471;&#31532;&#20108;&#21517;&#12290;&#22312;TTRE&#20219;&#21153;&#20013;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#25991;&#26412;&#21644;&#34920;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
FA&#22242;&#38431;&#21442;&#21152;&#20102;NTCIR-17 UFO&#30340;&#34920;&#26684;&#25968;&#25454;&#25552;&#21462;&#65288;TDE&#65289;&#21644;&#25991;&#26412;&#21040;&#34920;&#26684;&#20851;&#31995;&#25552;&#21462;&#65288;TTRE&#65289;&#20219;&#21153;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#25105;&#20204;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23448;&#26041;&#32467;&#26524;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#21033;&#29992;&#22522;&#20110;ELECTRA&#35821;&#35328;&#27169;&#22411;&#30340;&#21508;&#31181;&#22686;&#24378;&#25216;&#26415;&#20174;&#34920;&#26684;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#21162;&#21147;&#23548;&#33268;&#20102;93.43%&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;TDE&#20934;&#30830;&#29575;&#65292;&#24182;&#20351;&#25105;&#20204;&#22312;&#25490;&#34892;&#27036;&#19978;&#25490;&#21517;&#31532;&#20108;&#12290;&#36825;&#19968;&#21331;&#36234;&#25104;&#23601;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;TTRE&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#25991;&#26412;&#21644;&#34920;&#26684;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#20851;&#31995;&#65292;&#24182;&#39564;&#35777;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The FA team participated in the Table Data Extraction (TDE) and Text-to-Table Relationship Extraction (TTRE) tasks of the NTCIR-17 Understanding of Non-Financial Objects in Financial Reports (UFO). This paper reports our approach to solving the problems and discusses the official results. We successfully utilized various enhancement techniques based on the ELECTRA language model to extract valuable data from tables. Our efforts resulted in an impressive TDE accuracy rate of 93.43 %, positioning us in second place on the Leaderboard rankings. This outstanding achievement is a testament to our proposed approach's effectiveness. In the TTRE task, we proposed the rule-based method to extract meaningful relationships between the text and tables task and confirmed the performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;11&#31181;&#22522;&#30784;&#27169;&#22411;&#21644;&#35843;&#25972;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;7-10&#23681;&#20799;&#31461;&#22312;&#39640;&#32423;&#27979;&#35797;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;GPT&#31995;&#21015;&#30340;&#35843;&#25972;&#25351;&#20196;LLMs&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#20799;&#31461;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#22522;&#30784;LLMs&#22823;&#22810;&#26080;&#27861;&#35299;&#20915;ToM&#20219;&#21153;&#65292;&#32780;&#35843;&#25972;&#25351;&#20196;&#21017;&#36890;&#36807;&#22870;&#21169;&#21512;&#20316;&#24615;&#27807;&#36890;&#26377;&#21161;&#20110;&#25552;&#21319;LLM&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20320</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24515;&#28789;&#29702;&#35770;&#65306;11&#31181;&#26368;&#26032;&#27169;&#22411;&#19982;7-10&#23681;&#20799;&#31461;&#22312;&#39640;&#32423;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests. (arXiv:2310.20320v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;11&#31181;&#22522;&#30784;&#27169;&#22411;&#21644;&#35843;&#25972;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;7-10&#23681;&#20799;&#31461;&#22312;&#39640;&#32423;&#27979;&#35797;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;GPT&#31995;&#21015;&#30340;&#35843;&#25972;&#25351;&#20196;LLMs&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#20799;&#31461;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#22522;&#30784;LLMs&#22823;&#22810;&#26080;&#27861;&#35299;&#20915;ToM&#20219;&#21153;&#65292;&#32780;&#35843;&#25972;&#25351;&#20196;&#21017;&#36890;&#36807;&#22870;&#21169;&#21512;&#20316;&#24615;&#27807;&#36890;&#26377;&#21161;&#20110;&#25552;&#21319;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#32473;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22810;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#20363;&#22914;&#29702;&#35299;&#24847;&#22270;&#21644;&#20449;&#24565;&#30340;&#29702;&#35770;&#24515;&#28789;&#65288;ToM&#65289;&#33021;&#21147;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#65292;&#20026;&#36825;&#22330;&#26032;&#20852;&#36777;&#35770;&#22686;&#21152;&#19968;&#20123;&#35777;&#25454;&#65306;&#65288;i&#65289;&#27979;&#35797;11&#20010;&#22522;&#30784;&#27169;&#22411;&#21644;&#35843;&#25972;&#25351;&#20196;&#30340;LLMs&#30340;ToM&#30456;&#20851;&#33021;&#21147;&#65292;&#36229;&#36234;&#20027;&#23548;&#30340;&#34394;&#20551;&#20449;&#24565;&#33539;&#24335;&#65292;&#21253;&#25324;&#38750;&#25991;&#23383;&#30340;&#35821;&#35328;&#20351;&#29992;&#21644;&#36882;&#24402;&#30340;&#24847;&#22270;&#65307;&#65288;ii&#65289;&#20351;&#29992;&#26032;&#32534;&#20889;&#30340;&#26631;&#20934;&#21270;&#27979;&#35797;&#29256;&#26412;&#26469;&#35780;&#20272;LLMs&#30340;&#31283;&#20581;&#24615;&#65307;&#65288;iii&#65289;&#25552;&#31034;&#24182;&#35745;&#20998;&#24320;&#25918;&#38382;&#39064;&#21644;&#23553;&#38381;&#38382;&#39064;&#65307;&#65288;iv&#65289;&#23558;LLM&#30340;&#34920;&#29616;&#19982;7-10&#23681;&#20799;&#31461;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT&#31995;&#21015;&#30340;&#35843;&#25972;&#25351;&#20196;LLMs&#22312;&#20854;&#20182;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#36890;&#24120;&#20063;&#36229;&#36807;&#20102;&#20799;&#31461;&#30340;&#34920;&#29616;&#12290;&#22522;&#30784;LLMs&#22823;&#22810;&#26080;&#27861;&#35299;&#20915;ToM&#20219;&#21153;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#19987;&#38376;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35821;&#35328;&#21644;ToM&#30340;&#30456;&#20114;&#20851;&#32852;&#24615;&#21487;&#33021;&#26377;&#21161;&#20110;&#35299;&#37322;&#20026;&#20160;&#20040;&#35843;&#25972;&#25351;&#20196;&#20250;&#22686;&#21152;LLM&#30340;&#24615;&#33021;&#65306;&#22870;&#21169;&#21512;&#20316;&#24615;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;
To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27604;&#36739;&#24615;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#20135;&#21697;&#27604;&#36739;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.20274</link><description>&lt;p&gt;
&#20174;&#27604;&#36739;&#24615;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#24863;&#20852;&#36259;&#30340;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Extracting Entities of Interest from Comparative Product Reviews. (arXiv:2310.20274v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27604;&#36739;&#24615;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#20135;&#21697;&#27604;&#36739;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21508;&#31181;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#30340;&#29992;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#20135;&#21697;&#27604;&#36739;&#20449;&#24687;&#12290;&#20219;&#20309;&#19968;&#20010;&#27604;&#36739;&#24615;&#20135;&#21697;&#35780;&#35770;&#37117;&#26377;&#19977;&#20010;&#37325;&#35201;&#30340;&#20449;&#24687;&#23454;&#20307;&#65306;&#34987;&#27604;&#36739;&#20135;&#21697;&#30340;&#21517;&#31216;&#65292;&#29992;&#25143;&#35266;&#28857;&#65288;&#35859;&#35789;&#65289;&#20197;&#21450;&#34987;&#27604;&#36739;&#30340;&#29305;&#24449;&#25110;&#26041;&#38754;&#12290;&#25152;&#26377;&#36825;&#20123;&#20449;&#24687;&#23454;&#20307;&#24444;&#27492;&#20381;&#36182;&#24182;&#21463;&#21040;&#35780;&#35770;&#35821;&#35328;&#35268;&#21017;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#20123;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#24456;&#22909;&#22320;&#36890;&#36807;LSTM&#36827;&#34892;&#25429;&#25417;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#25163;&#21160;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#24182;&#35266;&#23519;&#21040;&#20854;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning based approach to extract product comparison information out of user reviews on various e-commerce websites. Any comparative product review has three major entities of information: the names of the products being compared, the user opinion (predicate) and the feature or aspect under comparison. All these informing entities are dependent on each other and bound by the rules of the language, in the review. We observe that their inter-dependencies can be captured well using LSTMs. We evaluate our system on existing manually labeled datasets and observe out-performance over the existing Semantic Role Labeling (SRL) framework popular for this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26827;&#23616;&#25945;&#31243;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#19979;&#26827;&#30340;&#26032;&#30693;&#35782;&#26469;&#28304;&#65292;&#24320;&#21457;&#20102;LEAP&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20260</link><description>&lt;p&gt;
&#20174;&#25945;&#31243;&#20013;&#23398;&#20064;&#19979;&#26827;: &#22522;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#26827;&#23616;&#35780;&#20272;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis. (arXiv:2310.20260v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26827;&#23616;&#25945;&#31243;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#19979;&#26827;&#30340;&#26032;&#30693;&#35782;&#26469;&#28304;&#65292;&#24320;&#21457;&#20102;LEAP&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19979;&#26827;&#31574;&#30053;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20851;&#27880;&#20110;&#20351;&#29992;&#25628;&#32034;&#31639;&#27861;&#20174;&#20043;&#21069;&#30340;&#26827;&#23616;&#20013;&#23398;&#20064;&#12290;&#26827;&#23616;&#25945;&#31243;&#21253;&#21547;&#22823;&#24072;&#30340;&#30693;&#35782;&#65292;&#35299;&#37322;&#19979;&#26827;&#31574;&#30053;&#65292;&#24182;&#19988;&#30456;&#27604;&#20256;&#32479;&#30340;&#19979;&#26827;&#26041;&#27861;&#38656;&#35201;&#26356;&#23567;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26827;&#23616;&#25945;&#31243;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#26469;&#28304;&#65292;&#20197;&#20415;&#20351;&#26426;&#22120;&#23398;&#20250;&#19979;&#26827;&#65292;&#36825;&#26159;&#20197;&#21069;&#26410;&#26366;&#25506;&#32034;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;LEAP&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26032;&#39062;&#19988;&#20855;&#26377;&#32467;&#26500;&#21270;&#65288;&#26827;&#23616;&#36208;&#27861;&#21644;&#23616;&#38754;&#29366;&#24577;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#65288;&#25991;&#23383;&#25551;&#36848;&#65289;&#25968;&#25454;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#33258;&#19968;&#26412;&#21547;&#26377;1164&#20010;&#21477;&#23376;&#30340;&#26827;&#23616;&#25945;&#31243;&#65292;&#35752;&#35770;&#20102;91&#22330;&#27604;&#36187;&#30340;&#25112;&#30053;&#36208;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#23545;&#20854;&#36827;&#34892;&#26631;&#27880;&#65292;&#21363;&#23427;&#20204;&#26159;&#21542;&#35752;&#35770;&#20102;&#19968;&#27425;&#36208;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25551;&#36848;&#30340;&#36208;&#27861;&#23545;&#27599;&#20010;&#30456;&#20851;&#21477;&#23376;&#36827;&#34892;&#24773;&#24863;&#26631;&#27880;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#35757;&#32451;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning chess strategies has been investigated widely, with most studies focussing on learning from previous games using search algorithms. Chess textbooks encapsulate grandmaster knowledge, explain playing strategies and require a smaller search space compared to traditional chess agents. This paper examines chess textbooks as a new knowledge source for enabling machines to learn how to play chess -- a resource that has not been explored previously. We developed the LEAP corpus, a first and new heterogeneous dataset with structured (chess move notations and board states) and unstructured data (textual descriptions) collected from a chess textbook containing 1164 sentences discussing strategic moves from 91 games. We firstly labelled the sentences based on their relevance, i.e., whether they are discussing a move. Each relevant sentence was then labelled according to its sentiment towards the described move. We performed empirical experiments that assess the performance of various tra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;PsyCoT&#30340;&#26032;&#39062;&#20010;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#24515;&#29702;&#38382;&#21367;&#20316;&#20026;&#24605;&#32500;&#38142;&#26465;&#36827;&#34892;&#20010;&#24615;&#35782;&#21035;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#22686;&#24378;&#23545;&#20010;&#24615;&#30340;&#21512;&#29702;&#25512;&#26029;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20256</link><description>&lt;p&gt;
PsyCoT: &#23558;&#24515;&#29702;&#38382;&#21367;&#20316;&#20026;&#24378;&#22823;&#30340;&#24605;&#32500;&#38142;&#26465;&#29992;&#20110;&#20010;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection. (arXiv:2310.20256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20256
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;PsyCoT&#30340;&#26032;&#39062;&#20010;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#24515;&#29702;&#38382;&#21367;&#20316;&#20026;&#24605;&#32500;&#38142;&#26465;&#36827;&#34892;&#20010;&#24615;&#35782;&#21035;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#22686;&#24378;&#23545;&#20010;&#24615;&#30340;&#21512;&#29702;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#65292;&#22914;ChatGPT&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;remarkable&#30340;&#38646;-shot&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;LLM&#22312;&#20010;&#24615;&#26816;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21363;&#36890;&#36807;&#20889;&#20316;&#25991;&#26412;&#26469;&#35782;&#21035;&#20010;&#20307;&#30340;&#20010;&#24615;&#65292;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#21463;&#21040;&#24515;&#29702;&#38382;&#21367;&#30340;&#21551;&#21457;&#65292;&#24515;&#29702;&#38382;&#21367;&#30001;&#24515;&#29702;&#23398;&#23478;&#31934;&#24515;&#35774;&#35745;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#26377;&#38024;&#23545;&#24615;&#30340;&#39033;&#30446;&#35780;&#20272;&#20010;&#20307;&#30340;&#20010;&#24615;&#29305;&#24449;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#39033;&#30446;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#32452;&#33391;&#22909;&#32467;&#26500;&#21270;&#30340;&#24605;&#32500;&#38142;&#26465;&#36807;&#31243;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#36807;&#31243;&#65292;LLM&#21487;&#20197;&#22686;&#24378;&#20854;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#23545;&#20010;&#24615;&#30340;&#21512;&#29702;&#25512;&#26029;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20010;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;PsyCoT&#65292;&#23427;&#27169;&#20223;&#20010;&#20307;&#20197;&#22810;&#36718;&#23545;&#35805;&#26041;&#24335;&#23436;&#25104;&#24515;&#29702;&#38382;&#21367;&#30340;&#26041;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;LLM&#20316;&#20026;&#19968;&#20010;&#22312;&#25991;&#26412;&#22788;&#29702;&#26041;&#21521;&#19978;&#20855;&#26377;&#19987;&#38271;&#30340;AI&#21161;&#25163;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual's personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of well-structured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.20246</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#20013;&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#35265;&#35299;&#19982;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#36866;&#29992;&#20110;&#21333;&#35821;&#35328;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#30340;&#24378;&#22823;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20445;&#25345;&#25928;&#26524;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#21644;&#35757;&#32451;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#65288;xMR&#65289;LLM&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21033;&#29992;&#32763;&#35793;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#25351;&#23548;&#25968;&#25454;&#38598;MGSM8KInstruct&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;xMR&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;xMR LLMs&#65292;&#34987;&#21629;&#21517;&#20026;MathOctopus&#65292;&#22312;&#20960;&#27425;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#24320;&#28304;LLMs&#21644;ChatGPT&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MathOctopus-13B&#22312;MGSM&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;47.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;ChatGPT&#30340;46.3%&#12290;&#38500;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#20174;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#20013;&#21457;&#29616;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#35266;&#23519;&#21644;&#35265;&#35299;&#65306;&#65288;1&#65289;&#22312;&#22810;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26368;&#22909;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290; &#65288;2&#65289;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290; &#65288;3&#65289;&#27169;&#22411;&#23545;&#20110;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#30340;&#22788;&#29702;&#26159;&#25361;&#25112;&#30340;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#65292;&#21160;&#24577;&#26356;&#26032;&#20107;&#20214;&#34920;&#31034;&#65292;&#36890;&#36807;&#22810;&#31867;&#21035;&#23398;&#20064;&#35299;&#20915;&#20102;&#26102;&#38388;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20449;&#24687;&#20849;&#20139;&#21644;&#25968;&#25454;&#21033;&#29992;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#33521;&#25991;&#21644;&#26085;&#25991;&#25968;&#25454;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#21644;&#20256;&#36882;&#23398;&#20064;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.20236</link><description>&lt;p&gt;
&#21160;&#24577;&#26356;&#26032;&#20107;&#20214;&#34920;&#31034;&#30340;&#22810;&#31867;&#21035;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning. (arXiv:2310.20236v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#65292;&#21160;&#24577;&#26356;&#26032;&#20107;&#20214;&#34920;&#31034;&#65292;&#36890;&#36807;&#22810;&#31867;&#21035;&#23398;&#20064;&#35299;&#20915;&#20102;&#26102;&#38388;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20449;&#24687;&#20849;&#20139;&#21644;&#25968;&#25454;&#21033;&#29992;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#33521;&#25991;&#21644;&#26085;&#25991;&#25968;&#25454;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#21644;&#20256;&#36882;&#23398;&#20064;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20851;&#31995;&#20998;&#31867;&#26159;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#20004;&#20010;&#25552;&#21450;&#20043;&#38388;&#30340;&#26102;&#38388;&#38142;&#25509;&#65288;TLINK&#65289;&#20851;&#31995;&#30340;&#25104;&#23545;&#20219;&#21153;&#65292;&#21253;&#25324;&#20107;&#20214;&#12289;&#26102;&#38388;&#21644;&#25991;&#26723;&#21019;&#24314;&#26102;&#38388;&#65288;DCT&#65289;&#12290;&#23427;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;1&#65289;&#28041;&#21450;&#20849;&#21516;&#25552;&#21450;&#30340;&#20004;&#20010;TLINK&#19981;&#20849;&#20139;&#20449;&#24687;&#65307;2&#65289;&#29616;&#26377;&#27169;&#22411;&#20351;&#29992;&#29420;&#31435;&#20998;&#31867;&#22120;&#23545;&#27599;&#20010;TLINK&#31867;&#21035;&#65288;E2E&#65292;E2T&#21644;E2D&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#26080;&#27861;&#20351;&#29992;&#20840;&#37096;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#65292;&#20801;&#35768;&#22312;&#22810;&#20010;TLINK&#20013;&#31649;&#29702;&#21160;&#24577;&#20107;&#20214;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#22788;&#29702;&#19977;&#20010;TLINK&#31867;&#21035;&#65292;&#20197;&#21033;&#29992;&#20840;&#37096;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33521;&#25991;&#21644;&#26085;&#25991;&#25968;&#25454;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#21644;&#20004;&#20010;&#20256;&#36882;&#23398;&#20064;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal relation classification is a pair-wise task for identifying the relation of a temporal link (TLINK) between two mentions, i.e. event, time, and document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs involving a common mention do not share information. 2) Existing models with independent classifiers for each TLINK category (E2E, E2T, and E2D) hinder from using the whole data. This paper presents an event centric model that allows to manage dynamic event representations across multiple TLINKs. Our model deals with three TLINK categories with multi-task learning to leverage the full size of data. The experimental results show that our proposal outperforms state-of-the-art models and two transfer learning baselines on both the English and Japanese data.
&lt;/p&gt;</description></item><item><title>GPT-4&#36890;&#36807;&#20102;&#20844;&#24320;&#30340;&#22312;&#32447;&#22270;&#28789;&#27979;&#35797;&#20013;&#30340;41%&#30340;&#28216;&#25103;&#65292;&#22312;&#35821;&#35328;&#39118;&#26684;&#21644;&#31038;&#20250;&#24773;&#24863;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#36739;&#20339;&#65292;&#20294;&#20173;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#27700;&#24179;&#12290;&#22270;&#28789;&#27979;&#35797;&#20173;&#28982;&#26159;&#35780;&#20272;&#33258;&#28982;&#20132;&#27969;&#21644;&#27450;&#39575;&#30340;&#30456;&#20851;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20216</link><description>&lt;p&gt;
GPT-4 &#26159;&#21542;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does GPT-4 Pass the Turing Test?. (arXiv:2310.20216v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20216
&lt;/p&gt;
&lt;p&gt;
GPT-4&#36890;&#36807;&#20102;&#20844;&#24320;&#30340;&#22312;&#32447;&#22270;&#28789;&#27979;&#35797;&#20013;&#30340;41%&#30340;&#28216;&#25103;&#65292;&#22312;&#35821;&#35328;&#39118;&#26684;&#21644;&#31038;&#20250;&#24773;&#24863;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#36739;&#20339;&#65292;&#20294;&#20173;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#27700;&#24179;&#12290;&#22270;&#28789;&#27979;&#35797;&#20173;&#28982;&#26159;&#35780;&#20272;&#33258;&#28982;&#20132;&#27969;&#21644;&#27450;&#39575;&#30340;&#30456;&#20851;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#24320;&#30340;&#22312;&#32447;&#22270;&#28789;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102; GPT-4&#12290;&#22312;&#34920;&#29616;&#26368;&#22909;&#30340; GPT-4 &#25552;&#31034;&#20013;&#65292;&#22312; 41% &#30340;&#28216;&#25103;&#20013;&#36890;&#36807;&#20102;&#27979;&#35797;&#65292;&#36229;&#36807;&#20102; ELIZA&#65288;27%&#65289;&#21644; GPT-3.5&#65288;14%&#65289;&#35774;&#23450;&#30340;&#22522;&#20934;&#65292;&#20294;&#36824;&#19981;&#22914;&#20154;&#31867;&#21442;&#19982;&#32773;&#65288;63%&#65289;&#30340;&#26426;&#20250;&#21644;&#22522;&#20934;&#12290;&#21442;&#19982;&#32773;&#30340;&#20915;&#31574;&#20027;&#35201;&#22522;&#20110;&#35821;&#35328;&#39118;&#26684;&#65288;35%&#65289;&#21644;&#31038;&#20250;&#24773;&#24863;&#29305;&#24449;&#65288;27%&#65289;&#65292;&#25903;&#25345;&#26234;&#33021;&#19981;&#36275;&#20197;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#30340;&#35266;&#28857;&#12290;&#21442;&#19982;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#29305;&#24449;&#65292;&#21253;&#25324;&#25945;&#32946;&#27700;&#24179;&#21644;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29087;&#24713;&#24230;&#65292;&#24182;&#19981;&#33021;&#39044;&#27979;&#34987;&#35782;&#21035;&#29575;&#65292;&#36825;&#34920;&#26126;&#21363;&#20351;&#26159;&#28145;&#20837;&#20102;&#35299;&#31995;&#32479;&#24182;&#39057;&#32321;&#19982;&#20854;&#20132;&#20114;&#30340;&#20154;&#65292;&#20063;&#20250;&#23481;&#26131;&#34987;&#27450;&#39575;&#12290;&#23613;&#31649;&#22270;&#28789;&#27979;&#35797;&#20316;&#20026;&#26234;&#33021;&#30340;&#27979;&#35797;&#20855;&#26377;&#24050;&#30693;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#22312;&#35780;&#20272;&#33258;&#28982;&#20132;&#27969;&#21644;&#27450;&#39575;&#26041;&#38754;&#20173;&#28982;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;&#20855;&#26377;&#20882;&#20805;&#20154;&#31867;&#33021;&#21147;&#30340; AI &#27169;&#22411;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#24191;&#27867;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#31574;&#30053;&#21644;&#26631;&#20934;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). Participants' decisions were based mainly on linguistic style (35%) and socio-emotional traits (27%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria fo
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20204</link><description>&lt;p&gt;
&#21033;&#29992;&#36817;&#26080;&#38480;&#21382;&#21490;&#30340;&#36890;&#29992;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History. (arXiv:2310.20204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20204
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#24320;&#21457;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#27515;&#20129;&#39044;&#27979;&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#19987;&#23478;&#24847;&#35265;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#35843;&#25972;&#35266;&#27979;&#31383;&#21475;&#22823;&#23567;&#12290;&#36825;&#32473;&#19987;&#23478;&#24102;&#26469;&#36127;&#25285;&#24182;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#36896;&#25104;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65288;REMed&#65289;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;REMed&#21487;&#20197;&#22522;&#26412;&#35780;&#20272;&#26080;&#38480;&#37327;&#30340;&#20020;&#24202;&#20107;&#20214;&#65292;&#36873;&#25321;&#30456;&#20851;&#30340;&#20107;&#20214;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#23454;&#26102;&#35266;&#23519;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;27&#20010;&#20020;&#24202;&#20219;&#21153;&#21644;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;EHR&#25968;&#25454;&#38598;&#30340;&#29420;&#31435;&#38431;&#21015;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#29305;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;REMed&#20248;&#20110;&#20854;&#20182;&#29616;&#20195;&#26550;&#26500;&#65292;&#23427;&#20204;&#26088;&#22312;&#22788;&#29702;&#23613;&#21487;&#33021;&#22810;&#30340;&#20107;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;REMed&#30340;&#20559;&#22909;&#19982;&#21307;&#23398;&#19987;&#23478;&#30340;&#20559;&#22909;&#23494;&#20999;&#30456;&#20284;&#12290;&#25105;&#20204;&#26399;&#26395;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#26174;&#33879;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;EVA&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#35270;&#39057;&#29255;&#27573;&#21644;&#30456;&#24212;&#30340;&#26085;&#33521;&#21644;&#20013;&#33521;&#24179;&#34892;&#23383;&#24149;&#23545;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#35270;&#35273;&#20449;&#24687;&#26080;&#27861;&#29983;&#25104;&#36866;&#24403;&#32763;&#35793;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#36873;&#25321;&#24615;&#27880;&#24847;&#27169;&#24335;&#30340;SAFA MMT&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.20201</link><description>&lt;p&gt;
&#35270;&#39057;&#26377;&#30410;&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Video-Helpful Multimodal Machine Translation. (arXiv:2310.20201v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20201
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;EVA&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#35270;&#39057;&#29255;&#27573;&#21644;&#30456;&#24212;&#30340;&#26085;&#33521;&#21644;&#20013;&#33521;&#24179;&#34892;&#23383;&#24149;&#23545;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#35270;&#35273;&#20449;&#24687;&#26080;&#27861;&#29983;&#25104;&#36866;&#24403;&#32763;&#35793;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#36873;&#25321;&#24615;&#27880;&#24847;&#27169;&#24335;&#30340;SAFA MMT&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#25968;&#25454;&#38598;&#30001;&#22270;&#20687;&#21644;&#35270;&#39057;&#23383;&#24149;&#25110;&#25945;&#23398;&#35270;&#39057;&#23383;&#24149;&#32452;&#25104;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23569;&#21253;&#21547;&#35821;&#35328;&#30340;&#27495;&#20041;&#65292;&#20351;&#24471;&#35270;&#35273;&#20449;&#24687;&#22312;&#29983;&#25104;&#36866;&#24403;&#30340;&#32763;&#35793;&#26041;&#38754;&#19981;&#36215;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#31946;&#23383;&#24149;&#25968;&#25454;&#38598;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#35270;&#39057;&#19981;&#19968;&#23450;&#26377;&#21161;&#20110;&#28040;&#38500;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;EVA&#65288;&#29992;&#20110;&#27169;&#31946;&#23383;&#24149;&#32763;&#35793;&#30340;&#24191;&#27867;&#35757;&#32451;&#38598;&#21644;&#26377;&#21161;&#20110;&#35270;&#39057;&#30340;&#35780;&#20272;&#38598;&#65289;&#65292;&#19968;&#20010;&#21253;&#21547;852k&#26085;&#33521;&#65288;Ja-En&#65289;&#24179;&#34892;&#23383;&#24149;&#23545;&#12289;520k&#20013;&#33521;&#65288;Zh-En&#65289;&#24179;&#34892;&#23383;&#24149;&#23545;&#21644;&#26469;&#33258;&#30005;&#24433;&#21644;&#30005;&#35270;&#21095;&#30340;&#30456;&#24212;&#35270;&#39057;&#29255;&#27573;&#30340;MMT&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#24191;&#27867;&#30340;&#35757;&#32451;&#38598;&#22806;&#65292;EVA&#36824;&#21253;&#21547;&#19968;&#20010;&#35270;&#39057;&#26377;&#30410;&#30340;&#35780;&#20272;&#38598;&#65292;&#20854;&#20013;&#23383;&#24149;&#26159;&#27169;&#31946;&#30340;&#65292;&#24182;&#19988;&#35270;&#39057;&#33021;&#22815;&#20445;&#35777;&#26377;&#21161;&#20110;&#28040;&#38500;&#27495;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36873;&#25321;&#24615;&#27880;&#24847;&#27169;&#24335;&#30340;SAFA MMT&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing multimodal machine translation (MMT) datasets consist of images and video captions or instructional video subtitles, which rarely contain linguistic ambiguity, making visual information ineffective in generating appropriate translations. Recent work has constructed an ambiguous subtitles dataset to alleviate this problem but is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA (Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En) parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In addition to the extensive training set, EVA contains a video-helpful evaluation set in which subtitles are ambiguous, and videos are guaranteed helpful for disambiguation. Furthermore, we propose SAFA, an MMT model based on the Selective Attention mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#24815;&#29992;&#35821;&#22659;&#20013;&#29983;&#25104;&#24310;&#32493;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#23383;&#38754;&#21644;&#24815;&#29992;&#19978;&#19979;&#25991;&#20013;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#35821;&#35328;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20195</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#31181;&#24815;&#29992;&#35821;&#22659;&#19979;&#29983;&#25104;&#24310;&#32493;
&lt;/p&gt;
&lt;p&gt;
Generating Continuations in Multilingual Idiomatic Contexts. (arXiv:2310.20195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#24815;&#29992;&#35821;&#22659;&#20013;&#29983;&#25104;&#24310;&#32493;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#23383;&#38754;&#21644;&#24815;&#29992;&#19978;&#19979;&#25991;&#20013;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#35821;&#35328;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#24815;&#29992;&#25110;&#23383;&#38754;&#22810;&#35789;&#34920;&#36798;&#26159;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#20309;&#35821;&#35328;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#20026;&#21253;&#21547;&#24815;&#29992;&#65288;&#25110;&#23383;&#38754;&#65289;&#34920;&#36798;&#30340;&#21465;&#36848;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#24310;&#32493;&#30340;&#20219;&#21153;&#21487;&#20197;&#35753;&#25105;&#20204;&#27979;&#35797;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#29702;&#35299;&#38750;&#32452;&#21512;&#24615;&#27604;&#21947;&#25991;&#26412;&#30340;&#32420;&#32454;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#35821;&#35328;&#65288;&#33521;&#35821;&#21644;&#33889;&#33796;&#29273;&#35821;&#65289;&#30340;&#25968;&#25454;&#38598;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#35774;&#32622;&#19979;&#65288;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#24494;&#35843;&#65289;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#23383;&#38754;&#19978;&#19979;&#25991;&#30340;&#24310;&#32493;&#26102;&#30053;&#20248;&#20110;&#24815;&#29992;&#19978;&#19979;&#25991;&#65292;&#20294;&#24046;&#36317;&#24456;&#23567;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#20013;&#30740;&#31350;&#30340;&#27169;&#22411;&#22312;&#20004;&#31181;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#21516;&#26679;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#29983;&#25104;&#27169;&#22411;&#22312;&#25191;&#34892;&#27492;&#20219;&#21153;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to process idiomatic or literal multiword expressions is a crucial aspect of understanding and generating any language. The task of generating contextually relevant continuations for narratives containing idiomatic (or literal) expressions can allow us to test the ability of generative language models (LMs) in understanding nuanced language containing non-compositional figurative text. We conduct a series of experiments using datasets in two distinct languages (English and Portuguese) under three different training settings (zero-shot, few-shot, and fine-tuned). Our results suggest that the models are only slightly better at generating continuations for literal contexts than idiomatic contexts, with exceedingly small margins. Furthermore, the models studied in this work perform equally well across both languages, indicating the robustness of generative models in performing this task.
&lt;/p&gt;</description></item><item><title>DIVKNOWQA&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;LLMs&#22312;&#36830;&#25509;&#24322;&#26500;&#30693;&#35782;&#28304;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.20170</link><description>&lt;p&gt;
DIVKNOWQA: &#36890;&#36807;&#30693;&#35782;&#24211;&#21644;&#25991;&#26412;&#36827;&#34892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#26469;&#35780;&#20272;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text. (arXiv:2310.20170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20170
&lt;/p&gt;
&lt;p&gt;
DIVKNOWQA&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;LLMs&#22312;&#36830;&#25509;&#24322;&#26500;&#30693;&#35782;&#28304;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#26159;&#24403;&#20165;&#20165;&#20381;&#36182;&#20854;&#20869;&#37096;&#30693;&#35782;&#26102;&#65292;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#23588;&#20854;&#26159;&#22312;&#22238;&#31572;&#38656;&#35201;&#23569;&#35265;&#30693;&#35782;&#30340;&#38382;&#39064;&#26102;&#12290;&#22522;&#20110;&#26816;&#32034;&#30340;LLMs&#24050;&#32463;&#25104;&#20026;&#23558;LLMs&#29282;&#22266;&#26681;&#26893;&#20110;&#22806;&#37096;&#30693;&#35782;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20027;&#35201;&#24378;&#35843;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#65292;&#22240;&#20026;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#25552;&#31034;&#20013;&#12290;&#24403;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#65289;&#26102;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20250;&#23558;&#20854;&#31616;&#21270;&#20026;&#33258;&#28982;&#25991;&#26412;&#65292;&#24573;&#30053;&#20102;&#24213;&#23618;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#39046;&#22495;&#20013;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#30340;&#32570;&#21475;&#65292;&#21363;&#32570;&#20047;&#19968;&#20010;&#30495;&#23454;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#23558;LLMs&#19982;&#24322;&#36136;&#30693;&#35782;&#28304;&#65288;&#22914;&#30693;&#35782;&#24211;&#21644;&#25991;&#26412;&#65289;&#36830;&#25509;&#36215;&#26469;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#32570;&#21475;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#38656;&#35201;&#26816;&#32034;&#20449;&#24687;&#30340;&#20004;&#36339;&#22810;&#28304;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving informat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAR-meets-RAG&#33539;&#24335;&#65292;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#26816;&#32034;&#21644;&#37325;&#20889;&#38454;&#27573;&#65292;&#20811;&#26381;&#20102;&#38646;&#26679;&#26412;&#20449;&#24687;&#26816;&#32034;&#20013;&#29616;&#26377;&#33539;&#24335;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.20158</link><description>&lt;p&gt;
GAR-meets-RAG&#33539;&#24335;&#29992;&#20110;&#38646;&#26679;&#26412;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval. (arXiv:2310.20158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20158
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAR-meets-RAG&#33539;&#24335;&#65292;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#26816;&#32034;&#21644;&#37325;&#20889;&#38454;&#27573;&#65292;&#20811;&#26381;&#20102;&#38646;&#26679;&#26412;&#20449;&#24687;&#26816;&#32034;&#20013;&#29616;&#26377;&#33539;&#24335;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#26597;&#35810;&#21644;&#19968;&#20010;&#25991;&#26723;&#35821;&#26009;&#24211;&#65292;&#20449;&#24687;&#26816;&#32034;(IR)&#20219;&#21153;&#26159;&#36755;&#20986;&#19968;&#20010;&#30456;&#20851;&#25991;&#26723;&#30340;&#25490;&#21517;&#21015;&#34920;&#12290;&#32467;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#22522;&#20110;&#23884;&#20837;&#30340;&#26816;&#32034;&#27169;&#22411;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#38646;&#26679;&#26412;&#26816;&#32034;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#21363;&#26080;&#27861;&#35775;&#38382;&#30446;&#26631;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#20854;&#20013;&#20004;&#20010;&#27969;&#34892;&#30340;&#33539;&#24335;&#26159;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;(GAR)&#25110;GAR&#65288;&#20026;&#26597;&#35810;&#29983;&#25104;&#38468;&#21152;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#26816;&#32034;&#65289;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#25110;RAG&#65288;&#23558;&#30456;&#20851;&#25991;&#26723;&#20316;&#20026;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#28982;&#21518;&#29983;&#25104;&#31572;&#26696;&#65289;&#12290;&#36825;&#20123;&#33539;&#24335;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;(i)&#22312;&#38646;&#26679;&#26412;&#35774;&#23450;&#20013;&#24456;&#38590;&#33719;&#24471;&#30340;&#39640;&#21484;&#22238;&#26816;&#32034;&#27169;&#22411;&#21644;(ii)&#36890;&#24120;&#38656;&#35201;&#33391;&#22909;&#21021;&#22987;&#21270;&#30340;&#39640;&#31934;&#30830;&#24230;(&#37325;&#26032;)&#25490;&#24207;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAR-meets-RAG&#24490;&#29615;&#20844;&#24335;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#33539;&#24335;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;GAR&#21644;RAG&#38454;&#27573;&#30340;&#36845;&#20195;&#25913;&#36827;&#26816;&#32034;(&#36890;&#36807;GAR)&#21644;&#37325;&#20889;(&#36890;&#36807;RAG)&#38454;&#27573;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a query and a document corpus, the information retrieval (IR) task is to output a ranked list of relevant documents. Combining large language models (LLMs) with embedding-based retrieval models, recent work shows promising results on the zero-shot retrieval problem, i.e., no access to labeled data from the target domain. Two such popular paradigms are generation-augmented retrieval or GAR (generate additional context for the query and then retrieve), and retrieval-augmented generation or RAG (retrieve relevant documents as context and then generate answers). The success of these paradigms hinges on (i) high-recall retrieval models, which are difficult to obtain in the zero-shot setting, and (ii) high-precision (re-)ranking models which typically need a good initialization. In this work, we propose a novel GAR-meets-RAG recurrence formulation that overcomes the challenges of existing paradigms. Our method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#22810;&#37325;&#20445;&#30495;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#24320;&#21457;&#23567;&#22411;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24179;&#34913;&#20302;&#20445;&#30495;&#24230;&#33258;&#21160;&#27880;&#37322;&#21644;&#39640;&#20445;&#30495;&#24230;&#20154;&#31867;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27880;&#37322;&#22810;&#26679;&#24615;&#21644;&#20449;&#24687;&#24615;&#30340;&#26597;&#35810;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.20153</link><description>&lt;p&gt;
&#19982;&#31232;&#30095;&#20154;&#31867;&#30417;&#30563;&#30456;&#36866;&#24212;&#30340;&#25104;&#26412;&#26377;&#25928;&#30340;&#20132;&#20114;&#22810;&#37325;&#20445;&#30495;&#24230;&#23398;&#20064;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision. (arXiv:2310.20153v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#22810;&#37325;&#20445;&#30495;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#24320;&#21457;&#23567;&#22411;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24179;&#34913;&#20302;&#20445;&#30495;&#24230;&#33258;&#21160;&#27880;&#37322;&#21644;&#39640;&#20445;&#30495;&#24230;&#20154;&#31867;&#27880;&#37322;&#65292;&#20197;&#26368;&#22823;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27880;&#37322;&#22810;&#26679;&#24615;&#21644;&#20449;&#24687;&#24615;&#30340;&#26597;&#35810;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#37096;&#32626;&#26102;&#30340;&#24040;&#22823;&#35268;&#27169;&#12289;&#26131;&#21463;&#38169;&#35823;&#20449;&#24687;&#24433;&#21709;&#20197;&#21450;&#39640;&#26114;&#30340;&#25968;&#25454;&#27880;&#37322;&#25104;&#26412;&#65292;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#22810;&#37325;&#20445;&#30495;&#24230;&#23398;&#20064;&#65288;IMFL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#24320;&#21457;&#23567;&#22411;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#24494;&#35843;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#22810;&#37325;&#20445;&#30495;&#24230;&#23398;&#20064;&#38382;&#39064;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#24179;&#34913;&#20302;&#20445;&#30495;&#24230;&#33258;&#21160;LLM&#27880;&#37322;&#21644;&#39640;&#20445;&#30495;&#24230;&#20154;&#31867;&#27880;&#37322;&#20197;&#26368;&#22823;&#21270;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#20339;&#33719;&#21462;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27880;&#37322;&#22810;&#26679;&#24615;&#21644;&#20449;&#24687;&#24615;&#30340;&#25506;&#32034;-&#24320;&#21457;&#26597;&#35810;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#21019;&#26032;&#35774;&#35745;&#65306;1&#65289;&#20174;&#20154;&#31867;&#27880;&#37322;&#26679;&#26412;&#20013;&#36873;&#25321;&#19978;&#19979;&#25991;&#20363;&#23376;&#26469;&#25913;&#36827;LLM&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#19968;&#33268;&#24615;&#23547;&#27714;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#20027;&#35201;&#20351;&#29992;&#24179;&#22343;&#31574;&#30053;&#36827;&#34892;&#19968;&#33268;&#24615;&#23547;&#27714;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#26234;&#33021;&#20307;&#20010;&#24615;&#21644;&#32593;&#32476;&#25299;&#25169;&#23545;&#21327;&#21830;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.20151</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#19968;&#33268;&#24615;&#23547;&#27714;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Consensus Seeking via Large Language Models. (arXiv:2310.20151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#19968;&#33268;&#24615;&#23547;&#27714;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#20027;&#35201;&#20351;&#29992;&#24179;&#22343;&#31574;&#30053;&#36827;&#34892;&#19968;&#33268;&#24615;&#23547;&#27714;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#26234;&#33021;&#20307;&#20010;&#24615;&#21644;&#32593;&#32476;&#25299;&#25169;&#23545;&#21327;&#21830;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#21327;&#20316;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#19968;&#33268;&#24615;&#23547;&#27714;&#12290;&#24403;&#22810;&#20010;&#26234;&#33021;&#20307;&#19968;&#36215;&#24037;&#20316;&#26102;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#26234;&#33021;&#20307;&#38388;&#30340;&#21327;&#21830;&#36798;&#25104;&#19968;&#33268;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#20010;&#19968;&#33268;&#24615;&#23547;&#27714;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#26159;&#19968;&#20010;&#25968;&#20540;&#65292;&#24182;&#19988;&#23427;&#20204;&#36890;&#36807;&#30456;&#20114;&#21327;&#21830;&#26469;&#36798;&#25104;&#19968;&#33268;&#20540;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#27809;&#26377;&#26126;&#30830;&#25351;&#23548;&#24212;&#37319;&#29992;&#21738;&#31181;&#31574;&#30053;&#26102;&#65292;LLM&#39537;&#21160;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#20351;&#29992;&#24179;&#22343;&#31574;&#30053;&#36827;&#34892;&#19968;&#33268;&#24615;&#23547;&#27714;&#65292;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#20598;&#23572;&#20250;&#20351;&#29992;&#20854;&#20182;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#26234;&#33021;&#20307;&#20010;&#24615;&#21644;&#32593;&#32476;&#25299;&#25169;&#23545;&#21327;&#21830;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#21457;&#29616;&#26377;&#26395;&#20026;&#29702;&#35299;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36951;&#24536;&#26694;&#26550;&#26469;&#22788;&#29702;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#25968;&#25454;&#20445;&#25252;&#36829;&#35268;&#12290;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#36951;&#24536;&#23618;&#21040;transformers&#20013;&#65292;&#24182;&#20351;&#29992;&#26377;&#36873;&#25321;&#30340;&#24072;&#29983;&#30446;&#26631;&#23398;&#20064;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#21024;&#38500;&#25968;&#25454;&#21518;&#26377;&#25928;&#22320;&#26356;&#26032;LLMs&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20150</link><description>&lt;p&gt;
&#24536;&#35760;&#20320;&#24819;&#24536;&#35760;&#30340;&#65306;LLMs&#30340;&#39640;&#25928;&#36951;&#24536;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unlearn What You Want to Forget: Efficient Unlearning for LLMs. (arXiv:2310.20150v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36951;&#24536;&#26694;&#26550;&#26469;&#22788;&#29702;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#25968;&#25454;&#20445;&#25252;&#36829;&#35268;&#12290;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#36951;&#24536;&#23618;&#21040;transformers&#20013;&#65292;&#24182;&#20351;&#29992;&#26377;&#36873;&#25321;&#30340;&#24072;&#29983;&#30446;&#26631;&#23398;&#20064;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#21024;&#38500;&#25968;&#25454;&#21518;&#26377;&#25928;&#22320;&#26356;&#26032;LLMs&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#35760;&#24518;&#21508;&#31181;&#25991;&#26412;&#25968;&#25454;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#38754;&#20020;&#38544;&#31169;&#38382;&#39064;&#21644;&#25968;&#25454;&#20445;&#25252;&#35268;&#23450;&#30340;&#36829;&#35268;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#25439;&#23475;&#39044;&#27979;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#36731;&#26494;&#22320;&#20174;&#36825;&#20123;&#27169;&#22411;&#20013;&#21024;&#38500;&#19982;&#20010;&#20154;&#29992;&#25143;&#30456;&#20851;&#30340;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36951;&#24536;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#26377;&#36873;&#25321;&#30340;&#24072;&#29983;&#30446;&#26631;&#30340;&#36731;&#37327;&#32423;&#36951;&#24536;&#23618;&#21040;transformers&#20013;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#21024;&#38500;&#21518;&#26377;&#25928;&#22320;&#26356;&#26032;LLMs&#65292;&#32780;&#26080;&#38656;&#23545;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#34701;&#21512;&#26426;&#21046;&#65292;&#20197;&#26377;&#25928;&#22320;&#32452;&#21512;&#19981;&#21516;&#30340;&#36951;&#24536;&#23618;&#65292;&#20197;&#22788;&#29702;&#19968;&#31995;&#21015;&#30340;&#36951;&#24536;&#25805;&#20316;&#12290;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our 
&lt;/p&gt;</description></item><item><title>EELBERT&#26159;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#23454;&#29616;&#24494;&#22411;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20934;&#30830;&#24615;&#22238;&#24402;&#21644;&#26174;&#33879;&#30340;&#27169;&#22411;&#23610;&#23544;&#32553;&#23567;&#12290;&#26368;&#23567;&#30340;&#27169;&#22411;UNO-EELBERT&#22312;GLUE&#24471;&#20998;&#19978;&#19982;&#23436;&#20840;&#35757;&#32451;&#30340;BERT-tiny&#30456;&#24046;4%&#65292;&#24182;&#19988;&#20307;&#31215;&#21482;&#26377;&#20854;15&#20493;&#20043;&#19968;&#65288;1.2MB&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.20144</link><description>&lt;p&gt;
EELBERT:&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#23454;&#29616;&#24494;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EELBERT: Tiny Models through Dynamic Embeddings. (arXiv:2310.20144v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20144
&lt;/p&gt;
&lt;p&gt;
EELBERT&#26159;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#23454;&#29616;&#24494;&#22411;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20934;&#30830;&#24615;&#22238;&#24402;&#21644;&#26174;&#33879;&#30340;&#27169;&#22411;&#23610;&#23544;&#32553;&#23567;&#12290;&#26368;&#23567;&#30340;&#27169;&#22411;UNO-EELBERT&#22312;GLUE&#24471;&#20998;&#19978;&#19982;&#23436;&#20840;&#35757;&#32451;&#30340;BERT-tiny&#30456;&#24046;4%&#65292;&#24182;&#19988;&#20307;&#31215;&#21482;&#26377;&#20854;15&#20493;&#20043;&#19968;&#65288;1.2MB&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;EELBERT&#65292;&#19968;&#31181;&#29992;&#20110;&#21387;&#32553;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;BERT&#65289;&#30340;&#26041;&#27861;&#65292;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#26368;&#23567;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#26367;&#25442;&#20026;&#21160;&#24577;&#30340;&#65292;&#21363;&#21363;&#26102;&#35745;&#31639;&#30340;&#23884;&#20837;&#23454;&#29616;&#26469;&#23454;&#29616;&#30340;&#12290;&#30001;&#20110;&#36755;&#20837;&#23884;&#20837;&#23618;&#21344;&#27169;&#22411;&#22823;&#23567;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36739;&#23567;&#30340;BERT&#21464;&#20307;&#65292;&#29992;&#23884;&#20837;&#35745;&#31639;&#20989;&#25968;&#26367;&#25442;&#35813;&#23618;&#26377;&#21161;&#20110;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#12290;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;BERT&#21464;&#20307;&#65288;EELBERT&#65289;&#19982;&#20256;&#32479;BERT&#27169;&#22411;&#30456;&#27604;&#20165;&#20855;&#26377;&#26368;&#23567;&#30340;&#22238;&#24402;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#20986;&#25105;&#20204;&#26368;&#23567;&#30340;&#27169;&#22411;UNO-EELBERT&#65292;&#20854;GLUE&#24471;&#20998;&#27604;&#23436;&#20840;&#35757;&#32451;&#30340;BERT-tiny&#39640;4&#65285;&#65292;&#21516;&#26102;&#20307;&#31215;&#23567;15&#20493;&#65288;1.2MB&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer accounts for a significant fraction of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550; DEPN&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32534;&#36753;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#31070;&#32463;&#20803;&#12290;&#36890;&#36807;&#24341;&#20837;&#38544;&#31169;&#31070;&#32463;&#20803;&#26816;&#27979;&#22120;&#21644;&#38544;&#31169;&#31070;&#32463;&#20803;&#32858;&#21512;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#31169;&#20154;&#25968;&#25454;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#24182;&#19988;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20138</link><description>&lt;p&gt;
DEPN: &#26816;&#27979;&#21644;&#32534;&#36753;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models. (arXiv:2310.20138v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550; DEPN&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32534;&#36753;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#31070;&#32463;&#20803;&#12290;&#36890;&#36807;&#24341;&#20837;&#38544;&#31169;&#31070;&#32463;&#20803;&#26816;&#27979;&#22120;&#21644;&#38544;&#31169;&#31070;&#32463;&#20803;&#32858;&#21512;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#31169;&#20154;&#25968;&#25454;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#24182;&#19988;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#20854;&#23545;&#25968;&#25454;&#35760;&#24518;&#21644;&#37325;&#22797;&#30340;&#33021;&#21147;&#24102;&#26469;&#20102;&#25968;&#25454;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#26377;&#25928;&#38477;&#20302;&#36825;&#20123;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DEPN&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32534;&#36753;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#31070;&#32463;&#20803;&#65292;&#37096;&#20998;&#21463;&#21040;&#30693;&#35782;&#31070;&#32463;&#20803;&#21644;&#27169;&#22411;&#32534;&#36753;&#30340;&#21551;&#21457;&#12290;&#22312;DEPN&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#38544;&#31169;&#31070;&#32463;&#20803;&#26816;&#27979;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#20301;&#19982;&#38544;&#31169;&#20449;&#24687;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#23427;&#20204;&#30340;&#28608;&#27963;&#35774;&#32622;&#20026;&#38646;&#26469;&#32534;&#36753;&#36825;&#20123;&#26816;&#27979;&#21040;&#30340;&#38544;&#31169;&#31070;&#32463;&#20803;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#31070;&#32463;&#20803;&#32858;&#21512;&#22120;&#65292;&#20197;&#25209;&#22788;&#29702;&#26041;&#24335;&#21435;&#38500;&#38544;&#31169;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#26377;&#25928;&#22320;&#38477;&#20302;&#31169;&#20154;&#25968;&#25454;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#25552;&#31034;&#35843;&#25972;&#65288;SPT&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#36873;&#25321;&#21512;&#36866;&#30340;&#25552;&#31034;&#23618;&#26469;&#25913;&#36827;&#25552;&#31034;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;SPT-DARTS&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#38376;&#65292;&#24182;&#25552;&#39640;&#25552;&#31034;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SPT&#26694;&#26550;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.20127</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#25552;&#31034;&#23618;&#25913;&#36827;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Improving Prompt Tuning with Learned Prompting Layers. (arXiv:2310.20127v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20127
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#25552;&#31034;&#35843;&#25972;&#65288;SPT&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#36873;&#25321;&#21512;&#36866;&#30340;&#25552;&#31034;&#23618;&#26469;&#25913;&#36827;&#25552;&#31034;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;SPT-DARTS&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#38376;&#65292;&#24182;&#25552;&#39640;&#25552;&#31034;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SPT&#26694;&#26550;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#36890;&#36807;&#22312;&#36755;&#20837;&#23884;&#20837;&#25110;&#38544;&#34255;&#29366;&#24577;&#20043;&#21069;&#28155;&#21152;&#36719;&#25552;&#31034;&#65292;&#21482;&#20248;&#21270;&#25552;&#31034;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTM&#65289;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25163;&#21160;&#36873;&#25321;&#20102;&#36828;&#38750;&#26368;&#20339;&#30340;&#25552;&#31034;&#23618;&#65292;&#24182;&#26410;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36873;&#25321;&#24615;&#25552;&#31034;&#35843;&#25972;&#65288;SPT&#65289;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#20013;&#38388;&#23618;&#25554;&#20837;&#19968;&#20010;&#30001;&#21487;&#23398;&#20064;&#27010;&#29575;&#38376;&#25511;&#21046;&#30340;&#25552;&#31034;&#26469;&#23398;&#20064;&#36873;&#25321;&#21512;&#36866;&#30340;&#25552;&#31034;&#23618;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;SPT-DARTS&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#21487;&#23398;&#20064;&#38376;&#65292;&#24182;&#25913;&#36827;&#23398;&#20064;&#25552;&#31034;&#23618;&#35774;&#32622;&#30340;&#26368;&#32456;&#25552;&#31034;&#35843;&#25972;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21313;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#21253;&#25324;&#20840;&#25968;&#25454;&#21644;&#23569;&#26679;&#26412;&#24773;&#26223;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;SPT&#26694;&#26550;&#21487;&#20197;&#27604;&#20043;&#21069;&#30340;PETuning&#22522;&#20934;&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#20351;&#29992;&#30340;&#25552;&#31034;&#23618;&#35774;&#32622;&#30456;&#24403;&#25110;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In this work, we propose a novel framework, \underline{S}elective \underline{P}rompt \underline{T}uning (SPT), that learns to select the proper prompt layers by inserting a prompt controlled by a learnable probabilistic gate at each intermediate layer. We further propose a novel bi-level optimization framework, SPT-DARTS, that can better optimize the learnable gates and improve the final prompt tuning performances of the learned prompt layer settings. We conduct extensive experiments with ten benchmark datasets under the full-data and few-shot scenarios. The results demonstrate that our SPT framework can perform better than the previous state-of-the-art PETuning baselines with comparable or fewer t
&lt;/p&gt;</description></item><item><title>Ling-CL&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#25968;&#25454;&#21644;&#29616;&#26377;&#35821;&#35328;&#22797;&#26434;&#24615;&#30693;&#35782;&#30340;&#35821;&#35328;&#35838;&#31243;&#65292;&#24110;&#21161;&#29702;&#35299;NLP&#27169;&#22411;&#23398;&#20064;&#30340;&#28508;&#22312;&#35821;&#35328;&#30693;&#35782;&#12290;&#27492;&#24037;&#20316;&#22312;&#22810;&#20010;NLP&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#30340;&#25351;&#26631;&#36873;&#25321;&#21644;&#20219;&#21153;&#25361;&#25112;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20121</link><description>&lt;p&gt;
Ling-CL:&#36890;&#36807;&#35821;&#35328;&#35838;&#31243;&#29702;&#35299;NLP&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Ling-CL: Understanding NLP Models through Linguistic Curricula. (arXiv:2310.20121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20121
&lt;/p&gt;
&lt;p&gt;
Ling-CL&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#25968;&#25454;&#21644;&#29616;&#26377;&#35821;&#35328;&#22797;&#26434;&#24615;&#30693;&#35782;&#30340;&#35821;&#35328;&#35838;&#31243;&#65292;&#24110;&#21161;&#29702;&#35299;NLP&#27169;&#22411;&#23398;&#20064;&#30340;&#28508;&#22312;&#35821;&#35328;&#30693;&#35782;&#12290;&#27492;&#24037;&#20316;&#22312;&#22810;&#20010;NLP&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#30340;&#25351;&#26631;&#36873;&#25321;&#21644;&#20219;&#21153;&#25361;&#25112;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#24515;&#29702;&#35821;&#35328;&#23398;&#21644;&#35821;&#35328;&#20064;&#24471;&#30740;&#31350;&#20013;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#34920;&#24449;&#26469;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#30340;&#35838;&#31243;&#65292;&#20197;&#29702;&#35299;&#27169;&#22411;&#22312;&#22788;&#29702;NLP&#20219;&#21153;&#26102;&#25152;&#23398;&#20064;&#30340;&#28508;&#22312;&#35821;&#35328;&#30693;&#35782;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#12289;&#29616;&#26377;&#20851;&#20110;&#35821;&#35328;&#22797;&#26434;&#24615;&#30340;&#30693;&#35782;&#20197;&#21450;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#26469;&#21046;&#23450;&#35821;&#35328;&#35838;&#31243;&#12290;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#22522;&#20934;NLP&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#25214;&#21040;&#20102;&#19968;&#32452;&#35821;&#35328;&#24230;&#37327;&#25351;&#26631;&#65292;&#36825;&#20123;&#25351;&#26631;&#21487;&#29992;&#26469;&#20102;&#35299;&#35299;&#20915;&#27599;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#25361;&#25112;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#20026;&#26410;&#26469;&#25152;&#26377;NLP&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#65292;&#20351;&#35821;&#35328;&#22797;&#26434;&#24615;&#33021;&#22815;&#22312;&#30740;&#31350;&#21644;&#24320;&#21457;&#36807;&#31243;&#20013;&#34987;&#26089;&#26399;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#20419;&#20351;&#20154;&#20204;&#23545;NLP&#20013;&#30340;&#40644;&#37329;&#26631;&#20934;&#21644;&#20844;&#24179;&#35780;&#20272;&#36827;&#34892;&#23457;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic curricula derived from data, existing knowledge about linguistic complexity, and model behavior during training. By analyzing several benchmark NLP datasets, our curriculum learning approaches identify sets of linguistic metrics (indices) that inform the challenges and reasoning required to address each task. Our work will inform future research in all NLP areas, allowing linguistic complexity to be considered early in the research and development process. In addition, our work prompts an examination of gold standards and fair evaluation in NLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#26684;&#24335;&#21270;&#31034;&#20363;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20256;&#32479;&#38382;&#39064;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.20111</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Making Large Language Models Better Data Creators. (arXiv:2310.20111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#26684;&#24335;&#21270;&#31034;&#20363;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20256;&#32479;&#38382;&#39064;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26174;&#33879;&#25552;&#21319;&#20102;&#25216;&#26415;&#27700;&#24179;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#25104;&#26412;&#12289;&#21709;&#24212;&#36895;&#24230;&#12289;&#25511;&#21046;&#33021;&#21147;&#20197;&#21450;&#38544;&#31169;&#21644;&#23433;&#20840;&#31561;&#26041;&#38754;&#30340;&#32771;&#34385;&#65292;&#23558;&#23427;&#20204;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#35757;&#32451;&#27169;&#22411;&#20173;&#28982;&#26159;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#65292;&#32780;&#36825;&#31181;&#25968;&#25454;&#33719;&#21462;&#24037;&#20316;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#20943;&#23569;&#20154;&#21147;&#24037;&#20316;&#37327;&#30340;&#25216;&#26415;&#34987;&#25552;&#20986;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;LLM&#36827;&#34892;&#25968;&#25454;&#26631;&#27880;&#25110;&#29983;&#25104;&#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#25968;&#25454;&#26631;&#27880;&#38656;&#35201;&#20180;&#32454;&#36873;&#25321;&#25968;&#25454;&#65292;&#32780;&#29983;&#25104;&#25968;&#25454;&#21017;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#21551;&#31034;&#24037;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#26684;&#24335;&#21270;&#31034;&#20363;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324;&#20256;&#32479;&#38382;&#39064;&#22330;&#26223;&#22312;&#20869;&#30340;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#20998;&#31867;&#32534;&#31243;&#35838;&#31243;&#20013;&#23398;&#29983;&#27714;&#21161;&#35831;&#27714;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20998;&#31867;&#26469;&#25552;&#39640;&#25945;&#32946;&#31995;&#32479;&#30340;&#25928;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20105</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#20998;&#31867;&#32534;&#31243;&#35838;&#31243;&#20013;&#23398;&#29983;&#27714;&#21161;&#35831;&#27714;
&lt;/p&gt;
&lt;p&gt;
Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models. (arXiv:2310.20105v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#20998;&#31867;&#32534;&#31243;&#35838;&#31243;&#20013;&#23398;&#29983;&#27714;&#21161;&#35831;&#27714;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20998;&#31867;&#26469;&#25552;&#39640;&#25945;&#32946;&#31995;&#32479;&#30340;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20998;&#31867;&#19982;&#25152;&#23547;&#27714;&#30340;&#24110;&#21161;&#31867;&#22411;&#30456;&#20851;&#30340;&#23398;&#29983;&#27714;&#21161;&#35831;&#27714;&#21487;&#20197;&#23454;&#29616;&#38024;&#23545;&#24615;&#30340;&#26377;&#25928;&#21709;&#24212;&#12290;&#33258;&#21160;&#20998;&#31867;&#36825;&#31867;&#35831;&#27714;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20284;&#20046;&#25552;&#20379;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#12289;&#32463;&#27982;&#23454;&#24800;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#21021;&#32423;&#32534;&#31243;&#35838;&#31243;&#20013;&#23545;&#23398;&#29983;&#27714;&#21161;&#35831;&#27714;&#36827;&#34892;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#22312;&#38646;-shot&#27979;&#35797;&#20013;&#65292;GPT-3.5&#21644;GPT-4&#22312;&#22823;&#22810;&#25968;&#31867;&#21035;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#24615;&#65292;&#32780;GPT-4&#22312;&#19982;&#35843;&#35797;&#30456;&#20851;&#30340;&#23376;&#31867;&#21035;&#30340;&#20998;&#31867;&#19978;&#32988;&#36807;GPT-3.5&#12290;&#23545;GPT-3.5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20197;&#33267;&#20110;&#23427;&#22312;&#21508;&#31867;&#21035;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#19978;&#25509;&#36817;&#20110;&#20004;&#20301;&#20154;&#31867;&#35780;&#20998;&#32773;&#20043;&#38388;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;LLMs&#36890;&#36807;&#33258;&#21160;&#20998;&#31867;&#23398;&#29983;&#38656;&#27714;&#26469;&#22686;&#24378;&#25945;&#32946;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate classification of student help requests with respect to the type of help being sought can enable the tailoring of effective responses. Automatically classifying such requests is non-trivial, but large language models (LLMs) appear to offer an accessible, cost-effective solution. This study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying help requests from students in an introductory programming class. In zero-shot trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories, while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests related to debugging. Fine-tuning the GPT-3.5 model improved its performance to such an extent that it approximated the accuracy and consistency across categories observed between two human raters. Overall, this study demonstrates the feasibility of using LLMs to enhance educational systems through the automated classification of student needs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#35328;&#20064;&#24471;&#30340;&#35748;&#30693;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20316;&#32773;&#35748;&#20026;&#29992;&#20110;&#35780;&#20272;&#21477;&#27861;&#33021;&#21147;&#30340;&#22522;&#20934;&#19981;&#22815;&#20005;&#26684;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#20005;&#36873;&#25968;&#25454;&#38598;&#26469;&#25506;&#32034;&#35821;&#27861;&#32467;&#26500;&#22522;&#30784;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2310.20093</link><description>&lt;p&gt;
&#35780;&#20272;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#35328;&#20064;&#24471;&#30340;&#35748;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Neural Language Models as Cognitive Models of Language Acquisition. (arXiv:2310.20093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#35328;&#20064;&#24471;&#30340;&#35748;&#30693;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20316;&#32773;&#35748;&#20026;&#29992;&#20110;&#35780;&#20272;&#21477;&#27861;&#33021;&#21147;&#30340;&#22522;&#20934;&#19981;&#22815;&#20005;&#26684;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#20005;&#36873;&#25968;&#25454;&#38598;&#26469;&#25506;&#32034;&#35821;&#27861;&#32467;&#26500;&#22522;&#30784;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#35757;&#32451;&#26041;&#24335;&#19982;&#20799;&#31461;&#35821;&#35328;&#20064;&#24471;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#20294;&#23427;&#20204;&#22312;&#35768;&#22810;&#25216;&#26415;&#20219;&#21153;&#19978;&#30340;&#25104;&#21151;&#20026;&#20854;&#20316;&#20026;&#35821;&#35328;&#31185;&#23398;&#29702;&#35770;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#35780;&#20272;LM&#30340;&#21477;&#27861;&#33021;&#21147;&#30340;&#19968;&#20123;&#20027;&#27969;&#22522;&#20934;&#21487;&#33021;&#19981;&#22815;&#20005;&#26684;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#27169;&#26495;&#30340;&#22522;&#20934;&#32570;&#20047;&#35821;&#35328;&#29702;&#35770;&#21644;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#24120;&#35265;&#30340;&#32467;&#26500;&#22810;&#26679;&#24615;&#12290;&#24403;&#20351;&#29992;&#23567;&#35268;&#27169;&#25968;&#25454;&#26469;&#27169;&#25311;&#20799;&#31461;&#35821;&#35328;&#20064;&#24471;&#26102;&#65292;&#31616;&#21333;&#30340;&#22522;&#20934;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#21305;&#37197;LM&#12290;&#25105;&#20204;&#20027;&#24352;&#20351;&#29992;&#24050;&#32463;&#36807;&#22823;&#37327;&#27597;&#35821;&#32773;&#35780;&#20272;&#36807;&#26799;&#24230;&#21487;&#25509;&#21463;&#24615;&#24182;&#35774;&#35745;&#29992;&#20110;&#25506;&#32034;&#35821;&#27861;&#32467;&#26500;&#22522;&#30784;&#30340;&#20005;&#36873;&#25968;&#25454;&#38598;&#12290;&#22312;&#20854;&#20013;&#19968;&#20010;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;LI-Adger&#19978;&#65292;LM&#35780;&#20272;&#21477;&#23376;&#30340;&#26041;&#24335;&#19982;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of neural language models (LMs) on many technological tasks has brought about their potential relevance as scientific theories of language despite some clear differences between LM training and child language acquisition. In this paper we argue that some of the most prominent benchmarks for evaluating the syntactic capacities of LMs may not be sufficiently rigorous. In particular, we show that the template-based benchmarks lack the structural diversity commonly found in the theoretical and psychological studies of language. When trained on small-scale data modeling child language acquisition, the LMs can be readily matched by simple baseline models. We advocate for the use of the readily available, carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers and are designed to probe the structural basis of grammar specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences in a way inconsistent with hu
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#31181;&#20851;&#38190;&#35789;&#20248;&#21270;&#27169;&#26495;&#25554;&#20837;&#26041;&#27861;&#65288;KOTI&#65289;&#20197;&#25913;&#21892;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20248;&#21270;&#20301;&#32622;&#21487;&#20197;&#22312;&#38646;&#23556;&#21644;&#23569;&#23556;&#35757;&#32451;&#35774;&#32622;&#20013;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20089</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#24335;&#23398;&#20064;&#30340;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#20013;&#20851;&#38190;&#35789;&#20248;&#21270;&#27169;&#26495;&#25554;&#20837;
&lt;/p&gt;
&lt;p&gt;
Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning. (arXiv:2310.20089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20089
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#31181;&#20851;&#38190;&#35789;&#20248;&#21270;&#27169;&#26495;&#25554;&#20837;&#26041;&#27861;&#65288;KOTI&#65289;&#20197;&#25913;&#21892;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20248;&#21270;&#20301;&#32622;&#21487;&#20197;&#22312;&#38646;&#23556;&#21644;&#23569;&#23556;&#35757;&#32451;&#35774;&#32622;&#20013;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#31508;&#35760;&#20998;&#31867;&#26159;&#19968;&#39033;&#24120;&#35265;&#30340;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24102;&#26377;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#24456;&#23569;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25552;&#31034;&#24335;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#23601;&#21487;&#20197;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;&#25552;&#31034;&#35774;&#35745;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#27169;&#26495;&#65288;&#21363;&#25552;&#31034;&#25991;&#26412;&#65289;&#30340;&#23450;&#20041;&#12290;&#28982;&#32780;&#65292;&#27169;&#26495;&#20301;&#32622;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#65292;&#36825;&#20284;&#20046;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#36890;&#24120;&#24456;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20851;&#38190;&#35789;&#20248;&#21270;&#27169;&#26495;&#25554;&#20837;&#26041;&#27861;&#65288;KOTI&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#20248;&#21270;&#20301;&#32622;&#22914;&#20309;&#22312;&#38646;&#23556;&#21644;&#23569;&#23556;&#35757;&#32451;&#35774;&#32622;&#20013;&#25913;&#21892;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical note classification is a common clinical NLP task. However, annotated data-sets are scarse. Prompt-based learning has recently emerged as an effective method to adapt pre-trained models for text classification using only few training examples. A critical component of prompt design is the definition of the template (i.e. prompt text). The effect of template position, however, has been insufficiently investigated. This seems particularly important in the clinical setting, where task-relevant information is usually sparse in clinical notes. In this study we develop a keyword-optimized template insertion method (KOTI) and show how optimizing position can improve performance on several clinical tasks in a zero-shot and few-shot training setting.
&lt;/p&gt;</description></item><item><title>&#20010;&#24615;&#21270;&#26159;NLP&#31995;&#32479;&#20013;&#29992;&#25143;&#20307;&#39564;&#30340;&#20851;&#38190;&#65292;&#26412;&#25991;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24635;&#32467;&#21644;&#26816;&#32034;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20219;&#21153;&#24863;&#30693;&#30340;&#24635;&#32467;&#22686;&#24378;&#20010;&#24615;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20081</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24635;&#32467;&#21644;&#26816;&#32034;&#25972;&#21512;&#65292;&#22686;&#24378;&#20010;&#24615;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models. (arXiv:2310.20081v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20081
&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26159;NLP&#31995;&#32479;&#20013;&#29992;&#25143;&#20307;&#39564;&#30340;&#20851;&#38190;&#65292;&#26412;&#25991;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24635;&#32467;&#21644;&#26816;&#32034;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20219;&#21153;&#24863;&#30693;&#30340;&#24635;&#32467;&#22686;&#24378;&#20010;&#24615;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#31995;&#32479;&#20013;&#29992;&#25143;&#20307;&#39564;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#26356;&#22909;&#22320;&#20010;&#24615;&#21270;&#29992;&#25143;&#20307;&#39564;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#19968;&#20010;&#30452;&#25509;&#30340;&#26041;&#27861;&#26159;&#23558;&#36807;&#21435;&#30340;&#29992;&#25143;&#25968;&#25454;&#24182;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20013;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#36755;&#20837;&#36807;&#38271;&#65292;&#36229;&#20986;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#65292;&#24182;&#19988;&#24341;&#36215;&#24310;&#36831;&#21644;&#25104;&#26412;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#25552;&#21462;&#30456;&#20851;&#30340;&#29992;&#25143;&#25968;&#25454;&#65288;&#21363;&#36873;&#25321;&#24615;&#26816;&#32034;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20197;&#26500;&#24314;&#19979;&#28216;&#20219;&#21153;&#30340;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#21463;&#38480;&#20110;&#28508;&#22312;&#30340;&#20449;&#24687;&#20002;&#22833;&#12289;&#32570;&#20047;&#26356;&#28145;&#20837;&#30340;&#29992;&#25143;&#29702;&#35299;&#21644;&#20919;&#21551;&#21160;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24635;&#32467;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#26816;&#32034;&#22686;&#24378;&#20010;&#24615;&#21270;&#19982;&#20219;&#21153;&#24863;&#30693;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization, the ability to tailor a system to individual users, is an essential factor in user experience with natural language processing (NLP) systems. With the emergence of Large Language Models (LLMs), a key question is how to leverage these models to better personalize user experiences. To personalize a language model's output, a straightforward approach is to incorporate past user data into the language model prompt, but this approach can result in lengthy inputs exceeding limitations on input length and incurring latency and cost issues. Existing approaches tackle such challenges by selectively extracting relevant user data (i.e. selective retrieval) to construct a prompt for downstream tasks. However, retrieval-based methods are limited by potential information loss, lack of more profound user understanding, and cold-start challenges. To overcome these limitations, we propose a novel summary-augmented approach by extending retrieval-augmented personalization with task-awar
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24212;&#29992;&#37096;&#20998;&#24352;&#37327;&#21270;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#23545;BERT&#21644;ViT&#31561;&#31070;&#32463;&#32593;&#32476;&#30340;&#23884;&#20837;&#23618;&#21387;&#32553;&#21644;&#37096;&#20998;&#24352;&#37327;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25171;&#30772;&#20102;&#24352;&#37327;&#20998;&#35299;&#39046;&#22495;&#30340;&#26032;&#23616;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.20077</link><description>&lt;p&gt;
&#37096;&#20998;&#24352;&#37327;&#21270;&#21464;&#21387;&#22120;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Partial Tensorized Transformers for Natural Language Processing. (arXiv:2310.20077v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20077
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24212;&#29992;&#37096;&#20998;&#24352;&#37327;&#21270;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#23545;BERT&#21644;ViT&#31561;&#31070;&#32463;&#32593;&#32476;&#30340;&#23884;&#20837;&#23618;&#21387;&#32553;&#21644;&#37096;&#20998;&#24352;&#37327;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25171;&#30772;&#20102;&#24352;&#37327;&#20998;&#35299;&#39046;&#22495;&#30340;&#26032;&#23616;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26550;&#26500;&#30001;&#20110;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24615;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24320;&#21019;&#20102;&#26032;&#23616;&#38754;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24191;&#27867;&#30340;&#23384;&#20648;&#21644;&#21442;&#25968;&#38656;&#27714;&#36890;&#24120;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24352;&#37327;&#21015;&#20998;&#35299;&#23545;&#25552;&#39640;BERT&#21644;ViT&#31561;&#21464;&#21387;&#22120;&#35270;&#35273;&#35821;&#35328;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#21644;&#21387;&#32553;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23884;&#20837;&#23618;&#21387;&#32553;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#37096;&#20998;&#24352;&#37327;&#21270;&#65288;PTNN&#65289;&#36890;&#36807;&#19968;&#31181;&#31639;&#27861;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;PTNN&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#21518;&#26399;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25171;&#30772;&#20102;&#24352;&#37327;&#20998;&#35299;&#39046;&#22495;&#30340;&#26032;&#23616;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer architecture has revolutionized Natural Language Processing (NLP) and other machine-learning tasks, due to its unprecedented accuracy. However, their extensive memory and parameter requirements often hinder their practical applications. In this work, we study the effect of tensor-train decomposition to improve the accuracy and compress transformer vision-language neural networks, namely BERT and ViT. We focus both on embedding-layer compression and partial tensorization of neural networks (PTNN) through an algorithmic approach. Our novel PTNN approach significantly improves the accuracy of existing models by up to 5%, all without the need for post-training adjustments, breaking new ground in the field of tensor decomposition.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#23398;&#20064;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22810;&#20219;&#21153;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20072</link><description>&lt;p&gt;
&#29992;&#25351;&#23548;&#35843;&#25972;&#23454;&#29616;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Evaluation of Generative Models with Instruction Tuning. (arXiv:2310.20072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#23398;&#20064;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22810;&#20219;&#21153;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;NLP&#39046;&#22495;&#19968;&#20010;&#38590;&#20197;&#36798;&#21040;&#30340;&#30446;&#26631;&#12290;&#26368;&#36817;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#26469;&#27169;&#25311;&#20154;&#31867;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#35780;&#20272;&#26631;&#20934;&#19978;&#30340;&#21028;&#26029;&#12290;&#21463;&#21040;&#25351;&#23548;&#35843;&#25972;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#23398;&#20064;&#24230;&#37327;&#26041;&#27861;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;HEAP&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#35780;&#20272;&#26631;&#20934;&#30340;&#20154;&#31867;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;HEAP&#19978;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#35780;&#20272;&#24615;&#33021;&#65292;&#23613;&#31649;&#26377;&#20123;&#35780;&#20272;&#26631;&#20934;&#30340;&#23398;&#20064;&#24182;&#19981;&#37027;&#20040;&#23481;&#26131;&#12290;&#27492;&#22806;&#65292;&#22810;&#20219;&#21153;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#23545;&#20110;&#26410;&#26469;&#32570;&#20047;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#20219;&#21153;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic evaluation of natural language generation has long been an elusive goal in NLP.A recent paradigm fine-tunes pre-trained language models to emulate human judgements for a particular task and evaluation criterion. Inspired by the generalization ability of instruction-tuned models, we propose a learned metric based on instruction tuning. To test our approach, we collected HEAP, a dataset of human judgements across various NLG tasks and evaluation criteria. Our findings demonstrate that instruction tuning language models on HEAP yields good performance on many evaluation tasks, though some criteria are less trivial to learn than others. Further, jointly training on multiple tasks can yield additional performance improvements, which can be beneficial for future tasks with little to no human annotated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#36866;&#24212;&#30340;&#26080;&#20248;&#21270;&#31639;&#27861;AdaICL&#65292;&#36890;&#36807;&#35782;&#21035;&#19981;&#30830;&#23450;&#30340;&#31034;&#20363;&#24182;&#36827;&#34892;&#22522;&#20110;&#35821;&#20041;&#22810;&#26679;&#24615;&#30340;&#36873;&#25321;&#65292;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#39044;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.20046</link><description>&lt;p&gt;
&#21738;&#20123;&#31034;&#20363;&#36866;&#21512;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65311;&#26397;&#30528;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#36873;&#25321;&#26041;&#21521;&#12290;(arXiv:2310.20046v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection. (arXiv:2310.20046v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#36866;&#24212;&#30340;&#26080;&#20248;&#21270;&#31639;&#27861;AdaICL&#65292;&#36890;&#36807;&#35782;&#21035;&#19981;&#30830;&#23450;&#30340;&#31034;&#20363;&#24182;&#36827;&#34892;&#22522;&#20110;&#35821;&#20041;&#22810;&#26679;&#24615;&#30340;&#36873;&#25321;&#65292;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#39044;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;ICL&#26159;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#23545;&#35757;&#32451;&#36807;&#30340;LLM&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#26631;&#27880;&#31034;&#20363;&#20316;&#20026;LLM&#30340;&#36755;&#20837;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29992;&#20110;ICL&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#26631;&#27880;&#31034;&#20363;&#30340;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#36866;&#24212;&#30340;&#26080;&#20248;&#21270;&#31639;&#27861;&#65292;&#31216;&#20026;AdaICL&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#27169;&#22411;&#19981;&#30830;&#23450;&#30340;&#31034;&#20363;&#65292;&#24182;&#36827;&#34892;&#22522;&#20110;&#35821;&#20041;&#22810;&#26679;&#24615;&#30340;&#31034;&#20363;&#36873;&#25321;&#12290;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#37319;&#26679;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#25928;&#26524;&#65292;&#32780;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#21487;&#20197;&#25552;&#39640;&#39044;&#31639;&#25928;&#29575;&#24182;&#24110;&#21161;LLM&#23398;&#20064;&#26032;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;AdaICL&#23558;&#20854;&#37319;&#26679;&#31574;&#30053;&#24314;&#27169;&#20026;&#19968;&#20010;&#26368;&#22823;&#35206;&#30422;&#38382;&#39064;&#65292;&#26681;&#25454;&#27169;&#22411;&#30340;&#21453;&#39304;&#21160;&#24577;&#35843;&#25972;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#36827;&#34892;&#36817;&#20284;&#27714;&#35299;&#12290;&#22312;&#20061;&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;LLM&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;AdaICL&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4.4%&#30340;&#20934;&#30830;&#24230;&#65292;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#20102;7.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) can adapt to new tasks via in-context learning (ICL). ICL is efficient as it does not require any parameter updates to the trained LLM, but only few annotated examples as input for the LLM. In this work, we investigate an active learning approach for ICL, where there is a limited budget for annotating examples. We propose a model-adaptive optimization-free algorithm, termed AdaICL, which identifies examples that the model is uncertain about, and performs semantic diversity-based example selection. Diversity-based sampling improves overall effectiveness, while uncertainty sampling improves budget efficiency and helps the LLM learn new information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage problem, that dynamically adapts based on the model's feedback and can be approximately solved via greedy algorithms. Extensive experiments on nine datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy points over SOTA (7.7%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#25968;&#25454;&#20197;&#25913;&#21892;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20033</link><description>&lt;p&gt;
&#29992;&#20110;&#20020;&#24202;&#24635;&#32467;&#20013;&#20107;&#23454;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#20223;&#32534;&#36753;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#25968;&#25454;&#20197;&#25913;&#21892;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#21644;LLaMA&#31995;&#21015;&#22312;&#25429;&#25417;&#21644;&#27987;&#32553;&#20851;&#38190;&#19978;&#19979;&#25991;&#20449;&#24687;&#21450;&#22312;&#24635;&#32467;&#20219;&#21153;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24322;&#24120;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#31038;&#21306;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#34394;&#26500;&#38382;&#39064;&#30340;&#25285;&#24551;&#20173;&#22312;&#19981;&#26029;&#19978;&#21319;&#12290;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#34394;&#26500;&#30340;&#25688;&#35201;&#65292;&#36825;&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;NLP&#20219;&#21153;&#65288;&#20363;&#22914;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#65289;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#38169;&#35823;&#30340;&#35786;&#26029;&#12290;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23454;&#29616;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#25215;&#35834;&#65292;&#20294;&#36825;&#31181;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#65292;&#32780;&#22312;&#20020;&#24202;&#39046;&#22495;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31649;&#36947;&#65292;&#20351;&#29992;ChatGPT&#20195;&#26367;&#20154;&#31867;&#19987;&#23478;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#20197;&#25913;&#21892;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#35199;&#29677;&#29273;&#35821;&#20013;&#26089;&#26399;&#26816;&#27979;&#25233;&#37057;&#30151;&#21644;&#39278;&#39135;&#38556;&#30861;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20102;&#26089;&#26399;&#26816;&#27979;&#26694;&#26550;&#20013;&#23450;&#20041;&#30340;&#20915;&#31574;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.20003</link><description>&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;&#35199;&#29677;&#29273;&#35821;&#20013;&#30340;&#25233;&#37057;&#30151;&#21644;&#39278;&#39135;&#38556;&#30861;&#65306;UNSL&#22312;MentalRiskES 2023&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Early Detection of Depression and Eating Disorders in Spanish: UNSL at MentalRiskES 2023. (arXiv:2310.20003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#35199;&#29677;&#29273;&#35821;&#20013;&#26089;&#26399;&#26816;&#27979;&#25233;&#37057;&#30151;&#21644;&#39278;&#39135;&#38556;&#30861;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20102;&#26089;&#26399;&#26816;&#27979;&#26694;&#26550;&#20013;&#23450;&#20041;&#30340;&#20915;&#31574;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MentalRiskES&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#35299;&#20915;&#19982;&#35199;&#29677;&#29273;&#35821;&#26089;&#26399;&#39118;&#38505;&#26816;&#27979;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#23613;&#26089;&#26816;&#27979;&#21040;&#26174;&#31034;&#20986;&#19981;&#21516;&#20219;&#21153;&#30340;&#24515;&#29702;&#38556;&#30861;&#36857;&#35937;&#30340;Telegram&#29992;&#25143;&#12290;&#20219;&#21153;1&#28041;&#21450;&#39278;&#39135;&#38556;&#30861;&#30340;&#26816;&#27979;&#65292;&#20219;&#21153;2&#20851;&#27880;&#25233;&#37057;&#30151;&#30340;&#26816;&#27979;&#65292;&#20219;&#21153;3&#26088;&#22312;&#26816;&#27979;&#19968;&#31181;&#26410;&#30693;&#30340;&#38556;&#30861;&#12290;&#36825;&#20123;&#20219;&#21153;&#34987;&#20998;&#25104;&#23376;&#20219;&#21153;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#23450;&#20041;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23567;&#32452;&#21442;&#19982;&#20102;&#20219;&#21153;1&#21644;&#20219;&#21153;2&#30340;&#23376;&#20219;&#21153;A&#65306;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#35780;&#20272;&#29992;&#25143;&#26159;&#31215;&#26497;&#36824;&#26159;&#28040;&#26497;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#26089;&#26399;&#26816;&#27979;&#26694;&#26550;&#23450;&#20041;&#30340;&#26631;&#20934;&#36827;&#34892;&#20915;&#31574;&#31574;&#30053;&#12290;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#21576;&#29616;&#20102;&#27599;&#20010;&#20219;&#21153;&#35299;&#20915;&#30340;&#37325;&#35201;&#35789;&#27719;&#30340;&#25193;&#23637;&#35789;&#27719;&#34920;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21382;&#21490;&#30340;&#20915;&#31574;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
MentalRiskES is a novel challenge that proposes to solve problems related to early risk detection for the Spanish language. The objective is to detect, as soon as possible, Telegram users who show signs of mental disorders considering different tasks. Task 1 involved the users' detection of eating disorders, Task 2 focused on depression detection, and Task 3 aimed at detecting an unknown disorder. These tasks were divided into subtasks, each one defining a resolution approach. Our research group participated in subtask A for Tasks 1 and 2: a binary classification problem that evaluated whether the users were positive or negative. To solve these tasks, we proposed models based on Transformers followed by a decision policy according to criteria defined by an early detection framework. One of the models presented an extended vocabulary with important words for each task to be solved. In addition, we applied a decision policy based on the history of predictions that the model performs duri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#24037;&#20855;&#22312;&#26448;&#26009;&#24037;&#31243;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;LLMs&#21487;&#20197;&#29992;&#20316;&#19968;&#32452;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#12289;&#33021;&#21147;&#21644;&#25351;&#20196;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#20026;&#20998;&#26512;&#21644;&#35774;&#35745;&#38382;&#39064;&#25552;&#20379;&#24378;&#22823;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#23454;&#39564;&#37325;&#28857;&#26159;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;MechGPT&#65292;&#22312;&#26448;&#26009;&#21147;&#23398;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;Fine-tuning&#37325;&#26032;&#35843;&#25972;&#21442;&#25968;&#65292;&#22686;&#24378;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19998</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#26816;&#32034;&#22686;&#24378;&#26412;&#20307;&#22270;&#21644;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#35299;&#37322;&#24615;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26448;&#26009;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design. (arXiv:2310.19998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#24037;&#20855;&#22312;&#26448;&#26009;&#24037;&#31243;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;LLMs&#21487;&#20197;&#29992;&#20316;&#19968;&#32452;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#12289;&#33021;&#21147;&#21644;&#25351;&#20196;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#20026;&#20998;&#26512;&#21644;&#35774;&#35745;&#38382;&#39064;&#25552;&#20379;&#24378;&#22823;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#23454;&#39564;&#37325;&#28857;&#26159;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;MechGPT&#65292;&#22312;&#26448;&#26009;&#21147;&#23398;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;Fine-tuning&#37325;&#26032;&#35843;&#25972;&#21442;&#25968;&#65292;&#22686;&#24378;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#31070;&#32463;&#32593;&#32476;&#26174;&#31034;&#20986;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#26448;&#26009;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#21046;&#36896;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#23427;&#20204;&#26377;&#25928;&#22320;&#22788;&#29702;&#20154;&#31867;&#35821;&#35328;&#12289;&#31526;&#21495;&#12289;&#20195;&#30721;&#21644;&#25968;&#20540;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25903;&#25345;&#26448;&#26009;&#24037;&#31243;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#26816;&#32034;&#26377;&#20851;&#20027;&#39064;&#39046;&#22495;&#30340;&#20851;&#38190;&#20449;&#24687;&#12289;&#24320;&#21457;&#30740;&#31350;&#20551;&#35774;&#12289;&#21457;&#29616;&#30693;&#35782;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#26426;&#21046;&#20851;&#31995;&#65292;&#24182;&#26681;&#25454;&#29289;&#29702;&#22522;&#26412;&#20107;&#23454;&#32534;&#20889;&#21644;&#25191;&#34892;&#20223;&#30495;&#20195;&#30721;&#36827;&#34892;&#20027;&#21160;&#30693;&#35782;&#29983;&#25104;&#12290;&#24403;LLMs&#34987;&#29992;&#20316;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#12289;&#33021;&#21147;&#21644;&#25351;&#20196;&#30340;AI&#20195;&#29702;&#38598;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#20026;&#20998;&#26512;&#21644;&#35774;&#35745;&#38382;&#39064;&#25552;&#20379;&#24378;&#22823;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#37325;&#28857;&#26159;&#20351;&#29992;&#22312;&#26448;&#26009;&#21147;&#23398;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#22522;&#30784;&#19978;&#24320;&#21457;&#30340;Fine-tuned&#27169;&#22411;MechGPT&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#23454;&#20102;Fine-tuning&#22914;&#20309;&#36171;&#20104;LLMs&#37325;&#26032;&#35843;&#25972;&#21442;&#25968;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer neural networks show promising capabilities, in particular for uses in materials analysis, design and manufacturing, including their capacity to work effectively with both human language, symbols, code, and numerical data. Here we explore the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrieving key information about subject areas, developing research hypotheses, discovery of mechanistic relationships across disparate areas of knowledge, and writing and executing simulation codes for active knowledge generation based on physical ground truths. When used as sets of AI agents with specific features, capabilities, and instructions, LLMs can provide powerful problem solution strategies for applications in analysis and design problems. Our experiments focus on using a fine-tuned model, MechGPT, developed based on training data in the mechanics of materials domain. We first affirm how finetuning endows LLMs with re
&lt;/p&gt;</description></item><item><title>BioInstruct&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#24615;&#25351;&#20196;&#25968;&#25454;&#38598;BioInstruct&#65292;&#36890;&#36807;GPT-4&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#35843;&#65292;&#20248;&#21270;&#20102;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19975</link><description>&lt;p&gt;
BioInstruct:&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19975
&lt;/p&gt;
&lt;p&gt;
BioInstruct&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#24615;&#25351;&#20196;&#25968;&#25454;&#38598;BioInstruct&#65292;&#36890;&#36807;GPT-4&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#35843;&#65292;&#20248;&#21270;&#20102;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;&#29305;&#23450;&#39046;&#22495;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21482;&#21457;&#34920;&#20102;&#24456;&#23569;&#30340;&#25351;&#20196;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioInstruct&#65292;&#36825;&#26159;&#19968;&#20010;&#23450;&#21046;&#30340;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;25,000&#20010;&#31034;&#20363;&#12290;&#36890;&#36807;&#20351;&#29992;&#19977;&#20010;&#20154;&#24037;&#31579;&#36873;&#30340;&#25351;&#20196;&#26679;&#26412;&#65292;&#20197;GPT-4&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#31034;&#65292;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#26088;&#22312;&#20248;&#21270;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LLaMA LLMs (1&amp;2,7B&amp;13B)&#36827;&#34892;&#20102;&#25351;&#20196;&#35843;&#25972;&#65292;&#24182;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#25351;&#20196;&#22914;&#20309;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#20351;&#29992;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&amp;2, 7B\&amp;13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;UNSL&#22312;eRisk 2023&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#28041;&#21450;&#20102;&#23545;&#25233;&#37057;&#30151;&#30151;&#29366;&#12289;&#30149;&#29702;&#24615;&#36172;&#21338;&#39118;&#38505;&#21644;&#39278;&#39135;&#32010;&#20081;&#36857;&#35937;&#30340;&#26816;&#27979;&#19982;&#20272;&#35745;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.19970</link><description>&lt;p&gt;
&#21033;&#29992;Transformer&#28508;&#21147;&#30340;&#31574;&#30053;&#65306;UNSL&#22312;eRisk 2023&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Strategies to Harness the Transformers' Potential: UNSL at eRisk 2023. (arXiv:2310.19970v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;UNSL&#22312;eRisk 2023&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#28041;&#21450;&#20102;&#23545;&#25233;&#37057;&#30151;&#30151;&#29366;&#12289;&#30149;&#29702;&#24615;&#36172;&#21338;&#39118;&#38505;&#21644;&#39278;&#39135;&#32010;&#20081;&#36857;&#35937;&#30340;&#26816;&#27979;&#19982;&#20272;&#35745;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLEF eRisk&#23454;&#39564;&#23460;&#25506;&#32034;&#19982;&#20114;&#32852;&#32593;&#39118;&#38505;&#26816;&#27979;&#30456;&#20851;&#30340;&#19981;&#21516;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;2023&#24180;&#30340;&#27604;&#36187;&#20013;&#65292;&#20219;&#21153;1&#26159;&#25628;&#32034;&#25233;&#37057;&#30151;&#30151;&#29366;&#65292;&#20854;&#30446;&#26631;&#26159;&#26681;&#25454;&#19982;BDI&#38382;&#21367;&#30151;&#29366;&#30456;&#20851;&#24615;&#25552;&#21462;&#29992;&#25143;&#30340;&#20889;&#20316;&#12290;&#20219;&#21153;2&#19982;&#26089;&#26399;&#21457;&#29616;&#30149;&#29702;&#24615;&#36172;&#21338;&#39118;&#38505;&#30340;&#38382;&#39064;&#26377;&#20851;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#23613;&#24555;&#26816;&#27979;&#21040;&#26377;&#39118;&#38505;&#30340;&#29992;&#25143;&#12290;&#26368;&#21518;&#65292;&#20219;&#21153;3&#26159;&#20272;&#35745;&#39278;&#39135;&#32010;&#20081;&#36857;&#35937;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23567;&#32452;&#21442;&#19982;&#20102;&#21069;&#20004;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#20219;&#21153;1&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#24456;&#26377;&#36259;&#12290;&#20004;&#20010;&#25552;&#26696;&#26159;&#22522;&#20110;&#35821;&#22659;&#21270;&#23884;&#20837;&#21521;&#37327;&#30340;&#30456;&#20284;&#24615;&#65292;&#21478;&#19968;&#20010;&#21017;&#22522;&#20110;&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;&#20219;&#21153;2&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#65292;&#20854;&#21518;&#36319;&#20915;&#31574;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CLEF eRisk Laboratory explores solutions to different tasks related to risk detection on the Internet. In the 2023 edition, Task 1 consisted of searching for symptoms of depression, the objective of which was to extract user writings according to their relevance to the BDI Questionnaire symptoms. Task 2 was related to the problem of early detection of pathological gambling risks, where the participants had to detect users at risk as quickly as possible. Finally, Task 3 consisted of estimating the severity levels of signs of eating disorders. Our research group participated in the first two tasks, proposing solutions based on Transformers. For Task 1, we applied different approaches that can be interesting in information retrieval tasks. Two proposals were based on the similarity of contextualized embedding vectors, and the other one was based on prompting, an attractive current technique of machine learning. For Task 2, we proposed three fine-tuned models followed by decision polic
&lt;/p&gt;</description></item><item><title>&#28145;&#23618;&#30340;transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#24335;&#27867;&#21270;&#33021;&#21147;&#19978;&#27604;&#27973;&#23618;&#27169;&#22411;&#26356;&#22909;&#65292;&#20294;&#39069;&#22806;&#23618;&#25968;&#30340;&#30456;&#23545;&#25910;&#30410;&#20250;&#36805;&#36895;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.19956</link><description>&lt;p&gt;
&#28145;&#24230;&#21644;&#23485;&#24230;&#23545;Transformer&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Depth and Width on Transformer Language Model Generalization. (arXiv:2310.19956v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19956
&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#30340;transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#24335;&#27867;&#21270;&#33021;&#21147;&#19978;&#27604;&#27973;&#23618;&#27169;&#22411;&#26356;&#22909;&#65292;&#20294;&#39069;&#22806;&#23618;&#25968;&#30340;&#30456;&#23545;&#25910;&#30410;&#20250;&#36805;&#36895;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#26032;&#30340;&#21477;&#23376;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24517;&#39035;&#20197;&#32452;&#21512;&#30340;&#26041;&#24335;&#36827;&#34892;&#27867;&#21270; - &#23558;&#29087;&#24713;&#30340;&#20803;&#32032;&#20197;&#26032;&#30340;&#26041;&#24335;&#32467;&#21512;&#36215;&#26469;&#12290;&#27169;&#22411;&#32467;&#26500;&#30340;&#21738;&#20123;&#26041;&#38754;&#20419;&#36827;&#20102;&#32452;&#21512;&#24335;&#27867;&#21270;&#65311;&#38024;&#23545;transformers&#65292;&#25105;&#20204;&#27979;&#35797;&#20551;&#35774;&#65292;&#21363;&#24403;transformers&#26356;&#28145;&#65288;&#20855;&#26377;&#26356;&#22810;&#23618;&#27425;&#65289;&#26102;&#65292;&#23427;&#20204;&#26356;&#23481;&#26131;&#36827;&#34892;&#32452;&#21512;&#24335;&#27867;&#21270;&#65292;&#36825;&#20010;&#20551;&#35774;&#22522;&#20110;&#26368;&#36817;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#12290;&#30001;&#20110;&#31616;&#21333;&#22320;&#22686;&#21152;&#23618;&#25968;&#20250;&#22686;&#21152;&#24635;&#21442;&#25968;&#25968;&#37327;&#65292;&#28151;&#28102;&#20102;&#28145;&#24230;&#21644;&#22823;&#23567;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#20197;&#20445;&#25345;&#24635;&#21442;&#25968;&#25968;&#37327;&#24658;&#23450;&#30340;&#26041;&#24335;&#26469;&#26435;&#34913;&#28145;&#24230;&#21644;&#23485;&#24230;&#65288;&#20998;&#21035;&#20026;41M&#12289;134M&#21644;374M&#20010;&#21442;&#25968;&#65289;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#27169;&#22411;&#39044;&#35757;&#32451;&#20026;LMS&#65292;&#28982;&#21518;&#22312;&#27979;&#35797;&#32452;&#21512;&#24335;&#27867;&#21270;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#32467;&#35770;&#65306;&#65288;1&#65289;&#22312;&#24494;&#35843;&#21518;&#65292;&#28145;&#23618;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#24773;&#20917;&#19979;&#27604;&#27973;&#23618;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#65292;&#20294;&#39069;&#22806;&#23618;&#27425;&#30340;&#30456;&#23545;&#25910;&#30410;&#36805;&#36895;&#20943;&#23567;&#65307;&#65288;2&#65289;&#22312;&#27599;&#20010;&#27169;&#22411;&#32452;&#20013;&#65292;&#28145;&#23618;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#32452;&#21512;&#24335;&#27867;&#21270;&#33021;&#21147;...&#65288;&#25991;&#26412;&#24050;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
To process novel sentences, language models (LMs) must generalize compositionally -- combine familiar elements in new ways. What aspects of a model's structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by recent theoretical and empirical work, that transformers generalize more compositionally when they are deeper (have more layers). Because simply adding layers increases the total number of parameters, confounding depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize better out-of-distribution than shallower models do, but the relative benefit of additional layers diminishes rapidly; (2) within each family, deeper models show bett
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Split-NER&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#20998;&#25104;&#25552;&#21462;&#23454;&#20307;&#25552;&#21450;&#36328;&#24230;&#21644;&#36328;&#24230;&#20998;&#31867;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#21033;&#29992;&#38382;&#31572;&#27169;&#22411;&#35299;&#20915;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.19942</link><description>&lt;p&gt;
Split-NER: &#36890;&#36807;&#20004;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#20998;&#31867;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications. (arXiv:2310.19942v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Split-NER&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#20998;&#25104;&#25552;&#21462;&#23454;&#20307;&#25552;&#21450;&#36328;&#24230;&#21644;&#36328;&#24230;&#20998;&#31867;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#21033;&#29992;&#38382;&#31572;&#27169;&#22411;&#35299;&#20915;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#20998;&#25104;&#20004;&#20010;&#36923;&#36753;&#23376;&#20219;&#21153;&#65306;&#65288;1&#65289;&#25552;&#21462;&#23454;&#20307;&#25552;&#21450;&#36328;&#24230;&#65292;&#26080;&#35770;&#23454;&#20307;&#31867;&#22411;&#22914;&#20309;&#65307;&#65288;2&#65289;&#23558;&#36328;&#24230;&#20998;&#31867;&#20026;&#23454;&#20307;&#31867;&#22411;&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#37117;&#24418;&#24335;&#21270;&#20026;&#38382;&#31572;&#38382;&#39064;&#65292;&#24182;&#20135;&#29983;&#20004;&#20010;&#21487;&#20197;&#20998;&#21035;&#20026;&#27599;&#20010;&#23376;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#30340;&#26356;&#36731;&#30340;&#27169;&#22411;&#12290;&#22312;&#22235;&#20010;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#20004;&#27493;&#27861;&#26082;&#26377;&#25928;&#21448;&#33410;&#30465;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;SplitNER&#22312;OntoNotes5.0&#12289;WNUT17&#21644;&#19968;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#22522;&#32447;&#65292;&#24182;&#22312;BioNLP13CG&#19978;&#34920;&#29616;&#30456;&#24403;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#19982;QA&#22522;&#32447;&#23545;&#29031;&#30456;&#27604;&#65292;&#23427;&#22312;&#35757;&#32451;&#26102;&#20943;&#23569;&#20102;&#26174;&#33879;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#28304;&#20110;&#20998;&#21035;&#23545;&#36328;&#24230;&#26816;&#27979;&#21644;&#20998;&#31867;&#36827;&#34892;BERT&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/c3sr/split-ner&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we address the NER problem by splitting it into two logical sub-tasks: (1) Span Detection which simply extracts entity mention spans irrespective of entity type; (2) Span Classification which classifies the spans into their entity types. Further, we formulate both sub-tasks as question-answering (QA) problems and produce two leaner models which can be optimized separately for each sub-task. Experiments with four cross-domain datasets demonstrate that this two-step approach is both effective and time efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17 and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all cases, it achieves a significant reduction in training time compared to its QA baseline counterpart. The effectiveness of our system stems from fine-tuning the BERT model twice, separately for span detection and classification. The source code can be found at https://github.com/c3sr/split-ner.
&lt;/p&gt;</description></item><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#30693;&#35782;&#21644;&#33021;&#21147;&#35780;&#20272;&#12289;&#23545;&#40784;&#35780;&#20272;&#21644;&#23433;&#20840;&#35780;&#20272;&#12290;&#23545;&#20110;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#20197;&#21450;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#26377;&#30410;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.19736</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models: A Comprehensive Survey. (arXiv:2310.19736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#30693;&#35782;&#21644;&#33021;&#21147;&#35780;&#20272;&#12289;&#23545;&#40784;&#35780;&#20272;&#21644;&#23433;&#20840;&#35780;&#20272;&#12290;&#23545;&#20110;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#20197;&#21450;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#26377;&#30410;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#21560;&#24341;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#19982;&#21452;&#20995;&#21073;&#19968;&#26679;&#65292;LLMs&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;&#23427;&#20204;&#21487;&#33021;&#21463;&#21040;&#31169;&#20154;&#25968;&#25454;&#27844;&#38706;&#65292;&#20135;&#29983;&#19981;&#36866;&#24403;&#12289;&#26377;&#23475;&#25110;&#35823;&#23548;&#24615;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#24555;&#36895;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#21487;&#33021;&#20986;&#29616;&#27809;&#26377;&#36275;&#22815;&#20445;&#38556;&#30340;&#36229;&#26234;&#33021;&#31995;&#32479;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#26377;&#30410;&#30340;&#21457;&#23637;&#65292;&#23545;LLMs&#36827;&#34892;&#20005;&#26684;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#23545;LLMs&#35780;&#20272;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#35780;&#20272;&#20998;&#20026;&#19977;&#22823;&#31867;&#21035;&#65306;&#30693;&#35782;&#21644;&#33021;&#21147;&#35780;&#20272;&#65292;&#23545;&#40784;&#35780;&#20272;&#21644;&#23433;&#20840;&#35780;&#20272;&#12290;&#38500;&#20102;&#20840;&#38754;&#22238;&#39038;&#35780;&#20272;&#26041;&#27861;&#21644;&#25216;&#26415;&#20043;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.  This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#19982;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35789;&#35821;&#36827;&#34892;&#26631;&#35760;&#25110;&#8220;&#19978;&#33394;&#8221;&#26469;&#26377;&#25928;&#22788;&#29702;&#39046;&#22495;&#26415;&#35821;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39046;&#22495;&#19987;&#29992;&#20219;&#21153;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.19708</link><description>&lt;p&gt;
&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#29992;&#26041;&#27861;&#65306;&#19968;&#31181;&#20016;&#23500;&#22810;&#24425;&#30340;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
Combining Language Models For Specialized Domains: A Colorful Approach. (arXiv:2310.19708v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#19982;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35789;&#35821;&#36827;&#34892;&#26631;&#35760;&#25110;&#8220;&#19978;&#33394;&#8221;&#26469;&#26377;&#25928;&#22788;&#29702;&#39046;&#22495;&#26415;&#35821;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39046;&#22495;&#19987;&#29992;&#20219;&#21153;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30446;&#30340;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#21644;&#26415;&#35821;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#20123;&#26415;&#35821;&#32463;&#24120;&#22312;&#21307;&#23398;&#25110;&#24037;&#19994;&#39046;&#22495;&#31561;&#19987;&#19994;&#39046;&#22495;&#20013;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#36890;&#24120;&#24456;&#38590;&#35299;&#37322;&#23558;&#36890;&#29992;&#35821;&#35328;&#19982;&#19987;&#38376;&#26415;&#35821;&#28151;&#21512;&#20351;&#29992;&#30340;&#28151;&#21512;&#35821;&#38899;&#12290;&#36825;&#23545;&#20110;&#22312;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#20869;&#25805;&#20316;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#25110;&#27425;&#32423;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#36890;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#35813;&#31574;&#30053;&#28041;&#21450;&#23545;&#27599;&#20010;&#21333;&#35789;&#36827;&#34892;&#26631;&#35760;&#25110;&#8220;&#19978;&#33394;&#8221;&#65292;&#20197;&#25351;&#31034;&#20854;&#19982;&#36890;&#29992;&#25110;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#22686;&#24378;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#28041;&#21450;&#19978;&#33394;&#21333;&#35789;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38598;&#25104;&#26415;&#35821;&#21040;&#35821;&#35328;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#39046;&#22495;&#19987;&#29992;&#20219;&#21153;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
General purpose language models (LMs) encounter difficulties when processing domain-specific jargon and terminology, which are frequently utilized in specialized fields such as medicine or industrial settings. Moreover, they often find it challenging to interpret mixed speech that blends general language with specialized jargon. This poses a challenge for automatic speech recognition systems operating within these specific domains. In this work, we introduce a novel approach that integrates domain-specific or secondary LM into general-purpose LM. This strategy involves labeling, or ``coloring'', each word to indicate its association with either the general or the domain-specific LM. We develop an optimized algorithm that enhances the beam search algorithm to effectively handle inferences involving colored words. Our evaluations indicate that this approach is highly effective in integrating jargon into language tasks. Notably, our method substantially lowers the error rate for domain-sp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22240;&#26524;&#21644;&#36947;&#24503;&#21028;&#26029;&#20219;&#21153;&#19978;&#19982;&#20154;&#31867;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#25910;&#38598;24&#31687;&#35748;&#30693;&#31185;&#23398;&#35770;&#25991;&#30340;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#65292;&#21457;&#29616;LLMs&#22312;&#32771;&#34385;&#22240;&#26524;&#21644;&#36947;&#24503;&#22240;&#32032;&#26102;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.19677</link><description>&lt;p&gt;
Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks - MoCa
&lt;/p&gt;
&lt;p&gt;
MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks. (arXiv:2310.19677v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22240;&#26524;&#21644;&#36947;&#24503;&#21028;&#26029;&#20219;&#21153;&#19978;&#19982;&#20154;&#31867;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#25910;&#38598;24&#31687;&#35748;&#30693;&#31185;&#23398;&#35770;&#25991;&#30340;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#32479;&#35745;&#20998;&#26512;&#26041;&#27861;&#65292;&#21457;&#29616;LLMs&#22312;&#32771;&#34385;&#22240;&#26524;&#21644;&#36947;&#24503;&#22240;&#32032;&#26102;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23545;&#29289;&#29702;&#21644;&#31038;&#20250;&#19990;&#30028;&#30340;&#24120;&#35782;&#29702;&#35299;&#26159;&#22522;&#20110;&#30452;&#35273;&#29702;&#35770;&#30340;&#12290;&#36825;&#20123;&#29702;&#35770;&#25903;&#25345;&#36827;&#34892;&#22240;&#26524;&#21644;&#36947;&#24503;&#21028;&#26029;&#12290;&#24403;&#21457;&#29983;&#19981;&#22909;&#30340;&#20107;&#24773;&#26102;&#65292;&#25105;&#20204;&#33258;&#28982;&#20250;&#38382;&#65306;&#35841;&#20570;&#20102;&#20160;&#20040;&#65292;&#20026;&#20160;&#20040;&#65311;&#35748;&#30693;&#31185;&#23398;&#30340;&#20016;&#23500;&#25991;&#29486;&#30740;&#31350;&#20102;&#20154;&#20204;&#30340;&#22240;&#26524;&#21644;&#36947;&#24503;&#30452;&#35273;&#12290;&#36825;&#39033;&#24037;&#20316;&#25581;&#31034;&#20102;&#19968;&#20123;&#31995;&#32479;&#22320;&#24433;&#21709;&#20154;&#20204;&#21028;&#26029;&#30340;&#22240;&#32032;&#65292;&#22914;&#35268;&#33539;&#36829;&#21453;&#20197;&#21450;&#20260;&#23475;&#26159;&#21542;&#21487;&#36991;&#20813;&#25110;&#19981;&#21487;&#36991;&#20813;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;24&#31687;&#35748;&#30693;&#31185;&#23398;&#35770;&#25991;&#30340;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#27880;&#37322;&#27599;&#20010;&#25925;&#20107;&#25152;&#30740;&#31350;&#30340;&#22240;&#32032;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22522;&#20110;&#25991;&#26412;&#22330;&#26223;&#19978;&#26159;&#21542;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#22240;&#26524;&#21644;&#36947;&#24503;&#21028;&#26029;&#30456;&#19968;&#33268;&#12290;&#22312;&#24635;&#20307;&#27700;&#24179;&#19978;&#65292;&#26368;&#36817;&#30340;LLM&#30340;&#19968;&#33268;&#24615;&#26377;&#25152;&#25913;&#21892;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#22312;&#32771;&#34385;&#36825;&#20123;&#19981;&#21516;&#22240;&#32032;&#26102;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human partic
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#20854;&#33021;&#21147;&#30340;&#36777;&#35770;&#32570;&#20047;&#32454;&#33268;&#30340;&#32771;&#34385;&#12290;&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#19977;&#20010;&#24120;&#35265;&#25209;&#35780;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;LLMs&#29702;&#35299;&#21644;&#24847;&#22270;&#38382;&#39064;&#30340;&#21153;&#23454;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.19671</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23545;&#24403;&#21069;&#36777;&#35770;&#30340;&#32454;&#24494;&#24046;&#21035;&#30340;&#38656;&#27714;&#21644;&#23545;&#29702;&#35299;&#30340;&#21153;&#23454;&#35266;&#28857;(arXiv:2310.19671v2 [cs.CL] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding. (arXiv:2310.19671v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19671
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#20854;&#33021;&#21147;&#30340;&#36777;&#35770;&#32570;&#20047;&#32454;&#33268;&#30340;&#32771;&#34385;&#12290;&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#19977;&#20010;&#24120;&#35265;&#25209;&#35780;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;LLMs&#29702;&#35299;&#21644;&#24847;&#22270;&#38382;&#39064;&#30340;&#21153;&#23454;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#12289;&#27969;&#30021;&#30340;&#25991;&#26412;&#26041;&#38754;&#26080;&#19982;&#20262;&#27604;&#12290;LLMs&#27491;&#22312;&#24555;&#36895;&#20986;&#29616;&#65292;&#24182;&#19988;&#20851;&#20110;LLM&#33021;&#21147;&#30340;&#36777;&#35770;&#24050;&#32463;&#24320;&#22987;&#65292;&#20294;&#21453;&#24605;&#28382;&#21518;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#32858;&#28966;&#20110;&#36777;&#35770;&#65292;&#24182;&#23545;LLM&#33021;&#21147;&#30340;&#19977;&#20010;&#37325;&#22797;&#20986;&#29616;&#30340;&#25209;&#35780;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#65306;i) LLMs&#21482;&#26159;&#27169;&#20223;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#32479;&#35745;&#27169;&#24335;&#65307;ii) LLMs&#25484;&#25569;&#20102;&#24418;&#24335;&#20294;&#24182;&#38750;&#21151;&#33021;&#24615;&#35821;&#35328;&#33021;&#21147;&#65307;iii) LLMs&#20013;&#30340;&#35821;&#35328;&#23398;&#20064;&#19981;&#33021;&#20026;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#25552;&#20379;&#20449;&#24687;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#35770;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#35266;&#28857;&#38656;&#35201;&#26356;&#22810;&#32454;&#24494;&#20043;&#22788;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#23545;LLMs&#20013;&#8220;&#30495;&#27491;&#8221;&#30340;&#29702;&#35299;&#21644;&#24847;&#22270;&#38382;&#39064;&#30340;&#21153;&#23454;&#35266;&#28857;&#12290;&#29702;&#35299;&#21644;&#24847;&#22270;&#28041;&#21450;&#21040;&#25105;&#20204;&#20551;&#35774;&#20182;&#20154;&#20855;&#26377;&#30340;&#19981;&#21487;&#35266;&#23519;&#30340;&#24515;&#29702;&#29366;&#24577;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#65306;&#23427;&#20204;&#20351;&#25105;&#20204;&#33021;&#22815;&#25277;&#35937;&#20986;&#22797;&#26434;&#30340;&#24213;&#23618;&#26426;&#21046;&#24182;&#39044;&#27979;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of `real' understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviou
&lt;/p&gt;</description></item><item><title>LLMaAA&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#21160;&#25209;&#27880;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#20351;&#29992;LLM&#30830;&#23450;&#39640;&#25928;&#25209;&#27880;&#20869;&#23481;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;LLM&#30340;&#28508;&#21147;&#24182;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.19596</link><description>&lt;p&gt;
LLMaAA: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#21160;&#25209;&#27880;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMaAA: Making Large Language Models as Active Annotators. (arXiv:2310.19596v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19596
&lt;/p&gt;
&lt;p&gt;
LLMaAA&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#21160;&#25209;&#27880;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#20351;&#29992;LLM&#30830;&#23450;&#39640;&#25928;&#25209;&#27880;&#20869;&#23481;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;LLM&#30340;&#28508;&#21147;&#24182;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#26222;&#36941;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22240;&#38656;&#27714;&#22823;&#37327;&#39640;&#36136;&#37327;&#26631;&#27880;&#25968;&#25454;&#32780;&#22768;&#21517;&#29436;&#34249;&#12290;&#23454;&#38469;&#19978;&#65292;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#20107;&#19994;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20986;&#33394;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#25512;&#21160;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#21457;&#23637;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#20165;&#20174;LLM&#20013;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#23384;&#22312;&#36136;&#37327;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#38656;&#35201;&#22810;&#20010;&#25968;&#37327;&#32423;&#30340;&#24050;&#26631;&#35760;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;LLM&#30340;&#28508;&#21147;&#24182;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMaAA&#65292;&#23427;&#23558;LLM&#20316;&#20026;&#25209;&#27880;&#22120;&#65292;&#24182;&#23558;&#23427;&#20204;&#25918;&#20837;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#20197;&#39640;&#25928;&#30830;&#23450;&#25209;&#27880;&#20869;&#23481;&#12290;&#20026;&#20102;&#21033;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#31283;&#20581;&#23398;&#20064;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#25209;&#27880;&#21644;&#35757;&#32451;&#36807;&#31243;&#65306;&#65288;1&#65289;&#25105;&#20204;&#20174;&#23567;&#30340;&#31034;&#33539;&#27744;&#20013;&#25277;&#21462;k-NN&#31034;&#20363;&#20316;&#20026;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#65288;2&#65289;&#25105;&#20204;&#37319;&#29992;&#31034;&#20363;&#21152;&#26435;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting techniqu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#37319;&#29992;&#32447;&#24615;&#21270;&#31574;&#30053;&#23558;&#36755;&#20986;&#26641;&#32467;&#26500;&#36716;&#21270;&#20026;&#31526;&#21495;&#24207;&#21015;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#23545;LLMs&#30340;&#24615;&#33021;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.19462</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Constituency Parsing using LLMs. (arXiv:2310.19462v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#37319;&#29992;&#32447;&#24615;&#21270;&#31574;&#30053;&#23558;&#36755;&#20986;&#26641;&#32467;&#26500;&#36716;&#21270;&#20026;&#31526;&#21495;&#24207;&#21015;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#23545;LLMs&#30340;&#24615;&#33021;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#26159;&#19968;&#20010;&#22522;&#30784;&#20294;&#23578;&#26410;&#35299;&#20915;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#30340;&#21331;&#36234;&#24615;&#33021;&#22312;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#19977;&#31181;&#32447;&#24615;&#21270;&#31574;&#30053;&#23558;&#36755;&#20986;&#30340;&#26641;&#32467;&#26500;&#36716;&#21270;&#20026;&#31526;&#21495;&#24207;&#21015;&#65292;&#20351;&#24471;LLMs&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#32447;&#24615;&#21270;&#26641;&#26469;&#35299;&#20915;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;ChatGPT&#12289;GPT-4&#12289;OPT&#12289;LLaMA&#21644;Alpaca&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#22120;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28085;&#30422;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20840;&#26679;&#26412;&#23398;&#20064;&#30340;&#19981;&#21516;&#35774;&#32622;&#65292;&#24182;&#22312;&#19968;&#20010;&#39046;&#22495;&#20869;&#21644;&#20116;&#20010;&#39046;&#22495;&#22806;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;LLMs&#30340;&#24615;&#33021;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constituency parsing is a fundamental yet unsolved natural language processing task. In this paper, we explore the potential of recent large language models (LLMs) that have exhibited remarkable performance across various domains and tasks to tackle this task. We employ three linearization strategies to transform output trees into symbol sequences, such that LLMs can solve constituency parsing by generating linearized trees. We conduct experiments using a diverse range of LLMs, including ChatGPT, GPT-4, OPT, LLaMA, and Alpaca, comparing their performance against the state-of-the-art constituency parsers. Our experiments encompass zero-shot, few-shot, and full-training learning settings, and we evaluate the models on one in-domain and five out-of-domain test datasets. Our findings reveal insights into LLMs' performance, generalization abilities, and challenges in constituency parsing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#20986;&#21475;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#24694;&#24847;&#20943;&#36895;&#30340;&#31283;&#20581;&#24615;, &#21457;&#29616;&#22797;&#26434;&#30340;&#26426;&#21046;&#26356;&#26131;&#21463;&#21040;&#24694;&#24847;&#20943;&#36895;&#30340;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#23545;&#25239;&#35757;&#32451;&#26080;&#25928;&#65292;&#20294;&#23545;&#35805;&#27169;&#22411;&#30340;&#36755;&#20837;&#28165;&#27927;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.19152</link><description>&lt;p&gt;
BERT&#22833;&#21435;&#32784;&#24515;&#23545;&#25239;&#24694;&#24847;&#20943;&#36895;&#19981;&#33021;&#20445;&#25345;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
BERT Lost Patience Won't Be Robust to Adversarial Slowdown. (arXiv:2310.19152v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#20986;&#21475;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#24694;&#24847;&#20943;&#36895;&#30340;&#31283;&#20581;&#24615;, &#21457;&#29616;&#22797;&#26434;&#30340;&#26426;&#21046;&#26356;&#26131;&#21463;&#21040;&#24694;&#24847;&#20943;&#36895;&#30340;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#23545;&#25239;&#35757;&#32451;&#26080;&#25928;&#65292;&#20294;&#23545;&#35805;&#27169;&#22411;&#30340;&#36755;&#20837;&#28165;&#27927;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#22810;&#20986;&#21475;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#24694;&#24847;&#20943;&#36895;&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#23457;&#26680;&#20854;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20943;&#36895;&#25915;&#20987;&#65292;&#29983;&#25104;&#32469;&#36807;&#26089;&#26399;&#36864;&#20986;&#28857;&#30340;&#33258;&#28982;&#24694;&#24847;&#25991;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#25152;&#24471;&#21040;&#30340;WAFFLE&#25915;&#20987;&#20316;&#20026;&#24037;&#20855;&#65292;&#23545;&#19977;&#31181;&#22810;&#20986;&#21475;&#26426;&#21046;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#25239;&#24694;&#24847;&#20943;&#36895;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#26174;&#33879;&#38477;&#20302;&#20102;&#36825;&#19977;&#31181;&#26041;&#27861;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#19979;&#25552;&#20379;&#30340;&#35745;&#31639;&#33410;&#30465;&#25928;&#26524;&#12290;&#26426;&#21046;&#36234;&#22797;&#26434;&#65292;&#36234;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#20943;&#36895;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#23545;&#25200;&#21160;&#30340;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#20102;&#35821;&#35328;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;&#25105;&#20204;&#30340;&#25915;&#20987;&#29983;&#25104;&#30340;&#24120;&#35265;&#25200;&#21160;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#19982;&#26631;&#20934;&#30340;&#24694;&#24847;&#25991;&#26412;&#25915;&#20987;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#25239;&#35757;&#32451;&#22312;&#25171;&#36133;&#25105;&#20204;&#30340;&#20943;&#36895;&#25915;&#20987;&#26041;&#38754;&#26159;&#26080;&#25928;&#30340;&#65292;&#20294;&#20351;&#29992;&#23545;&#35805;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#36827;&#34892;&#36755;&#20837;&#28165;&#27927;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we systematically evaluate the robustness of multi-exit language models against adversarial slowdown. To audit their robustness, we design a slowdown attack that generates natural adversarial text bypassing early-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark against adversarial slowdown. We then show our attack significantly reduces the computational savings provided by the three methods in both white-box and black-box settings. The more complex a mechanism is, the more vulnerable it is to adversarial slowdown. We also perform a linguistic analysis of the perturbed text inputs, identifying common perturbation patterns that our attack generates, and comparing them with standard adversarial text attacks. Moreover, we show that adversarial training is ineffective in defeating our slowdown attack, but input sanitization with a conversational model, e.g., ChatGPT, can r
&lt;/p&gt;</description></item><item><title>TeacherLM-7.1B&#26159;&#19968;&#20010;&#23567;&#22411;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#65292;&#25945;&#20250;&#20854;&#20182;&#27169;&#22411;&#8220;&#20026;&#20160;&#20040;&#8221;&#32780;&#19981;&#20165;&#20165;&#26159;&#8220;&#20160;&#20040;&#8221;&#12290;&#23427;&#22312;MMLU&#19978;&#21462;&#24471;&#20102;52.3&#30340;&#38646;&#26679;&#26412;&#24471;&#20998;&#65292;&#21516;&#26102;&#20855;&#26377;&#20986;&#33394;&#30340;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#12290;&#21457;&#24067;TeacherLM&#31995;&#21015;&#27169;&#22411;&#21644;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#20316;&#20026;&#24320;&#28304;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2310.19019</link><description>&lt;p&gt;
TeacherLM: &#25945;&#20154;&#25171;&#40060;&#32780;&#19981;&#26159;&#32473;&#40060;&#65292;&#35821;&#35328;&#24314;&#27169;&#21516;&#29702;
&lt;/p&gt;
&lt;p&gt;
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise. (arXiv:2310.19019v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19019
&lt;/p&gt;
&lt;p&gt;
TeacherLM-7.1B&#26159;&#19968;&#20010;&#23567;&#22411;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#65292;&#25945;&#20250;&#20854;&#20182;&#27169;&#22411;&#8220;&#20026;&#20160;&#20040;&#8221;&#32780;&#19981;&#20165;&#20165;&#26159;&#8220;&#20160;&#20040;&#8221;&#12290;&#23427;&#22312;MMLU&#19978;&#21462;&#24471;&#20102;52.3&#30340;&#38646;&#26679;&#26412;&#24471;&#20998;&#65292;&#21516;&#26102;&#20855;&#26377;&#20986;&#33394;&#30340;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#12290;&#21457;&#24067;TeacherLM&#31995;&#21015;&#27169;&#22411;&#21644;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#20316;&#20026;&#24320;&#28304;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24778;&#20154;&#30340;&#25512;&#29702;&#21644;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23567;&#22411;&#27169;&#22411;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TeacherLM-7.1B&#65292;&#33021;&#22815;&#32473;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26679;&#26412;&#36827;&#34892;&#30456;&#20851;&#22522;&#30784;&#30693;&#35782;&#12289;&#24605;&#32500;&#38142;&#21644;&#24120;&#35265;&#38169;&#35823;&#30340;&#27880;&#37322;&#65292;&#20351;&#27880;&#37322;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#31572;&#26696;&#65292;&#32780;&#19988;&#20351;&#20854;&#20182;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#8220;&#20026;&#20160;&#20040;&#8221;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#8220;&#20160;&#20040;&#8221;&#12290;TeacherLM-7.1B&#27169;&#22411;&#22312;MMLU&#19978;&#23454;&#29616;&#20102;52.3&#30340;&#38646;&#26679;&#26412;&#24471;&#20998;&#65292;&#36229;&#36807;&#20102;&#25317;&#26377;100B&#21442;&#25968;&#30340;&#22823;&#22810;&#25968;&#27169;&#22411;&#12290;&#26356;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#20854;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#12290;&#22522;&#20110;TeacherLM-7.1B&#65292;&#25105;&#20204;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#20351;&#29992;&#20102;&#26469;&#33258;OPT&#21644;BLOOM&#31995;&#21015;&#30340;&#19981;&#21516;&#21442;&#25968;&#30340;&#22810;&#20010;&#23398;&#29983;&#27169;&#22411;&#23545;58&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TeacherLM&#25552;&#20379;&#30340;&#25968;&#25454;&#22686;&#24378;&#24102;&#26469;&#20102;&#26174;&#30528;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#23558;&#20316;&#20026;&#24320;&#28304;&#21457;&#24067;TeacherLM&#31995;&#21015;&#27169;&#22411;&#21644;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn "why" instead of just "what". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.
&lt;/p&gt;</description></item><item><title>LoRAShear&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#21098;&#26525;&#21644;&#30693;&#35782;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#21098;&#26525;&#21644;&#21160;&#24577;&#24494;&#35843;&#65292;&#26377;&#25928;&#20943;&#23569;LLMs&#30340;&#21344;&#29992;&#31354;&#38388;&#24182;&#19988;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18356</link><description>&lt;p&gt;
LoRAShear: &#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#21098;&#26525;&#21644;&#30693;&#35782;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery. (arXiv:2310.18356v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18356
&lt;/p&gt;
&lt;p&gt;
LoRAShear&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#21098;&#26525;&#21644;&#30693;&#35782;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#21098;&#26525;&#21644;&#21160;&#24577;&#24494;&#35843;&#65292;&#26377;&#25928;&#20943;&#23569;LLMs&#30340;&#21344;&#29992;&#31354;&#38388;&#24182;&#19988;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#26684;&#23616;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#22312;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LoRAShear&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#32467;&#26500;&#21270;&#21098;&#26525;LLMs&#24182;&#24674;&#22797;&#30693;&#35782;&#12290;LoRAShear&#39318;&#20808;&#22312;LoRA&#27169;&#22359;&#19978;&#21019;&#24314;&#20381;&#36182;&#22270;&#65292;&#20197;&#21457;&#29616;&#26368;&#23567;&#21024;&#38500;&#32467;&#26500;&#24182;&#20998;&#26512;&#30693;&#35782;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;LoRA&#36866;&#37197;&#22120;&#19978;&#36827;&#34892;&#28176;&#36827;&#24335;&#32467;&#26500;&#21098;&#26525;&#65292;&#24182;&#23454;&#29616;&#20869;&#22312;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#20887;&#20313;&#32467;&#26500;&#20013;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#24674;&#22797;&#21098;&#26525;&#26399;&#38388;&#20002;&#22833;&#30340;&#30693;&#35782;&#65292;LoRAShear&#20180;&#32454;&#30740;&#31350;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24494;&#35843;&#26041;&#26696;&#65292;&#20351;&#29992;&#21160;&#24577;&#25968;&#25454;&#36866;&#37197;&#22120;&#65292;&#20197;&#26377;&#25928;&#32553;&#23567;&#19982;&#23436;&#25972;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#20351;&#29992;&#19968;&#22359;GPU&#22312;&#20960;&#22825;&#20869;&#65292;LoRAShear&#23558;LLMs&#30340;&#21344;&#29992;&#31354;&#38388;&#26377;&#25928;&#20943;&#23569;&#20102;20%&#65292;&#20165;&#26377;1.0%&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have transformed the landscape of artificial intelligence, while their enormous size presents significant challenges in terms of computational costs. We introduce LoRAShear, a novel efficient approach to structurally prune LLMs and recover knowledge. Given general LLMs, LoRAShear at first creates the dependency graphs over LoRA modules to discover minimally removal structures and analyze the knowledge distribution. It then proceeds progressive structured pruning on LoRA adaptors and enables inherent knowledge transfer to better preserve the information in the redundant structures. To recover the lost knowledge during pruning, LoRAShear meticulously studies and proposes a dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow down the performance gap to the full models. Numerical results demonstrate that by only using one GPU within a couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with only 1.0% performance d
&lt;/p&gt;</description></item><item><title>AllTogether&#26159;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#36890;&#36807;&#22686;&#24378;&#20219;&#21153;&#32972;&#26223;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22522;&#20110;HTML&#30340;Web&#23548;&#33322;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;HTML&#20195;&#30721;&#29255;&#27573;&#30340;&#38271;&#24230;&#21644;&#21382;&#21490;&#36712;&#36857;&#23545;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#22312;&#23454;&#26102;&#29615;&#22659;&#21453;&#39304;&#26041;&#38754;&#65292;&#20248;&#20110;&#20043;&#21069;&#30340;&#36880;&#27493;&#25351;&#23548;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26410;&#26469;LLM&#39537;&#21160;&#30340;Web&#20195;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.18331</link><description>&lt;p&gt;
AllTogether&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20351;&#29992;&#25340;&#25509;&#25552;&#31034;&#36827;&#34892;Web&#23548;&#33322;&#30340;&#25928;&#26524;&#36827;&#34892;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models. (arXiv:2310.18331v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18331
&lt;/p&gt;
&lt;p&gt;
AllTogether&#26159;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#36890;&#36807;&#22686;&#24378;&#20219;&#21153;&#32972;&#26223;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22522;&#20110;HTML&#30340;Web&#23548;&#33322;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;HTML&#20195;&#30721;&#29255;&#27573;&#30340;&#38271;&#24230;&#21644;&#21382;&#21490;&#36712;&#36857;&#23545;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#22312;&#23454;&#26102;&#29615;&#22659;&#21453;&#39304;&#26041;&#38754;&#65292;&#20248;&#20110;&#20043;&#21069;&#30340;&#36880;&#27493;&#25351;&#23548;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26410;&#26469;LLM&#39537;&#21160;&#30340;Web&#20195;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#29992;&#20110;Web&#23548;&#33322;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#20195;&#29702;&#65292;&#23427;&#20204;&#35299;&#37322;&#30446;&#26631;&#24182;&#19982;Web&#39029;&#38754;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20219;&#21153;&#20013;&#20351;&#29992;&#25340;&#25509;&#25552;&#31034;&#30340;&#25928;&#26524;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AllTogether&#65292;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#22686;&#24378;&#20219;&#21153;&#32972;&#26223;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;LLMs&#22312;&#22522;&#20110;HTML&#30340;Web&#23548;&#33322;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#26469;&#33258;&#24320;&#28304;Llama-2&#21644;&#21487;&#35775;&#38382;&#30340;GPT&#27169;&#22411;&#30340;&#25552;&#31034;&#23398;&#20064;&#21644;&#25351;&#20196;&#24494;&#35843;&#26469;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;Web&#23548;&#33322;&#20219;&#21153;&#20013;&#20248;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;HTML&#20195;&#30721;&#29255;&#27573;&#30340;&#38271;&#24230;&#21644;&#21382;&#21490;&#36712;&#36857;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#19988;&#20043;&#21069;&#30340;&#36880;&#27493;&#25351;&#23548;&#27604;&#23454;&#26102;&#29615;&#22659;&#21453;&#39304;&#26356;&#26377;&#25928;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26410;&#26469;LLM&#39537;&#21160;&#30340;Web&#20195;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLMs' performance in HTML-based web navigation. We evaluate the efficacy of this approach through prompt learning and instruction finetuning based on open-source Llama-2 and API-accessible GPT models. Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks. Additionally, we find that the length of HTML snippet and history trajectory significantly influence performance, and prior step-by-step instructions prove less effective than real-time environmental feedback. Overall, we believe our work provides valuable insights for future research in LLM-driven web agents.
&lt;/p&gt;</description></item><item><title>TarGEN&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22810;&#27493;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#33258;&#25105;&#20462;&#27491;&#26041;&#27861;&#30830;&#20445;&#21487;&#38752;&#30340;&#26631;&#31614;&#12290;&#22312;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#25928;&#26524;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2310.17876</link><description>&lt;p&gt;
TarGEN: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30446;&#26631;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
TarGEN: Targeted Data Generation with Large Language Models. (arXiv:2310.17876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17876
&lt;/p&gt;
&lt;p&gt;
TarGEN&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22810;&#27493;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#33258;&#25105;&#20462;&#27491;&#26041;&#27861;&#30830;&#20445;&#21487;&#38752;&#30340;&#26631;&#31614;&#12290;&#22312;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#25928;&#26524;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;&#30340;&#20852;&#36259;&#65292;&#26088;&#22312;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21512;&#25104;&#25968;&#25454;&#38598;&#24448;&#24448;&#32570;&#20047;&#22810;&#26679;&#24615;&#24182;&#19988;&#23384;&#22312;&#22122;&#22768;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TarGEN&#65292;&#19968;&#31181;&#21033;&#29992;LLM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22810;&#27493;&#25552;&#31034;&#31574;&#30053;&#12290;TarGEN&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#26080;&#38656;&#31181;&#23376;&#65307;&#23427;&#19981;&#38656;&#35201;&#29305;&#23450;&#30340;&#20219;&#21153;&#23454;&#20363;&#65292;&#25193;&#22823;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#20351;LLM&#33021;&#22815;&#22312;&#21019;&#24314;&#25968;&#25454;&#38598;&#36807;&#31243;&#20013;&#32416;&#27491;&#26631;&#35760;&#38169;&#35823;&#30340;&#23454;&#20363;&#65292;&#30830;&#20445;&#21487;&#38752;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;8&#20010;&#20219;&#21153;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#21407;&#22987;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#20102;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#20165;&#32534;&#30721;&#22120;&#12289;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#22312;&#21407;&#22987;&#27979;&#35797;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25928;&#26524;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) has sparked interest in data synthesis techniques, aiming to generate diverse and high-quality synthetic datasets. However, these synthetic datasets often suffer from a lack of diversity and added noise. In this paper, we present TarGEN, a multi-step prompting strategy for generating high-quality synthetic datasets utilizing a LLM. An advantage of TarGEN is its seedless nature; it does not require specific task instances, broadening its applicability beyond task replication. We augment TarGEN with a method known as self-correction empowering LLMs to rectify inaccurately labeled instances during dataset creation, ensuring reliable labels. To assess our technique's effectiveness, we emulate 8 tasks from the SuperGLUE benchmark and finetune various language models, including encoder-only, encoder-decoder, and decoder-only models on both synthetic and original training sets. Evaluation on the original test set reveals that models traine
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RadGraph&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#39118;&#26684;&#24863;&#30693;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#25253;&#21578;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#20998;&#24320;&#22788;&#29702;&#65292;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#20020;&#24202;&#19981;&#20934;&#30830;&#30340;&#25253;&#21578;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#29983;&#25104;&#19982;&#20010;&#20307;&#25918;&#23556;&#31185;&#21307;&#29983;&#39118;&#26684;&#23436;&#20840;&#30456;&#21516;&#30340;&#25253;&#21578;&#12290;</title><link>http://arxiv.org/abs/2310.17811</link><description>&lt;p&gt;
&#20351;&#29992;RadGraph&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#39118;&#26684;&#24863;&#30693;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting. (arXiv:2310.17811v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RadGraph&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#39118;&#26684;&#24863;&#30693;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#25253;&#21578;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#20998;&#24320;&#22788;&#29702;&#65292;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#20020;&#24202;&#19981;&#20934;&#30830;&#30340;&#25253;&#21578;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#29983;&#25104;&#19982;&#20010;&#20307;&#25918;&#23556;&#31185;&#21307;&#29983;&#39118;&#26684;&#23436;&#20840;&#30456;&#21516;&#30340;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20174;&#21307;&#23398;&#24433;&#20687;&#20013;&#29983;&#25104;&#25253;&#21578;&#26377;&#26395;&#25913;&#21892;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#30452;&#25509;&#20174;&#22270;&#20687;&#29983;&#25104;&#23436;&#25972;&#30340;&#25253;&#21578;&#26469;&#32771;&#34385;&#22270;&#20687;&#21040;&#25253;&#21578;&#30340;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#28151;&#28102;&#20102;&#25253;&#21578;&#30340;&#20869;&#23481;&#65288;&#22914;&#21457;&#29616;&#21644;&#20854;&#23646;&#24615;&#65289;&#19982;&#20854;&#39118;&#26684;&#65288;&#22914;&#26684;&#24335;&#21644;&#35789;&#27719;&#36873;&#25321;&#65289;&#65292;&#21487;&#33021;&#23548;&#33268;&#20020;&#24202;&#19981;&#20934;&#30830;&#30340;&#25253;&#21578;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#20004;&#27493;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20869;&#23481;&#65292;&#28982;&#21518;&#23558;&#25552;&#21462;&#30340;&#20869;&#23481;&#36716;&#21270;&#20026;&#19982;&#29305;&#23450;&#25918;&#23556;&#31185;&#21307;&#29983;&#39118;&#26684;&#30456;&#21305;&#37197;&#30340;&#25253;&#21578;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;RadGraph&#8212;&#8212;&#19968;&#31181;&#25253;&#21578;&#30340;&#22270;&#34920;&#31034;&#8212;&#8212;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#22312;&#23450;&#37327;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#30410;&#22788;&#12290;&#36890;&#36807;&#20020;&#24202;&#35780;&#20272;&#32773;&#36827;&#34892;&#30340;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;AI&#29983;&#25104;&#30340;&#25253;&#21578;&#19982;&#20010;&#20307;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#39118;&#26684;&#23436;&#20840;&#30456;&#21516;&#65292;&#26080;&#27861;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist 
&lt;/p&gt;</description></item><item><title>CodeFusion&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#20013;&#36935;&#21040;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.17680</link><description>&lt;p&gt;
CodeFusion: &#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17680
&lt;/p&gt;
&lt;p&gt;
CodeFusion&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#20013;&#36935;&#21040;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#19968;&#20010;&#24320;&#21457;&#32773;&#21482;&#33021;&#20462;&#25913;&#20854;&#26368;&#21518;&#19968;&#34892;&#20195;&#30721;&#65292;&#22312;&#27491;&#30830;&#20043;&#21069;&#65292;&#20182;&#20204;&#38656;&#35201;&#22810;&#23569;&#27425;&#20174;&#22836;&#24320;&#22987;&#32534;&#20889;&#20989;&#25968;&#21602;&#65311;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20063;&#26377;&#31867;&#20284;&#30340;&#38480;&#21046;&#65306;&#23427;&#20204;&#19981;&#23481;&#26131;&#37325;&#26032;&#32771;&#34385;&#20043;&#21069;&#29983;&#25104;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CodeFusion&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#23545;&#20197;&#32534;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#23436;&#25972;&#31243;&#24207;&#36827;&#34892;&#21435;&#22122;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#38024;&#23545;Bash&#12289;Python&#21644;Microsoft Excel&#26465;&#20214;&#26684;&#24335;(CF)&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#23545;CodeFusion&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CodeFusion&#65288;75M&#21442;&#25968;&#65289;&#22312;top-1&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#65288;350M-175B&#21442;&#25968;&#65289;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;top-3&#21644;top-5&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#20248;&#20110;&#23427;&#20204;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#22312;&#22810;&#26679;&#24615;&#19982;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.
&lt;/p&gt;</description></item><item><title>FormaT5&#26159;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#25551;&#36848;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;</title><link>http://arxiv.org/abs/2310.17306</link><description>&lt;p&gt;
FormaT5: &#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26465;&#20214;&#34920;&#26684;&#26684;&#24335;&#21270;&#30340;&#25277;&#26679;&#21644;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17306
&lt;/p&gt;
&lt;p&gt;
FormaT5&#26159;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#25551;&#36848;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#30340;&#26684;&#24335;&#21270;&#26159;&#21487;&#35270;&#21270;&#12289;&#23637;&#31034;&#21644;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#32534;&#20889;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#26469;&#33258;&#21160;&#26684;&#24335;&#21270;&#34920;&#26684;&#12290;&#20294;&#23545;&#29992;&#25143;&#26469;&#35828;&#65292;&#32534;&#20889;&#36825;&#26679;&#30340;&#35268;&#21017;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#20182;&#20204;&#29702;&#35299;&#21644;&#23454;&#29616;&#24213;&#23618;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;FormaT5&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#26399;&#26395;&#30340;&#26684;&#24335;&#36923;&#36753;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#19968;&#20010;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29992;&#25143;&#20026;&#36825;&#20123;&#20219;&#21153;&#25552;&#20379;&#30340;&#25551;&#36848;&#36890;&#24120;&#26159;&#19981;&#26126;&#30830;&#25110;&#21547;&#31946;&#30340;&#65292;&#36825;&#20351;&#24471;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#38590;&#20197;&#22312;&#19968;&#27493;&#20013;&#20934;&#30830;&#23398;&#20064;&#21040;&#25152;&#38656;&#30340;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#35268;&#33539;&#19981;&#36275;&#30340;&#38382;&#39064;&#24182;&#20943;&#23569;&#21442;&#25968;&#38169;&#35823;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;&#36825;&#20123;&#21344;&#20301;&#31526;&#21487;&#20197;&#30001;&#31532;&#20108;&#20010;&#27169;&#22411;&#25110;&#32773;&#24403;&#21487;&#29992;&#30340;&#34892;&#31034;&#20363;&#26102;&#65292;&#30001;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#32534;&#31243;&#31995;&#32479;&#22635;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#37327;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#36827;&#34892;&#20943;&#36731;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22238;&#28335;&#27861;&#32416;&#27491;&#65292;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Correction with Backtracking Reduces Hallucination in Summarization. (arXiv:2310.16176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#37327;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#36827;&#34892;&#20943;&#36731;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#28304;&#25991;&#20214;&#30340;&#33258;&#28982;&#35821;&#35328;&#25688;&#35201;&#65292;&#26082;&#31616;&#27905;&#21448;&#20445;&#30041;&#37325;&#35201;&#20803;&#32032;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#31070;&#32463;&#25991;&#26412;&#25688;&#35201;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65288;&#25110;&#26356;&#20934;&#30830;&#22320;&#35828;&#26159;&#28151;&#28102;&#65289;&#65292;&#21363;&#29983;&#25104;&#30340;&#25688;&#35201;&#21253;&#21547;&#28304;&#25991;&#20214;&#20013;&#27809;&#26377;&#26681;&#25454;&#30340;&#32454;&#33410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;CoBa&#65292;&#29992;&#20110;&#20943;&#23569;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#27493;&#39588;&#65306;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#36731;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#27979;&#37327;&#26377;&#20851;&#26465;&#20214;&#35789;&#27010;&#29575;&#21644;&#19978;&#19979;&#25991;&#35789;&#36317;&#31163;&#30340;&#31616;&#21333;&#32479;&#35745;&#20449;&#24687;&#21487;&#20197;&#23454;&#29616;&#21069;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#30452;&#35266;&#30340;&#22238;&#28335;&#27861;&#22312;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#24778;&#20154;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CoBa&#22312;&#20943;&#23569;&#25688;&#35201;&#24187;&#35273;&#26041;&#38754;&#26159;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstractive summarization aims at generating natural language summaries of a source document that are succinct while preserving the important elements. Despite recent advances, neural text summarization models are known to be susceptible to hallucinating (or more correctly confabulating), that is to produce summaries with details that are not grounded in the source document. In this paper, we introduce a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization. The approach is based on two steps: hallucination detection and mitigation. We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words. Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation. We thoroughly evaluate the proposed method with prior art on three benchmark datasets for text summarization. The results show that CoBa is effective and efficient in reducing hall
&lt;/p&gt;</description></item><item><title>FANToM&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#21387;&#21147;&#27979;&#35797;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#24471;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.15421</link><description>&lt;p&gt;
FANToM: &#22312;&#20132;&#20114;&#20013;&#23545;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. (arXiv:2310.15421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15421
&lt;/p&gt;
&lt;p&gt;
FANToM&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#21387;&#21147;&#27979;&#35797;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#24471;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#30340;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#32570;&#20047;&#20114;&#21160;&#24615;&#30340;&#34987;&#21160;&#25925;&#20107;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FANToM&#65292;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#24515;&#26234;&#29702;&#35770;&#30340;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#32467;&#21512;&#20102;&#24515;&#29702;&#23398;&#20013;&#30340;&#37325;&#35201;&#29702;&#35770;&#35201;&#27714;&#21644;&#23545;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#24517;&#35201;&#30340;&#32463;&#39564;&#32771;&#34385;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#22810;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#35201;&#27714;&#30456;&#21516;&#30340;&#22522;&#26412;&#25512;&#29702;&#26469;&#35782;&#21035;LLM&#20013;&#19981;&#23384;&#22312;&#25110;&#34394;&#20551;&#30340;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FANToM&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;LLM&#20063;&#34920;&#29616;&#27604;&#20154;&#31867;&#24046;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#29992;&#20110;&#36234;&#21335;&#31038;&#21306;&#30340;&#29983;&#25104;&#21069;&#35757;&#32451;&#36716;&#25442;&#22120;(GPT-2)&#65292;&#19987;&#27880;&#20110;COVID-19&#30456;&#20851;&#38382;&#31572;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-2&#27169;&#22411;&#22312;&#31038;&#21306;COVID-19&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.14602</link><description>&lt;p&gt;
&#29992;&#20110;&#36234;&#21335;&#31038;&#21306;&#22522;&#20110;COVID-19&#38382;&#31572;&#30340;&#29983;&#25104;&#21069;&#35757;&#32451;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer for Vietnamese Community-based COVID-19 Question Answering. (arXiv:2310.14602v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#29992;&#20110;&#36234;&#21335;&#31038;&#21306;&#30340;&#29983;&#25104;&#21069;&#35757;&#32451;&#36716;&#25442;&#22120;(GPT-2)&#65292;&#19987;&#27880;&#20110;COVID-19&#30456;&#20851;&#38382;&#31572;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-2&#27169;&#22411;&#22312;&#31038;&#21306;COVID-19&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#29983;&#25104;&#21069;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#65292;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24191;&#27867;&#28508;&#21147;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290; GPT&#24050;&#34987;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#38382;&#31572;&#31995;&#32479;&#20013;&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#26377;&#20851;GPT&#22312;&#36234;&#21335;&#35821;&#20013;&#24212;&#29992;&#30340;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#20173;&#28982;&#26377;&#38480;&#12290; &#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22312;&#36234;&#21335;&#35821;&#20013;&#29305;&#21035;&#20851;&#27880;COVID-19&#30456;&#20851;&#26597;&#35810;&#30340;&#31038;&#21306;&#38382;&#31572;&#20013;&#23454;&#29616;GPT-2&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#31038;&#21306;COVID-19&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#30340;&#36716;&#25442;&#22120;&#19982;SOTA&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-2&#27169;&#22411;&#26174;&#31034;&#20986;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20248;&#20110;&#20854;&#20182;SOTA&#27169;&#22411;&#20197;&#21450;&#20808;&#21069;&#30340;&#31038;&#21306;COVID-19&#38382;&#31572;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have provided empirical evidence of the wide-ranging potential of Generative Pre-trained Transformer (GPT), a pretrained language model, in the field of natural language processing. GPT has been effectively employed as a decoder within state-of-the-art (SOTA) question answering systems, yielding exceptional performance across various tasks. However, the current research landscape concerning GPT's application in Vietnamese remains limited. This paper aims to address this gap by presenting an implementation of GPT-2 for community-based question answering specifically focused on COVID-19 related queries in Vietnamese. We introduce a novel approach by conducting a comparative analysis of different Transformers vs SOTA models in the community-based COVID-19 question answering dataset. The experimental findings demonstrate that the GPT-2 models exhibit highly promising outcomes, outperforming other SOTA models as well as previous community-based COVID-19 question answering mod
&lt;/p&gt;</description></item><item><title>StereoMap&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21051;&#26495;&#21360;&#35937;&#20869;&#23481;&#27169;&#22411;&#30340;&#32500;&#24230;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#21508;&#32676;&#20307;&#30340;&#24863;&#30693;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#36825;&#20123;&#32676;&#20307;&#34920;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#24863;&#30693;&#65292;&#20855;&#26377;&#28151;&#21512;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.13673</link><description>&lt;p&gt;
&#31435;&#20307;&#22320;&#22270;&#65306;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20154;&#31867;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models. (arXiv:2310.13673v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13673
&lt;/p&gt;
&lt;p&gt;
StereoMap&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21051;&#26495;&#21360;&#35937;&#20869;&#23481;&#27169;&#22411;&#30340;&#32500;&#24230;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#21508;&#32676;&#20307;&#30340;&#24863;&#30693;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#36825;&#20123;&#32676;&#20307;&#34920;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#24863;&#30693;&#65292;&#20855;&#26377;&#28151;&#21512;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32534;&#30721;&#21644;&#20256;&#25773;&#26377;&#23475;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;StereoMap&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#23545;&#31038;&#20250;&#21508;&#32676;&#20307;&#30340;&#30475;&#27861;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#24515;&#29702;&#23398;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#20869;&#23481;&#27169;&#22411;&#65288;SCM&#65289;&#65292;&#35813;&#29702;&#35770;&#35748;&#20026;&#21051;&#26495;&#21360;&#35937;&#24182;&#19981;&#30456;&#21516;&#65292;&#32780;&#26159;&#28201;&#26262;&#21644;&#33021;&#21147;&#36825;&#20004;&#20010;&#32500;&#24230;&#30340;&#22240;&#32032;&#21010;&#20998;&#20102;&#21051;&#26495;&#21360;&#35937;&#30340;&#24615;&#36136;&#12290;&#22522;&#20110;SCM&#29702;&#35770;&#65292;StereoMap&#20351;&#29992;&#28201;&#26262;&#21644;&#33021;&#21147;&#36825;&#20004;&#20010;&#32500;&#24230;&#26469;&#25551;&#32472;LLMs&#23545;&#31038;&#20250;&#32676;&#20307;&#65288;&#36890;&#36807;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#23450;&#20041;&#65289;&#30340;&#24863;&#30693;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#21487;&#20197;&#35843;&#26597;LLMs&#21028;&#26029;&#30340;&#20851;&#38190;&#35789;&#21644;&#25512;&#29702;&#30340;&#35328;&#35821;&#21270;&#65292;&#20197;&#25581;&#31034;&#24433;&#21709;&#23427;&#20204;&#24863;&#30693;&#30340;&#28508;&#22312;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#23545;&#36825;&#20123;&#32676;&#20307;&#34920;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#24863;&#30693;&#65292;&#20855;&#26377;&#28151;&#21512;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is grounded in the Stereotype Content Model (SCM); a well-established theory from psychology. According to SCM, stereotypes are not all alike. Instead, the dimensions of Warmth and Competence serve as the factors that delineate the nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs' perceptions of social groups (defined by socio-demographic features) using the dimensions of Warmth and Competence. Furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of LLMs' judgments to uncover underlying factors influencing their perceptions. Our results show that LLMs exhibit a diverse range of perceptions towards these groups, characterized by mixed
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.13032</link><description>&lt;p&gt;
AI&#21453;&#39304;&#20419;&#36827;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13032
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#19981;&#20165;&#20559;&#22909;&#21333;&#19968;&#22238;&#22797;&#65292;&#32780;&#26159;&#24076;&#26395;&#24471;&#21040;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#36755;&#20986;&#20197;&#20379;&#36873;&#25321;&#12290;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#25628;&#32034;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#21644;&#22810;&#26679;&#21270;&#20505;&#36873;&#20154;&#32676;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;QD&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#31561;&#36136;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#31639;&#27861;&#25351;&#23450;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#36890;&#36807;AI&#21453;&#39304;&#25351;&#23548;&#25628;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;LMs&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#34987;&#25552;&#31034;&#26469;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#24615;&#26041;&#38754;&#12290;&#20511;&#21161;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;AI&#21453;&#39304;&#23454;&#29616;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;&#65288;QDAIF&#65289;&#65292;&#20854;&#20013;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;LMs&#26469;&#29983;&#25104;&#21464;&#24322;&#24182;&#35780;&#20272;&#20505;&#36873;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#38750;QDAIF&#31639;&#27861;&#30456;&#27604;&#65292;QDAIF&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25351;&#23450;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#21464;&#21270;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12289;&#35789;&#27719;&#37325;&#21472;&#21644;&#35821;&#35328;&#29305;&#24615;&#26159;&#24433;&#21709;&#24615;&#33021;&#21464;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.10385</link><description>&lt;p&gt;
&#23545;&#20110;&#38646;&#26679;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#21464;&#21270;&#30340;&#26356;&#22909;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance. (arXiv:2310.10385v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10385
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#21464;&#21270;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12289;&#35789;&#27719;&#37325;&#21472;&#21644;&#35821;&#35328;&#29305;&#24615;&#26159;&#24433;&#21709;&#24615;&#33021;&#21464;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;MNMT&#65289;&#20419;&#36827;&#20102;&#30693;&#35782;&#20998;&#20139;&#65292;&#20294;&#24448;&#24448;&#22312;&#38646;&#26679;&#26412;&#65288;ZS&#65289;&#32763;&#35793;&#36136;&#37327;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#25972;&#20307;ZS&#24615;&#33021;&#20302;&#19979;&#30340;&#21407;&#22240;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65306;ZS&#24615;&#33021;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#36825;&#34920;&#26126;MNMT&#30340;ZS&#33021;&#21147;&#24182;&#19981;&#26159;&#22343;&#21248;&#22320;&#24046;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#20135;&#29983;&#20102;&#21512;&#29702;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;40&#31181;&#35821;&#35328;&#30340;1,560&#20010;&#35821;&#35328;&#26041;&#21521;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#20010;&#24433;&#21709;ZS NMT&#24615;&#33021;&#21464;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;1&#65289;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;2&#65289;&#35789;&#27719;&#37325;&#21472;&#65292;3&#65289;&#35821;&#35328;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#26159;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#22240;&#32032;&#65292;&#35789;&#27719;&#37325;&#21472;&#22987;&#32456;&#24433;&#21709;ZS&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35821;&#35328;&#29305;&#24615;&#65292;&#22914;&#35821;&#35328;&#23478;&#26063;&#21644;&#20070;&#20889;&#31995;&#32479;&#65292;&#23588;&#20854;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Neural Machine Translation (MNMT) facilitates knowledge sharing but often suffers from poor zero-shot (ZS) translation qualities. While prior work has explored the causes of overall low ZS performance, our work introduces a fresh perspective: the presence of high variations in ZS performance. This suggests that MNMT does not uniformly exhibit poor ZS capability; instead, certain translation directions yield reasonable results. Through systematic experimentation involving 1,560 language directions spanning 40 languages, we identify three key factors contributing to high variations in ZS NMT performance: 1) target side translation capability 2) vocabulary overlap 3) linguistic properties. Our findings highlight that the target side translation quality is the most influential factor, with vocabulary overlap consistently impacting ZS performance. Additionally, linguistic properties, such as language family and writing system, play a role, particularly with smaller models. Furt
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Ziya-VL&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#32452;&#21452;&#35821;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#35270;&#35273;&#35821;&#20041;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#26597;&#35810;&#21464;&#25442;&#22120;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#22914;&#25351;&#20196;&#35843;&#25972;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.08166</link><description>&lt;p&gt;
Ziya-VL: &#21452;&#35821;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Ziya-VL: Bilingual Large Vision-Language Model via Multi-Task Instruction Tuning. (arXiv:2310.08166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Ziya-VL&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#32452;&#21452;&#35821;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#35270;&#35273;&#35821;&#20041;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#26597;&#35810;&#21464;&#25442;&#22120;&#21644;&#20248;&#21270;&#26041;&#26696;&#65292;&#22914;&#25351;&#20196;&#35843;&#25972;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#25193;&#22823;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;&#23556;&#20987;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25104;&#21151;&#36890;&#24120;&#23616;&#38480;&#20110;&#33521;&#35821;&#22330;&#26223;&#65292;&#21407;&#22240;&#26159;&#32570;&#20047;&#22823;&#35268;&#27169;&#21644;&#39640;&#36136;&#37327;&#30340;&#38750;&#33521;&#35821;&#22810;&#27169;&#36164;&#28304;&#65292;&#20351;&#24471;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#24314;&#31435;&#31454;&#20105;&#23545;&#25163;&#21464;&#24471;&#26497;&#20854;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Ziya-VL&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#32452;&#21452;&#35821;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#65292;&#26088;&#22312;&#23558;&#35270;&#35273;&#35821;&#20041;&#34701;&#20837;LLM&#20197;&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;Ziya-VL-Base&#21644;Ziya-VL-Chat&#32452;&#25104;&#65292;&#37319;&#29992;BLIP-2&#20013;&#30340;&#26597;&#35810;&#21464;&#25442;&#22120;&#65292;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#38454;&#27573;&#35757;&#32451;&#21644;&#20302;&#31209;&#36866;&#24212;&#27169;&#22359;&#31561;&#20248;&#21270;&#26041;&#26696;&#30340;&#36741;&#21161;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21050;&#28608;GPT-4&#22312;&#22810;&#27169;&#24577;&#22330;&#26223;&#20013;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#23558;&#25105;&#20204;&#25910;&#38598;&#30340;&#33521;&#25991;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#38598;&#32763;&#35793;&#25104;...
&lt;/p&gt;
&lt;p&gt;
Recent advancements enlarge the capabilities of large language models (LLMs) in zero-shot image-to-text generation and understanding by integrating multi-modal inputs. However, such success is typically limited to English scenarios due to the lack of large-scale and high-quality non-English multi-modal resources, making it extremely difficult to establish competitive counterparts in other languages. In this paper, we introduce the Ziya-VL series, a set of bilingual large-scale vision-language models (LVLMs) designed to incorporate visual semantics into LLM for multi-modal dialogue. Composed of Ziya-VL-Base and Ziya-VL-Chat, our models adopt the Querying Transformer from BLIP-2, further exploring the assistance of optimization schemes such as instruction tuning, multi-stage training and low-rank adaptation module for visual-language alignment. In addition, we stimulate the understanding ability of GPT-4 in multi-modal scenarios, translating our gathered English image-text datasets into 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#21512;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20943;&#23569;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#29616;&#23454;&#19990;&#30028;&#30340;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2310.06827</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20219;&#21153;&#25945;&#25480;&#35821;&#35328;&#27169;&#22411;&#26356;&#23569;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Teaching Language Models to Hallucinate Less with Synthetic Tasks. (arXiv:2310.06827v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06827
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#21512;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20943;&#23569;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#29616;&#23454;&#19990;&#30028;&#30340;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65288;&#22914;&#22522;&#20110;&#25991;&#26723;&#30340;&#38382;&#31572;&#12289;&#20250;&#35758;&#27010;&#36848;&#21644;&#20020;&#24202;&#25253;&#21578;&#29983;&#25104;&#65289;&#20013;&#32463;&#24120;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;&#20351;&#25152;&#26377;&#24517;&#35201;&#20449;&#24687;&#37117;&#22312;&#19978;&#19979;&#25991;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#20248;&#21270;LLMs&#20197;&#20943;&#23569;&#24187;&#35273;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#27599;&#20010;&#20248;&#21270;&#27493;&#39588;&#20013;&#26377;&#25928;&#35780;&#20272;&#24187;&#35273;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20943;&#23569;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#20063;&#21487;&#20197;&#20943;&#23569;&#29616;&#23454;&#19990;&#30028;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;SynTra&#65292;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#21512;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#26131;&#20110;&#35825;&#21457;&#21644;&#34913;&#37327;&#24187;&#35273;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#21512;&#25104;&#20219;&#21153;&#36827;&#34892;&#21069;&#32512;&#35843;&#20248;&#26469;&#20248;&#21270;LLM&#30340;&#31995;&#32479;&#28040;&#24687;&#65292;&#24182;&#26368;&#32456;&#23558;&#31995;&#32479;&#28040;&#24687;&#36716;&#31227;&#21040;&#29616;&#23454;&#20013;&#38590;&#20197;&#20248;&#21270;&#30340;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#29616;&#23454;&#30340;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65292;&#20351;&#29992;&#20165;&#21512;&#25104;&#26816;&#32034;&#20219;&#21153;&#36827;&#34892;&#30417;&#30563;&#65292;SynTra&#20943;&#23569;&#20102;&#20004;&#20010;&#20855;&#26377;13B&#21442;&#25968;&#30340;LLMs&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36890;&#36807;&#20248;&#21270;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#28040;&#24687;&#65292;&#21487;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#29616;&#23454;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the sy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#23884;&#20837;&#26041;&#27861;&#21644;&#35789;&#35821;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#35780;&#35770;&#20043;&#38388;&#30340;&#24773;&#24863;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#35780;&#35770;&#20013;&#30340;&#25991;&#26412;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#12289;&#25919;&#27835;&#23478;&#21644;&#21830;&#19994;&#20195;&#34920;&#33021;&#22815;&#36861;&#36394;&#20840;&#29699;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#24773;&#24863;&#30340;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.05964</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#34913;&#37327;&#25991;&#26412;&#30456;&#20851;&#24615;&#30340;&#23884;&#20837;&#26041;&#27861;: &#25581;&#31034;&#22312;&#32447;&#35780;&#35770;&#20013;&#30340;&#24773;&#24863;&#21644;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring Embeddings for Measuring Text Relatedness: Unveiling Sentiments and Relationships in Online Comments. (arXiv:2310.05964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#23884;&#20837;&#26041;&#27861;&#21644;&#35789;&#35821;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#35780;&#35770;&#20043;&#38388;&#30340;&#24773;&#24863;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#35780;&#35770;&#20013;&#30340;&#25991;&#26412;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#12289;&#25919;&#27835;&#23478;&#21644;&#21830;&#19994;&#20195;&#34920;&#33021;&#22815;&#36861;&#36394;&#20840;&#29699;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#24773;&#24863;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#22330;&#23548;&#33268;&#20114;&#32852;&#32593;&#20351;&#29992;&#37327;&#22686;&#38271;70%&#30340;&#22823;&#27969;&#34892;&#30149;&#20043;&#21518;&#65292;&#20840;&#19990;&#30028;&#30340;&#20154;&#20204;&#24320;&#22987;&#26356;&#22810;&#22320;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#12290;Twitter&#12289;Meta Threads&#12289;YouTube&#21644;Reddit&#31561;&#24212;&#29992;&#31243;&#24207;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#25968;&#23383;&#31354;&#38388;&#19981;&#34920;&#36798;&#20844;&#20247;&#24847;&#35265;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#35780;&#35770;&#20043;&#38388;&#30340;&#24773;&#24863;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#19981;&#21516;&#23186;&#20307;&#24179;&#21488;&#19978;&#20849;&#20139;&#24847;&#35265;&#30340;&#37325;&#35201;&#24615;&#65292;&#20351;&#29992;&#35789;&#23884;&#20837;&#20998;&#26512;&#21477;&#23376;&#21644;&#25991;&#26723;&#20013;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#23427;&#20351;&#30740;&#31350;&#20154;&#21592;&#12289;&#25919;&#27835;&#23478;&#21644;&#21830;&#19994;&#20195;&#34920;&#33021;&#22815;&#36861;&#36394;&#20840;&#29699;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#24773;&#24863;&#30340;&#36335;&#24452;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#34913;&#37327;&#20174;&#36825;&#20123;&#28909;&#38376;&#22312;&#32447;&#24179;&#21488;&#30340;&#29992;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#30340;&#25991;&#26412;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#25429;&#25417;&#35789;&#35821;&#38388;&#35821;&#20041;&#20851;&#31995;&#24182;&#26377;&#21161;&#20110;&#20998;&#26512;&#32593;&#32476;&#24773;&#24863;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
After a pandemic that caused internet usage to grow by 70%, there has been an increased number of people all across the world using social media. Applications like Twitter, Meta Threads, YouTube, and Reddit have become increasingly pervasive, leaving almost no digital space where public opinion is not expressed. This paper investigates sentiment and semantic relationships among comments across various social media platforms, as well as discusses the importance of shared opinions across these different media platforms, using word embeddings to analyze components in sentences and documents. It allows researchers, politicians, and business representatives to trace a path of shared sentiment among users across the world. This research paper presents multiple approaches that measure the relatedness of text extracted from user comments on these popular online platforms. By leveraging embeddings, which capture semantic relationships between words and help analyze sentiments across the web, we
&lt;/p&gt;</description></item><item><title>DeBERTinha&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#27493;&#39588;&#35757;&#32451;&#21644;&#24494;&#35843;&#36866;&#24212;DebertaV3 XSmall&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#23427;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#21028;&#26029;&#21477;&#23376;&#30456;&#20851;&#24615;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16844</link><description>&lt;p&gt;
DeBERTinha: &#19968;&#31181;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;DebertaV3 XSmall&#30340;&#22810;&#27493;&#39588;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian Portuguese Natural Language Processing Task. (arXiv:2309.16844v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16844
&lt;/p&gt;
&lt;p&gt;
DeBERTinha&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#27493;&#39588;&#35757;&#32451;&#21644;&#24494;&#35843;&#36866;&#24212;DebertaV3 XSmall&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#23427;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#21028;&#26029;&#21477;&#23376;&#30456;&#20851;&#24615;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#33521;&#35821;&#39044;&#35757;&#32451;DebertaV3 XSmall&#27169;&#22411;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#22810;&#27493;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#22312;&#33889;&#33796;&#29273;&#35821;&#19978;&#30340;&#26377;&#25928;&#35843;&#20248;&#12290;&#20351;&#29992;Carolina&#21644;BrWac&#30340;&#21021;&#22987;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#35299;&#20915;&#34920;&#24773;&#31526;&#21495;&#12289;HTML&#26631;&#31614;&#21644;&#32534;&#30721;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;SentencePiece&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;50,000&#20010;token&#30340;&#33889;&#33796;&#29273;&#35821;&#29305;&#23450;&#35789;&#27719;&#34920;&#12290;&#27169;&#22411;&#19981;&#26159;&#20174;&#22836;&#35757;&#32451;&#65292;&#32780;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#33521;&#35821;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#21021;&#22987;&#21270;&#32593;&#32476;&#30340;&#22823;&#37096;&#20998;&#65292;&#20165;&#21253;&#25324;&#38543;&#26426;&#23884;&#20837;&#65292;&#20197;&#35782;&#21035;&#20174;&#22836;&#35757;&#32451;&#30340;&#26114;&#36149;&#25104;&#26412;&#12290;&#37319;&#29992;&#26367;&#25442;&#26631;&#35760;&#26816;&#27979;&#20219;&#21153;&#20197;&#19982;DebertaV3&#35757;&#32451;&#30340;&#30456;&#21516;&#26684;&#24335;&#24494;&#35843;&#27169;&#22411;&#12290;&#36866;&#24212;&#30340;&#27169;&#22411;&#31216;&#20026;DeBERTinha&#65292;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#21028;&#26029;&#21477;&#23376;&#30456;&#20851;&#24615;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#32988;&#36807;BER&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach for adapting the DebertaV3 XSmall model pre-trained in English for Brazilian Portuguese natural language processing (NLP) tasks. A key aspect of the methodology involves a multistep training process to ensure the model is effectively tuned for the Portuguese language. Initial datasets from Carolina and BrWac are preprocessed to address issues like emojis, HTML tags, and encodings. A Portuguese-specific vocabulary of 50,000 tokens is created using SentencePiece. Rather than training from scratch, the weights of the pre-trained English model are used to initialize most of the network, with random embeddings, recognizing the expensive cost of training from scratch. The model is fine-tuned using the replaced token detection task in the same format of DebertaV3 training. The adapted model, called DeBERTinha, demonstrates effectiveness on downstream tasks like named entity recognition, sentiment analysis, and determining sentence relatedness, outperforming BER
&lt;/p&gt;</description></item><item><title>COCO-Counterfactuals&#26159;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#21453;&#20107;&#23454;&#20363;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#20363;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;COCO-Counterfactuals&#30340;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#25913;&#21892;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14356</link><description>&lt;p&gt;
COCO-Counterfactuals:&#33258;&#21160;&#26500;&#24314;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#21453;&#20107;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs. (arXiv:2309.14356v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14356
&lt;/p&gt;
&lt;p&gt;
COCO-Counterfactuals&#26159;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#21453;&#20107;&#23454;&#20363;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#20363;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;COCO-Counterfactuals&#30340;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#25913;&#21892;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#20363;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#20013;&#24050;&#35777;&#26126;&#23545;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23613;&#31649;&#21453;&#20107;&#23454;&#20363;&#22312;NLP&#39046;&#22495;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#29992;&#65292;&#20294;&#30001;&#20110;&#21019;&#24314;&#26368;&#23567;&#21453;&#20107;&#23454;&#21464;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#30340;&#38590;&#24230;&#65292;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#20363;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21453;&#20107;&#23454;&#20363;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;COCO-Counterfactuals&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;MS-COCO&#25968;&#25454;&#38598;&#30340;&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#25991;&#26412;&#26631;&#39064;&#30340;&#37197;&#23545;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#39564;&#35777;&#20102;COCO-Counterfactuals&#30340;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#21453;&#20107;&#23454;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;COCO-Counterfactuals&#22312;&#25913;&#21892;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#21017;&#65292;&#26368;&#23567;&#26465;&#20214;&#20381;&#36182;&#65288;MCD&#65289;&#20934;&#21017;&#65292;&#26469;&#25581;&#31034;&#22240;&#26524;&#35299;&#37322;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36873;&#25321;&#29702;&#30001;&#20505;&#36873;&#39033;&#19978;&#26410;&#36873;&#25321;&#37096;&#20998;&#19982;&#30446;&#26631;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#24378;&#21046;&#36873;&#25321;&#25152;&#26377;&#30340;&#26631;&#31614;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2309.13391</link><description>&lt;p&gt;
&#22240;&#26524;&#33258;&#25105;&#35299;&#37322;&#20013;&#30340;D-&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
D-Separation for Causal Self-Explanation. (arXiv:2309.13391v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#21017;&#65292;&#26368;&#23567;&#26465;&#20214;&#20381;&#36182;&#65288;MCD&#65289;&#20934;&#21017;&#65292;&#26469;&#25581;&#31034;&#22240;&#26524;&#35299;&#37322;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36873;&#25321;&#29702;&#30001;&#20505;&#36873;&#39033;&#19978;&#26410;&#36873;&#25321;&#37096;&#20998;&#19982;&#30446;&#26631;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#24378;&#21046;&#36873;&#25321;&#25152;&#26377;&#30340;&#26631;&#31614;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#21270;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26694;&#26550;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26368;&#22823;&#20114;&#20449;&#24687;&#65288;MMI&#65289;&#20934;&#21017;&#26469;&#25214;&#21040;&#26368;&#33021;&#35828;&#26126;&#30446;&#26631;&#26631;&#31614;&#30340;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20934;&#21017;&#21487;&#33021;&#21463;&#21040;&#19982;&#22240;&#26524;&#35299;&#37322;&#25110;&#30446;&#26631;&#26631;&#31614;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20934;&#21017;&#26469;&#25581;&#31034;&#22240;&#26524;&#35299;&#37322;&#65292;&#31216;&#20026;&#26368;&#23567;&#26465;&#20214;&#20381;&#36182;&#65288;MCD&#65289;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#24314;&#31435;&#22312;&#25105;&#20204;&#21457;&#29616;&#30340;&#38750;&#22240;&#26524;&#29305;&#24449;&#19982;&#30446;&#26631;&#26631;&#31614;&#36890;&#36807;&#22240;&#26524;&#35299;&#37322;&#34987;&#8220;&#20998;&#31163;&#8221;&#20043;&#19978;&#12290;&#36890;&#36807;&#22312;&#36873;&#25321;&#30340;&#29702;&#30001;&#20505;&#36873;&#39033;&#19978;&#32473;&#20986;&#26410;&#36873;&#25321;&#37096;&#20998;&#19982;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#20381;&#36182;&#26368;&#23567;&#21270;&#65292;&#24378;&#21046;&#36873;&#25321;&#25152;&#26377;&#30340;&#26631;&#31614;&#21407;&#22240;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31616;&#21333;&#32780;&#23454;&#29992;&#30340;&#20381;&#36182;&#24230;&#37327;&#65292;&#20855;&#20307;&#26159;KL&#25955;&#24230;&#65292;&#26469;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;MCD&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rationalization is a self-explaining framework for NLP models. Conventional work typically uses the maximum mutual information (MMI) criterion to find the rationale that is most indicative of the target label. However, this criterion can be influenced by spurious features that correlate with the causal rationale or the target label. Instead of attempting to rectify the issues of the MMI criterion, we propose a novel criterion to uncover the causal rationale, termed the Minimum Conditional Dependence (MCD) criterion, which is grounded on our finding that the non-causal features and the target label are \emph{d-separated} by the causal rationale. By minimizing the dependence between the unselected parts of the input and the target label conditioned on the selected rationale candidate, all the causes of the label are compelled to be selected. In this study, we employ a simple and practical measure of dependence, specifically the KL-divergence, to validate our proposed MCD criterion. Empir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2309.05961</link><description>&lt;p&gt;
&#35780;&#20272;&#28526;&#36215;&#28526;&#33853;&#65306;&#23545;&#19981;&#21516;&#24179;&#21488;&#38388;&#38382;&#31572;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering Trends across Diverse Platforms. (arXiv:2309.05961v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20845;&#20010;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#26597;&#35810;&#30340;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20114;&#21160;&#27700;&#24179;&#19982;&#31532;&#19968;&#20010;&#22238;&#31572;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26597;&#35810;&#26159;&#21542;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#22240;&#20854;&#24555;&#36895;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#30340;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#22238;&#31572;&#36895;&#24230;&#30340;&#24555;&#24930;&#21462;&#20915;&#20110;&#26597;&#35810;&#29305;&#23450;&#21644;&#29992;&#25143;&#30456;&#20851;&#30340;&#22240;&#32032;&#30340;&#32508;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20845;&#20010;&#39640;&#24230;&#27969;&#34892;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22238;&#31572;&#25152;&#33457;&#36153;&#30340;&#26102;&#38388;&#19982;&#20803;&#25968;&#25454;&#12289;&#38382;&#39064;&#30340;&#26500;&#25104;&#26041;&#24335;&#21644;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#21160;&#27700;&#24179;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#20803;&#25968;&#25454;&#21644;&#29992;&#25143;&#20114;&#21160;&#27169;&#24335;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#21738;&#20123;&#26597;&#35810;&#23558;&#36805;&#36895;&#33719;&#24471;&#21021;&#22987;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question Answering (CQA) platforms steadily gain popularity as they provide users with fast responses to their queries. The swiftness of these responses is contingent on a mixture of query-specific and user-related elements. This paper scrutinizes these contributing factors within the context of six highly popular CQA platforms, identified through their standout answering speed. Our investigation reveals a correlation between the time taken to yield the first response to a question and several variables: the metadata, the formulation of the questions, and the level of interaction among users. Additionally, by employing conventional machine learning models to analyze these metadata and patterns of user interaction, we endeavor to predict which queries will receive their initial responses promptly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25552;&#20986;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#23436;&#25972;&#25991;&#26723;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12896</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#39029;&#20998;&#31867;&#65306;&#35774;&#35745;&#12289;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25552;&#20986;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#23436;&#25972;&#25991;&#26723;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#21363;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#24615;&#36136;&#19978;&#65288;$X$&#65306;&#22810;&#36890;&#36947;&#12289;&#22810;&#39029;&#12289;&#22810;&#34892;&#19994;&#65307;$Y$&#65306;&#31867;&#21035;&#20998;&#24067;&#21644;&#26631;&#31614;&#38598;&#30340;&#22810;&#26679;&#24615;&#65289;&#21644;&#32771;&#34385;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65288;$f$&#65306;&#22810;&#39029;&#25991;&#26723;&#12289;&#39029;&#38754;&#27969;&#21644;&#25991;&#26723;&#25414;&#32465;&#20998;&#31867;&#65292;...&#65289;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20844;&#20849;&#30340;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#24182;&#35268;&#33539;&#20102;&#24212;&#29992;&#22330;&#26223;&#20013;&#20135;&#29983;&#30340;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#28608;&#21457;&#20102;&#20197;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#20026;&#30446;&#26631;&#30340;&#20215;&#20540;&#12290;&#23545;&#25552;&#20986;&#30340;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#21464;&#24471;&#26080;&#20851;&#32039;&#35201;&#65292;&#24182;&#38656;&#35201;&#26356;&#26032;&#20197;&#35780;&#20272;&#23454;&#38469;&#20013;&#33258;&#28982;&#21457;&#29983;&#30340;&#23436;&#25972;&#25991;&#26723;&#12290;&#36825;&#20010;&#29616;&#23454;&#24773;&#20917;&#26816;&#26597;&#20063;&#21628;&#21505;&#26356;&#25104;&#29087;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#28085;&#30422;&#26657;&#20934;&#35780;&#20272;&#12289;&#25512;&#29702;&#22797;&#26434;&#24615;&#65288;&#26102;&#38388;-&#20869;&#23384;&#65289;&#21644;&#19968;&#31995;&#21015;&#29616;&#23454;&#20998;&#25955;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09009</link><description>&lt;p&gt;
ChatGPT&#30340;&#34892;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3.5&#21644;GPT-4&#26159;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#26356;&#26032;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#30340;2023&#24180;3&#26376;&#21644;2023&#24180;6&#26376;&#29256;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28041;&#21450;&#22235;&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;1&#65289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;2&#65289;&#22238;&#31572;&#25935;&#24863;/&#21361;&#38505;&#38382;&#39064;&#65292;3&#65289;&#29983;&#25104;&#20195;&#30721;&#21644;4&#65289;&#35270;&#35273;&#25512;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#21644;GPT-4&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#22312;&#26102;&#38388;&#19978;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;GPT-4&#65288;2023&#24180;3&#26376;&#65289;&#22312;&#35782;&#21035;&#36136;&#25968;&#26041;&#38754;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65288;&#20934;&#30830;&#29575;&#20026;97.6%&#65289;&#65292;&#20294;GPT-4&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#30456;&#21516;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#38750;&#24120;&#24046;&#65288;&#20934;&#30830;&#29575;&#20026;2.4%&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;GPT-3.5&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#27604;GPT-3.5&#65288;2023&#24180;3&#26376;&#65289;&#35201;&#22909;&#24471;&#22810;&#12290;GPT-4&#22312;6&#26376;&#20221;&#23545;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#30340;&#24847;&#24895;&#36739;3&#26376;&#20221;&#35201;&#20302;&#65292;&#32780;&#26080;&#35770;&#26159;GPT-4&#36824;&#26159;GPT-3.5&#22312;6&#26376;&#20221;&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#37117;&#26377;&#26356;&#22810;&#30340;&#26684;&#24335;&#38169;&#35823;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#30456;&#21516;LLM&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.17256</link><description>&lt;p&gt;
&#20197;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#26681;&#25454;&#29992;&#25143;&#36807;&#21435;&#30340;&#34892;&#20026;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#21382;&#21490;&#20132;&#20114;&#35760;&#24405;&#19981;&#21487;&#29992;&#26102;&#65292;&#24320;&#21457;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#23601;&#26159;&#25152;&#35859;&#30340;&#31995;&#32479;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#22312;&#21019;&#19994;&#20225;&#19994;&#25110;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#20013;&#23588;&#20026;&#31361;&#20986;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#20854;&#20013;&#31995;&#32479;&#20173;&#28982;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#21382;&#21490;&#29992;&#25143;&#21644;&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#26469;&#20026;&#26032;&#29992;&#25143;&#25110;&#29289;&#21697;&#25552;&#20379;&#25512;&#33616;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#25105;&#20204;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#36164;&#26009;&#21644;&#29289;&#21697;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19978;&#19979;&#25991;&#33976;&#39311;&#30340;&#26041;&#27861;&#25104;&#21151;&#23558;&#30693;&#35782;&#26356;&#26032;&#20256;&#25773;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26356;&#24191;&#27867;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09306</link><description>&lt;p&gt;
&#36890;&#36807;&#33976;&#39311;&#23558;&#30693;&#35782;&#26356;&#26032;&#20256;&#25773;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Propagating Knowledge Updates to LMs Through Distillation. (arXiv:2306.09306v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19978;&#19979;&#25991;&#33976;&#39311;&#30340;&#26041;&#27861;&#25104;&#21151;&#23558;&#30693;&#35782;&#26356;&#26032;&#20256;&#25773;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26356;&#24191;&#27867;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23384;&#20648;&#21644;&#20351;&#29992;&#22823;&#37327;&#20851;&#20110;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#30693;&#35782;&#65292;&#20294;&#22914;&#20309;&#26356;&#26032;&#23384;&#20648;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#23578;&#19981;&#28165;&#26970;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#26356;&#26032;&#26041;&#27861;&#25104;&#21151;&#22320;&#27880;&#20837;&#20102;&#21407;&#23376;&#20107;&#23454;&#65292;&#20294;&#26356;&#26032;&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#26681;&#25454;&#27880;&#20837;&#30340;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#33976;&#39311;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#20851;&#20110;&#23454;&#20307;&#30340;&#30693;&#35782;&#30340;&#21516;&#26102;&#20256;&#25773;&#35813;&#30693;&#35782;&#20197;&#23454;&#29616;&#26356;&#24191;&#27867;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#20256;&#36755;&#38598;&#29983;&#25104;&#21644;&#20256;&#36755;&#38598;&#19978;&#30340;&#33976;&#39311;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20174;&#23454;&#20307;&#23450;&#20041;&#20013;&#29983;&#25104;&#24310;&#32493;&#26469;&#29983;&#25104;&#19968;&#20010;&#20256;&#36755;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#24067;&#65288;&#23398;&#29983;&#65289;&#19982;&#22312;&#20256;&#36755;&#38598;&#19978;&#32473;&#23450;&#23450;&#20041;&#26465;&#20214;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#24067;&#65288;&#25945;&#24072;&#65289;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20256;&#25773;&#30693;&#35782;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities and propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the student) matches the distribution of the LM conditioned on the definition (the teacher) on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowled
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.06815</link><description>&lt;p&gt;
TrojPrompt&#65306;&#22522;&#20110;&#40657;&#30418;&#26041;&#24335;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26408;&#39532;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#23398;&#20064;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36866;&#24212;&#24615;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;&#24182;&#22312;&#19987;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;&#24212;&#29992;&#31243;&#24207;&#21644;API&#20013;&#23637;&#29616;&#20102;&#26480;&#20986;&#30340;&#21069;&#26223;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;prompt&#23398;&#20064;&#30340;API&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#30340;&#23433;&#20840;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;prompt&#23398;&#20064;&#30340;PLM API&#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31163;&#25955;&#25552;&#31034;&#65292;&#23569;&#26679;&#26412;&#21644;&#40657;&#30418;&#35774;&#32622;&#26159;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrojPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#30340;&#40657;&#30418;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#29983;&#25104;&#36890;&#29992;&#30340;&#21644;&#38544;&#31192;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;API&#39537;&#21160;&#30340;&#36890;&#29992;&#35302;&#21457;&#22120;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#21463;&#23475;&#32773;PLM API&#65292;&#20026;&#21508;&#31181;&#36755;&#20837;&#29983;&#25104;&#36890;&#29992;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
&lt;/p&gt;</description></item><item><title>FACTIFY3M&#26159;&#19968;&#20010;&#20197;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#39564;&#35777;&#20026;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#12290;&#34394;&#20551;&#20449;&#24687;&#22914;&#20170;&#24050;&#25104;&#20026;&#24403;&#19979;&#37325;&#22823;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#36825;&#19968;&#25968;&#25454;&#38598;&#26088;&#22312;&#36890;&#36807;&#22810;&#27169;&#24335;&#39564;&#35777;&#26469;&#21450;&#26102;&#35782;&#21035;&#21644;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05523</link><description>&lt;p&gt;
FACTIFY3M: &#36890;&#36807;5W&#38382;&#31572;&#35299;&#37322;&#30340;&#22810;&#27169;&#24335;&#20107;&#23454;&#39564;&#35777;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering. (arXiv:2306.05523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05523
&lt;/p&gt;
&lt;p&gt;
FACTIFY3M&#26159;&#19968;&#20010;&#20197;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#39564;&#35777;&#20026;&#30446;&#26631;&#30340;&#25968;&#25454;&#38598;&#12290;&#34394;&#20551;&#20449;&#24687;&#22914;&#20170;&#24050;&#25104;&#20026;&#24403;&#19979;&#37325;&#22823;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#36825;&#19968;&#25968;&#25454;&#38598;&#26088;&#22312;&#36890;&#36807;&#22810;&#27169;&#24335;&#39564;&#35777;&#26469;&#21450;&#26102;&#35782;&#21035;&#21644;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#26159;&#24403;&#21069;&#20127;&#24453;&#35299;&#20915;&#30340;&#31038;&#20250;&#21361;&#26426;&#20043;&#19968;&#8212;&#8212;&#22823;&#32422;67%&#30340;&#32654;&#22269;&#20154;&#35748;&#20026;&#34394;&#20551;&#20449;&#24687;&#20250;&#20135;&#29983;&#22823;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#26377;10%&#30340;&#20154;&#26377;&#24847;&#35782;&#22320;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#12290;&#35777;&#25454;&#34920;&#26126;&#65292;&#34394;&#20551;&#20449;&#24687;&#21487;&#20197;&#25805;&#32437;&#27665;&#20027;&#36827;&#31243;&#21644;&#20844;&#20247;&#33286;&#35770;&#65292;&#24182;&#22312;&#21361;&#26426;&#26399;&#38388;&#24341;&#36215;&#32929;&#24066;&#21160;&#33633;&#12289;&#31038;&#20250;&#24656;&#24908;&#29978;&#33267;&#27515;&#20129;&#12290;&#22240;&#27492;&#65292;&#24212;&#21450;&#26102;&#35782;&#21035;&#24182;&#23613;&#21487;&#33021;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27599;&#22825;&#20998;&#20139;&#22823;&#32422;32&#20159;&#24352;&#22270;&#20687;&#21644;720,000 &#23567;&#26102;&#30340;&#35270;&#39057;&#65292;&#22240;&#27492;&#23545;&#20110;&#22810;&#27169;&#24335;&#34394;&#20551;&#20449;&#24687;&#30340;&#21487;&#25193;&#23637;&#24615;&#26816;&#27979;&#38656;&#35201;&#39640;&#25928;&#30340;&#20107;&#23454;&#39564;&#35777;&#12290;&#23613;&#31649;&#22312;&#25991;&#26412;&#27169;&#24335;&#19979;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#21462;&#24471;&#20102;&#36827;&#23637;(&#20363;&#22914;&#65292;FEVER, LIAR)&#65292;&#20294;&#23398;&#26415;&#30028;&#22312;&#22810;&#27169;&#24335;&#20107;&#23454;&#39564;&#35777;&#26041;&#38754;&#32570;&#20047;&#23454;&#36136;&#24615;&#30340;&#21162;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FACTIFY3M&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;300&#19975;&#20010;&#26679;&#26412;&#65292;&#36890;&#36807;&#22810;&#31181;&#27169;&#24335;&#21644;5W&#38382;&#31572;&#25552;&#39640;&#20102;&#20107;&#23454;&#39564;&#35777;&#39046;&#22495;&#30340;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combating disinformation is one of the burning societal crises -- about 67% of the American population believes that disinformation produces a lot of uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows that disinformation can manipulate democratic processes and public opinion, causing disruption in the share market, panic and anxiety in society, and even death during crises. Therefore, disinformation should be identified promptly and, if possible, mitigated. With approximately 3.2 billion images and 720,000 hours of video shared online daily on social media platforms, scalable detection of multimodal disinformation requires efficient fact verification. Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR), the research community lacks substantial effort in multimodal fact verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3 million samples that pushes the boundaries of the domain of fact verification via a multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#24320;&#25918;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#22823;&#22411;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#23545;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#30340;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#38656;&#35201;&#36827;&#34892;&#31934;&#32454;&#30340;&#35843;&#25972;&#21644;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.04751</link><description>&lt;p&gt;
&#39558;&#39548;&#33021;&#36208;&#22810;&#36828;&#65311;&#25506;&#32034;&#24320;&#25918;&#36164;&#28304;&#20013;&#25351;&#20196;&#35843;&#20248;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. (arXiv:2306.04751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#24320;&#25918;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#22823;&#22411;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#23545;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#30340;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#38656;&#35201;&#36827;&#34892;&#31934;&#32454;&#30340;&#35843;&#25972;&#21644;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#24320;&#25918;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#23613;&#31649;&#26368;&#36817;&#22768;&#31216;&#24320;&#25918;&#27169;&#22411;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#26377;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#20294;&#36825;&#20123;&#22768;&#31216;&#24120;&#24120;&#20276;&#38543;&#30528;&#26377;&#38480;&#30340;&#35780;&#20272;&#65292;&#20351;&#24471;&#38590;&#20197;&#20840;&#38754;&#27604;&#36739;&#27169;&#22411;&#24182;&#30830;&#23450;&#21508;&#31181;&#36164;&#28304;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#22823;&#22411;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#22823;&#23567;&#20026;6.7B&#21040;65B&#20010;&#21442;&#25968;&#65292;&#22312;12&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#25163;&#21160;&#31574;&#21010;&#30340;&#65288;&#20363;&#22914;OpenAssistant&#65289;&#21644;&#32508;&#21512;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;Alpaca&#65289;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#33258;&#21160;&#12289;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#20154;&#30340;&#25351;&#26631;&#23545;&#20854;&#22312;&#20107;&#23454;&#30693;&#35782;&#12289;&#25512;&#29702;&#12289;&#22810;&#35821;&#35328;&#12289;&#32534;&#30721;&#21644;&#24320;&#25918;&#24335;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#26041;&#38754;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;T\"ulu&#65292;&#25105;&#20204;&#22312;&#39640;&#36136;&#37327;&#24320;&#25918;&#36164;&#28304;&#32452;&#21512;&#19978;&#24494;&#35843;&#30340;&#34920;&#29616;&#26368;&#20339;&#30340;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#23545;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#30340;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#38656;&#35201;&#36827;&#34892;&#31934;&#32454;&#30340;&#35843;&#25972;&#21644;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce T\"ulu, our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources.  Our experiments show that different instru
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#38477;&#20302;&#26631;&#31614;&#33719;&#21462;&#30340;&#30740;&#31350;&#25104;&#26412;80&#65285;&#65292;&#21516;&#26102;&#20445;&#35777;CSS&#30740;&#31350;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04746</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#36827;&#34892;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#26377;&#25928;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;: &#22522;&#20110;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Model Annotations for Valid Downstream Statistical Inference in Social Science: Design-Based Semi-Supervised Learning. (arXiv:2306.04746v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04746
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#65292;&#24182;&#38477;&#20302;&#26631;&#31614;&#33719;&#21462;&#30340;&#30740;&#31350;&#25104;&#26412;80&#65285;&#65292;&#21516;&#26102;&#20445;&#35777;CSS&#30740;&#31350;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#65288;CSS&#65289;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#25991;&#26723;&#26469;&#35299;&#37322;&#31038;&#20250;&#21644;&#25919;&#27835;&#29616;&#35937;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;CSS&#30740;&#31350;&#20154;&#21592;&#39318;&#20808;&#33719;&#21462;&#25991;&#26723;&#30340;&#26631;&#31614;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22238;&#24402;&#20998;&#26512;&#26469;&#35299;&#37322;&#26631;&#31614;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#21487;&#20197;&#36890;&#36807;&#22312;&#35268;&#27169;&#19978;&#20415;&#23452;&#22320;&#27880;&#37322;&#25991;&#26723;&#26469;&#38477;&#20302;CSS&#30740;&#31350;&#25104;&#26412;&#65292;&#20294;&#36825;&#20123;&#26367;&#20195;&#26631;&#31614;&#36890;&#24120;&#26159;&#19981;&#23436;&#32654;&#21644;&#26377;&#20559;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;LLMs&#30340;&#36755;&#20986;&#36827;&#34892;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#65292;&#21516;&#26102;&#20445;&#35777;&#19982;CSS&#30740;&#31350;&#22522;&#26412;&#30456;&#20851;&#30340;&#32479;&#35745;&#23646;&#24615;-&#22914;&#28176;&#36817;&#26080;&#20559;&#24615;&#21644;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30452;&#25509;&#22312;&#19979;&#28216;&#32479;&#35745;&#20998;&#26512;&#20013;&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#26367;&#20195;&#26631;&#31614;&#20250;&#23548;&#33268;&#23454;&#36136;&#24615;&#20559;&#24046;&#21644;&#26080;&#25928;&#32622;&#20449;&#21306;&#38388;&#65292;&#21363;&#20351;&#26367;&#20195;&#20934;&#30830;&#24615;&#39640;&#36798;80-90&#65285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#26080;&#20559;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#22522;&#20110;&#35774;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;D-SSL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;LLM&#27880;&#37322;&#19982;&#26377;&#38024;&#23545;&#24615;&#30340;&#37319;&#26679;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#26631;&#31614;&#33719;&#21462;&#30340;CSS&#30740;&#31350;&#25104;&#26412;&#38477;&#20302;80&#65285;&#65292;&#32780;&#19981;&#24433;&#21709;&#32479;&#35745;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#34920;&#26126;&#65292;&#19982;&#30452;&#25509;&#20351;&#29992;LLM&#39044;&#27979;&#26631;&#31614;&#30456;&#27604;&#65292;D-SSL&#21487;&#20197;&#23558;&#22238;&#24402;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#22810;&#36798;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#30740;&#31350;&#20102;&#20004;&#31181;&#20154;&#31867;&#27880;&#37322;&#32773;&#21487;&#20197;&#29992;&#20110;&#27880;&#37322;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20998;&#35299;&#30340;&#20998;&#31867;&#23398;&#12290;</title><link>http://arxiv.org/abs/2306.04125</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34701;&#21512;&#20132;&#20114;: &#20154;&#31867;&#21644;&#33258;&#21160;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multimodal Fusion Interactions: A Study of Human and Automatic Quantification. (arXiv:2306.04125v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#30740;&#31350;&#20102;&#20004;&#31181;&#20154;&#31867;&#27880;&#37322;&#32773;&#21487;&#20197;&#29992;&#20110;&#27880;&#37322;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20998;&#35299;&#30340;&#20998;&#31867;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20960;&#20046;&#25152;&#26377;&#22810;&#27169;&#24577;&#38382;&#39064;&#21644;&#24212;&#29992;&#20013;&#65292;&#22810;&#27169;&#24577;&#34701;&#21512;&#22810;&#31181;&#24322;&#26500;&#21644;&#20114;&#32852;&#30340;&#20449;&#21495;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#20026;&#20102;&#36827;&#34892;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#25105;&#20204;&#38656;&#35201;&#29702;&#35299;&#27169;&#24577;&#21487;&#20197;&#23637;&#29616;&#30340;&#20132;&#20114;&#31867;&#22411;&#65306;&#27599;&#31181;&#27169;&#24577;&#22914;&#20309;&#21333;&#29420;&#25552;&#20379;&#23545;&#20219;&#21153;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#24403;&#23384;&#22312;&#20854;&#20182;&#27169;&#24577;&#26102;&#36825;&#20123;&#20449;&#24687;&#22914;&#20309;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20154;&#31867;&#27880;&#37322;&#32773;&#22914;&#20309;&#34987;&#21033;&#29992;&#26469;&#27880;&#37322;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#20004;&#31181;&#20998;&#31867;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65306;(1) &#37096;&#20998;&#26631;&#31614;&#65292;&#20854;&#20013;&#19981;&#21516;&#38543;&#26426;&#20998;&#37197;&#30340;&#27880;&#37322;&#32773;&#27880;&#37322;&#32473;&#23450;&#31532;&#19968;&#20010;&#12289;&#31532;&#20108;&#20010;&#21644;&#20004;&#20010;&#27169;&#24577;&#30340;&#26631;&#31614;&#65292;&#20197;&#21450;(2) &#21453;&#20107;&#23454;&#26631;&#31614;&#65292;&#20854;&#20013;&#21516;&#19968;&#27880;&#37322;&#32773;&#34987;&#35201;&#27714;&#22312;&#32473;&#20986;&#31532;&#19968;&#20010;&#27169;&#24577;&#20043;&#21069;&#27880;&#37322;&#26631;&#31614;&#65292;&#28982;&#21518;&#32473;&#20986;&#31532;&#20108;&#20010;&#27169;&#24577;&#65292;&#24182;&#35201;&#27714;&#20182;&#20204;&#26126;&#30830;&#22320;&#25512;&#29702;&#20182;&#20204;&#30340;&#31572;&#26696;&#22914;&#20309;&#25913;&#21464;&#65292;&#28982;&#21518;&#25552;&#20986;&#22522;&#20110;&#20449;&#24687;&#20998;&#35299;&#30340;&#21478;&#19968;&#31181;&#20998;&#31867;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal fusion of multiple heterogeneous and interconnected signals is a fundamental challenge in almost all multimodal problems and applications. In order to perform multimodal fusion, we need to understand the types of interactions that modalities can exhibit: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how human annotators can be leveraged to annotate two categorizations of multimodal interactions: (1) partial labels, where different randomly assigned annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator is tasked to annotate the label given the first modality before giving them the second modality and asking them to explicitly reason about how their answer changes, before proposing an alternative taxonomy based on (3) information decomposition, where annotator
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#21457;&#29616;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36234;&#22823;&#65292;&#20854;&#34920;&#31034;&#36234;&#30456;&#20284;&#20110;&#33041;&#25104;&#20687;&#30340;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.01930</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21644;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Structural Similarities Between Language Models and Neural Response Measurements. (arXiv:2306.01930v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#21457;&#29616;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36234;&#22823;&#65292;&#20854;&#34920;&#31034;&#36234;&#30456;&#20284;&#20110;&#33041;&#25104;&#20687;&#30340;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#22797;&#26434;&#30340;&#20869;&#37096;&#21160;&#24577;&#65292;&#20294;&#21487;&#20197;&#30740;&#31350;&#20854;&#35789;&#27719;&#21644;&#30701;&#35821;&#30340;&#34920;&#31034;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#20063;&#24456;&#38590;&#29702;&#35299;&#65292;&#20294;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#21487;&#20197;&#25552;&#20379;&#22312;&#21548;&#25110;&#35835;&#26102;&#28608;&#27963;&#30340;&#65288;&#22024;&#26434;&#30340;&#65289;&#35760;&#24405;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#20013;&#25552;&#21462;&#30456;&#20284;&#30340;&#35789;&#27719;&#21644;&#30701;&#35821;&#34920;&#31034;&#12290;&#26412;&#30740;&#31350;&#22312;&#33041;&#35299;&#30721;&#30340;&#32972;&#26223;&#19979;&#30740;&#31350;&#20102;&#36825;&#20123;&#34920;&#31034;&#25152;&#24341;&#21457;&#30340;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36234;&#22823;&#65292;&#20854;&#34920;&#31034;&#19982;&#33041;&#25104;&#20687;&#30340;&#31070;&#32463;&#21709;&#24212;&#27979;&#37327;&#36234;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have complicated internal dynamics, but induce representations of words and phrases whose geometry we can study. Human language processing is also opaque, but neural response measurements can provide (noisy) recordings of activation during listening or reading, from which we can extract similar representations of words and phrases. Here we study the extent to which the geometries induced by these representations, share similarities in the context of brain decoding. We find that the larger neural language models get, the more their representations are structurally similar to neural response measurements from brain imaging. Code is available at \url{https://github.com/coastalcph/brainlm}.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#36890;&#36807;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#35299;&#20915;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18654</link><description>&lt;p&gt;
&#20449;&#20208;&#19982;&#21629;&#36816;&#65306;Transformer&#22312;&#32452;&#21512;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18654
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#36890;&#36807;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#35299;&#20915;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#26434;&#22810;&#27493;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#21331;&#36234;&#65292;&#20294;&#21516;&#26102;&#22312;&#19968;&#20123;&#31616;&#21333;&#38382;&#39064;&#19978;&#20063;&#20250;&#20986;&#29616;&#22833;&#36133;&#12290;&#36825;&#24341;&#21457;&#20102;&#30097;&#38382;&#65306;&#36825;&#20123;&#38169;&#35823;&#26159;&#20598;&#28982;&#30340;&#65292;&#36824;&#26159;&#23427;&#20204;&#34920;&#26126;&#20102;&#26356;&#23454;&#36136;&#24615;&#30340;&#38480;&#21046;&#65311;&#20026;&#20102;&#25581;&#31034;Transformer&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#26497;&#38480; - &#22810;&#20301;&#25968;&#20056;&#27861;&#12289;&#36923;&#36753;&#32593;&#26684;&#35868;&#39064;&#21644;&#19968;&#20010;&#32463;&#20856;&#30340;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#12290; &#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#27493;&#39588;&#65292;&#24182;&#23558;&#36825;&#20123;&#27493;&#39588;&#32508;&#21512;&#25104;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23558;&#32452;&#21512;&#22411;&#20219;&#21153;&#36716;&#21270;&#20026;&#35745;&#31639;&#22270;&#65292;&#20197;&#31995;&#32479;&#22320;&#37327;&#21270;&#20854;&#22797;&#26434;&#24615;&#65292;&#24182;&#23558;&#25512;&#29702;&#27493;&#39588;&#20998;&#35299;&#20026;&#20013;&#38388;&#23376;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#36890;&#36807;&#23558;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#36716;&#21270;&#20026;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#26469;&#35299;&#20915;&#32452;&#21512;&#22411;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, wi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#25506;&#32034;&#24773;&#32490;&#20215;&#20540;&#19982;&#24773;&#32490;&#36733;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#32852;&#21512;&#39044;&#27979;&#35774;&#32622;&#20013;&#20351;&#29992;&#21028;&#21035;&#24615;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.17422</link><description>&lt;p&gt;
&#29702;&#35299;&#24773;&#32490;&#20215;&#20540;&#26159;&#19968;&#39033;&#32852;&#21512;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Understanding Emotion Valence is a Joint Deep Learning Task. (arXiv:2305.17422v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17422
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#25506;&#32034;&#24773;&#32490;&#20215;&#20540;&#19982;&#24773;&#32490;&#36733;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#32852;&#21512;&#39044;&#27979;&#35774;&#32622;&#20013;&#20351;&#29992;&#21028;&#21035;&#24615;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35828;&#35805;&#20154;&#30340;&#35805;&#35821;&#25110;&#20889;&#20316;&#20013;&#24773;&#32490;&#20215;&#20540;&#20998;&#26512;&#26377;&#21161;&#20110;&#29702;&#35299;&#23545;&#35805;&#20013;&#24773;&#32490;&#29366;&#24577;&#30340;&#28608;&#27963;&#21644;&#21464;&#21270;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#24773;&#32490;&#36733;&#20307;&#65288;EC&#65289;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;&#35828;&#35805;&#20154;&#25152;&#24863;&#21463;&#21040;&#30340;&#24773;&#32490;&#21450;&#20854;&#34920;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#20102;&#24773;&#32490;&#20215;&#20540;&#21644;EC&#20043;&#38388;&#30340;&#33258;&#28982;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#24773;&#32490;&#20215;&#20540;&#21644;EC&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#21333;&#20219;&#21153;&#12289;&#20004;&#27493;&#27861;&#21644;&#32852;&#21512;&#35774;&#32622;&#12290;&#25105;&#20204;&#27604;&#36739;&#21644;&#35780;&#20272;&#20102;&#29983;&#25104;&#24615;&#65288;GPT-2&#65289;&#21644;&#21028;&#21035;&#24615;&#65288;BERT&#65289;&#26550;&#26500;&#22312;&#27599;&#31181;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20010;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30495;&#23454;&#26631;&#31614;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#21478;&#19968;&#20010;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#65292;&#21028;&#21035;&#24615;&#27169;&#22411;&#22312;&#32852;&#21512;&#39044;&#27979;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#24773;&#32490;&#20215;&#20540;&#21644;EC&#39044;&#27979;&#20219;&#21153;&#30340;&#26368;&#20339;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#33021;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The valence analysis of speakers' utterances or written posts helps to understand the activation and variations of the emotional state throughout the conversation. More recently, the concept of Emotion Carriers (EC) has been introduced to explain the emotion felt by the speaker and its manifestations. In this work, we investigate the natural inter-dependency of valence and ECs via a multi-task learning approach. We experiment with Pre-trained Language Models (PLM) for single-task, two-step, and joint settings for the valence and EC prediction tasks. We compare and evaluate the performance of generative (GPT-2) and discriminative (BERT) architectures in each setting. We observed that providing the ground truth label of one task improves the prediction performance of the models in the other task. We further observed that the discriminative model achieves the best trade-off of valence and EC prediction tasks in the joint prediction setting. As a result, we attain a single model that perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19979;&#26356;&#24555;&#22320;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17333</link><description>&lt;p&gt;
&#21482;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19979;&#26356;&#24555;&#22320;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#22823;&#65292;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#30340;&#23384;&#20648;&#31354;&#38388;&#25968;&#37327;&#21464;&#24471;&#36807;&#39640;&#12290;&#38646;&#38454;&#65288;ZO&#65289;&#26041;&#27861;&#29702;&#35770;&#19978;&#20165;&#20351;&#29992;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#23601;&#21487;&#20197;&#20272;&#35745;&#26799;&#24230;&#65292;&#20294;&#36890;&#24120;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#30340;&#36895;&#24230;&#38750;&#24120;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65288;MeZO&#65289;&#65292;&#23558;&#32463;&#20856;&#30340;ZO-SGD&#26041;&#27861;&#36866;&#24212;&#20110;&#21407;&#22320;&#25805;&#20316;&#65292;&#20174;&#32780;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#21482;&#20351;&#29992;&#19968;&#24352;A100 80GB GPU&#65292;MeZO&#23601;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;300&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#32780;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21487;&#20197;&#22312;&#30456;&#21516;&#30340;&#39044;&#31639;&#19979;&#20165;&#35757;&#32451;&#19968;&#20010;27&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#22411;&#31867;&#22411;&#65288;&#25513;&#30721;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65289;&#12289;&#27169;&#22411;&#35268;&#27169;&#65288;&#39640;&#36798;66B&#65289;&#21644;&#19979;&#28216;&#20219;&#21153;&#65288;&#20998;&#31867;&#12289;&#22810;&#39033;&#36873;&#25321;&#21644;&#29983;&#25104;&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#65288;1&#65289;MeZO&#26126;&#26174;&#20248;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#32447;&#24615;PR&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25193;&#25955;-&#35821;&#35328;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#22522;&#20110;7&#20010;&#35270;&#35273;&#35821;&#35328;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;CLIP&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16397</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#26159;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25193;&#25955;-&#35821;&#35328;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#22522;&#20110;7&#20010;&#35270;&#35273;&#35821;&#35328;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;CLIP&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23450;&#24615;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19982;&#37492;&#21035;&#24335;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#19981;&#21516;&#65292;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#32622;&#20110;&#33258;&#21160;&#32454;&#31890;&#24230;&#23450;&#37327;&#35780;&#20272;&#39640;&#32423;&#29616;&#35937;&#65288;&#22914;&#32452;&#21512;&#24615;&#65289;&#30340;&#20219;&#21153;&#20013;&#26159;&#19968;&#39033;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#20004;&#39033;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;DiffusionITM&#30340;&#26032;&#26041;&#27861;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#31283;&#23450;&#25193;&#25955;&#65289;&#36716;&#25442;&#20026;&#20219;&#20309;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;(ITM)&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;7&#20010;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12289;&#20559;&#24046;&#35780;&#20272;&#21644;&#35814;&#32454;&#20998;&#26512;&#30340;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Stable Diffusion + DiffusionITM&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#65288;&#22914;CLEVR&#21644;Winoground&#31561;&#65289;&#19978;&#20248;&#20110;CLIP&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MS-COCO&#19978;&#24494;&#35843;&#20445;&#25345;&#22270;&#20687;&#29305;&#24449;&#30340;&#36716;&#31227;&#35774;&#32622;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM. Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#26597;&#25214;&#22330;&#26223;&#20013;&#65292;LLMs&#30340;&#34920;&#26684;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#22312;&#21253;&#25324;&#25968;&#25454;&#27934;&#23519;&#29983;&#25104;&#21644;&#22522;&#20110;&#26597;&#35810;&#29983;&#25104;&#30340;&#24773;&#22659;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#39640;&#24615;&#33021;&#30340;LLMs&#22312;&#34920;&#26684;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.14987</link><description>&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#26597;&#25214;&#22330;&#26223;&#20013;&#30740;&#31350;LLMs&#30340;&#34920;&#26684;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios. (arXiv:2305.14987v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#26597;&#25214;&#22330;&#26223;&#20013;&#65292;LLMs&#30340;&#34920;&#26684;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#22312;&#21253;&#25324;&#25968;&#25454;&#27934;&#23519;&#29983;&#25104;&#21644;&#22522;&#20110;&#26597;&#35810;&#29983;&#25104;&#30340;&#24773;&#22659;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#39640;&#24615;&#33021;&#30340;LLMs&#22312;&#34920;&#26684;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#29992;&#25143;&#20026;&#20102;&#23454;&#29616;&#20854;&#20449;&#24687;&#26597;&#25214;&#30446;&#30340;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#26469;&#29702;&#35299;&#21644;&#25805;&#20316;&#36825;&#20123;&#25968;&#25454;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#25913;&#21892;&#29992;&#25143;&#25928;&#29575;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#20449;&#24687;&#26597;&#25214;&#24212;&#29992;&#20013;&#37319;&#29992;LLMs&#30340;&#29366;&#20917;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#20004;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#26597;&#25214;&#22330;&#26223;&#19979;&#20351;&#29992;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;LLMs&#30340;&#34920;&#26684;&#29983;&#25104;&#33021;&#21147;&#12290;&#20854;&#20013;&#21253;&#25324;LogicNLG&#21644;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;LoTNLG&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25968;&#25454;&#27934;&#23519;&#29983;&#25104;&#65307;&#20197;&#21450;FeTaQA&#21644;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;F2WTQ&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22522;&#20110;&#26597;&#35810;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22260;&#32469;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#23637;&#24320;&#65292;&#20998;&#21035;&#35780;&#20272;LLMs&#22312;&#34920;&#26684;&#29983;&#25104;&#12289;&#33258;&#21160;&#35780;&#20272;&#21644;&#21453;&#39304;&#29983;&#25104;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#39640;&#24615;&#33021;LLMs&#65292;&#29305;&#21035;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the adoption of LLMs in real-world applications for table information seeking remains underexplored. In this paper, we investigate the table-to-text capabilities of different LLMs using four datasets within two real-world information seeking scenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets for data insight generation, along with the FeTaQA and our newly-constructed F2WTQ datasets for query-based generation. We structure our investigation around three research questions, evaluating the performance of LLMs in table-to-text generation, automated evaluation, and feedback generation, respectively. Experimental results indicate that the current high-performing LLM, specificall
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;BIG-bench&#23454;&#39564;&#35760;&#24405;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#20855;&#26377;&#21487;&#39044;&#27979;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23547;&#25214;&#20449;&#24687;&#20016;&#23500;&#30340;&#23376;&#38598;&#20197;&#26368;&#22823;&#31243;&#24230;&#24674;&#22797;&#25972;&#20010;&#38598;&#21512;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14947</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#21487;&#39044;&#27979;&#24615;&#22914;&#20309;&#65311;&#23545;BIG-bench&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench. (arXiv:2305.14947v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;BIG-bench&#23454;&#39564;&#35760;&#24405;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#20855;&#26377;&#21487;&#39044;&#27979;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23547;&#25214;&#20449;&#24687;&#20016;&#23500;&#30340;&#23376;&#38598;&#20197;&#26368;&#22823;&#31243;&#24230;&#24674;&#22797;&#25972;&#20010;&#38598;&#21512;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#30340;&#21487;&#39044;&#27979;&#24615;&#65306;&#22312;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#23478;&#26063;&#12289;&#21442;&#25968;&#25968;&#37327;&#12289;&#20219;&#21153;&#25968;&#37327;&#21644;&#19978;&#19979;&#25991;&#31034;&#20363;&#25968;&#37327;&#30340;&#36807;&#21435;&#23454;&#39564;&#35760;&#24405;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#33021;&#21542;&#20934;&#30830;&#39044;&#27979;LLM&#22312;&#26032;&#23454;&#39564;&#37197;&#32622;&#19978;&#30340;&#24615;&#33021;&#65311;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#23545;LLM&#29992;&#25143;&#65288;&#20363;&#22914;&#65292;&#20915;&#23450;&#23581;&#35797;&#21738;&#20123;&#27169;&#22411;&#65289;&#12289;&#24320;&#21457;&#32773;&#65288;&#20363;&#22914;&#65292;&#20248;&#20808;&#35780;&#20272;&#20195;&#34920;&#24615;&#20219;&#21153;&#65289;&#21644;&#30740;&#31350;&#31038;&#21306;&#65288;&#20363;&#22914;&#65292;&#35782;&#21035;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#30340;&#38590;&#20197;&#39044;&#27979;&#30340;&#33021;&#21147;&#65289;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;&#25105;&#20204;&#22312;BIG-bench&#30340;&#23454;&#39564;&#35760;&#24405;&#19978;&#30740;&#31350;&#20102;&#24615;&#33021;&#39044;&#27979;&#38382;&#39064;&#12290;&#22312;&#38543;&#26426;&#30340;&#35757;&#32451;-&#27979;&#35797;&#20998;&#31163;&#20013;&#65292;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#39044;&#27979;&#22120;&#30340;R^2&#24471;&#20998;&#36229;&#36807;95%&#65292;&#34920;&#26126;&#23454;&#39564;&#35760;&#24405;&#20013;&#23384;&#22312;&#21487;&#23398;&#20064;&#30340;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23547;&#25214;&#8220;small-bench&#8221;&#30340;&#38382;&#39064;&#65292;&#21363;&#20174;BIG-bench&#20219;&#21153;&#20013;&#23547;&#25214;&#20449;&#24687;&#20016;&#23500;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#20174;&#20013;&#26368;&#22823;&#31243;&#24230;&#22320;&#24674;&#22797;&#25972;&#20010;&#38598;&#21512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the predictability of large language model (LLM) capabilities: given records of past experiments using different model families, numbers of parameters, tasks, and numbers of in-context examples, can we accurately predict LLM performance on new experiment configurations? Answering this question has practical implications for LLM users (e.g., deciding which models to try), developers (e.g., prioritizing evaluation on representative tasks), and the research community (e.g., identifying hard-to-predict capabilities that warrant further investigation).  We study the performance prediction problem on experiment records from BIG-bench. On a random train-test split, an MLP-based predictor achieves an $R^2$ score greater than 95%, indicating the presence of learnable patterns within the experiment records. We then formulate the problem of searching for "small-bench," an informative subset of BIG-bench tasks from which the performance on the full set can be maximally recovered. We
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#27867;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#21644;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23547;&#27714;&#35299;&#20915;&#20551;&#26032;&#38395;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;GPT-4&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#27867;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#22312;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#12289;&#28201;&#24230;&#12289;&#25552;&#31034;&#12289;&#29256;&#26412;&#25511;&#21046;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#32593;&#32476;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#38469;&#35265;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#24067;&#20102;&#26032;&#39062;&#30340;&#33521;&#27861;&#37197;&#23545;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;LIAR-New&#65292;&#20026;&#20449;&#24687;&#30495;&#23454;&#24615;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2305.14928</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#38752;&#30340;&#20551;&#26032;&#38395;&#32531;&#35299;&#65306;&#27867;&#21270;&#65292;&#19981;&#30830;&#23450;&#24615;&#21644;GPT-4
&lt;/p&gt;
&lt;p&gt;
Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4. (arXiv:2305.14928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#27867;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#21644;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23547;&#27714;&#35299;&#20915;&#20551;&#26032;&#38395;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;GPT-4&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#27867;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#22312;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#12289;&#28201;&#24230;&#12289;&#25552;&#31034;&#12289;&#29256;&#26412;&#25511;&#21046;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#32593;&#32476;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#38469;&#35265;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#24067;&#20102;&#26032;&#39062;&#30340;&#33521;&#27861;&#37197;&#23545;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;LIAR-New&#65292;&#20026;&#20449;&#24687;&#30495;&#23454;&#24615;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#31038;&#20250;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#23578;&#26410;&#25214;&#21040;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20851;&#27880;&#27867;&#21270;&#65292;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#26080;&#27861;&#23436;&#32654;&#20998;&#31867;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#26356;&#23454;&#29992;&#30340;&#24037;&#20855;&#26469;&#35780;&#20272;&#20449;&#24687;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;GPT-4&#22312;&#22810;&#20010;&#35774;&#23450;&#21644;&#35821;&#35328;&#20013;&#21487;&#20197;&#32988;&#36807;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25506;&#32034;&#27867;&#21270;&#65292;&#25581;&#31034;&#20102;GPT-4&#21644;RoBERTa-large&#22312;&#22833;&#25928;&#27169;&#24335;&#19978;&#30340;&#24046;&#24322;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#19981;&#21487;&#33021;&#30340;&#20363;&#23376;&#24182;&#26174;&#33879;&#25913;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#65292;&#28201;&#24230;&#65292;&#25552;&#31034;&#65292;&#29256;&#26412;&#25511;&#21046;&#65292;&#21487;&#35299;&#37322;&#24615;&#21644;&#32593;&#32476;&#26816;&#32034;&#30340;&#32467;&#26524;&#65292;&#27599;&#20010;&#32467;&#26524;&#37117;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#35265;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20855;&#26377;&#26032;&#39062;&#30340;&#33521;&#27861;&#37197;&#23545;&#20551;&#26032;&#38395;&#25968;&#25454;&#21644;&#21487;&#34892;&#24615;&#26631;&#31614;&#30340;LIAR-New&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;ALCE&#65292;&#26159;&#39318;&#20010;&#33258;&#21160;LLMs&#24341;&#25991;&#35780;&#20272;&#22522;&#20934;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;&#65292;&#25552;&#39640;&#20854;&#20107;&#23454;&#27491;&#30830;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#65307;&#25552;&#31034;LLMs&#29305;&#23450;&#30340;&#20851;&#38190;&#35789;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24341;&#25991;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14627</link><description>&lt;p&gt;
&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Enabling Large Language Models to Generate Text with Citations. (arXiv:2305.14627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;ALCE&#65292;&#26159;&#39318;&#20010;&#33258;&#21160;LLMs&#24341;&#25991;&#35780;&#20272;&#22522;&#20934;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;&#65292;&#25552;&#39640;&#20854;&#20107;&#23454;&#27491;&#30830;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#65307;&#25552;&#31034;LLMs&#29305;&#23450;&#30340;&#20851;&#38190;&#35789;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24341;&#25991;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#20449;&#24687;&#23547;&#25214;&#24037;&#20855;&#65292;&#20294;&#29983;&#25104;&#30340;&#36755;&#20986;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#12290;&#26412;&#25991;&#26088;&#22312;&#23454;&#29616;LLMs&#29983;&#25104;&#24102;&#24341;&#25991;&#30340;&#25991;&#26412;&#65292;&#25552;&#39640;&#20854;&#20107;&#23454;&#27491;&#30830;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ALCE&#65292;&#36825;&#26159;&#39318;&#20010;&#33258;&#21160;LLMs&#24341;&#25991;&#35780;&#20272;&#22522;&#20934;&#12290;ALCE&#25910;&#38598;&#20102;&#21508;&#31181;&#38382;&#39064;&#21644;&#26816;&#32034;&#35821;&#26009;&#24211;&#65292;&#24182;&#35201;&#27714;&#24314;&#31435;&#31471;&#21040;&#31471;&#31995;&#32479;&#20197;&#26816;&#32034;&#25903;&#25345;&#35777;&#25454;&#24182;&#29983;&#25104;&#24102;&#26377;&#24341;&#25991;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#27839;&#30528;&#27969;&#30021;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#24341;&#25991;&#36136;&#37327;&#19977;&#20010;&#32500;&#24230;&#26500;&#24314;&#33258;&#21160;&#25351;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#21644;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#31995;&#32479;&#20173;&#26377;&#30456;&#24403;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;--&#20363;&#22914;&#65292;&#25552;&#31034;LLMs&#29305;&#23450;&#30340;&#20851;&#38190;&#35789;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24341;&#25991;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#21457;&#23637;&#33021;&#22815;&#29983;&#25104;&#21487;&#39564;&#35777;&#21644;&#21487;&#20449;&#36182;&#36755;&#20986;&#30340;LLMs&#25552;&#20379;&#20102;&#22362;&#23454;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, we aim to enable LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare with different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We build automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvements -for example,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#22312;&#27604;&#36739;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25991;&#26412;&#23454;&#20307;&#27604;&#36739;&#25968;&#25454;&#30340;&#26041;&#27861;&#21644;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14457</link><description>&lt;p&gt;
&#20026;&#27604;&#36739;&#25512;&#29702;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-training Language Models for Comparative Reasoning. (arXiv:2305.14457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#22312;&#27604;&#36739;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25991;&#26412;&#23454;&#20307;&#27604;&#36739;&#25968;&#25454;&#30340;&#26041;&#27861;&#21644;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20854;&#22312;&#25991;&#26412;&#27604;&#36739;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21487;&#25193;&#23637;&#30340;&#29992;&#20110;&#25910;&#38598;&#22522;&#20110;&#25991;&#26412;&#23454;&#20307;&#27604;&#36739;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19977;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#27604;&#36739;&#38382;&#31572;&#12289;&#38382;&#21477;&#29983;&#25104;&#21644;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#22823;&#22823;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#26412;&#24037;&#20316;&#36824;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#27604;&#36739;&#25512;&#29702;&#32508;&#21512;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel framework to pre-train language models for enhancing their abilities of comparative reasoning over texts. While recent research has developed models for NLP tasks that require comparative reasoning, they suffer from costly manual data labeling and limited generalizability to different tasks. Our approach involves a scalable method for collecting data for text-based entity comparison, which leverages both structured and unstructured data, and the design of three novel pre-training tasks. Evaluation on a range of downstream tasks including comparative question answering, question generation, and summarization shows that our pre-training framework significantly improves the comparative reasoning abilities of language models, especially under low-resource conditions. This work also releases the first integrated benchmark for comparative reasoning over texts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32416;&#27491;&#34920;&#31034;&#20559;&#35265;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21363;&#21453;&#20107;&#23454;&#22686;&#24378;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21453;&#20107;&#23454;&#22686;&#24378;&#30456;&#27604;&#20110;&#26410;&#20462;&#27491;&#30340;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#20559;&#35265;&#26657;&#27491;&#26041;&#27861;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#27169;&#22411;&#20998;&#26512;&#36827;&#19968;&#27493;&#25351;&#20986;&#65292;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#19982;&#30495;&#23454;&#21453;&#20107;&#23454;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.14083</link><description>&lt;p&gt;
&#23545;&#20110;&#34920;&#31034;&#20559;&#35265;&#19979;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Augmentation for Multimodal Learning Under Presentation Bias. (arXiv:2305.14083v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32416;&#27491;&#34920;&#31034;&#20559;&#35265;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21363;&#21453;&#20107;&#23454;&#22686;&#24378;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21453;&#20107;&#23454;&#22686;&#24378;&#30456;&#27604;&#20110;&#26410;&#20462;&#27491;&#30340;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#20559;&#35265;&#26657;&#27491;&#26041;&#27861;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#27169;&#22411;&#20998;&#26512;&#36827;&#19968;&#27493;&#25351;&#20986;&#65292;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#19982;&#30495;&#23454;&#21453;&#20107;&#23454;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#26631;&#31614;&#36890;&#24120;&#26159;&#20174;&#31995;&#32479;&#24076;&#26395;&#40723;&#21169;&#30340;&#29992;&#25143;&#34892;&#20026;&#20013;&#24471;&#20986;&#30340;&#12290;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#38543;&#30528;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#21644;&#29305;&#24449;&#30340;&#25552;&#20379;&#65292;&#38656;&#35201;&#35757;&#32451;&#26032;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#26410;&#26469;&#29992;&#25143;&#34892;&#20026;&#30340;&#20559;&#35265;&#65292;&#36827;&#32780;&#23548;&#33268;&#26631;&#31614;&#20013;&#30340;&#34920;&#31034;&#20559;&#35265;&#65292;&#36825;&#25439;&#23475;&#20102;&#35757;&#32451;&#26032;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#26041;&#27861;&#65292;&#21363;&#21453;&#20107;&#23454;&#22686;&#24378;&#65292;&#36890;&#36807;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#26631;&#31614;&#26469;&#32416;&#27491;&#34920;&#31034;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#30456;&#27604;&#26410;&#20462;&#27491;&#30340;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#20559;&#35265;&#26657;&#27491;&#26041;&#27861;&#65292;&#21453;&#20107;&#23454;&#22686;&#24378;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#27169;&#22411;&#20998;&#26512;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#19982;&#30495;&#23454;&#21453;&#20107;&#23454;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior, inducing a presentation bias in the labels that compromises the ability to train new models. In this paper, we propose counterfactual augmentation, a novel causal method for correcting presentation bias using generated counterfactual labels. Our empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods. Model analyses further indicate that the generated counterfactuals align closely with true counterfactuals in an oracle setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11554</link><description>&lt;p&gt;
ToolkenGPT&#65306;&#36890;&#36807;&#24037;&#20855;&#23884;&#20837;&#25193;&#20805;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. (arXiv:2305.11554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolkenGPT&#30340;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#22806;&#37096;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#29992;&#24037;&#20855;&#28436;&#31034;&#25968;&#25454;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#26082;&#36153;&#26102;&#21448;&#21463;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#24037;&#20855;&#38598;&#12290;&#26368;&#36817;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#20363;&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#26159;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21482;&#20801;&#35768;&#28436;&#31034;&#20960;&#27425;&#65292;&#23548;&#33268;&#23545;&#24037;&#20855;&#30340;&#29702;&#35299;&#19981;&#22815;&#20805;&#20998;&#12290;&#27492;&#22806;&#65292;&#24403;&#26377;&#22823;&#37327;&#24037;&#20855;&#21487;&#20379;&#36873;&#25321;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#33021;&#23436;&#20840;&#26080;&#27861;&#27491;&#24120;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$\textbf{ToolkenGPT}$&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;$\underline{&#24037;&#20855;}$&#34920;&#31034;&#20026;&#19968;&#20010;$\underline{token}$&#65288;$\textit{toolken}$&#65289;&#65292;&#24182;&#20026;&#20854;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#65292;&#20351;&#24471;&#24037;&#20855;&#35843;&#29992;&#19982;&#29983;&#25104;&#24120;&#35268;&#21333;&#35789;&#26631;&#35760;&#30340;&#26041;&#24335;&#30456;&#21516;&#12290;&#19968;&#26086;&#35302;&#21457;&#20102;toolken&#65292;LLM&#34987;&#25552;&#31034;&#23436;&#25104;&#24037;&#20855;&#25191;&#34892;&#25152;&#38656;&#30340;&#21442;&#25968;&#12290;ToolkenGPT&#25552;&#20379;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;1&#65289;&#24341;&#20837;&#20102;toolken&#30340;&#27010;&#24565;&#65292;&#20197;&#25193;&#20805;LLM&#19982;&#22806;&#37096;&#24037;&#20855;&#30340;&#20132;&#20114;&#65292;2&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#20363;&#65292;&#21033;&#29992;tool embeddings&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;3&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\underline{tool}$ as a to$\underline{ken}$ ($\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#23485;&#26494;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20026;&#38382;&#31572;&#31995;&#32479;&#32508;&#21512;&#20869;&#22806;&#20998;&#24067;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#26174;&#31034;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03971</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23485;&#26494;&#20248;&#21270;&#29992;&#20110;&#24378;&#38887;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Adaptive loose optimization for robust question answering. (arXiv:2305.03971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#23485;&#26494;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20026;&#38382;&#31572;&#31995;&#32479;&#32508;&#21512;&#20869;&#22806;&#20998;&#24067;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#26174;&#31034;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#26041;&#27861;&#20197;&#21033;&#29992;&#25968;&#25454;&#20559;&#24046;&#20026;&#29305;&#28857;&#65292;&#22914;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#35821;&#35328;&#20808;&#39564;&#21644;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;&#25277;&#21462;&#24335;&#38382;&#31572;&#65289;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#30446;&#21069;&#30340;&#21435;&#20559;&#26041;&#27861;&#24448;&#24448;&#20197;&#22312;&#20998;&#24067;&#20869;&#34920;&#29616;&#19981;&#20339;&#20026;&#20195;&#20215;&#33719;&#24471;&#26377;&#21033;&#30340;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#21435;&#20559;&#26041;&#27861;&#21017;&#22312;&#33719;&#24471;&#39640;&#20998;&#24067;&#20869;&#34920;&#29616;&#30340;&#21516;&#26102;&#29306;&#29298;&#20102;&#30456;&#24403;&#25968;&#37327;&#30340;&#20998;&#24067;&#22806;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#38590;&#20197;&#24212;&#23545;&#22797;&#26434;&#21464;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#23485;&#26494;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#20026;&#38382;&#31572;&#31995;&#32479;&#32508;&#21512;&#20004;&#32773;&#26368;&#20339;&#34920;&#29616;&#32780;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#26681;&#25454;&#23567;&#25209;&#37327;&#35757;&#32451;&#25968;&#25454;&#19978;&#20808;&#21069;&#21644;&#24403;&#21069;&#20248;&#21270;&#29366;&#24577;&#20043;&#38388;&#30340;&#27604;&#29575;&#33258;&#36866;&#24212;&#22320;&#20943;&#23569;&#25439;&#22833;&#12290;&#36825;&#31181;&#23485;&#26494;&#20248;&#21270;&#21487;&#20197;&#29992;&#26469;&#38450;&#27490;&#38750;&#20984;&#20248;&#21270;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#24182;&#24110;&#21161;&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering methods are well-known for leveraging data bias, such as the language prior in visual question answering and the position bias in machine reading comprehension (extractive question answering). Current debiasing methods often come at the cost of significant in-distribution performance to achieve favorable out-of-distribution generalizability, while non-debiasing methods sacrifice a considerable amount of out-of-distribution performance in order to obtain high in-distribution performance. Therefore, it is challenging for them to deal with the complicated changing real-world situations. In this paper, we propose a simple yet effective novel loss function with adaptive loose optimization, which seeks to make the best of both worlds for question answering. Our main technical contribution is to reduce the loss adaptively according to the ratio between the previous and current optimization state on mini-batch training data. This loose optimization can be used to prevent non
&lt;/p&gt;</description></item><item><title>Unlimiformer&#26159;&#19968;&#31181;Transformer&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#25152;&#26377;&#23618;&#30340;&#27880;&#24847;&#35745;&#31639;&#21368;&#36733;&#21040;&#21333;&#20010;k&#36817;&#37051;&#32034;&#24341;&#19978;&#65292;&#20174;&#32780;&#21487;&#22788;&#29702;&#26080;&#38480;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#23398;&#20064;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2305.01625</link><description>&lt;p&gt;
&#26080;&#38480;&#38271;&#24230;&#36755;&#20837;&#30340;&#38271;&#36317;&#31163;Transformer-Unlimiformer
&lt;/p&gt;
&lt;p&gt;
Unlimiformer: Long-Range Transformers with Unlimited Length Input. (arXiv:2305.01625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01625
&lt;/p&gt;
&lt;p&gt;
Unlimiformer&#26159;&#19968;&#31181;Transformer&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#25152;&#26377;&#23618;&#30340;&#27880;&#24847;&#35745;&#31639;&#21368;&#36733;&#21040;&#21333;&#20010;k&#36817;&#37051;&#32034;&#24341;&#19978;&#65292;&#20174;&#32780;&#21487;&#22788;&#29702;&#26080;&#38480;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#23398;&#20064;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36890;&#24120;&#23545;&#36755;&#20837;&#38271;&#24230;&#26377;&#39044;&#23450;&#20041;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#21442;&#32771;&#36755;&#20837;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;-Unlimiformer&#65292;&#21487;&#20197;&#21253;&#35013;&#20219;&#20309;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#65292;&#24182;&#23558;&#25152;&#26377;&#23618;&#30340;&#27880;&#24847;&#35745;&#31639;&#21368;&#36733;&#21040;&#21333;&#20010;k&#36817;&#37051;&#32034;&#24341;&#19978;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#38271;&#25991;&#26723;&#21644;&#22810;&#25991;&#26723;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;Unlimiformer&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#24635;&#32467;350k&#20196;&#29260;&#38271;&#30340;&#36755;&#20837;&#32780;&#19981;&#36827;&#34892;&#27979;&#35797;&#26102;&#30340;&#25130;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models typically have a predefined bound to their input length, because of their need to potentially attend to every token in the input. In this work, we propose Unlimiformer: a general approach that can wrap any existing pretrained encoder-decoder transformer, and offload the attention computation across all layers to a single $k$-nearest-neighbor index; this index can be kept on either the GPU or CPU memory and queried in sub-linear time. This way, we can index extremely long input sequences, while every attention head in every decoder layer retrieves its top-$k$ keys, instead of attending to every key. We demonstrate Unlimiformers's efficacy on several long-document and multi-document summarization benchmarks, showing that it can summarize even 350k token-long inputs from the BookSum dataset, without any input truncation at test time. Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#20195;&#30721;&#32508;&#21512;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;EvalPlus&#65292;&#29992;&#20110;&#35780;&#20272;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01210</link><description>&lt;p&gt;
ChatGPT&#29983;&#25104;&#30340;&#20195;&#30721;&#30495;&#30340;&#27491;&#30830;&#21527;&#65311;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#20005;&#26684;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. (arXiv:2305.01210v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#20195;&#30721;&#32508;&#21512;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;EvalPlus&#65292;&#29992;&#20110;&#35780;&#20272;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#32508;&#21512;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#34987;&#38271;&#26399;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#30452;&#25509;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#20013;&#29992;&#25143;&#30340;&#24847;&#22270;&#29983;&#25104;&#20195;&#30721;&#12290;&#20195;&#30721;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#31574;&#21010;&#22909;&#30340;&#32508;&#21512;&#38382;&#39064;&#21644;&#21508;&#31181;&#36755;&#20837;/&#36755;&#20986;&#27979;&#35797;&#29992;&#20363;&#65292;&#34987;&#29992;&#26469;&#34913;&#37327;&#21508;&#31181;LLMs&#22312;&#20195;&#30721;&#32508;&#21512;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#27979;&#35797;&#29992;&#20363;&#22312;&#23436;&#20840;&#35780;&#20272;&#29983;&#25104;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#26041;&#38754;&#65292;&#25968;&#37327;&#21644;&#36136;&#37327;&#37117;&#21487;&#33021;&#26377;&#25152;&#38480;&#21046;&#12290;&#36825;&#31181;&#29616;&#26377;&#22522;&#20934;&#20013;&#30340;&#38480;&#21046;&#24341;&#20986;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#22312;LLMs&#26102;&#20195;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#30495;&#30340;&#27491;&#30830;&#21527;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvalPlus&#8212;&#8212;&#19968;&#20010;&#35780;&#20272;LLM-synthesized&#20195;&#30721;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#20005;&#26684;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#12290;EvalPlus&#25509;&#21463;&#22522;&#30784;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#33258;&#21160;&#36755;&#20837;&#29983;&#25104;&#27493;&#39588;&#65292;&#20351;&#29992;LLM-based&#21644;&#22522;&#20110;&#21464;&#24322;&#30340;&#26041;&#27861;&#29983;&#25104;&#21644;&#22810;&#26679;&#21270;&#22823;&#37327;&#26032;&#30340;&#27979;&#35797;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code according to user intent written in natural language. Code evaluation datasets, containing curated synthesis problems with input/output test-cases, are used to measure the performance of various LLMs on code synthesis. However, test-cases in these datasets can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis benchmarking framework to rigorously evaluate the functional correctness of LLM-synthesized code. In short, EvalPlus takes in the base evaluation dataset and uses an automatic input generation step to produce and diversify large amounts of new test inputs using both LLM-based and mutation-based
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20351;&#29992;&#33258;&#27880;&#35760;&#36827;&#34892;&#25512;&#29702;&#21644;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#26126;&#30830;&#24605;&#32771;&#12289;&#35760;&#24405;&#33258;&#24049;&#30340;&#24819;&#27861;&#65292;&#24182;&#25972;&#21512;&#20808;&#21069;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.00833</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#33258;&#27880;&#35760;&#36827;&#34892;&#25512;&#29702;&#21644;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Learning to Reason and Memorize with Self-Notes. (arXiv:2305.00833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20351;&#29992;&#33258;&#27880;&#35760;&#36827;&#34892;&#25512;&#29702;&#21644;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#26126;&#30830;&#24605;&#32771;&#12289;&#35760;&#24405;&#33258;&#24049;&#30340;&#24819;&#27861;&#65292;&#24182;&#25972;&#21512;&#20808;&#21069;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#19981;&#33021;&#20445;&#30041;&#20197;&#20379;&#23558;&#26469;&#20351;&#29992;&#30340;&#20808;&#21069;&#25512;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#20801;&#35768;&#27169;&#22411;&#36827;&#34892;&#33258;&#27880;&#35760;&#12290;&#19982;&#26368;&#36817;&#30340;&#24605;&#32500;&#38142;&#25110;&#33609;&#31295;&#26412;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#38543;&#26102;&#20559;&#31163;&#36755;&#20837;&#19978;&#19979;&#25991;&#26469;&#26126;&#30830;&#24605;&#32771;&#21644;&#35760;&#24405;&#33258;&#24049;&#30340;&#24819;&#27861;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#38405;&#35835;&#19978;&#19979;&#25991;&#26102;&#21363;&#26102;&#25512;&#29702;&#65292;&#24182;&#25972;&#21512;&#20808;&#21069;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#35760;&#24518;&#24182;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#32455;&#36755;&#20837;&#25991;&#26412;&#30340;&#33258;&#27880;&#35760;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#24605;&#32500;&#38142;&#21644;&#33609;&#31295;&#26412;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use. We propose a simple method for solving both of these problems by allowing the model to take Self-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model can deviate from the input context at any time to explicitly think and write down its thoughts. This allows the model to perform reasoning on the fly as it reads the context and even integrate previous reasoning steps, thus enhancing its memory with useful information and enabling multi-step reasoning. Experiments across a wide variety of tasks demonstrate that our method can outperform chain-of-thought and scratchpad methods by taking Self-Notes that interleave the input text.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#20013;&#20851;&#38190;&#35789;&#21450;&#20854;&#20301;&#32622;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#26631;&#35760;&#25511;&#21046;&#20851;&#38190;&#35789;&#30456;&#23545;&#20301;&#32622;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#29992;&#25143;&#24847;&#22270;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.09516</link><description>&lt;p&gt;
&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#20851;&#38190;&#35789;&#21644;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
Controlling keywords and their positions in text generation. (arXiv:2304.09516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#20013;&#20851;&#38190;&#35789;&#21450;&#20854;&#20301;&#32622;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#26631;&#35760;&#25511;&#21046;&#20851;&#38190;&#35789;&#30456;&#23545;&#20301;&#32622;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#29992;&#25143;&#24847;&#22270;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#25353;&#29031;&#29992;&#25143;&#24847;&#22270;&#25511;&#21046;&#29983;&#25104;&#32467;&#26524;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#25351;&#23450;&#24212;&#21253;&#21547;&#22312;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20851;&#38190;&#35789;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#36824;&#19981;&#36275;&#20197;&#29983;&#25104;&#21453;&#26144;&#29992;&#25143;&#24847;&#22270;&#30340;&#25991;&#26412;&#12290;&#20363;&#22914;&#65292;&#23558;&#37325;&#35201;&#20851;&#38190;&#35789;&#25918;&#22312;&#25991;&#26412;&#24320;&#22836;&#26377;&#21161;&#20110;&#21560;&#24341;&#35835;&#32773;&#30340;&#27880;&#24847;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#19981;&#20801;&#35768;&#36825;&#31181;&#28789;&#27963;&#30340;&#25511;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#20013;&#20851;&#38190;&#35789;&#21644;&#20854;&#20301;&#32622;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#29305;&#27530;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20851;&#38190;&#35789;&#30340;&#30456;&#23545;&#20301;&#32622;&#12290;&#25688;&#35201;&#29983;&#25104;&#21644;&#25925;&#20107;&#29983;&#25104;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25511;&#21046;&#20851;&#38190;&#35789;&#21450;&#20854;&#20301;&#32622;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#25511;&#21046;&#20851;&#38190;&#35789;&#20301;&#32622;&#21487;&#20197;&#29983;&#25104;&#27604;&#22522;&#32447;&#26356;&#31526;&#21512;&#29992;&#25143;&#24847;&#22270;&#30340;&#25688;&#35201;&#25991;&#26412;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges in text generation is to control generation as intended by a user. Previous studies have proposed to specify the keywords that should be included in the generated text. However, this is insufficient to generate text which reflect the user intent. For example, placing the important keyword beginning of the text would helps attract the reader's attention, but existing methods do not enable such flexible control. In this paper, we tackle a novel task of controlling not only keywords but also the position of each keyword in the text generation. To this end, we show that a method using special tokens can control the relative position of keywords. Experimental results on summarization and story generation tasks show that the proposed method can control keywords and their positions. We also demonstrate that controlling the keyword positions can generate summary texts that are closer to the user's intent than baseline. We release our code.
&lt;/p&gt;</description></item><item><title>&#37322;&#25918;&#20102;OpenAssistant Conversations&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20840;&#29699;&#36229;&#36807;1,000&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20154;&#24037;&#29983;&#25104;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#21161;&#25163;&#39118;&#26684;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#36890;&#36807;SFT&#21644;RLHF&#26377;&#25928;&#22320;&#29992;&#20110;LLM&#23545;&#40784;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07327</link><description>&lt;p&gt;
OpenAssistant Conversations -- &#27665;&#20027;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
OpenAssistant Conversations -- Democratizing Large Language Model Alignment. (arXiv:2304.07327v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07327
&lt;/p&gt;
&lt;p&gt;
&#37322;&#25918;&#20102;OpenAssistant Conversations&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20840;&#29699;&#36229;&#36807;1,000&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20154;&#24037;&#29983;&#25104;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#21161;&#25163;&#39118;&#26684;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#36890;&#36807;SFT&#21644;RLHF&#26377;&#25928;&#22320;&#29992;&#20110;LLM&#23545;&#40784;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21487;&#29992;&#24615;&#24182;&#25512;&#21160;&#20854;&#24555;&#36895;&#24212;&#29992;&#65292;&#22914;ChatGPT&#25152;&#31034;&#12290; &#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#31561;&#23545;&#40784;&#25216;&#26415;&#22823;&#22823;&#38477;&#20302;&#20102;&#26377;&#25928;&#21457;&#25381;LLM&#33021;&#21147;&#25152;&#38656;&#30340;&#25216;&#33021;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290; &#28982;&#32780;&#65292;&#20687;RLHF&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;&#23545;&#40784;&#25216;&#26415;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#26114;&#36149;&#19988;&#20445;&#23494;&#12290; &#20026;&#20102;&#27665;&#20027;&#21270;&#22823;&#35268;&#27169;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;OpenAssistant Conversations&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20840;&#29699;&#36229;&#36807;1,000&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20154;&#24037;&#29983;&#25104;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#21161;&#25163;&#39118;&#26684;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;161,443&#26465;&#28040;&#24687;&#65292;&#20998;&#24067;&#22312;66,497&#20010;&#23545;&#35805;&#26641;&#20013;&#65292;&#24182;&#22312;35&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#29992;461,292&#20010;&#36136;&#37327;&#35780;&#20998;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;OpenAssistant Conversations&#21487;&#20197;&#36890;&#36807;SFT&#21644;RLHF&#26377;&#25928;&#22320;&#29992;&#20110;LLM&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#21457;&#24067;&#35821;&#26009;&#24211;&#65292;&#20351;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#31038;&#21306;&#33021;&#22815;&#36827;&#19968;&#27493;&#30740;&#31350;&#27665;&#20027;&#21270;LLM&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#20154;&#31867;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GenExpan&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#21069;&#32512;&#26641;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03531</link><description>&lt;p&gt;
&#20174;&#26816;&#32034;&#21040;&#29983;&#25104;&#65306;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Retrieval to Generation: Efficient and Effective Entity Set Expansion. (arXiv:2304.03531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GenExpan&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#21069;&#32512;&#26641;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38598;&#25193;&#23637;&#65288;ESE&#65289;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#25193;&#23637;&#30001;&#23567;&#30340;&#31181;&#23376;&#23454;&#20307;&#38598;&#25551;&#36848;&#30340;&#30446;&#26631;&#35821;&#20041;&#31867;&#30340;&#23454;&#20307;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;ESE&#26041;&#27861;&#26159;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#38656;&#35201;&#25552;&#21462;&#23454;&#20307;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#35745;&#31639;&#31181;&#23376;&#23454;&#20307;&#21644;&#20505;&#36873;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20004;&#20010;&#30446;&#30340;&#65292;&#23427;&#20204;&#24517;&#39035;&#36845;&#20195;&#22320;&#36941;&#21382;&#35821;&#26009;&#24211;&#21644;&#25968;&#25454;&#38598;&#20013;&#25552;&#20379;&#30340;&#23454;&#20307;&#35789;&#27719;&#65292;&#23548;&#33268;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#26816;&#32034;&#30340;ESE&#26041;&#27861;&#28040;&#32791;&#30340;&#26102;&#38388;&#19982;&#23454;&#20307;&#35789;&#27719;&#21644;&#35821;&#26009;&#24211;&#30340;&#22823;&#23567;&#25104;&#32447;&#24615;&#22686;&#38271;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;ESE&#26694;&#26550;&#65292;Generative Entity Set Expansion (GenExpan)&#65292;&#23427;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;ESE&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#21069;&#32512;&#26641;&#26469;&#20445;&#35777;&#23454;&#20307;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#37319;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#31867;&#21517;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#31867;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Set Expansion (ESE) is a critical task aiming to expand entities of the target semantic class described by a small seed entity set. Most existing ESE methods are retrieval-based frameworks that need to extract the contextual features of entities and calculate the similarity between seed entities and candidate entities. To achieve the two purposes, they should iteratively traverse the corpus and the entity vocabulary provided in the datasets, resulting in poor efficiency and scalability. The experimental results indicate that the time consumed by the retrieval-based ESE methods increases linearly with entity vocabulary and corpus size. In this paper, we firstly propose a generative ESE framework, Generative Entity Set Expansion (GenExpan), which utilizes a generative pre-trained language model to accomplish ESE task. Specifically, a prefix tree is employed to guarantee the validity of entity generation, and automatically generated class names are adopted to guide the model to gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#21452;&#37325;&#24130;&#24459;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#24182;&#24314;&#31435;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03216</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;Pareto&#21069;&#27839;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#21452;&#37325;&#24130;&#24459;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#24182;&#24314;&#31435;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#32473;&#23450;&#26041;&#21521;&#30340;&#27867;&#21270;&#24615;&#33021;&#22914;&#20309;&#38543;&#20854;&#37319;&#26679;&#27604;&#20363;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;&#36890;&#36807;&#35757;&#32451;200&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#12289;&#26041;&#21521;&#21644;&#24635;&#20219;&#21153;&#25968;&#37327;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#26102;&#65292;&#26631;&#37327;&#21270;&#23548;&#33268;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#26435;&#34913;&#21069;&#27839;&#65292;&#35813;&#21069;&#27839;&#20559;&#31163;&#20102;&#20256;&#32479;&#30340;Pareto&#21069;&#27839;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#37325;&#24130;&#24459;&#26469;&#39044;&#27979;MNMT&#20013;&#29420;&#29305;&#30340;&#24615;&#33021;&#26435;&#34913;&#21069;&#27839;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35821;&#35328;&#12289;&#25968;&#25454;&#20805;&#36275;&#24615;&#21644;&#20219;&#21153;&#25968;&#37327;&#26041;&#38754;&#37117;&#24456;&#40065;&#26834;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;MNMT&#20013;&#30340;&#26679;&#26412;&#27604;&#20363;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#22522;&#20110;&#21452;&#37325;&#24130;&#24459;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study how the generalization performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, directions, and total numbers of tasks, we find that scalarization leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus. That is, the performance of certain translation directions does not improve with the increase of its weight in the multi-task optimization objective, which poses greater challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy and number of tasks. Finally, we formulate sample ratio selection in MNMT as an optimization problem based on the Double Power Law, which achieves better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01295</link><description>&lt;p&gt;
&#26377;&#25928;&#22320;&#23545;&#40784;&#36328;&#35821;&#35328;&#20250;&#35805;&#20219;&#21153;&#30340;&#25552;&#31034;&#35843;&#25972;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#20250;&#35805;&#20219;&#21153;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;XSGD&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;Schema-Guided Dialogue&#65288;SGD&#65289;&#32763;&#35793;&#25104;105&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#27979;&#35797;&#20102;&#23545;&#40784;&#25552;&#31034;&#25152;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23545;&#35805;&#20219;&#21153;&#65288;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;GLEN&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;3,465&#31181;&#19981;&#21516;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#26631;&#27880;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#38454;&#27573;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#22823;&#26412;&#20307;&#22823;&#23567;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09093</link><description>&lt;p&gt;
GLEN&#65306;&#38754;&#21521;&#25968;&#21315;&#31181;&#31867;&#22411;&#30340;&#36890;&#29992;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GLEN: General-Purpose Event Detection for Thousands of Types. (arXiv:2303.09093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09093
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;GLEN&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;3,465&#31181;&#19981;&#21516;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#26631;&#27880;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#38454;&#27573;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#22823;&#26412;&#20307;&#22823;&#23567;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25277;&#21462;&#31995;&#32479;&#30340;&#21457;&#23637;&#19968;&#30452;&#21463;&#38480;&#20110;&#32570;&#20047;&#24191;&#27867;&#35206;&#30422;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20351;&#20107;&#20214;&#25277;&#21462;&#31995;&#32479;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;GLEN&#65292;&#28085;&#30422;&#20102;3,465&#31181;&#19981;&#21516;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#26412;&#20307;&#27604;&#20219;&#20309;&#24403;&#21069;&#25968;&#25454;&#38598;&#37117;&#22823;20&#20493;&#20197;&#19978;&#12290;GLEN&#21033;&#29992;DWD&#21472;&#21152;&#25216;&#26415;&#21019;&#24314;&#65292;&#36890;&#36807;&#25552;&#20379;&#32500;&#22522;&#30334;&#31185;Qnode&#21644;PropBank&#35282;&#33394;&#38598;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20351;&#29992;PropBank&#30340;&#29616;&#26377;&#26631;&#27880;&#20316;&#20026;&#38388;&#25509;&#30417;&#30563;&#26469;&#23436;&#25104;&#21019;&#24314;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#38454;&#27573;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;GLEN&#30340;&#22823;&#26412;&#20307;&#22823;&#23567;&#21644;&#37096;&#20998;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65288;F1&#20998;&#25968;&#25552;&#39640;&#20102;&#32422;10%&#65289;&#65292;&#19982;&#20256;&#32479;&#30340;&#20998;&#31867;&#22522;&#32447;&#21644;&#36739;&#26032;&#30340;&#22522;&#20110;&#23450;&#20041;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#26174;&#31034;&#26631;&#31614;&#22122;&#22768;&#20173;&#28982;&#26159;&#25552;&#39640;&#24615;&#33021;&#30340;&#26368;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of event extraction systems has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 3,465 different event types, making it over 20x larger in ontology than any current dataset. GLEN is created by utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables us to use the abundant existing annotation for PropBank as distant supervision. In addition, we also propose a new multi-stage event detection model specifically designed to handle the large ontology size and partial labels in GLEN. We show that our model exhibits superior performance (~10% F1 gain) compared to both conventional classification baselines and newer definition-based models. Finally, we perform error analysis and show that label noise is still the largest challenge for improving performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2212.10764</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#25490;&#21517;&#30340;&#21015;&#34920;&#32423;&#21035;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#26088;&#22312;&#23558;&#22312;&#65288;&#25968;&#25454;&#20016;&#23500;&#65289;&#28304;&#39046;&#22495;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#65288;&#36164;&#28304;&#26377;&#38480;&#65289;&#30446;&#26631;&#39046;&#22495;&#65292;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#65292;&#23427;&#21305;&#37197;&#24182;&#23545;&#40784;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#20294;&#22312;&#25490;&#21517;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#21364;&#26159;&#38646;&#25955;&#30340;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#20960;&#31181;&#23454;&#29616;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#25490;&#21517;&#30340;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#23457;&#26597;&#20043;&#21069;&#30340;&#24037;&#20316;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#23454;&#26045;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#39033;&#30446;&#32423;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32858;&#21512;&#30340;&#25152;&#26377;&#21015;&#34920;&#20013;&#23545;&#36827;&#34892;&#25490;&#21517;&#30340;&#39033;&#30446;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#65292;&#20294;&#24573;&#30053;&#20102;&#21015;&#34920;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#21015;&#34920;&#30340;&#32467;&#26500;&#24212;&#35813;&#34987;&#21033;&#29992;&#65292;&#22240;&#20026;&#23427;&#26159;&#25490;&#21517;&#38382;&#39064;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#24230;&#37327;&#26159;&#22312;&#21015;&#34920;&#19978;&#23450;&#20041;&#21644;&#35745;&#31639;&#30340;&#65292;&#32780;&#19981;&#26159;&#22312;&#39033;&#30446;&#26412;&#36523;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10564</link><description>&lt;p&gt;
&#26080;&#35270;&#35273;&#22522;&#32447;&#30340;&#22810;&#27169;&#24335;&#35821;&#27861;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
A Vision-free Baseline for Multimodal Grammar Induction. (arXiv:2212.10564v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37197;&#23545;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20449;&#21495;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#65288;&#22914;MSCOCO&#65289;&#20013;&#30340;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;LLM&#30340;C-PCFG&#65288;LC-PCFG&#65289;&#65292;&#22312;&#21508;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#27861;&#24402;&#32435;&#24615;&#33021;&#12290;&#19982;&#24102;&#22270;&#20687;&#30340;&#35821;&#27861;&#24402;&#32435;&#30456;&#27604;&#65292;LC-PCFG&#22312;&#35821;&#26009;&#24211;F1&#24471;&#20998;&#19978;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;7.9&#20010;&#28857;&#65292;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;85&#65285;&#65292;&#35757;&#32451;&#36895;&#24230;&#21152;&#24555;&#20102;1.7&#20493;&#12290;&#22312;&#19977;&#20010;&#36741;&#21161;&#35270;&#39057;&#30340;&#35821;&#27861;&#24402;&#32435;&#22522;&#20934;&#20013;&#65292;LC-PCFG&#22312;&#35821;&#26009;&#24211;F1&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26368;&#22810;7.7&#20010;&#28857;&#65292;&#35757;&#32451;&#36895;&#24230;&#21152;&#24555;&#20102;8.8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work has shown that paired vision-language signals substantially improve grammar induction in multimodal datasets such as MSCOCO. We investigate whether advancements in large language models (LLMs) that are only trained with text could provide strong assistance for grammar induction in multimodal settings. We find that our text-only approach, an LLM-based C-PCFG (LC-PCFG), outperforms previous multi-modal methods, and achieves state-of-the-art grammar induction performance for various multimodal datasets. Compared to image-aided grammar induction, LC-PCFG outperforms the prior state-of-the-art by 7.9 Corpus-F1 points, with an 85% reduction in parameter count and 1.7x faster training speed. Across three video-assisted grammar induction benchmarks, LC-PCFG outperforms prior state-of-the-art by up to 7.7 Corpus-F1, with 8.8x faster training. These results shed light on the notion that text-only language models might include visually grounded cues that aid in grammar induction in mult
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#21518;&#22788;&#29702;&#21644;&#19968;&#31181;&#26032;&#26550;&#26500;CI-BERT&#23558;&#27010;&#24565;&#22120;&#25237;&#24433;&#32435;&#20837;&#25152;&#26377;&#23618;&#20013;&#12290;&#27010;&#24565;&#22120;&#21518;&#22788;&#29702;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#35265;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11087</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#22120;&#36741;&#21161;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Conceptor-Aided Debiasing of Large Language Models. (arXiv:2211.11087v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#21518;&#22788;&#29702;&#21644;&#19968;&#31181;&#26032;&#26550;&#26500;CI-BERT&#23558;&#27010;&#24565;&#22120;&#25237;&#24433;&#32435;&#20837;&#25152;&#26377;&#23618;&#20013;&#12290;&#27010;&#24565;&#22120;&#21518;&#22788;&#29702;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#35265;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21453;&#26144;&#20102;&#23427;&#20204;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26410;&#33021;&#21435;&#20559;&#35265;&#25110;&#32773;&#20250;&#29306;&#29298;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#22120;&#8212;&#8212;&#19968;&#31181;&#36719;&#25237;&#24433;&#26041;&#27861;&#8212;&#8212;&#26469;&#35782;&#21035;&#21644;&#21435;&#38500;&#22914;BERT&#21644;GPT&#31561;LLMs&#20013;&#30340;&#20559;&#35265;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24212;&#29992;&#27010;&#24565;&#22120;&#30340;&#26041;&#27861;&#65306;&#65288;1&#65289;&#36890;&#36807;&#21518;&#22788;&#29702;&#36827;&#34892;&#20559;&#35265;&#23376;&#31354;&#38388;&#25237;&#24433;&#65307;&#65288;2&#65289;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#8212;&#8212;&#27010;&#24565;&#22120;&#20171;&#20837;BERT(CI-BERT)&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#26126;&#30830;&#22320;&#23558;&#27010;&#24565;&#22120;&#25237;&#24433;&#32435;&#20837;&#25152;&#26377;&#23618;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27010;&#24565;&#22120;&#21518;&#22788;&#29702;&#22312;&#20445;&#25345;&#25110;&#25552;&#39640;LLMs&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#35265;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#37117;&#24456;&#31283;&#20581;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23545;&#29616;&#26377;&#20559;&#35265;&#23376;&#31354;&#38388;&#30340;&#36923;&#36753;&#25805;&#20316;&#26469;&#26377;&#25928;&#22320;&#20943;&#36731;&#20132;&#38598;&#20559;&#35265;&#12290;&#34429;&#28982;CI-BERT&#30340;&#35757;&#32451;&#32771;&#34385;&#20102;&#25152;&#26377;&#23618;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use conceptors--a soft projection method--to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining or improving LLMs' performance on the GLUE benchmark. Also, it is robust in various scenarios and can mitigate intersectional bias efficiently by its logical operation on the existing bias subspaces. Although CI-BERT's training takes all layers' bias into account and can beat its post-processing counterpa
&lt;/p&gt;</description></item><item><title>EffEval&#26159;&#19968;&#31181;&#23545;&#26426;&#22120;&#32763;&#35793;&#35780;&#20215;&#25351;&#26631;&#25928;&#29575;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;TinyBERT&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#20102;&#26368;&#20339;&#24179;&#34913;&#65292;CPU&#21152;&#36895;&#27604;GPU&#26356;&#26174;&#33879;&#65292;WMD&#36817;&#20284;&#27809;&#26377;&#25552;&#39640;&#25928;&#29575;&#20294;&#38477;&#20302;&#20102;&#36136;&#37327;&#65292;&#36866;&#37197;&#22120;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#26631;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.09593</link><description>&lt;p&gt;
EffEval:&#19968;&#31181;&#20840;&#38754;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#35780;&#20215;&#25351;&#26631;&#25928;&#29575;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics. (arXiv:2209.09593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09593
&lt;/p&gt;
&lt;p&gt;
EffEval&#26159;&#19968;&#31181;&#23545;&#26426;&#22120;&#32763;&#35793;&#35780;&#20215;&#25351;&#26631;&#25928;&#29575;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;TinyBERT&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#20102;&#26368;&#20339;&#24179;&#34913;&#65292;CPU&#21152;&#36895;&#27604;GPU&#26356;&#26174;&#33879;&#65292;WMD&#36817;&#20284;&#27809;&#26377;&#25552;&#39640;&#25928;&#29575;&#20294;&#38477;&#20302;&#20102;&#36136;&#37327;&#65292;&#36866;&#37197;&#22120;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#26631;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25928;&#29575;&#26159;&#20419;&#36827;&#21253;&#23481;&#24615;&#21644;&#20943;&#23569;&#29615;&#22659;&#25104;&#26412;&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;LLM&#26102;&#20195;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#26426;&#22120;&#32763;&#35793;&#35780;&#20215;&#25351;&#26631;&#30340;&#25928;&#29575;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#29992;&#36731;&#37327;&#32423;&#26367;&#20195;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;transformers&#65292;&#24182;&#22312;LLM&#34920;&#31034;&#20043;&#19978;&#37319;&#29992;&#32447;&#24615;&#21644;&#20108;&#27425;&#36817;&#20284;&#30340;&#23545;&#40784;&#31639;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#20010;&#65288;&#26080;&#21442;&#32771;&#21644;&#26377;&#21442;&#32771;&#65289;&#25351;&#26631;&#22312;&#19977;&#20010;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#65292;&#24182;&#26816;&#26597;&#20102;16&#20010;&#36731;&#37327;&#32423;transformers&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#26469;&#30740;&#31350;COMET&#31561;&#25351;&#26631;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;a&#65289;TinyBERT&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#20102;&#26368;&#20339;&#24179;&#34913;&#65292;&#65288;b&#65289;CPU&#21152;&#36895;&#27604;GPU&#26356;&#26174;&#33879;&#65292;&#65288;c&#65289;WMD&#36817;&#20284;&#27809;&#26377;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#38477;&#20302;&#20102;&#36136;&#37327;&#65292;&#65288;d&#65289;&#36866;&#37197;&#22120;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#65288;&#20851;&#20110;&#21453;&#21521;&#20256;&#25773;&#36895;&#24230;&#21644;&#20869;&#23384;&#35201;&#27714;&#65289;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#26631;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiency is a key property to foster inclusiveness and reduce environmental costs, especially in an era of LLMs. In this work, we provide a comprehensive evaluation of efficiency for MT evaluation metrics. Our approach involves replacing computation-intensive transformers with lighter alternatives and employing linear and quadratic approximations for alignment algorithms on top of LLM representations. We evaluate six (reference-free and reference-based) metrics across three MT datasets and examine 16 lightweight transformers. In addition, we look into the training efficiency of metrics like COMET by utilizing adapters. Our results indicate that (a) TinyBERT provides the optimal balance between quality and efficiency, (b) CPU speed-ups are more substantial than those on GPU; (c) WMD approximations yield no efficiency gains while reducing quality and (d) adapters enhance training efficiency (regarding backward pass speed and memory requirements) as well as, in some cases, metric qualit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35789;&#35821;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#30452;&#25509;&#30340;&#32852;&#31995;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#38500;&#20102;&#22270;&#20687;&#27169;&#25311;&#22806;&#65292;&#21463;&#35797;&#32773;&#21487;&#33021;&#37319;&#29992;&#30340;&#20854;&#20182;&#31574;&#30053;&#65292;&#24182;&#20998;&#26512;&#20102;&#20219;&#21153;&#35299;&#20915;&#26159;&#21542;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2206.15381</link><description>&lt;p&gt;
&#35789;&#35821;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#32852;&#31995;&#26377;&#22810;&#30452;&#25509;&#65311;
&lt;/p&gt;
&lt;p&gt;
How direct is the link between words and images?. (arXiv:2206.15381v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35789;&#35821;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#30452;&#25509;&#30340;&#32852;&#31995;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#38500;&#20102;&#22270;&#20687;&#27169;&#25311;&#22806;&#65292;&#21463;&#35797;&#32773;&#21487;&#33021;&#37319;&#29992;&#30340;&#20854;&#20182;&#31574;&#30053;&#65292;&#24182;&#20998;&#26512;&#20102;&#20219;&#21153;&#35299;&#20915;&#26159;&#21542;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#30340;&#35789;&#23884;&#20837;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#32852;&#31995;&#12290;&#22312;&#36825;&#19968;&#30740;&#31350;&#20013;&#65292;Gunther&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#39033;&#34892;&#20026;&#23454;&#39564;&#26469;&#25506;&#31350;&#35789;&#35821;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#21442;&#19982;&#32773;&#23558;&#34987;&#21576;&#29616;&#19968;&#20010;&#30446;&#26631;&#21517;&#35789;&#21644;&#19968;&#23545;&#22270;&#20687;&#65292;&#20854;&#20013;&#19968;&#20010;&#30001;&#27169;&#22411;&#36873;&#25321;&#65292;&#21478;&#19968;&#20010;&#38543;&#26426;&#36873;&#25321;&#12290;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#36873;&#25321;&#19982;&#30446;&#26631;&#21517;&#35789;&#26368;&#21305;&#37197;&#30340;&#22270;&#20687;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#27169;&#22411;&#36873;&#25321;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;Gunther&#31561;&#20154;&#24471;&#20986;&#20102;&#35789;&#35821;&#21644;&#20855;&#20307;&#20307;&#39564;&#20043;&#38388;&#23384;&#22312;&#30452;&#25509;&#32852;&#31995;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20197;&#20182;&#20204;&#30340;&#23454;&#39564;&#20026;&#20986;&#21457;&#28857;&#65292;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#12290;1.&#38500;&#20102;&#21033;&#29992;&#32473;&#23450;&#22270;&#20687;&#30340;&#35270;&#35273;&#21270;&#27169;&#25311;&#22806;&#65292;&#21463;&#35797;&#32773;&#21487;&#33021;&#36824;&#20351;&#29992;&#20102;&#21738;&#20123;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65311;&#36825;&#20010;&#35774;&#32622;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#26469;&#33258;&#22270;&#20687;&#30340;&#35270;&#35273;&#20449;&#24687;&#65311;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#32431;&#25991;&#26412;&#34920;&#31034;&#26469;&#35299;&#20915;&#23427;&#65311;
&lt;/p&gt;
&lt;p&gt;
Current word embedding models despite their success, still suffer from their lack of grounding in the real world. In this line of research, Gunther et al. 2022 proposed a behavioral experiment to investigate the relationship between words and images. In their setup, participants were presented with a target noun and a pair of images, one chosen by their model and another chosen randomly. Participants were asked to select the image that best matched the target noun. In most cases, participants preferred the image selected by the model. Gunther et al., therefore, concluded the possibility of a direct link between words and embodied experience. We took their experiment as a point of departure and addressed the following questions. 1. Apart from utilizing visually embodied simulation of given images, what other strategies might subjects have used to solve this task? To what extent does this setup rely on visual information from images? Can it be solved using purely textual representations?
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#35745;&#31639;&#22522;&#20110;&#39044;&#35757;&#32451;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#25991;&#26412;&#34920;&#31034;&#21644;&#35270;&#35273;&#24863;&#30693;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2206.08823</link><description>&lt;p&gt;
&#24102;&#26377;&#35270;&#35273;&#30340;&#35821;&#35328;: &#23545;&#22522;&#20110;&#24863;&#30693;&#30340;&#35789;&#35821;&#21644;&#21477;&#23376;&#23884;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Language with Vision: a Study on Grounded Word and Sentence Embeddings. (arXiv:2206.08823v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#35745;&#31639;&#22522;&#20110;&#39044;&#35757;&#32451;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#25991;&#26412;&#34920;&#31034;&#21644;&#35270;&#35273;&#24863;&#30693;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#19982;&#35270;&#35273;&#32852;&#31995;&#36215;&#26469;&#26159;&#19968;&#20010;&#31215;&#26497;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#26469;&#33258;&#35270;&#35273;&#30340;&#24863;&#30693;&#30693;&#35782;&#34701;&#20837;&#21040;&#22522;&#20110;&#25991;&#26412;&#30340;&#34920;&#31034;&#20013;&#65292;&#26500;&#24314;&#31526;&#21512;&#35748;&#30693;&#30340;&#35789;&#35821;&#21644;&#21477;&#23376;&#34920;&#31034;&#12290;&#23613;&#31649;&#22312;&#35821;&#35328;&#24863;&#30693;&#26041;&#38754;&#24050;&#32463;&#23581;&#35797;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#22312;&#25991;&#26412;&#34920;&#31034;&#21644;&#20154;&#31867;&#32463;&#39564;&#20043;&#38388;&#23454;&#29616;&#26368;&#20248;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#39046;&#22495;&#12290;&#19968;&#20123;&#24120;&#35265;&#38382;&#39064;&#26159;&#65306;&#35270;&#35273;&#24863;&#30693;&#26159;&#21542;&#23545;&#25277;&#35937;&#35789;&#35821;&#26377;&#20248;&#21183;&#65292;&#36824;&#26159;&#20165;&#38480;&#20110;&#20855;&#20307;&#35789;&#35821;&#65311;&#22312;&#25991;&#26412;&#21644;&#35270;&#35273;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#20160;&#20040;&#65311;&#22270;&#20687;&#30340;&#24863;&#30693;&#30693;&#35782;&#23545;&#20110;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#23884;&#20837;&#26159;&#21542;&#26377;&#20248;&#21183;&#65311;&#20511;&#21161;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#38750;&#24120;&#26377;&#25928;&#30340;&#35745;&#31639;&#22522;&#20110;&#39044;&#35757;&#32451;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grounding language in vision is an active field of research seeking to construct cognitively plausible word and sentence representations by incorporating perceptual knowledge from vision into text-based representations. Despite many attempts at language grounding, achieving an optimal equilibrium between textual representations of the language and our embodied experiences remains an open field. Some common concerns are the following. Is visual grounding advantageous for abstract words, or is its effectiveness restricted to concrete words? What is the optimal way of bridging the gap between text and vision? To what extent is perceptual knowledge from images advantageous for acquiring high-quality embeddings? Leveraging the current advances in machine learning and natural language processing, the present study addresses these questions by proposing a simple yet very effective computational grounding model for pre-trained word embeddings. Our model effectively balances the interplay betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#26631;&#31614;&#30340;&#26032;&#30446;&#26631;&#20989;&#25968;BAT&#65292;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#32467;&#26500;&#21644;&#23398;&#20064;&#29575;&#31574;&#30053;&#65292;&#22312;50&#20010;epochs&#20869;&#23454;&#29616;&#20102;&#26356;&#24555;&#12289;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;F&#24471;&#20998;&#65292;&#20026;&#33258;&#21160;&#26631;&#35760;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2206.07264</link><description>&lt;p&gt;
&#20026;&#33258;&#21160;&#26631;&#31614;&#32780;&#29983;&#65306;&#26032;&#30446;&#26631;&#20989;&#25968;&#30340;&#26356;&#24555;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Born for Auto-Tagging: Faster and better with new objective functions. (arXiv:2206.07264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#26631;&#31614;&#30340;&#26032;&#30446;&#26631;&#20989;&#25968;BAT&#65292;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#32467;&#26500;&#21644;&#23398;&#20064;&#29575;&#31574;&#30053;&#65292;&#22312;50&#20010;epochs&#20869;&#23454;&#29616;&#20102;&#26356;&#24555;&#12289;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;F&#24471;&#20998;&#65292;&#20026;&#33258;&#21160;&#26631;&#35760;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#25552;&#21462;&#26159;&#25991;&#26412;&#25366;&#25496;&#30340;&#20219;&#21153;&#65292;&#24212;&#29992;&#20110;&#22686;&#21152;&#25628;&#32034;&#24341;&#25806;&#20248;&#21270;&#21644;&#24191;&#21578;&#30340;&#25628;&#32034;&#37327;&#12290;&#22312;&#33258;&#21160;&#26631;&#35760;&#20013;&#23454;&#26045;&#65292;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#23545;&#22312;&#32447;&#25991;&#31456;&#21644;&#29031;&#29255;&#36827;&#34892;&#22823;&#35268;&#27169;&#26631;&#35760;&#12290;BAT&#26159;&#20026;&#33258;&#21160;&#26631;&#35760;&#32780;&#21457;&#26126;&#30340;&#65292;&#23427;&#20316;&#20026;awoo&#30340;AI&#33829;&#38144;&#24179;&#21488;&#65288;AMP&#65289;&#25552;&#20379;&#26381;&#21153;&#12290;awoo AMP&#19981;&#20165;&#25552;&#20379;&#23450;&#21046;&#30340;&#25512;&#33616;&#31995;&#32479;&#26381;&#21153;&#65292;&#36824;&#25552;&#39640;&#20102;&#30005;&#23376;&#21830;&#21153;&#30340;&#36716;&#21270;&#29575;&#12290;BAT&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;4&#23618;&#32467;&#26500;&#22312;50&#20010;epochs&#36798;&#21040;&#20102;&#26368;&#20339;&#30340;F&#24471;&#20998;&#65292;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#65292;&#25928;&#26524;&#26356;&#22909;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#27604;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;100&#20010;epochs&#38656;&#35201;&#26356;&#28145;&#30340;&#23618;&#25968;&#12290;&#20026;&#20102;&#29983;&#25104;&#20016;&#23500;&#21644;&#28165;&#26224;&#30340;&#26631;&#31614;&#65292;awoo&#21019;&#24314;&#20102;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#19982;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;F1&#24471;&#20998;&#30340;&#21516;&#26102;&#22686;&#24378;&#20102;F2&#24471;&#20998;&#12290;&#20026;&#20102;&#20445;&#35777;F&#24471;&#20998;&#30340;&#26356;&#22909;&#34920;&#29616;&#65292;awoo&#25913;&#36827;&#20102;Transformer&#25552;&#20986;&#30340;&#23398;&#20064;&#29575;&#31574;&#30053;&#65292;&#22686;&#21152;&#20102;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyword extraction is a task of text mining. It is applied to increase search volume in SEO and ads. Implemented in auto-tagging, it makes tagging on a mass scale of online articles and photos efficiently and accurately. BAT is invented for auto-tagging which served as awoo's AI marketing platform (AMP). awoo AMP not only provides service as a customized recommender system but also increases the converting rate in E-commerce. The strength of BAT converges faster and better than other SOTA models, as its 4-layer structure achieves the best F scores at 50 epochs. In other words, it performs better than other models which require deeper layers at 100 epochs. To generate rich and clean tags, awoo creates new objective functions to maintain similar ${\rm F_1}$ scores with cross-entropy while enhancing ${\rm F_2}$ scores simultaneously. To assure the even better performance of F scores awoo revamps the learning rate strategy proposed by Transformer \cite{Transformer} to increase ${\rm F_1}$ 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#35843;&#25972;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#22312;&#22810;&#31181;&#35821;&#35328;&#21644;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#35770;&#26159;&#35813;&#26041;&#27861;&#23545;&#22810;&#35821;&#35328;NLI&#26377;&#30410;&#65292;&#21516;&#26102;&#23545;NER&#12289;XSR&#21644;&#36328;&#35821;&#35328;QA&#20063;&#26377;&#25913;&#36827;&#65292;&#23588;&#20854;&#23545;&#26576;&#20123;&#35821;&#35328;&#26356;&#20026;&#26126;&#26174;&#12290;&#32780;&#21333;&#35821;QA&#24615;&#33021;&#27809;&#26377;&#25913;&#21892;&#65292;&#26377;&#26102;&#29978;&#33267;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2204.06457</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#35843;&#25972;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#23545;&#38646;&#32763;&#35793;&#36716;&#31227;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Cross-Lingual Adjustment of Contextual Word Representations on Zero-Shot Transfer. (arXiv:2204.06457v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.06457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#35843;&#25972;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#22312;&#22810;&#31181;&#35821;&#35328;&#21644;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#35770;&#26159;&#35813;&#26041;&#27861;&#23545;&#22810;&#35821;&#35328;NLI&#26377;&#30410;&#65292;&#21516;&#26102;&#23545;NER&#12289;XSR&#21644;&#36328;&#35821;&#35328;QA&#20063;&#26377;&#25913;&#36827;&#65292;&#23588;&#20854;&#23545;&#26576;&#20123;&#35821;&#35328;&#26356;&#20026;&#26126;&#26174;&#12290;&#32780;&#21333;&#35821;QA&#24615;&#33021;&#27809;&#26377;&#25913;&#21892;&#65292;&#26377;&#26102;&#29978;&#33267;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22914;mBERT&#25110;XLM-R&#21487;&#20197;&#22312;&#21508;&#31181;IR&#21644;NLP&#20219;&#21153;&#20013;&#23454;&#29616;&#38646;&#32763;&#35793;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;Cao&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;mBERT&#36328;&#35821;&#35328;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#23567;&#22411;&#24179;&#34892;&#35821;&#26009;&#24211;&#20351;&#19981;&#21516;&#35821;&#35328;&#20013;&#30456;&#20851;&#21333;&#35789;&#30340;&#23884;&#20837;&#30456;&#20284;&#12290;&#20182;&#20204;&#22312;&#20116;&#31181;&#27431;&#27954;&#35821;&#35328;&#30340;NLI&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#22312;&#21253;&#21547;&#35199;&#29677;&#29273;&#35821;&#12289;&#20420;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#21360;&#22320;&#35821;&#22312;&#20869;&#30340;&#35821;&#35328;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#25193;&#23637;&#20102;&#20182;&#20204;&#30340;&#21407;&#22987;&#23454;&#29616;&#26469;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65288;XSR&#12289;NER&#21644;QA&#65289;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#26426;&#21046;&#65288;&#25345;&#32493;&#23398;&#20064;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22235;&#31181;&#35821;&#35328;&#37325;&#29616;&#20102;NLI&#30340;&#22686;&#30410;&#65292;&#22312;&#19977;&#31181;&#35821;&#35328;&#19978;&#26174;&#31034;&#20986;&#20102;&#25913;&#36827;&#30340;NER&#12289;XSR&#21644;&#36328;&#35821;&#35328;QA&#32467;&#26524;&#65288;&#23613;&#31649;&#26576;&#20123;&#36328;&#35821;&#35328;QA&#30340;&#22686;&#30410;&#22312;&#32479;&#35745;&#19978;&#19981;&#26174;&#33879;&#65289;&#65292;&#32780;&#21333;&#35821;QA&#24615;&#33021;&#20174;&#26410;&#25552;&#39640;&#65292;&#26377;&#26102;&#29978;&#33267;&#19979;&#38477;&#12290;&#23545;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#30340;&#36317;&#31163;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multilingual language models such as mBERT or XLM-R enable zero-shot cross-lingual transfer in various IR and NLP tasks. Cao et al. (2020) proposed a data- and compute-efficient method for cross-lingual adjustment of mBERT that uses a small parallel corpus to make embeddings of related words across languages similar to each other. They showed it to be effective in NLI for five European languages. In contrast we experiment with a typologically diverse set of languages (Spanish, Russian, Vietnamese, and Hindi) and extend their original implementations to new tasks (XSR, NER, and QA) and an additional training regime (continual learning). Our study reproduced gains in NLI for four languages, showed improved NER, XSR, and cross-lingual QA results in three languages (though some cross-lingual QA gains were not statistically significant), while mono-lingual QA performance never improved and sometimes degraded. Analysis of distances between contextualized embeddings of related and unrel
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;WEAVER&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;&#26087;&#30693;&#35782;&#34701;&#20837;&#21040;&#26032;&#27169;&#22411;&#20013;&#65292;&#20197;&#26377;&#25928;&#38477;&#20302;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#23454;&#29616;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#32456;&#36523;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2202.10101</link><description>&lt;p&gt;
BERT WEAVER&#65306;&#20351;&#29992;&#21152;&#26435;&#24179;&#22343;&#20351;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23454;&#29616;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models in the Biomedical Domain. (arXiv:2202.10101v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10101
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;WEAVER&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;&#26087;&#30693;&#35782;&#34701;&#20837;&#21040;&#26032;&#27169;&#22411;&#20013;&#65292;&#20197;&#26377;&#25928;&#38477;&#20302;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#23454;&#29616;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#32456;&#36523;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#36716;&#31227;&#23398;&#20064;&#26041;&#38754;&#30340;&#21457;&#23637;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#21462;&#20915;&#20110;&#39640;&#36136;&#37327;&#30340;&#25163;&#21160;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#12290;&#23588;&#20854;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#24050;&#32463;&#34920;&#26126;&#19968;&#31181;&#35757;&#32451;&#35821;&#26009;&#24211;&#19981;&#36275;&#20197;&#23398;&#20064;&#33021;&#22815;&#22312;&#26032;&#25968;&#25454;&#19978;&#39640;&#25928;&#39044;&#27979;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#38656;&#35201;&#20855;&#22791;&#32456;&#36523;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#26032;&#25968;&#25454;&#21487;&#29992;&#26102;&#25552;&#39640;&#24615;&#33021; - &#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;WEAVER&#65292;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#26087;&#30693;&#35782;&#34701;&#20837;&#21040;&#26032;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39034;&#24207;&#24212;&#29992;WEAVER&#20250;&#20135;&#29983;&#31867;&#20284;&#20110;&#19968;&#27425;&#24615;&#20351;&#29992;&#25152;&#26377;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#20998;&#24067;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;&#30001;&#20110;&#27809;&#26377;&#25968;&#25454;&#20849;&#20139;&#30340;&#24517;&#35201;&#65292;&#22240;&#27492;&#25152;&#20171;&#32461;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#38544;&#31169;&#26159;&#19968;&#39033;&#20851;&#27880;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in transfer learning have boosted the advancements in natural language processing tasks. The performance is, however, dependent on high-quality, manually annotated training data. Especially in the biomedical domain, it has been shown that one training corpus is not enough to learn generic models that are able to efficiently predict on new data. Therefore, state-of-the-art models need the ability of lifelong learning in order to improve performance as soon as new data are available - without the need of re-training the whole model from scratch. We present WEAVER, a simple, yet efficient post-processing method that infuses old knowledge into the new model, thereby reducing catastrophic forgetting. We show that applying WEAVER in a sequential manner results in similar word embedding distributions as doing a combined training on all data at once, while being computationally more efficient. Because there is no need of data sharing, the presented method is also easily app
&lt;/p&gt;</description></item></channel></rss>