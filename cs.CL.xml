<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#26080;&#32467;&#26500;&#25968;&#25454;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#28508;&#21147;&#21450;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.13010</link><description>&lt;p&gt;
&#26080;&#32467;&#26500;&#25968;&#25454;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65306;&#25105;&#20204;&#33021;&#21542;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unstructured and structured data: Can we have the best of both worlds with large language models?. (arXiv:2304.13010v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#26080;&#32467;&#26500;&#25968;&#25454;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#28508;&#21147;&#21450;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#26080;&#32467;&#26500;&#25968;&#25454;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#24182;&#27010;&#36848;&#20102;&#26500;&#24314;&#36866;&#29992;&#20110;&#20004;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#38382;&#31572;&#31995;&#32479;&#25152;&#28041;&#21450;&#30340;&#19968;&#20123;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an opinion on the potential of using large language models to query on both unstructured and structured data. It also outlines some research challenges related to the topic of building question-answering systems for both types of data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;Multi-Chain Reasoning (MCR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26816;&#26597;&#22810;&#20010;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65292;&#35299;&#20915;&#22810;&#36339;QA&#38382;&#39064;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MCR&#32988;&#36807;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#35299;&#37322;&#36136;&#37327;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.13007</link><description>&lt;p&gt;
&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65306;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;&#38382;&#39064;&#35299;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Answering Questions by Meta-Reasoning over Multiple Chains of Thought. (arXiv:2304.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;Multi-Chain Reasoning (MCR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26816;&#26597;&#22810;&#20010;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65292;&#35299;&#20915;&#22810;&#36339;QA&#38382;&#39064;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MCR&#32988;&#36807;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#35299;&#37322;&#36136;&#37327;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#36339;&#38382;&#39064;&#35299;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#36890;&#24120;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#24605;&#32771;&#27493;&#39588;&#65288;CoT&#65289;&#65292;&#28982;&#21518;&#25165;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#24120;&#26469;&#35828;&#65292;&#22810;&#20010;&#38142;&#26465;&#34987;&#25277;&#26679;&#24182;&#36890;&#36807;&#26368;&#32456;&#31572;&#26696;&#30340;&#25237;&#31080;&#26426;&#21046;&#36827;&#34892;&#32858;&#21512;&#65292;&#20294;&#20013;&#38388;&#27493;&#39588;&#26412;&#36523;&#34987;&#20002;&#24323;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#32771;&#34385;&#38142;&#20043;&#38388;&#30340;&#20013;&#38388;&#27493;&#39588;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#19981;&#25552;&#20379;&#39044;&#27979;&#31572;&#26696;&#30340;&#32479;&#19968;&#35299;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340; Multi-Chain Reasoning (MCR) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36229;&#36234;&#22810;&#20010;&#24605;&#32771;&#38142;&#65292;&#32780;&#19981;&#26159;&#32858;&#21512;&#22238;&#31572;&#12290;MCR&#26816;&#26597;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#39044;&#27979;&#31572;&#26696;&#26102;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;MCR&#22312;7&#20010;&#22810;&#36339;QA&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;MCR&#30340;&#35299;&#37322;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, en
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;11&#31181;&#19981;&#21516;&#30340;&#21360;&#24230;&#35821;&#35328;&#30340;&#38388;&#35821;&#35328;Seq2seq&#35821;&#20041;&#20998;&#26512;&#25968;&#25454;&#38598;IE-SEMPARSE&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;seq2seq&#27169;&#22411;&#22312;&#20854;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13005</link><description>&lt;p&gt;
&#35780;&#20272;&#21360;&#24230;&#35821;&#35328;&#38388;&#35821;&#20041;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating Inter-Bilingual Semantic Parsing for Indian Languages. (arXiv:2304.13005v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;11&#31181;&#19981;&#21516;&#30340;&#21360;&#24230;&#35821;&#35328;&#30340;&#38388;&#35821;&#35328;Seq2seq&#35821;&#20041;&#20998;&#26512;&#25968;&#25454;&#38598;IE-SEMPARSE&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;seq2seq&#27169;&#22411;&#22312;&#20854;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21360;&#24230;&#35821;&#35328;&#65288;IndicNLP&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20687;&#35821;&#20041;&#35299;&#26512;&#36825;&#26679;&#30340;&#22797;&#26434;&#32467;&#26500;&#20219;&#21153;&#32570;&#20047;&#25968;&#25454;&#38598;&#12290;&#23548;&#33268;&#27492;&#20005;&#37325;&#32570;&#21475;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#36923;&#36753;&#24418;&#24335;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20351;&#24471;&#33521;&#35821;&#21040;&#22810;&#35821;&#35328;&#30340;&#32763;&#35793;&#21464;&#24471;&#22256;&#38590;&#12290; &#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#23558;&#36923;&#36753;&#24418;&#24335;&#12289;&#24847;&#22270;&#21644;&#27133;&#19982;&#32763;&#35793;&#30340;&#38750;&#32467;&#26500;&#21270;&#35805;&#35821;&#23545;&#40784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;11&#31181;&#19981;&#21516;&#30340;&#21360;&#24230;&#35821;&#35328;&#30340;&#38388;&#35821;&#35328;Seq2seq&#35821;&#20041;&#20998;&#26512;&#25968;&#25454;&#38598;IE-SEMPARSE&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#25152;&#25552;&#20986;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;seq2seq&#27169;&#22411;&#22312;&#20960;&#31181;&#35757;&#32451;-&#27979;&#35797;&#31574;&#30053;&#20043;&#38388;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#21407;&#22987;&#22810;&#35821;&#35328;&#35821;&#20041;&#20998;&#26512;&#25968;&#25454;&#38598;&#65288;&#22914;mTOP&#12289;&#22810;&#35821;&#35328;TOP&#21644;multiATIS++&#65289;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;IE-SEMPARSE&#22871;&#20214;&#20043;&#38388;&#30340;&#39640;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress in Natural Language Generation for Indian languages (IndicNLP), there is a lack of datasets around complex structured tasks such as semantic parsing. One reason for this imminent gap is the complexity of the logical form, which makes English to multilingual translation difficult. The process involves alignment of logical forms, intents and slots with translated unstructured utterance. To address this, we propose an Inter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinct Indian languages. We highlight the proposed task's practicality, and evaluate existing multilingual seq2seq models across several train-test strategies. Our experiment reveals a high correlation across performance of original multilingual semantic parsing datasets (such as mTOP, multilingual TOP and multiATIS++) and our proposed IE-SEMPARSE suite.
&lt;/p&gt;</description></item><item><title>AudioGPT&#26159;&#19968;&#31181;&#22810;&#27169;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#38899;&#39057;&#20449;&#24687;&#24182;&#25903;&#25345;&#21475;&#35821;&#23545;&#35805;&#65292;&#20854;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22788;&#29702;&#35821;&#38899;&#12289;&#38899;&#20048;&#12289;&#22768;&#38899;&#21644;&#20154;&#22836;&#20687;&#26041;&#38754;&#26377;&#30528;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.12995</link><description>&lt;p&gt;
AudioGPT&#65306;&#29702;&#35299;&#21644;&#29983;&#25104;&#35821;&#38899;&#12289;&#38899;&#20048;&#12289;&#22768;&#38899;&#21644;&#20154;&#22836;&#20687;
&lt;/p&gt;
&lt;p&gt;
AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head. (arXiv:2304.12995v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12995
&lt;/p&gt;
&lt;p&gt;
AudioGPT&#26159;&#19968;&#31181;&#22810;&#27169;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#38899;&#39057;&#20449;&#24687;&#24182;&#25903;&#25345;&#21475;&#35821;&#23545;&#35805;&#65292;&#20854;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22788;&#29702;&#35821;&#38899;&#12289;&#38899;&#20048;&#12289;&#22768;&#38899;&#21644;&#20154;&#22836;&#20687;&#26041;&#38754;&#26377;&#30528;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#25361;&#25112;&#20102;&#25105;&#20204;&#23545;&#23398;&#20064;&#21644;&#35748;&#30693;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#24403;&#21069;&#30340;LLM&#26080;&#27861;&#22788;&#29702;&#22797;&#26434;&#30340;&#38899;&#39057;&#20449;&#24687;&#25110;&#36827;&#34892;&#21475;&#35821;&#20132;&#27969;&#65288;&#22914;Siri&#25110;Alexa&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AudioGPT&#30340;&#22810;&#27169;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#34917;&#20805;&#20102;LLM&#65288;&#21363;ChatGPT&#65289;&#65306;1&#65289;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#38899;&#39057;&#20449;&#24687;&#24182;&#35299;&#20915;&#20247;&#22810;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65307;2&#65289;&#25552;&#20379;&#36755;&#20837;/&#36755;&#20986;&#25509;&#21475;&#65288;ASR&#65292;TTS&#65289;&#20197;&#25903;&#25345;&#21475;&#35821;&#23545;&#35805;&#12290;&#38543;&#30528;&#23545;&#20154;&#31867;&#24847;&#22270;&#29702;&#35299;&#21644;&#19982;&#22522;&#30784;&#27169;&#22411;&#21327;&#20316;&#30340;&#22810;&#27169;&#24335;LLM&#30340;&#35780;&#20272;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21407;&#21017;&#21644;&#36807;&#31243;&#65292;&#24182;&#27979;&#35797;&#20102;AudioGPT&#30340;&#19968;&#33268;&#24615;&#12289;&#33021;&#21147;&#21644;&#31283;&#20581;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;AudioGPT&#22312;&#35299;&#20915;&#20855;&#26377;&#35821;&#38899;&#12289;&#38899;&#20048;&#12289;&#22768;&#38899;&#21644;&#20154;&#22836;&#20687;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;AI&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and gene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#27979;&#35797;&#65292;&#20197;&#34913;&#37327;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#27979;&#35797;&#28085;&#30422;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#26377;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#24320;&#21457;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#22343;&#34913;&#30340;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.12986</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Measuring Massive Multitask Chinese Understanding. (arXiv:2304.12986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#27979;&#35797;&#65292;&#20197;&#34913;&#37327;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#27979;&#35797;&#28085;&#30422;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#26377;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#24320;&#21457;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#22343;&#34913;&#30340;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#21457;&#27491;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#32570;&#20047;&#30456;&#24212;&#30340;&#33021;&#21147;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#65292;&#20197;&#34913;&#37327;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#12290;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26377;15&#20010;&#23376;&#20219;&#21153;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#26377;8&#20010;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#24179;&#22343;&#27604;&#34920;&#29616;&#26368;&#24046;&#30340;&#27169;&#22411;&#39640;&#20986;&#36817;22&#20010;&#30334;&#20998;&#28857;&#12290;&#22312;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#20013;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24179;&#22343;&#38646;&#26679;&#26412;&#20934;&#30830;&#24230;&#22343;&#26410;&#36229;&#36807;0.5&#12290;&#22312;&#23376;&#39046;&#22495;&#20013;&#65292;&#21482;&#26377;GPT-3.5-turbo&#27169;&#22411;&#22312;&#20020;&#24202;&#21307;&#23398;&#20013;&#23454;&#29616;&#20102;0.703&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24230;&#65292;&#36825;&#26159;&#25152;&#26377;&#27169;&#22411;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;&#25152;&#26377;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#65292;&#26368;&#39640;&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24230;&#20165;&#36798;&#21040;0.259&#12290;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#22810;&#20010;&#23398;&#31185;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#24320;&#21457;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#22343;&#34913;&#30340;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large-scale Chinese language models is flourishing, yet there is a lack of corresponding capability assessments. Therefore, we propose a test to measure the multitask accuracy of large Chinese language models. This test encompasses four major domains, including medicine, law, psychology, and education, with 15 subtasks in medicine and 8 subtasks in education. We found that the best-performing models in the zero-shot setting outperformed the worst-performing models by nearly 22 percentage points on average. Across the four major domains, the average zero-shot accuracy of all models did not exceed 0.5. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot accuracy of 0.703 in clinical medicine, which was the highest accuracy among all models across all subtasks. All models performed poorly in the legal domain, with the highest zero-shot accuracy reaching only 0.259. By comprehensively evaluating the breadth and depth of knowledge across multiple discipli
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DSTC11&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#36319;&#36394;&#30340;&#20250;&#35805;&#24847;&#22270;&#35825;&#23548;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#23376;&#20219;&#21153;&#21644;&#19977;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32473;&#20986;&#20102;&#31616;&#21333;&#30340;&#22522;&#32447;&#65292;&#29992;&#20197;&#35780;&#20272;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#20854;&#20013;&#26088;&#22312;&#22312;&#23458;&#25143;&#26381;&#21153;&#20132;&#20114;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#33258;&#21160;&#35825;&#23548;&#23458;&#25143;&#24847;&#22270;&#24182;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.12982</link><description>&lt;p&gt;
&#38754;&#21521;DSTC11&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#36319;&#36394;&#30340;&#20250;&#35805;&#24847;&#22270;&#35825;&#23548;
&lt;/p&gt;
&lt;p&gt;
Intent Induction from Conversations for Task-Oriented Dialogue Track at DSTC 11. (arXiv:2304.12982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DSTC11&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#36319;&#36394;&#30340;&#20250;&#35805;&#24847;&#22270;&#35825;&#23548;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#23376;&#20219;&#21153;&#21644;&#19977;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#32473;&#20986;&#20102;&#31616;&#21333;&#30340;&#22522;&#32447;&#65292;&#29992;&#20197;&#35780;&#20272;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#20854;&#20013;&#26088;&#22312;&#22312;&#23458;&#25143;&#26381;&#21153;&#20132;&#20114;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#33258;&#21160;&#35825;&#23548;&#23458;&#25143;&#24847;&#22270;&#24182;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#34394;&#25311;&#21161;&#25163;&#30340;&#38656;&#27714;&#21644;&#26222;&#21450;&#22686;&#21152;&#65292;&#36817;&#24180;&#26469;&#30340;&#19968;&#20123;&#24037;&#20316;&#30740;&#31350;&#20102;&#36890;&#36807;&#33258;&#21160;&#35825;&#23548;&#24847;&#22270;&#25110;&#35825;&#23548;&#27133;&#21644;&#23545;&#35805;&#29366;&#24577;&#26469;&#21152;&#36895;&#26426;&#22120;&#20154;&#26550;&#26500;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19987;&#29992;&#22522;&#20934;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#20351;&#24471;&#36827;&#23637;&#38590;&#20197;&#36319;&#36394;&#65292;&#31995;&#32479;&#20043;&#38388;&#30340;&#27604;&#36739;&#20063;&#38590;&#20197;&#36827;&#34892;&#12290;&#26412;&#27425;&#25361;&#25112;&#36187;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;&#22312;&#20154;&#31867;&#20195;&#29702;&#21644;&#23458;&#25143;&#20043;&#38388;&#30340;&#23458;&#25143;&#26381;&#21153;&#20132;&#20114;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#33258;&#21160;&#35825;&#23548;&#23458;&#25143;&#24847;&#22270;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#23376;&#20219;&#21153;&#26469;&#36880;&#27493;&#35299;&#20915;&#33258;&#21160;&#35825;&#23548;&#24847;&#22270;&#21644;&#30456;&#24212;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#36866;&#21512;&#36827;&#34892;&#20219;&#21153;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#22522;&#32447;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25361;&#25112;&#36187;&#30340;&#25552;&#20132;&#21644;&#32467;&#26524;&#65292;&#20849;&#25910;&#21040;&#20102;&#26469;&#33258;34&#20010;&#22242;&#38431;&#30340;&#25552;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increasing demand for and adoption of virtual assistants, recent work has investigated ways to accelerate bot schema design through the automatic induction of intents or the induction of slots and dialogue states. However, a lack of dedicated benchmarks and standardized evaluation has made progress difficult to track and comparisons between systems difficult to make. This challenge track, held as part of the Eleventh Dialog Systems Technology Challenge, introduces a benchmark that aims to evaluate methods for the automatic induction of customer intents in a realistic setting of customer service interactions between human agents and customers. We propose two subtasks for progressively tackling the automatic induction of intents and corresponding evaluation methodologies. We then present three datasets suitable for evaluating the tasks and propose simple baselines. Finally, we summarize the submissions and results of the challenge track, for which we received submissions from 34 tea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;GMU&#22242;&#38431;&#22312;SemEval-2023&#20849;&#20139;&#20219;&#21153;AfriSenti-SemEval&#20013;&#25152;&#20351;&#29992;&#30340;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#65292;&#20351;&#29992;AfroXLMR-large&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#24341;&#20837;&#20102;&#22686;&#24378;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#22522;&#20110;Phylogeny&#30340;&#36866;&#37197;&#22120;&#35843;&#25972;&#20197;&#24471;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12979</link><description>&lt;p&gt;
&#22522;&#20110;Phylogeny&#30340;&#36866;&#37197;&#22120;&#22312;SemEval-2023&#31532;12&#39033;&#20219;&#21153;&#65306;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
GMNLP at SemEval-2023 Task 12: Sentiment Analysis with Phylogeny-Based Adapters. (arXiv:2304.12979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;GMU&#22242;&#38431;&#22312;SemEval-2023&#20849;&#20139;&#20219;&#21153;AfriSenti-SemEval&#20013;&#25152;&#20351;&#29992;&#30340;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#65292;&#20351;&#29992;AfroXLMR-large&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#24341;&#20837;&#20102;&#22686;&#24378;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#22522;&#20110;Phylogeny&#30340;&#36866;&#37197;&#22120;&#35843;&#25972;&#20197;&#24471;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GMU&#23545;SemEval-2023&#20849;&#20139;&#20219;&#21153;AfriSenti-SemEval&#30340;&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#12290;&#25105;&#20204;&#21442;&#19982;&#20102;&#21333;&#35821;&#35328;&#12289;&#22810;&#35821;&#35328;&#21644;&#38646;&#26679;&#26412;&#19977;&#20010;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#21021;&#22987;&#21270;&#20026;AfroXLMR-large&#30340;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#39044;&#35757;&#32451;&#24182;&#30456;&#24212;&#24494;&#35843;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22686;&#24378;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#12290;&#38500;&#20102;&#24494;&#35843;&#22806;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#22522;&#20110;Phylogeny&#30340;&#36866;&#37197;&#22120;&#35843;&#25972;&#26469;&#21019;&#24314;&#22810;&#20010;&#27169;&#22411;&#65292;&#24182;&#23558;&#26368;&#20339;&#27169;&#22411;&#38598;&#25104;&#21040;&#26368;&#32456;&#25552;&#20132;&#20013;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#31532;5&#36712;&#36947;Amharic&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;F1&#20998;&#25968;&#65292;&#27604;&#35813;&#36712;&#36947;&#19978;&#31532;&#20108;&#26368;&#20339;&#24615;&#33021;&#31995;&#32479;&#39640;&#20986;6.2&#20010;F1&#20998;&#25968;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#21442;&#19982;&#25152;&#26377;15&#20010;&#36712;&#36947;&#30340;10&#20010;&#31995;&#32479;&#20013;&#25490;&#21517;&#31532;5&#12290;
&lt;/p&gt;
&lt;p&gt;
This report describes GMU's sentiment analysis system for the SemEval-2023 shared task AfriSenti-SemEval. We participated in all three sub-tasks: Monolingual, Multilingual, and Zero-Shot. Our approach uses models initialized with AfroXLMR-large, a pre-trained multilingual language model trained on African languages and fine-tuned correspondingly. We also introduce augmented training data along with original training data. Alongside finetuning, we perform phylogeny-based adapter tuning to create several models and ensemble the best models for the final submission. Our system achieves the best F1-score on track 5: Amharic, with 6.2 points higher F1-score than the second-best performing system on this track. Overall, our system ranks 5th among the 10 systems participating in all 15 tracks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25670;&#33073;&#26426;&#22120;&#32763;&#35793;&#20013;&#21477;&#23376;&#32423;&#33539;&#24335;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#19977;&#20010;&#38556;&#30861;&#26469;&#23454;&#29616;&#65306;&#20351;&#29992;&#36275;&#22815;&#22823;&#30340;&#26631;&#20934;Transformer&#26550;&#26500;&#12289;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#23558;&#25991;&#26723;&#32423;&#20449;&#24687;&#36716;&#21270;&#20026;&#36866;&#21512;&#35757;&#32451;&#30340;&#24418;&#24335;&#12289;&#22522;&#20110;&#33258;&#21160;&#25991;&#26723;&#20998;&#31867;&#30340;&#35780;&#20272;&#21327;&#35758;&#26469;&#26377;&#25928;&#22320;&#35782;&#21035;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#20004;&#20010;&#38750;&#24120;&#19981;&#21516;&#30340;&#25991;&#26723;&#32423;&#32763;&#35793;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#27492;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.12959</link><description>&lt;p&gt;
&#36867;&#31163;&#26426;&#22120;&#32763;&#35793;&#20013;&#21477;&#23376;&#32423;&#33539;&#24335;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Escaping the sentence-level paradigm in machine translation. (arXiv:2304.12959v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25670;&#33073;&#26426;&#22120;&#32763;&#35793;&#20013;&#21477;&#23376;&#32423;&#33539;&#24335;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#19977;&#20010;&#38556;&#30861;&#26469;&#23454;&#29616;&#65306;&#20351;&#29992;&#36275;&#22815;&#22823;&#30340;&#26631;&#20934;Transformer&#26550;&#26500;&#12289;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#23558;&#25991;&#26723;&#32423;&#20449;&#24687;&#36716;&#21270;&#20026;&#36866;&#21512;&#35757;&#32451;&#30340;&#24418;&#24335;&#12289;&#22522;&#20110;&#33258;&#21160;&#25991;&#26723;&#20998;&#31867;&#30340;&#35780;&#20272;&#21327;&#35758;&#26469;&#26377;&#25928;&#22320;&#35782;&#21035;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#20004;&#20010;&#38750;&#24120;&#19981;&#21516;&#30340;&#25991;&#26723;&#32423;&#32763;&#35793;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#27492;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#25991;&#26723;&#35821;&#22659;&#23545;&#20110;&#35299;&#20915;&#19968;&#31995;&#21015;&#32763;&#35793;&#27169;&#31946;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20107;&#23454;&#19978;&#65292;&#25991;&#26723;&#35774;&#32622;&#20960;&#20046;&#26159;&#25152;&#26377;&#32763;&#35793;&#30340;&#33258;&#28982;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#32763;&#35793;&#65288;&#21253;&#25324;&#30740;&#31350;&#21644;&#29983;&#20135;&#65289;&#22312;&#20960;&#21313;&#24180;&#21069;&#30340;&#21477;&#23376;&#32423;&#32763;&#35793;&#33539;&#24335;&#20013;&#20173;&#28982;&#20572;&#28382;&#19981;&#21069;&#65292;&#36825;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31454;&#20105;&#21387;&#21147;&#65292;&#36825;&#20123;&#27169;&#22411;&#22825;&#29983;&#23601;&#26159;&#22522;&#20110;&#25991;&#26723;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25670;&#33073;&#36825;&#31181;&#22256;&#22659;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#19977;&#20010;&#38556;&#30861;&#65306;&#25105;&#20204;&#24212;&#35813;&#20351;&#29992;&#20160;&#20040;&#26550;&#26500;&#65311;&#25105;&#20204;&#20174;&#21738;&#37324;&#33719;&#21462;&#35757;&#32451;&#23427;&#20204;&#30340;&#25991;&#26723;&#32423;&#20449;&#24687;&#65311;&#20197;&#21450;&#25105;&#20204;&#22914;&#20309;&#30693;&#36947;&#23427;&#20204;&#26159;&#21542;&#36275;&#22815;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
It is well-known that document context is vital for resolving a range of translation ambiguities, and in fact the document setting is the most natural setting for nearly all translation. It is therefore unfortunate that machine translation -- both research and production -- largely remains stuck in a decades-old sentence-level translation paradigm. It is also an increasingly glaring problem in light of competitive pressure from large language models, which are natively document-based. Much work in document-context machine translation exists, but for various reasons has been unable to catch hold. This paper suggests a path out of this rut by addressing three impediments at once: what architectures should we use? where do we get document-level information for training them? and how do we know whether they are any good? In contrast to work on specialized architectures, we show that the standard Transformer architecture is sufficient, provided it has enough capacity. Next, we address the t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#28155;&#21152;&#20102;&#21487;&#20197;&#22788;&#29702;&#21477;&#27861;&#27495;&#20041;&#30340;&#38750;&#30830;&#23450;&#24615;&#26632;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#19968;&#20010;&#38750;&#30830;&#23450;&#24615;&#19979;&#25512;&#33258;&#21160;&#26426;&#12290;</title><link>http://arxiv.org/abs/2304.12955</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#30830;&#23450;&#24615;&#26632;
&lt;/p&gt;
&lt;p&gt;
Nondeterministic Stacks in Neural Networks. (arXiv:2304.12955v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#28155;&#21152;&#20102;&#21487;&#20197;&#22788;&#29702;&#21477;&#27861;&#27495;&#20041;&#30340;&#38750;&#30830;&#23450;&#24615;&#26632;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#19968;&#20010;&#38750;&#30830;&#23450;&#24615;&#19979;&#25512;&#33258;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#35328;&#20013;&#20805;&#28385;&#20102; &#32452;&#25104;&#24615;&#21477;&#27861;&#32467;&#26500;&#65292;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#35821;&#35328;&#30340;&#35745;&#31639;&#26426;&#31995;&#32479;&#26041;&#38754;&#20570;&#20986;&#20102;&#31361;&#30772;&#24615;&#30340;&#25913;&#36827;&#65292;&#20294;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#22312;&#22788;&#29702;&#35821;&#27861;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#28155;&#21152;&#26632; &#25968;&#25454;&#32467;&#26500;&#65292;&#20174;&#35821;&#27861;&#21644;&#26632;&#20043;&#38388;&#30340;&#29702;&#35770;&#20851;&#31995;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37319;&#29992;&#30340;&#26159;&#35774;&#35745;&#29992;&#20110;&#36319;&#36394;&#19968;&#20010;&#21477;&#27861;&#20998;&#26512;&#30340;&#30830;&#23450;&#24615;&#26632;&#65292;&#32780;&#22312;&#35821;&#35328;&#20013;&#38656;&#35201;&#37319;&#29992;&#38750;&#30830;&#23450;&#24615;&#26632;&#36827;&#34892;&#35299;&#26512;&#30340;&#21477;&#27861;&#27495;&#20041;&#26497;&#20854;&#24120;&#35265;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#23558;&#38750;&#30830;&#23450;&#24615;&#26632;&#32435;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24046;&#24322;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#39640;&#25928;&#22320;&#27169;&#25311;&#20102;&#19968;&#20010;&#38750;&#30830;&#23450;&#24615;&#19979;&#25512;&#33258;&#21160;&#26426;&#65292;&#34920;&#31034;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#35745;&#31639;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human language is full of compositional syntactic structures, and although neural networks have contributed to groundbreaking improvements in computer systems that process language, widely-used neural network architectures still exhibit limitations in their ability to process syntax. To address this issue, prior work has proposed adding stack data structures to neural networks, drawing inspiration from theoretical connections between syntax and stacks. However, these methods employ deterministic stacks that are designed to track one parse at a time, whereas syntactic ambiguity, which requires a nondeterministic stack to parse, is extremely common in language. In this dissertation, we remedy this discrepancy by proposing a method of incorporating nondeterministic stacks into neural networks. We develop a differentiable data structure that efficiently simulates a nondeterministic pushdown automaton, representing an exponential number of computations with a dynamic programming algorithm. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30001;&#19981;&#21516;&#35821;&#35328;&#30340;7&#20010;&#35821;&#20041;&#20851;&#31995;&#23450;&#20041;&#30340;&#35821;&#20041;&#32593;&#32476;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#20041;&#32593;&#32476;&#20855;&#26377;&#26222;&#36941;&#30340;&#22522;&#26412;&#29305;&#24615;&#65306;&#31232;&#30095;&#12289;&#39640;&#24230;&#32858;&#38598;&#21644;&#33258;&#25105;&#32452;&#32455;&#21270;&#65292;&#24182;&#21576;&#29616;&#20986;&#24130;&#24459;&#24230;&#25968;&#20998;&#24067;&#12290;&#19968;&#20123;&#32593;&#32476;&#26174;&#31034;&#20986;&#35821;&#35328;&#29305;&#23450;&#30340;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#21463;&#35821;&#27861;&#35268;&#21017;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#26469;&#33258;&#39640;&#24230;&#23624;&#25240;&#35821;&#35328;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.12940</link><description>&lt;p&gt;
&#35821;&#20041;&#32593;&#32476;&#30340;&#25299;&#25169;&#24615;&#36136;&#21644;&#32452;&#32455;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
Topological properties and organizing principles of semantic networks. (arXiv:2304.12940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30001;&#19981;&#21516;&#35821;&#35328;&#30340;7&#20010;&#35821;&#20041;&#20851;&#31995;&#23450;&#20041;&#30340;&#35821;&#20041;&#32593;&#32476;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#20041;&#32593;&#32476;&#20855;&#26377;&#26222;&#36941;&#30340;&#22522;&#26412;&#29305;&#24615;&#65306;&#31232;&#30095;&#12289;&#39640;&#24230;&#32858;&#38598;&#21644;&#33258;&#25105;&#32452;&#32455;&#21270;&#65292;&#24182;&#21576;&#29616;&#20986;&#24130;&#24459;&#24230;&#25968;&#20998;&#24067;&#12290;&#19968;&#20123;&#32593;&#32476;&#26174;&#31034;&#20986;&#35821;&#35328;&#29305;&#23450;&#30340;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#21463;&#35821;&#27861;&#35268;&#21017;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#26469;&#33258;&#39640;&#24230;&#23624;&#25240;&#35821;&#35328;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#30340;&#22686;&#21152;&#65292;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25104;&#20026;&#35745;&#31639;&#26426;&#31639;&#27861;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#31243;&#24207;&#20381;&#38752;&#35821;&#20041;&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35774;&#35745;NLP&#31639;&#27861;&#26102;&#24517;&#39035;&#32771;&#34385;&#35821;&#20041;&#32593;&#32476;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#20294;&#23427;&#20204;&#30340;&#32467;&#26500;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;11&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;7&#20010;&#35821;&#20041;&#20851;&#31995;&#23450;&#20041;&#30340;ConceptNet&#35821;&#20041;&#32593;&#32476;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#20041;&#32593;&#32476;&#20855;&#26377;&#26222;&#36941;&#30340;&#22522;&#26412;&#29305;&#24615;&#65306;&#23427;&#20204;&#26159;&#31232;&#30095;&#30340;&#12289;&#39640;&#24230;&#38598;&#32858;&#30340;&#65292;&#24182;&#21576;&#29616;&#20986;&#24130;&#24459;&#24230;&#25968;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#32593;&#32476;&#37117;&#26159;&#33258;&#25105;&#32452;&#32455;&#30340;&#12290;&#19968;&#20123;&#32593;&#32476;&#26174;&#31034;&#20986;&#35821;&#35328;&#29305;&#23450;&#30340;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#21463;&#35821;&#27861;&#35268;&#21017;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#39640;&#24230;&#23624;&#25240;&#35821;&#35328;(&#22914;&#25289;&#19969;&#35821;&#12289;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;)&#30340;&#32593;&#32476;&#22312;&#24230;&#25968;&#20998;&#24067;&#26041;&#38754;&#26377;&#23792;&#20540;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting natural language is an increasingly important task in computer algorithms due to the growing availability of unstructured textual data. Natural Language Processing (NLP) applications rely on semantic networks for structured knowledge representation. The fundamental properties of semantic networks must be taken into account when designing NLP algorithms, yet they remain to be structurally investigated. We study the properties of semantic networks from ConceptNet, defined by 7 semantic relations from 11 different languages. We find that semantic networks have universal basic properties: they are sparse, highly clustered, and exhibit power-law degree distributions. Our findings show that the majority of the considered networks are scale-free. Some networks exhibit language-specific properties determined by grammatical rules, for example networks from highly inflected languages, such as e.g. Latin, German, French and Spanish, show peaks in the degree distribution that deviate 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#26377;&#25928;&#20943;&#36731;&#26032;&#38395;&#21644;&#35777;&#25454;&#20869;&#23481;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.12888</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#37325;&#23545;&#25239;&#21435;&#20559;&#32622;&#23454;&#29616;&#30340;&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution Evidence-aware Fake News Detection via Dual Adversarial Debiasing. (arXiv:2304.12888v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#26377;&#25928;&#20943;&#36731;&#26032;&#38395;&#21644;&#35777;&#25454;&#20869;&#23481;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#26088;&#22312;&#23545;&#26032;&#38395;&#21644;&#22522;&#20110;&#26032;&#38395;&#20869;&#23481;&#26816;&#32034;&#30340;&#35777;&#25454;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#26597;&#25214;&#32479;&#19968;&#24615;&#25110;&#19981;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#35777;&#25454;&#24863;&#30693;&#26816;&#27979;&#27169;&#22411;&#20250;&#21463;&#21040;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#21363;&#26032;&#38395;/&#35777;&#25454;&#20869;&#23481;&#21644;&#30495;&#23454;/&#20551;&#26032;&#38395;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#24456;&#38590;&#25512;&#24191;&#21040;&#36234;&#30028;&#24773;&#20917;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;DAL&#20013;&#21152;&#20837;&#20102;&#26032;&#38395;&#26041;&#38754;&#21644;&#35777;&#25454;&#26041;&#38754;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#23427;&#20204;&#30340;&#30446;&#26631;&#37117;&#26159;&#30495;&#20551;&#26032;&#38395;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;DAL&#20250;&#36870;&#21521;&#20248;&#21270;&#26032;&#38395;&#26041;&#38754;&#21644;&#35777;&#25454;&#26041;&#38754;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#20197;&#20943;&#36731;&#26032;&#38395;&#21644;&#35777;&#25454;&#20869;&#23481;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;DAL&#36824;&#20248;&#21270;&#20027;&#35201;&#30340;&#20551;&#26032;&#38395;&#39044;&#27979;&#22120;&#65292;&#35753;&#26032;&#38395;-&#35777;&#25454;&#20132;&#20114;&#27169;&#22359;&#33021;&#22815;&#34987;&#23398;&#20064;&#12290;&#36825;&#20010;&#36807;&#31243;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#25945;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#26032;&#38395;&#35777;&#25454;&#25512;&#29702;&#65292;&#24182;&#23558;&#26816;&#27979;&#20551;&#26032;&#38395;&#30340;&#36127;&#38754;&#24433;&#21709;&#38477;&#33267;&#26368;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence-aware fake news detection aims to conduct reasoning between news and evidence, which is retrieved based on news content, to find uniformity or inconsistency. However, we find evidence-aware detection models suffer from biases, i.e., spurious correlations between news/evidence contents and true/fake news labels, and are hard to be generalized to Out-Of-Distribution (OOD) situations. To deal with this, we propose a novel Dual Adversarial Learning (DAL) approach. We incorporate news-aspect and evidence-aspect debiasing discriminators, whose targets are both true/fake news labels, in DAL. Then, DAL reversely optimizes news-aspect and evidence-aspect debiasing discriminators to mitigate the impact of news and evidence content biases. At the same time, DAL also optimizes the main fake news predictor, so that the news-evidence interaction module can be learned. This process allows us to teach evidence-aware fake news detection models to better conduct news-evidence reasoning, and min
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24212;&#29992;transformer&#27169;&#22411;&#21644;&#25968;&#25454;&#22686;&#24378;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20174;&#32780;&#22686;&#24378;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12847</link><description>&lt;p&gt;
NLP-LTU&#22312;SemEval-2023&#20219;&#21153;10&#20013;&#30340;&#24212;&#29992;: &#25968;&#25454;&#22686;&#24378;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23545;&#38750;&#24179;&#34913;&#25968;&#25454;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
NLP-LTU at SemEval-2023 Task 10: The Impact of Data Augmentation and Semi-Supervised Learning Techniques on Text Classification Performance on an Imbalanced Dataset. (arXiv:2304.12847v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24212;&#29992;transformer&#27169;&#22411;&#21644;&#25968;&#25454;&#22686;&#24378;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20174;&#32780;&#22686;&#24378;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#26816;&#27979;&#21644;&#20998;&#31867;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#30340;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#65292;&#20197;&#24212;&#23545;SemEval23&#20219;&#21153;10&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#24494;&#35843;&#21518;&#30340;transformer&#27169;&#22411;&#65288;BERTweet&#12289;RoBERTa&#21644;DeBERTa&#65289;&#30340;&#38598;&#25104;&#12290;&#20026;&#20102;&#32531;&#35299;&#19982;&#31867;&#21035;&#19981;&#24179;&#34913;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#25968;&#25454;&#22686;&#24378;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#25968;&#25454;&#22686;&#24378;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22238;&#35793;&#65292;&#19981;&#26159;&#22312;&#25152;&#26377;&#31867;&#21035;&#19978;&#65292;&#23601;&#26159;&#21482;&#22312;&#27424;&#34920;&#31034;&#30340;&#31867;&#21035;&#19978;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#20102;&#36825;&#20123;&#31574;&#30053;&#23545;&#31649;&#36947;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#26377;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#20869;&#25968;&#25454;&#21487;&#29992;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a methodology for task 10 of SemEval23, focusing on detecting and classifying online sexism in social media posts. The task is tackling a serious issue, as detecting harmful content on social media platforms is crucial for mitigating the harm of these posts on users. Our solution for this task is based on an ensemble of fine-tuned transformer-based models (BERTweet, RoBERTa, and DeBERTa). To alleviate problems related to class imbalance, and to improve the generalization capability of our model, we also experiment with data augmentation and semi-supervised learning. In particular, for data augmentation, we use back-translation, either on all classes, or on the underrepresented classes only. We analyze the impact of these strategies on the overall performance of the pipeline through extensive experiments. while for semi-supervised learning, we found that with a substantial amount of unlabelled, in-domain data available, semi-supervised learning can enhance the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20844;&#27665;&#31185;&#23398;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#34920;&#26126;&#36825;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#24182;&#21560;&#24341;&#31215;&#26497;&#30340;&#24535;&#24895;&#32773;&#65292;&#20294;&#38656;&#35201;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#12289;&#38271;&#26399;&#21442;&#19982;&#21644;&#27861;&#24459;&#21644;&#20262;&#29702;&#38382;&#39064;&#31561;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.12836</link><description>&lt;p&gt;
&#19968;&#39033;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20844;&#27665;&#31185;&#23398;&#39033;&#30446;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Lessons Learned from a Citizen Science Project for Natural Language Processing. (arXiv:2304.12836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20844;&#27665;&#31185;&#23398;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#34920;&#26126;&#36825;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#24182;&#21560;&#24341;&#31215;&#26497;&#30340;&#24535;&#24895;&#32773;&#65292;&#20294;&#38656;&#35201;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#12289;&#38271;&#26399;&#21442;&#19982;&#21644;&#27861;&#24459;&#21644;&#20262;&#29702;&#38382;&#39064;&#31561;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20351;&#29992;&#24102;&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#24456;&#38590;&#33719;&#24471;&#65292;&#24182;&#19988;&#25193;&#23637;&#27880;&#37322;&#39033;&#30446;&#20063;&#24456;&#22256;&#38590;&#65292;&#22240;&#27492;&#27880;&#37322;&#20219;&#21153;&#24120;&#24120;&#34987;&#22806;&#21253;&#32473;&#26377;&#20607;&#30340;&#20247;&#21253;&#24037;&#20154;&#12290;&#20844;&#27665;&#31185;&#23398;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#34987;&#24320;&#21457;&#30340;&#20247;&#21253;&#26367;&#20195;&#26041;&#26696;&#12290;&#20026;&#20102;&#35843;&#26597;&#20844;&#27665;&#31185;&#23398;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#36866;&#29992;&#20110;&#27492;&#39046;&#22495;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#37325;&#26032;&#27880;&#37322;&#29616;&#26377;&#20247;&#21253;&#25968;&#25454;&#38598;&#30340;&#37096;&#20998;&#20869;&#23481;&#65292;&#19982;&#19981;&#21516;&#24535;&#24895;&#32773;&#32676;&#20307;&#21442;&#19982;&#20844;&#27665;&#31185;&#23398;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#24182;&#21560;&#24341;&#31215;&#26497;&#30340;&#24535;&#24895;&#32773;&#65292;&#20294;&#20063;&#38656;&#35201;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#12289;&#38271;&#26399;&#21442;&#19982;&#21644;&#27861;&#24459;&#21644;&#20262;&#29702;&#38382;&#39064;&#31561;&#22240;&#32032;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#25351;&#21335;&#65292;&#24182;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#24110;&#21161;&#26410;&#26469;&#36827;&#34892;&#20844;&#27665;&#31185;&#23398;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many Natural Language Processing (NLP) systems use annotated corpora for training and evaluation. However, labeled data is often costly to obtain and scaling annotation projects is difficult, which is why annotation tasks are often outsourced to paid crowdworkers. Citizen Science is an alternative to crowdsourcing that is relatively unexplored in the context of NLP. To investigate whether and how well Citizen Science can be applied in this setting, we conduct an exploratory study into engaging different groups of volunteers in Citizen Science for NLP by re-annotating parts of a pre-existing crowdsourced dataset. Our results show that this can yield high-quality annotations and attract motivated volunteers, but also requires considering factors such as scalability, participation over time, and legal and ethical issues. We summarize lessons learned in the form of guidelines and provide our code and data to aid future work on Citizen Science.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#27010;&#24565; troenpy &#26469;&#37327;&#21270;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#25991;&#26723;&#20998;&#31867;&#21644;&#24207;&#21015;&#25968;&#25454;&#26435;&#37325;&#26041;&#26696;&#65292;&#24182;&#23450;&#20041;&#20102;&#37327;&#23376; troenpy &#37327;&#21270;&#37327;&#23376;&#31995;&#32479;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12833</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#30830;&#23450;&#24615;&#20449;&#24687;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A New Information Theory of Certainty for Machine Learning. (arXiv:2304.12833v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#27010;&#24565; troenpy &#26469;&#37327;&#21270;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#25991;&#26723;&#20998;&#31867;&#21644;&#24207;&#21015;&#25968;&#25454;&#26435;&#37325;&#26041;&#26696;&#65292;&#24182;&#23450;&#20041;&#20102;&#37327;&#23376; troenpy &#37327;&#21270;&#37327;&#23376;&#31995;&#32479;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20811;&#21171;&#24503;&#183;&#39321;&#20892;&#25552;&#20986;&#20102;&#29109;&#30340;&#27010;&#24565;&#26469;&#37327;&#21270;&#36890;&#20449;&#32534;&#30721;&#29702;&#35770;&#20013;&#38543;&#26426;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29109;&#30340;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#29305;&#24615;&#20063;&#38480;&#21046;&#20102;&#20854;&#22312;&#25968;&#23398;&#24314;&#27169;&#20013;&#30340;&#30452;&#25509;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565; troenpy&#65292;&#20316;&#20026;&#29109;&#30340;&#35268;&#33539;&#23545;&#20598;&#65292;&#26469;&#37327;&#21270;&#24213;&#23618;&#20998;&#24067;&#30340;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#24212;&#29992;&#12290;&#31532;&#19968;&#20010;&#26159;&#29992;&#20110;&#20256;&#32479;&#30340;&#25991;&#26723;&#20998;&#31867;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110; troenpy &#26435;&#37325;&#26041;&#26696;&#26469;&#21033;&#29992;&#25991;&#26723;&#20998;&#31867;&#26631;&#31614;&#12290;&#31532;&#20108;&#20010;&#26159;&#38024;&#23545;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#25105; troenpy &#26435;&#37325;&#26041;&#26696;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#21253;&#21547;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#23454;&#29616;&#26174;&#33879;&#30340;&#22256;&#24785;&#24230;&#38477;&#20302;&#12290;&#25105;&#20204;&#36824;&#23450;&#20041;&#20102;&#37327;&#23376; troenpy &#20316;&#20026; Von Neumann &#29109;&#30340;&#23545;&#20598;&#65292;&#20197;&#37327;&#21270;&#37327;&#23376;&#31995;&#32479;&#30340;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Claude Shannon coined entropy to quantify the uncertainty of a random distribution for communication coding theory. We observe that the uncertainty nature of entropy also limits its direct usage in mathematical modeling. Therefore we propose a new concept troenpy,as the canonical dual of entropy, to quantify the certainty of the underlying distribution. We demonstrate two applications in machine learning. The first is for the classical document classification, we develop a troenpy based weighting scheme to leverage the document class label. The second is a self-troenpy weighting scheme for sequential data and show that it can be easily included in neural network based language models and achieve dramatic perplexity reduction. We also define quantum troenpy as the dual of the Von Neumann entropy to quantify the certainty of quantum systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23384;&#22312;&#24615;&#21035;&#21270;&#35821;&#35328;&#26102;&#65292;NLP&#35821;&#22659;&#20013;&#20063;&#23384;&#22312;&#30528;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#30007;&#24615;&#20559;&#35265;&#12290;&#35843;&#26597;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#28085;&#30422;&#20102;&#24615;&#21035;&#21270;&#35821;&#35328;&#19982;&#35821;&#35328;&#20043;&#38388;&#27495;&#20041;&#20851;&#31995;&#30340;&#26032;&#23383;&#20856;&#8220;Ava&#8221;&#12290;</title><link>http://arxiv.org/abs/2304.12810</link><description>&lt;p&gt;
&#36229;&#36234;&#8220;&#30007;&#24615;&#20934;&#21017;&#8221;&#65306;NLP&#35821;&#22659;&#20013;&#30340;&#38544;&#24615;&#30007;&#24615;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Transcending the "Male Code": Implicit Masculine Biases in NLP Contexts. (arXiv:2304.12810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12810
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23384;&#22312;&#24615;&#21035;&#21270;&#35821;&#35328;&#26102;&#65292;NLP&#35821;&#22659;&#20013;&#20063;&#23384;&#22312;&#30528;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#30007;&#24615;&#20559;&#35265;&#12290;&#35843;&#26597;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#28085;&#30422;&#20102;&#24615;&#21035;&#21270;&#35821;&#35328;&#19982;&#35821;&#35328;&#20043;&#38388;&#27495;&#20041;&#20851;&#31995;&#30340;&#26032;&#23383;&#20856;&#8220;Ava&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#34394;&#25311;&#21161;&#25163;&#65288;VAs&#65289;&#30340;&#24615;&#21035;&#20559;&#24046;&#38382;&#39064;&#65292;&#25209;&#21028;&#24615;&#23398;&#35828;&#24050;&#32463;&#25552;&#39640;&#20102;&#20154;&#20204;&#30340;&#27880;&#24847;&#12290;&#22823;&#37096;&#20998;&#30740;&#31350;&#38598;&#20013;&#22312;&#35821;&#35328;&#20013;&#30340;&#26174;&#24615;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#22899;&#24615;&#12289;&#22899;&#23401;&#12289;&#22899;&#24615;&#35748;&#21516;&#20154;&#32676;&#21644;&#24615;&#21035;&#37239;&#20799;&#30340;&#27495;&#35270;&#65292;&#20197;&#21450;&#36890;&#36807;&#35789;&#21521;&#37327;&#23884;&#20837;&#30340;&#38544;&#24615;&#20851;&#32852;&#65307;&#32780;&#23545;&#20110;&#30007;&#24615;&#21644;&#27602;&#24615;&#30007;&#24615;&#65292;&#24615;&#21035;&#21644;&#24615;&#21035;&#20108;&#20803;&#20998;&#31867;&#30340;&#28151;&#20026;&#19968;&#35848;&#65292;&#24456;&#23569;&#26377;&#22522;&#20110;&#30007;&#24615;&#21644;&#30007;&#24615;&#27668;&#27010;&#30340;&#26377;&#38480;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#24517;&#39035;&#36136;&#35810;&#22914;&#20309;&#23558;&#30007;&#24615;&#27668;&#27010;&#8220;&#32534;&#30721;&#8221;&#21040;&#35821;&#35328;&#20013;&#21450;&#20854;&#23558;&#8220;&#30007;&#24615;&#8221;&#20316;&#20026;&#35821;&#35328;&#40664;&#35748;&#20540;&#30340;&#20551;&#35774;&#65306;&#38544;&#24615;&#30007;&#24615;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20004;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#23384;&#22312;&#24615;&#21035;&#21270;&#35821;&#35328;&#26102;&#65292;&#24615;&#21035;&#20559;&#35265;&#23588;&#20854;&#26159;&#30007;&#24615;&#20559;&#35265;&#20063;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20559;&#35265;&#19982;NLP&#19978;&#19979;&#25991;&#30340;&#20851;&#31995;&#32454;&#24494;&#19988;&#30456;&#20851;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;AVA&#30340;&#26032;&#23383;&#20856;&#65292;&#28085;&#30422;&#20102;&#24615;&#21035;&#21270;&#35821;&#35328;&#19982;&#35821;&#35328;&#20043;&#38388;&#30340;&#27495;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical scholarship has elevated the problem of gender bias in data sets used to train virtual assistants (VAs). Most work has focused on explicit biases in language, especially against women, girls, femme-identifying people, and genderqueer folk; implicit associations through word embeddings; and limited models of gender and masculinities, especially toxic masculinities, conflation of sex and gender, and a sex/gender binary framing of the masculine as diametric to the feminine. Yet, we must also interrogate how masculinities are "coded" into language and the assumption of "male" as the linguistic default: implicit masculine biases. To this end, we examined two natural language processing (NLP) data sets. We found that when gendered language was present, so were gender biases and especially masculine biases. Moreover, these biases related in nuanced ways to the NLP context. We offer a new dictionary called AVA that covers ambiguous associations between gendered language and the langua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22768;&#38899;&#26159;&#21542;&#21487;&#20197;&#20855;&#22791;&#21487;&#29233;&#24615;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#21487;&#29233;&#30340;&#35821;&#38899;&#29305;&#36136;&#65292;&#21363;&#21487;&#29233;&#30340;&#22768;&#38899;&#23398;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#24180;&#40836;&#27573;&#21644;&#19981;&#21516;&#29305;&#24449;&#30340;&#35821;&#38899;&#24863;&#21463;&#30740;&#31350;&#65292;&#21457;&#29616;&#21487;&#29233;&#24615;&#19982;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#27969;&#21033;&#24230;&#21644;&#20154;&#24037;&#24615;&#31561;&#26041;&#38754;&#26377;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29233;&#21971;&#38899;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#36827;&#34892;&#22768;&#38899;&#36136;&#37327;&#12289;&#35748;&#30693;&#35780;&#20272;&#12289;&#34892;&#20026;&#21453;&#24212;&#21644;&#24773;&#24863;&#25253;&#21578;&#30340;&#30740;&#31350;&#26469;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.12809</link><description>&lt;p&gt;
&#35821;&#38899;&#21161;&#25163;&#21487;&#20197;&#21548;&#36215;&#26469;&#21487;&#29233;&#21527;&#65311;&#36808;&#21521;&#21487;&#29233;&#21971;&#38899;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Can Voice Assistants Sound Cute? Towards a Model of Kawaii Vocalics. (arXiv:2304.12809v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22768;&#38899;&#26159;&#21542;&#21487;&#20197;&#20855;&#22791;&#21487;&#29233;&#24615;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#21487;&#29233;&#30340;&#35821;&#38899;&#29305;&#36136;&#65292;&#21363;&#21487;&#29233;&#30340;&#22768;&#38899;&#23398;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#24180;&#40836;&#27573;&#21644;&#19981;&#21516;&#29305;&#24449;&#30340;&#35821;&#38899;&#24863;&#21463;&#30740;&#31350;&#65292;&#21457;&#29616;&#21487;&#29233;&#24615;&#19982;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#27969;&#21033;&#24230;&#21644;&#20154;&#24037;&#24615;&#31561;&#26041;&#38754;&#26377;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29233;&#21971;&#38899;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#36827;&#34892;&#22768;&#38899;&#36136;&#37327;&#12289;&#35748;&#30693;&#35780;&#20272;&#12289;&#34892;&#20026;&#21453;&#24212;&#21644;&#24773;&#24863;&#25253;&#21578;&#30340;&#30740;&#31350;&#26469;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#26412;&#30340;&#8220;&#21487;&#29233;&#8221;&#27010;&#24565;&#25110;&#34920;&#36798;&#21487;&#29233;&#12289;&#33030;&#24369;&#21644;/&#25110;&#39749;&#21147;&#30340;&#26041;&#24335;&#26159;&#19968;&#31181;&#20840;&#29699;&#25991;&#21270;&#36755;&#20986;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#21644;&#34394;&#25311;&#35282;&#33394;&#30340;&#35270;&#35273;&#22806;&#35266;&#12289;&#38750;&#35821;&#35328;&#34892;&#20026;&#21644;&#22768;&#38899;&#20013;&#25506;&#32034;&#21487;&#29233;&#24615;&#20316;&#20026;&#35774;&#35745;&#29305;&#24449;&#21644;&#29992;&#25143;&#20307;&#39564;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#21021;&#27493;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22768;&#38899;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#25506;&#32034;&#35821;&#38899;&#21161;&#25163;&#35821;&#38899;&#30340;&#22768;&#38899;&#29305;&#36136;&#65292;&#21363;&#21487;&#29233;&#30340;&#22768;&#38899;&#23398;&#65292;&#20855;&#22791;&#21487;&#29233;&#24615;&#12290;&#26681;&#25454;&#19968;&#20010;&#21253;&#21547;&#24180;&#40836;&#30340;&#21487;&#29233;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#24180;&#36731;&#21644;&#24180;&#32769;&#30340;&#26085;&#35821;&#30005;&#33041;&#35821;&#38899;&#30340;&#21487;&#29233;&#24230;&#36827;&#34892;&#20102;&#29992;&#25143;&#24863;&#30693;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#29233;&#24615;&#19982;&#24615;&#21035;&#21644;&#24180;&#40836;&#30340;&#24863;&#30693;&#30456;&#20132;&#65292;&#21363;&#24615;&#21035;&#27169;&#31946;&#21644;&#22899;&#23401;&#27668;&#36136;&#65292;&#20197;&#21450;VA&#30340;&#29305;&#24449;&#65292;&#21363;&#27969;&#21033;&#24230;&#21644;&#20154;&#24037;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29233;&#21971;&#38899;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#30740;&#31350;&#22768;&#38899;&#36136;&#37327;&#12289;&#35748;&#30693;&#35780;&#20272;&#12289;&#34892;&#20026;&#21453;&#24212;&#21644;&#24773;&#24863;&#25253;&#21578;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Japanese notion of "kawaii" or expressions of cuteness, vulnerability, and/or charm is a global cultural export. Work has explored kawaii-ness as a design feature and factor of user experience in the visual appearance, nonverbal behaviour, and sound of robots and virtual characters. In this initial work, we consider whether voices can be kawaii by exploring the vocal qualities of voice assistant speech, i.e., kawaii vocalics. Drawing from an age-inclusive model of kawaii, we ran a user perceptions study on the kawaii-ness of younger- and older-sounding Japanese computer voices. We found that kawaii-ness intersected with perceptions of gender and age, i.e., gender ambiguous and girlish, as well as VA features, i.e., fluency and artificiality. We propose an initial model of kawaii vocalics to be validated through the identification and study of vocal qualities, cognitive appraisals, behavioural responses, and affective reports.
&lt;/p&gt;</description></item><item><title>S4&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#19982;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#27604;&#23384;&#22312;&#22235;&#20010;BLEU&#20998;&#25968;&#28857;&#30340;&#24046;&#36317;&#65292;&#38656;&#35201;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#24357;&#34917;&#20854;&#26080;&#27861;&#22312;&#21333;&#20010;&#38544;&#34255;&#29366;&#24577;&#20013;&#24635;&#32467;&#23436;&#25972;&#30340;&#28304;&#21477;&#23376;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2304.12776</link><description>&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#19981;&#22815;&#29992;&#65306;&#26426;&#22120;&#32763;&#35793;&#38656;&#35201;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
State Spaces Aren't Enough: Machine Translation Needs Attention. (arXiv:2304.12776v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12776
&lt;/p&gt;
&lt;p&gt;
S4&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#19982;&#21464;&#21387;&#22120;&#27169;&#22411;&#30456;&#27604;&#23384;&#22312;&#22235;&#20010;BLEU&#20998;&#25968;&#28857;&#30340;&#24046;&#36317;&#65292;&#38656;&#35201;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#24357;&#34917;&#20854;&#26080;&#27861;&#22312;&#21333;&#20010;&#38544;&#34255;&#29366;&#24577;&#20013;&#24635;&#32467;&#23436;&#25972;&#30340;&#28304;&#21477;&#23376;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#30340;&#32467;&#26500;&#29366;&#24577;&#31354;&#38388;&#65288;S4&#65289;&#26159;&#19968;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#35270;&#35273;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#38899;&#39057;&#12290;&#30001;&#20110;&#23427;&#30340;&#25968;&#23398;&#20844;&#24335;&#65292;&#23427;&#23558;&#20854;&#36755;&#20837;&#21387;&#32553;&#20026;&#19968;&#20010;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#33021;&#22815;&#25429;&#33719;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38656;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;S4&#24212;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#65292;&#24182;&#22312;WMT'14&#21644;WMT'16&#19978;&#35780;&#20272;&#20102;&#20960;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#20307;&#12290;&#19982;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#25104;&#21151;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;S4&#22312;BLEU&#28857;&#25968;&#19978;&#33853;&#21518;&#20110;&#21464;&#21387;&#22120;&#32422;4&#20010;&#28857;&#65292;&#24182;&#19988;&#20196;&#20154;&#24863;&#21040;&#22256;&#24785;&#30340;&#26159;&#65292;&#23427;&#22312;&#22788;&#29702;&#38271;&#21477;&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24046;&#36317;&#26159;&#30001;&#20110;S4&#26080;&#27861;&#22312;&#21333;&#20010;&#38544;&#34255;&#29366;&#24577;&#20013;&#24635;&#32467;&#23436;&#25972;&#30340;&#28304;&#21477;&#23376;&#25152;&#33268;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured State Spaces for Sequences (S4) is a recently proposed sequence model with successful applications in various tasks, e.g. vision, language modeling, and audio. Thanks to its mathematical formulation, it compresses its input to a single hidden state, and is able to capture long range dependencies while avoiding the need for an attention mechanism. In this work, we apply S4 to Machine Translation (MT), and evaluate several encoder-decoder variants on WMT'14 and WMT'16. In contrast with the success in language modeling, we find that S4 lags behind the Transformer by approximately 4 BLEU points, and that it counter-intuitively struggles with long sentences. Finally, we show that this gap is caused by S4's inability to summarize the full source sentence in a single hidden state, and show that we can close the gap by introducing an attention mechanism.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;PCL&#65292;&#36890;&#36807;&#20419;&#36827;&#27169;&#22411;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#31283;&#23450;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;PLMs&#22312;&#24212;&#23545;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12764</link><description>&lt;p&gt;
&#24102;&#25200;&#21160;&#19968;&#33268;&#24615;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation with Perturbation Consistency Learning. (arXiv:2304.12764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;PCL&#65292;&#36890;&#36807;&#20419;&#36827;&#27169;&#22411;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#31283;&#23450;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;PLMs&#22312;&#24212;&#23545;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#22312;&#24212;&#23545;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#23548;&#33268;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30495;&#23454;&#27979;&#35797;&#22330;&#26223;&#20013;&#22833;&#36133;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;(TTA)&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#21363;&#22312;&#27979;&#35797;&#26102;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#20197;&#36866;&#24212;&#27979;&#35797;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;TTA&#26041;&#27861;&#20381;&#36182;&#20110;&#32463;&#36807;&#33391;&#22909;&#35774;&#35745;&#30340;&#36741;&#21161;&#20219;&#21153;&#25110;&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#33258;&#35757;&#32451;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#25552;&#21319;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#24182;&#27809;&#26377;&#36798;&#21040;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#36825;&#31181;&#22256;&#22659;&#65292;&#26412;&#25991;&#36873;&#21462;&#20102;&#20004;&#31181;&#20856;&#22411;&#30340;TTA&#26041;&#27861;(Tent&#21644;OIL)&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#21457;&#29616;&#31283;&#23450;&#30340;&#39044;&#27979;&#26159;&#23454;&#29616;&#33391;&#22909;&#24179;&#34913;&#30340;&#20851;&#38190;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#25200;&#21160;&#19968;&#33268;&#24615;&#23398;&#20064;(PCL)&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#26679;&#26412;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#12290;&#22312;&#23545;&#25239;&#25915;&#20987;&#21644;&#36328;&#22495;&#24773;&#20917;&#19979;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, pre-trained language models (PLMs) do not cope well with the distribution shift problem, resulting in models trained on the training set failing in real test scenarios. To address this problem, the test-time adaptation (TTA) shows great potential, which updates model parameters to suit the test data at the testing time. Existing TTA methods rely on well-designed auxiliary tasks or self-training strategies based on pseudo-label. However, these methods do not achieve good trade-offs regarding performance gains and computational costs. To obtain some insights into such a dilemma, we take two representative TTA methods, i.e., Tent and OIL, for exploration and find that stable prediction is the key to achieving a good balance. Accordingly, in this paper, we propose perturbation consistency learning (PCL), a simple test-time adaptation method to promote the model to make stable predictions for samples with distribution shifts. Extensive experiments on adversarial robustness and cr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;BERT&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#23618;&#27425;&#25429;&#25417;&#21040;&#30340;&#34920;&#31034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38901;&#24459;&#26159;BERT&#23398;&#20064;&#21040;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19968;&#37096;&#20998;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#20013;&#38388;&#23618;&#12290;</title><link>http://arxiv.org/abs/2304.12706</link><description>&lt;p&gt;
BERT&#23398;&#20064;&#21040;&#20102;&#20160;&#20040;&#20851;&#20110;&#38901;&#24459;&#30340;&#30693;&#35782;?
&lt;/p&gt;
&lt;p&gt;
What does BERT learn about prosody?. (arXiv:2304.12706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;BERT&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#23618;&#27425;&#25429;&#25417;&#21040;&#30340;&#34920;&#31034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38901;&#24459;&#26159;BERT&#23398;&#20064;&#21040;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19968;&#37096;&#20998;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#20013;&#38388;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#65292;&#21462;&#24471;&#20102;&#35768;&#22810;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#21253;&#25324;&#38901;&#24459;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#27169;&#22411;&#35774;&#35745;&#24182;&#19981;&#23450;&#20041;&#39044;&#20808;&#30830;&#23450;&#30340;&#35821;&#35328;&#30446;&#26631;&#65292;&#32780;&#26159;&#26088;&#22312;&#23398;&#20064;&#35821;&#35328;&#30340;&#24191;&#20041;&#34920;&#31034;&#65292;&#22240;&#27492;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#38544;&#24335;&#25429;&#33719;&#30340;&#34920;&#31034;&#23545;&#20110;&#24357;&#21512;&#21487;&#35299;&#37322;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#38750;&#24120;&#37325;&#35201;&#12290;&#20960;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27169;&#22411;&#25429;&#33719;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#23427;&#20204;&#34920;&#31034;&#33021;&#21147;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#23578;&#26410;&#25506;&#32034;&#38901;&#24459;&#26159;&#21542;&#26159;&#27169;&#22411;&#23398;&#20064;&#30340;&#35821;&#35328;&#32467;&#26500;&#20449;&#24687;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;BERT&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#23618;&#27425;&#25429;&#25417;&#21040;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;&#20851;&#38901;&#24459;&#31361;&#20986;&#30340;&#20449;&#24687;&#36328;&#36234;&#35768;&#22810;&#23618;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#20013;&#38388;&#23618;&#65292;&#36825;&#34920;&#26126;&#38901;&#24459;&#30830;&#23454;&#26159;BERT&#23398;&#20064;&#21040;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have become nearly ubiquitous in natural language processing applications achieving state-of-the-art results in many tasks including prosody. As the model design does not define predetermined linguistic targets during training but rather aims at learning generalized representations of the language, analyzing and interpreting the representations that models implicitly capture is important in bridging the gap between interpretability and model performance. Several studies have explored the linguistic information that models capture providing some insights on their representational capacity. However, the current studies have not explored whether prosody is part of the structural information of the language that models learn. In this work, we perform a series of experiments on BERT probing the representations captured at different layers. Our results show that information about prosodic prominence spans across many layers but is mostly focused in middle layers suggesting th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26368;&#22823;&#32534;&#30721;&#36895;&#29575;&#38477;&#20302;&#19979;&#30340;&#21477;&#23376;&#34920;&#31034;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;Sentence-BERT&#20013;&#21152;&#20837;&#19968;&#20010;&#39069;&#22806;&#30340;&#23398;&#20064;&#25237;&#24433;&#23618;&#65292;&#24182;&#35777;&#26126;&#20854;&#21487;&#20197;&#22312;&#35821;&#20041;&#30456;&#20851;&#20219;&#21153;&#20013;&#33719;&#24471;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12674</link><description>&lt;p&gt;
&#26368;&#22823;&#32534;&#30721;&#36895;&#29575;&#38477;&#20302;&#19979;&#30340;&#21477;&#23376;&#34920;&#31034;&#21387;&#32553;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Compressing Sentence Representation with maximum Coding Rate Reduction. (arXiv:2304.12674v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12674
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26368;&#22823;&#32534;&#30721;&#36895;&#29575;&#38477;&#20302;&#19979;&#30340;&#21477;&#23376;&#34920;&#31034;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;Sentence-BERT&#20013;&#21152;&#20837;&#19968;&#20010;&#39069;&#22806;&#30340;&#23398;&#20064;&#25237;&#24433;&#23618;&#65292;&#24182;&#35777;&#26126;&#20854;&#21487;&#20197;&#22312;&#35821;&#20041;&#30456;&#20851;&#20219;&#21153;&#20013;&#33719;&#24471;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#20013;&#65292;&#38656;&#35201;&#20351;&#29992;&#21477;&#23376;&#34920;&#31034;&#26469;&#36827;&#34892;&#35821;&#20041;&#26816;&#32034;&#20219;&#21153;&#12290;&#22312;&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#30456;&#24403;&#26377;&#25928;&#22320;&#35745;&#31639;&#36825;&#20123;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#39640;&#32500;&#21477;&#23376;&#23884;&#20837;&#12290;&#23454;&#38469;&#19978;&#23384;&#22312;&#22823;&#22411;&#21644;&#23567;&#22411;&#27169;&#22411;&#20043;&#38388;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#31354;&#38388;&#21644;&#26102;&#38388;&#30828;&#20214;&#38480;&#21046;&#65292;&#38656;&#35201;&#22312;&#20351;&#29992;&#36739;&#23567;&#27169;&#22411;(&#36890;&#24120;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#31616;&#29256;&#26412;)&#26102;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26368;&#22823;&#32534;&#30721;&#36895;&#29575;&#38477;&#20302;(MCR2)&#30446;&#26631;&#30340;&#22522;&#30784;&#19978;&#23398;&#20064;&#39069;&#22806;&#30340;&#25237;&#24433;&#23618;&#65292;&#35780;&#20272;&#20102;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;Sentence-BERT&#30340;&#27169;&#22411;&#33976;&#39311;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;MCR2&#26159;&#19968;&#31181;&#20026;&#20102;&#36890;&#29992;&#27969;&#24418;&#32858;&#31867;&#32780;&#24320;&#21457;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#24230;&#21644;&#21477;&#23376;&#23884;&#20837;&#22823;&#23567;&#26041;&#38754;&#20943;&#23567;&#30340;&#26032;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#35821;&#20041;&#30456;&#20851;&#20219;&#21153;&#20013;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most natural language inference problems, sentence representation is needed for semantic retrieval tasks. In recent years, pre-trained large language models have been quite effective for computing such representations. These models produce high-dimensional sentence embeddings. An evident performance gap between large and small models exists in practice. Hence, due to space and time hardware limitations, there is a need to attain comparable results when using the smaller model, which is usually a distilled version of the large language model. In this paper, we assess the model distillation of the sentence representation model Sentence-BERT by augmenting the pre-trained distilled model with a projection layer additionally learned on the Maximum Coding Rate Reduction (MCR2)objective, a novel approach developed for general-purpose manifold clustering. We demonstrate that the new language model with reduced complexity and sentence embedding size can achieve comparable results on semantic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;&#23454;&#29616;&#26377;&#25928;&#30340;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#65292;&#20197;&#25552;&#39640;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12633</link><description>&lt;p&gt;
PUNR: &#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#30340;&#26032;&#38395;&#25512;&#33616;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
PUNR: Pre-training with User Behavior Modeling for News Recommendation. (arXiv:2304.12633v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#20219;&#21153;&#23454;&#29616;&#26377;&#25928;&#30340;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#65292;&#20197;&#25552;&#39640;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25512;&#33616;&#26088;&#22312;&#22522;&#20110;&#29992;&#25143;&#34892;&#20026;&#39044;&#27979;&#28857;&#20987;&#34892;&#20026;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#24314;&#27169;&#29992;&#25143;&#34920;&#31034;&#26159;&#25512;&#33616;&#39318;&#36873;&#26032;&#38395;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#38598;&#20013;&#22312;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#30340;&#25913;&#36827;&#19978;&#12290;&#28982;&#32780;&#65292;&#36824;&#32570;&#20047;&#38024;&#23545;&#29992;&#25143;&#34920;&#31034;&#20248;&#21270;&#30340;&#22522;&#20110;PLM&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#33539;&#20363;&#65292;&#21363;&#29992;&#25143;&#34892;&#20026;&#25513;&#34109;&#21644;&#29992;&#25143;&#34892;&#20026;&#29983;&#25104;&#65292;&#22343;&#33268;&#21147;&#20110;&#26377;&#25928;&#30340;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#25143;&#34892;&#20026;&#25513;&#34109;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#24674;&#22797;&#22522;&#20110;&#19978;&#19979;&#25991;&#34892;&#20026;&#30340;&#25513;&#34109;&#29992;&#25143;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#26356;&#24378;&#22823;&#12289;&#26356;&#20840;&#38754;&#30340;&#29992;&#25143;&#26032;&#38395;&#38405;&#35835;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#29992;&#25143;&#34892;&#20026;&#29983;&#25104;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#22686;&#24378;&#20174;&#29992;&#25143;&#32534;&#30721;&#22120;&#27966;&#29983;&#20986;&#30340;&#29992;&#25143;&#34920;&#31034;&#21521;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19978;&#36848;&#39044;&#35757;&#32451;&#30340;&#29992;&#25143;&#24314;&#27169;&#26469;&#36827;&#34892;&#26032;&#38395;&#25512;&#33616;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
News recommendation aims to predict click behaviors based on user behaviors. How to effectively model the user representations is the key to recommending preferred news. Existing works are mostly focused on improvements in the supervised fine-tuning stage. However, there is still a lack of PLM-based unsupervised pre-training methods optimized for user representations. In this work, we propose an unsupervised pre-training paradigm with two tasks, i.e. user behavior masking and user behavior generation, both towards effective user behavior modeling. Firstly, we introduce the user behavior masking pre-training task to recover the masked user behaviors based on their contextual behaviors. In this way, the model could capture a much stronger and more comprehensive user news reading pattern. Besides, we incorporate a novel auxiliary user behavior generation pre-training task to enhance the user representation vector derived from the user encoder. We use the above pre-trained user modeling en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;NRM&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#31561;&#20215;&#26597;&#35810;&#30340;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;NRM&#32467;&#26524;&#19982;&#20855;&#26377;&#31561;&#20215;&#26597;&#35810;&#30340;&#31232;&#30095;&#26816;&#32034;&#31995;&#32479;&#30340;&#32467;&#26524;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#29983;&#25104;&#31561;&#20215;&#26597;&#35810;&#12290;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23545;&#26816;&#32034;&#25928;&#26524;&#21644;&#27599;&#31181;&#26041;&#27861;&#29983;&#25104;&#30340;&#35789;&#39033;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2304.12631</link><description>&lt;p&gt;
BM25&#31616;&#21333;&#26131;&#25026;&#65306;&#29992;&#31232;&#30095;&#36924;&#36817;&#35299;&#37322;&#23494;&#38598;&#27169;&#22411;&#30340;&#25490;&#21517;&#21015;&#34920;
&lt;/p&gt;
&lt;p&gt;
Explain like I am BM25: Interpreting a Dense Model's Ranked-List with a Sparse Approximation. (arXiv:2304.12631v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;NRM&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#31561;&#20215;&#26597;&#35810;&#30340;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;NRM&#32467;&#26524;&#19982;&#20855;&#26377;&#31561;&#20215;&#26597;&#35810;&#30340;&#31232;&#30095;&#26816;&#32034;&#31995;&#32479;&#30340;&#32467;&#26524;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#29983;&#25104;&#31561;&#20215;&#26597;&#35810;&#12290;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23545;&#26816;&#32034;&#25928;&#26524;&#21644;&#27599;&#31181;&#26041;&#27861;&#29983;&#25104;&#30340;&#35789;&#39033;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#20110;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#65292;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;(NRMs)&#22240;&#20026;&#21487;&#20197;&#36890;&#36807;&#31264;&#23494;&#25991;&#26723;&#34920;&#31034;&#26469;&#25429;&#25417;&#35821;&#20041;&#24847;&#20041;&#32780;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#19981;&#20381;&#36182;&#20110;&#26126;&#30830;&#30340;&#35789;&#39033;&#21305;&#37197;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#21487;&#35835;&#24615;&#24046;&#12290;&#20026;&#20102;&#29983;&#25104;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#26597;&#35810;&#35299;&#37322;&#30340;&#26032;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#31561;&#20215;&#26597;&#35810;&#8221;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;NRM&#32467;&#26524;&#19982;&#20855;&#26377;&#31561;&#20215;&#26597;&#35810;&#30340;&#31232;&#30095;&#26816;&#32034;&#31995;&#32479;&#30340;&#32467;&#26524;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#29983;&#25104;&#31561;&#20215;&#26597;&#35810;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;(&#22914;&#22522;&#20110;RM3&#30340;&#26597;&#35810;&#25193;&#23637;)&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23545;&#26816;&#32034;&#25928;&#26524;&#21644;&#27599;&#31181;&#26041;&#27861;&#29983;&#25104;&#30340;&#35789;&#39033;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural retrieval models (NRMs) have been shown to outperform their statistical counterparts owing to their ability to capture semantic meaning via dense document representations. These models, however, suffer from poor interpretability as they do not rely on explicit term matching. As a form of local per-query explanations, we introduce the notion of equivalent queries that are generated by maximizing the similarity between the NRM's results and the result set of a sparse retrieval system with the equivalent query. We then compare this approach with existing methods such as RM3-based query expansion and contrast differences in retrieval effectiveness and in the terms generated by each approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20316;&#32773;&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#20351;&#29992;&#30340;&#38024;&#23545;&#37329;&#20122;&#29747;&#36798;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;&#30340;&#31995;&#32479;&#65292;&#24182;&#22312;&#35813;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#20108;&#39640;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2304.12569</link><description>&lt;p&gt;
KINLP&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#30340;&#24212;&#29992;&#65306;&#37329;&#20122;&#29747;&#36798;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
KINLP at SemEval-2023 Task 12: Kinyarwanda Tweet Sentiment Analysis. (arXiv:2304.12569v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20316;&#32773;&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#20351;&#29992;&#30340;&#38024;&#23545;&#37329;&#20122;&#29747;&#36798;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;&#30340;&#31995;&#32479;&#65292;&#24182;&#22312;&#35813;&#31454;&#36187;&#20013;&#21462;&#24471;&#20102;&#31532;&#20108;&#39640;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20316;&#32773;&#21442;&#21152;SemEval-2023&#20219;&#21153;12&#65288;&#38750;&#27954;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65289;&#30340;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#30528;&#37325;&#20110;&#37329;&#20122;&#29747;&#36798;&#35821;&#65292;&#24182;&#20351;&#29992;&#20102;&#20855;&#26377;&#35821;&#35328;&#29305;&#24322;&#24615;&#30340;&#27169;&#22411;&#12290;&#37329;&#20122;&#29747;&#36798;&#35821;&#30340;&#35789;&#27719;&#24418;&#24577;&#23398;&#34987;&#24314;&#27169;&#20026;&#20004;&#23618;&#21464;&#24418;&#37329;&#21018;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#19988;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#26159;&#20351;&#29992;&#22810;&#20219;&#21153;&#25513;&#30721;&#24418;&#24577;&#39044;&#27979;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#35813;&#27169;&#22411;&#37096;&#32626;&#22312;&#19968;&#20010;&#23454;&#39564;&#24179;&#21488;&#19978;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#22312;&#26080;&#38656;&#32534;&#20889;&#26426;&#22120;&#23398;&#20064;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#20849;&#20139;&#20219;&#21153;&#30340;&#26368;&#32456;&#25552;&#20132;&#20013;&#33719;&#24471;&#20102;34&#25903;&#22242;&#38431;&#20013;&#30340;&#31532;&#20108;&#21517;&#65292;&#21462;&#24471;&#20102;72.50%&#30340;&#21152;&#26435;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#20998;&#26512;&#31361;&#20986;&#20102;&#22312;&#35813;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#25361;&#25112;&#65292;&#24182;&#30830;&#23450;&#20102;&#38656;&#35201;&#25913;&#36827;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the system entered by the author to the SemEval-2023 Task 12: Sentiment analysis for African languages. The system focuses on the Kinyarwanda language and uses a language-specific model. Kinyarwanda morphology is modeled in a two tier transformer architecture and the transformer model is pre-trained on a large text corpus using multi-task masked morphology prediction. The model is deployed on an experimental platform that allows users to experiment with the pre-trained language model fine-tuning without the need to write machine learning code. Our final submission to the shared task achieves second ranking out of 34 teams in the competition, achieving 72.50% weighted F1 score. Our analysis of the evaluation results highlights challenges in achieving high accuracy on the task and identifies areas for improvement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#26041;&#27861;&#8212;&#8212;\textsc{RenderDiffusion}&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#23427;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#25991;&#26412;&#24182;&#23454;&#29616;&#20102;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#23383;&#24418;&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12519</link><description>&lt;p&gt;
RenderDiffusion: &#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RenderDiffusion: Text Generation as Image Generation. (arXiv:2304.12519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#26041;&#27861;&#8212;&#8212;\textsc{RenderDiffusion}&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#23427;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#25991;&#26412;&#24182;&#23454;&#29616;&#20102;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#23383;&#24418;&#22270;&#20687;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#25991;&#26412;&#29983;&#25104;&#30340;&#26032;&#29983;&#25104;&#33539;&#24335;&#12290;&#32771;&#34385;&#21040;&#25991;&#26412;&#30340;&#31163;&#25955;&#20998;&#31867;&#29305;&#24615;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26041;&#27861;&#8212;&#8212;\textsc{RenderDiffusion}&#65292;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#36335;&#26159;&#23558;&#30446;&#26631;&#25991;&#26412;&#21576;&#29616;&#20026;&#21253;&#21547;&#35270;&#35273;&#35821;&#35328;&#20869;&#23481;&#30340;"&#23383;&#24418;&#22270;&#20687;"&#12290;&#36825;&#26679;&#65292;&#26465;&#20214;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#23383;&#24418;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#28982;&#21518;&#33258;&#28982;&#22320;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have become a new generative paradigm for text generation. Considering the discrete categorical nature of text, in this paper, we propose \textsc{RenderDiffusion}, a novel diffusion approach for text generation via text-guided image generation. Our key idea is to render the target text as a \emph{glyph image} containing visual language content. In this way, conditional text generation can be cast as a glyph image generation task, and it is then natural to apply continuous diffusion models to discrete texts. Specially, we utilize a cascaded architecture (\ie a base and a super-resolution diffusion model) to generate high-fidelity glyph images, conditioned on the input text. Furthermore, we design a text grounding module to transform and refine the visual language content from generated glyph images into the final texts. In experiments over four conditional text generation tasks and two classes of metrics (\ie quality and diversity), \textsc{RenderDiffusion} can achieve 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#27880;&#37322;&#26631;&#35760;&#21644;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#35299;&#37322;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;LiveNLI&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#35782;&#21035;&#20154;&#24037;&#26631;&#27880;&#30340;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;GPT-3&#22312;&#26631;&#31614;&#39044;&#27979;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.12443</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#37322;&#29702;&#35299;&#21644;&#39044;&#27979;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;
&lt;/p&gt;
&lt;p&gt;
Understanding and Predicting Human Label Variation in Natural Language Inference through Explanation. (arXiv:2304.12443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12443
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#27880;&#37322;&#26631;&#35760;&#21644;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#35299;&#37322;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;LiveNLI&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#35782;&#21035;&#20154;&#24037;&#26631;&#27880;&#30340;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;GPT-3&#22312;&#26631;&#31614;&#39044;&#27979;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#20154;&#24037;&#26631;&#27880;&#30340;&#24046;&#24322;&#25110;&#27880;&#37322;&#19981;&#19968;&#33268;&#29616;&#35937;&#12290;&#20026;&#20102;&#20351;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#19988;&#21487;&#20449;&#65292;&#38656;&#35201;&#35782;&#21035;&#36825;&#31181;&#24046;&#24322;&#24182;&#33021;&#22815;&#36827;&#34892;&#35299;&#37322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#29983;&#24577;&#26377;&#25928;&#30340;&#35299;&#37322;&#25968;&#25454;&#38598;&#65288;LiveNLI&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;122&#20010;&#33521;&#25991;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#39033;&#30446;&#30340;&#27880;&#37322;&#21644;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#27599;&#20010;&#39033;&#30446;&#33267;&#23569;&#26377;10&#20010;&#27880;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#30340;&#35299;&#37322;&#36827;&#34892;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;GPT-3&#30340;&#33021;&#21147;&#26469;&#39044;&#27979;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#21518;&#30340;&#26631;&#31614;&#20998;&#24067;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human label variation (Plank 2022), or annotation disagreement, exists in many natural language processing (NLP) tasks. To be robust and trusted, NLP models need to identify such variation and be able to explain it. To this end, we created the first ecologically valid explanation dataset with diverse reasoning, LiveNLI. LiveNLI contains annotators' highlights and free-text explanations for the label(s) of their choice for 122 English Natural Language Inference items, each with at least 10 annotations. We used its explanations for chain-of-thought prompting, and found there is still room for improvement in GPT-3's ability to predict label distribution with in-context learning.
&lt;/p&gt;</description></item><item><title>TIGTEC&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#25991;&#26412;&#23545;&#25239;&#20107;&#20363;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#26412;&#22320;&#29305;&#24449;&#37325;&#35201;&#24615;&#38024;&#23545;&#21644;&#20462;&#25913;&#23545;&#20998;&#31867;&#32467;&#26524;&#24433;&#21709;&#26368;&#22823;&#30340;&#21333;&#35789;&#65292;&#24182;&#22312;&#20195;&#20215;&#20989;&#25968;&#20013;&#20351;&#29992;&#35821;&#20041;&#36317;&#31163;&#35780;&#20272;&#23545;&#25239;&#24615;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#12289;&#31232;&#30095;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#19988;&#26082;&#21487;&#20197;&#26159;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#26080;&#20851;&#27169;&#22411;&#30340;&#65292;&#38750;&#24120;&#26041;&#20415;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.12425</link><description>&lt;p&gt;
TIGTEC&#65306;&#22522;&#20110;&#26631;&#35760;&#37325;&#35201;&#24615;&#30340;&#25991;&#26412;&#23545;&#25239;&#20107;&#20363;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TIGTEC : Token Importance Guided TExt Counterfactuals. (arXiv:2304.12425v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12425
&lt;/p&gt;
&lt;p&gt;
TIGTEC&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#25991;&#26412;&#23545;&#25239;&#20107;&#20363;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#26412;&#22320;&#29305;&#24449;&#37325;&#35201;&#24615;&#38024;&#23545;&#21644;&#20462;&#25913;&#23545;&#20998;&#31867;&#32467;&#26524;&#24433;&#21709;&#26368;&#22823;&#30340;&#21333;&#35789;&#65292;&#24182;&#22312;&#20195;&#20215;&#20989;&#25968;&#20013;&#20351;&#29992;&#35821;&#20041;&#36317;&#31163;&#35780;&#20272;&#23545;&#25239;&#24615;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#12289;&#31232;&#30095;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#19988;&#26082;&#21487;&#20197;&#26159;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#26080;&#20851;&#27169;&#22411;&#30340;&#65292;&#38750;&#24120;&#26041;&#20415;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#20107;&#20363;&#26159;&#19968;&#31181;&#36890;&#36807;&#25913;&#21464;&#23454;&#20363;&#26469;&#32763;&#36716;&#20998;&#31867;&#22120;&#36755;&#20986;&#20197;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TIGTEC&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#31232;&#30095;&#12289;&#21487;&#20449;&#21644;&#22810;&#26679;&#24615;&#30340;&#23545;&#25239;&#24615;&#35299;&#37322;&#30340;&#39640;&#25928;&#27169;&#22359;&#26041;&#27861;&#12290;TIGTEC&#26159;&#19968;&#31181;&#25991;&#26412;&#32534;&#36753;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#26412;&#22320;&#29305;&#24449;&#37325;&#35201;&#24615;&#38024;&#23545;&#21644;&#20462;&#25913;&#23545;&#20998;&#31867;&#32467;&#26524;&#24433;&#21709;&#26368;&#22823;&#30340;&#21333;&#35789;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26412;&#22320;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#38598;&#25104;&#35821;&#20041;&#36317;&#31163;&#30340;&#20195;&#20215;&#20989;&#25968;&#23545;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#35299;&#37322;&#36827;&#34892;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#39640;&#25928;&#30340;&#35299;&#31354;&#38388;&#25628;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TIGTEC&#22312;&#25104;&#21151;&#29575;&#12289;&#31232;&#30095;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;&#35813;&#26041;&#27861;&#26082;&#21487;&#20197;&#26159;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#26080;&#20851;&#27169;&#22411;&#30340;&#65292;&#38750;&#24120;&#26041;&#20415;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual examples explain a prediction by highlighting changes of instance that flip the outcome of a classifier. This paper proposes TIGTEC, an efficient and modular method for generating sparse, plausible and diverse counterfactual explanations for textual data. TIGTEC is a text editing heuristic that targets and modifies words with high contribution using local feature importance. A new attention-based local feature importance is proposed. Counterfactual candidates are generated and assessed with a cost function integrating semantic distance, while the solution space is efficiently explored in a beam search fashion. The conducted experiments show the relevance of TIGTEC in terms of success rate, sparsity, diversity and plausibility. This method can be used in both model-specific or model-agnostic way, which makes it very convenient for generating counterfactual explanations.
&lt;/p&gt;</description></item><item><title>ChatGPT&#33021;&#36890;&#36807;&#21508;&#31181;&#19987;&#19994;&#21644;&#35768;&#21487;&#32771;&#35797;&#65292;&#20294;&#20854;&#24403;&#21069;&#29256;&#26412;&#26356;&#20687;&#26159;&#19968;&#20010;&#20013;&#25991;&#25151;&#38388;&#32780;&#38750;&#20154;&#24037;&#24847;&#35782;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#25512;&#29702;&#38169;&#35823;&#21644;&#19981;&#20934;&#30830;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2304.12411</link><description>&lt;p&gt;
ChatGPT (2&#26376;13&#26085;&#29256;&#26412;)&#26159;&#19968;&#20010;&#20013;&#25991;&#25151;&#38388;
&lt;/p&gt;
&lt;p&gt;
ChatGPT (Feb 13 Version) is a Chinese Room. (arXiv:2304.12411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12411
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#33021;&#36890;&#36807;&#21508;&#31181;&#19987;&#19994;&#21644;&#35768;&#21487;&#32771;&#35797;&#65292;&#20294;&#20854;&#24403;&#21069;&#29256;&#26412;&#26356;&#20687;&#26159;&#19968;&#20010;&#20013;&#25991;&#25151;&#38388;&#32780;&#38750;&#20154;&#24037;&#24847;&#35782;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#25512;&#29702;&#38169;&#35823;&#21644;&#19981;&#20934;&#30830;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22240;&#33021;&#22815;&#36890;&#36807;&#21508;&#31181;&#19987;&#19994;&#21644;&#35768;&#21487;&#32771;&#35797;&#32780;&#24341;&#36215;&#20102;&#31215;&#26497;&#21644;&#28040;&#26497;&#30340;&#26032;&#38395;&#25253;&#36947;&#12290;&#36825;&#34920;&#26126;ChatGPT&#21487;&#33021;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#26082;&#21487;&#20197;&#24847;&#21619;&#30528;&#23427;&#26159;&#19968;&#20010;&#20013;&#25991;&#25151;&#38388;&#65292;&#20063;&#21487;&#20197;&#24847;&#21619;&#30528;&#23427;&#26159;&#20154;&#24037;&#24847;&#35782;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;ChatGPT&#30340;&#29366;&#24577;&#26356;&#20687;&#26159;&#19968;&#20010;&#20013;&#25991;&#25151;&#38388;&#36824;&#26159;&#25509;&#36817;&#20154;&#24037;&#24847;&#35782;&#30340;&#38382;&#39064;&#20173;&#23384;&#22312;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#35777;&#26126;&#20102;&#24403;&#21069;&#29256;&#26412;&#30340;ChatGPT&#65288;2&#26376;13&#26085;&#29256;&#26412;&#65289;&#26159;&#19968;&#20010;&#20013;&#25991;&#25151;&#38388;&#12290;&#23613;&#31649;&#23384;&#22312;&#28508;&#22312;&#30340;&#35748;&#30693;&#32852;&#31995;&#30340;&#35777;&#25454;&#65292;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#38169;&#35823;&#12290;&#21516;&#26102;&#65292;&#25105;&#35777;&#26126;ChatGPT&#33021;&#22815;&#29983;&#25104;&#23545;&#21516;&#19968;&#38382;&#39064;&#30340;&#25152;&#26377;&#21487;&#33021;&#20998;&#31867;&#21709;&#24212;&#65292;&#24182;&#22238;&#22797;&#24102;&#26377;&#38169;&#35823;&#31034;&#20363;&#65292;&#22240;&#27492;&#36136;&#30097;&#23427;&#20316;&#20026;&#23398;&#20064;&#24037;&#20855;&#30340;&#25928;&#29992;&#12290;&#25105;&#36824;&#23637;&#31034;&#20102;ChatGPT&#21487;&#20197;&#36827;&#34892;&#20154;&#24037;&#24187;&#35273;&#65292;&#36825;&#34987;&#23450;&#20041;&#20026;&#29983;&#25104;&#33258;&#20449;&#38169;&#35823;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has gained both positive and negative publicity after reports suggesting that it is able to pass various professional and licensing examinations. This suggests that ChatGPT may pass Turing Test in the near future. However, a computer program that passing Turing Test can either mean that it is a Chinese Room or artificially conscious. Hence, the question of whether the current state of ChatGPT is more of a Chinese Room or approaching artificial consciousness remains. Here, I demonstrate that the current version of ChatGPT (Feb 13 version) is a Chinese Room. Despite potential evidence of cognitive connections, ChatGPT exhibits critical errors in causal reasoning. At the same time, I demonstrate that ChatGPT can generate all possible categorical responses to the same question and response with erroneous examples; thus, questioning its utility as a learning tool. I also show that ChatGPT is capable of artificial hallucination, which is defined as generating confidently wrong replie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.12410</link><description>&lt;p&gt;
PEFT-Ref: &#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#21442;&#32771;&#26550;&#26500;&#21644;&#31867;&#22411;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;(PEFT)&#25216;&#26415;&#26088;&#22312;&#25913;&#21892;&#23436;&#20840;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#38543;&#30528;&#19981;&#21516;&#30340;PEFT&#25216;&#26415;&#19981;&#26029;&#20986;&#29616;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#27604;&#36739;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#19979;&#26041;&#38754;&#65306;(i)&#23427;&#20204;&#28155;&#21152;&#21040;PLM&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;(ii)&#19981;&#21516;&#31867;&#22411;&#21644;&#31243;&#24230;&#30340;&#25928;&#29575;&#25913;&#36827;&#65292;(iii)&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;(iv)&#32467;&#26500;&#21644;&#21151;&#33021;&#24046;&#24322;&#22914;&#20309;&#19982;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#26679;&#30340;&#27604;&#36739;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#21516;&#26102;&#23558;&#24046;&#24322;&#38548;&#31163;&#21040;&#19982;&#26631;&#20934;&#32452;&#20214;&#30340;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#26631;&#20934;&#21270;&#21644;&#38548;&#31163;&#24046;&#24322;&#30340;&#36807;&#31243;&#65292;&#20986;&#29616;&#20102;PEFT&#25216;&#26415;&#30340;&#27169;&#22359;&#21270;&#35270;&#22270;&#65292;&#19981;&#20165;&#25903;&#25345;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#31216;&#20026;PEFT-Ref&#65292;&#21253;&#25324;&#19971;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#22788;&#29702;PEFT&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#21487;&#29992;&#20316;&#24320;&#21457;&#26032;PEFT&#25216;&#26415;&#21644;&#27604;&#36739;&#29616;&#26377;&#25216;&#26415;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent parameter-efficient finetuning (PEFT) techniques aim to improve over the considerable cost of fully finetuning large pretrained language models (PLM). As different PEFT techniques proliferate, it is becoming difficult to compare them, in particular in terms of (i) the structure and functionality they add to the PLM, (ii) the different types and degrees of efficiency improvements achieved, (iii) performance at different downstream tasks, and (iv) how differences in structure and functionality relate to efficiency and task performance. To facilitate such comparisons, this paper presents a reference framework which standardises aspects shared by different PEFT techniques, while isolating differences to specific locations and interactions with the standard components. Through this process of standardising and isolating differences, a modular view of PEFT techniques emerges, supporting not only direct comparison of different techniques and their efficiency and task performance, but a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20351;&#29992;&#40657;&#30418;API&#36827;&#34892;&#27602;&#24615;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#20381;&#36182;&#32487;&#25215;&#30340;&#33258;&#21160;&#27602;&#24615;&#35780;&#20998;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#24314;&#35758;&#37319;&#29992;&#26356;&#21152;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#35780;&#20272;&#27602;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12397</link><description>&lt;p&gt;
&#35770;&#20351;&#29992;&#40657;&#30418;API&#36827;&#34892;&#27602;&#24615;&#35780;&#20272;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research. (arXiv:2304.12397v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20351;&#29992;&#40657;&#30418;API&#36827;&#34892;&#27602;&#24615;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#20381;&#36182;&#32487;&#25215;&#30340;&#33258;&#21160;&#27602;&#24615;&#35780;&#20998;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#24314;&#35758;&#37319;&#29992;&#26356;&#21152;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#35780;&#20272;&#27602;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27602;&#24615;&#30340;&#24863;&#30693;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#19981;&#26029;&#28436;&#21464;&#65292;&#32780;&#19988;&#22312;&#19981;&#21516;&#30340;&#22320;&#29702;&#21644;&#25991;&#21270;&#32972;&#26223;&#20013;&#24448;&#24448;&#23384;&#22312;&#24046;&#24322;&#12290;&#21516;&#26679;&#65292;&#29992;&#20110;&#26816;&#27979;&#27602;&#24615;&#30340;&#21830;&#19994;&#40657;&#30418;API&#65288;&#20363;&#22914;Perspective API&#65289;&#20063;&#19981;&#26159;&#38745;&#24577;&#30340;&#65292;&#32780;&#32463;&#24120;&#37325;&#26032;&#35757;&#32451;&#20197;&#35299;&#20915;&#20219;&#20309;&#26410;&#34987;&#20851;&#27880;&#30340;&#24369;&#28857;&#21644;&#20559;&#35265;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#21464;&#21270;&#23545;&#27604;&#36739;&#26088;&#22312;&#36943;&#21046;&#27602;&#24615;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#30340;&#30456;&#23545;&#20248;&#21155;&#30340;&#30740;&#31350;&#21457;&#29616;&#30340;&#21487;&#37325;&#22797;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20381;&#36182;&#32487;&#25215;&#30340;&#33258;&#21160;&#27602;&#24615;&#35780;&#20998;&#26469;&#27604;&#36739;&#27169;&#22411;&#21644;&#25216;&#26415;&#30340;&#30740;&#31350;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#37325;&#26032;&#23545;HELM&#30340;&#25152;&#26377;&#27169;&#22411;&#36827;&#34892;&#26368;&#26032;&#29256;&#26412;API&#30340;&#27602;&#24615;&#35780;&#20998;&#65292;&#23548;&#33268;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#19981;&#21516;&#25490;&#21517;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#23558;&#30740;&#31350;&#20043;&#38388;&#36827;&#34892;&#39532;&#34562;&#25340;&#25509;&#22411;&#27604;&#36739;&#26102;&#35201;&#35880;&#24910;&#65292;&#24182;&#20026;&#35780;&#20272;&#27602;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26356;&#21152;&#26377;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#25552;&#20986;&#24314;&#35758;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/X/XXX&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. Rescoring all models from HELM, a widely respected living benchmark, for toxicity with the recent version of the API led to a different ranking of widely used foundation models. We suggest caution in applying apples-to-apples comparisons between studies and lay recommendations for a more structured approach to evaluating toxicity over time. Code and data are available at https
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12395</link><description>&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#30340;&#26497;&#38480;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Extreme Classification for Answer Type Prediction in Question Answering. (arXiv:2304.12395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#20013;&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#31572;&#26696;&#31867;&#22411;&#39044;&#27979;&#65288;SMART&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#30340;&#26377;&#29992;&#27493;&#39588;&#12290; SMART&#20219;&#21153;&#28041;&#21450;&#39044;&#27979;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#21069;k&#20010;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#31867;&#22411;&#12290;&#30001;&#20110;KG&#20013;&#23384;&#22312;&#22823;&#37327;&#31867;&#22411;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Transformer&#27169;&#22411;&#65288;XBERT&#65289;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#36890;&#36807;&#23558;KG&#31867;&#22411;&#22522;&#20110;&#38382;&#39064;&#25991;&#26412;&#20351;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#20855;&#20307;&#22320;&#25913;&#21892;&#20102;XBERT&#27969;&#31243;&#30340;&#32858;&#31867;&#38454;&#27573;&#65292;&#21033;&#29992;&#20174;KG&#20013;&#27966;&#29983;&#30340;&#25991;&#26412;&#21644;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;SMART&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#65292;&#24182;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic answer type prediction (SMART) is known to be a useful step towards effective question answering (QA) systems. The SMART task involves predicting the top-$k$ knowledge graph (KG) types for a given natural language question. This is challenging due to the large number of types in KGs. In this paper, we propose use of extreme multi-label classification using Transformer models (XBERT) by clustering KG types using structural and semantic features based on question text. We specifically improve the clustering stage of the XBERT pipeline using textual and structural features derived from KGs. We show that these features can improve end-to-end performance for the SMART task, and yield state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20302;&#25104;&#26412;&#19979;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;LoRA&#21644;Stanford Alpaca&#25968;&#25454;&#38598;&#30340;Eluwa&#27169;&#22411;&#31995;&#21015;&#65292;&#21487;&#22823;&#24133;&#25552;&#39640;Facebook&#30340;OPT 1.3B&#12289;2.7B&#21644;6.7B&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;40&#32654;&#20803;&#30340;&#35745;&#31639;&#25104;&#26412;&#21363;&#21487;&#35753;&#36739;&#23567;&#30340;&#27169;&#22411;&#20855;&#26377;&#21644;&#22823;3&#20493;&#27169;&#22411;&#19968;&#26679;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12370</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#19979;&#26356;&#22909;&#30340;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Better Question-Answering Models on a Budget. (arXiv:2304.12370v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20302;&#25104;&#26412;&#19979;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;LoRA&#21644;Stanford Alpaca&#25968;&#25454;&#38598;&#30340;Eluwa&#27169;&#22411;&#31995;&#21015;&#65292;&#21487;&#22823;&#24133;&#25552;&#39640;Facebook&#30340;OPT 1.3B&#12289;2.7B&#21644;6.7B&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;40&#32654;&#20803;&#30340;&#35745;&#31639;&#25104;&#26412;&#21363;&#21487;&#35753;&#36739;&#23567;&#30340;&#27169;&#22411;&#20855;&#26377;&#21644;&#22823;3&#20493;&#27169;&#22411;&#19968;&#26679;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#21644;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;-&#31572;&#26696;&#25968;&#25454;&#38598;&#20351;&#24471;&#26356;&#23567;&#30340;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#22320;&#24494;&#35843;&#21040;&#20855;&#26377;&#22797;&#26434;&#23545;&#35805;&#33021;&#21147;&#30340;&#31243;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Eluwa&#65292;&#19968;&#31181;&#20351;&#29992;Stanford Alpaca&#25968;&#25454;&#38598;&#30340;LoRA&#27169;&#22411;&#31995;&#21015;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#21319;Facebook&#30340;OPT 1.3B&#12289;2.7B&#21644;6.7B&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#35753;GPT-4&#35780;&#20272;&#23427;&#20204;&#23545;&#28085;&#30422;&#22522;&#30784;&#30693;&#35782;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#21644;&#20854;&#20182;&#20219;&#21153;&#30340;&#25552;&#31034;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20302;&#25104;&#26412;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#21487;&#20197;&#24494;&#35843;&#21040;&#21644;&#22823;3&#20493;&#27169;&#22411;&#19968;&#26679;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20165;&#38656;40&#32654;&#20803;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank adaptation (LoRA) and question-answer datasets from large language models have made it much easier for much smaller models to be finetuned to the point where they display sophisticated conversational abilities. In this paper, we present Eluwa, a family of LoRA models that use the Stanford Alpaca dataset and massively improve the capabilities of Facebook's OPT 1.3B, 2.7B and 6.7B models. We benchmark these models in multiple ways, including letting GPT-4 judge their answers to prompts that span general knowledge, writing, programming and other tasks. We show that smaller models here can be fine-tuned to be as performant as models 3x larger - all for as little as 40 USD in compute.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21270;&#26641;&#32467;&#26500;&#30340;&#22312;&#32447;&#26085;&#24535;&#35299;&#26512;&#26041;&#27861;USTEP&#65292;&#35813;&#26041;&#27861;&#22312;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2304.12331</link><description>&lt;p&gt;
USTEP: &#22522;&#20110;&#28436;&#21270;&#26641;&#32467;&#26500;&#30340;&#22312;&#32447;&#26085;&#24535;&#35299;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
USTEP: Structuration des logs en flux gr{\^a}ce {\`a} un arbre de recherche {\'e}volutif. (arXiv:2304.12331v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21270;&#26641;&#32467;&#26500;&#30340;&#22312;&#32447;&#26085;&#24535;&#35299;&#26512;&#26041;&#27861;USTEP&#65292;&#35813;&#26041;&#27861;&#22312;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#34892;&#26102;&#26085;&#24535;&#35760;&#24405;&#20102;&#26377;&#20215;&#20540;&#30340;&#31995;&#32479;&#20449;&#24687;&#12290;&#23427;&#20204;&#34987;&#24191;&#27867;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24320;&#21457;&#21644;&#30417;&#25511;&#30446;&#30340;&#12290;&#35299;&#26512;&#26085;&#24535;&#28040;&#24687;&#20197;&#32467;&#26500;&#21270;&#20854;&#26684;&#24335;&#26159;&#26085;&#24535;&#25366;&#25496;&#20219;&#21153;&#30340;&#32463;&#20856;&#39044;&#22791;&#27493;&#39588;&#12290;&#30001;&#20110;&#23427;&#20204;&#20986;&#29616;&#22312;&#19978;&#28216;&#65292;&#35299;&#26512;&#25805;&#20316;&#21487;&#33021;&#25104;&#20026;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#22788;&#29702;&#26102;&#38388;&#29942;&#39048;&#12290;&#35299;&#26512;&#36136;&#37327;&#20063;&#30452;&#25509;&#24433;&#21709;&#20854;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21270;&#26641;&#32467;&#26500;&#30340;&#22312;&#32447;&#26085;&#24535;&#35299;&#26512;&#26041;&#27861;USTEP&#12290;&#23545;&#26469;&#33258;&#19981;&#21516;&#23454;&#38469;&#31995;&#32479;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#22312;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;USTEP&#22312;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logs record valuable system information at runtime. They are widely used by data-driven approaches for development and monitoring purposes. Parsing log messages to structure their format is a classic preliminary step for log-mining tasks. As they appear upstream, parsing operations can become a processing time bottleneck for downstream applications. The quality of parsing also has a direct influence on their efficiency. Here, we propose USTEP, an online log parsing method based on an evolving tree structure. Evaluation results on a wide panel of datasets coming from different real-world systems demonstrate USTEP superiority in terms of both effectiveness and robustness when compared to other online methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Skip-gram&#33410;&#28857;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#35745;&#31639;&#26725;&#25509;&#24230;&#35782;&#21035;&#37325;&#35201;&#33410;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;GRAPH-wGD&#65292;&#26377;&#25928;&#22320;&#25552;&#20379;&#20840;&#23616;&#24615;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.12036</link><description>&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;&#26725;&#25509;&#24230;&#37325;&#35201;&#33410;&#28857;&#29983;&#25104;Skip-gram&#33410;&#28857;&#23884;&#20837;&#30340;&#21518;&#32493;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness. (arXiv:2304.12036v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Skip-gram&#33410;&#28857;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#35745;&#31639;&#26725;&#25509;&#24230;&#35782;&#21035;&#37325;&#35201;&#33410;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;GRAPH-wGD&#65292;&#26377;&#25928;&#22320;&#25552;&#20379;&#20840;&#23616;&#24615;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26159;&#32534;&#30721;&#36830;&#32493;&#30690;&#37327;&#31354;&#38388;&#20013;&#30340;&#20851;&#31995;&#20449;&#24687;&#21516;&#26102;&#20445;&#30041;&#32593;&#32476;&#22266;&#26377;&#23646;&#24615;&#21644;&#32467;&#26500;&#30340;&#37325;&#35201;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;DeepWalk&#12289;LINE&#12289;struc2vec&#12289;PTE&#12289;UserItem2vec&#21644;RWJBG&#31561;&#26080;&#30417;&#30563;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#20174;Skip-gram&#27169;&#22411;&#20013;&#20986;&#29616;&#65292;&#24182;&#22312;&#35832;&#22914;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#29992;&#20110;&#23884;&#20837;&#30340;&#35299;&#37322;&#26041;&#27861;&#21644;&#29702;&#35770;&#30740;&#31350;&#65292;&#25552;&#20379;Skip-gram&#23884;&#20837;&#30340;&#21518;&#32493;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#22312;&#35889;&#32858;&#31867;&#24863;&#30693;&#23616;&#37096;&#25200;&#21160;&#19979;&#35745;&#31639;&#26725;&#25509;&#24230;&#26469;&#25214;&#21040;Skip-gram&#23884;&#20837;&#30340;&#20840;&#23616;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRAPH-wGD&#30340;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#20801;&#35768;&#26816;&#32034;top-q&#20840;&#23616;&#24615;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node representation learning in a network is an important machine learning technique for encoding relational information in a continuous vector space while preserving the inherent properties and structures of the network. Recently, unsupervised node embedding methods such as DeepWalk, LINE, struc2vec, PTE, UserItem2vec, and RWJBG have emerged from the Skip-gram model and perform better performance in several downstream tasks such as node classification and link prediction than the existing relational models. However, providing post-hoc explanations of Skip-gram-based embeddings remains a challenging problem because of the lack of explanation methods and theoretical studies applicable for embeddings. In this paper, we first show that global explanations to the Skip-gram-based embeddings can be found by computing bridgeness under a spectral cluster-aware local perturbation. Moreover, a novel gradient-based explanation method, which we call GRAPH-wGD, is proposed that allows the top-q glo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;NAIST-SIC-Aligned&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#23545;&#40784;&#30340;&#33521;&#26085;&#24179;&#34892;&#21516;&#22768;&#20256;&#35793;&#25968;&#25454;&#38598;&#12290;&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#32463;&#36807;&#23450;&#37327;&#25110;&#23450;&#24615;&#39564;&#35777;&#30340;&#27599;&#20010;&#27493;&#39588;&#65292;&#20197;&#30830;&#20445;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;SI&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.11766</link><description>&lt;p&gt;
NAIST-SIC-Aligned&#65306;&#33258;&#21160;&#23545;&#40784;&#30340;&#33521;&#26085;&#21516;&#22768;&#20256;&#35793;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NAIST-SIC-Aligned: Automatically-Aligned English-Japanese Simultaneous Interpretation Corpus. (arXiv:2304.11766v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;NAIST-SIC-Aligned&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#23545;&#40784;&#30340;&#33521;&#26085;&#24179;&#34892;&#21516;&#22768;&#20256;&#35793;&#25968;&#25454;&#38598;&#12290;&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#32463;&#36807;&#23450;&#37327;&#25110;&#23450;&#24615;&#39564;&#35777;&#30340;&#27599;&#20010;&#27493;&#39588;&#65292;&#20197;&#30830;&#20445;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;SI&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#21033;&#29992;&#21516;&#22768;&#20256;&#35793;&#65288;SI&#65289;&#25968;&#25454;&#26469;&#24433;&#21709;&#21516;&#22768;&#26426;&#22120;&#32763;&#35793;&#65288;SiMT&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NAIST-SIC-Aligned&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#23545;&#40784;&#30340;&#33521;&#26085;&#24179;&#34892;&#21516;&#22768;&#20256;&#35793;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#19968;&#20010;&#38750;&#23545;&#40784;&#35821;&#26009;&#24211;NAIST-SIC&#24320;&#22987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23545;&#40784;&#26041;&#27861;&#65292;&#20351;&#35821;&#26009;&#24211;&#20855;&#26377;&#24179;&#34892;&#24615;&#65292;&#20174;&#32780;&#36866;&#21512;&#27169;&#22411;&#35757;&#32451;&#12290;&#31532;&#19968;&#38454;&#27573;&#26159;&#31895;&#30053;&#23545;&#40784;&#65292;&#22312;&#27492;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#22312;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#25191;&#34892;&#19968;&#20010;&#22810;&#23545;&#22810;&#30340;&#26144;&#23556;&#65307;&#31532;&#20108;&#38454;&#27573;&#26159;&#32454;&#31890;&#24230;&#23545;&#40784;&#65292;&#22312;&#27492;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#25191;&#34892;&#35821;&#21477;&#20869;&#37096;&#21644;&#35821;&#21477;&#38388;&#36807;&#28388;&#26469;&#25552;&#39640;&#23545;&#40784;&#23545;&#30340;&#36136;&#37327;&#12290;&#20026;&#30830;&#20445;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#32463;&#36807;&#20102;&#23450;&#37327;&#25110;&#23450;&#24615;&#30340;&#39564;&#35777;&#12290;&#36825;&#26159;&#25991;&#29486;&#20013;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;SI&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25163;&#21160;&#31934;&#36873;&#20102;&#19968;&#20010;&#23567;&#22411;&#27979;&#35797;&#38598;&#29992;&#20110;&#35780;&#20272;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
It remains a question that how simultaneous interpretation (SI) data affects simultaneous machine translation (SiMT). Research has been limited due to the lack of a large-scale training corpus. In this work, we aim to fill in the gap by introducing NAIST-SIC-Aligned, which is an automatically-aligned parallel English-Japanese SI dataset. Starting with a non-aligned corpus NAIST-SIC, we propose a two-stage alignment approach to make the corpus parallel and thus suitable for model training. The first stage is coarse alignment where we perform a many-to-many mapping between source and target sentences, and the second stage is fine-grained alignment where we perform intra- and inter-sentence filtering to improve the quality of aligned pairs. To ensure the quality of the corpus, each step has been validated either quantitatively or qualitatively. This is the first open-sourced large-scale parallel SI dataset in the literature. We also manually curated a small test set for evaluation purpose
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#21319;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#29305;&#21035;&#26159;ToM&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11490</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#26234;&#29702;&#35770;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#31034;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#25552;&#21319;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#29305;&#21035;&#26159;ToM&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#20219;&#21153;&#38656;&#35201;&#29702;&#35299;&#20195;&#29702;&#20154;&#30340;&#20449;&#24565;&#12289;&#30446;&#26631;&#21644;&#24515;&#29702;&#29366;&#24577;&#65292;&#23545;&#20110;&#28041;&#21450;&#20154;&#31867;&#30340;&#24120;&#35782;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#25552;&#39640;LLM&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#27979;&#37327;&#20102;GPT-4&#21644;&#19977;&#20010;GPT-3.5&#21464;&#20307;&#65288;Davinci-2&#12289;Davinci-3&#12289;GPT-3.5-Turbo&#65289;&#30340;ToM&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#23427;&#20204;&#30340;ToM&#29702;&#35299;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21253;&#21547;&#20004;&#27493;&#24605;&#32500;&#25512;&#29702;&#21644;&#36880;&#27493;&#24605;&#32771;&#35828;&#26126;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#35757;&#32451;&#30340;LLMs&#65288;&#38500;Davinci-2&#22806;&#30340;&#25152;&#26377;&#27169;&#22411;&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;ToM&#20934;&#30830;&#24615;&#12290;GPT-4&#22312;&#38646;&#36718;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;&#36817;80%&#30340;ToM&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#19981;&#36275;&#27979;&#35797;&#38598;&#19978;87%&#30340;&#20154;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#25552;&#20379;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25552;&#31034;&#26102;&#65292;GPT-4&#21644;&#19977;&#20010;GPT-3.5&#21464;&#20307;&#30340;ToM&#20934;&#30830;&#24615;&#26174;&#33879;&#39640;&#20110;&#26080;&#25552;&#31034;&#26102;&#65292;&#20854;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#65288;GPT-3.5-Turbo&#65289;&#36798;&#21040;&#20102;92%&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#21319;LLM&#22312;&#22797;&#26434;&#25512;&#29702;&#23588;&#20854;&#26159;ToM&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#38750;&#27954;14&#20010;&#19981;&#21516;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.11256</link><description>&lt;p&gt;
UBC-DLNLP&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#30340;&#36129;&#29486;&#65306;&#36716;&#31227;&#23398;&#20064;&#23545;&#38750;&#27954;&#24773;&#24863;&#20998;&#26512;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
UBC-DLNLP at SemEval-2023 Task 12: Impact of Transfer Learning on African Sentiment Analysis. (arXiv:2304.11256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#38750;&#27954;14&#20010;&#19981;&#21516;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval 2023 AfriSenti-SemEval&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#36129;&#29486;&#65292;&#20854;&#20013;&#25105;&#20204;&#35299;&#20915;&#20102;14&#31181;&#19981;&#21516;&#38750;&#27954;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#23436;&#20840;&#30417;&#30563;&#30340;&#26465;&#20214;&#19979;&#24320;&#21457;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65288;&#23376;&#20219;&#21153;A&#21644;B&#65289;&#12290;&#25105;&#20204;&#36824;&#20026;&#38646;-shot&#35774;&#32622;&#65288;&#23376;&#20219;&#21153;C&#65289;&#24320;&#21457;&#20102;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#20845;&#31181;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#19968;&#20123;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#35843;&#25972;&#20197;&#21450;&#26368;&#21518;&#30340;&#24494;&#35843;&#38454;&#27573;&#12290;&#25105;&#20204;&#25928;&#26524;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#24320;&#21457;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;70.36&#30340;F1&#20998;&#25968;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;66.13&#30340;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#65292;&#36716;&#31227;&#23398;&#20064;&#21644;&#24494;&#35843;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#35821;&#35328;&#21644;&#39046;&#22495;&#30340;&#20854;&#20182;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe our contribution to the SemEVAl 2023 AfriSenti-SemEval shared task, where we tackle the task of sentiment analysis in 14 different African languages. We develop both monolingual and multilingual models under a full supervised setting (subtasks A and B). We also develop models for the zero-shot setting (subtask C). Our approach involves experimenting with transfer learning using six language models, including further pertaining of some of these models as well as a final finetuning stage. Our best performing models achieve an F1-score of 70.36 on development data and an F1-score of 66.13 on test data. Unsurprisingly, our results demonstrate the effectiveness of transfer learning and fine-tuning techniques for sentiment analysis across multiple languages. Our approach can be applied to other sentiment analysis tasks in different languages and domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;LOT&#8221;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#20174;&#27491;&#38754;&#21644;&#36127;&#38754;&#35757;&#32451;&#20449;&#21495;&#20013;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#31163;&#25955;&#24230;&#23558;&#29983;&#25104;&#21521;&#37327;&#20174;&#19981;&#23433;&#20840;&#23376;&#31354;&#38388;&#25351;&#21521;&#23433;&#20840;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2304.11220</link><description>&lt;p&gt;
&#23398;&#20064;&#8220;&#19981;&#23398;&#20064;&#8221;: &#26397;&#21521;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#29983;&#25104;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Learn What NOT to Learn: Towards Generative Safety in Chatbots. (arXiv:2304.11220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;LOT&#8221;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#20174;&#27491;&#38754;&#21644;&#36127;&#38754;&#35757;&#32451;&#20449;&#21495;&#20013;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#31163;&#25955;&#24230;&#23558;&#29983;&#25104;&#21521;&#37327;&#20174;&#19981;&#23433;&#20840;&#23376;&#31354;&#38388;&#25351;&#21521;&#23433;&#20840;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#12289;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#27169;&#22411;&#23588;&#20854;&#23481;&#26131;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#20869;&#23481;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22312;&#22522;&#20110;Web&#30340;&#31038;&#20132;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#12290;&#20808;&#21069;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#23384;&#22312;&#32570;&#28857;&#65292;&#22914;&#25171;&#26029;&#23545;&#35805;&#27969;&#31243;&#12289;&#23545;&#26410;&#35265;&#36807;&#30340;&#26377;&#27602;&#36755;&#20837;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12289;&#20026;&#20102;&#23433;&#20840;&#32780;&#29306;&#29298;&#23545;&#35805;&#36136;&#37327;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;LOT&#8221;&#65288;Learn NOT to&#65289;&#65292;&#23427;&#37319;&#29992;&#23545;&#27604;&#25439;&#22833;&#26469;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#21516;&#26102;&#20174;&#27491;&#38754;&#21644;&#36127;&#38754;&#35757;&#32451;&#20449;&#21495;&#20013;&#23398;&#20064;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#30456;&#36739;&#20110;&#26631;&#20934;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#35821;&#35328;&#20998;&#24067;&#20013;&#33258;&#21160;&#33719;&#24471;&#27491;&#12289;&#36127;&#20449;&#21495;&#12290;LOT&#26694;&#26550;&#21033;&#29992;&#31163;&#25955;&#24230;&#23558;&#29983;&#25104;&#21521;&#37327;&#20174;&#19981;&#23433;&#20840;&#23376;&#31354;&#38388;&#25351;&#21521;&#23433;&#20840;&#23376;&#31354;&#38388;&#65292;&#21516;&#26102;&#32500;&#25345;&#23545;&#35805;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#39640;&#65292;&#22312;SafeDialog&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational models that are generative and open-domain are particularly susceptible to generating unsafe content since they are trained on web-based social data. Prior approaches to mitigating this issue have drawbacks, such as disrupting the flow of conversation, limited generalization to unseen toxic input contexts, and sacrificing the quality of the dialogue for the sake of safety. In this paper, we present a novel framework, named "LOT" (Learn NOT to), that employs a contrastive loss to enhance generalization by learning from both positive and negative training signals. Our approach differs from the standard contrastive learning framework in that it automatically obtains positive and negative signals from the safe and unsafe language distributions that have been learned beforehand. The LOT framework utilizes divergence to steer the generations away from the unsafe subspace and towards the safe subspace while sustaining the flow of conversation. Our approach is memory and time-ef
&lt;/p&gt;</description></item><item><title>CB-Conformer &#26159;&#19968;&#31181;&#20026;&#20102;&#25552;&#39640;&#26377;&#20559;&#24046;&#35789;&#35782;&#21035;&#32780;&#25552;&#20986;&#30340; Conformer&#65292;&#24341;&#20837;&#20102; Contextual Biasing Module &#21644; Self-Adaptive Language Model &#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#21333;&#35789;&#20559;&#24046;&#20449;&#24687;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.09607</link><description>&lt;p&gt;
CB-Conformer: &#38024;&#23545;&#20559;&#24046;&#35789;&#35782;&#21035;&#30340;&#35821;&#22659;&#20559;&#24046;Conformer
&lt;/p&gt;
&lt;p&gt;
CB-Conformer: Contextual biasing Conformer for biased word recognition. (arXiv:2304.09607v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09607
&lt;/p&gt;
&lt;p&gt;
CB-Conformer &#26159;&#19968;&#31181;&#20026;&#20102;&#25552;&#39640;&#26377;&#20559;&#24046;&#35789;&#35782;&#21035;&#32780;&#25552;&#20986;&#30340; Conformer&#65292;&#24341;&#20837;&#20102; Contextual Biasing Module &#21644; Self-Adaptive Language Model &#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#21333;&#35789;&#20559;&#24046;&#20449;&#24687;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;&#26377;&#20559;&#24046;&#30340;&#21333;&#35789;&#20449;&#24687;&#26469;&#25552;&#39640;&#30446;&#26631;&#39046;&#22495;&#20013;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35805;&#39064;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#22266;&#23450;&#30340;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#30721;&#65292;&#35201;&#20040;&#24341;&#20837;&#19968;&#20010;&#24222;&#22823;&#30340;&#20559;&#24046;&#27169;&#22359;&#65292;&#23548;&#33268;&#36866;&#24212;&#24615;&#24046;&#65292;&#25512;&#29702;&#36895;&#24230;&#24930;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CB-Conformer&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#20559;&#24046;&#27169;&#22359;&#21644;&#33258;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#26377;&#20559;&#24046;&#30340;&#21333;&#35789;&#35782;&#21035;&#12290;&#19978;&#19979;&#25991;&#20559;&#24046;&#27169;&#22359;&#23558;&#38899;&#39057;&#29255;&#27573;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#32452;&#21512;&#36215;&#26469;&#65292;&#20165;&#26377;&#21407;&#22987;Conformer&#27169;&#22411;&#21442;&#25968;&#30340;0.2&#65285;&#12290;&#33258;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#26377;&#20559;&#24046;&#30340;&#21333;&#35789;&#21484;&#22238;&#29575;&#21644;&#31934;&#30830;&#24230;&#20462;&#25913;&#20854;&#20869;&#37096;&#26435;&#37325;&#65292;&#20351;&#24471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#26356;&#20851;&#27880;&#26377;&#20559;&#24046;&#30340;&#21333;&#35789;&#65292;&#24182;&#27604;&#26631;&#20934;&#30340;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26356;&#25104;&#21151;&#22320;&#38598;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#35821;&#38899;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#27979;&#35797;&#38598;&#65292;&#21457;&#29616;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the mismatch between the source and target domains, how to better utilize the biased word information to improve the performance of the automatic speech recognition model in the target domain becomes a hot research topic. Previous approaches either decode with a fixed external language model or introduce a sizeable biasing module, which leads to poor adaptability and slow inference. In this work, we propose CB-Conformer to improve biased word recognition by introducing the Contextual Biasing Module and the Self-Adaptive Language Model to vanilla Conformer. The Contextual Biasing Module combines audio fragments and contextual information, with only 0.2% model parameters of the original Conformer. The Self-Adaptive Language Model modifies the internal weights of biased words based on their recall and precision, resulting in a greater focus on biased words and more successful integration with the automatic speech recognition model than the standard fixed language model. In addition
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MLA&#65289;&#20197;&#26368;&#22823;&#21270;&#25512;&#29702;&#36136;&#37327;&#24182;&#23454;&#29616;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#38899;&#21040;&#25991;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;S2T NMT&#65289;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#21516;&#22768;&#20256;&#35793;&#65288;SimulST&#65289;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#39057;&#35889;&#22270;&#20219;&#21153;&#65292;&#22312;TTS&#19978;&#20855;&#26377;&#39640;&#25928;&#29575;&#25910;&#30410;&#65292;&#22312;NMT&#21644;SimulST&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08453</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#25913;&#36827;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Autoregressive NLP Tasks via Modular Linearized Attention. (arXiv:2304.08453v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MLA&#65289;&#20197;&#26368;&#22823;&#21270;&#25512;&#29702;&#36136;&#37327;&#24182;&#23454;&#29616;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#38899;&#21040;&#25991;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;S2T NMT&#65289;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#21516;&#22768;&#20256;&#35793;&#65288;SimulST&#65289;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#39057;&#35889;&#22270;&#20219;&#21153;&#65292;&#22312;TTS&#19978;&#20855;&#26377;&#39640;&#25928;&#29575;&#25910;&#30410;&#65292;&#22312;NMT&#21644;SimulST&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#30340;&#27169;&#22411;&#24517;&#39035;&#22312;&#26368;&#32456;&#24212;&#29992;&#20110;&#36793;&#32536;&#25110;&#20854;&#20182;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#23567;&#22411;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#23558;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#65292;&#20294;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#33258;&#22238;&#24402;&#20219;&#21153;&#32780;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;MLA&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#20010;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21253;&#25324;cosFormer&#65292;&#20197;&#26368;&#22823;&#21270;&#25512;&#29702;&#36136;&#37327;&#24182;&#23454;&#29616;&#26174;&#30528;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#33258;&#22238;&#24402;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#38899;&#21040;&#25991;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;S2T NMT&#65289;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#21516;&#22768;&#20256;&#35793;&#65288;SimulST&#65289;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#39057;&#35889;&#22270;&#20219;&#21153;&#65292;&#22312;TTS&#19978;&#20855;&#26377;&#39640;&#25928;&#29575;&#25910;&#30410;&#65292;&#22312;NMT&#21644;SimulST&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various natural language processing (NLP) tasks necessitate models that are efficient and small based on their ultimate application at the edge or in other resource-constrained environments. While prior research has reduced the size of these models, increasing computational efficiency without considerable performance impacts remains difficult, especially for autoregressive tasks. This paper proposes {modular linearized attention (MLA), which combines multiple efficient attention mechanisms, including cosFormer, to maximize inference quality while achieving notable speedups. We validate this approach on several autoregressive NLP tasks, including speech-to-text neural machine translation (S2T NMT), speech-to-text simultaneous translation (SimulST), and autoregressive text-to-spectrogram, noting efficiency gains on TTS and competitive performance for NMT and SimulST during training and inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#22312;&#20013;&#25991;&#35821;&#35328;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.07987</link><description>&lt;p&gt;
&#20013;&#25991;&#24320;&#25918;&#24335;&#25351;&#20196;&#24191;&#20041;&#35821;&#35328;&#27169;&#22411;&#65306;&#21021;&#27493;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#22312;&#20013;&#25991;&#35821;&#35328;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#26500;&#24314;&#24191;&#20041;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#38543;&#30528;InstructGPT&#21644;ChatGPT&#30340;&#21457;&#24067;&#65292;&#23427;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#36824;&#26410;&#25506;&#32034;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#20219;&#21153;&#19978;&#26159;&#21542;&#21487;&#20197;&#20687;&#33521;&#35821;&#20219;&#21153;&#37027;&#26679;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#20196;&#35843;&#25972;&#26469;&#25191;&#34892;&#65292;&#20197;&#21450;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#25152;&#38656;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35843;&#25972;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39033;&#30446;&#65292;&#35797;&#22270;&#36890;&#36807;&#36866;&#24212;4&#20010;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#37319;&#29992;&#21508;&#31181;&#26041;&#27861;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#32422;20&#19975;&#20010;&#20013;&#25991;&#25351;&#20196;&#35843;&#25972;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#26816;&#26597;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#33521;&#25991;&#21644;&#20013;&#25991;&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#24182;&#23545;&#19968;&#20123;&#28508;&#22312;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31616;&#35201;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT~\citep{ouyang2022training} and ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning.  To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33521;&#21360;ST&#30340;e2e&#26550;&#26500;&#65292;&#21516;&#26102;&#23558;&#20004;&#20010;&#19981;&#23436;&#32654;&#30340;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#29992;&#20110;&#29983;&#25104;&#24182;&#34892;&#25968;&#25454;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#21576;&#29616;&#20986;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03169</link><description>&lt;p&gt;
&#38024;&#23545;&#40065;&#26834;&#35821;&#38899;&#32763;&#35793;&#30340;&#36873;&#25321;&#24615;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Selective Data Augmentation for Robust Speech Translation. (arXiv:2304.03169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33521;&#21360;ST&#30340;e2e&#26550;&#26500;&#65292;&#21516;&#26102;&#23558;&#20004;&#20010;&#19981;&#23436;&#32654;&#30340;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#29992;&#20110;&#29983;&#25104;&#24182;&#34892;&#25968;&#25454;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#21576;&#29616;&#20986;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#23558;&#19968;&#31181;&#35821;&#35328;&#30340;&#35821;&#38899;&#36716;&#21270;&#20026;&#21478;&#19968;&#31181;&#35821;&#35328;&#30340;&#25991;&#23383;&#12290;&#31471;&#21040;&#31471;&#65288;e2e&#65289;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#30001;&#20110;&#20855;&#26377;&#20943;&#23569;&#24310;&#36831;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#20248;&#36234;&#24615;&#33021;&#32780;&#27604;&#20018;&#32852;&#31995;&#32479;&#21463;&#21040;&#27426;&#36814;&#12290;&#34429;&#28982;&#36164;&#28304;&#23494;&#38598;&#22411;&#65292;&#20294;e2e-ST&#31995;&#32479;&#20855;&#26377;&#20445;&#30041;&#35821;&#38899;&#30340;&#21442;&#25968;&#21644;&#38750;&#35821;&#35328;&#29305;&#24449;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#19982;&#20018;&#32852;&#31995;&#32479;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;e2e&#26550;&#26500;&#26469;&#36827;&#34892;&#33521;&#21360;&#65288;en-hi&#65289;ST&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#19981;&#23436;&#32654;&#30340;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#23558;Libri-trans en&#25991;&#26412;&#32763;&#35793;&#25104;hi&#25991;&#26412;&#12290;&#34429;&#28982;&#27599;&#20010;&#26381;&#21153;&#37117;&#20250;&#21333;&#29420;&#25552;&#20379;MT&#25968;&#25454;&#20197;&#29983;&#25104;&#24182;&#34892;ST&#25968;&#25454;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;MT&#25968;&#25454;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#24110;&#21161;&#40065;&#26834;ST&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#23548;&#33268;&#27604;&#24378;&#21147;MT&#25968;&#25454;&#22686;&#24378;&#26356;&#22909;&#30340;ST&#65288;BLEU&#24471;&#20998;&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#22522;&#20934;&#26041;&#27861;&#25552;&#39640;&#20102;1.59 BLEU&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech translation (ST) systems translate speech in one language to text in another language. End-to-end ST systems (e2e-ST) have gained popularity over cascade systems because of their enhanced performance due to reduced latency and computational cost. Though resource intensive, e2e-ST systems have the inherent ability to retain para and non-linguistic characteristics of the speech unlike cascade systems. In this paper, we propose to use an e2e architecture for English-Hindi (en-hi) ST. We use two imperfect machine translation (MT) services to translate Libri-trans en text into hi text. While each service gives MT data individually to generate parallel ST data, we propose a data augmentation strategy of noisy MT data to aid robust ST. The main contribution of this paper is the proposal of a data augmentation strategy. We show that this results in better ST (BLEU score) compared to brute force augmentation of MT data. We observed an absolute improvement of 1.59 BLEU score with our appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#39069;&#22806;&#30340;&#26631;&#35760;&#22686;&#24378;&#36755;&#20837;&#65292;&#24341;&#20837;&#24490;&#29615;&#65292;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340; Transformer &#27169;&#22411;&#65288;&#22914; BERT&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27861;&#24459;&#25991;&#20214;&#39029;&#38754;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.02787</link><description>&lt;p&gt;
&#27861;&#24459;&#25991;&#20214;&#39029;&#38754;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Classification of Legal Document Pages. (arXiv:2304.02787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#39069;&#22806;&#30340;&#26631;&#35760;&#22686;&#24378;&#36755;&#20837;&#65292;&#24341;&#20837;&#24490;&#29615;&#65292;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340; Transformer &#27169;&#22411;&#65288;&#22914; BERT&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27861;&#24459;&#25991;&#20214;&#39029;&#38754;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#38656;&#35201;&#22788;&#29702;&#12289;&#32034;&#24341;&#21644;&#26816;&#32034;&#19987;&#19994;&#25991;&#26723;&#65288;&#22914; PDF &#26684;&#24335;&#31561;&#65289;&#30340;&#21830;&#19994;&#24212;&#29992;&#65292;&#23558;&#20219;&#20309;&#32473;&#23450;&#25991;&#26723;&#30340;&#39029;&#38754;&#20998;&#31867;&#20026;&#20854;&#30456;&#24212;&#31867;&#22411;&#36890;&#24120;&#26159;&#24517;&#35201;&#30340;&#12290;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#19987;&#27880;&#20110;&#21333;&#39029;&#25991;&#26723;&#65292;&#35201;&#20040;&#23558;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#39029;&#38754;&#29420;&#31435;&#22788;&#29702;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#21033;&#29992;&#30456;&#37051;&#39029;&#38754;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22686;&#24378;&#25991;&#26723;&#39029;&#38754;&#20998;&#31867;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#36890;&#24120;&#19981;&#33021;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#19978;&#36848;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#20851;&#20110;&#21069;&#19968;&#39029;&#30340;&#39034;&#24207;&#20449;&#24687;&#30340;&#39069;&#22806;&#26631;&#35760;&#26469;&#22686;&#24378;&#36755;&#20837;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#36825;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340; Transformer &#27169;&#22411;&#65288;&#22914; BERT&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#27861;&#24459;&#25991;&#20214;&#39029;&#38754;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many business applications that require the processing, indexing, and retrieval of professional documents such as legal briefs (in PDF format etc.), it is often essential to classify the pages of any given document into their corresponding types beforehand. Most existing studies in the field of document image classification either focus on single-page documents or treat multiple pages in a document independently. Although in recent years a few techniques have been proposed to exploit the context information from neighboring pages to enhance document page classification, they typically cannot be utilized with large pre-trained language models due to the constraint on input length. In this paper, we present a simple but effective approach that overcomes the above limitation. Specifically, we enhance the input with extra tokens carrying sequential information about previous pages - introducing recurrence - which enables the usage of pre-trained Transformer models like BERT for context
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37325;&#25490;&#30340;&#37327;&#21270;&#26041;&#27861;RPTQ&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37327;&#21270;&#26102;&#30001;&#20110;&#20449;&#36947;&#28608;&#27963;&#33539;&#22260;&#19981;&#21516;&#32780;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;&#23454;&#29616;&#35813;&#26041;&#27861;&#21518;&#65292;&#25105;&#20204;&#23558;LLL&#27169;&#22411;&#25512;&#21160;&#21040;3&#20301;&#28608;&#27963;&#12290;</title><link>http://arxiv.org/abs/2304.01089</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#25490;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
RPTQ: Reorder-based Post-training Quantization for Large Language Models. (arXiv:2304.01089v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37325;&#25490;&#30340;&#37327;&#21270;&#26041;&#27861;RPTQ&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37327;&#21270;&#26102;&#30001;&#20110;&#20449;&#36947;&#28608;&#27963;&#33539;&#22260;&#19981;&#21516;&#32780;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;&#23454;&#29616;&#35813;&#26041;&#27861;&#21518;&#65292;&#25105;&#20204;&#23558;LLL&#27169;&#22411;&#25512;&#21160;&#21040;3&#20301;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#32780;&#24341;&#21457;&#30340;&#37096;&#32626;&#25361;&#25112;&#12290;&#26412;&#25991;&#25351;&#20986;&#65292;LLL&#27169;&#22411;&#37327;&#21270;&#30340;&#20027;&#35201;&#38590;&#28857;&#22312;&#20110;&#20449;&#36947;&#20043;&#38388;&#19981;&#21516;&#30340;&#28608;&#27963;&#33539;&#22260;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#31163;&#32676;&#20540;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37325;&#25490;&#30340;&#37327;&#21270;&#26041;&#27861;RPTQ&#65292;&#29992;&#20110;&#35299;&#20915;LLL&#27169;&#22411;&#37327;&#21270;&#38382;&#39064;&#12290;RPTQ&#36890;&#36807;&#37325;&#26032;&#25490;&#21015;&#28608;&#27963;&#20013;&#30340;&#20449;&#36947;&#65292;&#24182;&#25353;&#31751;&#37327;&#21270;&#20449;&#36947;&#65292;&#20174;&#32780;&#20943;&#23569;&#20449;&#36947;&#33539;&#22260;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#36991;&#20813;&#26174;&#24335;&#37325;&#25490;&#20943;&#23569;&#23384;&#20648;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#23454;&#29616;&#20102;&#35813;&#26041;&#27861;&#21518;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;LLL&#27169;&#22411;&#25512;&#21160;&#21040;3&#20301;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language models (LLMs) have demonstrated outstanding performance on various tasks, but their deployment poses challenges due to their enormous model size. In this paper, we identify that the main challenge in quantizing LLMs stems from the different activation ranges between the channels, rather than just the issue of outliers.We propose a novel reorder-based quantization approach, RPTQ, that addresses the issue of quantizing the activations of LLMs. RPTQ rearranges the channels in the activations and then quantizing them in clusters, thereby reducing the impact of range difference of channels. In addition, we reduce the storage and computation overhead by avoiding explicit reordering. By implementing this approach, we achieved a significant breakthrough by pushing LLM models to 3 bit activation for the first time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#21644;&#26041;&#38754;&#29305;&#23450;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;NLG&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#34920;&#26126;ChatGPT&#20316;&#20026;NLG&#35780;&#20272;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#19968;&#33268;&#65292;&#23588;&#20854;&#26159;&#22312;&#27969;&#30021;&#24230;&#26041;&#38754;&#12290;&#36825;&#25552;&#37266;&#20154;&#20204;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#21807;&#19968;&#30340;&#33258;&#21160;NLG&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2303.04048</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#35780;&#20215;&#25351;&#26631;&#21487;&#38752;&#21527;&#65311;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good NLG Evaluator? A Preliminary Study. (arXiv:2303.04048v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04048
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#21644;&#26041;&#38754;&#29305;&#23450;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;NLG&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#34920;&#26126;ChatGPT&#20316;&#20026;NLG&#35780;&#20272;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#19968;&#33268;&#65292;&#23588;&#20854;&#26159;&#22312;&#27969;&#30021;&#24230;&#26041;&#38754;&#12290;&#36825;&#25552;&#37266;&#20154;&#20204;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#21807;&#19968;&#30340;&#33258;&#21160;NLG&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#20197;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#20026;&#22522;&#30784;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;ChatGPT&#20316;&#20026;&#19968;&#31181;&#35780;&#20272;&#25351;&#26631;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#32771;&#34385;&#21040;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#27169;&#22411;&#30340;&#36136;&#37327;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;NLG&#25351;&#26631;&#20197;&#20854;&#31967;&#31957;&#30340;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#32780;&#38395;&#21517;&#65292;&#22240;&#27492;&#25105;&#20204;&#26159;&#21542;&#20250;&#35748;&#20026;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;NLG&#35780;&#20272;&#25351;&#26631;&#12290;&#22312;&#36825;&#31687;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#20803;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;ChatGPT&#20316;&#20026;NLG&#25351;&#26631;&#30340;&#21487;&#38752;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ChatGPT&#35270;&#20026;&#20154;&#31867;&#35780;&#20272;&#22120;&#65292;&#24182;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#65288;&#20363;&#22914;&#25688;&#35201;&#65289;&#21644;&#26041;&#38754;&#29305;&#23450;&#65288;&#20363;&#22914;&#30456;&#20851;&#24615;&#65289;&#36827;&#34892;&#35828;&#26126;&#65292;&#20197;&#20419;&#20351;ChatGPT&#35780;&#20272;NLG&#27169;&#22411;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#25688;&#35201;&#12289;&#25925;&#20107;&#29983;&#25104;&#21644;&#32763;&#35793;&#22312;&#20869;&#30340;&#20116;&#20010;NLG&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#26041;&#38754;&#65288;&#20363;&#22914;&#27969;&#30021;&#24230;&#65289;&#65292;ChatGPT&#24182;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#19968;&#33268;&#12290;&#36825;&#25552;&#37266;&#20154;&#20204;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#21807;&#19968;&#30340;&#33258;&#21160;NLG&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and 
&lt;/p&gt;</description></item><item><title>Hitachi&#22242;&#38431;&#21442;&#21152;SemEval-2023&#31532;3&#39033;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#34920;&#26126;&#36328;&#35821;&#35328;/&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#25910;&#38598;&#22806;&#37096;&#24179;&#34913;&#25968;&#25454;&#38598;&#21487;&#20197;&#26377;&#30410;&#20110;&#27969;&#27966;&#21644;&#26694;&#26550;&#26816;&#27979;&#65292;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#20420;&#35821;&#27969;&#27966;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.01794</link><description>&lt;p&gt;
Hitachi&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#34920;&#29616;&#65306;&#25506;&#32034;&#36328;&#35821;&#35328;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#32447;&#26032;&#38395;&#20013;&#30340;&#27969;&#27966;&#21644;&#26694;&#26550;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies for Genre and Framing Detection in Online News. (arXiv:2303.01794v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01794
&lt;/p&gt;
&lt;p&gt;
Hitachi&#22242;&#38431;&#21442;&#21152;SemEval-2023&#31532;3&#39033;&#20219;&#21153;&#65292;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#34920;&#26126;&#36328;&#35821;&#35328;/&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#25910;&#38598;&#22806;&#37096;&#24179;&#34913;&#25968;&#25454;&#38598;&#21487;&#20197;&#26377;&#30410;&#20110;&#27969;&#27966;&#21644;&#26694;&#26550;&#26816;&#27979;&#65292;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#20420;&#35821;&#27969;&#27966;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Hitachi&#22242;&#38431;&#21442;&#21152;SemEval-2023&#31532;3&#39033;&#20219;&#21153;&#8220;&#22312;&#22810;&#35821;&#35328;&#35774;&#32622;&#20013;&#26816;&#27979;&#22312;&#32447;&#26032;&#38395;&#20013;&#30340;&#27969;&#27966;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#8221;&#30340;&#24773;&#20917;&#12290;&#37492;&#20110;&#20219;&#21153;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#20219;&#21153;&#24615;&#36136;&#21644;&#20302;&#36164;&#28304;&#29615;&#22659;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#36328;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#20197;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;(a)&#36328;&#35821;&#35328;/&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20197;&#21450;(b)&#25910;&#38598;&#22806;&#37096;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26377;&#30410;&#20110;&#27969;&#27966;&#21644;&#26694;&#26550;&#26816;&#27979;&#12290;&#25105;&#20204;&#20174;&#32467;&#26524;&#26500;&#24314;&#20102;&#38598;&#25104;&#27169;&#22411;&#65292;&#24182;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#20420;&#35821;&#27969;&#27966;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explains the participation of team Hitachi to SemEval-2023 Task 3 "Detecting the genre, the framing, and the persuasion techniques in online news in a multi-lingual setup.'' Based on the multilingual, multi-task nature of the task and the low-resource setting, we investigated different cross-lingual and multi-task strategies for training the pretrained language models. Through extensive experiments, we found that (a) cross-lingual/multi-task training, and (b) collecting an external balanced dataset, can benefit the genre and framing detection. We constructed ensemble models from the results and achieved the highest macro-averaged F1 scores in Italian and Russian genre categorization subtasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36136;&#37327;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#21483;&#20570;Metric Estimation&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#21442;&#32771;&#32763;&#35793;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#33258;&#21160;&#21270;&#35780;&#20272;&#24230;&#37327;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20154;&#24037;&#27880;&#37322;&#36153;&#26102;&#36153;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.09008</link><description>&lt;p&gt;
&#31351;&#20154;&#30340;&#36136;&#37327;&#35780;&#20272;&#65306;&#22312;&#27809;&#26377;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#22522;&#20110;&#21442;&#32771;&#30340;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Poor Man's Quality Estimation: Predicting Reference-Based MT Metrics Without the Reference. (arXiv:2301.09008v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09008
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36136;&#37327;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#21483;&#20570;Metric Estimation&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#21442;&#32771;&#32763;&#35793;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#33258;&#21160;&#21270;&#35780;&#20272;&#24230;&#37327;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20154;&#24037;&#27880;&#37322;&#36153;&#26102;&#36153;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#65288;QE&#65289;&#26159;&#22312;&#19981;&#26597;&#30475;&#21442;&#32771;&#25991;&#29486;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#32763;&#35793;&#20551;&#35774;&#30340;&#20154;&#31867;&#21028;&#26029;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;QE&#31995;&#32479;&#27491;&#22312;&#23454;&#29616;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#26174;&#30528;&#30456;&#20851;&#24615;&#65292;&#20294;&#23427;&#20204;&#30340;&#35745;&#31639;&#37327;&#22823;&#24182;&#19988;&#38656;&#35201;&#20154;&#31867;&#27880;&#37322;&#65292;&#36825;&#20123;&#27880;&#37322;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#21644;&#36164;&#37329;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#25351;&#26631;&#20272;&#35745;&#65288;ME&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#39044;&#27979;&#33258;&#21160;&#21270;&#24230;&#37327;&#20998;&#25968;&#65292;&#21516;&#26679;&#20063;&#27809;&#26377;&#21442;&#32771;&#25991;&#29486;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#21363;&#20351;&#27809;&#26377;&#21442;&#32771;&#25991;&#29486;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20063;&#33021;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#20272;&#35745;&#33258;&#21160;&#21270;&#24230;&#37327;&#65288;$ \rho = 60 \% $&#23545;&#20110;BLEU&#65292;$ \rho = 51 \% $&#23545;&#20110;&#20854;&#20182;&#24230;&#37327;&#65289;&#12290;&#22240;&#20026;&#33258;&#21160;&#21270;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#20851;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;ME&#20219;&#21153;&#20026;&#39044;&#35757;&#32451;QE&#27169;&#22411;&#26381;&#21153;&#12290;&#23545;&#20110;QE&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;TER&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;$ \rho = 23 \% $&#65289;&#27604;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65288;$ \rho = 20 \% $&#65289;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine translation quality estimation (QE) predicts human judgements of a translation hypothesis without seeing the reference. State-of-the-art QE systems based on pretrained language models have been achieving remarkable correlations with human judgements yet they are computationally heavy and require human annotations, which are slow and expensive to create. To address these limitations, we define the problem of metric estimation (ME) where one predicts the automated metric scores also without the reference. We show that even without access to the reference, our model can estimate automated metrics ($\rho$=60% for BLEU, $\rho$=51% for other metrics) at the sentence-level. Because automated metrics correlate with human judgements, we can leverage the ME task for pre-training a QE model. For the QE task, we find that pre-training on TER is better ($\rho$=23%) than training for scratch ($\rho$=20%).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#26029;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#26469;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;InferEM&#12290;&#27169;&#22411;&#21516;&#26102;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06373</link><description>&lt;p&gt;
InferEM: &#25512;&#26029;&#35828;&#35805;&#32773;&#24847;&#22270;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06373
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#26029;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#26469;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;InferEM&#12290;&#27169;&#22411;&#21516;&#26102;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20849;&#24773;&#22238;&#22797;&#29983;&#25104;&#30340;&#26041;&#27861;&#19968;&#33324;&#30452;&#25509;&#32534;&#30721;&#25972;&#20010;&#23545;&#35805;&#21382;&#21490;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#30721;&#22120;&#29983;&#25104;&#21451;&#22909;&#30340;&#21453;&#39304;&#12290;&#36825;&#20123;&#26041;&#27861;&#24378;&#35843;&#24314;&#27169;&#24773;&#22659;&#20449;&#24687;&#65292;&#20294;&#24573;&#35270;&#20102;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#30452;&#25509;&#24847;&#22270;&#12290;&#25105;&#20204;&#35748;&#20026;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#34920;&#36798;&#20102;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InferEM&#30340;&#26032;&#27169;&#22411;&#29992;&#20110;&#20849;&#24773;&#22238;&#22797;&#29983;&#25104;&#12290;&#25105;&#20204;&#23558;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#21333;&#29420;&#32534;&#30721;&#65292;&#36890;&#36807;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#19982;&#25972;&#20010;&#23545;&#35805;&#34701;&#21512;&#20197;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#30340;&#24515;&#29702;&#65292;&#29468;&#27979;&#23545;&#35805;&#32773;&#21487;&#33021;&#25552;&#21069;&#35828;&#20123;&#20160;&#20040;&#12290;&#20026;&#24179;&#34913;&#21457;&#35328;&#39044;&#27979;&#21644;&#22238;&#22797;&#29983;&#25104;&#30340;&#20248;&#21270;&#36895;&#29575;&#65292;InferEM&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches to empathetic response generation typically encode the entire dialogue history directly and put the output into a decoder to generate friendly feedback. These methods focus on modelling contextual information but neglect capturing the direct intention of the speaker. We argue that the last utterance in the dialogue empirically conveys the intention of the speaker. Consequently, we propose a novel model named InferEM for empathetic response generation. We separately encode the last utterance and fuse it with the entire dialogue through the multi-head attention based intention fusion module to capture the speaker's intention. Besides, we utilize previous utterances to predict the last utterance, which simulates human's psychology to guess what the interlocutor may speak in advance. To balance the optimizing rates of the utterance prediction and response generation, a multi-task learning strategy is designed for InferEM. Experimental results demonstrate the plausibility
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#31995;&#24863;&#30693;&#35821;&#35328;&#22270;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#32852;&#21512;&#25512;&#29702;&#35821;&#35328;&#21644;&#22270;&#20851;&#20110;&#23454;&#20307;&#20851;&#31995;&#65292;&#24182;&#22312;&#22810;&#20010;QA&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.00975</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#31995;&#24863;&#30693;&#30340;&#35821;&#35328;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Relation-Aware Language-Graph Transformer for Question Answering. (arXiv:2212.00975v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#31995;&#24863;&#30693;&#35821;&#35328;&#22270;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#32852;&#21512;&#25512;&#29702;&#35821;&#35328;&#21644;&#22270;&#20851;&#20110;&#23454;&#20307;&#20851;&#31995;&#65292;&#24182;&#22312;&#22810;&#20010;QA&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#26159;&#19968;&#39033;&#38656;&#35201;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#29615;&#22659;&#30340;&#20219;&#21153;&#65292;&#35768;&#22810;&#30456;&#20851;&#24037;&#20316;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;(LM)&#65292;&#20197;&#23545;&#30693;&#35782;&#22270;&#35889;(KG)&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38754;&#21521;&#38382;&#31572;&#30340;&#22522;&#20110;GNN&#30340;&#27169;&#22359;&#24182;&#26410;&#21033;&#29992;KG&#30340;&#20016;&#23500;&#20851;&#31995;&#20449;&#24687;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;LM&#21644;KG&#20043;&#38388;&#30340;&#26377;&#38480;&#20449;&#24687;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Question Answering Transformer(QAT)&#65292;&#23427;&#26088;&#22312;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#32852;&#21512;&#25512;&#29702;&#35821;&#35328;&#21644;&#22270;&#20851;&#20110;&#23454;&#20307;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;QAT&#26500;&#24314;&#20102;&#20803;&#36335;&#24452;&#20196;&#29260;&#65292;&#36825;&#20123;&#20196;&#29260;&#23398;&#20064;&#22522;&#20110;&#19981;&#21516;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20851;&#31995;&#30340;&#20851;&#31995;&#20013;&#24515;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#20851;&#31995;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#36890;&#36807;&#36328;&#27169;&#24577;&#30456;&#20851;&#20301;&#32622;&#20559;&#24046;&#20840;&#38754;&#25972;&#21512;&#20102;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#20197;&#25351;&#23548;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30456;&#20851;&#23454;&#20307;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;QAT&#22312;&#22810;&#20010;QA&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question Answering (QA) is a task that entails reasoning over natural language contexts, and many relevant works augment language models (LMs) with graph neural networks (GNNs) to encode the Knowledge Graph (KG) information. However, most existing GNN-based modules for QA do not take advantage of rich relational information of KGs and depend on limited information interaction between the LM and the KG. To address these issues, we propose Question Answering Transformer (QAT), which is designed to jointly reason over language and graphs with respect to entity relations in a unified manner. Specifically, QAT constructs Meta-Path tokens, which learn relation-centric embeddings based on diverse structural and semantic relations. Then, our Relation-Aware Self-Attention module comprehensively integrates different modalities via the Cross-Modal Relative Position Bias, which guides information exchange between relevant entites of different modalities. We validate the effectiveness of QAT on com
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#36716;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#23618;&#35821;&#35328;&#30446;&#26631;&#21644;&#23616;&#37096;&#35270;&#28857;&#20113;&#26500;&#24314;&#29289;&#29702;&#26377;&#25928;&#30340;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27493;&#39588;3D&#35268;&#21010;&#20219;&#21153;&#65292;&#21363;&#20351;&#20351;&#29992;&#26410;&#30693;&#23545;&#35937;&#20173;&#33021;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.04604</link><description>&lt;p&gt;
StructDiffusion&#65306;&#20351;&#29992;&#26410;&#30693;&#23545;&#35937;&#36827;&#34892;&#29289;&#29702;&#26377;&#25928;&#32467;&#26500;&#30340;&#35821;&#35328;&#25351;&#23548;&#21019;&#24314;
&lt;/p&gt;
&lt;p&gt;
StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects. (arXiv:2211.04604v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#36716;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#23618;&#35821;&#35328;&#30446;&#26631;&#21644;&#23616;&#37096;&#35270;&#28857;&#20113;&#26500;&#24314;&#29289;&#29702;&#26377;&#25928;&#30340;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27493;&#39588;3D&#35268;&#21010;&#20219;&#21153;&#65292;&#21363;&#20351;&#20351;&#29992;&#26410;&#30693;&#23545;&#35937;&#20173;&#33021;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#29615;&#22659;&#20013;&#36816;&#20316;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#33021;&#22815;&#23558;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#25104;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#37197;&#32622;&#65292;&#21363;&#20351;&#36825;&#20123;&#29289;&#20307;&#20197;&#21069;&#27809;&#35265;&#36807;&#12290;&#26412;&#25991;&#20851;&#27880;&#22914;&#20309;&#22312;&#26080;&#38656;&#36880;&#27493;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#29289;&#29702;&#26377;&#25928;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;StructDiffusion&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25193;&#25955;&#27169;&#22411;&#21644;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#36716;&#25442;&#22120;&#65292;&#26681;&#25454;&#23616;&#37096;&#35270;&#28857;&#20113;&#21644;&#39640;&#32423;&#35821;&#35328;&#30446;&#26631;&#65288;&#22914;&#8220;&#25670;&#26700;&#23376;&#8221;&#65289;&#65292;&#26500;&#24314;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#25191;&#34892;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35821;&#35328;&#26465;&#20214;&#30340;&#22810;&#27493;&#39588;3D&#35268;&#21010;&#20219;&#21153;&#12290;&#19982;&#35757;&#32451;&#22312;&#29305;&#23450;&#32467;&#26500;&#19978;&#30340;&#29616;&#26377;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;StructDiffusion&#29978;&#33267;&#25552;&#39640;&#20102;&#23558;&#26410;&#30693;&#23545;&#35937;&#32452;&#35013;&#25104;&#29289;&#29702;&#26377;&#25928;&#32467;&#26500;&#30340;&#25104;&#21151;&#29575;&#65292;&#24179;&#22343;&#21487;&#25552;&#39640;16&#65285;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#25311;&#21644;&#23454;&#38469;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#20013;&#20351;&#29992;&#20445;&#30041;&#23545;&#35937;&#30340;&#23454;&#39564;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25193;&#25955;&#27169;&#22411;&#21644;&#30896;&#25758;&#37492;&#21035;&#22120;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#23454;&#29616;&#23545;&#25311;&#21512;&#24615;&#20197;&#21450;&#23545;&#35821;&#35328;&#25351;&#23548;&#29289;&#29702;&#26377;&#25928;&#32467;&#26500;&#26500;&#24314;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this work, we focus on the problem of building physically-valid structures without step-by-step instructions. We propose StructDiffusion, which combines a diffusion model and an object-centric transformer to construct structures given partial-view point clouds and high-level language goals, such as "set the table". Our method can perform multiple challenging language-conditioned multi-step 3D planning tasks using one model. StructDiffusion even improves the success rate of assembling physically-valid structures out of unseen objects by on average 16% over an existing multi-modal transformer model trained on specific structures. We show experiments on held-out objects in both simulation and on real-world rearrangement tasks. Importantly, we show how integrating both a diffusion model and a collision-discriminator model allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Aug-imodels &#26694;&#26550;&#65292;&#21033;&#29992; LLMs &#30340;&#30693;&#35782;&#22312;&#25311;&#21512;&#36807;&#31243;&#20013;&#26500;&#24314;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992; LLMs&#65292;&#20855;&#22791;&#23436;&#20840;&#30340;&#36879;&#26126;&#24615;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;&#26041;&#24335;&#30340;&#23454;&#29616;&#65292;&#24182;&#22312;&#22810;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.11799</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#26469;&#22686;&#24378;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Augmenting Interpretable Models with LLMs during Training. (arXiv:2209.11799v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Aug-imodels &#26694;&#26550;&#65292;&#21033;&#29992; LLMs &#30340;&#30693;&#35782;&#22312;&#25311;&#21512;&#36807;&#31243;&#20013;&#26500;&#24314;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992; LLMs&#65292;&#20855;&#22791;&#23436;&#20840;&#30340;&#36879;&#26126;&#24615;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;&#26041;&#24335;&#30340;&#23454;&#29616;&#65292;&#24182;&#22312;&#22810;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36827;&#20837;&#39640;&#39118;&#38505;&#39046;&#22495;&#65288;&#20363;&#22914;&#21307;&#23398;&#65289;&#21644;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#65292;&#23545;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#21487;&#35299;&#37322;&#27169;&#22411;&#65288;Aug-imodels&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#25152;&#23398;&#20064;&#30340;&#30693;&#35782;&#24314;&#31435;&#26497;&#20854;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;Aug-imodels&#22312;&#25311;&#21512;&#36807;&#31243;&#20013;&#20351;&#29992;LLMs&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23436;&#20840;&#30340;&#36879;&#26126;&#24615;&#65292;&#24182;&#19988;&#19982;LLMs&#30456;&#27604;&#65292;&#25512;&#29702;&#36895;&#24230;&#21644;&#20869;&#23384;&#24615;&#33021;&#26377;&#20102;&#22823;&#20110;1000&#20493;&#30340;&#25552;&#39640;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#31181;Aug-imodels&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20855;&#20307;&#23454;&#20363;&#65306;&#65288;i&#65289;Aug-GAM&#65292;&#23427;&#20351;&#29992;&#26469;&#33258;LLM&#30340;&#35299;&#32806;&#23884;&#20837;&#26469;&#22686;&#24378;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65307;&#65288;ii&#65289;Aug-Tree&#65292;&#23427;&#36890;&#36807;LLM&#29305;&#24449;&#25193;&#23637;&#26469;&#22686;&#24378;&#20915;&#31574;&#26641;&#12290;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20248;&#20110;&#20854;&#26410;&#22686;&#24378;&#30340;&#23545;&#29031;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their proliferation into high-stakes domains (e.g. medicine) and compute-limited settings has created a burgeoning need for interpretability and efficiency. We address this need by proposing Augmented Interpretable Models (Aug-imodels), a framework for leveraging the knowledge learned by LLMs to build extremely efficient and interpretable models. Aug-imodels use LLMs during fitting but not during inference, allowing complete transparency and often a speed/memory improvement of greater than 1,000x for inference compared to LLMs. We explore two instantiations of Aug-imodels in natural-language processing: (i) Aug-GAM, which augments a generalized additive model with decoupled embeddings from an LLM and (ii) Aug-Tree, which augments a decision tree with LLM feature expansions. Across a variety of text-classification datasets, both outperform their non-augmented co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#25277;&#35937;&#20250;&#35758;&#25688;&#35201;&#30340;&#35843;&#26597;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#25688;&#35201;&#24418;&#24335;&#22312;&#22810;&#26041;&#35848;&#35805;&#20013;&#23588;&#20854;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.04163</link><description>&lt;p&gt;
&#25277;&#35937;&#20250;&#35758;&#25688;&#35201;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Abstractive Meeting Summarization: A Survey. (arXiv:2208.04163v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#25277;&#35937;&#20250;&#35758;&#25688;&#35201;&#30340;&#35843;&#26597;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#25688;&#35201;&#24418;&#24335;&#22312;&#22810;&#26041;&#35848;&#35805;&#20013;&#23588;&#20854;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21487;&#38752;&#22320;&#35782;&#21035;&#21644;&#24635;&#32467;&#35848;&#35805;&#20013;&#26368;&#37325;&#35201;&#30340;&#35201;&#28857;&#30340;&#31995;&#32479;&#23558;&#22312;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#19979;&#20855;&#26377;&#20215;&#20540;&#65292;&#20174;&#21830;&#19994;&#20250;&#35758;&#21040;&#21307;&#30103;&#21672;&#35810;&#21040;&#23458;&#25143;&#26381;&#21153;&#21628;&#21483;&#12290;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#21457;&#26126;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#25552;&#39640;&#25277;&#35937;&#27010;&#25324;&#30340;&#24418;&#24335;&#24320;&#20102;&#19968;&#25159;&#38376;&#65292;&#36825;&#31181;&#24418;&#24335;&#30340;&#27010;&#25324;&#23588;&#20854;&#36866;&#21512;&#22810;&#26041;&#35848;&#35805;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#25277;&#35937;&#20250;&#35758;&#25688;&#35201;&#20219;&#21153;&#25152;&#24341;&#36215;&#30340;&#25361;&#25112;&#30340;&#27010;&#36848;&#65292;&#20197;&#21450;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
A system that could reliably identify and sum up the most important points of a conversation would be valuable in a wide variety of real-world contexts, from business meetings to medical consultations to customer service calls. Recent advances in deep learning, and especially the invention of encoder-decoder architectures, has significantly improved language generation systems, opening the door to improved forms of abstractive summarization, a form of summarization particularly well-suited for multi-party conversation. In this paper, we provide an overview of the challenges raised by the task of abstractive meeting summarization and of the data sets, models and evaluation metrics that have been used to tackle the problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DRAGNET++, &#36890;&#36807;&#32771;&#34385;&#23545;&#35805;&#32447;&#31243;&#30340;&#35821;&#20041;&#12289;&#20256;&#25773;&#32467;&#26500;&#21644;&#29992;&#25143;&#20132;&#20114;&#65292;&#20197;&#39044;&#27979;&#25512;&#25991;&#30340;&#22238;&#22797;&#38142;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20167;&#24680;&#31243;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21487;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25552;&#20379;&#22312;&#24694;&#24847;&#23545;&#35805;&#21319;&#32423;&#20043;&#21069;&#35782;&#21035;&#21644;&#31649;&#29702;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2206.08406</link><description>&lt;p&gt;
&#39044;&#27979;Twitter&#23545;&#35805;&#32447;&#31243;&#20013;&#30340;&#20167;&#24680;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DRAGNET++, &#36890;&#36807;&#32771;&#34385;&#23545;&#35805;&#32447;&#31243;&#30340;&#35821;&#20041;&#12289;&#20256;&#25773;&#32467;&#26500;&#21644;&#29992;&#25143;&#20132;&#20114;&#65292;&#20197;&#39044;&#27979;&#25512;&#25991;&#30340;&#22238;&#22797;&#38142;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20167;&#24680;&#31243;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21487;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25552;&#20379;&#22312;&#24694;&#24847;&#23545;&#35805;&#21319;&#32423;&#20043;&#21069;&#35782;&#21035;&#21644;&#31649;&#29702;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#25991;&#26159;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#20013;&#26368;&#31616;&#27905;&#30340;&#20132;&#27969;&#24418;&#24335;&#65292;&#19968;&#26465;&#25512;&#25991;&#26377;&#21487;&#33021;&#26159;&#23545;&#35805;&#20013;&#25171;&#36896;&#25110;&#30772;&#22351;&#35752;&#35770;&#30340;&#28508;&#22312;&#23186;&#20171;&#12290;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#23481;&#26131;&#33719;&#24471;&#65292;&#38459;&#27490;&#20854;&#20256;&#25773;&#23545;&#20110;&#31038;&#20132;&#23186;&#20307;&#20844;&#21496;&#21644;&#29992;&#25143;&#26469;&#35828;&#26159;&#26497;&#20026;&#37325;&#35201;&#30340;&#65292;&#21487;&#20197;&#25512;&#36827;&#33391;&#22909;&#30340;&#20132;&#27969;&#26041;&#24335;&#12290;&#30446;&#21069;&#38500;&#20102;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#22806;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#19987;&#27880;&#20110;&#20998;&#31867;&#21333;&#20010;&#25512;&#25991;&#65292;&#32780;&#24573;&#30053;&#20102;&#25512;&#25991;&#20043;&#38388;&#30340;&#23545;&#35805;&#32447;&#31243;/&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGNET++&#65292;&#26088;&#22312;&#36890;&#36807;&#25512;&#25991;&#30340;&#22238;&#22797;&#38142;&#39044;&#27979;&#23427;&#21487;&#33021;&#24102;&#26469;&#30340;&#20167;&#24680;&#31243;&#24230;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#23545;&#35805;&#32447;&#31243;&#30340;&#35821;&#20041;&#21644;&#20256;&#25773;&#32467;&#26500;&#20197;&#21450;&#32447;&#31243;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;DRAGNET++&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35748;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21487;&#20197;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#39044;&#27979;&#21644;&#31649;&#29702;&#24694;&#24847;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tweets are the most concise form of communication in online social media, wherein a single tweet has the potential to make or break the discourse of the conversation. Online hate speech is more accessible than ever, and stifling its propagation is of utmost importance for social media companies and users for congenial communication. Most of the research barring a recent few has focused on classifying an individual tweet regardless of the tweet thread/context leading up to that point. One of the classical approaches to curb hate speech is to adopt a reactive strategy after the hate speech postage. The ex-post facto strategy results in neglecting subtle posts that do not show the potential to instigate hate speech on their own but may portend in the subsequent discussion ensuing in the post's replies. In this paper, we propose DRAGNET++, which aims to predict the intensity of hatred that a tweet can bring in through its reply chain in the future. It uses the semantic and propagating stru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#25277;&#35937;&#21270;&#25688;&#35201;&#20013;&#25277;&#35937;&#24615;&#21644;&#20107;&#23454;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#21253;&#21547;&#20154;&#31867;&#20107;&#23454;&#21028;&#26029;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#34429;&#28982;&#25277;&#35937;&#24615;&#30340;&#22686;&#21152;&#36890;&#24120;&#20250;&#23548;&#33268;&#20107;&#23454;&#24615;&#30340;&#19979;&#38477;&#65292;&#20294;&#20107;&#23454;&#24615;&#30340;&#34928;&#20943;&#29575;&#21462;&#20915;&#20110;&#31995;&#32479;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#26032;&#30340;&#20107;&#23454;&#24230;&#25351;&#26631;&#21487;&#20197;&#35843;&#25972;&#25277;&#35937;&#31243;&#24230;&#30340;&#31243;&#24230;&#65292;&#24357;&#34917;&#20107;&#23454;&#24615;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2108.02859</link><description>&lt;p&gt;
&#22312;&#25277;&#35937;&#21270;&#25688;&#35201;&#20013;&#26435;&#34913;&#25277;&#35937;&#24615;&#21644;&#20107;&#23454;&#24615;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Tradeoff Between Abstractiveness and Factuality in Abstractive Summarization. (arXiv:2108.02859v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.02859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#25277;&#35937;&#21270;&#25688;&#35201;&#20013;&#25277;&#35937;&#24615;&#21644;&#20107;&#23454;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#21253;&#21547;&#20154;&#31867;&#20107;&#23454;&#21028;&#26029;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#34429;&#28982;&#25277;&#35937;&#24615;&#30340;&#22686;&#21152;&#36890;&#24120;&#20250;&#23548;&#33268;&#20107;&#23454;&#24615;&#30340;&#19979;&#38477;&#65292;&#20294;&#20107;&#23454;&#24615;&#30340;&#34928;&#20943;&#29575;&#21462;&#20915;&#20110;&#31995;&#32479;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#26032;&#30340;&#20107;&#23454;&#24230;&#25351;&#26631;&#21487;&#20197;&#35843;&#25972;&#25277;&#35937;&#31243;&#24230;&#30340;&#31243;&#24230;&#65292;&#24357;&#34917;&#20107;&#23454;&#24615;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#21270;&#25688;&#35201;&#30340;&#31070;&#32463;&#27169;&#22411;&#24448;&#24448;&#20250;&#29983;&#25104;&#27969;&#30021;&#32780;&#24418;&#24335;&#33391;&#22909;&#30340;&#36755;&#20986;&#65292;&#20294;&#19982;&#36755;&#20837;&#25991;&#26723;&#30340;&#35821;&#20041;&#24544;&#23454;&#24230;&#25110;&#20107;&#23454;&#24615;&#19981;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#20107;&#23454;&#24615;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#20998;&#26512;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#25688;&#35201;&#20013;&#25277;&#35937;&#24615;&#21644;&#20107;&#23454;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35299;&#30721;&#32422;&#26463;&#36880;&#28176;&#22686;&#21152;&#25277;&#35937;&#24615;&#65292;&#21487;&#35270;&#21270;&#20102;&#20107;&#23454;&#24615;&#30340;&#21464;&#21270;&#29575;&#65292;&#24182;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;&#25277;&#35937;&#24615;&#30340;&#22686;&#21152;&#36890;&#24120;&#20250;&#23548;&#33268;&#20107;&#23454;&#24615;&#30340;&#19979;&#38477;&#65292;&#20294;&#20107;&#23454;&#24615;&#30340;&#34928;&#20943;&#29575;&#21462;&#20915;&#20110;&#31995;&#32479;&#30340;&#35757;&#32451;&#25968;&#25454;&#31561;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21253;&#21547;&#20154;&#31867;&#20107;&#23454;&#21028;&#26029;&#30340;&#25968;&#25454;&#38598;&#65307;&#19968;&#20010;&#21253;&#21547;10.2k&#20010;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#20855;&#26377;&#31995;&#32479;&#22320;&#21464;&#21270;&#30340;&#25277;&#35937;&#31243;&#24230;&#65307;&#21478;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#25688;&#35201;&#27169;&#22411;&#30340;4.2k&#20010;&#25688;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#20107;&#23454;&#24230;&#25351;&#26631;&#65292;&#35843;&#25972;&#20102;&#25277;&#35937;&#31243;&#24230;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural models for abstractive summarization tend to generate output that is fluent and well-formed but lacks semantic faithfulness, or factuality, with respect to the input documents. In this paper, we analyze the tradeoff between abstractiveness and factuality of generated summaries across multiple datasets and models, using extensive human evaluations of factuality. In our analysis, we visualize the rates of change in factuality as we gradually increase abstractiveness using a decoding constraint, and we observe that, while increased abstractiveness generally leads to a drop in factuality, the rate of factuality decay depends on factors such as the data that the system was trained on. We introduce two datasets with human factuality judgements; one containing 10.2k generated summaries with systematically varied degrees of abstractiveness; the other containing 4.2k summaries from five different summarization models. We propose new factuality metrics that adjust for the degree of abstra
&lt;/p&gt;</description></item></channel></rss>