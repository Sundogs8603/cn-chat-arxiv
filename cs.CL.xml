<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#23457;&#35745;&#26085;&#24535;&#20316;&#20026;&#30417;&#30563;&#65292;&#23454;&#29616;&#22312;&#29305;&#23450;&#20020;&#24202;&#32972;&#26223;&#19979;&#12289;&#29305;&#23450;&#26102;&#38388;&#28857;&#30340;&#31508;&#35760;&#30456;&#20851;&#24615;&#26816;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20010;&#21035;&#31508;&#35760;&#25776;&#20889;&#20250;&#35805;&#20013;&#21738;&#20123;&#31508;&#35760;&#20250;&#34987;&#38405;&#35835;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20020;&#24202;&#21307;&#29983;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#35813;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#39640;&#25928;&#22320;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.08494</link><description>&lt;p&gt;
&#29992;&#20110;&#21160;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20449;&#24687;&#26816;&#32034;&#30340;&#26426;&#22120;&#23398;&#20064;&#27010;&#24565;&#21270;
&lt;/p&gt;
&lt;p&gt;
Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes. (arXiv:2308.08494v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08494
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#23457;&#35745;&#26085;&#24535;&#20316;&#20026;&#30417;&#30563;&#65292;&#23454;&#29616;&#22312;&#29305;&#23450;&#20020;&#24202;&#32972;&#26223;&#19979;&#12289;&#29305;&#23450;&#26102;&#38388;&#28857;&#30340;&#31508;&#35760;&#30456;&#20851;&#24615;&#26816;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20010;&#21035;&#31508;&#35760;&#25776;&#20889;&#20250;&#35805;&#20013;&#21738;&#20123;&#31508;&#35760;&#20250;&#34987;&#38405;&#35835;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20020;&#24202;&#21307;&#29983;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#35813;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#39640;&#25928;&#22320;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#21307;&#29983;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#31579;&#36873;&#30149;&#20154;&#31508;&#35760;&#24182;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#35760;&#24405;&#26159;&#20020;&#24202;&#21307;&#29983;&#20518;&#24608;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#36890;&#36807;&#22312;&#35760;&#24405;&#36807;&#31243;&#20013;&#20027;&#21160;&#21644;&#21160;&#24577;&#22320;&#26816;&#32034;&#30456;&#20851;&#31508;&#35760;&#65292;&#25105;&#20204;&#21487;&#20197;&#20943;&#23569;&#26597;&#25214;&#30456;&#20851;&#30149;&#20363;&#21382;&#21490;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27010;&#24565;&#21270;&#20102;&#20351;&#29992;EHR&#23457;&#35745;&#26085;&#24535;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#26469;&#28304;&#65292;&#20197;&#30417;&#30563;&#29305;&#23450;&#20020;&#24202;&#32972;&#26223;&#19979;&#12289;&#29305;&#23450;&#26102;&#38388;&#28857;&#30340;&#31508;&#35760;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37325;&#28857;&#25918;&#22312;&#32039;&#24613;&#31185;&#23460;&#30340;&#21160;&#24577;&#26816;&#32034;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#29420;&#29305;&#20449;&#24687;&#26816;&#32034;&#21644;&#31508;&#35760;&#32534;&#20889;&#27169;&#24335;&#30340;&#39640;&#37325;&#30151;&#35774;&#32622;&#12290;&#25105;&#20204;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#21738;&#20123;&#31508;&#35760;&#20250;&#22312;&#20010;&#21035;&#31508;&#35760;&#25776;&#20889;&#20250;&#35805;&#20013;&#34987;&#38405;&#35835;&#26041;&#38754;&#21487;&#20197;&#23454;&#29616;0.963&#30340;AUC&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#22810;&#21517;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#39640;&#25928;&#22320;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#12290;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The large amount of time clinicians spend sifting through patient notes and documenting in electronic health records (EHRs) is a leading cause of clinician burnout. By proactively and dynamically retrieving relevant notes during the documentation process, we can reduce the effort required to find relevant patient history. In this work, we conceptualize the use of EHR audit logs for machine learning as a source of supervision of note relevance in a specific clinical context, at a particular point in time. Our evaluation focuses on the dynamic retrieval in the emergency department, a high acuity setting with unique patterns of information retrieval and note writing. We show that our methods can achieve an AUC of 0.963 for predicting which notes will be read in an individual note writing session. We additionally conduct a user study with several clinicians and find that our framework can help clinicians retrieve relevant information more efficiently. Demonstrating that our framework and m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2308.08493</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#26102;&#38388;&#26053;&#34892;&#65306;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#26159;&#25351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#29702;&#35299;LLMs&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#26377;&#25928;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#36890;&#36807;&#35782;&#21035;&#20174;&#23567;&#30340;&#38543;&#26426;&#26679;&#26412;&#20013;&#25277;&#21462;&#30340;&#21333;&#20010;&#23454;&#20363;&#20013;&#30340;&#28508;&#22312;&#27745;&#26579;&#65292;&#28982;&#21518;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20272;&#35745;&#21333;&#20010;&#23454;&#20363;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#65306;&#21363;&#19968;&#20010;&#30001;&#25968;&#25454;&#38598;&#21517;&#31216;&#12289;&#20998;&#21306;&#31867;&#22411;&#21644;&#21442;&#32771;&#23454;&#20363;&#30340;&#21021;&#22987;&#37096;&#20998;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#35201;&#27714;LLM&#23436;&#25104;&#23427;&#12290;&#22914;&#26524;LLM&#30340;&#36755;&#20986;&#19982;&#21442;&#32771;&#23454;&#20363;&#30340;&#21518;&#19968;&#37096;&#20998;&#23436;&#20840;&#25110;&#25509;&#36817;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#23454;&#20363;&#34987;&#26631;&#35760;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20102;&#35299;&#25972;&#20010;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24819;&#27861;&#12290;&#31532;&#19968;&#20010;&#24819;&#27861;&#26159;&#26631;&#35760;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#21306;&#65292;&#35813;&#20998;&#21306;&#20013;&#30340;&#23454;&#20363;&#22823;&#22810;&#25968;&#37117;&#34987;&#21028;&#26029;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#22522;&#20110;&#22068;&#21767;-&#38899;&#32032;&#23383;&#32423;&#30456;&#20851;&#24615;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#30340;&#20004;&#31181;&#26032;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20934;&#30830;&#23545;&#40784;&#38899;&#39057;&#21644;&#35270;&#39057;&#27969;&#65292;&#24182;&#19988;&#20805;&#20998;&#21033;&#29992;&#27169;&#24577;&#20114;&#34917;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08488</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22068;&#21767;-&#38899;&#32032;&#23383;&#32423;&#30456;&#20851;&#24615;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder. (arXiv:2308.08488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#22522;&#20110;&#22068;&#21767;-&#38899;&#32032;&#23383;&#32423;&#30456;&#20851;&#24615;&#30340;&#35270;&#35273;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#30340;&#20004;&#31181;&#26032;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20934;&#30830;&#23545;&#40784;&#38899;&#39057;&#21644;&#35270;&#39057;&#27969;&#65292;&#24182;&#19988;&#20805;&#20998;&#21033;&#29992;&#27169;&#24577;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#35266;&#23519;&#21040;&#65292;&#22312;&#20302;&#36136;&#37327;&#35270;&#39057;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#19979;&#65292;&#20174;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#21040;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#30053;&#26377;&#25913;&#36827;&#12290;&#25454;&#35748;&#20026;&#65292;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#20043;&#38388;&#19981;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#19987;&#38376;&#30340;&#36755;&#20837;&#34920;&#31034;&#23548;&#33268;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#21548;&#35821;&#38899;&#35782;&#21035;&#65288;AVSR&#65289;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#35757;&#32451;&#26694;&#26550;&#19979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26222;&#36890;&#35805;&#20013;&#22068;&#21767;&#24418;&#29366;&#21644;&#38899;&#33410;&#32423;&#38899;&#32032;&#23383;&#21333;&#20803;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#24314;&#31435;&#20934;&#30830;&#30340;&#24103;&#32423;&#38899;&#33410;&#36793;&#30028;&#12290;&#36825;&#20351;&#24471;&#22312;&#35270;&#35273;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#36328;&#27169;&#24577;&#34701;&#21512;&#36807;&#31243;&#20013;&#33021;&#22815;&#23545;&#40784;&#35270;&#39057;&#21644;&#38899;&#39057;&#27969;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#39057;&#24341;&#23548;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#32534;&#30721;&#22120;&#65288;CMFE&#65289;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#20027;&#35201;&#35757;&#32451;&#21442;&#25968;&#26469;&#23454;&#29616;&#22810;&#20010;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#23618;&#30340;&#20805;&#20998;&#21033;&#29992;&#27169;&#24577;&#20114;&#34917;&#24615;&#12290;&#22312;&#23454;&#39564;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
In recent research, slight performance improvement is observed from automatic speech recognition systems to audio-visual speech recognition systems in the end-to-end framework with low-quality videos. Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem. In this paper, we propose two novel techniques to improve audio-visual speech recognition (AVSR) under a pre-training and fine-tuning training framework. First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes. This enables accurate alignment of video and audio streams during visual model pre-training and cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity. Experiments on the
&lt;/p&gt;</description></item><item><title>TBIN&#27169;&#22411;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#31639;&#27861;&#21644;&#22522;&#20110;&#22359;&#20301;&#31227;&#30340;&#33258;&#27880;&#24847;&#21147;&#26041;&#27861;&#35299;&#20915;&#20102;&#21033;&#29992;&#38271;&#25991;&#26412;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#36827;&#34892;CTR&#39044;&#27979;&#26102;&#30340;&#25130;&#26029;&#38382;&#39064;&#21644;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08483</link><description>&lt;p&gt;
TBIN: &#27169;&#22411;&#21270;&#38271;&#25991;&#26412;&#34892;&#20026;&#25968;&#25454;&#29992;&#20110;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TBIN: Modeling Long Textual Behavior Data for CTR Prediction. (arXiv:2308.08483v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08483
&lt;/p&gt;
&lt;p&gt;
TBIN&#27169;&#22411;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#31639;&#27861;&#21644;&#22522;&#20110;&#22359;&#20301;&#31227;&#30340;&#33258;&#27880;&#24847;&#21147;&#26041;&#27861;&#35299;&#20915;&#20102;&#21033;&#29992;&#38271;&#25991;&#26412;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#36827;&#34892;CTR&#39044;&#27979;&#26102;&#30340;&#25130;&#26029;&#38382;&#39064;&#21644;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#21463;&#21040;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#32321;&#33635;&#24433;&#21709;&#65292;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#20197;&#25991;&#26412;&#26684;&#24335;&#32452;&#32455;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;LMs&#26469;&#22312;&#35821;&#20041;&#23618;&#38754;&#19978;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#26469;&#25913;&#36827;&#39044;&#27979;&#12290;&#34429;&#28982;&#26377;&#21069;&#26223;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#19981;&#24471;&#19981;&#25130;&#26029;&#25991;&#26412;&#25968;&#25454;&#20197;&#20943;&#23569;LMs&#20013;&#33258;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#35745;&#31639;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#30740;&#31350;&#34920;&#26126;&#38271;&#26102;&#38388;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;CTR&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#24037;&#20316;&#36890;&#24120;&#23558;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20852;&#36259;&#21387;&#32553;&#25104;&#19968;&#20010;&#29305;&#24449;&#21521;&#37327;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#34892;&#20026;&#30340;&#20852;&#36259;&#20999;&#22359;&#32593;&#32476;&#65288;TBIN&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#39640;&#25928;&#30340;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#31639;&#27861;&#21644;&#22522;&#20110;&#22359;&#20301;&#31227;&#30340;&#33258;&#27880;&#24847;&#21147;&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;&#24471;&#21040;&#30340;&#29992;&#25143;&#22810;&#26679;&#21270;&#20852;&#36259;&#26159;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction plays a pivotal role in the success of recommendations. Inspired by the recent thriving of language models (LMs), a surge of works improve prediction by organizing user behavior data in a \textbf{textual} format and using LMs to understand user interest at a semantic level. While promising, these works have to truncate the textual data to reduce the quadratic computational overhead of self-attention in LMs. However, it has been studied that long user behavior data can significantly benefit CTR prediction. In addition, these works typically condense user diverse interests into a single feature vector, which hinders the expressive capability of the model. In this paper, we propose a \textbf{T}extual \textbf{B}ehavior-based \textbf{I}nterest Chunking \textbf{N}etwork (TBIN), which tackles the above limitations by combining an efficient locality-sensitive hashing algorithm and a shifted chunk-based self-attention. The resulting user diverse interests are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#38598;&#25104;CTC&#21644;&#36741;&#21161;&#25439;&#22833;&#27491;&#21017;&#21270;&#25913;&#36827;CTC-AED&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DAL&#26041;&#27861;&#22312;&#27880;&#24847;&#21147;&#37325;&#35780;&#20998;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;PMP&#26041;&#27861;&#22312;CTC&#21069;&#32512;&#25628;&#32034;&#21644;&#36138;&#23146;&#25628;&#32034;&#20013;&#34920;&#29616;&#21331;&#36234;&#12290;</title><link>http://arxiv.org/abs/2308.08449</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;CTC&#21644;&#36741;&#21161;&#25439;&#22833;&#27491;&#21017;&#21270;&#25913;&#36827;CTC-AED&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving CTC-AED model with integrated-CTC and auxiliary loss regularization. (arXiv:2308.08449v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#38598;&#25104;CTC&#21644;&#36741;&#21161;&#25439;&#22833;&#27491;&#21017;&#21270;&#25913;&#36827;CTC-AED&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DAL&#26041;&#27861;&#22312;&#27880;&#24847;&#21147;&#37325;&#35780;&#20998;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;PMP&#26041;&#27861;&#22312;CTC&#21069;&#32512;&#25628;&#32034;&#21644;&#36138;&#23146;&#25628;&#32034;&#20013;&#34920;&#29616;&#21331;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#65288;AED&#65289;&#30340;&#32852;&#21512;&#35757;&#32451;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#19982;&#22823;&#22810;&#25968;&#20998;&#24320;&#35745;&#31639;CTC&#21644;AED&#25439;&#22833;&#30340;&#28151;&#21512;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#38598;&#25104;CTC&#21033;&#29992;AED&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25351;&#23548;CTC&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#37319;&#29992;&#20004;&#31181;&#34701;&#21512;&#26041;&#27861;&#65292;&#21363;&#30452;&#25509;&#30456;&#21152;&#30340;logits&#65288;DAL&#65289;&#21644;&#20445;&#30041;&#26368;&#22823;&#27010;&#29575;&#65288;PMP&#65289;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#20223;&#23556;&#21464;&#25442;&#27880;&#24847;&#32467;&#26524;&#20197;&#21305;&#37197;CTC&#30340;&#32500;&#24230;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#32500;&#24230;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36741;&#21161;&#25439;&#22833;&#27491;&#21017;&#21270;&#20197;&#21152;&#36895;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DAL&#26041;&#27861;&#22312;&#27880;&#24847;&#21147;&#37325;&#35780;&#20998;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;PMP&#26041;&#27861;&#22312;CTC&#21069;&#32512;&#25628;&#32034;&#21644;&#36138;&#23146;&#25628;&#32034;&#20013;&#34920;&#29616;&#21331;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectionist temporal classification (CTC) and attention-based encoder decoder (AED) joint training has been widely applied in automatic speech recognition (ASR). Unlike most hybrid models that separately calculate the CTC and AED losses, our proposed integrated-CTC utilizes the attention mechanism of AED to guide the output of CTC. In this paper, we employ two fusion methods, namely direct addition of logits (DAL) and preserving the maximum probability (PMP). We achieve dimensional consistency by adaptively affine transforming the attention results to match the dimensions of CTC. To accelerate model convergence and improve accuracy, we introduce auxiliary loss regularization for accelerated convergence. Experimental results demonstrate that the DAL method performs better in attention rescoring, while the PMP method excels in CTC prefix beam search and greedy search.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#26333;&#20809;&#20559;&#24046;&#30340;&#25439;&#22833;&#25277;&#26679;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#21477;&#23376;&#32423;&#21644;&#27573;&#33853;&#32423;&#38899;&#32032;&#36716;&#25442;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08442</link><description>&lt;p&gt;
&#20943;&#36731;&#21477;&#23376;&#32423;&#38899;&#32032;&#36716;&#25442;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction. (arXiv:2308.08442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#26333;&#20809;&#20559;&#24046;&#30340;&#25439;&#22833;&#25277;&#26679;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#21477;&#23376;&#32423;&#21644;&#27573;&#33853;&#32423;&#38899;&#32032;&#36716;&#25442;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#36716;&#25991;&#26412;&#20256;&#36755;&#36716;&#25442;&#22120; (T5) &#26368;&#36817;&#34987;&#32771;&#34385;&#29992;&#20110;&#38899;&#32032;&#22270;( G2P )&#36716;&#25442;&#12290;&#20316;&#20026;&#21518;&#32493;&#30740;&#31350;&#65292;&#19968;&#31181;&#22522;&#20110; T5 &#30340;&#26080;&#20998;&#35789;&#23383;&#33410;&#32423;&#27169;&#22411; ByT5&#65292;&#22312;&#34920;&#31034;&#27599;&#20010;&#36755;&#20837;&#23383;&#31526;&#26102;&#20351;&#29992;&#20854;&#30456;&#24212;&#30340; UTF-8 &#32534;&#30721;&#65292;&#26368;&#36817;&#22312;&#21333;&#35789;&#32423; G2P &#36716;&#25442;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#21477;&#23376;&#32423;&#25110;&#27573;&#33853;&#32423; G2P &#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#33021;&#25552;&#39640;&#21487;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#26356;&#36866;&#21512;&#22788;&#29702;&#24322;&#38899;&#23383;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#36830;&#25509;&#38899;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#20351;&#29992; ByT5 &#24182;&#19981;&#31616;&#21333;&#12290;&#30001;&#20110; ByT5 &#22312;&#23383;&#31526;&#32423;&#21035;&#19978;&#25805;&#20316;&#65292;&#23427;&#38656;&#35201;&#36739;&#38271;&#30340;&#35299;&#30721;&#27493;&#39588;&#65292;&#36825;&#20250;&#22240;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#20013;&#24120;&#35265;&#30340;&#26333;&#20809;&#20559;&#24046;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#25439;&#22833;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#36890;&#36807;&#20943;&#36731;&#36825;&#31181;&#26333;&#20809;&#20559;&#24046;&#21487;&#20197;&#25552;&#39640;&#21477;&#23376;&#32423;&#21644;&#27573;&#33853;&#32423; G2P &#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-Text Transfer Transformer (T5) has recently been considered for the Grapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-free byte-level model based on T5 referred to as ByT5, recently gave promising results on word-level G2P conversion by representing each input character with its corresponding UTF-8 encoding. Although it is generally understood that sentence-level or paragraph-level G2P can improve usability in real-world applications as it is better suited to perform on heteronyms and linking sounds between words, we find that using ByT5 for these scenarios is nontrivial. Since ByT5 operates on the character level, it requires longer decoding steps, which deteriorates the performance due to the exposure bias commonly observed in auto-regressive generation models. This paper shows that the performance of sentence-level and paragraph-level G2P can be improved by mitigating such exposure bias using our proposed loss-based sampling method.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#26631;&#31614;&#23569;&#26679;&#26412;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#30340;&#26631;&#31614;&#25551;&#36848;&#21644;&#31867;&#21035;&#20449;&#24687;&#26469;&#23398;&#20064;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#21407;&#22411;&#65292;&#24182;&#25972;&#21512;&#28151;&#21512;&#27880;&#24847;&#21147;&#26469;&#20943;&#23569;&#22122;&#22768;&#21644;&#25429;&#25417;&#26356;&#22810;&#20449;&#24687;&#20016;&#23500;&#30340;&#35821;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#21462;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#20540;&#23545;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08413</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#26631;&#31614;&#23569;&#26679;&#26412;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Enhanced Multi-Label Few-Shot Product Attribute-Value Extraction. (arXiv:2308.08413v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08413
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#26631;&#31614;&#23569;&#26679;&#26412;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#30340;&#26631;&#31614;&#25551;&#36848;&#21644;&#31867;&#21035;&#20449;&#24687;&#26469;&#23398;&#20064;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#21407;&#22411;&#65292;&#24182;&#25972;&#21512;&#28151;&#21512;&#27880;&#24847;&#21147;&#26469;&#20943;&#23569;&#22122;&#22768;&#21644;&#25429;&#25417;&#26356;&#22810;&#20449;&#24687;&#20016;&#23500;&#30340;&#35821;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#21462;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#20540;&#23545;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23646;&#24615;&#20540;&#25552;&#21462;&#65288;AVE&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#30005;&#23376;&#21830;&#21153;&#27599;&#22825;&#37117;&#20250;&#26377;&#24102;&#26377;&#26032;&#23646;&#24615;&#20540;&#23545;&#30340;&#26032;&#20135;&#21697;&#36827;&#20837;&#24066;&#22330;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#22810;&#26631;&#31614;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20013;&#21046;&#23450;AVE&#65292;&#26088;&#22312;&#22522;&#20110;&#23569;&#37327;&#30340;&#35757;&#32451;&#31034;&#20363;&#25552;&#21462;&#26410;&#35265;&#36807;&#30340;&#23646;&#24615;&#20540;&#23545;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#30693;&#35782;&#22686;&#24378;&#27880;&#24847;&#21147;&#26694;&#26550;&#65288;KEAF&#65289;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#26631;&#31614;&#25551;&#36848;&#21644;&#31867;&#21035;&#20449;&#24687;&#26469;&#23398;&#20064;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#21407;&#22411;&#12290;&#27492;&#22806;&#65292;KEAF&#36890;&#36807;&#35745;&#31639;&#19982;&#26631;&#31614;&#30456;&#20851;&#30340;&#26435;&#37325;&#21644;&#26597;&#35810;&#30456;&#20851;&#30340;&#26435;&#37325;&#65292;&#25972;&#21512;&#20102;&#28151;&#21512;&#27880;&#24847;&#21147;&#65292;&#20197;&#20943;&#23569;&#22122;&#22768;&#24182;&#25429;&#25417;&#26356;&#22810;&#20449;&#24687;&#20016;&#23500;&#30340;&#35821;&#20041;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#26631;&#31614;&#25512;&#29702;&#65292;KEAF&#36827;&#19968;&#27493;&#36890;&#36807;&#25972;&#21512;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#23398;&#20064;&#21160;&#24577;&#38408;&#20540;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;KEAF&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing attribute-value extraction (AVE) models require large quantities of labeled data for training. However, new products with new attribute-value pairs enter the market every day in real-world e-Commerce. Thus, we formulate AVE in multi-label few-shot learning (FSL), aiming to extract unseen attribute value pairs based on a small number of training examples. We propose a Knowledge-Enhanced Attentive Framework (KEAF) based on prototypical networks, leveraging the generated label description and category information to learn more discriminative prototypes. Besides, KEAF integrates with hybrid attention to reduce noise and capture more informative semantics for each class by calculating the label-relevant and query-related weights. To achieve multi-label inference, KEAF further learns a dynamic threshold by integrating the semantic information from both the support set and the query set. Extensive experiments with ablation studies conducted on two datasets demonstrate that KEAF outpe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#25345;&#32493;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#36830;&#32493;&#20449;&#24687;&#26816;&#32034;&#30340;&#22810;&#20027;&#39064;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25345;&#32493;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#24182;&#25552;&#39640;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08378</link><description>&lt;p&gt;
&#25512;&#36827;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#25345;&#32493;&#32456;&#36523;&#23398;&#20064;&#65306;&#23450;&#20041;&#12289;&#25968;&#25454;&#38598;&#12289;&#26694;&#26550;&#21644;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Advancing continual lifelong learning in neural information retrieval: definition, dataset, framework, and empirical evaluation. (arXiv:2308.08378v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#25345;&#32493;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#36830;&#32493;&#20449;&#24687;&#26816;&#32034;&#30340;&#22810;&#20027;&#39064;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25345;&#32493;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#24182;&#25552;&#39640;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#25351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23398;&#20064;&#21644;&#36866;&#24212;&#26032;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#19981;&#24433;&#21709;&#20854;&#22312;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#26377;&#22810;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#20173;&#32570;&#20047;&#26126;&#30830;&#30340;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#19988;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#20856;&#22411;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#25345;&#32493;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#36830;&#32493;&#20449;&#24687;&#26816;&#32034;&#30340;&#22810;&#20027;&#39064;&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25345;&#32493;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#26694;&#26550;&#65292;&#21253;&#25324;&#20856;&#22411;&#26816;&#32034;&#27169;&#22411;&#21644;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#25104;&#21151;&#22320;&#38450;&#27490;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#25552;&#39640;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#23884;&#20837;&#30340;&#26816;&#32034;&#26041;&#24335;&#36739;&#20256;&#32479;&#30340;&#22522;&#20110;&#32034;&#24341;&#30340;&#26816;&#32034;&#26041;&#24335;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#21319;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning refers to the capability of a machine learning model to learn and adapt to new information, without compromising its performance on previously learned tasks. Although several studies have investigated continual learning methods for information retrieval tasks, a well-defined task formulation is still lacking, and it is unclear how typical learning strategies perform in this context. To address this challenge, a systematic task formulation of continual neural information retrieval is presented, along with a multiple-topic dataset that simulates continuous information retrieval. A comprehensive continual neural information retrieval framework consisting of typical retrieval models and continual learning strategies is then proposed. Empirical evaluations illustrate that the proposed framework can successfully prevent catastrophic forgetting in neural information retrieval and enhance performance on previously learned tasks. The results indicate that embedding-based retr
&lt;/p&gt;</description></item><item><title>SummHelper&#26159;&#19968;&#20010;&#21327;&#20316;&#24335;&#20154;&#26426;&#25688;&#35201;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#36741;&#21161;&#36807;&#31243;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#21644;&#20462;&#25913;&#25991;&#26412;&#20869;&#23481;&#65292;&#24182;&#29983;&#25104;&#19968;&#20221;&#36830;&#36143;&#30340;&#25688;&#35201;&#12290;&#29992;&#25143;&#30740;&#31350;&#26174;&#31034;&#35813;&#24212;&#29992;&#31243;&#24207;&#22312;&#33258;&#21160;&#21270;&#24341;&#23548;&#21644;&#20010;&#20154;&#36755;&#20837;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.08363</link><description>&lt;p&gt;
SummHelper&#65306;&#21327;&#20316;&#24335;&#20154;&#26426;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SummHelper: Collaborative Human-Computer Summarization. (arXiv:2308.08363v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08363
&lt;/p&gt;
&lt;p&gt;
SummHelper&#26159;&#19968;&#20010;&#21327;&#20316;&#24335;&#20154;&#26426;&#25688;&#35201;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#36741;&#21161;&#36807;&#31243;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#21644;&#20462;&#25913;&#25991;&#26412;&#20869;&#23481;&#65292;&#24182;&#29983;&#25104;&#19968;&#20221;&#36830;&#36143;&#30340;&#25688;&#35201;&#12290;&#29992;&#25143;&#30740;&#31350;&#26174;&#31034;&#35813;&#24212;&#29992;&#31243;&#24207;&#22312;&#33258;&#21160;&#21270;&#24341;&#23548;&#21644;&#20010;&#20154;&#36755;&#20837;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#20027;&#35201;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#23545;&#20154;&#31867;&#24178;&#39044;&#21644;&#25511;&#21046;&#30340;&#31354;&#38388;&#26377;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SummHelper&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#36741;&#21161;&#31995;&#32479;&#65292;&#26088;&#22312;&#20419;&#36827;&#20154;&#26426;&#21327;&#20316;&#12290;&#21021;&#22987;&#38454;&#27573;&#28041;&#21450;&#20869;&#23481;&#36873;&#25321;&#65292;&#31995;&#32479;&#25512;&#33616;&#28508;&#22312;&#20869;&#23481;&#65292;&#20801;&#35768;&#29992;&#25143;&#25509;&#21463;&#12289;&#20462;&#25913;&#25110;&#24341;&#20837;&#20854;&#20182;&#36873;&#25321;&#12290;&#38543;&#21518;&#30340;&#20869;&#23481;&#25972;&#21512;&#38454;&#27573;&#65292;SummHelper&#20174;&#36825;&#20123;&#36873;&#25321;&#20013;&#29983;&#25104;&#19968;&#20010;&#36830;&#36143;&#30340;&#25688;&#35201;&#65292;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#25688;&#35201;&#19982;&#28304;&#25991;&#26412;&#20043;&#38388;&#30340;&#21487;&#35270;&#26144;&#23556;&#36827;&#34892;&#20248;&#21270;&#12290;&#23567;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#26174;&#31034;&#25105;&#20204;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#26377;&#25928;&#24615;&#65292;&#21442;&#19982;&#32773;&#29305;&#21035;&#36190;&#36175;&#33258;&#21160;&#24341;&#23548;&#21644;&#20010;&#20154;&#36755;&#20837;&#26426;&#20250;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches for text summarization are predominantly automatic, with rather limited space for human intervention and control over the process. In this paper, we introduce SummHelper, a 2-phase summarization assistant designed to foster human-machine collaboration. The initial phase involves content selection, where the system recommends potential content, allowing users to accept, modify, or introduce additional selections. The subsequent phase, content consolidation, involves SummHelper generating a coherent summary from these selections, which users can then refine using visual mappings between the summary and the source text. Small-scale user studies reveal the effectiveness of our application, with participants being especially appreciative of the balance between automated guidance and opportunities for personal input.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27493;&#35299;&#27602;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#38454;&#27573;&#36827;&#34892;&#35299;&#27602;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#26080;&#27602;&#25552;&#31034;&#36827;&#34892;&#36830;&#32493;&#29983;&#25104;&#26469;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#35774;&#35745;Detox-Chain&#26469;&#26657;&#20934;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.08295</link><description>&lt;p&gt;
&#20998;&#27493;&#35299;&#27602;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Detoxify Language Model Step-by-Step. (arXiv:2308.08295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27493;&#35299;&#27602;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#38454;&#27573;&#36827;&#34892;&#35299;&#27602;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#26080;&#27602;&#25552;&#31034;&#36827;&#34892;&#36830;&#32493;&#29983;&#25104;&#26469;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#35774;&#35745;Detox-Chain&#26469;&#26657;&#20934;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#27602;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#22312;&#20445;&#25345;&#29983;&#25104;&#33021;&#21147;&#30340;&#21516;&#26102;&#36991;&#20813;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#30340;&#23433;&#20840;&#24615;&#65292;&#20808;&#21069;&#30340;&#35299;&#27602;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#25110;&#22312;&#21333;&#27493;&#39588;&#20013;&#20174;&#19981;&#21516;&#26041;&#38754;&#32422;&#26463;&#29983;&#25104;&#26469;&#35299;&#27602;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#27839;&#30528;&#26377;&#27602;&#25552;&#31034;&#29983;&#25104;&#65292;&#35299;&#27602;&#26041;&#27861;&#30340;&#24037;&#20316;&#26041;&#21521;&#19982;&#20043;&#30456;&#21453;&#65292;&#36825;&#20123;&#26041;&#27861;&#23558;&#22823;&#22823;&#24433;&#21709;LLM&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#22914;&#35805;&#35821;&#36830;&#36143;&#24615;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#20914;&#31361;&#65292;&#25105;&#20204;&#23558;&#35299;&#27602;&#36807;&#31243;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#23376;&#27493;&#39588;&#65292;&#20854;&#20013;&#35299;&#27602;&#38598;&#20013;&#22312;&#36755;&#20837;&#38454;&#27573;&#65292;&#38543;&#21518;&#30340;&#36830;&#32493;&#29983;&#25104;&#22522;&#20110;&#26080;&#27602;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;Detox-Chain&#26469;&#26657;&#20934;LLMs&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#26377;&#24207;&#30340;&#26041;&#24335;&#36830;&#25509;&#19978;&#36848;&#23376;&#27493;&#39588;&#65292;&#36825;&#20351;&#24471;LLMs&#21487;&#20197;&#36827;&#34892;&#36830;&#32493;&#30340;&#35299;&#27602;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detoxification for LLMs is challenging since it requires models to avoid generating harmful content while maintaining the generation capability. To ensure the safety of generations, previous detoxification methods detoxify the models by changing the data distributions or constraining the generations from different aspects in a single-step manner. However, these approaches will dramatically affect the generation quality of LLMs, e.g., discourse coherence and semantic consistency, since language models tend to generate along the toxic prompt while detoxification methods work in the opposite direction. To handle such a conflict, we decompose the detoxification process into different sub-steps, where the detoxification is concentrated in the input stage and the subsequent continual generation is based on the non-toxic prompt. Besides, we also calibrate the strong reasoning ability of LLMs by designing a Detox-Chain to connect the above sub-steps in an orderly manner, which allows LLMs to d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#25193;&#23637;&#39044;&#35757;&#32451;&#23545;&#31264;&#23494;&#36890;&#36947;&#26816;&#32034;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#35813;&#26041;&#27861;&#36827;&#34892;&#26597;&#35810;&#29983;&#25104;&#24182;&#20256;&#36882;&#25193;&#23637;&#30340;&#30693;&#35782;&#32473;&#26816;&#32034;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#32593;&#32476;&#25628;&#32034;&#20219;&#21153;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08285</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#25193;&#23637;&#39044;&#35757;&#32451;&#29992;&#20110;&#31264;&#23494;&#36890;&#36947;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval. (arXiv:2308.08285v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#25193;&#23637;&#39044;&#35757;&#32451;&#23545;&#31264;&#23494;&#36890;&#36947;&#26816;&#32034;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#35813;&#26041;&#27861;&#36827;&#34892;&#26597;&#35810;&#29983;&#25104;&#24182;&#20256;&#36882;&#25193;&#23637;&#30340;&#30693;&#35782;&#32473;&#26816;&#32034;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#32593;&#32476;&#25628;&#32034;&#20219;&#21153;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25991;&#26723;&#25193;&#23637;&#39044;&#35757;&#32451;&#22312;&#31264;&#23494;&#36890;&#36947;&#26816;&#32034;&#20013;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#25991;&#26723;&#25193;&#23637;&#65292;&#21363;&#26597;&#35810;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#38024;&#23545;&#36890;&#36947;&#26816;&#32034;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#26377;&#25928;&#22320;&#23558;&#25193;&#23637;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#26816;&#32034;&#22120;&#12290;&#36825;&#20123;&#31574;&#30053;&#21253;&#25324;&#23545;&#27604;&#23398;&#20064;&#21644;&#29942;&#39048;&#26597;&#35810;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#26469;&#20943;&#23569;&#23545;LLM&#25512;&#29702;&#30340;&#20381;&#36182;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;LLM&#30340;&#25991;&#26723;&#25193;&#23637;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#32593;&#32476;&#25628;&#32034;&#20219;&#21153;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#21644;&#36328;&#39046;&#22495;&#26816;&#32034;&#33021;&#21147;&#65292;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26356;&#20855;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we systematically study the potential of pre-training with Large Language Model(LLM)-based document expansion for dense passage retrieval. Concretely, we leverage the capabilities of LLMs for document expansion, i.e. query generation, and effectively transfer expanded knowledge to retrievers using pre-training strategies tailored for passage retrieval. These strategies include contrastive learning and bottlenecked query generation. Furthermore, we incorporate a curriculum learning strategy to reduce the reliance on LLM inferences. Experimental results demonstrate that pre-training with LLM-based document expansion significantly boosts the retrieval performance on large-scale web-search tasks. Our work shows strong zero-shot and out-of-domain retrieval abilities, making it more widely applicable for retrieval when initializing with no human-labeled data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#23436;&#20840;&#25351;&#23450;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#22312;&#35821;&#27861;&#24402;&#32435;&#20219;&#21153;&#20013;&#20351;&#29992;&#35813;&#22522;&#20934;&#35780;&#20272;&#20102;&#19981;&#21516;&#26550;&#26500;&#30340;&#32593;&#32476;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30446;&#26631;&#65288;MDL&#65289;&#35757;&#32451;&#30340;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08253</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#22312;&#35821;&#27861;&#24402;&#32435;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Neural Network Generalization for Grammar Induction. (arXiv:2308.08253v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#23436;&#20840;&#25351;&#23450;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#22312;&#35821;&#27861;&#24402;&#32435;&#20219;&#21153;&#20013;&#20351;&#29992;&#35813;&#22522;&#20934;&#35780;&#20272;&#20102;&#19981;&#21516;&#26550;&#26500;&#30340;&#32593;&#32476;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30446;&#26631;&#65288;MDL&#65289;&#35757;&#32451;&#30340;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#22914;&#20309;&#65311;&#21363;&#20351;&#23545;&#20110;&#35821;&#27861;&#24402;&#32435;&#20219;&#21153;&#36825;&#26679;&#30446;&#26631;&#27867;&#21270;&#23436;&#20840;&#24050;&#30693;&#30340;&#20219;&#21153;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#20063;&#26410;&#33021;&#32473;&#20986;&#26126;&#30830;&#30340;&#31572;&#26696;&#65292;&#21482;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#36827;&#34892;&#20102;&#38750;&#24120;&#26377;&#38480;&#30340;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#25104;&#21151;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23436;&#20840;&#25351;&#23450;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24230;&#37327;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#27169;&#22411;&#21644;&#19968;&#20010;&#24418;&#24335;&#35821;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#27169;&#22411;&#22312;&#26410;&#35265;&#26679;&#26412;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#20998;&#37197;&#19968;&#20010;&#27867;&#21270;&#24471;&#20998;&#65292;&#36825;&#20010;&#24471;&#20998;&#19982;&#27169;&#22411;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#25104;&#21453;&#27604;&#12290;&#36825;&#20010;&#22522;&#20934;&#21253;&#21547;&#20102;&#35832;&#22914;$a^nb^n$&#65292;$a^nb^nc^n$&#65292;$a^nb^mc^{n+m}$&#20197;&#21450;Dyck-1&#21644;2&#31561;&#35821;&#35328;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#35780;&#20272;&#20102;&#19968;&#20123;&#26550;&#26500;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30446;&#26631;&#65288;MDL&#65289;&#35757;&#32451;&#30340;&#32593;&#32476;&#27604;&#20351;&#29992;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;&#35813;&#22522;&#20934;&#21487;&#22312;https://github.com/taucompling/bliss&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
How well do neural networks generalize? Even for grammar induction tasks, where the target generalization is fully known, previous works have left the question open, testing very limited ranges beyond the training set and using different success criteria. We provide a measure of neural network generalization based on fully specified formal languages. Given a model and a formal grammar, the method assigns a generalization score representing how well a model generalizes to unseen samples in inverse relation to the amount of data it was trained on. The benchmark includes languages such as $a^nb^n$, $a^nb^nc^n$, $a^nb^mc^{n+m}$, and Dyck-1 and 2. We evaluate selected architectures using the benchmark and find that networks trained with a Minimum Description Length objective (MDL) generalize better and using less data than networks trained using standard loss functions. The benchmark is available at https://github.com/taucompling/bliss.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20004;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#26041;&#27861;&#26469;&#28608;&#27963;&#35821;&#35328;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#32467;&#26524;&#27809;&#26377;&#26126;&#26174;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08241</link><description>&lt;p&gt;
TEST: &#25991;&#26412;&#21407;&#22411;&#23545;&#40784;&#23884;&#20837;&#20197;&#28608;&#27963;LLM&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series. (arXiv:2308.08241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08241
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20004;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#26041;&#27861;&#26469;&#28608;&#27963;&#35821;&#35328;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#32467;&#26524;&#27809;&#26377;&#26126;&#26174;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#20004;&#31181;&#20351;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23436;&#25104;&#26102;&#38388;&#24207;&#21015;&#65288;TS&#65289;&#20219;&#21153;&#30340;&#31574;&#30053;&#65306;LLM-for-TS&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#19968;&#20010;&#38024;&#23545;TS&#25968;&#25454;&#30340;&#22522;&#30784;&#22823;&#27169;&#22411;&#65307;TS-for-LLM&#65292;&#20351;&#39044;&#35757;&#32451;&#30340;LLM&#33021;&#22815;&#22788;&#29702;TS&#25968;&#25454;&#12290;&#37492;&#20110;&#25968;&#25454;&#31215;&#32047;&#19981;&#36275;&#12289;&#36164;&#28304;&#26377;&#38480;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;TS-for-LLM&#26041;&#27861;&#65292;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;LLM&#30340;TS&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#28608;&#27963;LLM&#23545;TS&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;TEST&#12290;&#23427;&#39318;&#20808;&#23545;TS&#36827;&#34892;&#26631;&#35760;&#21270;&#22788;&#29702;&#65292;&#24314;&#31435;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23454;&#20363;&#12289;&#29305;&#24449;&#21644;&#25991;&#26412;&#21407;&#22411;&#23545;&#40784;&#23545;&#23427;&#20204;&#36827;&#34892;&#23884;&#20837;&#65292;&#28982;&#21518;&#21019;&#24314;&#25552;&#31034;&#20197;&#20351;LLM&#26356;&#23481;&#26131;&#25509;&#21463;&#23884;&#20837;&#65292;&#24182;&#26368;&#32456;&#23454;&#26045;TS&#20219;&#21153;&#12290;&#20351;&#29992;8&#20010;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#21644;&#22823;&#23567;&#30340;LLM&#23545;TS&#20998;&#31867;&#21644;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23613;&#31649;&#20854;&#32467;&#26524;&#19981;&#33021;&#26174;&#33879;&#36229;&#36234;&#24403;&#21069;&#20026;TS&#20219;&#21153;&#23450;&#21046;&#30340;SOTA&#27169;&#22411;&#65292;&#20294;&#36890;&#36807;&#23558;LLM&#35270;&#20026;&#27169;&#24335;&#26426;&#22120;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;TS&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work summarizes two strategies for completing time-series (TS) tasks using today's language model (LLM): LLM-for-TS, design and train a fundamental large model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS data. Considering the insufficient data accumulation, limited resources, and semantic context requirements, this work focuses on TS-for-LLM methods, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed them by instance-wise, feature-wise, and text-prototype-aligned contrast, and then creates prompts to make LLM more open to embeddings, and finally implements TS tasks. Experiments are carried out on TS classification and forecasting tasks using 8 LLMs with different structures and sizes. Although its results cannot significantly outperform the current SOTA models customized for TS tasks, by treating LLM as the pattern machine, 
&lt;/p&gt;</description></item><item><title>MemoChat&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35843;&#20248;&#25351;&#20196;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#22791;&#24536;&#24405;&#26469;&#20445;&#25345;&#23545;&#35805;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08239</link><description>&lt;p&gt;
MemoChat: &#36890;&#36807;&#35843;&#25972;LLMs&#20351;&#29992;&#22791;&#24536;&#24405;&#20197;&#20445;&#25345;&#19968;&#33268;&#24615;&#30340;&#38271;&#36317;&#31163;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation. (arXiv:2308.08239v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08239
&lt;/p&gt;
&lt;p&gt;
MemoChat&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35843;&#20248;&#25351;&#20196;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#22791;&#24536;&#24405;&#26469;&#20445;&#25345;&#23545;&#35805;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MemoChat&#65292;&#19968;&#20010;&#29992;&#20110;&#20248;&#21270;&#25351;&#20196;&#30340;&#27969;&#27700;&#32447;&#65292;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26377;&#25928;&#22320;&#20351;&#29992;&#33258;&#34892;&#32452;&#32455;&#30340;&#22791;&#24536;&#24405;&#26469;&#20445;&#25345;&#19968;&#33268;&#30340;&#38271;&#36317;&#31163;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#12290;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#30340;&#8220;&#35760;&#24518;-&#26816;&#32034;-&#21709;&#24212;&#8221;&#24490;&#29615;&#23637;&#31034;&#20102;&#19968;&#20010;&#38271;&#36317;&#31163;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#12290;&#36825;&#35201;&#27714;&#25105;&#20204;&#20026;&#27599;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#31934;&#24515;&#35774;&#35745;&#23450;&#21046;&#30340;&#35843;&#20248;&#25351;&#20196;&#12290;&#36825;&#20123;&#25351;&#20196;&#26159;&#20174;&#19968;&#31995;&#21015;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#37325;&#24314;&#30340;&#65292;&#20197;&#25945;&#23548;LLMs&#35760;&#24518;&#21644;&#26816;&#32034;&#36807;&#21435;&#30340;&#23545;&#35805;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#22791;&#24536;&#24405;&#25552;&#39640;&#26410;&#26469;&#23545;&#35805;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36992;&#35831;&#19987;&#23478;&#25163;&#21160;&#27880;&#37322;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#36317;&#31163;&#23545;&#35805;&#19968;&#33268;&#24615;&#30340;&#27979;&#35797;&#38598;&#12290;&#22312;&#28041;&#21450;&#24320;&#28304;&#21644;&#21487;&#35775;&#38382;API&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19977;&#31181;&#27979;&#35797;&#22330;&#26223;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;MemoChat&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#36229;&#36234;&#20102;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MemoChat, a pipeline for refining instructions that enables large language models (LLMs) to effectively employ self-composed memos for maintaining consistent long-range open-domain conversations. We demonstrate a long-range open-domain conversation through iterative "memorization-retrieval-response" cycles. This requires us to carefully design tailored tuning instructions for each distinct stage. The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations. We invite experts to manually annotate a test set designed to evaluate the consistency of long-range conversations questions. Experiments on three testing scenarios involving both open-source and API-accessible chatbots at scale verify the efficacy of MemoChat, which outperforms strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#36890;&#36807;&#23545;NLP&#20013;&#22522;&#20110;Transformer&#30340;MTL&#26041;&#27861;&#20197;&#21450;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21508;&#38454;&#27573;&#30340;&#25361;&#25112;&#36827;&#34892;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;&#30456;&#20851;&#39046;&#22495;&#30340;&#27010;&#36848;&#21644;&#21160;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.08234</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey. (arXiv:2308.08234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#36890;&#36807;&#23545;NLP&#20013;&#22522;&#20110;Transformer&#30340;MTL&#26041;&#27861;&#20197;&#21450;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21508;&#38454;&#27573;&#30340;&#25361;&#25112;&#36827;&#34892;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;&#30456;&#20851;&#39046;&#22495;&#30340;&#27010;&#36848;&#21644;&#21160;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20174;&#35757;&#32451;&#21040;&#22312;&#29983;&#20135;&#20013;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#26377;&#25928;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#26356;&#26032;&#22810;&#20010;&#27169;&#22411;&#21487;&#33021;&#22797;&#26434;&#12289;&#26114;&#36149;&#19988;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#20316;&#20026;&#25913;&#36827;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#26412;&#35843;&#30740;&#39318;&#20808;&#27010;&#36848;&#20102;NLP&#20013;&#22522;&#20110;Transformer&#30340;MTL&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#20013;&#20351;&#29992;MTL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#24037;&#31243;&#12289;&#27169;&#22411;&#24320;&#21457;&#12289;&#37096;&#32626;&#21644;&#30417;&#25511;&#38454;&#27573;&#30340;&#25361;&#25112;&#12290;&#26412;&#39033;&#35843;&#30740;&#38598;&#20013;&#20110;&#22522;&#20110;Transformer&#30340;MTL&#26550;&#26500;&#65292;&#24182;&#25454;&#25105;&#20204;&#25152;&#30693;&#26159;&#39318;&#21019;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing adoption of natural language processing (NLP) models across industries has led to practitioners' need for machine learning systems to handle these models efficiently, from training to serving them in production. However, training, deploying, and updating multiple models can be complex, costly, and time-consuming, mainly when using transformer-based pre-trained language models. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency and performance through joint training, rather than training separate models. Motivated by this, we first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the challenges and opportunities of using MTL approaches throughout typical ML lifecycle phases, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best of our knowledge, is novel in that it 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoCoSA&#26041;&#27861;&#65292;&#21033;&#29992;&#21160;&#37327;&#23545;&#27604;&#21644;&#32467;&#26500;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#35299;&#20915;&#20102;&#22522;&#20110;&#32467;&#26500;&#21644;&#22522;&#20110;&#25551;&#36848;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#20041;&#20016;&#23500;&#23454;&#20307;&#30340;&#27867;&#21270;&#25512;&#29702;&#21644;&#23545;&#26410;&#35265;&#23454;&#20307;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08204</link><description>&lt;p&gt;
MoCoSA: &#21160;&#37327;&#23545;&#27604;&#19982;&#32467;&#26500;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MoCoSA: Momentum Contrast for Knowledge Graph Completion with Structure-Augmented Pre-trained Language Models. (arXiv:2308.08204v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoCoSA&#26041;&#27861;&#65292;&#21033;&#29992;&#21160;&#37327;&#23545;&#27604;&#21644;&#32467;&#26500;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#35299;&#20915;&#20102;&#22522;&#20110;&#32467;&#26500;&#21644;&#22522;&#20110;&#25551;&#36848;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#20041;&#20016;&#23500;&#23454;&#20307;&#30340;&#27867;&#21270;&#25512;&#29702;&#21644;&#23545;&#26410;&#35265;&#23454;&#20307;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#33258;&#21160;&#25512;&#26029;&#20986;&#32570;&#22833;&#30340;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#22522;&#20110;&#32467;&#26500;&#21644;&#22522;&#20110;&#25551;&#36848;&#20004;&#31867;&#12290;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#20351;&#29992;&#23454;&#20307;&#23884;&#20837;&#26469;&#26377;&#25928;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#20851;&#31995;&#20107;&#23454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32467;&#26500;&#20449;&#24687;&#26377;&#38480;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#35821;&#20041;&#20016;&#23500;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#24182;&#19988;&#26080;&#27861;&#27867;&#21270;&#21040;&#26410;&#35265;&#23454;&#20307;&#12290;&#22522;&#20110;&#25551;&#36848;&#30340;&#26041;&#27861;&#21017;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26469;&#29702;&#35299;&#25991;&#26412;&#20449;&#24687;&#65292;&#23545;&#26410;&#35265;&#23454;&#20307;&#23637;&#31034;&#20986;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#36127;&#37319;&#26679;&#26102;&#38754;&#20020;&#22256;&#38590;&#65292;&#24182;&#19988;&#24120;&#24120;&#33853;&#21518;&#20110;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#37327;&#23545;&#27604;&#21644;&#32467;&#26500;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;MoCoSA&#65289;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Completion (KGC) aims to conduct reasoning on the facts within knowledge graphs and automatically infer missing links. Existing methods can mainly be categorized into structure-based or description-based. On the one hand, structure-based methods effectively represent relational facts in knowledge graphs using entity embeddings. However, they struggle with semantically rich real-world entities due to limited structural information and fail to generalize to unseen entities. On the other hand, description-based methods leverage pre-trained language models (PLMs) to understand textual information. They exhibit strong robustness towards unseen entities. However, they have difficulty with larger negative sampling and often lag behind structure-based methods. To address these issues, in this paper, we propose Momentum Contrast for knowledge graph completion with Structure-Augmented pre-trained language models (MoCoSA), which allows the PLM to perceive the structural informatio
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#30005;&#20449;&#30340;&#31995;&#32479;&#22312;VoxCeleb2023&#35828;&#35805;&#20154;&#35782;&#21035;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#22312;VoxCeleb2&#19978;&#35757;&#32451;&#30340;ResNet&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#34701;&#21512;&#21644;&#24471;&#20998;&#26657;&#20934;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;0.1066&#30340;minDCF&#21644;1.980%&#30340;EER&#12290;</title><link>http://arxiv.org/abs/2308.08181</link><description>&lt;p&gt;
&#20013;&#22269;&#30005;&#20449;&#23545;VoxCeleb2023&#35828;&#35805;&#20154;&#35782;&#21035;&#25361;&#25112;&#30340;&#31995;&#32479;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023. (arXiv:2308.08181v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08181
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#30005;&#20449;&#30340;&#31995;&#32479;&#22312;VoxCeleb2023&#35828;&#35805;&#20154;&#35782;&#21035;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#22312;VoxCeleb2&#19978;&#35757;&#32451;&#30340;ResNet&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#34701;&#21512;&#21644;&#24471;&#20998;&#26657;&#20934;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;0.1066&#30340;minDCF&#21644;1.980%&#30340;EER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25216;&#26415;&#25253;&#21578;&#25551;&#36848;&#20102;&#20013;&#22269;&#30005;&#20449;&#38024;&#23545;VoxCeleb2023&#35828;&#35805;&#20154;&#35782;&#21035;&#25361;&#25112;&#65288;VoxSRC 2023&#65289;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#30001;&#20960;&#20010;&#22312;VoxCeleb2&#19978;&#35757;&#32451;&#30340;ResNet&#21464;&#31181;&#32452;&#25104;&#65292;&#21518;&#26469;&#36827;&#34892;&#20102;&#34701;&#21512;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#27599;&#20010;&#21464;&#31181;&#21644;&#34701;&#21512;&#31995;&#32479;&#37117;&#36827;&#34892;&#20102;&#24471;&#20998;&#26657;&#20934;&#12290;&#26368;&#32456;&#25552;&#20132;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;0.1066&#30340;minDCF&#21644;1.980%&#30340;EER&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report describes ChinaTelecom system for Track 1 (closed) of the VoxCeleb2023 Speaker Recognition Challenge (VoxSRC 2023). Our system consists of several ResNet variants trained only on VoxCeleb2, which were fused for better performance later. Score calibration was also applied for each variant and the fused system. The final submission achieved minDCF of 0.1066 and EER of 1.980%.
&lt;/p&gt;</description></item><item><title>RSpell&#26159;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#30340;&#26816;&#32034;&#22686;&#24378;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25340;&#38899;&#27169;&#31946;&#21305;&#37197;&#26469;&#25628;&#32034;&#39046;&#22495;&#26415;&#35821;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;CSC&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36807;&#31243;&#25511;&#21046;&#26426;&#21046;&#21644;&#36845;&#20195;&#31574;&#30053;&#65292;RSpell&#22312;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#22330;&#26223;&#19979;&#37117;&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.08176</link><description>&lt;p&gt;
RSpell&#65306;&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#30340;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#30340;&#26816;&#32034;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese Spelling Check. (arXiv:2308.08176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08176
&lt;/p&gt;
&lt;p&gt;
RSpell&#26159;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#30340;&#26816;&#32034;&#22686;&#24378;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25340;&#38899;&#27169;&#31946;&#21305;&#37197;&#26469;&#25628;&#32034;&#39046;&#22495;&#26415;&#35821;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;CSC&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36807;&#31243;&#25511;&#21046;&#26426;&#21046;&#21644;&#36845;&#20195;&#31574;&#30053;&#65292;RSpell&#22312;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#22330;&#26223;&#19979;&#37117;&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#26159;&#25351;&#22312;&#20013;&#25991;&#25991;&#26412;&#20013;&#26816;&#27979;&#21644;&#32416;&#27491;&#25340;&#20889;&#38169;&#35823;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#20351;CSC&#27169;&#22411;&#20855;&#22791;&#36328;&#39046;&#22495;&#32416;&#38169;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RSpell&#30340;&#26816;&#32034;&#22686;&#24378;&#25340;&#20889;&#26816;&#26597;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25628;&#32034;&#30456;&#24212;&#30340;&#39046;&#22495;&#26415;&#35821;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;CSC&#27169;&#22411;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#25340;&#38899;&#27169;&#31946;&#21305;&#37197;&#26469;&#25628;&#32034;&#26415;&#35821;&#65292;&#24182;&#23558;&#20854;&#19982;&#36755;&#20837;&#32452;&#21512;&#21518;&#36755;&#20837;&#21040;CSC&#27169;&#22411;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36807;&#31243;&#25511;&#21046;&#26426;&#21046;&#65292;&#21160;&#24577;&#35843;&#25972;&#22806;&#37096;&#30693;&#35782;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#24378;RSpell&#26694;&#26550;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#27861;&#24459;&#12289;&#21307;&#23398;&#21644;&#20844;&#25991;&#20889;&#20316;&#19977;&#20010;&#39046;&#22495;&#30340;CSC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;RSpell&#22312;&#38646;&#26679;&#26412;&#21644;&#24494;&#35843;&#22330;&#26223;&#19979;&#37117;&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chinese Spelling Check (CSC) refers to the detection and correction of spelling errors in Chinese texts. In practical application scenarios, it is important to make CSC models have the ability to correct errors across different domains. In this paper, we propose a retrieval-augmented spelling check framework called RSpell, which searches corresponding domain terms and incorporates them into CSC models. Specifically, we employ pinyin fuzzy matching to search for terms, which are combined with the input and fed into the CSC model. Then, we introduce an adaptive process control mechanism to dynamically adjust the impact of external knowledge on the model. Additionally, we develop an iterative strategy for the RSpell framework to enhance reasoning capabilities. We conducted experiments on CSC datasets in three domains: law, medicine, and official document writing. The results demonstrate that RSpell achieves state-of-the-art performance in both zero-shot and fine-tuning scenarios, demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31616;&#21333;&#30340;&#32531;&#23384;&#25552;&#39640;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24494;&#35843;&#26816;&#32034;&#27169;&#22359;&#24182;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#23545;&#35805;&#31995;&#32479;&#27169;&#22411;&#65292;&#31995;&#32479;&#21487;&#20197;&#21160;&#24577;&#26356;&#26032;&#24182;&#22788;&#29702;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#23545;&#35805;&#22330;&#26223;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#22312;&#38750;&#31354;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#19978;&#25552;&#39640;&#20102;6.7%&#12290;</title><link>http://arxiv.org/abs/2308.08169</link><description>&lt;p&gt;
&#22686;&#24378;&#24615;&#33021;&#65306;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#22411;&#31995;&#32479;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#23545;&#35805;&#22330;&#26223;&#19979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Performance on Seen and Unseen Dialogue Scenarios using Retrieval-Augmented End-to-End Task-Oriented System. (arXiv:2308.08169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31616;&#21333;&#30340;&#32531;&#23384;&#25552;&#39640;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24494;&#35843;&#26816;&#32034;&#27169;&#22359;&#24182;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#23545;&#35805;&#31995;&#32479;&#27169;&#22411;&#65292;&#31995;&#32479;&#21487;&#20197;&#21160;&#24577;&#26356;&#26032;&#24182;&#22788;&#29702;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#23545;&#35805;&#22330;&#26223;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#22312;&#38750;&#31354;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#19978;&#25552;&#39640;&#20102;6.7%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#36827;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#32531;&#23384;&#20351;&#24471;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#26356;&#21152;&#28789;&#27963;&#12290;&#36825;&#20010;&#32531;&#23384;&#33021;&#22815;&#21160;&#24577;&#26356;&#26032;&#31995;&#32479;&#24182;&#22788;&#29702;&#29616;&#26377;&#21644;&#26410;&#30693;&#30340;&#23545;&#35805;&#22330;&#26223;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#26816;&#32034;&#27169;&#22359;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20415;&#20174;&#32531;&#23384;&#20013;&#26377;&#25928;&#22320;&#26816;&#32034;&#21040;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#23545;&#35805;&#31995;&#32479;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#23545;&#35805;&#26102;&#21487;&#20197;&#24341;&#29992;&#21644;&#32852;&#31995;&#23545;&#35805;&#21382;&#21490;&#21644;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#12290;&#32531;&#23384;&#30340;&#26500;&#24314;&#38750;&#24120;&#31616;&#21333;&#65292;&#32780;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#20027;&#24178;&#27169;&#22411;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#20860;&#23481;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#38750;&#31354;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#25552;&#39640;&#20102;6.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end task-oriented dialogue (TOD) systems have achieved promising performance by leveraging sophisticated natural language understanding and natural language generation capabilities of pre-trained models. This work enables the TOD systems with more flexibility through a simple cache. The cache provides the flexibility to dynamically update the TOD systems and handle both existing and unseen dialogue scenarios. Towards this end, we first fine-tune a retrieval module to effectively retrieve the most relevant information entries from the cache. We then train end-to-end TOD models that can refer to and ground on both dialogue history and retrieved information during TOD generation. The cache is straightforward to construct, and the backbone models of TOD systems are compatible with existing pre-trained generative models. Extensive experiments demonstrate the superior performance of our framework, with a notable improvement in non-empty joint goal accuracy by 6.7% compared to strong b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HurricaneSARC&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;15,000&#26465;&#27880;&#37322;&#20026;&#35773;&#21050;&#24847;&#22270;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35773;&#21050;&#26816;&#27979;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20013;&#38388;&#20219;&#21153;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;0.70&#30340;F1&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.08156</link><description>&lt;p&gt;
&#28798;&#38590;&#32972;&#26223;&#19979;&#30340;&#35773;&#21050;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sarcasm Detection in a Disaster Context. (arXiv:2308.08156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HurricaneSARC&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;15,000&#26465;&#27880;&#37322;&#20026;&#35773;&#21050;&#24847;&#22270;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35773;&#21050;&#26816;&#27979;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20013;&#38388;&#20219;&#21153;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;0.70&#30340;F1&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#28798;&#23475;&#26399;&#38388;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;Twitter&#31561;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#23547;&#27714;&#24110;&#21161;&#12289;&#25552;&#20379;&#20851;&#20110;&#28798;&#24773;&#30340;&#20449;&#24687;&#65292;&#25110;&#34920;&#36798;&#23545;&#28798;&#24773;&#28436;&#21464;&#25110;&#20844;&#20849;&#25919;&#31574;&#21644;&#25351;&#23548;&#26041;&#38024;&#30340;&#34065;&#35270;&#12290;&#22312;&#28798;&#38590;&#32972;&#26223;&#20013;&#29702;&#35299;&#36825;&#31181;&#35328;&#35770;&#24418;&#24335;&#23545;&#25913;&#36827;&#23545;&#28798;&#23475;&#30456;&#20851;&#25512;&#25991;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HurricaneSARC&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;15,000&#26465;&#26631;&#35760;&#20102;&#35773;&#21050;&#24847;&#22270;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#35773;&#21050;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#36798;&#21040;0.70&#30340;F1&#20540;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#20013;&#38388;&#20219;&#21153;&#30340;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;HurricaneSARC&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;https://github.com/tsosea2/HurricaneSarc&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
During natural disasters, people often use social media platforms such as Twitter to ask for help, to provide information about the disaster situation, or to express contempt about the unfolding event or public policies and guidelines. This contempt is in some cases expressed as sarcasm or irony. Understanding this form of speech in a disaster-centric context is essential to improving natural language understanding of disaster-related tweets. In this paper, we introduce HurricaneSARC, a dataset of 15,000 tweets annotated for intended sarcasm, and provide a comprehensive investigation of sarcasm detection using pre-trained language models. Our best model is able to obtain as much as 0.70 F1 on our dataset. We also demonstrate that the performance on HurricaneSARC can be improved by leveraging intermediate task transfer learning. We release our data and code at https://github.com/tsosea2/HurricaneSarc.
&lt;/p&gt;</description></item><item><title>AutoGen&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#21487;&#20197;&#20114;&#30456;&#23545;&#35805;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#19979;&#19968;&#20195;LLM&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#26234;&#33021;&#65292;&#20248;&#38597;&#22320;&#22788;&#29702;&#19981;&#23436;&#32654;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#20195;&#29702;&#23545;&#35805;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.08155</link><description>&lt;p&gt;
AutoGen:&#36890;&#36807;&#22810;&#20195;&#29702;&#23545;&#35805;&#26694;&#26550;&#23454;&#29616;&#19979;&#19968;&#20195;LLM&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. (arXiv:2308.08155v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08155
&lt;/p&gt;
&lt;p&gt;
AutoGen&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#21487;&#20197;&#20114;&#30456;&#23545;&#35805;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#19979;&#19968;&#20195;LLM&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#26234;&#33021;&#65292;&#20248;&#38597;&#22320;&#22788;&#29702;&#19981;&#23436;&#32654;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#20195;&#29702;&#23545;&#35805;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;AutoGen&#65292;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#21487;&#20197;&#20114;&#30456;&#23545;&#35805;&#30340;&#20195;&#29702;&#26469;&#24320;&#21457;LLM&#24212;&#29992;&#31243;&#24207;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;AutoGen&#20195;&#29702;&#21487;&#20197;&#23450;&#21046;&#12289;&#21487;&#23545;&#35805;&#65292;&#24182;&#19988;&#21487;&#20197;&#26080;&#32541;&#22320;&#20801;&#35768;&#20154;&#31867;&#21442;&#19982;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#21508;&#31181;&#27169;&#24335;&#19979;&#36816;&#34892;&#65292;&#21033;&#29992;LLM&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#24037;&#20855;&#30340;&#32452;&#21512;&#12290;AutoGen&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#22810;&#20010;&#20248;&#21183;&#65306;a&#65289;&#23427;&#33021;&#22815;&#20248;&#38597;&#22320;&#22788;&#29702;&#36825;&#20123;LLM&#30340;&#24378;&#22823;&#20294;&#19981;&#23436;&#32654;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65307;b&#65289;&#23427;&#21033;&#29992;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#26234;&#33021;&#65292;&#36890;&#36807;&#20195;&#29702;&#20043;&#38388;&#30340;&#23545;&#35805;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#33258;&#21160;&#21270;&#65307;c&#65289;&#23427;&#31616;&#21270;&#21644;&#32479;&#19968;&#20102;&#22797;&#26434;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29616;&#65292;&#20316;&#20026;&#33258;&#21160;&#21270;&#20195;&#29702;&#23545;&#35805;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#24320;&#21457;&#20154;&#21592;&#22914;&#20309;&#36731;&#26494;&#20351;&#29992;AutoGen&#26377;&#25928;&#22320;&#35299;&#20915;&#20219;&#21153;&#25110;&#26500;&#24314;&#24212;&#29992;&#31243;&#24207;&#65292;&#28085;&#30422;&#32534;&#31243;&#12289;&#25968;&#23398;&#12289;&#36816;&#31609;&#23398;&#12289;&#23089;&#20048;&#12289;&#22312;&#32447;&#20915;&#31574;&#12289;&#38382;&#31572;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents AutoGen, a new framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. AutoGen's design offers multiple advantages: a) it gracefully navigates the strong but imperfect generation and reasoning abilities of these LLMs; b) it leverages human understanding and intelligence, while providing valuable automation through conversations between agents; c) it simplifies and unifies the implementation of complex LLM workflows as automated agent chats. We provide many diverse examples of how developers can easily use AutoGen to effectively solve tasks or build applications, ranging from coding, mathematics, operations research, entertainment, online decision-making, question answering, etc.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#32763;&#35793;&#21477;&#23376;&#23545;&#36827;&#34892;&#25490;&#24207;&#26469;&#33410;&#30465;&#35745;&#31639;&#33021;&#21147;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.08153</link><description>&lt;p&gt;
&#29992;&#25968;&#25454;&#25490;&#24207;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#30340;NMT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast Training of NMT Model with Data Sorting. (arXiv:2308.08153v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#32763;&#35793;&#21477;&#23376;&#23545;&#36827;&#34892;&#25490;&#24207;&#26469;&#33410;&#30465;&#35745;&#31639;&#33021;&#21147;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#24050;&#32463;&#20026;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31561;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#24102;&#26469;&#20102;&#38761;&#21629;&#65292;&#24182;&#19988;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#25552;&#39640;Transformer&#26694;&#26550;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#25913;&#36827;&#30340;&#28508;&#22312;&#39046;&#22495;&#20043;&#19968;&#26159;&#35299;&#20915;Transformer&#35745;&#31639;&#24182;&#19988;&#21518;&#32493;&#33293;&#24323;&#31354;&#26631;&#35760;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#20174;&#32780;&#20943;&#36731;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25209;&#22788;&#29702;&#20043;&#21069;&#26681;&#25454;&#21477;&#23376;&#38271;&#24230;&#25490;&#24207;&#32763;&#35793;&#21477;&#23376;&#23545;&#30340;&#31639;&#27861;&#65292;&#26368;&#23567;&#21270;&#35745;&#31639;&#33021;&#37327;&#30340;&#28010;&#36153;&#12290;&#30001;&#20110;&#25490;&#24207;&#30340;&#25968;&#37327;&#21487;&#33021;&#36829;&#21453;&#30456;&#20114;&#29420;&#31435;&#21644;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#25968;&#25454;&#20551;&#35774;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#36827;&#34892;&#37096;&#20998;&#25490;&#24207;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#33521;&#25991;-&#38889;&#25991;&#21644;&#33521;&#25991;-&#21346;&#24178;&#36798;&#35821;&#23545;&#30340;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#33410;&#30465;&#20102;&#35745;&#31639;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26550;&#26500;&#26080;&#20851;&#65292;&#22240;&#27492;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Transformer model has revolutionized Natural Language Processing tasks such as Neural Machine Translation, and many efforts have been made to study the Transformer architecture, which increased its efficiency and accuracy. One potential area for improvement is to address the computation of empty tokens that the Transformer computes only to discard them later, leading to an unnecessary computational burden. To tackle this, we propose an algorithm that sorts translation sentence pairs based on their length before batching, minimizing the waste of computing power. Since the amount of sorting could violate the independent and identically distributed (i.i.d) data assumption, we sort the data partially. In experiments, we apply the proposed method to English-Korean and English-Luganda language pairs for machine translation and show that there are gains in computational time while maintaining the performance. Our method is independent of architectures, so that it can be easily integrated 
&lt;/p&gt;</description></item><item><title>MDDial&#26159;&#31532;&#19968;&#20010;&#33521;&#35821;&#24046;&#24322;&#35786;&#26029;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#26500;&#24314;&#21644;&#35780;&#20272;&#31471;&#21040;&#31471;&#30340;ADD&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35780;&#20998;&#26041;&#27861;&#26469;&#34913;&#37327;ADD&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08147</link><description>&lt;p&gt;
MDDial: &#19968;&#20221;&#24102;&#26377;&#21487;&#38752;&#24615;&#35780;&#20272;&#30340;&#22810;&#36718;&#24046;&#24322;&#35786;&#26029;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MDDial: A Multi-turn Differential Diagnosis Dialogue Dataset with Reliability Evaluation. (arXiv:2308.08147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08147
&lt;/p&gt;
&lt;p&gt;
MDDial&#26159;&#31532;&#19968;&#20010;&#33521;&#35821;&#24046;&#24322;&#35786;&#26029;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#26500;&#24314;&#21644;&#35780;&#20272;&#31471;&#21040;&#31471;&#30340;ADD&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35780;&#20998;&#26041;&#27861;&#26469;&#34913;&#37327;ADD&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24046;&#24322;&#35786;&#26029;&#65288;ADD&#65289;&#30340;&#23545;&#35805;&#31995;&#32479;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#23545;&#35805;&#31995;&#32479;&#26377;&#26395;&#25552;&#20379;&#20415;&#25463;&#35775;&#38382;&#21644;&#38477;&#20302;&#21307;&#30103;&#25104;&#26412;&#12290;&#26500;&#24314;&#31471;&#21040;&#31471;&#30340;ADD&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#23545;&#35805;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23613;&#31649;&#38750;&#33521;&#35821;&#25968;&#25454;&#38598;&#23384;&#22312;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#33521;&#35821;ADD&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MDDial&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33521;&#35821;&#24046;&#24322;&#35786;&#26029;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#26500;&#24314;&#21644;&#35780;&#20272;&#31471;&#21040;&#31471;&#30340;ADD&#23545;&#35805;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#21333;&#29420;&#25110;&#32773;&#36890;&#36807;&#19968;&#20010;&#32508;&#21512;&#21152;&#26435;&#20998;&#25968;&#26469;&#35780;&#20272;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#30151;&#29366;&#12290;&#36825;&#31181;&#26041;&#27861;&#24573;&#30053;&#20102;&#30151;&#29366;&#21644;&#35786;&#26029;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35780;&#20998;&#26041;&#27861;&#26469;&#34913;&#37327;ADD&#31995;&#32479;&#65292;&#32771;&#34385;&#20102;&#30151;&#29366;&#21644;&#35786;&#26029;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#35813;&#35780;&#20998;&#36824;&#34920;&#31034;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems for Automatic Differential Diagnosis (ADD) have a wide range of real-life applications. These dialogue systems are promising for providing easy access and reducing medical costs. Building end-to-end ADD dialogue systems requires dialogue training datasets. However, to the best of our knowledge, there is no publicly available ADD dialogue dataset in English (although non-English datasets exist). Driven by this, we introduce MDDial, the first differential diagnosis dialogue dataset in English which can aid to build and evaluate end-to-end ADD dialogue systems. Additionally, earlier studies present the accuracy of diagnosis and symptoms either individually or as a combined weighted score. This method overlooks the connection between the symptoms and the diagnosis. We introduce a unified score for the ADD system that takes into account the interplay between symptoms and diagnosis. This score also indicates the system's reliability. To the end, we train two moderate-size of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Radio2Text&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;mmWave&#30340;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#20855;&#26377;&#36229;&#36807;13,000&#20010;&#35789;&#30340;&#35789;&#27719;&#37327;&#12290;&#36890;&#36807;&#35774;&#35745;&#30340;&#27969;&#24335;Transformer&#21644;Guidance Initialization&#25216;&#26415;&#65292;Radio2Text&#23454;&#29616;&#20102;&#23545;&#27969;&#24335;ASR&#20855;&#26377;&#22823;&#35789;&#27719;&#37327;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2308.08125</link><description>&lt;p&gt;
Radio2Text&#65306;&#20351;&#29992;mmWave&#26080;&#32447;&#30005;&#20449;&#21495;&#36827;&#34892;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals. (arXiv:2308.08125v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Radio2Text&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;mmWave&#30340;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#20855;&#26377;&#36229;&#36807;13,000&#20010;&#35789;&#30340;&#35789;&#27719;&#37327;&#12290;&#36890;&#36807;&#35774;&#35745;&#30340;&#27969;&#24335;Transformer&#21644;Guidance Initialization&#25216;&#26415;&#65292;Radio2Text&#23454;&#29616;&#20102;&#23545;&#27969;&#24335;ASR&#20855;&#26377;&#22823;&#35789;&#27719;&#37327;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#22522;&#20110;&#35821;&#38899;&#35782;&#21035;&#20026;&#38899;&#39057;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22810;&#21487;&#33021;&#24615;&#65292;&#22914;&#20250;&#35758;&#28436;&#35762;&#36716;&#24405;&#21644;&#31363;&#21548;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#23454;&#38469;&#22330;&#26223;&#30340;&#21487;&#34892;&#24615;&#65292;&#24310;&#36831;&#21644;&#21487;&#35782;&#21035;&#30340;&#35789;&#27719;&#37327;&#26159;&#20004;&#20010;&#19981;&#23481;&#24573;&#35270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Radio2Text&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;mmWave&#30340;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#20854;&#35789;&#27719;&#37327;&#36229;&#36807;13,000&#20010;&#35789;&#12290;Radio2Text&#22522;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#27969;&#24335;Transformer&#65292;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#19982;&#35821;&#38899;&#30456;&#20851;&#29305;&#24449;&#30340;&#34920;&#31034;&#65292;&#20026;&#20855;&#26377;&#22823;&#35789;&#27719;&#37327;&#30340;&#27969;&#24335;ASR&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20026;&#20102;&#32531;&#35299;&#27969;&#24335;&#32593;&#32476;&#26080;&#27861;&#35775;&#38382;&#25972;&#20010;&#26410;&#26469;&#36755;&#20837;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Guidance Initialization&#65292;&#36890;&#36807;&#26435;&#37325;&#32487;&#25215;&#65292;&#20419;&#36827;&#23558;&#19982;&#20840;&#23616;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#29305;&#24449;&#30693;&#35782;&#20174;&#38750;&#27969;&#24335;Transformer&#20256;&#36882;&#32473;&#31934;&#24515;&#35774;&#35745;&#30340;&#27969;&#24335;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Millimeter wave (mmWave) based speech recognition provides more possibility for audio-related applications, such as conference speech transcription and eavesdropping. However, considering the practicality in real scenarios, latency and recognizable vocabulary size are two critical factors that cannot be overlooked. In this paper, we propose Radio2Text, the first mmWave-based system for streaming automatic speech recognition (ASR) with a vocabulary size exceeding 13,000 words. Radio2Text is based on a tailored streaming Transformer that is capable of effectively learning representations of speech-related features, paving the way for streaming ASR with a large vocabulary. To alleviate the deficiency of streaming networks unable to access entire future inputs, we propose the Guidance Initialization that facilitates the transfer of feature knowledge related to the global context from the non-streaming Transformer to the tailored streaming Transformer through weight inheritance. Further, we
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#21462;&#21644;&#28040;&#38500;&#21453;&#19987;&#23478;PEMs&#20013;&#30340;&#27531;&#32570;&#33021;&#21147;&#26469;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#21644;&#21435;&#27602;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08090</link><description>&lt;p&gt;
&#25226;&#39640;&#19979;&#20998;&#28165;&#26970;&#65306;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#27169;&#22359;&#25805;&#20316;&#36827;&#34892;&#27169;&#22411;&#27531;&#32570;&#24615;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation. (arXiv:2308.08090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08090
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#21462;&#21644;&#28040;&#38500;&#21453;&#19987;&#23478;PEMs&#20013;&#30340;&#27531;&#32570;&#33021;&#21147;&#26469;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#21644;&#21435;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23384;&#22312;&#19982;&#19981;&#30495;&#23454;&#21644;&#26377;&#27602;&#24615;&#26377;&#20851;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#21442;&#25968;&#39640;&#25928;&#27169;&#22359;&#65288;PEMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#20026;&#27169;&#22411;&#36171;&#20104;&#26032;&#25216;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#21033;&#29992;PEMs&#36827;&#34892;&#27531;&#32570;&#24615;&#21435;&#23398;&#20064;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;PEMs&#25805;&#20316;&#26041;&#27861;&#65292;&#21363;&#8220;&#25552;&#21462;-&#20943;&#21435;&#8221;&#65288;Ext-Sub&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#8220;&#19987;&#23478;&#8221;PEMs&#21644;&#8220;&#21453;&#19987;&#23478;&#8221;PEMs&#26469;&#22686;&#24378;LLMs&#30340;&#30495;&#23454;&#24615;&#21644;&#21435;&#27602;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#21453;&#19987;&#23478;PEMs&#20063;&#20855;&#26377;&#23453;&#36149;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#25797;&#38271;&#29983;&#25104;&#34394;&#26500;&#20869;&#23481;&#65292;&#36825;&#38656;&#35201;&#35821;&#35328;&#24314;&#27169;&#21644;&#36923;&#36753;&#21465;&#36848;&#33021;&#21147;&#12290;&#19982;&#20165;&#20165;&#21542;&#23450;&#21442;&#25968;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#25552;&#21462;&#21644;&#28040;&#38500;&#21453;&#19987;&#23478;PEMs&#20013;&#30340;&#27531;&#32570;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#19968;&#33324;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of ``expert'' PEM and ``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of tru
&lt;/p&gt;</description></item><item><title>&#22312;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#38656;&#35201;&#20851;&#27880;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#26368;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#24314;&#27169;&#26694;&#26550;&#65292;&#24110;&#21161;&#20225;&#19994;&#28145;&#20837;&#20102;&#35299;&#21644;&#35780;&#20272;&#36825;&#20123;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.08061</link><description>&lt;p&gt;
&#39640;&#25104;&#26412;&#22256;&#22659;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#26368;&#20248;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models. (arXiv:2308.08061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08061
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#38656;&#35201;&#20851;&#27880;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#26368;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#24314;&#27169;&#26694;&#26550;&#65292;&#24110;&#21161;&#20225;&#19994;&#28145;&#20837;&#20102;&#35299;&#21644;&#35780;&#20272;&#36825;&#20123;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20026;&#20219;&#20309;&#20135;&#21697;/&#24212;&#29992;&#31243;&#24207;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#36890;&#24120;&#24076;&#26395;&#20855;&#22791;&#19977;&#20010;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#24212;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#22312;&#25105;&#20204;&#23545;&#39046;&#22495;&#30693;&#35782;&#30340;&#21457;&#23637;&#20013;&#21487;&#20197;&#25193;&#23637;&#20854;&#29992;&#36884;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#24212;&#35813;&#26159;&#21487;&#35780;&#20272;&#30340;&#65292;&#36825;&#26679;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#21487;&#20197;&#26377;&#28165;&#26224;&#30340;&#24615;&#33021;&#25351;&#26631;&#21644;&#35745;&#31639;&#36825;&#20123;&#25351;&#26631;&#30340;&#21487;&#34892;&#26041;&#24335;&#12290;&#26368;&#21518;&#65292;&#37096;&#32626;&#24212;&#23613;&#21487;&#33021;&#22320;&#25104;&#26412;&#26368;&#20248;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36825;&#19977;&#20010;&#30446;&#26631;&#65288;&#21363;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#26368;&#20248;&#21270;&#65289;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24448;&#24448;&#26159;&#30456;&#23545;&#29420;&#31435;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23613;&#31649;&#20854;&#22312;&#20256;&#32479;NLP&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20225;&#19994;&#22312;&#23545;&#36825;&#39033;&#25216;&#26415;&#36827;&#34892;&#37325;&#22823;&#25237;&#36164;&#20043;&#21069;&#38656;&#35201;&#20180;&#32454;&#35780;&#20272;&#25152;&#26377;&#19977;&#20010;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#24314;&#27169;&#26694;&#26550;&#65292;&#20026;&#20225;&#19994;&#25552;&#20379;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#22797;&#26434;&#22240;&#32032;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying machine learning models in production for any product/application, there are three properties that are commonly desired. First, the models should be generalizable, in that we can extend it to further use cases as our knowledge of the domain area develops. Second they should be evaluable, so that there are clear metrics for performance and the calculation of those metrics in production settings are feasible. Finally, the deployment should be cost-optimal as far as possible. In this paper we propose that these three objectives (i.e. generalization, evaluation and cost-optimality) can often be relatively orthogonal and that for large language models, despite their performance over conventional NLP models, enterprises need to carefully assess all the three factors before making substantial investments in this technology. We propose a framework for generalization, evaluation and cost-modeling specifically tailored to large language models, offering insights into the intricaci
&lt;/p&gt;</description></item><item><title>DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08043</link><description>&lt;p&gt;
DiagGPT:&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08043
&lt;/p&gt;
&lt;p&gt;
DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;AI&#27169;&#22411;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36741;&#21161;&#20154;&#31867;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;AI&#20316;&#20026;&#32842;&#22825;&#20195;&#29702;&#20154;&#30340;&#37325;&#35201;&#24212;&#29992;&#26159;&#22238;&#31572;&#20154;&#31867;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;LLMs&#22312;&#22238;&#31572;&#19968;&#33324;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#29087;&#32451;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#35786;&#26029;&#22330;&#26223;(&#22914;&#27861;&#24459;&#25110;&#21307;&#30103;&#21672;&#35810;)&#20013;&#65292;&#22522;&#26412;&#30340;&#38382;&#31572;&#23545;&#35805;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;(TOD)&#65292;&#20854;&#20013;AI&#32842;&#22825;&#20195;&#29702;&#38656;&#35201;&#20027;&#21160;&#25552;&#38382;&#24182;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#12290;&#20197;&#21069;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;TOD&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#24403;&#21069;&#30340;LLMs&#24182;&#26410;&#22266;&#26377;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DiagGPT (Diagnosis GPT)&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;LLMs&#25512;&#24191;&#21040;TOD&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, are becoming increasingly sophisticated, demonstrating capabilities that closely resemble those of humans. These AI models are playing an essential role in assisting humans with a wide array of tasks in daily life. A significant application of AI is its use as a chat agent, responding to human inquiries across various domains. Current LLMs have shown proficiency in answering general questions. However, basic question-answering dialogue often falls short in complex diagnostic scenarios, such as legal or medical consultations. These scenarios typically necessitate Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively pose questions and guide users towards specific task completion. Previous fine-tuning models have underperformed in TOD, and current LLMs do not inherently possess this capability. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD scenarios. Our e
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#20154;&#24037;&#32676;&#20307;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26500;&#24314;&#23454;&#39564;&#32676;&#20307;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20856;&#22411;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2308.08032</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#32676;&#20307;&#22312;&#31070;&#32463;&#27169;&#22411;&#20013;&#30740;&#31350;&#24515;&#29702;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Using Artificial Populations to Study Psychological Phenomena in Neural Models. (arXiv:2308.08032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08032
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#32676;&#20307;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26500;&#24314;&#23454;&#39564;&#32676;&#20307;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20856;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#28608;&#22686;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#35797;&#22270;&#22312;&#27169;&#22411;&#20013;&#26816;&#27979;&#20154;&#31867;&#35748;&#30693;&#34892;&#20026;&#23384;&#22312;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19982;&#20154;&#31867;&#24515;&#29702;&#23398;&#19968;&#26679;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#34892;&#20026;&#30340;&#30740;&#31350;&#24517;&#39035;&#22312;&#36866;&#24403;&#35268;&#27169;&#30340;&#36866;&#24403;&#32676;&#20307;&#20013;&#36827;&#34892;&#65292;&#25165;&#33021;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24037;&#20316;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#39640;&#25928;&#22320;&#26500;&#24314;&#23454;&#39564;&#32676;&#20307;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#24037;&#20855; PopulationLM &#24050;&#32463;&#24320;&#28304;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25991;&#29486;&#20013;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#20174;&#24403;&#21069;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#24037;&#20316;&#20013;&#25552;&#20379;&#20102;&#21160;&#26426;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20854;&#20182;&#31185;&#23398;&#30028;&#30340;&#26041;&#27861;&#35770;&#25945;&#35757;&#65292;&#24182;&#35797;&#22270;&#23637;&#31034;&#23427;&#20204;&#22312;&#20004;&#20010;&#20154;&#24037;&#32676;&#20307;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#22522;&#20110;&#32676;&#20307;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#20856;&#22411;&#34892;&#20026;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent proliferation of research into transformer based natural language processing has led to a number of studies which attempt to detect the presence of human-like cognitive behavior in the models. We contend that, as is true of human psychology, the investigation of cognitive behavior in language models must be conducted in an appropriate population of an appropriate size for the results to be meaningful. We leverage work in uncertainty estimation in a novel approach to efficiently construct experimental populations. The resultant tool, PopulationLM, has been made open source. We provide theoretical grounding in the uncertainty estimation literature and motivation from current cognitive work regarding language models. We discuss the methodological lessons from other scientific communities and attempt to demonstrate their application to two artificial population studies. Through population based experimentation we find that language models exhibit behavior consistent with typical
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35821;&#35328;&#31070;&#32463;&#34920;&#31034;&#30340;&#31471;&#21040;&#31471;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#25628;&#32034;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#20851;&#38190;&#35789;&#25628;&#32034;&#31995;&#32479;&#65292;&#22312;&#38271;&#26597;&#35810;&#21644;&#19981;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#26597;&#35810;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.08027</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#35821;&#35328;&#31070;&#32463;&#34920;&#31034;&#30340;&#31471;&#21040;&#31471;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
End-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations. (arXiv:2308.08027v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35821;&#35328;&#31070;&#32463;&#34920;&#31034;&#30340;&#31471;&#21040;&#31471;&#24320;&#25918;&#35789;&#27719;&#20851;&#38190;&#35789;&#25628;&#32034;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#20851;&#38190;&#35789;&#25628;&#32034;&#31995;&#32479;&#65292;&#22312;&#38271;&#26597;&#35810;&#21644;&#19981;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#26597;&#35810;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20851;&#38190;&#35789;&#25628;&#32034;&#31995;&#32479;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#36755;&#20986;&#19978;&#36816;&#34892;&#65292;&#23548;&#33268;&#23427;&#20204;&#20855;&#26377;&#22797;&#26434;&#30340;&#32034;&#24341;&#21644;&#25628;&#32034;&#27969;&#31243;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#24320;&#22987;&#23545;&#19981;&#38656;&#35201;ASR&#30340;&#26041;&#27861;&#24863;&#20852;&#36259;&#65292;&#20197;&#31616;&#21270;&#25628;&#32034;&#27969;&#31243;&#12290;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;ASR-free&#20851;&#38190;&#35789;&#25628;&#32034;&#27169;&#22411;&#65292;&#23427;&#22312;&#20445;&#25345;&#39640;&#25928;&#21644;&#31616;&#21270;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#26597;&#35810;&#21644;&#25991;&#26723;&#36890;&#36807;&#19968;&#23545;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#36827;&#34892;&#32534;&#30721;&#65292;&#32534;&#30721;&#36890;&#36807;&#28857;&#31215;&#36827;&#34892;&#32452;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#23545;&#27169;&#22411;&#30340;&#35814;&#32454;&#20998;&#26512;&#23545;&#36825;&#39033;&#24037;&#20316;&#36827;&#34892;&#20102;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22810;&#35821;&#35328;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#23613;&#31649;&#22312;&#30701;&#26597;&#35810;&#21644;&#35789;&#27719;&#20869;&#30340;&#26597;&#35810;&#26041;&#38754;&#26410;&#33021;&#19982;&#24378;&#22522;&#20110;ASR&#30340;&#20256;&#32479;&#20851;&#38190;&#35789;&#25628;&#32034;&#31995;&#32479;&#21305;&#37197;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38271;&#26597;&#35810;&#21644;&#26410;&#20986;&#29616;&#22312;&#35757;&#32451;&#38598;&#30340;&#26597;&#35810;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional keyword search systems operate on automatic speech recognition (ASR) outputs, which causes them to have a complex indexing and search pipeline. This has led to interest in ASR-free approaches to simplify the search procedure. We recently proposed a neural ASR-free keyword search model which achieves competitive performance while maintaining an efficient and simplified pipeline, where queries and documents are encoded with a pair of recurrent neural network encoders and the encodings are combined with a dot-product. In this article, we extend this work with multilingual pretraining and detailed analysis of the model. Our experiments show that the proposed multilingual training significantly improves the model performance and that despite not matching a strong ASR-based conventional keyword search system for short queries and queries comprising in-vocabulary words, the proposed model outperforms the ASR-based system for long queries and queries that do not appear in the trai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#26159;&#21542;&#33021;&#22815;&#20986;&#29616;&#31867;&#20284;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#22238;&#25351;&#32467;&#26500;&#30340;&#29616;&#35937;&#65292;&#21457;&#29616;&#24102;&#26377;&#22238;&#25351;&#32467;&#26500;&#30340;&#35821;&#35328;&#23545;&#31070;&#32463;&#27169;&#22411;&#26469;&#35828;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#36825;&#20123;&#32467;&#26500;&#20250;&#22312;&#27169;&#22411;&#20043;&#38388;&#33258;&#28982;&#22320;&#20986;&#29616;&#65292;&#24182;&#19988;&#22686;&#21152;&#23545;&#35828;&#35805;&#32773;&#25928;&#29575;&#30340;&#21387;&#21147;&#20250;&#22686;&#21152;&#36825;&#20123;&#32467;&#26500;&#30340;&#26222;&#36941;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07984</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#20986;&#29616;&#20102;&#35821;&#29992;&#24615;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Anaphoric Structure Emerges Between Neural Networks. (arXiv:2308.07984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#26159;&#21542;&#33021;&#22815;&#20986;&#29616;&#31867;&#20284;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#22238;&#25351;&#32467;&#26500;&#30340;&#29616;&#35937;&#65292;&#21457;&#29616;&#24102;&#26377;&#22238;&#25351;&#32467;&#26500;&#30340;&#35821;&#35328;&#23545;&#31070;&#32463;&#27169;&#22411;&#26469;&#35828;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#36825;&#20123;&#32467;&#26500;&#20250;&#22312;&#27169;&#22411;&#20043;&#38388;&#33258;&#28982;&#22320;&#20986;&#29616;&#65292;&#24182;&#19988;&#22686;&#21152;&#23545;&#35828;&#35805;&#32773;&#25928;&#29575;&#30340;&#21387;&#21147;&#20250;&#22686;&#21152;&#36825;&#20123;&#32467;&#26500;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#29992;&#23398;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#24471;&#35828;&#35805;&#32773;&#33021;&#22815;&#36890;&#36807;&#30465;&#30053;&#21644;&#22238;&#25351;&#31561;&#32467;&#26500;&#39640;&#25928;&#22320;&#36827;&#34892;&#20132;&#27969;&#65292;&#32780;&#19981;&#20250;&#20007;&#22833;&#24847;&#20041;&#12290;&#36825;&#20123;&#32467;&#26500;&#35201;&#27714;&#21548;&#32773;&#35299;&#37322;&#19968;&#20010;&#22810;&#20041;&#30340;&#24418;&#24335;&#65292;&#27604;&#22914;&#20195;&#35789;&#65292;&#24182;&#19988;&#25512;&#26029;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#26469;&#30830;&#23450;&#20195;&#35789;&#25152;&#25351;&#30340;&#23545;&#35937;&#12290;&#23613;&#31649;&#20250;&#24341;&#20837;&#27495;&#20041;&#65292;&#22238;&#25351;&#22312;&#20154;&#31867;&#35821;&#35328;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20013;&#22238;&#25351;&#32467;&#26500;&#30340;&#36215;&#28304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#22312;&#35757;&#32451;&#29992;&#20110;&#35299;&#20915;&#20132;&#27969;&#20219;&#21153;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#20986;&#29616;&#31867;&#20284;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;&#39318;&#20808;&#65292;&#23613;&#31649;&#21487;&#33021;&#22686;&#21152;&#27495;&#20041;&#65292;&#24102;&#26377;&#22238;&#25351;&#32467;&#26500;&#30340;&#35821;&#35328;&#23545;&#31070;&#32463;&#27169;&#22411;&#26469;&#35828;&#26159;&#21487;&#23398;&#20064;&#30340;&#12290;&#20854;&#27425;&#65292;&#22238;&#25351;&#32467;&#26500;&#22312;&#27169;&#22411;&#20043;&#38388;&#8220;&#33258;&#28982;&#8221;&#22320;&#20986;&#29616;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#24341;&#20837;&#23545;&#35828;&#35805;&#32773;&#26126;&#30830;&#30340;&#25928;&#29575;&#21387;&#21147;&#20250;&#22686;&#21152;&#36825;&#20123;&#32467;&#26500;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pragmatics is core to natural language, enabling speakers to communicate efficiently with structures like ellipsis and anaphora that can shorten utterances without loss of meaning. These structures require a listener to interpret an ambiguous form - like a pronoun - and infer the speaker's intended meaning - who that pronoun refers to. Despite potential to introduce ambiguity, anaphora is ubiquitous across human language. In an effort to better understand the origins of anaphoric structure in natural language, we look to see if analogous structures can emerge between artificial neural networks trained to solve a communicative task. We show that: first, despite the potential for increased ambiguity, languages with anaphoric structures are learnable by neural models. Second, anaphoric structures emerge between models 'naturally' without need for additional constraints. Finally, introducing an explicit efficiency pressure on the speaker increases the prevalence of these structures. We con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20114;&#32852;&#32593;&#19978;&#21322;&#30495;&#30456;&#30340;&#24191;&#27867;&#23384;&#22312;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#21322;&#30495;&#30456;&#26816;&#27979;&#27169;&#22411;&#21644;&#22768;&#26126;&#32534;&#36753;&#27169;&#22411;&#30340;&#20840;&#38754;&#27969;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;T5&#27169;&#22411;&#36827;&#34892;&#21463;&#25511;&#22768;&#26126;&#32534;&#36753;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34394;&#20551;&#20449;&#24687;&#25581;&#31359;&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21322;&#30495;&#30456;&#26816;&#27979;&#27169;&#22411;&#19978;&#21019;&#36896;&#20102;&#26032;&#30340;&#24615;&#33021;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.07973</link><description>&lt;p&gt;
"&#35880;&#38450;&#27450;&#39575;": &#36890;&#36807;&#21463;&#25511;&#22768;&#26126;&#32534;&#36753;&#26816;&#27979;&#21322;&#30495;&#30456;&#24182;&#25581;&#31359;
&lt;/p&gt;
&lt;p&gt;
"Beware of deception": Detecting Half-Truth and Debunking it through Controlled Claim Editing. (arXiv:2308.07973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20114;&#32852;&#32593;&#19978;&#21322;&#30495;&#30456;&#30340;&#24191;&#27867;&#23384;&#22312;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#21322;&#30495;&#30456;&#26816;&#27979;&#27169;&#22411;&#21644;&#22768;&#26126;&#32534;&#36753;&#27169;&#22411;&#30340;&#20840;&#38754;&#27969;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;T5&#27169;&#22411;&#36827;&#34892;&#21463;&#25511;&#22768;&#26126;&#32534;&#36753;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34394;&#20551;&#20449;&#24687;&#25581;&#31359;&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21322;&#30495;&#30456;&#26816;&#27979;&#27169;&#22411;&#19978;&#21019;&#36896;&#20102;&#26032;&#30340;&#24615;&#33021;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#21322;&#30495;&#30456;&#21363;&#21253;&#21547;&#19968;&#20123;&#30495;&#23454;&#20449;&#24687;&#20294;&#26368;&#32456;&#20855;&#26377;&#27450;&#39575;&#24615;&#30340;&#38472;&#36848;&#36234;&#26469;&#36234;&#22810;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#21322;&#30495;&#30456;&#26816;&#27979;&#27169;&#22411;&#21644;&#22768;&#26126;&#32534;&#36753;&#27169;&#22411;&#30340;&#20840;&#38754;&#27969;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;T5&#27169;&#22411;&#36827;&#34892;&#21463;&#25511;&#22768;&#26126;&#32534;&#36753;&#65307;&#36825;&#37324;&#30340;&#8220;&#21463;&#25511;&#8221;&#24847;&#21619;&#30528;&#23545;&#22768;&#26126;&#30340;&#36873;&#23450;&#37096;&#20998;&#36827;&#34892;&#31934;&#30830;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32534;&#36753;&#21518;&#30340;&#22768;&#26126;&#19978;&#21462;&#24471;&#20102;&#24179;&#22343;BLEU&#20998;&#25968;0.88&#65288;&#22312;0-1&#30340;&#33539;&#22260;&#20869;&#65289;&#21644;85%&#30340;&#34394;&#20551;&#20449;&#24687;&#25581;&#31359;&#24471;&#20998;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22522;&#20110;T5&#30340;&#26041;&#27861;&#22312;&#34394;&#20551;&#20449;&#24687;&#25581;&#31359;&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT2&#65292;RoBERTa&#65292;PEGASUS&#21644;Tailor&#65292;&#24179;&#22343;&#25913;&#36827;&#20998;&#21035;&#20026;82%&#65292;57%&#65292;42%&#21644;23%&#12290;&#36890;&#36807;&#25193;&#23637;LIAR PLUS&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;&#21322;&#30495;&#30456;&#26816;&#27979;&#27169;&#22411;&#26041;&#38754;&#23454;&#29616;&#20102;82%&#30340;F1&#20998;&#25968;&#65292;&#21019;&#36896;&#20102;&#35813;&#39046;&#22495;&#30340;&#26032;&#22522;&#20934;&#12290;&#23613;&#31649;&#20043;&#21069;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36807;&#21322;&#30495;&#30456;&#26816;&#27979;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#19978;&#26159;&#21069;&#25152;&#26410;&#26377;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of half-truths, which are statements containing some truth but that are ultimately deceptive, has risen with the increasing use of the internet. To help combat this problem, we have created a comprehensive pipeline consisting of a half-truth detection model and a claim editing model. Our approach utilizes the T5 model for controlled claim editing; "controlled" here means precise adjustments to select parts of a claim. Our methodology achieves an average BLEU score of 0.88 (on a scale of 0-1) and a disinfo-debunk score of 85% on edited claims. Significantly, our T5-based approach outperforms other Language Models such as GPT2, RoBERTa, PEGASUS, and Tailor, with average improvements of 82%, 57%, 42%, and 23% in disinfo-debunk scores, respectively. By extending the LIAR PLUS dataset, we achieve an F1 score of 82% for the half-truth detection model, setting a new benchmark in the field. While previous attempts have been made at half-truth detection, our approach is, to the b
&lt;/p&gt;</description></item><item><title>MultiSChuBERT&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#65292;&#22312;&#23398;&#26415;&#25991;&#26723;&#36136;&#37327;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#32467;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#12289;&#36880;&#28176;&#35299;&#20923;&#35270;&#35273;&#23376;&#27169;&#22411;&#26435;&#37325;&#20197;&#21450;&#37319;&#29992;&#26368;&#26032;&#25991;&#26412;&#23884;&#20837;&#26367;&#25442;&#26631;&#20934;BERT$_{\textrm{BASE}}$&#23884;&#20837;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.07971</link><description>&lt;p&gt;
MultiSChuBERT: &#39640;&#25928;&#30340;&#23398;&#26415;&#25991;&#26723;&#36136;&#37327;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction. (arXiv:2308.07971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07971
&lt;/p&gt;
&lt;p&gt;
MultiSChuBERT&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#65292;&#22312;&#23398;&#26415;&#25991;&#26723;&#36136;&#37327;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#32467;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#12289;&#36880;&#28176;&#35299;&#20923;&#35270;&#35273;&#23376;&#27169;&#22411;&#26435;&#37325;&#20197;&#21450;&#37319;&#29992;&#26368;&#26032;&#25991;&#26412;&#23884;&#20837;&#26367;&#25442;&#26631;&#20934;BERT$_{\textrm{BASE}}$&#23884;&#20837;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#25991;&#26723;&#36136;&#37327;&#30340;&#33258;&#21160;&#35780;&#20272;&#26159;&#19968;&#39033;&#20855;&#26377;&#39640;&#28508;&#21147;&#24433;&#21709;&#30340;&#22256;&#38590;&#20219;&#21153;&#12290;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#23558;&#35270;&#35273;&#20449;&#24687;&#19982;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#23398;&#26415;&#25991;&#26723;&#36136;&#37327;&#39044;&#27979;&#20219;&#21153;&#19978;&#25552;&#39640;&#24615;&#33021;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#30340;&#39044;&#27979;&#27169;&#22411;MultiSChuBERT&#12290;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#65288;SChuBERT&#65289;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#65288;Inception V3&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#23398;&#26415;&#25991;&#26723;&#36136;&#37327;&#39044;&#27979;&#26041;&#38754;&#26377;&#19977;&#20010;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32467;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#32467;&#26524;&#19978;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36880;&#28176;&#35299;&#20923;&#35270;&#35273;&#23376;&#27169;&#22411;&#30340;&#26435;&#37325;&#21487;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#25968;&#25454;&#30340;&#36235;&#21183;&#65292;&#20174;&#32780;&#25552;&#39640;&#32467;&#26524;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37319;&#29992;&#26368;&#26032;&#30340;&#25991;&#26412;&#23884;&#20837;&#26367;&#25442;&#26631;&#20934;BERT$_{\textrm{BASE}}$&#23884;&#20837;&#26102;&#22810;&#27169;&#24577;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic assessment of the quality of scholarly documents is a difficult task with high potential impact. Multimodality, in particular the addition of visual information next to text, has been shown to improve the performance on scholarly document quality prediction (SDQP) tasks. We propose the multimodal predictive model MultiSChuBERT. It combines a textual model based on chunking full paper text and aggregating computed BERT chunk-encodings (SChuBERT), with a visual model based on Inception V3.Our work contributes to the current state-of-the-art in SDQP in three ways. First, we show that the method of combining visual and textual embeddings can substantially influence the results. Second, we demonstrate that gradual-unfreezing of the weights of the visual sub-model, reduces its tendency to ovefit the data, improving results. Third, we show the retained benefit of multimodality when replacing standard BERT$_{\textrm{BASE}}$ embeddings with more recent state-of-the-art text embedding 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#36827;&#34892;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20511;&#37492;&#20889;&#20316;&#25945;&#32946;&#23454;&#36341;&#65292;&#21033;&#29992;&#22810;&#38454;&#27573;&#21644;&#22810;&#20219;&#21153;&#30340;&#26694;&#26550;&#26469;&#25945;&#23548;LLMs&#20010;&#24615;&#21270;&#29983;&#25104;&#65292;&#20174;&#32780;&#25552;&#21319;&#20854;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07968</link><description>&lt;p&gt;
&#25945;&#20250;LLMs&#20010;&#24615;&#21270;&#8212;&#21463;&#20889;&#20316;&#25945;&#32946;&#21551;&#21457;&#30340;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Teach LLMs to Personalize -- An Approach inspired by Writing Education. (arXiv:2308.07968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#36827;&#34892;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20511;&#37492;&#20889;&#20316;&#25945;&#32946;&#23454;&#36341;&#65292;&#21033;&#29992;&#22810;&#38454;&#27573;&#21644;&#22810;&#20219;&#21153;&#30340;&#26694;&#26550;&#26469;&#25945;&#23548;LLMs&#20010;&#24615;&#21270;&#29983;&#25104;&#65292;&#20174;&#32780;&#25552;&#21319;&#20854;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#26159;&#36817;&#24180;&#26469;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#36890;&#36807;&#35774;&#35745;&#23450;&#21046;&#30340;&#29305;&#24449;&#25110;&#27169;&#22411;&#26469;&#19987;&#27880;&#20110;&#29305;&#23450;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#21463;&#21040;&#20889;&#20316;&#25945;&#32946;&#23454;&#36341;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#21644;&#22810;&#20219;&#21153;&#30340;&#26694;&#26550;&#26469;&#25945;&#23548;LLMs&#36827;&#34892;&#20010;&#24615;&#21270;&#29983;&#25104;&#12290;&#22312;&#20889;&#20316;&#25351;&#23548;&#20013;&#65292;&#20174;&#36164;&#28304;&#20013;&#25776;&#20889;&#25991;&#31456;&#30340;&#20219;&#21153;&#36890;&#24120;&#34987;&#20998;&#35299;&#20026;&#22810;&#20010;&#27493;&#39588;&#65292;&#21253;&#25324;&#26597;&#25214;&#12289;&#35780;&#20272;&#12289;&#24635;&#32467;&#12289;&#32508;&#21512;&#21644;&#25972;&#21512;&#20449;&#24687;&#12290;&#31867;&#20284;&#22320;&#65292;&#25105;&#20204;&#30340;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#21253;&#25324;&#22810;&#20010;&#38454;&#27573;&#65306;&#26816;&#32034;&#12289;&#25490;&#24207;&#12289;&#25688;&#35201;&#12289;&#32508;&#21512;&#21644;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#35774;&#32622;&#65292;&#36827;&#19968;&#27493;&#24110;&#21161;&#27169;&#22411;&#25552;&#39640;&#20854;&#29983;&#25104;&#33021;&#21147;&#65292;&#36825;&#21463;&#21040;&#25945;&#32946;&#35266;&#23519;&#21040;&#30340;&#23398;&#29983;&#38405;&#35835;&#33021;&#21147;&#21644;&#20889;&#20316;&#33021;&#21147;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized text generation is an emerging research area that has attracted much attention in recent years. Most studies in this direction focus on a particular domain by designing bespoke features or models. In this work, we propose a general approach for personalized text generation using large language models (LLMs). Inspired by the practice of writing education, we develop a multistage and multitask framework to teach LLMs for personalized generation. In writing instruction, the task of writing from sources is often decomposed into multiple steps that involve finding, evaluating, summarizing, synthesizing, and integrating information. Analogously, our approach to personalized text generation consists of multiple stages: retrieval, ranking, summarization, synthesis, and generation. In addition, we introduce a multitask setting that helps the model improve its generation ability further, which is inspired by the observation in education that a student's reading proficiency and writi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#27979;&#35797;&#21644;&#20462;&#22797;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;TIN&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#30456;&#20284;&#35821;&#22659;&#19979;&#30456;&#21516;&#21629;&#21517;&#23454;&#20307;&#30340;NER&#39044;&#27979;&#30456;&#21516;&#65292;&#20174;&#32780;&#25552;&#39640;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07937</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#27979;&#35797;&#21644;&#25913;&#36827;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automated Testing and Improvement of Named Entity Recognition Systems. (arXiv:2308.07937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#27979;&#35797;&#21644;&#20462;&#22797;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;TIN&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#30456;&#20284;&#35821;&#22659;&#19979;&#30456;&#21516;&#21629;&#21517;&#23454;&#20307;&#30340;NER&#39044;&#27979;&#30456;&#21516;&#65292;&#20174;&#32780;&#25552;&#39640;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#31995;&#32479;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#12290;&#36825;&#20123;&#31995;&#32479;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#22914;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#31572;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#21644;&#38590;&#20197;&#22788;&#29702;&#24615;&#21487;&#33021;&#20351;NER&#31995;&#32479;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21464;&#24471;&#19981;&#21487;&#38752;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20363;&#22914;&#65292;NER&#31995;&#32479;&#21487;&#33021;&#38169;&#35823;&#22320;&#23558;&#22899;&#24615;&#21517;&#23383;&#35782;&#21035;&#20026;&#21270;&#23398;&#29289;&#36136;&#65292;&#25110;&#32773;&#26080;&#27861;&#35782;&#21035;&#20986;&#23569;&#25968;&#32676;&#20307;&#30340;&#21517;&#23383;&#65292;&#20174;&#32780;&#23548;&#33268;&#29992;&#25143;&#19981;&#28385;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#24191;&#27867;&#36866;&#29992;&#30340;&#26041;&#27861;&#8212;&#8212;TIN&#65292;&#29992;&#20110;&#33258;&#21160;&#27979;&#35797;&#21644;&#20462;&#22797;&#21508;&#31181;NER&#31995;&#32479;&#12290;&#33258;&#21160;&#27979;&#35797;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#30456;&#20284;&#35821;&#22659;&#19979;&#30456;&#21516;&#21629;&#21517;&#23454;&#20307;&#30340;NER&#39044;&#27979;&#24212;&#35813;&#30456;&#21516;&#12290;&#33258;&#21160;&#20462;&#22797;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#30456;&#20284;&#30340;&#21629;&#21517;&#23454;&#20307;&#22312;&#30456;&#21516;&#35821;&#22659;&#19979;&#24212;&#35813;&#20855;&#26377;&#30456;&#21516;&#30340;NER&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition (NER) systems have seen rapid progress in recent years due to the development of deep neural networks. These systems are widely used in various natural language processing applications, such as information extraction, question answering, and sentiment analysis. However, the complexity and intractability of deep neural networks can make NER systems unreliable in certain circumstances, resulting in incorrect predictions. For example, NER systems may misidentify female names as chemicals or fail to recognize the names of minority groups, leading to user dissatisfaction. To tackle this problem, we introduce TIN, a novel, widely applicable approach for automatically testing and repairing various NER systems. The key idea for automated testing is that the NER predictions of the same named entities under similar contexts should be identical. The core idea for automated repairing is that similar named entities should have the same NER prediction under the same context.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;ChatGPT 3.5&#26469;&#36827;&#34892;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#65292;&#29305;&#21035;&#20851;&#27880;&#22806;&#27719;&#24066;&#22330;&#65292;&#36890;&#36807;&#38646;-shot&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;ChatGPT&#22312;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#32422;35&#65285;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.07935</link><description>&lt;p&gt;
&#29992;ChatGPT&#21464;&#38761;&#37329;&#34701;&#39046;&#22495;&#30340;&#24773;&#32490;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Transforming Sentiment Analysis in the Financial Domain with ChatGPT. (arXiv:2308.07935v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;ChatGPT 3.5&#26469;&#36827;&#34892;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#65292;&#29305;&#21035;&#20851;&#27880;&#22806;&#27719;&#24066;&#22330;&#65292;&#36890;&#36807;&#38646;-shot&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;ChatGPT&#22312;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#32422;35&#65285;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#22312;&#35299;&#35835;&#24066;&#22330;&#36235;&#21183;&#21644;&#25351;&#23548;&#25112;&#30053;&#20132;&#26131;&#20915;&#31574;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#24050;&#32463;&#20351;&#29992;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#65292;&#20294;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;ChatGPT 3.5&#65289;&#22312;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#24378;&#35843;&#22806;&#27719;&#24066;&#22330;&#65288;forex&#65289;&#65292;&#24320;&#21019;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#37319;&#29992;&#38646;-shot&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;&#19968;&#20221;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#22806;&#27719;&#30456;&#20851;&#26032;&#38395;&#26631;&#39064;&#25968;&#25454;&#38598;&#19978;&#26816;&#39564;&#22810;&#20010;ChatGPT&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#24471;&#20998;&#21644;&#24773;&#32490;&#20998;&#31867;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#31561;&#25351;&#26631;&#35780;&#20272;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#39044;&#27979;&#24773;&#32490;&#21644;&#24066;&#22330;&#22238;&#25253;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20316;&#20026;&#19968;&#31181;&#39069;&#22806;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#19982;FinBERT&#30456;&#27604;&#65292;ChatGPT&#22312;&#24773;&#32490;&#20998;&#26512;&#26041;&#38754;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#32422;35&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial sentiment analysis plays a crucial role in decoding market trends and guiding strategic trading decisions. Despite the deployment of advanced deep learning techniques and language models to refine sentiment analysis in finance, this study breaks new ground by investigating the potential of large language models, particularly ChatGPT 3.5, in financial sentiment analysis, with a strong emphasis on the foreign exchange market (forex). Employing a zero-shot prompting approach, we examine multiple ChatGPT prompts on a meticulously curated dataset of forex-related news headlines, measuring performance using metrics such as precision, recall, f1-score, and Mean Absolute Error (MAE) of the sentiment class. Additionally, we probe the correlation between predicted sentiment and market returns as an additional evaluation approach. ChatGPT, compared to FinBERT, a well-established sentiment analysis model for financial texts, exhibited approximately 35\% enhanced performance in sentiment 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30196;&#21574;&#26816;&#27979;&#27169;&#22411;&#65292;&#23558;&#22270;&#29255;&#21644;&#25551;&#36848;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#30196;&#21574;&#26679;&#26412;&#19982;&#20581;&#24247;&#26679;&#26412;&#22312;&#25991;&#26412;&#19982;&#22270;&#29255;&#30456;&#20851;&#24615;&#21644;&#22270;&#29255;&#28966;&#28857;&#21306;&#22495;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20174;&#32780;&#21487;&#20197;&#25552;&#39640;&#30196;&#21574;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07933</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#35780;&#20272;&#29992;&#20110;&#30196;&#21574;&#26816;&#27979;&#30340;&#22270;&#29255;&#25551;&#36848;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment. (arXiv:2308.07933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30196;&#21574;&#26816;&#27979;&#27169;&#22411;&#65292;&#23558;&#22270;&#29255;&#21644;&#25551;&#36848;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#30196;&#21574;&#26679;&#26412;&#19982;&#20581;&#24247;&#26679;&#26412;&#22312;&#25991;&#26412;&#19982;&#22270;&#29255;&#30456;&#20851;&#24615;&#21644;&#22270;&#29255;&#28966;&#28857;&#21306;&#22495;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20174;&#32780;&#21487;&#20197;&#25552;&#39640;&#30196;&#21574;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#29255;&#25551;&#36848;&#35821;&#38899;&#36827;&#34892;&#30196;&#21574;&#26816;&#27979;&#24050;&#32463;&#30740;&#31350;&#20102;30&#24180;&#12290;&#23613;&#31649;&#26377;&#36825;&#20040;&#38271;&#30340;&#21382;&#21490;&#65292;&#20808;&#21069;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#27491;&#24120;&#20154;&#21644;&#24739;&#26377;&#30196;&#21574;&#30151;&#30340;&#24739;&#32773;&#20043;&#38388;&#35821;&#38899;&#27169;&#24335;&#30340;&#24046;&#24322;&#65292;&#20294;&#27809;&#26377;&#30452;&#25509;&#21033;&#29992;&#22270;&#29255;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#23558;&#22270;&#29255;&#21644;&#25551;&#36848;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#34701;&#20837;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#30340;&#30196;&#21574;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#30196;&#21574;&#26679;&#26412;&#21644;&#20581;&#24247;&#26679;&#26412;&#22312;&#25991;&#26412;&#19982;&#22270;&#29255;&#30456;&#20851;&#24615;&#20197;&#21450;&#22270;&#29255;&#30340;&#28966;&#28857;&#21306;&#22495;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#24046;&#24322;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#30196;&#21574;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25991;&#26412;&#19982;&#22270;&#29255;&#30340;&#30456;&#20851;&#24615;&#23545;&#26679;&#26412;&#30340;&#21477;&#23376;&#36827;&#34892;&#25490;&#24207;&#21644;&#36807;&#28388;&#12290;&#25105;&#20204;&#36824;&#26681;&#25454;&#22270;&#29255;&#30340;&#28966;&#28857;&#21306;&#22495;&#30830;&#23450;&#20102;&#35805;&#39064;&#65292;&#24182;&#26681;&#25454;&#28966;&#28857;&#21306;&#22495;&#23545;&#21477;&#23376;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using picture description speech for dementia detection has been studied for 30 years. Despite the long history, previous models focus on identifying the differences in speech patterns between healthy subjects and patients with dementia but do not utilize the picture information directly. In this paper, we propose the first dementia detection models that take both the picture and the description texts as inputs and incorporate knowledge from large pre-trained image-text alignment models. We observe the difference between dementia and healthy samples in terms of the text's relevance to the picture and the focused area of the picture. We thus consider such a difference could be used to enhance dementia detection accuracy. Specifically, we use the text's relevance to the picture to rank and filter the sentences of the samples. We also identified focused areas of the picture as topics and categorized the sentences according to the focused areas. We propose three advanced models that pre-pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340;3D&#20960;&#20309;&#19982;2D&#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07931</link><description>&lt;p&gt;
&#31934;&#31616;&#29305;&#24449;&#22330;&#20351;&#24471;&#35821;&#35328;&#24341;&#23548;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340;3D&#20960;&#20309;&#19982;2D&#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21644;&#35821;&#35328;&#30417;&#30563;&#30340;&#22270;&#20687;&#27169;&#22411;&#21253;&#21547;&#20102;&#19990;&#30028;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#23545;&#20110;&#27867;&#21270;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#38656;&#35201;&#23545; 3D &#20960;&#20309;&#30340;&#35814;&#32454;&#29702;&#35299;&#65292;&#36825;&#22312; 2D &#22270;&#20687;&#29305;&#24449;&#20013;&#24448;&#24448;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340; 3D &#20960;&#20309;&#19982; 2D &#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#26469;&#24357;&#21512;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340; 2D &#21040; 3D &#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545; 6 &#33258;&#30001;&#24230;&#25235;&#21462;&#21644;&#25918;&#32622;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20808;&#39564;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#33258;&#28982;&#27867;&#21270;&#12290;&#36890;&#36807;&#20174;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; CLIP &#20013;&#31934;&#31616;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#30001;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#26032;&#39062;&#23545;&#35937;&#36827;&#34892;&#25805;&#20316;&#30340;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#26410;&#35265;&#36807;&#30340;&#34920;&#36798;&#21644;&#26032;&#39062;&#31867;&#21035;&#30340;&#29289;&#20307;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;Meituan&#25628;&#32034;&#20013;&#36827;&#34892;&#30456;&#20851;&#24615;&#24314;&#27169;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#21644;&#21305;&#37197;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.07711</link><description>&lt;p&gt;
SPM: Meituan&#25628;&#32034;&#20013;&#29992;&#20110;&#30456;&#20851;&#24615;&#24314;&#27169;&#30340;&#32467;&#26500;&#21270;&#39044;&#35757;&#32451;&#21644;&#21305;&#37197;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search. (arXiv:2308.07711v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;Meituan&#25628;&#32034;&#20013;&#36827;&#34892;&#30456;&#20851;&#24615;&#24314;&#27169;&#30340;&#26032;&#39062;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#21644;&#21305;&#37197;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#21830;&#25628;&#32034;&#20013;&#65292;&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#28385;&#36275;&#29992;&#25143;&#20307;&#39564;&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#19982;&#20256;&#32479;&#30340;&#30005;&#21830;&#24179;&#21488;&#19981;&#21516;&#65292;&#29992;&#25143;&#22312;&#32654;&#22242;&#31561;&#29983;&#27963;&#26381;&#21153;&#24179;&#21488;&#19978;&#36827;&#34892;&#25628;&#32034;&#20027;&#35201;&#26159;&#20026;&#20102;&#20135;&#21697;&#20379;&#24212;&#21830;&#65292;&#36825;&#20123;&#20379;&#24212;&#21830;&#36890;&#24120;&#25317;&#26377;&#20016;&#23500;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20363;&#22914;&#21517;&#31216;&#12289;&#22320;&#22336;&#12289;&#31867;&#21035;&#12289;&#25104;&#21315;&#19978;&#19975;&#30340;&#20135;&#21697;&#12290;&#20351;&#29992;&#36825;&#20123;&#20016;&#23500;&#30340;&#32467;&#26500;&#21270;&#20869;&#23481;&#36827;&#34892;&#25628;&#32034;&#30456;&#20851;&#24615;&#24314;&#27169;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20027;&#35201;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19981;&#21516;&#23383;&#27573;&#30340;&#32467;&#26500;&#21270;&#25991;&#26723;&#23384;&#22312;&#35821;&#35328;&#20998;&#24067;&#24046;&#24322;&#65292;&#26080;&#27861;&#30452;&#25509;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65288;&#22914;BERT&#65289;&#12290;&#65288;2&#65289;&#19981;&#21516;&#23383;&#27573;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#65292;&#19988;&#38271;&#24230;&#24046;&#24322;&#24456;&#22823;&#65292;&#24456;&#38590;&#25552;&#21462;&#23545;&#30456;&#20851;&#24615;&#21305;&#37197;&#26377;&#24110;&#21161;&#30340;&#25991;&#26723;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#39044;&#35757;&#32451;&#21644;&#21305;&#37197;&#26550;&#26500;&#65292;&#29992;&#20110;&#20016;&#23500;&#32467;&#26500;&#30340;&#30456;&#20851;&#24615;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In e-commerce search, relevance between query and documents is an essential requirement for satisfying user experience. Different from traditional e-commerce platforms that offer products, users search on life service platforms such as Meituan mainly for product providers, which usually have abundant structured information, e.g. name, address, category, thousands of products. Modeling search relevance with these rich structured contents is challenging due to the following issues: (1) there is language distribution discrepancy among different fields of structured document, making it difficult to directly adopt off-the-shelf pretrained language model based methods like BERT. (2) different fields usually have different importance and their length vary greatly, making it difficult to extract document information helpful for relevance matching.  To tackle these issues, in this paper we propose a novel two-stage pretraining and matching architecture for relevance matching with rich structure
&lt;/p&gt;</description></item><item><title>&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.05342</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05342
&lt;/p&gt;
&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20013;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#19968;&#30452;&#22312;&#19981;&#26029;&#25552;&#39640;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;&#25552;&#31034;&#30340;&#30740;&#31350;&#22686;&#24378;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20803;&#35748;&#30693;&#25552;&#31034; (MP)&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#20154;&#31867;&#20869;&#30465;&#25512;&#29702;&#36807;&#31243;&#21551;&#21457;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;MP&#65292;LLMs&#32463;&#21382;&#19968;&#31995;&#21015;&#26377;&#32467;&#26500;&#12289;&#33258;&#25105;&#24847;&#35782;&#30340;&#35780;&#20272;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#22312;&#30693;&#35782;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#20116;&#20010;&#24120;&#35265;&#30340;LLMs&#65306;Llama2&#12289;Vicuna&#12289;PaLM&#12289;GPT-3.5&#21644;GPT-4&#65292;&#23427;&#20204;&#37117;&#28085;&#30422;&#20102;&#26469;&#33258;GLUE&#21644;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#21508;&#31181;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#37197;&#22791;MP&#30340;PaLM&#25509;&#36817;&#20854;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#36328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;MP&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce metacognitive prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting meth
&lt;/p&gt;</description></item><item><title>SeACo-Paraformer&#26159;&#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#28909;&#35789;&#33258;&#23450;&#20041;&#33021;&#21147;&#30340;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36807;&#28388;&#22823;&#35268;&#27169;&#28909;&#35789;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03266</link><description>&lt;p&gt;
SeACo-Paraformer:&#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#28909;&#35789;&#33258;&#23450;&#20041;&#33021;&#21147;&#30340;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability. (arXiv:2308.03266v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03266
&lt;/p&gt;
&lt;p&gt;
SeACo-Paraformer&#26159;&#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#28909;&#35789;&#33258;&#23450;&#20041;&#33021;&#21147;&#30340;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36807;&#28388;&#22823;&#35268;&#27169;&#28909;&#35789;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#35789;&#33258;&#23450;&#20041;&#26159;ASR&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#33258;&#23450;&#20041;&#23454;&#20307;&#12289;&#20154;&#29289;&#21644;&#20854;&#20182;&#30701;&#35821;&#30340;&#21517;&#31216;&#20855;&#26377;&#20215;&#20540;&#12290;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;ASR&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#38544;&#24335;&#21644;&#26174;&#24335;&#24314;&#27169;&#31574;&#30053;&#37117;&#24471;&#21040;&#20102;&#21457;&#23637;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#36824;&#19981;&#38169;&#65292;&#20294;&#20173;&#23384;&#22312;&#26576;&#20123;&#32570;&#28857;&#65292;&#20363;&#22914;&#22312;&#25928;&#26524;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#20041;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;Paraformer (SeACo-Paraformer)&#30340;&#38750;&#33258;&#22238;&#24402;ASR&#31995;&#32479;&#65292;&#20855;&#26377;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#28909;&#35789;&#33258;&#23450;&#20041;&#33021;&#21147;&#12290;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;AED&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#22522;&#20110;NAR&#27169;&#22411;&#30340;&#25928;&#29575;&#20197;&#21450;&#22312;&#19978;&#19979;&#25991;&#24314;&#27169;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;&#22312;50,000&#23567;&#26102;&#30340;&#24037;&#19994;&#22823;&#25968;&#25454;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#33258;&#23450;&#20041;&#21644;&#24120;&#35268;ASR&#20219;&#21153;&#20013;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36807;&#28388;&#22823;&#35268;&#27169;&#30340;&#28909;&#35789;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hotword customization is one of the important issues remained in ASR field it is of value to enable users of ASR systems to customize names of entities, persons and other phrases. The past few years have seen both implicit and explicit modeling strategies for ASR contextualization developed. While these approaches have performed adequately, they still exhibit certain shortcomings such as instability in effectiveness. In this paper we propose Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) a novel NAR based ASR system with flexible and effective hotword customization ability. It combines the accuracy of the AED-based model, the efficiency of the NAR model, and the excellent performance in contextualization. In 50,000 hours industrial big data experiments, our proposed model outperforms strong baselines in customization and general ASR tasks. Besides, we explore an efficient way to filter large scale incoming hotwords for further improvement. The source codes and industrial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16082</link><description>&lt;p&gt;
EnrichEvent: &#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#20026;&#26032;&#20986;&#29616;&#30340;&#20107;&#20214;&#25552;&#20379;&#20016;&#23500;&#30340;&#31038;&#20132;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction. (arXiv:2307.16082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35782;&#21035;&#26032;&#20852;&#31038;&#20132;&#20107;&#20214;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#31038;&#20132;&#25968;&#25454;&#36827;&#34892;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#24179;&#21488;&#24050;&#25104;&#20026;&#20256;&#25773;&#21644;&#35752;&#35770;&#30495;&#23454;&#20107;&#20214;&#20449;&#24687;&#30340;&#20851;&#38190;&#24179;&#21488;&#65292;&#20026;&#21450;&#26089;&#21457;&#29616;&#26377;&#26032;&#38395;&#20215;&#20540;&#30340;&#20107;&#20214;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#20165;&#21033;&#29992;&#20851;&#38190;&#35789;&#31361;&#21457;&#24615;&#25110;&#32593;&#32476;&#32467;&#26500;&#26469;&#26816;&#27979;&#28909;&#28857;&#20107;&#20214;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20107;&#20214;&#21644;&#31038;&#20132;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35328;&#65292;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#22312;&#36798;&#21040;&#36235;&#21183;&#29366;&#24577;&#20043;&#21069;&#35782;&#21035;&#20986;&#26032;&#20986;&#29616;&#30340;&#31038;&#20132;&#20107;&#20214;&#12290;&#31038;&#20132;&#25968;&#25454;&#65292;&#20363;&#22914;&#25512;&#25991;&#65292;&#20855;&#26377;&#25340;&#20889;&#38169;&#35823;&#12289;&#19981;&#23436;&#25972;&#24615;&#12289;&#27495;&#20041;&#24615;&#21644;&#35821;&#35328;&#19981;&#35268;&#33539;&#24615;&#65292;&#20197;&#21450;&#24847;&#35265;&#26041;&#38754;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#26469;&#23398;&#20064;&#20107;&#20214;&#30340;&#28436;&#21464;&#29305;&#24449;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20960;&#20046;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#27969;&#24335;&#31038;&#20132;&#25968;&#25454;&#30340;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social platforms have emerged as a crucial platform for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. However, most existing approaches for event detection solely exploit keyword burstiness or network structures to detect hot events. Thus, they often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15780</link><description>&lt;p&gt;
LLM-Rec: &#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
LLM-Rec: Personalized Recommendation via Prompting Large Language Models. (arXiv:2307.15780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#37319;&#29992;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;LLM-Rec&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#31574;&#30053;&#65306;&#65288;1&#65289;&#22522;&#30784;&#24341;&#23548;&#65292;&#65288;2&#65289;&#25512;&#33616;&#39537;&#21160;&#24341;&#23548;&#65292;&#65288;3&#65289;&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#65292;&#21644;&#65288;4&#65289;&#25512;&#33616;&#39537;&#21160;+&#21442;&#19982;&#24341;&#23548;&#24341;&#23548;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#21407;&#22987;&#20869;&#23481;&#25551;&#36848;&#19982;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#36755;&#20837;&#25991;&#26412;&#32467;&#21512;&#36215;&#26469;&#65292;&#37319;&#29992;&#36825;&#20123;&#24341;&#23548;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20010;&#24615;&#21270;&#20869;&#23481;&#25512;&#33616;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#30340;&#24341;&#23548;&#21644;&#36755;&#20837;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation. Our proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that combining the original content description with the augmented input text generated by LLM using these prompting strategies leads to improved recommendation performance. This finding highlights the importance of incorporating diverse prompts and input augmentation techniques to enhance the recommendation capabilities with large language models for personalized content recommendation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#26368;&#20248;&#24494;&#35843;&#27169;&#22411;&#22312;&#24179;&#34913;&#20934;&#30830;&#24230;&#19978;&#32988;&#36807;GPT-3.5&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.14385</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#25968;&#25454;&#39044;&#27979;&#24515;&#29702;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Mental Health Prediction via Online Text Data. (arXiv:2307.14385v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#26368;&#20248;&#24494;&#35843;&#27169;&#22411;&#22312;&#24179;&#34913;&#20934;&#30830;&#24230;&#19978;&#32988;&#36807;GPT-3.5&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#25552;&#21319;&#20351;&#24471;&#22810;&#31181;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLM&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#29702;&#35299;&#21644;&#25913;&#36827;&#30740;&#31350;&#20960;&#20046;&#27809;&#26377;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#22810;&#31181;LLM&#65288;&#21253;&#25324;Alpaca&#65292;Alpaca-LoRA&#21644;GPT-3.5&#65289;&#22312;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#22810;&#20010;&#24515;&#29702;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#38646;-shot&#25552;&#31034;&#12289;&#23569;-shot&#25552;&#31034;&#21644;&#25351;&#20196;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#38646;-shot&#21644;&#23569;-shot&#25552;&#31034;&#35774;&#35745;&#19978;&#22312;&#24515;&#29702;&#20581;&#24247;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26377;&#38480;&#20294;&#26377;&#21069;&#26223;&#30340;&#24615;&#33021;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;LLM&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#24494;&#35843;&#27169;&#22411;&#65292;Mental-Alpaca&#65292;&#22312;&#24179;&#34913;&#20934;&#30830;&#24230;&#19978;&#27604;GPT-3.5&#65288;&#20307;&#31215;&#22823;25&#20493;&#65289;&#39640;&#20986;16.7\%&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25345;&#24179;&#12290;&#25105;&#20204;&#24635;&#32467;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent technology boost of large language models (LLMs) has empowered a variety of applications. However, there is very little research on understanding and improving LLMs' capability for the mental health domain. In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, and GPT-3.5, on various mental health prediction tasks via online text data. We conduct a wide range of experiments, covering zero-shot prompting, few-shot prompting, and instruction finetuning. The results indicate the promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned model, Mental-Alpaca, outperforms GPT-3.5 (25 times bigger) by 16.7\% on balanced accuracy and performs on par with the state-of-the-art task-specific model. We summarize our find
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.12267</link><description>&lt;p&gt;
&#38754;&#21521;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#35770;&#25991;&#30340;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#33021;&#22815;&#22312;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#27969;&#30021;&#22238;&#31572;&#12290;&#23613;&#31649;&#25215;&#35748;&#25216;&#26415;&#36827;&#27493;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#25945;&#32946;&#32773;&#20063;&#25285;&#24515;&#23398;&#29983;&#21487;&#33021;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#24182;&#23558;&#20854;&#20551;&#20882;&#20026;&#33258;&#24049;&#30340;&#21407;&#21019;&#20316;&#21697;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;AI&#20869;&#23481;&#26816;&#27979;&#30740;&#31350;&#26159;&#22522;&#20110;&#36825;&#20123;&#25285;&#24551;&#36827;&#34892;&#30340;&#65292;&#20294;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;AI&#20869;&#23481;&#26816;&#27979;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#20551;&#35774;&#19968;&#20010;&#25991;&#26412;&#35201;&#20040;&#23436;&#20840;&#30001;&#20154;&#31867;&#32534;&#20889;&#65292;&#35201;&#20040;&#23436;&#20840;&#30001;AI&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#20869;&#23481;&#26816;&#27979;&#22312;&#19968;&#20010;&#23569;&#26377;&#25506;&#32034;&#20294;&#21364;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26816;&#27979;&#30340;&#25991;&#26412;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#28151;&#21512;&#25991;&#26412;&#65289;&#21327;&#20316;&#32534;&#20889;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#26816;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20174;&#32473;&#23450;&#30340;&#28151;&#21512;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#31867;&#32534;&#20889;&#20869;&#23481;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#36716;&#25442;&#28857;&#65288;&#36793;&#30028;&#26816;&#27979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2307.11787</link><description>&lt;p&gt;
LLM&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#26377;&#25152;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#20225;&#19994;&#21644;&#28040;&#36153;&#32773;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#34429;&#28982;&#36825;&#31867;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#23427;&#20204;&#20316;&#20026;&#35748;&#30693;&#20027;&#20307;&#30340;&#35843;&#26597;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#23545;GPT-3&#21644;ChatGPT&#22312;&#19968;&#20010;&#26469;&#33258;&#35748;&#30693;&#31185;&#23398;&#25991;&#29486;&#30340;&#26377;&#38480;&#25968;&#25454;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#27604;&#36739;&#21028;&#23450;&#26469;&#30830;&#23450;&#20505;&#36873;&#22238;&#24212;&#30340;&#20248;&#21155;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#32477;&#23545;&#35780;&#20998;&#65292;&#27604;&#36739;&#35780;&#20272;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#36739;&#23567;&#30340;&#24320;&#28304;LLMs&#36798;&#21040;&#20102;&#19982;&#26356;&#22823;&#30340;&#20844;&#20849;&#35775;&#38382;API&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07889</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#21028;&#23450;&#36827;&#34892;&#38646;&#26679;&#26412;NLG&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Zero-shot NLG evaluation through Pairware Comparisons with LLMs. (arXiv:2307.07889v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#27604;&#36739;&#21028;&#23450;&#26469;&#30830;&#23450;&#20505;&#36873;&#22238;&#24212;&#30340;&#20248;&#21155;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#32477;&#23545;&#35780;&#20998;&#65292;&#27604;&#36739;&#35780;&#20272;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#36739;&#23567;&#30340;&#24320;&#28304;LLMs&#36798;&#21040;&#20102;&#19982;&#26356;&#22823;&#30340;&#20844;&#20849;&#35775;&#38382;API&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#36755;&#20986;&#26159;&#33267;&#20851;&#37325;&#35201;&#20294;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#33258;&#21160;NLG&#35780;&#20272;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#29305;&#23450;&#20219;&#21153;&#29305;&#23450;&#39046;&#22495;&#30340;&#65292;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#21644;&#23646;&#24615;&#36827;&#34892;&#24037;&#31243;&#35774;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38646;&#26679;&#26412;NLG&#35780;&#20272;&#30340;&#31283;&#20581;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#37197;&#23545;&#27604;&#36739;&#21028;&#23450;&#30340;&#26041;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#21160;&#26426;&#26159;&#65292;&#21363;&#20351;&#20316;&#20026;&#20154;&#31867;&#65292;&#30830;&#23450;&#20004;&#20010;&#36873;&#39033;&#20013;&#21738;&#20010;&#26356;&#22909;&#35201;&#27604;&#29420;&#31435;&#23458;&#35266;&#35780;&#20998;&#27599;&#20010;&#36873;&#39033;&#26356;&#23481;&#26131;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#24182;&#21033;&#29992;LLMs&#26032;&#20852;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#27979;FlanT5&#65292;&#30830;&#23450;&#20004;&#20010;&#20505;&#36873;&#22238;&#24212;&#20013;&#21738;&#19968;&#20010;&#26356;&#22909;&#65292;&#32780;&#19981;&#26159;&#25351;&#23450;&#32477;&#23545;&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27604;&#36739;&#35780;&#20272;&#26159;&#27604;&#32477;&#23545;&#35780;&#20998;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#36739;&#23567;&#30340;&#24320;&#28304;LLMs&#33021;&#22815;&#36798;&#21040;&#19982;&#26356;&#22823;&#30340;&#20844;&#20849;&#35775;&#38382;API&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating Natural Language Generation (NLG) outputs is crucial but laborious and expensive. While various automatic NLG assessment methods have been proposed, they often are quite task-specific and have to be engineered with a particular domain and attribute in mind. In this work, we propose a robust zero-shot approach to NLG evaluation using pairwise comparative judgment with open-source Large Language Models (LLMs). The motivation for this approach is that even as humans, it is easier to determine which of two options are better, than it is to independently objectively score each option. We use this insight and leverage the emergent abilities of LLMs, where we probe FlanT5 to determine which of two candidate responses is better, rather than assigning absolute scores. Our results demonstrate that comparative assessment is a more effective approach than absolute scoring, enabling smaller open-source LLMs to achieve comparable performance to larger public access APIs. We evaluate syste
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#30340;&#31283;&#20581;&#25163;&#21183;&#23884;&#20837;&#25552;&#21462;&#12290;&#38024;&#23545;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#22120;&#34429;&#28982;&#26159;&#29702;&#24819;&#30340;&#36873;&#25321;&#65292;&#20294;&#30001;&#20110;&#22495;&#19981;&#21305;&#37197;&#21644;&#25163;&#21183;&#35821;&#35328;&#20013;&#30340;&#25361;&#25112;&#24615;&#23039;&#21183;&#65292;&#20854;&#22312;&#25163;&#21183;&#35821;&#35328;&#25968;&#25454;&#19978;&#30340;&#31283;&#20581;&#24615;&#26377;&#25152;&#27424;&#32570;&#12290;&#20851;&#38190;&#28857;&#22522;&#20110;&#30340;&#27169;&#22411;&#20173;&#28982;&#20248;&#20110;&#22270;&#20687;&#22522;&#20110;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#35757;&#32451;&#26041;&#24335;&#38480;&#21046;&#20102;&#20854;&#22312;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.17558</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#30340;&#31283;&#20581;&#25163;&#21183;&#23884;&#20837;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Towards the extraction of robust sign embeddings for low resource sign language recognition. (arXiv:2306.17558v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#23454;&#29616;&#23545;&#20302;&#36164;&#28304;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#30340;&#31283;&#20581;&#25163;&#21183;&#23884;&#20837;&#25552;&#21462;&#12290;&#38024;&#23545;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#22120;&#34429;&#28982;&#26159;&#29702;&#24819;&#30340;&#36873;&#25321;&#65292;&#20294;&#30001;&#20110;&#22495;&#19981;&#21305;&#37197;&#21644;&#25163;&#21183;&#35821;&#35328;&#20013;&#30340;&#25361;&#25112;&#24615;&#23039;&#21183;&#65292;&#20854;&#22312;&#25163;&#21183;&#35821;&#35328;&#25968;&#25454;&#19978;&#30340;&#31283;&#20581;&#24615;&#26377;&#25152;&#27424;&#32570;&#12290;&#20851;&#38190;&#28857;&#22522;&#20110;&#30340;&#27169;&#22411;&#20173;&#28982;&#20248;&#20110;&#22270;&#20687;&#22522;&#20110;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#35757;&#32451;&#26041;&#24335;&#38480;&#21046;&#20102;&#20854;&#22312;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23396;&#31435;&#30340;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#36890;&#24120;&#24212;&#29992;&#20110;&#21253;&#21547;&#30001;&#19968;&#32452;&#26377;&#38480;&#25163;&#21183;&#25191;&#34892;&#32773;&#32531;&#24930;&#32780;&#28165;&#26224;&#25191;&#34892;&#30340;&#30456;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#26465;&#20214;&#12289;&#20849;&#21516;&#21457;&#38899;&#30340;&#25163;&#21183;&#12289;&#23567;&#25968;&#25454;&#38598;&#20197;&#21450;&#23545;&#29420;&#31435;&#28436;&#35762;&#32773;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#22788;&#29702;&#25163;&#21183;&#35821;&#35328;&#35270;&#39057;&#12290;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#22120;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#29702;&#24819;&#30340;&#20505;&#36873;&#32773;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#35757;&#32451;&#38598;&#19982;&#25163;&#21183;&#35821;&#35328;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23039;&#21183;&#20043;&#38388;&#23384;&#22312;&#39046;&#22495;&#19981;&#21305;&#37197;&#65292;&#23427;&#20204;&#22312;&#25163;&#21183;&#35821;&#35328;&#25968;&#25454;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#19978;&#20173;&#28982;&#32570;&#20047;&#31283;&#20581;&#24615;&#65292;&#20851;&#38190;&#28857;&#22522;&#20110;&#30340;&#27169;&#22411;&#36890;&#24120;&#20173;&#28982;&#20248;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#19982;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#30340;&#24120;&#35265;&#23454;&#36341;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20851;&#38190;&#28857;&#22522;&#20110;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#27599;&#20010;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#37117;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#12290;&#36825;&#20123;&#22240;&#32032;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25163;&#21183;&#35821;&#35328;&#35782;&#21035;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Isolated Sign Language Recognition (SLR) has mostly been applied on relatively large datasets containing signs executed slowly and clearly by a limited group of signers. In real-world scenarios, however, we are met with challenging visual conditions, coarticulated signing, small datasets, and the need for signer independent models. To tackle this difficult problem, we require a robust feature extractor to process the sign language videos. One could expect human pose estimators to be ideal candidates. However, due to a domain mismatch with their training sets and challenging poses in sign language, they lack robustness on sign language data and image based models often still outperform keypoint based models. Furthermore, whereas the common practice of transfer learning with image based models yields even higher accuracy, keypoint based models are typically trained from scratch on every SLR dataset. These factors limit their usefulness for SLR. From the existing literature, it is also no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#37327;&#21270;&#29702;&#35299;&#30340;&#25506;&#31350;&#65292;&#24182;&#36136;&#30097;&#20043;&#21069;&#30740;&#31350;&#20013;&#20851;&#20110;LLMs&#29702;&#35299;&#26497;&#23569;&#25968;&#31867;&#22411;&#30340;&#37327;&#35789;&#33021;&#21147;&#21576;&#29616;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#35828;&#27861;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#23637;&#31034;&#20854;&#19982;&#20197;&#21069;&#30740;&#31350;&#25152;&#23637;&#31034;&#30340;&#34892;&#20026;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.07384</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#37327;&#21270;&#29702;&#35299;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Probing Quantifier Comprehension in Large Language Models. (arXiv:2306.07384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#37327;&#21270;&#29702;&#35299;&#30340;&#25506;&#31350;&#65292;&#24182;&#36136;&#30097;&#20043;&#21069;&#30740;&#31350;&#20013;&#20851;&#20110;LLMs&#29702;&#35299;&#26497;&#23569;&#25968;&#31867;&#22411;&#30340;&#37327;&#35789;&#33021;&#21147;&#21576;&#29616;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#35828;&#27861;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#23637;&#31034;&#20854;&#19982;&#20197;&#21069;&#30740;&#31350;&#25152;&#23637;&#31034;&#30340;&#34892;&#20026;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23427;&#20204;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36234;&#26469;&#36234;&#22909;&#12290;&#20294;&#21363;&#20351;&#22312;&#20855;&#20307;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#65292;LLMs &#22312;&#21542;&#23450;&#25110;&#37327;&#21270;&#29702;&#35299;&#31561;&#31616;&#21333;&#35821;&#35328;&#27979;&#35797;&#20013;&#20173;&#28982;&#22833;&#36133;&#12290;&#20197;&#21069;&#27979;&#35797; LLMs &#23545;&#20110;&#29702;&#35299;&#37327;&#35789;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#30528;&#27169;&#22411;&#30340;&#19981;&#26029;&#22686;&#22823;&#65292;&#23427;&#20204;&#22312;&#29702;&#35299;&#22823;&#22810;&#25968;&#31867;&#22411;&#30340;&#37327;&#35789;&#26102;&#21464;&#24471;&#26356;&#22909;&#65292;&#20294;&#22312;&#29702;&#35299;&#26497;&#23569;&#25968;&#31867;&#22411;&#30340;&#37327;&#35789;&#26102;&#21464;&#24471;&#36234;&#26469;&#36234;&#24046;&#65292;&#20174;&#32780;&#21576;&#29616;&#20986;&#21453;&#27604;&#20363;&#32553;&#25918;&#27861;&#21017;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#36136;&#30097;&#20102;&#22312; LLMs &#20013;&#21453;&#27604;&#20363;&#32553;&#25918;&#26497;&#23569;&#25968;&#31867;&#22411;&#37327;&#35789;&#29702;&#35299;&#33021;&#21147;&#30340;&#35828;&#27861;&#65292;&#24182;&#34920;&#26126;&#36825;&#26159;&#19981;&#21512;&#36866;&#30340;&#27979;&#35797;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26469;&#27979;&#37327; LLMs &#30340;&#37327;&#21270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#38543;&#30528;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;&#36825;&#20123;&#34892;&#20026;&#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#25152;&#23637;&#31034;&#30340;&#19981;&#21516;&#12290;LLMs &#33021;&#22815;&#19981;&#26029;&#29702;&#35299;&#21547;&#20041;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
With their increasing size, Large language models (LLMs) are becoming increasingly good at language understanding tasks. But even with high performance on specific downstream task, LLMs fail at simple linguistic tests for negation or quantifier understanding. Previous work on testing capability of LLMs on understanding quantifiers suggest that as the size of the models increase, they get better at understanding most-type quantifiers but get increasingly worse at understanding few-type quantifiers, thus presenting a case of an inverse-scaling law. In this paper, we question the claims of inverse scaling of few-type quantifier understanding in LLMs and show that it is a result of inappropriate testing methodology. We also present alternate methods to measure quantifier comprehension in LLMs and show that as the size of the models increase, these behaviours are different from what is shown in previous research. LLMs are consistently able to understand the difference between the meaning of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#31995;&#32479;Allophant&#65292;&#32467;&#21512;&#32452;&#25104;&#24615;&#38899;&#32032;&#23884;&#20837;&#26041;&#27861;&#21644;&#20010;&#21035;&#30417;&#30563;&#30340;&#35821;&#38899;&#23646;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#32467;&#26500;&#65292;&#20351;&#24471;&#35813;&#31995;&#32479;&#33021;&#22815;&#20302;&#36164;&#28304;&#36827;&#34892;&#35821;&#38899;&#35782;&#21035;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#22788;&#29702;&#38476;&#29983;&#38899;&#32032;&#21644;&#38899;&#32032;&#24211;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04306</link><description>&lt;p&gt;
Allophant: &#24102;&#26377;&#21457;&#38899;&#23646;&#24615;&#30340;&#36328;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes. (arXiv:2306.04306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#31995;&#32479;Allophant&#65292;&#32467;&#21512;&#32452;&#25104;&#24615;&#38899;&#32032;&#23884;&#20837;&#26041;&#27861;&#21644;&#20010;&#21035;&#30417;&#30563;&#30340;&#35821;&#38899;&#23646;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#32467;&#26500;&#65292;&#20351;&#24471;&#35813;&#31995;&#32479;&#33021;&#22815;&#20302;&#36164;&#28304;&#36827;&#34892;&#35821;&#38899;&#35782;&#21035;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#22788;&#29702;&#38476;&#29983;&#38899;&#32032;&#21644;&#38899;&#32032;&#24211;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#31995;&#32479;Allophant&#12290;&#23427;&#21482;&#38656;&#35201;&#36328;&#35821;&#35328;&#30446;&#26631;&#35821;&#31181;&#30340;&#38899;&#32032;&#28165;&#21333;&#21363;&#21487;&#36827;&#34892;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#20102;&#32452;&#25104;&#24615;&#38899;&#32032;&#23884;&#20837;&#26041;&#27861;&#20197;&#21450;&#20010;&#21035;&#30417;&#30563;&#30340;&#35821;&#38899;&#23646;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;Allophoible&#25968;&#25454;&#24211;&#30340;&#25193;&#23637;&#12290;&#36890;&#36807;&#23558;&#35813;&#25968;&#25454;&#24211;&#19982;&#22522;&#20110;&#36317;&#31163;&#30340;&#22270;&#38899;&#36716;&#25442;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;PHOIBLE&#28165;&#21333;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#23545;34&#31181;&#35821;&#35328;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#21152;&#20837;&#25552;&#39640;&#20102;&#35813;&#27169;&#22411;&#22788;&#29702;&#38476;&#29983;&#38899;&#32032;&#21644;&#38899;&#32032;&#24211;&#30340;&#33021;&#21147;&#12290;&#22312;&#21463;&#30417;&#30563;&#35821;&#35328;&#19978;&#65292;&#19982;&#27809;&#26377;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;11&#20010;&#30334;&#20998;&#28857;&#30340;&#38899;&#32032;&#35823;&#24046;&#29575;&#25913;&#36827;&#12290;&#22312;84&#31181;&#38646;-shot&#36801;&#31227;&#35821;&#35328;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;PER&#19979;&#38477;&#20102;2.63&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes Allophant, a multilingual phoneme recognizer. It requires only a phoneme inventory for cross-lingual transfer to a target language, allowing for low-resource recognition. The architecture combines a compositional phone embedding approach with individually supervised phonetic attribute classifiers in a multi-task architecture. We also introduce Allophoible, an extension of the PHOIBLE database. When combined with a distance based mapping approach for grapheme-to-phoneme outputs, it allows us to train on PHOIBLE inventories directly. By training and evaluating on 34 languages, we found that the addition of multi-task learning improves the model's capability of being applied to unseen phonemes and phoneme inventories. On supervised languages we achieve phoneme error rate improvements of 11 percentage points (pp.) compared to a baseline without multi-task learning. Evaluation of zero-shot transfer on 84 languages yielded a decrease in PER of 2.63 pp. over the baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2306.01102</link><description>&lt;p&gt;
LLMatic: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558; LLMs &#35270;&#20026;&#21464;&#24322;&#21644;&#20132;&#21449;&#24037;&#20855;&#12290;&#21516;&#26102;&#65292;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#24050;&#30693;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#24615;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558; LLMs &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19982; QD &#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; LLMatic&#65292;&#19968;&#20010;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034; (NAS) &#31639;&#27861;&#12290;&#34429;&#28982; LLMs &#36890;&#36807;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892; NAS &#32771;&#39564;&#22256;&#38590;&#65292;&#20294; LLMatic &#21033;&#29992;&#31243;&#24207;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992; QD &#26469;&#36827;&#34892;&#25552;&#31034;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312; CIFAR-10 &#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102; LLMatic&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#20165;&#36827;&#34892; 2000 &#27425;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11095</link><description>&lt;p&gt;
&#28608;&#21457;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#20197;&#23454;&#29616;&#38646;-shot&#20219;&#21153;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;Whisper&#30340;&#26032;&#20852;&#21151;&#33021;&#65292;&#22312;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;&#27169;&#22411;&#21518;&#65292;&#36866;&#24212;&#20102;&#26410;&#35265;&#36807;&#30340;AVSR&#65292;CS-ASR&#21644;ST&#19977;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#35201;&#20040;&#21033;&#29992;&#21478;&#19968;&#20010;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#25805;&#20316;&#40664;&#35748;&#25552;&#31034;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#20351;&#36825;&#19977;&#20010;&#38646;-shot&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;10%&#21040;45&#65285;&#65292;&#29978;&#33267;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;SotA&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;Whisper&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#21253;&#25324;&#20854;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#21475;&#38899;&#30340;&#20559;&#22909;&#20197;&#21450;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/jasonppy/PromptingWhisper&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
&lt;/p&gt;</description></item><item><title>SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.09781</link><description>&lt;p&gt;
SpecInfer&#65306;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09781
&lt;/p&gt;
&lt;p&gt;
SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#22240;&#27492;&#24555;&#36895;&#21644;&#24265;&#20215;&#22320;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;SpecInfer&#65292;&#19968;&#20010;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;LLM&#25512;&#26029;&#12290;SpecInfer&#32972;&#21518;&#30340;&#20851;&#38190;&#26159;&#23558;&#21508;&#31181;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#20307;&#25552;&#21319;&#35843;&#25972;&#65292;&#20849;&#21516;&#39044;&#27979;LLM&#30340;&#36755;&#20986;&#65307; &#39044;&#27979;&#32467;&#26524;&#32452;&#32455;&#25104;&#19968;&#20010;&#20196;&#29260;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#37117;&#34920;&#31034;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26641;&#30340;&#24182;&#34892;&#35299;&#30721;&#26426;&#21046;&#65292;&#20197;LMM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#20196;&#29260;&#26641;&#25152;&#20195;&#34920;&#30340;&#25152;&#26377;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#30340;&#27491;&#30830;&#24615;&#12290;SpecInfer&#20351;&#29992;LLM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#65292;&#32780;&#19981;&#26159;&#22686;&#37327;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#29983;&#25104;&#24335;LLM&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#21487;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#21495;&#25945;&#32946;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;COT&#21512;&#29702;&#21270;&#20449;&#21495;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.03453</link><description>&lt;p&gt;
T-SciQ: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#21495;&#25945;&#25480;&#22810;&#27169;&#24577;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#22312;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering. (arXiv:2305.03453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#21495;&#25945;&#32946;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;COT&#21512;&#29702;&#21270;&#20449;&#21495;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#26399;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36824;&#23637;&#31034;&#20102;&#25191;&#34892;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#20197;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22797;&#26434;&#22810;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#20363;&#22914;&#36890;&#36807;&#29992;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#30340;&#38142;&#24335;&#24605;&#36335;&#26469;&#35843;&#25972;&#22810;&#27169;&#22411;&#27169;&#22411;&#36827;&#34892;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#39640;&#36136;&#37327;COT&#21512;&#29702;&#21270;&#36890;&#24120;&#26159;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#28041;&#21450;&#20887;&#20313;&#20449;&#24687;&#25110;&#20002;&#22833;&#37325;&#35201;&#20449;&#24687;&#65292;&#27880;&#37322;&#21512;&#29702;&#21270;&#36890;&#24120;&#19981;&#22826;&#20934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;T-SciQ&#65292;&#26088;&#22312;&#20351;&#29992;LLM&#20449;&#21495;&#25945;&#25480;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#12290;T-SciQ&#26041;&#27861;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;CoT&#21512;&#29702;&#21270;&#20449;&#21495;&#65292;&#24182;&#20808;&#36827;&#22320;&#35757;&#32451;&#36739;&#23567;&#30340;&#27169;&#22411;&#20197;&#22312;&#22797;&#26434;&#27169;&#24577;&#20013;&#25191;&#34892;CoT&#24605;&#32500;&#25512;&#29702;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#26174;&#30528;&#20943;&#23569;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21512;&#29702;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the redundant information involved or the essential information missed. To address these issues, we propose a novel method termed \emph{T-SciQ} that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#30701;&#35821;&#25366;&#25496;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#19978;&#19979;&#25991;&#24863;&#30693;Transformer&#36716;&#24405;&#22120;(CATT)&#27169;&#22411;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.08862</link><description>&lt;p&gt;
&#36817;&#20284;&#26368;&#36817;&#37051;&#30701;&#35821;&#25366;&#25496;&#22312;&#19978;&#19979;&#25991;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition. (arXiv:2304.08862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#30701;&#35821;&#25366;&#25496;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#19978;&#19979;&#25991;&#24863;&#30693;Transformer&#36716;&#24405;&#22120;(CATT)&#27169;&#22411;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#30701;&#35821;&#25366;&#25496;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#31471;&#21040;&#31471;&#19978;&#19979;&#25991;&#24863;&#30693;Transformer&#36716;&#24405;&#22120;(CATT)&#27169;&#22411;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#21442;&#32771;&#26597;&#35810;&#65292;&#25105;&#20204;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#25366;&#25496;&#20102;&#33509;&#24178;&#30456;&#20284;&#30340;&#30701;&#35821;&#20316;&#20026;&#36127;&#20363;&#65292;&#24182;&#23558;&#36825;&#20123;&#30701;&#35821;&#19982;&#38543;&#26426;&#21644;&#30495;&#23454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#19968;&#36215;&#29992;&#20316;&#19978;&#19979;&#25991;&#21015;&#34920;&#20013;&#30340;&#36127;&#20363;&#12290;&#36890;&#36807;&#23558;&#36817;&#20284;&#26368;&#36817;&#37051;&#30701;&#35821;&#65288;ANN-P&#65289;&#21253;&#21547;&#22312;&#19978;&#19979;&#25991;&#21015;&#34920;&#20013;&#65292;&#25105;&#20204;&#40723;&#21169;&#23398;&#20064;&#34920;&#31034;&#26469;&#21306;&#20998;&#30456;&#20284;&#20294;&#19981;&#23436;&#20840;&#30456;&#21516;&#30340;&#20559;&#35265;&#30701;&#35821;&#65292;&#20174;&#32780;&#22312;&#20559;&#35265;&#28165;&#21333;&#20013;&#23384;&#22312;&#20960;&#20010;&#30456;&#20284;&#30340;&#30701;&#35821;&#26102;&#25552;&#39640;&#20559;&#35265;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#30456;&#23545;&#23383;&#35823;&#29575;&#36798;7&#65285;&#30340;&#19978;&#19979;&#25991;&#37096;&#20998;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#24182;&#35780;&#20272;&#20102;CATT&#26041;&#27861;&#22312;&#20018;&#27969;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an extension to train end-to-end Context-Aware Transformer Transducer ( CATT ) models by using a simple, yet efficient method of mining hard negative phrases from the latent space of the context encoder. During training, given a reference query, we mine a number of similar phrases using approximate nearest neighbour search. These sampled phrases are then used as negative examples in the context list alongside random and ground truth contextual information. By including approximate nearest neighbour phrases (ANN-P) in the context list, we encourage the learned representation to disambiguate between similar, but not identical, biasing phrases. This improves biasing accuracy when there are several similar phrases in the biasing inventory. We carry out experiments in a large-scale data regime obtaining up to 7% relative word error rate reductions for the contextual portion of test data. We also extend and evaluate CATT approach in streaming applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2304.01752</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#23569;&#26679;&#26412;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#36719;&#25552;&#31034;&#23398;&#20064;&#26159;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#29992;&#30340;&#26368;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26032;&#39046;&#22495;&#24341;&#21457;&#30340;&#20998;&#24067;&#20559;&#31227;&#26469;&#32553;&#23567;&#27169;&#24577;&#24046;&#36317;&#12290;&#34429;&#28982;&#35813;&#26041;&#27861;&#24615;&#33021;&#39640;&#25928;&#65292;&#20294;&#20173;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#27169;&#22411;&#19978;&#21487;&#33021;&#20250;&#23548;&#33268;&#35745;&#31639;&#19978;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340; V-L &#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65292;&#35757;&#32451;&#36895;&#24230;&#24555;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasible for large models with billions of parameters. To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Linear Feature Alignment (LFA), a simple line
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#30456;&#20284;&#26696;&#20363;&#21305;&#37197;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65306;&#21496;&#27861;&#29305;&#24449;&#21477;&#23376;&#35782;&#21035;&#27169;&#22359;&#12289;&#26696;&#20363;&#21305;&#37197;&#27169;&#22359;&#12289;&#29305;&#24449;&#21477;&#23376;&#23545;&#40784;&#27169;&#22359;&#21644;&#20914;&#31361;&#28040;&#27495;&#27169;&#22359;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35782;&#21035;&#26696;&#20363;&#20013;&#30340;&#37325;&#35201;&#20449;&#24687;&#21644;&#23545;&#40784;&#20004;&#20010;&#26696;&#20363;&#20013;&#30340;&#29305;&#24449;&#21477;&#65292;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#30456;&#20284;&#24615;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.01622</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#37322;&#24615;&#30456;&#20284;&#26696;&#20363;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An interpretability framework for Similar case matching. (arXiv:2304.01622v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#30456;&#20284;&#26696;&#20363;&#21305;&#37197;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65306;&#21496;&#27861;&#29305;&#24449;&#21477;&#23376;&#35782;&#21035;&#27169;&#22359;&#12289;&#26696;&#20363;&#21305;&#37197;&#27169;&#22359;&#12289;&#29305;&#24449;&#21477;&#23376;&#23545;&#40784;&#27169;&#22359;&#21644;&#20914;&#31361;&#28040;&#27495;&#27169;&#22359;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35782;&#21035;&#26696;&#20363;&#20013;&#30340;&#37325;&#35201;&#20449;&#24687;&#21644;&#23545;&#40784;&#20004;&#20010;&#26696;&#20363;&#20013;&#30340;&#29305;&#24449;&#21477;&#65292;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#30456;&#20284;&#24615;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#26696;&#20363;&#21305;&#37197;&#65288;SCM&#65289;&#26088;&#22312;&#30830;&#23450;&#20004;&#20010;&#26696;&#20214;&#26159;&#21542;&#30456;&#20284;&#12290;&#35813;&#20219;&#21153;&#22312;&#27861;&#24459;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24110;&#21161;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#24555;&#36895;&#25214;&#21040;&#30456;&#20851;&#26696;&#20363;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#23427;&#20204;&#12290;&#29616;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#19978;&#65292;&#32780;&#19981;&#26159;&#22312;&#20854;&#21487;&#35299;&#37322;&#24615;&#19978;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;SCM&#31649;&#36947;&#26694;&#26550;&#65292;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65306;&#21496;&#27861;&#29305;&#24449;&#21477;&#23376;&#35782;&#21035;&#27169;&#22359;&#12289;&#26696;&#20363;&#21305;&#37197;&#27169;&#22359;&#12289;&#29305;&#24449;&#21477;&#23376;&#23545;&#40784;&#27169;&#22359;&#21644;&#20914;&#31361;&#28040;&#27495;&#27169;&#22359;&#12290;&#19982;&#29616;&#26377;&#30340;SCM&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#35782;&#21035;&#21253;&#21547;&#22522;&#26412;&#20449;&#24687;&#30340;&#26696;&#20363;&#29305;&#24449;&#21477;&#65292;&#22522;&#20110;&#25552;&#21462;&#30340;&#29305;&#24449;&#21477;&#32467;&#26524;&#36827;&#34892;&#30456;&#20284;&#26696;&#20363;&#21305;&#37197;&#65292;&#24182;&#23545;&#20004;&#20010;&#26696;&#20363;&#20013;&#30340;&#29305;&#24449;&#21477;&#36827;&#34892;&#23545;&#40784;&#65292;&#20197;&#25552;&#20379;&#35777;&#25454;&#25903;&#25345;&#26696;&#20363;&#30340;&#30456;&#20284;&#24615;&#12290;SCM&#32467;&#26524;&#21487;&#33021;&#19982;&#29305;&#24449;&#21477;&#23545;&#40784;&#32467;&#26524;&#20135;&#29983;&#20914;&#31361;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#19968;&#27493;&#28040;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Similar Case Matching (SCM) is designed to determine whether two cases are similar. The task has an essential role in the legal system, helping legal professionals to find relevant cases quickly and thus deal with them more efficiently. Existing research has focused on improving the model's performance but not on its interpretability. Therefore, this paper proposes a pipeline framework for interpretable SCM, which consists of four modules: a judicial feature sentence identification module, a case matching module, a feature sentence alignment module, and a conflict disambiguation module. Unlike existing SCM methods, our framework will identify feature sentences in a case that contain essential information, perform similar case matching based on the extracted feature sentence results, and align the feature sentences in the two cases to provide evidence for the similarity of the cases. SCM results may conflict with feature sentence alignment results, and our framework further disambiguate
&lt;/p&gt;</description></item><item><title>&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16166</link><description>&lt;p&gt;
&#27809;&#26377;&#27491;&#30830;&#24615;&#30340;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#37325;&#35201;&#65306;&#22312;NLP&#39046;&#22495;&#20013;&#27979;&#35797;&#20195;&#30721;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16166
&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20854;&#22312;&#30740;&#31350;&#23454;&#39564;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20195;&#30721;&#27491;&#30830;&#24615;&#24448;&#24448;&#20165;&#22522;&#20110;&#32467;&#26524;&#30340;&#24863;&#30693;&#36136;&#37327;&#32780;&#34987;&#20551;&#23450;&#12290;&#36825;&#24102;&#26469;&#20102;&#38169;&#35823;&#32467;&#26524;&#21644;&#28508;&#22312;&#35823;&#23548;&#24615;&#21457;&#29616;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#20851;&#27880;&#32467;&#26524;&#37325;&#29616;&#24212;&#35813;&#19982;&#24378;&#35843;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#25903;&#25345;&#25105;&#20204;&#21521;NLP&#31038;&#21306;&#21457;&#20986;&#30340;&#21495;&#21484;&#65292;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#24182;&#32416;&#27491;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;Conformer&#26550;&#26500;&#30340;&#24320;&#28304;&#23454;&#29616;&#20013;&#30340;&#19977;&#20010;Bug&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Bug&#30340;&#23384;&#22312;&#24182;&#19981;&#20250;&#22952;&#30861;&#33719;&#24471;&#33391;&#22909;&#30340;&#21644;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#65292;&#21453;&#32780;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#33021;&#25552;&#20379;&#38169;&#35823;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#39033;&#30740;&#31350;&#21628;&#21505;&#37319;&#29992;&#26088;&#22312;&#20419;&#36827;NLP&#30740;&#31350;&#20013;&#27491;&#30830;&#24615;&#30340;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#25991;&#26412;&#21040;&#26757;&#23572;&#39057;&#35889;&#29983;&#25104;&#22120;&#30340;&#26080;&#26631;&#35760;&#36716;&#20889;&#39046;&#22495;&#33258;&#36866;&#24212;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#29983;&#25104;&#26757;&#23572;&#39057;&#35889;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26032;&#39046;&#22495;&#30340;&#20165;&#25991;&#26412;&#25968;&#25454;&#26469;&#36866;&#24212;ASR&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;ASR&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#33258;&#36866;&#24212;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#19978;&#36229;&#36807;&#20102;&#32423;&#32852;TTS&#31995;&#32479;&#19982;&#22768;&#30721;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.14036</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#30340;&#25991;&#26412;&#21040;&#26757;&#23572;&#39057;&#35889;&#29983;&#25104;&#22120;&#30340;&#26080;&#26631;&#35760;&#36716;&#20889;&#39046;&#22495;&#33258;&#36866;&#24212;&#31471;&#21040;&#31471;ASR
&lt;/p&gt;
&lt;p&gt;
Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator. (arXiv:2302.14036v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#25991;&#26412;&#21040;&#26757;&#23572;&#39057;&#35889;&#29983;&#25104;&#22120;&#30340;&#26080;&#26631;&#35760;&#36716;&#20889;&#39046;&#22495;&#33258;&#36866;&#24212;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#29983;&#25104;&#26757;&#23572;&#39057;&#35889;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26032;&#39046;&#22495;&#30340;&#20165;&#25991;&#26412;&#25968;&#25454;&#26469;&#36866;&#24212;ASR&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;ASR&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#33258;&#36866;&#24212;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#19978;&#36229;&#36807;&#20102;&#32423;&#32852;TTS&#31995;&#32479;&#19982;&#22768;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#29992;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#12289;&#20165;&#26377;&#25991;&#26412;&#30340;&#25968;&#25454;&#25110;&#32773;&#20108;&#32773;&#30340;&#28151;&#21512;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#31181;&#38598;&#25104;&#30340;&#25991;&#26412;&#22522;&#30784;&#35757;&#32451;&#36741;&#21161;&#27169;&#22359;&#12290;&#35813;&#27169;&#22359;&#32467;&#21512;&#20102;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#35828;&#35805;&#20154;&#25991;&#26412;&#21040;&#26757;&#23572;&#39057;&#35889;&#29983;&#25104;&#22120;&#21644;&#22522;&#20110;GAN&#30340;&#22686;&#24378;&#22120;&#65292;&#20197;&#25552;&#39640;&#39057;&#35889;&#36136;&#37327;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#29983;&#25104;&#26757;&#23572;&#39057;&#35889;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#26032;&#39046;&#22495;&#30340;&#20165;&#25991;&#26412;&#25968;&#25454;&#65292;&#21487;&#20197;&#23558;ASR&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#19982;&#20165;&#35757;&#32451;&#20110;&#36716;&#24405;&#35821;&#38899;&#30340;&#31995;&#32479;&#30456;&#27604;&#26174;&#33879;&#25552;&#39640;&#20102;ASR&#20934;&#30830;&#24615;&#12290;&#23427;&#36824;&#22312;&#33258;&#36866;&#24212;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#19978;&#36229;&#36807;&#20102;&#32423;&#32852;TTS&#31995;&#32479;&#19982;&#22768;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an end-to-end Automatic Speech Recognition (ASR) system that can be trained on transcribed speech data, text-only data, or a mixture of both. The proposed model uses an integrated auxiliary block for text-based training. This block combines a non-autoregressive multi-speaker text-to-mel-spectrogram generator with a GAN-based enhancer to improve the spectrogram quality. The proposed system can generate a mel-spectrogram dynamically during training. It can be used to adapt the ASR model to a new domain by using text-only data from this domain. We demonstrate that the proposed training method significantly improves ASR accuracy compared to the system trained on transcribed speech only. It also surpasses cascade TTS systems with the vocoder in the adaptation quality and training speed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;660&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#24182;&#19981;&#22343;&#21248;&#20998;&#24067;&#22312;&#20854;&#21508;&#20010;&#32452;&#20214;&#19978;&#12290;&#36890;&#36807;&#31227;&#38500;&#32422;70%&#30340;&#27880;&#24847;&#21147;&#22836;&#21644;&#32422;20%&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#20219;&#21153;&#25191;&#34892;&#34920;&#29616;&#20165;&#26377;&#36731;&#24494;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#22312;OPT-66B&#20013;&#65292;&#23384;&#22312;&#19968;&#23567;&#37096;&#20998;&#27880;&#24847;&#21147;&#22836;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#24402;&#32435;&#25805;&#20316;&#20855;&#26377;&#39640;&#25928;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09095</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#35268;&#27169;&#30340;&#20316;&#29992;: &#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;660&#20159;&#23610;&#24230;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale. (arXiv:2212.09095v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;660&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#24182;&#19981;&#22343;&#21248;&#20998;&#24067;&#22312;&#20854;&#21508;&#20010;&#32452;&#20214;&#19978;&#12290;&#36890;&#36807;&#31227;&#38500;&#32422;70%&#30340;&#27880;&#24847;&#21147;&#22836;&#21644;&#32422;20%&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;&#20219;&#21153;&#25191;&#34892;&#34920;&#29616;&#20165;&#26377;&#36731;&#24494;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#22312;OPT-66B&#20013;&#65292;&#23384;&#22312;&#19968;&#23567;&#37096;&#20998;&#27880;&#24847;&#21147;&#22836;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#24402;&#32435;&#25805;&#20316;&#20855;&#26377;&#39640;&#25928;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#35268;&#27169;&#22686;&#21152;&#26102;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;660&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;OPT-66B&#65289;&#22312;14&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#26159;&#21542;&#22343;&#21248;&#20998;&#24067;&#22312;&#20854;&#25152;&#26377;&#30340;&#32452;&#20214;&#19978;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#32422;70%&#30340;&#27880;&#24847;&#21147;&#22836;&#21644;&#32422;20%&#30340;&#21069;&#39304;&#32593;&#36335;&#21487;&#20197;&#31227;&#38500;&#32780;&#20219;&#21153;&#34920;&#29616;&#20165;&#26377;&#36731;&#24494;&#19979;&#38477;&#12290;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#19978;&#19979;&#25991;&#31034;&#20363;&#25968;&#37327;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#19981;&#37325;&#35201;&#30340;&#27880;&#24847;&#21147;&#22836;&#30340;&#38598;&#21512;&#23384;&#22312;&#36739;&#22823;&#30340;&#37325;&#21472;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21457;&#29616;OPT-66B&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#27880;&#24847;&#21147;&#22836;&#22312;&#25191;&#34892;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#20851;&#30340;&#22522;&#30784;&#24402;&#32435;&#25805;&#20316;&#65288;&#21363;&#21069;&#32512;&#21305;&#37197;&#21644;&#22797;&#21046;&#65289;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case: $\sim$70% of attention heads and $\sim$20% of feed forward networks can be removed with minimal decline in task performance. We find substantial overlap in the set of attention heads (un)important for in-context learning across tasks and number of in-context examples. We also address our hypothesis through a task-agnostic lens, finding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with in-context learning, namely, prefix matching and copying. These in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#29983;&#25104;&#30340;&#35270;&#39057;&#23383;&#24149;&#20013;&#25552;&#21462;&#35821;&#20041;&#20803;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#21462;&#23454;&#20307;&#12289;&#23454;&#20307;&#23646;&#24615;&#12289;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#35270;&#39057;&#20998;&#31867;&#12290;&#25552;&#21462;&#20449;&#24687;&#30340;&#36136;&#37327;&#21463;&#21040;&#20107;&#20214;&#23450;&#20301;&#36136;&#37327;&#21644;&#23383;&#24149;&#29983;&#25104;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.02982</link><description>&lt;p&gt;
&#29983;&#25104;&#35270;&#39057;&#23383;&#24149;&#20013;&#30340;&#20107;&#20214;&#21644;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Event and Entity Extraction from Generated Video Captions. (arXiv:2211.02982v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#29983;&#25104;&#30340;&#35270;&#39057;&#23383;&#24149;&#20013;&#25552;&#21462;&#35821;&#20041;&#20803;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#21462;&#23454;&#20307;&#12289;&#23454;&#20307;&#23646;&#24615;&#12289;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#35270;&#39057;&#20998;&#31867;&#12290;&#25552;&#21462;&#20449;&#24687;&#30340;&#36136;&#37327;&#21463;&#21040;&#20107;&#20214;&#23450;&#20301;&#36136;&#37327;&#21644;&#23383;&#24149;&#29983;&#25104;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20154;&#24037;&#36827;&#34892;&#22810;&#23186;&#20307;&#25968;&#25454;&#27880;&#37322;&#32791;&#26102;&#19988;&#26114;&#36149;&#65292;&#32780;&#21487;&#38752;&#30340;&#33258;&#21160;&#29983;&#25104;&#35821;&#20041;&#20803;&#25968;&#25454;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#21160;&#29983;&#25104;&#30340;&#35270;&#39057;&#23383;&#24149;&#20013;&#25552;&#21462;&#35821;&#20041;&#20803;&#25968;&#25454;&#30340;&#26694;&#26550;&#12290;&#20316;&#20026;&#20803;&#25968;&#25454;&#65292;&#25105;&#20204;&#32771;&#34385;&#23454;&#20307;&#12289;&#23454;&#20307;&#23646;&#24615;&#12289;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#35270;&#39057;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#27169;&#22411;&#65292;&#21363;&#36974;&#34109;&#36716;&#25442;&#22120;&#65288;MT&#65289;&#21644;&#24182;&#34892;&#35299;&#30721;&#65288;PVDC&#65289;&#65292;&#20026;ActivityNet Captions&#25968;&#25454;&#38598;&#30340;&#35270;&#39057;&#29983;&#25104;&#23383;&#24149;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20174;&#29983;&#25104;&#30340;&#23383;&#24149;&#20013;&#25552;&#21462;&#23454;&#20307;&#12289;&#23454;&#20307;&#23646;&#24615;&#12289;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#35270;&#39057;&#20998;&#31867;&#26159;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25552;&#21462;&#20449;&#24687;&#30340;&#36136;&#37327;&#20027;&#35201;&#21463;&#21040;&#35270;&#39057;&#20013;&#20107;&#20214;&#23450;&#20301;&#30340;&#36136;&#37327;&#20197;&#21450;&#20107;&#20214;&#23383;&#24149;&#29983;&#25104;&#30340;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotation of multimedia data by humans is time-consuming and costly, while reliable automatic generation of semantic metadata is a major challenge. We propose a framework to extract semantic metadata from automatically generated video captions. As metadata, we consider entities, the entities' properties, relations between entities, and the video category. We employ two state-of-the-art dense video captioning models with masked transformer (MT) and parallel decoding (PVDC) to generate captions for videos of the ActivityNet Captions dataset. Our experiments show that it is possible to extract entities, their properties, relations between entities, and the video category from the generated captions. We observe that the quality of the extracted information is mainly influenced by the quality of the event localization in the video as well as the performance of the event caption generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#33258;&#30417;&#30563;&#22312;&#22788;&#29702;&#35821;&#35328;&#20559;&#35265;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#22235;&#20010;&#20559;&#35265;&#20219;&#21153;&#65288;&#35786;&#26029;&#12289;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#25913;&#20889;&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#21035;&#30340;&#20219;&#21153;&#25551;&#36848;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2112.08637</link><description>&lt;p&gt;
&#20998;&#26512;&#33258;&#30417;&#30563;&#22312;&#22788;&#29702;&#35821;&#35328;&#20559;&#35265;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Limits of Self-Supervision in Handling Bias in Language. (arXiv:2112.08637v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#33258;&#30417;&#30563;&#22312;&#22788;&#29702;&#35821;&#35328;&#20559;&#35265;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#22235;&#20010;&#20559;&#35265;&#20219;&#21153;&#65288;&#35786;&#26029;&#12289;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#25913;&#20889;&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#21035;&#30340;&#20219;&#21153;&#25551;&#36848;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#36755;&#20837;&#24050;&#25104;&#20026;&#20174;&#22823;&#35268;&#27169;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20986;&#30456;&#23545;&#20934;&#30830;&#36755;&#20986;&#30340;&#27969;&#34892;&#26426;&#21046;&#65292;&#32780;&#21516;&#26102;&#21448;&#20960;&#20046;&#27809;&#26377;&#19978;&#19979;&#25991;&#30417;&#30563;&#12290;&#36825;&#20063;&#26377;&#21161;&#20110;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#20174;&#26080;&#26631;&#35760;&#25991;&#26412;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#32431;&#31929;&#25429;&#25417;&#19979;&#28216;&#20219;&#21153;&#30340;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#33258;&#28982;&#20063;&#26292;&#38706;&#20110;&#35768;&#22810;&#19981;&#24076;&#26395;&#30340;&#20869;&#23481;&#65292;&#22914;&#31181;&#26063;&#20027;&#20041;&#21644;&#24615;&#21035;&#27495;&#35270;&#30340;&#35821;&#35328;&#65292;&#30446;&#21069;&#23545;&#27169;&#22411;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#24847;&#35782;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#24182;&#20840;&#38754;&#35780;&#20272;&#36825;&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;&#22235;&#20010;&#20559;&#35265;&#20219;&#21153;&#65288;&#35786;&#26029;&#12289;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#25913;&#20889;&#65289;&#20013;&#25429;&#25417;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19977;&#31867;&#20219;&#21153;&#25551;&#36848;&#65306;&#38472;&#36848;&#12289;&#38382;&#39064;&#21644;&#23436;&#25104;&#65292;&#24182;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#20351;&#29992;&#20102;&#35768;&#22810;&#35789;&#27719;&#21464;&#20307;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#36825;&#20123;&#20219;&#21153;&#25551;&#36848;&#30340;&#25552;&#31034;&#23545;&#27599;&#20010;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting inputs with natural language task descriptions has emerged as a popular mechanism to elicit reasonably accurate outputs from large-scale generative language models with little to no in-context supervision. This also helps gain insight into how well language models capture the semantics of a wide range of downstream tasks purely from self-supervised pre-training on massive corpora of unlabeled text. Such models have naturally also been exposed to a lot of undesirable content like racist and sexist language and there is limited work on awareness of models along these dimensions. In this paper, we define and comprehensively evaluate how well such language models capture the semantics of four tasks for bias: diagnosis, identification, extraction and rephrasing. We define three broad classes of task descriptions for these tasks: statement, question, and completion, with numerous lexical variants within each class. We study the efficacy of prompting for each task using these classe
&lt;/p&gt;</description></item></channel></rss>