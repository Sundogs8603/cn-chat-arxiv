<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20204</link><description>&lt;p&gt;
&#21033;&#29992;&#36817;&#26080;&#38480;&#21382;&#21490;&#30340;&#36890;&#29992;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History. (arXiv:2310.20204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20204
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#24320;&#21457;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#27515;&#20129;&#39044;&#27979;&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#19987;&#23478;&#24847;&#35265;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#35843;&#25972;&#35266;&#27979;&#31383;&#21475;&#22823;&#23567;&#12290;&#36825;&#32473;&#19987;&#23478;&#24102;&#26469;&#36127;&#25285;&#24182;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#36896;&#25104;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65288;REMed&#65289;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;REMed&#21487;&#20197;&#22522;&#26412;&#35780;&#20272;&#26080;&#38480;&#37327;&#30340;&#20020;&#24202;&#20107;&#20214;&#65292;&#36873;&#25321;&#30456;&#20851;&#30340;&#20107;&#20214;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#23454;&#26102;&#35266;&#23519;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;27&#20010;&#20020;&#24202;&#20219;&#21153;&#21644;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;EHR&#25968;&#25454;&#38598;&#30340;&#29420;&#31435;&#38431;&#21015;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#29305;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;REMed&#20248;&#20110;&#20854;&#20182;&#29616;&#20195;&#26550;&#26500;&#65292;&#23427;&#20204;&#26088;&#22312;&#22788;&#29702;&#23613;&#21487;&#33021;&#22810;&#30340;&#20107;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;REMed&#30340;&#20559;&#22909;&#19982;&#21307;&#23398;&#19987;&#23478;&#30340;&#20559;&#22909;&#23494;&#20999;&#30456;&#20284;&#12290;&#25105;&#20204;&#26399;&#26395;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#26174;&#33879;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550; DEPN&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32534;&#36753;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#31070;&#32463;&#20803;&#12290;&#36890;&#36807;&#24341;&#20837;&#38544;&#31169;&#31070;&#32463;&#20803;&#26816;&#27979;&#22120;&#21644;&#38544;&#31169;&#31070;&#32463;&#20803;&#32858;&#21512;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#31169;&#20154;&#25968;&#25454;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#24182;&#19988;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.20138</link><description>&lt;p&gt;
DEPN: &#26816;&#27979;&#21644;&#32534;&#36753;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models. (arXiv:2310.20138v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550; DEPN&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32534;&#36753;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#31070;&#32463;&#20803;&#12290;&#36890;&#36807;&#24341;&#20837;&#38544;&#31169;&#31070;&#32463;&#20803;&#26816;&#27979;&#22120;&#21644;&#38544;&#31169;&#31070;&#32463;&#20803;&#32858;&#21512;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#31169;&#20154;&#25968;&#25454;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#24182;&#19988;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#20854;&#23545;&#25968;&#25454;&#35760;&#24518;&#21644;&#37325;&#22797;&#30340;&#33021;&#21147;&#24102;&#26469;&#20102;&#25968;&#25454;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#26377;&#25928;&#38477;&#20302;&#36825;&#20123;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DEPN&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32534;&#36753;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#31070;&#32463;&#20803;&#65292;&#37096;&#20998;&#21463;&#21040;&#30693;&#35782;&#31070;&#32463;&#20803;&#21644;&#27169;&#22411;&#32534;&#36753;&#30340;&#21551;&#21457;&#12290;&#22312;DEPN&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#38544;&#31169;&#31070;&#32463;&#20803;&#26816;&#27979;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#20301;&#19982;&#38544;&#31169;&#20449;&#24687;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#23427;&#20204;&#30340;&#28608;&#27963;&#35774;&#32622;&#20026;&#38646;&#26469;&#32534;&#36753;&#36825;&#20123;&#26816;&#27979;&#21040;&#30340;&#38544;&#31169;&#31070;&#32463;&#20803;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#31070;&#32463;&#20803;&#32858;&#21512;&#22120;&#65292;&#20197;&#25209;&#22788;&#29702;&#26041;&#24335;&#21435;&#38500;&#38544;&#31169;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#26377;&#25928;&#22320;&#38477;&#20302;&#31169;&#20154;&#25968;&#25454;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#25506;&#27979;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#31867;&#26041;&#26696;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.16570</link><description>&lt;p&gt;
&#32473;&#25105;&#20107;&#23454;&#65281;&#20851;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#25506;&#27979;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models. (arXiv:2310.16570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16570
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#25506;&#27979;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#31867;&#26041;&#26696;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#22312;&#19990;&#30028;&#30693;&#35782;&#20016;&#23500;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#36215;&#20102;&#31038;&#21306;&#23545;&#20110;&#37327;&#21270;PLMs&#20013;&#23384;&#22312;&#30340;&#20107;&#23454;&#30693;&#35782;&#37327;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#36825;&#35299;&#37322;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#33021;&#35777;&#26126;&#23427;&#20204;&#20316;&#20026;&#30693;&#35782;&#24211;&#20351;&#29992;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29992;&#20110;&#25506;&#27979;PLMs&#20107;&#23454;&#30693;&#35782;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#65306;(1) &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36755;&#20837;&#12289;&#36755;&#20986;&#21644;&#34987;&#25506;&#27979;&#30340;PLMs&#22914;&#20309;&#36866;&#24212;&#30340;&#20998;&#31867;&#26041;&#26696;&#65307;(2) &#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#20107;&#23454;&#25506;&#27979;&#30340;&#25968;&#25454;&#38598;&#27010;&#36848;&#65307;(3) &#25105;&#20204;&#32508;&#21512;&#20102;&#20851;&#20110;PLMs&#20013;&#30693;&#35782;&#20445;&#30041;&#21644;&#25552;&#31034;&#20248;&#21270;&#30340;&#35266;&#28857;&#65292;&#20998;&#26512;&#20102;&#23558;PLMs&#20316;&#20026;&#30693;&#35782;&#24211;&#24212;&#29992;&#30340;&#38556;&#30861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models (PLMs) are trained on vast unlabeled data, rich in world knowledge. This fact has sparked the interest of the community in quantifying the amount of factual knowledge present in PLMs, as this explains their performance on downstream tasks, and potentially justifies their use as knowledge bases. In this work, we survey methods and datasets that are used to probe PLMs for factual knowledge. Our contributions are: (1) We propose a categorization scheme for factual probing methods that is based on how their inputs, outputs and the probed PLMs are adapted; (2) We provide an overview of the datasets used for factual probing; (3) We synthesize insights about knowledge retention and prompt optimization in PLMs, analyze obstacles to adopting PLMs as knowledge bases and outline directions for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#35299;&#37322;&#26694;&#26550;&#65292;&#20174;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#39118;&#26684;&#24046;&#24322;&#24182;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#39118;&#26684;&#65292;&#21019;&#24314;&#20102;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#31036;&#35980;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#31036;&#35980;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#30340;&#21464;&#21270;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#31867;&#21035;&#23545;&#39118;&#26684;&#21464;&#21270;&#30340;&#36129;&#29486;&#21644;&#20102;&#35299;&#19990;&#30028;&#21508;&#22320;&#20154;&#20204;&#30340;&#19981;&#21516;&#27807;&#36890;&#26041;&#24335;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#21644;&#35299;&#37322;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07135</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#39118;&#26684;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparing Styles across Languages. (arXiv:2310.07135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#35299;&#37322;&#26694;&#26550;&#65292;&#20174;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#39118;&#26684;&#24046;&#24322;&#24182;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#39118;&#26684;&#65292;&#21019;&#24314;&#20102;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#31036;&#35980;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#31036;&#35980;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#30340;&#21464;&#21270;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#31867;&#21035;&#23545;&#39118;&#26684;&#21464;&#21270;&#30340;&#36129;&#29486;&#21644;&#20102;&#35299;&#19990;&#30028;&#21508;&#22320;&#20154;&#20204;&#30340;&#19981;&#21516;&#27807;&#36890;&#26041;&#24335;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#21644;&#35299;&#37322;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#36328;&#35821;&#35328;&#39118;&#26684;&#30340;&#24046;&#24322;&#26377;&#21161;&#20110;&#35757;&#32451;&#20154;&#31867;&#21644;&#35745;&#31639;&#26426;&#29983;&#25104;&#31526;&#21512;&#25991;&#21270;&#32972;&#26223;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35299;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#39118;&#26684;&#24046;&#24322;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;(1)&#21487;&#20197;&#29983;&#25104;&#20219;&#20309;&#35821;&#35328;&#30340;&#20840;&#38754;&#39118;&#26684;&#35789;&#20856;&#65292;(2)&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#32479;&#19968;&#20026;&#21487;&#27604;&#36739;&#30340;&#35789;&#27719;&#31867;&#21035;&#12290;&#25105;&#20204;&#24212;&#29992;&#35813;&#26694;&#26550;&#27604;&#36739;&#20102;&#31036;&#35980;&#35821;&#35328;&#65292;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#31036;&#35980;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#32034;&#20102;&#31036;&#35980;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#35821;&#35328;&#31867;&#21035;&#23545;&#39118;&#26684;&#21464;&#21270;&#30340;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#27934;&#23519;&#21147;&#65292;&#20102;&#35299;&#19990;&#30028;&#21508;&#22320;&#20154;&#20204;&#30340;&#19981;&#21516;&#27807;&#36890;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how styles differ across languages is advantageous for training both humans and computers to generate culturally appropriate text. We introduce an explanation framework to extract stylistic differences from multilingual LMs and compare styles across languages. Our framework (1) generates comprehensive style lexica in any language and (2) consolidates feature importances from LMs into comparable lexical categories. We apply this framework to compare politeness, creating the first holistic multilingual politeness dataset and exploring how politeness varies across four languages. Our approach enables an effective evaluation of how distinct linguistic categories contribute to stylistic variations and provides interpretable insights into how people communicate differently around the world.
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.04406</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#32479;&#19968;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#12289;&#34892;&#21160;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04406
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19968;&#31995;&#21015;&#20915;&#31574;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#34892;&#21160;&#36807;&#31243;&#65292;&#24182;&#26410;&#33021;&#24191;&#27867;&#37096;&#32626;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LATS&#65288;&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;LLMs&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#30456;&#20114;&#21327;&#21516;&#12290;LATS&#20511;&#37492;&#20102;&#27169;&#22411;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#24605;&#24819;&#65292;&#23558;LLMs&#29992;&#20316;&#20195;&#29702;&#12289;&#20215;&#20540;&#20989;&#25968;&#21644;&#20248;&#21270;&#22120;&#65292;&#37325;&#26032;&#21033;&#29992;&#20854;&#28508;&#22312;&#30340;&#20248;&#21183;&#20197;&#25552;&#21319;&#20915;&#31574;&#33021;&#21147;&#12290;&#20851;&#38190;&#30340;&#19968;&#28857;&#26159;LATS&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#32534;&#31243;&#12289;HotPotQA&#21644;WebShop&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LATS&#22312;&#25512;&#29702;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#32534;&#31243;&#26041;&#38754;&#65292;LATS&#23454;&#29616;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on Hu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;OpenAI GPT-3.5&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39550;&#39542;&#36712;&#36857;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#35268;&#21010;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01415</link><description>&lt;p&gt;
GPT-Driver: &#20351;&#29992;GPT&#23398;&#20064;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
GPT-Driver: Learning to Drive with GPT. (arXiv:2310.01415v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;OpenAI GPT-3.5&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39550;&#39542;&#36712;&#36857;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#35268;&#21010;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;OpenAI GPT-3.5&#27169;&#22411;&#36716;&#21270;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#21487;&#38752;&#36816;&#21160;&#35268;&#21010;&#22120;&#12290;&#36816;&#21160;&#35268;&#21010;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#26680;&#24515;&#25361;&#25112;&#65292;&#26088;&#22312;&#35268;&#21010;&#19968;&#20010;&#23433;&#20840;&#33298;&#36866;&#30340;&#39550;&#39542;&#36712;&#36857;&#12290;&#29616;&#26377;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#20027;&#35201;&#21033;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#38754;&#23545;&#26032;&#39062;&#21644;&#26410;&#30693;&#30340;&#39550;&#39542;&#22330;&#26223;&#26102;&#23637;&#29616;&#20986;&#19981;&#36275;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22266;&#26377;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#21644;&#27867;&#21270;&#28508;&#21147;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#22522;&#26412;&#35265;&#35299;&#26159;&#23558;&#36816;&#21160;&#35268;&#21010;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20043;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#35270;&#35282;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35268;&#21010;&#22120;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#20026;&#35821;&#35328;&#35760;&#21495;&#65292;&#24182;&#21033;&#29992;LLM&#36890;&#36807;&#23545;&#22352;&#26631;&#30340;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#39550;&#39542;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple yet effective approach that can transform the OpenAI GPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion planning is a core challenge in autonomous driving, aiming to plan a driving trajectory that is safe and comfortable. Existing motion planners predominantly leverage heuristic methods to forecast driving trajectories, yet these approaches demonstrate insufficient generalization capabilities in the face of novel and unseen driving scenarios. In this paper, we propose a novel approach to motion planning that capitalizes on the strong reasoning capabilities and generalization potential inherent to Large Language Models (LLMs). The fundamental insight of our approach is the reformulation of motion planning as a language modeling problem, a perspective not previously explored. Specifically, we represent the planner inputs and outputs as language tokens, and leverage the LLM to generate driving trajectories through a language description of coo
&lt;/p&gt;</description></item><item><title>LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12307</link><description>&lt;p&gt;
LongLoRA: &#39640;&#25928;&#30340;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12307
&lt;/p&gt;
&lt;p&gt;
LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#8212;&#8212;LongLoRA&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;GPU&#36164;&#28304;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26041;&#38754;&#21152;&#24555;&#20102;LLM&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#12290;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#31264;&#23494;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#20294;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#23436;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#19978;&#19979;&#25991;&#30340;&#25193;&#23637;&#65292;&#22312;&#19982;&#20351;&#29992;&#20256;&#32479;&#27880;&#24847;&#21147;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#21482;&#29992;&#20004;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#22312;&#25512;&#29702;&#20013;&#26159;&#21487;&#36873;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-effici
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10313</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPT4&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20391;&#37325;&#20110;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#35270;&#35273;&#27169;&#22411;&#26469;&#24320;&#21457;&#36890;&#29992;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#26080;&#27861;&#20445;&#25345;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20173;&#28982;&#26159;&#22810;&#27169;&#24577;LLM&#65288;MLLM&#65289;&#20013;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EMT&#65306;&#29992;&#20110;&#35780;&#20272;MLLM&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;MLLM&#20316;&#20026;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;EMT&#26469;&#35780;&#20272;&#20960;&#20010;&#24320;&#28304;&#30340;&#24494;&#35843;MLLM&#65292;&#24182;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;MLLM&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#26080;&#27861;&#20445;&#25345;&#19982;&#20182;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32487;&#32493;&#24494;&#35843;LLaVA&#65292;&#19968;&#31181;MLLM&#65292;&#24182;&#21033;&#29992;EMT&#26469;&#35780;&#20272;&#25972;&#20010;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#30340;&#24494;&#35843;&#38454;&#27573;&#26159;&#20851;&#38190;&#30340;&#65292;&#36807;&#26089;&#20572;&#27490;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#20302;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#20559;&#35265;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#24433;&#21709;&#30340;&#21338;&#22763;&#35770;&#25991;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20559;&#35265;&#23545;&#26816;&#27979;&#20219;&#21153;&#30340;&#24433;&#21709;&#21253;&#25324;&#21487;&#35299;&#37322;&#24615;&#12289;&#20882;&#29359;&#24615;&#21051;&#26495;&#21360;&#35937;&#21644;&#20844;&#24179;&#24615;&#19977;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;&#24403;&#21069;&#22312;&#27979;&#37327;&#21644;&#20943;&#36731;&#20559;&#35265;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#23558;&#31038;&#20250;&#31185;&#23398;&#32435;&#20837;&#21040;&#30740;&#31350;&#20013;&#12290;</title><link>http://arxiv.org/abs/2308.16549</link><description>&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#8212;&#8212;&#35770;&#36848;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Thesis Distillation: Investigating The Impact of Bias in NLP Models on Hate Speech Detection. (arXiv:2308.16549v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#20559;&#35265;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#24433;&#21709;&#30340;&#21338;&#22763;&#35770;&#25991;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20559;&#35265;&#23545;&#26816;&#27979;&#20219;&#21153;&#30340;&#24433;&#21709;&#21253;&#25324;&#21487;&#35299;&#37322;&#24615;&#12289;&#20882;&#29359;&#24615;&#21051;&#26495;&#21360;&#35937;&#21644;&#20844;&#24179;&#24615;&#19977;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;&#24403;&#21069;&#22312;&#27979;&#37327;&#21644;&#20943;&#36731;&#20559;&#35265;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#23558;&#31038;&#20250;&#31185;&#23398;&#32435;&#20837;&#21040;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#25105;&#30340;&#21338;&#22763;&#35770;&#25991;&#24037;&#20316;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20174;&#21487;&#35299;&#37322;&#24615;&#12289;&#20882;&#29359;&#24615;&#21051;&#26495;&#21360;&#35937;&#21644;&#20844;&#24179;&#24615;&#19977;&#20010;&#26041;&#38754;&#25506;&#35752;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#20559;&#35265;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#25105;&#35752;&#35770;&#20102;&#35770;&#25991;&#30340;&#20027;&#35201;&#35201;&#28857;&#20197;&#21450;&#23427;&#20204;&#23545;&#26356;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#30340;&#30410;&#22788;&#12290;&#26368;&#21518;&#65292;&#25105;&#35752;&#35770;&#20102;&#37325;&#35201;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#30340;&#35770;&#25991;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#20174;&#36825;&#19977;&#20010;&#26041;&#38754;&#24433;&#21709;&#20102;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#12290;&#38500;&#38750;&#25105;&#20204;&#24320;&#22987;&#23558;&#31038;&#20250;&#31185;&#23398;&#32435;&#20837;&#21040;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#20013;&#65292;&#21542;&#21017;&#25105;&#20204;&#23558;&#26080;&#27861;&#26377;&#25928;&#22320;&#20811;&#26381;&#30446;&#21069;&#22312;&#27979;&#37327;&#21644;&#20943;&#36731;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20559;&#35265;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is a summary of the work in my PhD thesis. In which, I investigate the impact of bias in NLP models on the task of hate speech detection from three perspectives: explainability, offensive stereotyping bias, and fairness. I discuss the main takeaways from my thesis and how they can benefit the broader NLP community. Finally, I discuss important future research directions. The findings of my thesis suggest that bias in NLP models impacts the task of hate speech detection from all three perspectives. And that unless we start incorporating social sciences in studying bias in NLP models, we will not effectively overcome the current limitations of measuring and mitigating bias in NLP models.
&lt;/p&gt;</description></item><item><title>Statler&#26159;&#19968;&#20010;&#20026;LLMs&#36171;&#20104;&#20102;&#26126;&#30830;&#30340;&#12289;&#32500;&#25345;&#29366;&#24577;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#20195;LLMs&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25512;&#29702;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2306.17840</link><description>&lt;p&gt;
Statler&#65306;&#29992;&#20110;&#20855;&#36523;&#25512;&#29702;&#30340;&#20445;&#25345;&#29366;&#24577;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Statler: State-Maintaining Language Models for Embodied Reasoning. (arXiv:2306.17840v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17840
&lt;/p&gt;
&lt;p&gt;
Statler&#26159;&#19968;&#20010;&#20026;LLMs&#36171;&#20104;&#20102;&#26126;&#30830;&#30340;&#12289;&#32500;&#25345;&#29366;&#24577;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#20195;LLMs&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25512;&#29702;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#26426;&#22120;&#20154;&#25191;&#34892;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;LLMs&#30340;&#26377;&#38480;&#19978;&#19979;&#25991;&#31383;&#21475;&#20351;&#24471;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#25512;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#20855;&#36523;&#20219;&#21153;&#65288;&#20363;&#22914;&#25105;&#20204;&#26399;&#26395;&#19968;&#20010;&#23478;&#24237;&#26426;&#22120;&#20154;&#25191;&#34892;&#30340;&#20219;&#21153;&#65289;&#36890;&#24120;&#38656;&#35201;&#35268;&#21010;&#32773;&#32771;&#34385;&#24456;&#20037;&#20043;&#21069;&#33719;&#24471;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#26426;&#22120;&#20154;&#22312;&#29615;&#22659;&#20013;&#36935;&#21040;&#30340;&#35768;&#22810;&#23545;&#35937;&#30340;&#23646;&#24615;&#65289;&#12290;&#36890;&#36807;LLM&#30340;&#38544;&#21547;&#20869;&#37096;&#34920;&#31034;&#26469;&#25429;&#33719;&#19990;&#30028;&#29366;&#24577;&#30340;&#23581;&#35797;&#20250;&#22240;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#21382;&#21490;&#20013;&#21487;&#29992;&#30340;&#19982;&#20219;&#21153;&#21644;&#29615;&#22659;&#30456;&#20851;&#30340;&#20449;&#24687;&#26377;&#38480;&#32780;&#21464;&#24471;&#22797;&#26434;&#65292;&#32780;&#20381;&#36182;&#36890;&#36807;&#25552;&#31034;&#21521;LLM&#20256;&#36882;&#20449;&#24687;&#30340;&#26041;&#27861;&#21017;&#21463;&#20854;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Statler&#65292;&#19968;&#20010;&#20026;LLMs&#36171;&#20104;&#20102;&#26126;&#30830;&#30340;&#12289;&#20316;&#20026;&#8220;&#35760;&#24518;&#8221;&#30340;&#19990;&#30028;&#29366;&#24577;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36825;&#31181;&#35760;&#24518;&#38543;&#26102;&#38388;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) provide a promising tool that enable robots to perform complex robot reasoning tasks. However, the limited context window of contemporary LLMs makes reasoning over long time horizons difficult. Embodied tasks such as those that one might expect a household robot to perform typically require that the planner consider information acquired a long time ago (e.g., properties of the many objects that the robot previously encountered in the environment). Attempts to capture the world state using an LLM's implicit internal representation is complicated by the paucity of task- and environment-relevant information available in a robot's action history, while methods that rely on the ability to convey information via the prompt to the LLM are subject to its limited context window. In this paper, we propose Statler, a framework that endows LLMs with an explicit representation of the world state as a form of ``memory'' that is maintained over time. Integral to Statler i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22635;&#20805;&#22240;&#26524;&#27169;&#26495;&#26469;&#29983;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20043;&#21069;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#21644;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#30097;&#34385;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#20182;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31038;&#20132;&#25512;&#29702;&#22522;&#20934;&#65288;BigToM&#65289;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21442;&#19982;&#32773;&#35780;&#20215;&#36825;&#20010;&#22522;&#20934;&#30340;&#36136;&#37327;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.15448</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20132;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Understanding Social Reasoning in Language Models with Language Models. (arXiv:2306.15448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22635;&#20805;&#22240;&#26524;&#27169;&#26495;&#26469;&#29983;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20043;&#21069;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#21644;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#30097;&#34385;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#20182;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31038;&#20132;&#25512;&#29702;&#22522;&#20934;&#65288;BigToM&#65289;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21442;&#19982;&#32773;&#35780;&#20215;&#36825;&#20010;&#22522;&#20934;&#30340;&#36136;&#37327;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20102;&#35299;&#23427;&#20204;&#29702;&#35299;&#20154;&#31867;&#24515;&#29702;&#29366;&#24577;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#26377;&#25928;&#30340;&#20132;&#20114;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#26377;&#20154;&#23581;&#35797;&#35780;&#20272;LLM&#30340;&#29702;&#35770;&#24515;&#26234;&#65288;ToM&#65289;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;ToM&#30340;&#19968;&#33268;&#31243;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25506;&#32034;&#20027;&#39064;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#23384;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#20043;&#21069;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#65292;&#65288;2&#65289;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#30097;&#34385;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22635;&#20805;&#22240;&#26524;&#27169;&#26495;&#26469;&#29983;&#25104;&#19982;LLM&#30340;&#35780;&#20272;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20026;LLM&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31038;&#20132;&#25512;&#29702;&#22522;&#20934;&#65288;BigToM&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;25&#20010;&#25511;&#21046;&#21644;5000&#20010;&#27169;&#22411;&#20889;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#20043;&#21069;&#20247;&#21253;&#35780;&#20272;&#30456;&#27604;&#65292;&#20154;&#31867;&#21442;&#19982;&#32773;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#30340;&#36136;&#37327;&#35780;&#20215;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11300</link><description>&lt;p&gt;
RS5M&#65306;&#29992;&#20110;&#36965;&#24863;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#21033;&#29992;&#24050;&#26377;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#22495;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#36801;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#24357;&#21512;&#20102;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#65288;RS&#65289;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;500&#19975;&#24352;&#24102;&#26377;&#33521;&#25991;&#25551;&#36848;&#30340;RS&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#20165;&#24102;&#26631;&#31614;&#30340;RS&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.11863</link><description>&lt;p&gt;
&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#30340;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#21333;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#22823;&#33041;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#19982;&#22823;&#33041;&#30340;&#30740;&#31350;&#37117;&#20351;&#29992;&#20102;&#31867;&#20284;GPT-2&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#26159;&#21542;&#26356;&#22823;&#30340;&#24320;&#28304;&#27169;&#22411;&#65288;&#22914;OPT&#21644;LLaMA&#31995;&#21015;&#65289;&#26356;&#36866;&#29992;&#20110;&#39044;&#27979;&#20351;&#29992;fMRI&#35760;&#24405;&#30340;&#22823;&#33041;&#21453;&#24212;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20174;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#22823;&#33041;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#36328;3&#20010;&#21463;&#35797;&#32773;&#30340;&#20445;&#30041;&#27979;&#35797;&#38598;&#30456;&#20851;&#24615;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;&#24403;&#25193;&#23637;fMRI&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#26102;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#30340;&#23545;&#25968;&#32447;&#24615;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#23545;&#20351;&#29992;HuBERT&#65292;WavLM&#21644;Whisper&#30340;&#22768;&#23398;&#32534;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#24102;&#26469;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22122;&#38899;&#22825;&#33457;&#26495;&#20998;&#26512;&#20102;&#36825;&#20123;&#22823;&#35268;&#27169;&#19988;&#39640;&#24615;&#33021;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations from transformer-based unidirectional language models are known to be effective at predicting brain responses to natural language. However, most studies comparing language models to brains have used GPT-2 or similarly sized language models. Here we tested whether larger open-source models such as those from the OPT and LLaMA families are better at predicting brain responses recorded using fMRI. Mirroring scaling results from other contexts, we found that brain prediction performance scales log-linearly with model size from 125M to 30B parameter models, with ~15% increased encoding performance as measured by correlation with a held-out test set across 3 subjects. Similar log-linear behavior was observed when scaling the size of the fMRI training set. We also characterized scaling for acoustic encoding models that use HuBERT, WavLM, and Whisper, and we found comparable improvements with model size. A noise ceiling analysis of these large, high-performance encoding models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16894</link><description>&lt;p&gt;
ViewRefer: &#22522;&#20110;GPT&#21644;&#26679;&#20363;&#24341;&#23548;&#30340;&#22810;&#35270;&#35282;&#30693;&#35782;&#22788;&#29702;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22810;&#35270;&#35282;&#36755;&#20837;&#30340;3D&#22330;&#26223;&#65292;&#21487;&#20197;&#32531;&#35299;3D&#35270;&#35273;&#23450;&#20301;&#20013;&#30340;&#35270;&#35282;&#24046;&#24322;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23884;&#20837;&#22312;&#25991;&#26412;&#27169;&#24577;&#20013;&#30340;&#35270;&#35282;&#32447;&#32034;&#65292;&#24182;&#19988;&#26410;&#33021;&#26435;&#34913;&#19981;&#21516;&#35270;&#22270;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#25506;&#32034;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#12290;&#20854;&#20013;&#65292;ViewRefer&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#30693;&#35782;&#65292;&#23558;&#21333;&#19968;&#30340;&#23450;&#20301;&#25991;&#26412;&#25193;&#23637;&#20026;&#22810;&#20010;&#20960;&#20309;&#19968;&#33268;&#30340;&#25551;&#36848;&#65307;&#21516;&#26102;&#65292;&#22312;3D&#27169;&#24577;&#20013;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#34701;&#21512;&#27169;&#22359;&#21644;&#35270;&#22270;&#38388;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#29289;&#20307;&#30340;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#29992;&#20110;&#35760;&#24518;&#19981;&#21516;&#35270;&#35282;&#19979;&#30340;&#22330;&#26223;&#26080;&#20851;&#30693;&#35782;&#65292;&#20174;&#20004;&#20010;&#26041;&#38754;&#22686;&#24378;&#20102;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding 3D scenes from multi-view inputs has been proven to alleviate the view discrepancy issue in 3D visual grounding. However, existing methods normally neglect the view cues embedded in the text modality and fail to weigh the relative importance of different views. In this paper, we propose ViewRefer, a multi-view framework for 3D visual grounding exploring how to grasp the view knowledge from both text and 3D modalities. For the text branch, ViewRefer leverages the diverse linguistic knowledge of large-scale language models, e.g., GPT, to expand a single grounding text to multiple geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer fusion module with inter-view attention is introduced to boost the interaction of objects across views. On top of that, we further present a set of learnable multi-view prototypes, which memorize scene-agnostic knowledge for different views, and enhance the framework from two perspectives: a view-guided attention module 
&lt;/p&gt;</description></item></channel></rss>