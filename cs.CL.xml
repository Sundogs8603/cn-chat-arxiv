<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#25361;&#25112;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22768;&#38899;&#21311;&#21517;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#38544;&#34255;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#25252;&#35821;&#35328;&#20869;&#23481;&#21644;&#24773;&#24863;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#32452;&#32455;&#32773;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#33050;&#26412;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2404.02677</link><description>&lt;p&gt;
VoicePrivacy 2024&#25361;&#25112;&#35780;&#20272;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
The VoicePrivacy 2024 Challenge Evaluation Plan
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25361;&#25112;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22768;&#38899;&#21311;&#21517;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#38544;&#34255;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#25252;&#35821;&#35328;&#20869;&#23481;&#21644;&#24773;&#24863;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#32452;&#32455;&#32773;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#33050;&#26412;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#30340;&#20219;&#21153;&#26159;&#20026;&#35821;&#38899;&#25968;&#25454;&#24320;&#21457;&#19968;&#31181;&#22768;&#38899;&#21311;&#21517;&#21270;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#38544;&#34255;&#35828;&#35805;&#32773;&#30340;&#35821;&#38899;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#25252;&#35821;&#35328;&#20869;&#23481;&#21644;&#24773;&#24863;&#29366;&#24577;&#12290;&#32452;&#32455;&#32773;&#25552;&#20379;&#24320;&#21457;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#33050;&#26412;&#65292;&#20197;&#21450;&#22522;&#20110;&#21442;&#19982;&#32773;&#35201;&#27714;&#24418;&#25104;&#30340;&#22522;&#32447;&#21311;&#21517;&#21270;&#31995;&#32479;&#21644;&#22521;&#35757;&#36164;&#28304;&#21015;&#34920;&#12290;&#21442;&#19982;&#32773;&#24212;&#29992;&#20182;&#20204;&#24320;&#21457;&#30340;&#21311;&#21517;&#21270;&#31995;&#32479;&#65292;&#36816;&#34892;&#35780;&#20272;&#33050;&#26412;&#65292;&#24182;&#23558;&#35780;&#20272;&#32467;&#26524;&#21644;&#21311;&#21517;&#21270;&#30340;&#35821;&#38899;&#25968;&#25454;&#25552;&#20132;&#32473;&#32452;&#32455;&#32773;&#12290;&#32467;&#26524;&#23558;&#22312;&#19982;Interspeech 2024&#21516;&#26399;&#20030;&#21150;&#30340;&#30740;&#35752;&#20250;&#19978;&#23637;&#31034;&#65292;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#34987;&#36992;&#35831;&#23637;&#31034;&#20182;&#20204;&#30340;&#25361;&#25112;&#31995;&#32479;&#24182;&#25552;&#20132;&#39069;&#22806;&#30340;&#30740;&#35752;&#20250;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02677v1 Announce Type: cross  Abstract: The task of the challenge is to develop a voice anonymization system for speech data which conceals the speaker's voice identity while protecting linguistic content and emotional states. The organizers provide development and evaluation datasets and evaluation scripts, as well as baseline anonymization systems and a list of training resources formed on the basis of the participants' requests. Participants apply their developed anonymization systems, run evaluation scripts and submit evaluation results and anonymized speech data to the organizers. Results will be presented at a workshop held in conjunction with Interspeech 2024 to which all participants are invited to present their challenge systems and to submit additional workshop papers.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934; LIConBench&#65292;&#32858;&#28966;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#20013;&#24615;&#33021;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#26631;&#35760;&#38271;&#24230;&#19981;&#36229;&#36807;20K&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;</title><link>https://arxiv.org/abs/2404.02060</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Long-context LLMs Struggle with Long In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934; LIConBench&#65292;&#32858;&#28966;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#20013;&#24615;&#33021;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#26631;&#35760;&#38271;&#24230;&#19981;&#36229;&#36807;20K&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#36229;&#36807;32K&#26631;&#35760;&#30340;&#38271;&#24207;&#21015;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#35780;&#20272;&#20027;&#35201;&#23616;&#38480;&#22312;&#22256;&#24785;&#24230;&#21644;&#21512;&#25104;&#20219;&#21153;&#31561;&#25351;&#26631;&#19978;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#23427;&#20204;&#22312;&#26356;&#24494;&#22937;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934;&#65288;LIConBench&#65289;&#65292;&#30528;&#37325;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#12290;&#25105;&#20204;&#31934;&#24515;&#36873;&#25321;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#26631;&#31614;&#33539;&#22260;&#36328;&#24230;&#20026;28&#33267;174&#31867;&#65292;&#28085;&#30422;&#20102;&#20174;2K&#21040;50K&#30340;&#19981;&#21516;&#36755;&#20837;&#65288;&#23569;&#37327;&#28436;&#31034;&#65289;&#38271;&#24230;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35201;&#27714;LLMs&#29702;&#35299;&#25972;&#20010;&#36755;&#20837;&#65292;&#20197;&#35782;&#21035;&#24222;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#20197;&#36827;&#34892;&#27491;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;13&#20010;&#38271;&#19978;&#19979;&#25991;LLMs&#12290;&#25105;&#20204;&#21457;&#29616;&#38271;&#19978;&#19979;&#25991;LLMs&#22312;&#26631;&#35760;&#38271;&#24230;&#20026;20K&#20197;&#19979;&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#65292;&#24182;&#19988;&#21033;&#29992;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#20250;&#24102;&#26469;&#24615;&#33021;&#19978;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02060v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#25512;&#29305;&#24773;&#24863;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#22810;&#27169;&#24577;&#26684;&#24335;&#65292;&#24182;&#36827;&#34892;&#22522;&#20934;&#23454;&#39564;&#65292;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20351;&#29992;&#24773;&#24863;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#26102;&#65292;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2404.01753</link><description>&lt;p&gt;
M2SA: &#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#25512;&#29305;&#24773;&#24863;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#22810;&#27169;&#24577;&#26684;&#24335;&#65292;&#24182;&#36827;&#34892;&#22522;&#20934;&#23454;&#39564;&#65292;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20351;&#29992;&#24773;&#24863;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#26102;&#65292;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38754;&#21521;&#23398;&#20064;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20998;&#26512;&#22810;&#27169;&#24577;&#20219;&#21153;&#26102;&#38656;&#35201;&#26356;&#28165;&#26224;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#20808;&#21069;&#20851;&#20110;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#20294;&#26412;&#25991;&#36890;&#36807;&#31616;&#21333;&#30340;&#25972;&#29702;&#36807;&#31243;&#23558;&#29616;&#26377;&#30340;&#25991;&#26412;&#25512;&#29305;&#24773;&#24863;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#22810;&#27169;&#24577;&#26684;&#24335;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#30740;&#31350;&#31038;&#21306;&#20869;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#22686;&#24378;&#25968;&#25454;&#38598;&#36827;&#34892;&#22522;&#20934;&#23454;&#39564;&#24182;&#25253;&#21578;&#20102;&#30740;&#31350;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#27604;&#36739;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#37197;&#32622;&#26102;&#65292;&#20351;&#29992;&#24773;&#24863;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01753v1 Announce Type: new  Abstract: In recent years, multimodal natural language processing, aimed at learning from diverse data types, has garnered significant attention. However, there needs to be more clarity when it comes to analysing multimodal tasks in multi-lingual contexts. While prior studies on sentiment analysis of tweets have predominantly focused on the English language, this paper addresses this gap by transforming an existing textual Twitter sentiment dataset into a multimodal format through a straightforward curation process. Our work opens up new avenues for sentiment-related research within the research community. Additionally, we conduct baseline experiments utilising this augmented dataset and report the findings. Notably, our evaluations reveal that when comparing unimodal and multimodal configurations, using a sentiment-tuned large language model as a text encoder performs exceptionally well.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Poincar&#233;&#35299;&#37322;&#26041;&#27861;&#22312;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^2logn)&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#25237;&#24433;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#23618;&#27425;&#32858;&#31867;&#36807;&#31243;&#21487;&#20197;&#35270;&#20026;&#26500;&#24314;&#26368;&#23567;&#29983;&#25104;&#26641;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#26377;&#25928;&#30340;&#31639;&#27861;</title><link>https://arxiv.org/abs/2403.16554</link><description>&lt;p&gt;
PE&#65306;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#25991;&#26412;&#23618;&#27425;&#29983;&#25104;&#30340;Poincar&#233;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PE: A Poincare Explanation Method for Fast Text Hierarchy Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16554
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Poincar&#233;&#35299;&#37322;&#26041;&#27861;&#22312;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^2logn)&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#25237;&#24433;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#23618;&#27425;&#32858;&#31867;&#36807;&#31243;&#21487;&#20197;&#35270;&#20026;&#26500;&#24314;&#26368;&#23567;&#29983;&#25104;&#26641;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#26377;&#25928;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16554v1 &#20844;&#21578;&#31867;&#22411;: cross &#25688;&#35201;: NLP&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#30740;&#31350;&#37325;&#28857;&#24050;&#32463;&#36716;&#31227;&#21040;&#23618;&#27425;&#23646;&#24615;&#65288;HA&#65289;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#32791;&#26102;&#30340;&#36138;&#23146;&#25628;&#32034;&#26469;&#24314;&#27169;&#38750;&#36830;&#32493;&#32452;&#21512;&#65292;&#24573;&#30053;&#20102;&#29305;&#24449;&#34920;&#31034;&#20013;&#28508;&#22312;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;Poincar&#233;&#35299;&#37322;&#65288;PE&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#36229;&#20960;&#20309;&#31354;&#38388;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(n^2logn)$&#12290;&#21463;Poincar&#233;&#27169;&#22411;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#23884;&#20837;&#25237;&#24433;&#21040;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#65292;&#36825;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#23545;&#21477;&#27861;&#21644;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25237;&#24433;&#31354;&#38388;&#20013;&#30340;&#23618;&#27425;&#32858;&#31867;&#36807;&#31243;&#21487;&#20197;&#34987;&#35270;&#20026;&#26500;&#24314;&#26368;&#23567;&#29983;&#25104;&#26641;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16554v1 Announce Type: cross  Abstract: The black-box nature of deep learning models in NLP hinders their widespread application. The research focus has shifted to Hierarchical Attribution (HA) for its ability to model feature interactions. Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations. In this work, we introduce a novel method, namely Poincar\'e Explanation (PE), for modeling feature interactions using hyperbolic spaces in an $O(n^2logn)$ time complexity. Inspired by Poincar\'e model, we propose a framework to project the embeddings into hyperbolic spaces, which exhibit better inductive biases for syntax and semantic hierarchical structures. Eventually, we prove that the hierarchical clustering process in the projected space could be viewed as building a minimum spanning tree and propose a time efficient algorithm. Experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#24369;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20195;&#30721;&#35757;&#32451;&#21644;&#25512;&#29702;&#26469;&#25913;&#21892;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02567</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#20174;LLMs&#20013;&#24341;&#20986;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#32467;&#26500;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Eliciting Better Multilingual Structured Reasoning from LLMs through Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02567
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#24369;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20195;&#30721;&#35757;&#32451;&#21644;&#25512;&#29702;&#26469;&#25913;&#21892;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#22312;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30740;&#31350;&#20165;&#38480;&#20110;&#33521;&#35821;&#25110;&#31616;&#21333;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;xSTREET&#30340;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20845;&#31181;&#35821;&#35328;&#30340;&#22235;&#20010;&#20219;&#21153;&#12290;xSTREET&#26292;&#38706;&#20102;&#22522;&#26412;LLM&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#25512;&#29702;&#20219;&#21153;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#24314;&#31435;&#22312;LLM&#22312;&#20195;&#30721;&#19978;&#35757;&#32451;&#26356;&#22909;&#30340;&#25512;&#29702;&#36825;&#19968;&#35266;&#28857;&#22522;&#30784;&#19978;&#12290;&#39318;&#20808;&#65292;&#22312;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#23558;&#20195;&#30721;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#22810;&#35821;&#35328;&#27880;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#31243;&#24207;&#20195;&#30721;&#19981;&#21464;&#12290;&#20854;&#27425;&#65292;&#22312;&#25512;&#26029;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#21253;&#21547;&#36880;&#27493;&#20195;&#30721;&#21407;&#35821;&#30340;&#25552;&#31034;&#32467;&#26500;&#26469;&#24357;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#25512;&#23548;&#20986;&#26032;&#20107;&#23454;&#24182;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;xSTREET&#19978;&#34920;&#29616;&#20986;&#20102;&#25913;&#36827;&#30340;&#22810;&#35821;&#35328;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#31185;&#23398;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02567v1 Announce Type: cross  Abstract: Development of large language models (LLM) have shown progress on reasoning, though studies have been limited to English or simple reasoning tasks. We thus introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks. We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multi-lingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reaso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#32467;&#26524;&#34920;&#26126;RAG&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01193</link><description>&lt;p&gt;
RAGged Edges: Retrieval-Augmented Chatbots&#30340;&#21452;&#20995;&#21073;
&lt;/p&gt;
&lt;p&gt;
RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#32467;&#26524;&#34920;&#26126;RAG&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273; - &#29983;&#25104;&#30475;&#20284;&#27491;&#30830;&#20294;&#38169;&#35823;&#20449;&#24687;&#30340;&#20542;&#21521;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#24456;&#20851;&#38190;&#65292;&#23601;&#20687;&#26368;&#36817;&#30340;&#27861;&#38498;&#26696;&#20363;&#20013;&#30475;&#21040;&#30340;&#37027;&#26679;&#65292;ChatGPT&#30340;&#20351;&#29992;&#23548;&#33268;&#20102;&#19981;&#23384;&#22312;&#30340;&#27861;&#24459;&#35009;&#20915;&#30340;&#24341;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#22806;&#37096;&#30693;&#35782;&#19982;&#25552;&#31034;&#38598;&#25104;&#26469;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26469;&#25269;&#21046;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26088;&#22312;&#35825;&#23548;&#24187;&#35273;&#30340;&#25552;&#31034;&#26469;&#23545;RAG&#19982;&#26631;&#20934;LLMs&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;RAG&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24403;&#25552;&#31034;&#30452;&#25509;&#19982;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#29702;&#35299;&#30456;&#30683;&#30462;&#26102;&#65292;RAG&#20173;&#28982;&#20250;&#34987;&#35823;&#23548;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#24187;&#35273;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RAG&#37096;&#32626;&#30340;&#23454;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01193v1 Announce Type: cross  Abstract: Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical, as seen in recent court cases where ChatGPT's use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#35282;&#33394;&#23450;&#20041;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25972;&#21512;&#26816;&#32034;&#20449;&#24687;&#20197;&#29983;&#25104;&#26356;&#21152;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18150</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#35282;&#33394;&#23450;&#20041;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25972;&#21512;&#26816;&#32034;&#20449;&#24687;&#20197;&#29983;&#25104;&#26356;&#21152;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#23558;&#26469;&#33258;&#26816;&#32034;&#30340;&#39069;&#22806;&#20449;&#24687;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#26377;&#25928;&#21033;&#29992;&#26816;&#32034;&#20449;&#24687;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#26377;&#26102;&#20250;&#24573;&#35270;&#25110;&#34987;&#38169;&#35823;&#24341;&#23548;&#12290;&#20854;&#20851;&#38190;&#21407;&#22240;&#22312;&#20110;LLMs&#30340;&#35757;&#32451;&#27809;&#26377;&#28165;&#26224;&#22320;&#35753;LLMs&#23398;&#20250;&#22914;&#20309;&#21033;&#29992;&#20855;&#26377;&#19981;&#21516;&#36136;&#37327;&#30340;&#26816;&#32034;&#25991;&#26412;&#36755;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35282;&#65292;&#23558;LLMs&#22312;RAG&#20013;&#30340;&#35282;&#33394;&#35270;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#36825;&#24847;&#21619;&#30528;&#26080;&#35770;&#26816;&#32034;&#25991;&#26412;&#30340;&#27491;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#25110;&#26377;&#29992;&#24615;&#22914;&#20309;&#65292;LLMs&#37117;&#33021;&#19968;&#33268;&#22320;&#25972;&#21512;&#26816;&#32034;&#25991;&#26412;&#20013;&#30340;&#30693;&#35782;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#29983;&#25104;&#27604;&#26816;&#32034;&#25991;&#26412;&#26356;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20248;&#21270;LLMs&#29992;&#20110;RAG&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18150v1 Announce Type: cross  Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CEPE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24182;&#34892;&#32534;&#30721;&#25193;&#23637;&#20102;&#29616;&#26377;&#20165;&#35299;&#30721;&#22120;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.16617</link><description>&lt;p&gt;
&#20855;&#26377;&#24182;&#34892;&#19978;&#19979;&#25991;&#32534;&#30721;&#30340;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Long-Context Language Modeling with Parallel Context Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16617
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CEPE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24182;&#34892;&#32534;&#30721;&#25193;&#23637;&#20102;&#29616;&#26377;&#20165;&#35299;&#30721;&#22120;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#21040;&#22788;&#29702;&#26356;&#38271;&#30340;&#36755;&#20837;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;transformers&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#21450;&#20301;&#32622;&#32534;&#30721;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Context Expansion with Parallel Encoding&#65288;CEPE&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#20165;&#35299;&#30721;&#22120;LLMs&#65292;&#20197;&#25193;&#23637;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;CEPE&#37319;&#29992;&#19968;&#20010;&#23567;&#22411;&#32534;&#30721;&#22120;&#26469;&#20998;&#22359;&#22788;&#29702;&#38271;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#20351;&#20923;&#32467;&#30340;&#35299;&#30721;&#22120;&#33021;&#22815;&#21033;&#29992;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#12290;CEPE&#39640;&#25928;&#12289;&#36890;&#29992;&#19988;&#22810;&#21151;&#33021;&#65306;&#36890;&#36807;&#20351;&#29992;8K&#26631;&#35760;&#25991;&#26723;&#36827;&#34892;&#35757;&#32451;&#65292;CEPE&#23558;LLAMA-2&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;128K&#26631;&#35760;&#65292;&#20165;&#20351;&#29992;1/6&#30340;&#20869;&#23384;&#21363;&#21487;&#33719;&#24471;10&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;CEPE&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#24615;&#33021;&#12290;CEPE&#22312;&#26816;&#32034;&#22686;&#24378;&#24212;&#29992;&#20013;&#20063;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#21017;&#36864;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16617v1 Announce Type: new  Abstract: Extending large language models (LLMs) to process longer inputs is crucial for numerous applications. However, the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window. We introduce Context Expansion with Parallel Encoding (CEPE), a framework that can be applied to any existing decoder-only LLMs to extend their context window. CEPE adopts a small encoder to process long inputs chunk by chunk and enables the frozen decoder to leverage additional contexts via cross-attention. CEPE is efficient, generalizable, and versatile: trained with 8K-token documents, CEPE extends the context window of LLAMA-2 to 128K tokens, offering 10x the throughput with only 1/6 of the memory. CEPE yields strong performance on language modeling and in-context learning. CEPE also excels in retrieval-augmented applications, while existing long-context models degenerat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13254</link><description>&lt;p&gt;
CounterCurate: &#36890;&#36807;&#23545;&#29031;&#20363;&#23376;&#22686;&#24378;&#29289;&#29702;&#21644;&#35821;&#20041;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CounterCurate&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#20840;&#38754;&#25552;&#21319;&#23545;&#27604;&#21644;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#38382;&#39064;&#65306;&#24573;&#35270;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#25512;&#29702;&#65288;&#35745;&#25968;&#21644;&#20301;&#32622;&#29702;&#35299;&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21453;&#20107;&#23454;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21019;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#31354;&#30333;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;LLaVA&#65289;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#32452;&#21512;&#25512;&#29702;&#20013;&#20960;&#20046;&#26080;&#27861;&#32988;&#20219;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;GLIGEN&#29983;&#25104;&#24494;&#35843;&#25968;&#25454;&#65292;&#20351;&#24471;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65306;&#22312;&#25105;&#20204;&#26032;&#30340;&#31574;&#21010;&#30340;Flickr30k-Positions&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CLIP&#21644;LLaVA&#30340;&#24615;&#33021;&#20998;&#21035;&#25552;&#39640;&#20102;+33%&#21644;+37%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19987;&#20026;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#32780;&#35774;&#35745;&#30340;"&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#35268;&#33539;&#27491;&#23383;&#27861;"&#65288;NOTA&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#38463;&#25289;&#20271;&#25991;&#25340;&#20889;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#30830;&#20445;&#23545;&#20854;&#29420;&#29305;&#38899;&#38901;&#21644;&#24418;&#24577;&#29305;&#24449;&#30340;&#20934;&#30830;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.12940</link><description>&lt;p&gt;
&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#30340;&#35268;&#33539;&#27491;&#23383;&#27861;
&lt;/p&gt;
&lt;p&gt;
Normalized Orthography for Tunisian Arabic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19987;&#20026;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#32780;&#35774;&#35745;&#30340;"&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#35268;&#33539;&#27491;&#23383;&#27861;"&#65288;NOTA&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#38463;&#25289;&#20271;&#25991;&#25340;&#20889;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#30830;&#20445;&#23545;&#20854;&#29420;&#29305;&#38899;&#38901;&#21644;&#24418;&#24577;&#29305;&#24449;&#30340;&#20934;&#30830;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#65288;ISO 693-3&#65306;aeb&#65289;&#26159;&#31361;&#23612;&#26031;&#29305;&#26377;&#30340;&#19968;&#31181;&#35821;&#35328;&#21464;&#20307;&#65292;&#26368;&#21021;&#28304;&#33258;&#38463;&#25289;&#20271;&#35821;&#65292;&#24182;&#21463;&#21040;&#22810;&#31181;&#21382;&#21490;&#24433;&#21709;&#30340;&#20016;&#23500;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#20026;&#20351;&#29992;&#38463;&#25289;&#20271;&#25991;&#25340;&#20889;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;"CODA*&#25351;&#21335;"&#30340;"&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#35268;&#33539;&#27491;&#23383;&#27861;"&#65288;NOTA&#65289;&#65292;&#37325;&#28857;&#22312;&#20110;&#29992;&#25143;&#21451;&#22909;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#29992;&#20110;&#35821;&#35328;&#36164;&#28304;&#24320;&#21457;&#30446;&#30340;&#12290;&#26356;&#26032;&#21518;&#30340;&#26631;&#20934;&#26088;&#22312;&#35299;&#20915;&#19982;&#20934;&#30830;&#34920;&#29616;&#31361;&#23612;&#26031;&#35821;&#38899;&#38901;&#21644;&#24418;&#24577;&#29420;&#29305;&#29305;&#24449;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;&#36825;&#23558;&#36890;&#36807;&#32416;&#27491;&#22522;&#20110;&#19982;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#30456;&#20284;&#24615;&#30340;&#38899;&#35793;&#25152;&#24341;&#21457;&#30340;&#38382;&#39064;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12940v1 Announce Type: new  Abstract: Tunisian Arabic (ISO 693-3: aeb) is a distinct linguistic variety native to Tunisia, initially stemmed from the Arabic language and enriched by a multitude of historical influences. This research introduces the "Normalized Orthography for Tunisian Arabic" (NOTA), an adaptation of CODA* guidelines tailored for transcribing Tunisian Arabic using the Arabic script for language resource development purposes, with an emphasis on user-friendliness and consistency. The updated standard seeks to address challenges related to accurately representing the unique characteristics of Tunisian phonology and morphology. This will be achieved by rectifying problems arising from transcriptions based on resemblances to Modern Standard Arabic.
&lt;/p&gt;</description></item><item><title>FormulaQA&#26159;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#20197;&#21450;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#24212;&#23545;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#30340;&#28508;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.12692</link><description>&lt;p&gt;
FormulaQA&#65306;&#19968;&#20010;&#22522;&#20110;&#20844;&#24335;&#30340;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12692
&lt;/p&gt;
&lt;p&gt;
FormulaQA&#26159;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#20197;&#21450;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#24212;&#23545;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#30340;&#28508;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#20844;&#24335;&#26159;&#20154;&#31867;&#22312;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#26102;&#30340;&#22522;&#26412;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#20540;&#25512;&#29702;&#25968;&#25454;&#38598;&#24456;&#23569;&#26126;&#30830;&#25351;&#20986;&#25512;&#29702;&#27493;&#39588;&#20013;&#20351;&#29992;&#30340;&#20844;&#24335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21021;&#20013;&#29289;&#29702;&#32771;&#35797;&#30340;&#20844;&#24335;&#39537;&#21160;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;FormulaQA&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22823;&#23567;&#20174;7B&#21040;&#36229;&#36807;100B&#21442;&#25968;&#30340;LLMs&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24605;&#32500;&#38142;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#24182;&#25506;&#32034;&#20102;&#22312;&#25552;&#20379;&#22806;&#37096;&#20844;&#24335;&#25968;&#25454;&#24211;&#26102;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23545;&#22823;&#23567;&#19981;&#36229;&#36807;2B&#30340;&#36739;&#23567;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#24378;&#35843;&#20102;&#24403;&#24212;&#29992;&#20110;&#25105;&#20204;&#22797;&#26434;&#12289;&#22522;&#20110;&#20844;&#24335;&#30340;FormulaQA&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#25913;&#36827;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12692v1 Announce Type: new  Abstract: The application of formulas is a fundamental ability of humans when addressing numerical reasoning problems. However, existing numerical reasoning datasets seldom explicitly indicate the formulas employed during the reasoning steps. To bridge this gap, we propose a question answering dataset for formula-based numerical reasoning called FormulaQA, from junior high school physics examinations. We further conduct evaluations on LLMs with size ranging from 7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thoughts methods and we explored the approach of using retrieval-augmented LLMs when providing an external formula database. We also fine-tune on smaller models with size not exceeding 2B. Our empirical findings underscore the significant potential for improvement in existing models when applied to our complex, formula-driven FormulaQA.
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26126;&#30830;&#25552;&#31034;&#26102;&#33021;&#22815;&#30456;&#24403;&#20934;&#30830;&#22320;&#35782;&#21035;&#35821;&#20041;&#19981;&#30830;&#23450;&#30340;&#21477;&#23376;&#65292;&#20294;&#23545;&#20854;&#36827;&#34892;&#27491;&#30830;&#35299;&#37322;&#21017;&#26356;&#20026;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.12486</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#26816;&#27979;&#21644;&#29702;&#35299;&#35821;&#20041;&#19981;&#30830;&#23450;&#24615;&#65311;&#38382;&#38382;DUST&#65281;
&lt;/p&gt;
&lt;p&gt;
Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the DUST!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12486
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26126;&#30830;&#25552;&#31034;&#26102;&#33021;&#22815;&#30456;&#24403;&#20934;&#30830;&#22320;&#35782;&#21035;&#35821;&#20041;&#19981;&#30830;&#23450;&#30340;&#21477;&#23376;&#65292;&#20294;&#23545;&#20854;&#36827;&#34892;&#27491;&#30830;&#35299;&#37322;&#21017;&#26356;&#20026;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#35821;&#35328;&#20351;&#29992;&#20013;&#65292;&#35828;&#35805;&#32773;&#32463;&#24120;&#20351;&#29992;&#35821;&#20041;&#19981;&#30830;&#23450;&#30340;&#21477;&#23376;&#65292;&#21363;&#20869;&#23481;&#19981;&#36275;&#20197;&#23436;&#20840;&#20256;&#36798;&#20854;&#20449;&#24687;&#25110;&#20197;&#21807;&#19968;&#26041;&#24335;&#35299;&#37322;&#23427;&#20204;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#8220;&#35821;&#20041;&#19981;&#30830;&#23450;&#21477;&#23376;&#31867;&#22411;&#20998;&#32452;&#25968;&#25454;&#38598;&#8221;&#65288;DUST&#65289;&#65292;&#29992;&#26469;&#30740;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#27491;&#30830;&#35782;&#21035;&#21644;&#35299;&#37322;&#35821;&#20041;&#19981;&#30830;&#23450;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#26126;&#30830;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30456;&#24403;&#20934;&#30830;&#22320;&#35782;&#21035;&#35821;&#20041;&#19981;&#30830;&#23450;&#30340;&#21477;&#23376;&#65292;&#20294;&#23545;&#20854;&#36827;&#34892;&#27491;&#30830;&#35299;&#37322;&#21017;&#26356;&#20026;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#35299;&#37322;&#35821;&#20041;&#19981;&#30830;&#23450;&#30340;&#21477;&#23376;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#36739;&#23567;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#19982;&#35821;&#20041;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35770;&#25551;&#36848;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12486v1 Announce Type: new  Abstract: In everyday language use, speakers frequently utter and interpret sentences that are semantically underspecified, namely, whose content is insufficient to fully convey their message or interpret them univocally. For example, to interpret the underspecified sentence "Don't spend too much", which leaves implicit what (not) to spend, additional linguistic context or outside knowledge is needed. In this work, we propose a novel Dataset of semantically Underspecified Sentences grouped by Type (DUST) and use it to study whether pre-trained language models (LMs) correctly identify and interpret underspecified sentences. We find that newer LMs are reasonably able to identify underspecified sentences when explicitly prompted. However, interpreting them correctly is much harder for any LMs. Our experiments show that when interpreting underspecified sentences, LMs exhibit little uncertainty, contrary to what theoretical accounts of underspecificati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;\textbf{MoEI}&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;EI&#30456;&#20851;&#20219;&#21153;&#38598;&#21512;\textsc{EiBench}&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#24773;&#24863;&#26234;&#21147;&#22686;&#24378;&#22120;&#30340;&#38598;&#25104;&#65292;&#32508;&#21512;&#25552;&#39640;&#20102;LLM&#30340;&#24773;&#24863;&#26234;&#21147;&#65288;EI&#65289;&#32780;&#19981;&#25439;&#23475;&#20854;&#26222;&#36866;&#26234;&#33021;&#65288;GI&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.10073</link><description>&lt;p&gt;
&#21516;&#26102;&#37325;&#35270;&#65306;&#22312;&#19981;&#25439;&#23475;&#26222;&#36866;&#26234;&#33021;&#30340;&#24773;&#24863;&#26234;&#33021;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#24378;&#24773;&#32490;&#26234;&#21147;
&lt;/p&gt;
&lt;p&gt;
Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;\textbf{MoEI}&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;EI&#30456;&#20851;&#20219;&#21153;&#38598;&#21512;\textsc{EiBench}&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#24773;&#24863;&#26234;&#21147;&#22686;&#24378;&#22120;&#30340;&#38598;&#25104;&#65292;&#32508;&#21512;&#25552;&#39640;&#20102;LLM&#30340;&#24773;&#24863;&#26234;&#21147;&#65288;EI&#65289;&#32780;&#19981;&#25439;&#23475;&#20854;&#26222;&#36866;&#26234;&#33021;&#65288;GI&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#26234;&#21147;&#65288;EI&#65289;&#21253;&#25324;&#24773;&#24863;&#24863;&#30693;&#12289;&#24773;&#24863;&#35748;&#30693;&#21644;&#24773;&#24863;&#34920;&#36798;&#65292;&#22312;&#25552;&#39640;&#24403;&#21069;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#35805;&#24335;&#26222;&#36866;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#19982;&#29992;&#25143;&#20132;&#20114;&#20307;&#39564;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#23545;EI&#30456;&#20851;&#30340;&#20998;&#31867;&#25110;&#22238;&#24402;&#20219;&#21153;&#30340;&#22825;&#30495;&#24494;&#35843;&#25552;&#39640;&#20854;&#24773;&#24863;&#24863;&#30693;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#23548;&#33268;&#20102;EI&#30340;&#19981;&#23436;&#20840;&#22686;&#24378;&#21644;&#23545;&#26222;&#36866;&#26234;&#33021;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;EI&#30456;&#20851;&#20219;&#21153;&#38598;&#21512;\textsc{EiBench}&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#35206;&#30422;&#20102;EI&#30340;&#19977;&#20010;&#26041;&#38754;&#30340;&#20219;&#21153;&#25351;&#31034;&#65292;&#20026;&#32508;&#21512;&#22686;&#24378;LLM&#30340;EI&#22880;&#23450;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#24773;&#32490;&#26234;&#21147;&#22686;&#24378;&#26041;&#27861;&#65288;\textbf{MoEI}&#65289;&#65292;&#21253;&#21547;&#20102;&#27169;&#22359;&#21270;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#24773;&#24863;&#26234;&#21147;&#22686;&#24378;&#22120;&#30340;&#38598;&#25104;&#65292;&#20197;&#21516;&#26102;&#25552;&#39640;EI&#21644;&#20445;&#25345;&#26222;&#36866;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10073v1 Announce Type: new  Abstract: Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce \textsc{EiBench}, a large-scale collection of EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel \underline{\textbf{Mo}}dular \underline{\textbf{E}}motional \underline{\textbf{I}}ntelligence enhancement method (\textbf{MoEI}), consisting of Modular
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;Chinese MentalBERT&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#20998;&#26512;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#24515;&#29702;&#23398;&#35789;&#20856;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09151</link><description>&lt;p&gt;
Chinese MentalBERT: &#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#38024;&#23545;&#20013;&#22269;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#20998;&#26512;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;Chinese MentalBERT&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#20998;&#26512;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#24515;&#29702;&#23398;&#35789;&#20856;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#31038;&#20132;&#23186;&#20307;&#30340;&#24433;&#21709;&#65292;&#24515;&#29702;&#38382;&#39064;&#22312;&#24403;&#21069;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#31038;&#20132;&#23186;&#20307;&#25104;&#20026;&#20010;&#20154;&#20998;&#20139;&#24863;&#21463;&#30340;&#37325;&#35201;&#20986;&#21475;&#12290;&#36825;&#23548;&#33268;&#27599;&#22825;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#20854;&#20013;&#36127;&#38754;&#24773;&#32490;&#26377;&#28508;&#21147;&#24341;&#21457;&#21361;&#26426;&#12290;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#20986;&#33021;&#22815;&#39640;&#25928;&#20998;&#26512;&#36825;&#20123;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#26174;&#31034;&#20986;&#25928;&#26524;&#65292;&#20294;&#38024;&#23545;&#24515;&#29702;&#23398;&#31561;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23384;&#22312;&#26126;&#26174;&#32570;&#22833;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25910;&#38598;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#20016;&#23500;&#20102;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;336&#19975;&#26465;&#25991;&#26412;&#26465;&#30446;&#30340;&#32508;&#21512;&#25968;&#25454;&#24211;&#12290;&#20026;&#25552;&#39640;&#27169;&#22411;&#22312;&#24515;&#29702;&#25991;&#26412;&#20998;&#26512;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#24515;&#29702;&#23398;&#35789;&#20856;&#34701;&#20837;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#26426;&#21046;&#12290;&#22312;&#29616;&#26377;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#36827;&#34892;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09151v1 Announce Type: new Abstract: In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings. This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations. There is a recognized need for models capable of efficient analysis. While pre-trained language models have demonstrated their effectiveness broadly, there's a noticeable gap in pre-trained models tailored for specialized domains like psychology. To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million text entries. To enhance the model's applicability to psychological text analysis, we integrated psychological lexicons into the pre-training masking mechanism. Building on an existing Chinese language mod
&lt;/p&gt;</description></item><item><title>SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09025</link><description>&lt;p&gt;
SLEB: &#36890;&#36807;&#20887;&#20313;&#39564;&#35777;&#21644;&#28040;&#38500;Transformer&#22359;&#20248;&#21270;LLM&#30340;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09025
&lt;/p&gt;
&lt;p&gt;
SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#32473;&#23454;&#38469;&#37096;&#32626;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#31934;&#31616;&#65292;&#19968;&#31181;&#26088;&#22312;&#20943;&#23567;LLM&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#20887;&#20313;&#32452;&#20214;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#31934;&#31616;&#26377;&#24076;&#26395;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31471;&#21040;&#31471;LLM&#25512;&#29702;&#21152;&#36895;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLEB&#65292;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;Transformer&#22359;&#20316;&#20026;&#31934;&#31616;&#30340;&#22522;&#26412;&#21333;&#20301;&#65292;&#22240;&#20026;LLM&#22312;&#30456;&#37051;&#22359;&#30340;&#36755;&#20986;&#20043;&#38388;&#20855;&#26377;&#22359;&#32423;&#21035;&#30340;&#20887;&#20313;&#21644;&#39640;&#30456;&#20284;&#24615;&#12290;&#36825;&#20010;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22686;&#24378;LLM&#30340;&#22788;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SLEB&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09025v1 Announce Type: new Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#24615;&#35843;&#26597;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21382;&#21490;&#12289;&#21457;&#23637;&#21644;&#21407;&#29702;&#65292;&#26088;&#22312;&#24110;&#21161;&#24191;&#27867;&#30340;&#35835;&#32773;&#32676;&#20307;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#32972;&#26223;&#21644;&#21407;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.06853</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21382;&#21490;&#12289;&#21457;&#23637;&#21644;&#21407;&#29702;-&#19968;&#39033;&#32508;&#36848;&#24615;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
History, Development, and Principles of Large Language Models-An Introductory Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06853
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#24615;&#35843;&#26597;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21382;&#21490;&#12289;&#21457;&#23637;&#21644;&#21407;&#29702;&#65292;&#26088;&#22312;&#24110;&#21161;&#24191;&#27867;&#30340;&#35835;&#32773;&#32676;&#20307;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#32972;&#26223;&#21644;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#22522;&#30707;&#65292;&#21033;&#29992;&#25968;&#23398;&#26041;&#27861;&#26469;&#25512;&#24191;&#35821;&#35328;&#35268;&#24459;&#21644;&#30693;&#35782;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;&#32463;&#36807;&#20960;&#21313;&#24180;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#35821;&#35328;&#24314;&#27169;&#20174;&#26368;&#21021;&#30340;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#21457;&#23637;&#21040;&#24403;&#20170;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LLMs&#30340;&#24555;&#36895;&#28436;&#36827;&#24050;&#32463;&#36798;&#21040;&#20102;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#27700;&#24179;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;LLMs&#22312;&#25913;&#21892;&#24037;&#20316;&#21644;&#20010;&#20154;&#29983;&#27963;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#19968;&#33324;&#20174;&#19994;&#20154;&#21592;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#32972;&#26223;&#21644;&#21407;&#29702;&#20102;&#35299;&#26377;&#38480;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;LLMs&#30340;&#32508;&#36848;&#37117;&#38598;&#20013;&#22312;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#20351;&#29992;&#20102;&#19987;&#38376;&#30340;&#35821;&#35328;&#65292;&#32473;&#32570;&#20047;&#30456;&#20851;&#32972;&#26223;&#30693;&#35782;&#30340;&#20174;&#19994;&#20154;&#21592;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#31616;&#26126;&#25212;&#35201;&#30340;LLMs&#27010;&#36848;&#65292;&#20197;&#24110;&#21161;&#26356;&#24191;&#27867;&#30340;&#35835;&#32773;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models serve as a cornerstone in natural language processing (NLP), utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models (SLMs) to the contemporary landscape of large language models (LLMs). Notably, the swift evolution of LLMs has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that LLMs offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most LLMs reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of LLMs to assist a broader audience. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#21512;&#20316;&#35757;&#32451;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12289;&#30693;&#35782;&#26356;&#26032;&#19981;&#36275;&#20197;&#21450;&#25512;&#29702;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#30693;&#35782;&#25512;&#29702;&#21644;&#25512;&#29702;&#32467;&#26524;&#36861;&#36394;&#12290;</title><link>https://arxiv.org/abs/2402.04978</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21327;&#20316;&#30340;&#22686;&#24378;&#22411;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#25512;&#29702;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#21512;&#20316;&#35757;&#32451;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12289;&#30693;&#35782;&#26356;&#26032;&#19981;&#36275;&#20197;&#21450;&#25512;&#29702;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#30693;&#35782;&#25512;&#29702;&#21644;&#25512;&#29702;&#32467;&#26524;&#36861;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36935;&#21040;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#34394;&#26500;&#38382;&#39064;&#12289;&#30693;&#35782;&#26356;&#26032;&#19981;&#36275;&#20197;&#21450;&#25512;&#29702;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#33258;&#30001;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#20854;&#20013;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#21644;LLMs&#20043;&#38388;&#23494;&#20999;&#21512;&#20316;&#12290;&#35813;&#26041;&#26696;&#39318;&#20808;&#20351;&#29992;LLMs&#36845;&#20195;&#22320;&#25506;&#32034;KG&#65292;&#36873;&#25321;&#24615;&#22320;&#26816;&#32034;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#30693;&#35782;&#23376;&#22270;&#20197;&#25903;&#25345;&#25512;&#29702;&#12290;&#28982;&#21518;&#24341;&#23548;LLMs&#36827;&#19968;&#27493;&#32452;&#21512;&#20869;&#22312;&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#22312;&#23376;&#22270;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#26126;&#30830;&#38416;&#36848;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#36825;&#31181;&#21327;&#20316;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#65292;&#24182;&#20415;&#20110;&#36861;&#36394;&#25512;&#29702;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLM&#30340;&#30693;&#35782;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#36719;&#26631;&#31614;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20445;&#35777;&#23398;&#29983;&#27169;&#22411;&#19982;&#25945;&#24072;&#27169;&#22411;&#30340;&#24615;&#33021;&#38750;&#24120;&#30456;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#31185;&#23398;&#25945;&#32946;&#35780;&#20272;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.15842</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#35780;&#20272;&#30340;LLM&#30340;&#33258;&#21160;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLM&#30340;&#30693;&#35782;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#36719;&#26631;&#31614;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20445;&#35777;&#23398;&#29983;&#27169;&#22411;&#19982;&#25945;&#24072;&#27169;&#22411;&#30340;&#24615;&#33021;&#38750;&#24120;&#30456;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#31185;&#23398;&#25945;&#32946;&#35780;&#20272;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#29305;&#21035;&#38024;&#23545;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;LLM&#30340;&#39044;&#27979;&#27010;&#29575;&#65288;&#20316;&#20026;&#36719;&#26631;&#31614;&#65289;&#26469;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65288;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;LLM&#20805;&#24403;&#25945;&#24072;&#27169;&#22411;&#12290;&#36825;&#36890;&#36807;&#19968;&#20010;&#19987;&#38376;&#20026;&#20102;&#20174;LLM&#30340;&#36755;&#20986;&#27010;&#29575;&#20013;&#23398;&#20064;&#32780;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#20197;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#19982;&#25945;&#24072;&#30340;&#24615;&#33021;&#38750;&#24120;&#30456;&#20284;&#12290;&#20026;&#20102;&#39564;&#35777;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;6,684&#20010;&#23398;&#29983;&#23545;&#31185;&#23398;&#38382;&#39064;&#30340;&#20889;&#20316;&#22238;&#31572;&#21644;&#19977;&#20010;&#20154;&#24037;&#19987;&#23478;&#35780;&#20998;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;7T&#12290;&#25105;&#20204;&#23558;&#20934;&#30830;&#24615;&#19982;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;TinyBERT&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
This study proposes a method for knowledge distillation (KD) of fine-tuned Large Language Models (LLMs) into smaller, more efficient, and accurate neural networks. We specifically target the challenge of deploying these models on resource-constrained devices. Our methodology involves training the smaller student model (Neural Network) using the prediction probabilities (as soft labels) of the LLM, which serves as a teacher model. This is achieved through a specialized loss function tailored to learn from the LLM's output probabilities, ensuring that the student model closely mimics the teacher's performance. To validate the performance of the KD approach, we utilized a large dataset, 7T, containing 6,684 student-written responses to science questions and three mathematical reasoning datasets with student-written responses graded by human experts. We compared accuracy with state-of-the-art (SOTA) distilled models, TinyBERT, and artificial neural network (ANN) models. Results have shown 
&lt;/p&gt;</description></item><item><title>&#21363;&#20351;&#26159;&#20219;&#21153;&#32422;&#26463;&#20063;&#20250;&#24433;&#21709;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#21363;&#20351;&#36825;&#20123;&#32422;&#26463;&#19982;&#35268;&#36991;&#26080;&#20851;&#65292;&#20063;&#20250;&#23548;&#33268;&#29616;&#26377;&#26816;&#27979;&#22120;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;</title><link>https://arxiv.org/abs/2311.08369</link><description>&lt;p&gt;
&#25351;&#20196;&#26041;&#24335;&#30340;&#37325;&#35201;&#24615;&#65306;&#21363;&#20351;&#20219;&#21153;&#32422;&#26463;&#20063;&#20250;&#24433;&#21709;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08369
&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#26159;&#20219;&#21153;&#32422;&#26463;&#20063;&#20250;&#24433;&#21709;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#21363;&#20351;&#36825;&#20123;&#32422;&#26463;&#19982;&#35268;&#36991;&#26080;&#20851;&#65292;&#20063;&#20250;&#23548;&#33268;&#29616;&#26377;&#26816;&#27979;&#22120;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28389;&#29992;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#24615;&#33021;&#21487;&#38752;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#12290;&#24403;&#29992;&#25143;&#25351;&#31034;LLMs&#29983;&#25104;&#25991;&#26412;&#26102;&#65292;&#25351;&#20196;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#21253;&#21547;&#19981;&#21516;&#30340;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#20026;LLM&#26816;&#27979;&#21019;&#24314;&#25968;&#25454;&#38598;&#26102;&#24182;&#27809;&#26377;&#28085;&#30422;&#36825;&#31181;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#27169;&#24335;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#20219;&#21153;&#23548;&#21521;&#30340;&#32422;&#26463;&#8212;&#8212;&#36825;&#20123;&#32422;&#26463;&#33258;&#28982;&#20250;&#21253;&#21547;&#22312;&#25351;&#20196;&#20013;&#65292;&#24182;&#19988;&#19982;&#26816;&#27979;&#35268;&#36991;&#26080;&#20851;&#8212;&#8212;&#20063;&#20250;&#23548;&#33268;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#22312;&#26816;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#36739;&#22823;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#20197;&#23398;&#29983;&#20316;&#25991;&#20889;&#20316;&#20026;&#29616;&#23454;&#39046;&#22495;&#65292;&#24182;&#26681;&#25454;&#20960;&#20010;&#22240;&#32032;&#25163;&#21160;&#21019;&#24314;&#22522;&#20110;&#20316;&#25991;&#36136;&#37327;&#30340;&#20219;&#21153;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24102;&#26377;&#36825;&#31181;&#32422;&#26463;&#30340;&#25351;&#20196;&#29983;&#25104;&#30340;&#25991;&#26412;&#19978;&#65292;&#24403;&#21069;&#26816;&#27979;&#22120;&#24615;&#33021;&#30340;&#26631;&#20934;&#20559;&#24046;&#65288;SD&#65289;&#26174;&#33879;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08369v2 Announce Type: replace  Abstract: To combat the misuse of Large Language Models (LLMs), many recent studies have presented LLM-generated-text detectors with promising performance. When users instruct LLMs to generate texts, the instruction can include different constraints depending on the user's need. However, most recent studies do not cover such diverse instruction patterns when creating datasets for LLM detection. In this paper, we find that even task-oriented constraints -- constraints that would naturally be included in an instruction and are not related to detection-evasion -- cause existing detectors to have a large variance in detection performance. We focus on student essay writing as a realistic domain and manually create task-oriented constraints based on several factors for essay quality. Our experiments show that the standard deviation (SD) of current detector performance on texts generated by an instruction with such a constraint is significantly large
&lt;/p&gt;</description></item><item><title>LLM-Pat&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#24182;&#27604;&#36739;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#8220;&#20804;&#24351;&#8221;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#21028;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2305.12519</link><description>&lt;p&gt;
LLM&#20146;&#23376;&#37492;&#23450;&#65306;LLM&#36951;&#20256;&#32487;&#25215;&#20013;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12519
&lt;/p&gt;
&lt;p&gt;
LLM-Pat&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#24182;&#27604;&#36739;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#8220;&#20804;&#24351;&#8221;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#21028;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#29983;&#25104;&#25658;&#24102;&#21508;&#31181;&#28389;&#29992;&#39118;&#38505;&#30340;&#25991;&#26412;&#65292;&#21253;&#25324;&#25220;&#34989;&#12289;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#21457;&#24067;&#34394;&#20551;&#35780;&#35770;&#65292;&#25110;&#32773;&#21046;&#20316;&#24341;&#20154;&#27880;&#30446;&#30340;&#34394;&#20551;&#25512;&#25991;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20005;&#37325;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#65292;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;LLM&#20146;&#23376;&#37492;&#23450;&#65288;LLM-Pat&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20219;&#20309;&#20505;&#36873;&#25991;&#26412;&#65288;"&#23376;&#31867;"&#65289;&#65292;LLM-Pat&#20351;&#29992;&#19968;&#20010;&#20013;&#38388;LLM&#65288;"&#29238;&#31867;"&#65289;&#37325;&#24314;&#19982;&#32473;&#23450;&#25991;&#26412;&#23545;&#24212;&#30340;"&#20804;&#24351;"&#25991;&#26412;&#65292;&#28982;&#21518;&#34913;&#37327;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;"&#20804;&#24351;"&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#39640;&#30456;&#20284;&#24615;&#34920;&#26126;&#20505;&#36873;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#65292;&#31867;&#20284;&#20110;&#22522;&#22240;&#29305;&#24449;&#12290;&#25105;&#20204;&#24050;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12519v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Detecting whether a text is machine-generated has thus become increasingly important. While existing detection methods exhibit superior performance, they often lack generalizability due to their heavy dependence on training data. To alleviate this problem, we propose a model-related generated text detection method, the LLM Paternity Test (LLM-Pat). Specifically, given any candidate text (\textit{child}), LLM-Pat employs an intermediary LLM (\textit{parent}) to reconstruct a \textit{sibling} text corresponding to the given text and then measures the similarity between candidate texts and their sibling texts. High similarity indicates that the candidate text is machine-generated, akin to genetic traits. We have constructed datasets encom
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09074</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#30721;&#27169;&#25311;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09074
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#30452;&#32447;&#31243;&#24207;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#22788;&#29702;&#36825;&#26679;&#31616;&#21333;&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#36739;&#24046;&#8212;&#8212;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#27169;&#25311;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#24207;&#31639;&#27861;&#21644;&#23884;&#22871;&#24490;&#29615;&#36229;&#36234;&#20102;&#30452;&#32447;&#31243;&#24207;&#30340;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#31243;&#24207;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30452;&#25509;&#24433;&#21709;LLMs&#27169;&#25311;&#20854;&#25191;&#34892;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#21482;&#26377;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#25165;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#12290;LLMs&#30340;&#20195;&#30721;&#27169;&#25311;&#19982;&#23427;&#20204;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35760;&#24518;&#33021;&#21147;&#23384;&#22312;&#30683;&#30462;&#65306;&#22312;&#35760;&#24518;&#23545;&#20219;&#21153;&#26377;&#23475;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#22810;&#26679;&#24615;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#21152;&#20837;&#22810;&#26679;&#24615;&#30446;&#26631;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2401.05054</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding. (arXiv:2401.05054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#22810;&#26679;&#24615;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#21152;&#20837;&#22810;&#26679;&#24615;&#30446;&#26631;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#20135;&#29983;&#19981;&#20165;&#27491;&#30830;&#32780;&#19988;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#12290;&#26368;&#36817;&#65292;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#22312;&#29983;&#25104;&#31639;&#27861;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#20135;&#29983;&#26368;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20026;&#29983;&#25104;&#22810;&#26679;&#21270;&#36755;&#20986;&#32780;&#25552;&#20986;&#30340;&#29616;&#26377;&#31639;&#27861;&#20027;&#35201;&#22522;&#20110;&#27874;&#26463;&#25628;&#32034;&#25110;&#38543;&#26426;&#25277;&#26679;&#65292;&#22240;&#27492;&#20854;&#36755;&#20986;&#36136;&#37327;&#21463;&#38480;&#20110;&#36825;&#20123;&#22522;&#26412;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;--&#36890;&#36807;&#23558;&#22810;&#26679;&#24615;&#30446;&#26631;&#24378;&#21152;&#21040;MBR&#35299;&#30721;&#20013;&#26469;&#24320;&#21457;&#20419;&#36827;&#22810;&#26679;&#24615;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;MBR&#30340;&#21464;&#20307;&#65292;&#21363;&#22810;&#26679;&#24615;MBR&#65288;DMBR&#65289;&#21644;k-medoids MBR&#65288;KMBR&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#19968;&#32452;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#21508;&#31181;&#23450;&#21521;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;DMBR&#21644;KMBR&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20256;&#32479;
&lt;/p&gt;
&lt;p&gt;
One of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse. Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed for generating diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying methods. In this paper, we investigate an alternative approach -- we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and $k$-medoids MBR (KMBR), methods to generate a set of sentences with high quality and diversity. We evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a large language model with prompting. The experimental results show that the proposed method achieves a better trad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#26469;&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#30340;&#26032;&#26041;&#24335;&#65292;&#20351;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#33021;&#22815;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2401.04319</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#20102;&#35299;&#24744;&#30340;&#38656;&#27714;&#65306;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. (arXiv:2401.04319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#26469;&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#30340;&#26032;&#26041;&#24335;&#65292;&#20351;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#33021;&#22815;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23450;&#20301;&#26041;&#24335;&#65292;&#21363;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#21487;&#20197;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#32467;&#26500;&#21270;&#36923;&#36753;&#35821;&#35328;&#65292;&#21363;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#12290;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#23581;&#35797;&#21033;&#29992;LLMs&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20294;&#26159;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;1&#65289;&#20808;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#31616;&#21333;&#30340;&#8220;&#35753;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#22320;&#24605;&#32771;&#8221;&#25552;&#31034;&#65292;&#35201;&#20040;&#22312;&#28436;&#31034;&#20013;&#25552;&#20379;&#22266;&#23450;&#30340;&#31034;&#20363;&#32780;&#19981;&#32771;&#34385;&#25552;&#31034;&#21644;&#38382;&#39064;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#65292;&#22312;&#19968;&#20123;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#32467;&#26500;&#21270;&#35821;&#35328;&#36716;&#25442;&#65289;&#20013;&#20351;LLMs&#26080;&#25928;&#12290;(2) &#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#38381;&#28304;&#27169;&#22411;&#25110;&#36807;&#24230;&#23454;&#29616;&#30340;&#27169;&#22411;&#20013;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. Past research indicates that the reasoning ability of LLMs can be effectively enhanced through chain-of-thought (CoT) prompting. But existing methods still have some limitations: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and questions, making LLMs ineffective in some complex reasoning tasks such as structured language transformation. (2) Previous methods are often implemented in closed-source models or exces
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#36817;&#20284;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;AMBR&#65289;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#24555;&#22320;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36845;&#20195;&#28040;&#38500;&#27861;&#31639;&#27861;&#26469;&#35299;&#20915;&#20013;&#20301;&#25968;&#35782;&#21035;&#38382;&#39064;&#65292;&#20197;&#36798;&#21040;&#21152;&#36895;&#35299;&#30721;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.02749</link><description>&lt;p&gt;
&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#26356;&#24555;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding. (arXiv:2401.02749v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#36817;&#20284;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;AMBR&#65289;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#24555;&#22320;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36845;&#20195;&#28040;&#38500;&#27861;&#31639;&#27861;&#26469;&#35299;&#20915;&#20013;&#20301;&#25968;&#35782;&#21035;&#38382;&#39064;&#65292;&#20197;&#36798;&#21040;&#21152;&#36895;&#35299;&#30721;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#22312;&#24191;&#27867;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#26367;&#20195;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;MBR&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#35745;&#31639;MBR&#30446;&#26631;&#65292;&#36825;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#38656;&#35201;&#21709;&#24212;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#19981;&#21487;&#34892;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#21098;&#26525;(CBP)&#26041;&#27861;&#26469;&#38477;&#20302;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#23427;&#33021;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#20294;&#26159;&#23427;&#38656;&#35201;&#20351;&#29992;&#24320;&#21457;&#38598;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#25165;&#33021;&#21457;&#25381;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#36817;&#20284;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;AMBR&#65289;&#35299;&#30721;&#26041;&#27861;&#12290;AMBR&#22522;&#20110;&#20197;&#19979;&#35266;&#23519;&#24471;&#20986;&#65306;&#35745;&#31639;&#22522;&#20110;&#26679;&#26412;&#30340;MBR&#30446;&#26631;&#30340;&#38382;&#39064;&#26159;&#20013;&#20301;&#25968;&#35782;&#21035;&#38382;&#39064;&#12290;AMBR&#20351;&#29992;&#20102;&#36845;&#20195;&#28040;&#38500;&#27861;&#65288;CSH&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to beam search decoding for a wide range of text generation tasks. However, MBR requires a huge amount of time for inference to compute the MBR objective, which makes the method infeasible in many situations where response time is critical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently been proposed to reduce the inference time in machine translation tasks. Although it is shown to significantly reduce the amount of computation, it requires hyperparameter tuning using a development set to be effective. To this end, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a hyperparameter-free method to run MBR decoding approximately. AMBR is derived from the observation that the problem of computing the sample-based MBR objective is the medoid identification problem. AMBR uses the Correlated Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best approximation algorithm to date
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#20010;&#21035;&#30340;&#27979;&#35797;&#36755;&#20837;&#37325;&#26032;&#20889;&#20316;&#20219;&#21153;&#25552;&#31034;&#65292;&#20351;&#20854;&#26356;&#20855;&#20307;&#12289;&#26126;&#30830;&#21644;&#23436;&#25972;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25351;&#23548;&#65292;&#23454;&#29616;&#20102;&#32422;10%&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.02107</link><description>&lt;p&gt;
&#23454;&#20363;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#30340;&#20851;&#24576;&#65306;&#20026;&#23454;&#20363;&#37325;&#20889;&#25552;&#31034;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance. (arXiv:2310.02107v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02107
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#20010;&#21035;&#30340;&#27979;&#35797;&#36755;&#20837;&#37325;&#26032;&#20889;&#20316;&#20219;&#21153;&#25552;&#31034;&#65292;&#20351;&#20854;&#26356;&#20855;&#20307;&#12289;&#26126;&#30830;&#21644;&#23436;&#25972;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25351;&#23548;&#65292;&#23454;&#29616;&#20102;&#32422;10%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#19968;&#30452;&#26159;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#33410;&#30465;&#20154;&#21147;&#65288;&#21363;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#30340;&#27880;&#37322;&#65289;&#65307;&#22240;&#27492;&#65292;&#38646;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#20063;&#20139;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#30528;&#37325;&#20110;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#20219;&#21153;&#25351;&#23548;&#65288;&#20363;&#22914;&#8220;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#24605;&#32771;&#8221;&#65289;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#27491;&#30830;&#35299;&#20915;&#38382;&#39064;&#65292;&#27599;&#20010;&#21333;&#29420;&#30340;&#27979;&#35797;&#23454;&#20363;&#38656;&#35201;&#26356;&#20180;&#32454;&#22320;&#35774;&#35745;&#21644;&#23450;&#21046;&#30340;&#25351;&#23548;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PRoMPTd&#65292;&#19968;&#31181;&#20026;&#27599;&#20010;&#20010;&#21035;&#30340;&#27979;&#35797;&#36755;&#20837;&#37325;&#26032;&#20889;&#20316;&#20219;&#21153;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#26356;&#21152;&#20855;&#20307;&#12289;&#26126;&#30830;&#21644;&#23436;&#25972;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4&#20316;&#20026;&#20219;&#21153;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#28085;&#30422;&#31639;&#26415;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#31561;&#20219;&#21153;&#30340;8&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;PRoMPTd&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PRoMPTd&#22312;&#22797;&#26434;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#32477;&#23545;&#25913;&#36827;&#32422;&#20026;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling large language models (LLMs) to perform tasks in zero-shot has been an appealing goal owing to its labor-saving (i.e., requiring no task-specific annotations); as such, zero-shot prompting approaches also enjoy better task generalizability. To improve LLMs' zero-shot performance, prior work has focused on devising more effective task instructions (e.g., ``let's think step by step'' ). However, we argue that, in order for an LLM to solve them correctly in zero-shot, individual test instances need more carefully designed and customized instructions. To this end, we propose PRoMPTd, an approach that rewrites the task prompt for each individual test input to be more specific, unambiguous, and complete, so as to provide better guidance to the task LLM. We evaluated PRoMPTd on eight datasets covering tasks including arithmetics, logical reasoning, and code generation, using GPT-4 as the task LLM. Notably, PRoMPTd achieves an absolute improvement of around 10% on the complex MATH dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14348</link><description>&lt;p&gt;
&#36890;&#36807;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#25269;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;LLMs&#21487;&#33021;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#26377;&#23475;&#25110;&#24694;&#24847;&#20869;&#23481;&#12290;&#23613;&#31649;&#26377;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#24182;&#38450;&#27490;&#23427;&#20204;&#29983;&#25104;&#19981;&#36866;&#24403;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#23545;&#40784;&#36890;&#24120;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#20248;&#21270;&#25110;&#25163;&#24037;&#26500;&#24314;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#20197;&#38450;&#33539;&#28508;&#22312;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#26500;&#24314;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#65292;&#36890;&#36807;&#20855;&#26377;&#31283;&#20581;&#23545;&#40784;&#26816;&#26597;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#23545;&#21407;&#22987;LLM&#36827;&#34892;&#20219;&#20309;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;RA-LLM&#22312;&#38450;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#22522;&#20110;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2309.02726</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20551;&#35774;&#21457;&#29616;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. (arXiv:2309.02726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#22522;&#20110;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31185;&#23398;&#23478;&#35266;&#23519;&#19990;&#30028;&#24182;&#35797;&#22270;&#25552;&#20986;&#35299;&#37322;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#30340;&#20551;&#35774;&#26102;&#65292;&#20551;&#35774;&#24402;&#32435;&#34987;&#35748;&#20026;&#26159;&#20027;&#35201;&#30340;&#25512;&#29702;&#31867;&#22411;&#12290;&#36807;&#21435;&#20851;&#20110;&#20551;&#35774;&#24402;&#32435;&#30340;&#30740;&#31350;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;&#65288;1&#65289;&#25968;&#25454;&#38598;&#30340;&#35266;&#23519;&#27880;&#37322;&#19981;&#26159;&#21407;&#22987;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#32780;&#26159;&#25163;&#21160;&#36873;&#25321;&#30340;&#21477;&#23376;&#65288;&#23548;&#33268;&#20102;&#19968;&#20010;&#23553;&#38381;&#39046;&#22495;&#30340;&#35774;&#32622;&#65289;&#65307;&#65288;2&#65289;&#23454;&#38469;&#30340;&#20551;&#35774;&#27880;&#37322;&#20027;&#35201;&#26159;&#24120;&#35782;&#30693;&#35782;&#65292;&#20351;&#24471;&#20219;&#21153;&#19981;&#22826;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;50&#31687;&#21457;&#34920;&#22312;&#39030;&#32423;&#31038;&#20250;&#31185;&#23398;&#26399;&#21002;&#19978;&#30340;&#26368;&#26032;&#35770;&#25991;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#25910;&#38598;&#20102;&#24320;&#21457;&#35770;&#25991;&#20013;&#30340;&#20551;&#35774;&#25152;&#38656;&#30340;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#20165;&#36890;&#36807;&#19968;&#22534;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#23601;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#21487;&#20197;&#35299;&#20915;&#20197;&#21069;&#20851;&#20110;&#20551;&#35774;&#24402;&#32435;&#30340;&#30740;&#31350;&#25152;&#38754;&#20020;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction has a limited setting that (1) the observation annotations of the dataset are not raw web corpus but are manually selected sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses annotations are mostly commonsense knowledge, making the task less challenging. In this work, we propose the first NLP dataset for social science academic hypotheses discovery, consisting of 50 recent papers published in top social science journals. Raw web corpora that are necessary for developing hypotheses in the published papers are also collected in the dataset, with the final goal of creating a system that automatically generates valid, novel, and helpful (to human researchers) hypotheses, given only a pile of raw web corpora. The new dataset can tackle the previou
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#39640;&#24615;&#33021;&#31070;&#32463;&#32763;&#35793;&#20998;&#31867;&#22120;&#20013;&#23384;&#22312;&#30340;&#8220;&#32874;&#26126;&#30340;&#27721;&#26031;&#8221;&#34892;&#20026;&#65292;&#23427;&#21033;&#29992;&#34394;&#20551;&#30456;&#20851;&#24615;&#32780;&#38750;&#30495;&#23454;&#30340;&#32763;&#35793;&#20449;&#21495;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20027;&#39064;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#20851;&#20110;&#34394;&#20551;&#20027;&#39064;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20197;&#21450;&#26377;&#20851;&#34394;&#20551;&#20027;&#39064;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.13170</link><description>&lt;p&gt;
&#22312;&#20998;&#31867;&#20013;&#27979;&#37327;&#34394;&#20551;&#30456;&#20851;&#24615;&#65306;&#35793;&#25991;&#20013;&#30340;&#8220;&#32874;&#26126;&#30340;&#27721;&#26031;&#8221;
&lt;/p&gt;
&lt;p&gt;
Measuring Spurious Correlation in Classification: 'Clever Hans' in Translationese. (arXiv:2308.13170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13170
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#39640;&#24615;&#33021;&#31070;&#32463;&#32763;&#35793;&#20998;&#31867;&#22120;&#20013;&#23384;&#22312;&#30340;&#8220;&#32874;&#26126;&#30340;&#27721;&#26031;&#8221;&#34892;&#20026;&#65292;&#23427;&#21033;&#29992;&#34394;&#20551;&#30456;&#20851;&#24615;&#32780;&#38750;&#30495;&#23454;&#30340;&#32763;&#35793;&#20449;&#21495;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20027;&#39064;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#20851;&#20110;&#34394;&#20551;&#20027;&#39064;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20197;&#21450;&#26377;&#20851;&#34394;&#20551;&#20027;&#39064;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#39640;&#24615;&#33021;&#31070;&#32463;&#32763;&#35793;&#20998;&#31867;&#22120;&#20013;&#23384;&#22312;&#8220;&#32874;&#26126;&#30340;&#27721;&#26031;&#8221;&#34892;&#20026;&#65292;&#21363;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#22120;&#21033;&#29992;&#25968;&#25454;&#19982;&#30446;&#26631;&#20998;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#29305;&#21035;&#26159;&#20027;&#39064;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#30340;&#32763;&#35793;&#20449;&#21495;&#12290;&#32763;&#35793;&#20449;&#21495;&#24494;&#22937;&#65288;&#23588;&#20854;&#26159;&#23545;&#20110;&#19987;&#19994;&#32763;&#35793;&#65289;&#65292;&#24182;&#19988;&#19982;&#25968;&#25454;&#20013;&#30340;&#35768;&#22810;&#20854;&#20182;&#20449;&#21495;&#31454;&#20105;&#65292;&#22914;&#27969;&#27966;&#12289;&#39118;&#26684;&#12289;&#20316;&#32773;&#21644;&#23588;&#20854;&#26159;&#20027;&#39064;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#24635;&#20307;&#38382;&#39064;&#65292;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#21040;&#24213;&#26377;&#22810;&#23569;&#26159;&#30001;&#20110;&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#26159;&#20998;&#31867;&#22120;&#23454;&#38469;&#38024;&#23545;&#30340;&#20449;&#21495;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#24494;&#22937;&#30340;&#30446;&#26631;&#20449;&#21495;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65288;&#20302;&#36164;&#28304;&#65289;&#25968;&#25454;&#29615;&#22659;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#22522;&#20110;&#20027;&#39064;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#20174;&#20004;&#20010;&#26041;&#21521;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;i&#65289;&#22312;&#27809;&#26377;&#20851;&#20110;&#34394;&#20551;&#20027;&#39064;&#20449;&#24687;&#21450;&#20854;&#22312;&#25968;&#25454;&#20013;&#20998;&#24067;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#65288;ii&#65289;&#22312;&#26377;&#20851;&#34394;&#20551;&#20027;&#39064;&#20449;&#24687;&#21450;&#20854;&#22312;&#25968;&#25454;&#20013;&#20998;&#24067;&#30340;&#19968;&#20123;&#25351;&#31034;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown evidence of 'Clever Hans' behavior in high-performance neural translationese classifiers, where BERT-based classifiers capitalize on spurious correlations, in particular topic information, between data and target classification labels, rather than genuine translationese signals. Translationese signals are subtle (especially for professional translation) and compete with many other signals in the data such as genre, style, author, and, in particular, topic. This raises the general question of how much of the performance of a classifier is really due to spurious correlations in the data versus the signals actually targeted for by the classifier, especially for subtle target signals and in challenging (low resource) data settings. We focus on topic-based spurious correlation and approach the question from two directions: (i) where we have no knowledge about spurious topic information and its distribution in the data, (ii) where we have some indication about the natur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#19981;&#38656;&#35201;&#32534;&#30721;&#25110;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;&#20960;&#20046;&#23436;&#32654;&#30340;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.04914</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#65292;&#20351;&#29992;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models. (arXiv:2302.04914v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#19981;&#38656;&#35201;&#32534;&#30721;&#25110;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;&#20960;&#20046;&#23436;&#32654;&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#24211;&#23545;&#20110;&#26448;&#26009;&#31185;&#23398;&#21644;&#24037;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#26469;&#24320;&#21457;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20174;&#30740;&#31350;&#35770;&#25991;&#30340;&#20840;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#24555;&#36895;&#24320;&#21457;&#35268;&#27169;&#36866;&#20013;&#30340;&#25968;&#25454;&#24211;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#19981;&#38656;&#35201;&#32534;&#30721;&#65292;&#19981;&#38656;&#35201;&#20851;&#20110;&#25552;&#21462;&#23646;&#24615;&#30340;&#20808;&#39564;&#30693;&#35782;&#25110;&#27169;&#22411;&#35757;&#32451;&#65292;&#19988;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;&#20960;&#20046;&#23436;&#32654;&#30340;&#31934;&#30830;&#24230;&#12290;&#35813;&#26041;&#27861;&#26159;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#65292;&#38500;&#20102;&#19968;&#20010;&#38656;&#35201;&#20154;&#24037;&#36741;&#21161;&#30340;&#27493;&#39588;&#65292;&#36890;&#24120;&#21482;&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#30340;&#20154;&#21147;&#21171;&#21160;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#20960;&#20046;&#21487;&#20197;&#19982;&#20219;&#20309;&#27492;&#31867;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#12290;&#36825;&#37324;&#35780;&#20272;&#20102;GPT-3/3.5&#12289;bart&#21644;DeBERTaV3&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#22312;&#25552;&#21462;&#20307;&#27169;&#37327;&#25968;&#25454;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#39640;&#36798;90%&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and comprehensive material databases extracted from research papers are critical for materials science and engineering but require significant human effort to develop. In this paper we present a simple method of extracting materials data from full texts of research papers suitable for quickly developing modest-sized databases. The method requires minimal to no coding, prior knowledge about the extracted property, or model training, and provides high recall and almost perfect precision in the resultant database. The method is fully automated except for one human-assisted step, which typically requires just a few hours of human labor. The method builds on top of natural language processing and large general language models but can work with almost any such model. The language models GPT-3/3.5, bart and DeBERTaV3 are evaluated here for comparison. We provide a detailed detailed analysis of the methods performance in extracting bulk modulus data, obtaining up to 90% precision at 9
&lt;/p&gt;</description></item></channel></rss>