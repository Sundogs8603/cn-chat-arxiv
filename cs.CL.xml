<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>DreamLLM&#26159;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;&#29983;&#25104;&#35821;&#35328;&#21644;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;DreamLLM&#33021;&#22815;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#20132;&#32455;&#20869;&#23481;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11499</link><description>&lt;p&gt;
DreamLLM&#65306;&#21327;&#21516;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
DreamLLM: Synergistic Multimodal Comprehension and Creation. (arXiv:2309.11499v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11499
&lt;/p&gt;
&lt;p&gt;
DreamLLM&#26159;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;&#29983;&#25104;&#35821;&#35328;&#21644;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;DreamLLM&#33021;&#22815;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#20132;&#32455;&#20869;&#23481;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DreamLLM&#65292;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#39318;&#27425;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#21033;&#29992;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;&#20043;&#38388;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;DreamLLM&#36981;&#24490;&#20004;&#20010;&#22522;&#26412;&#21407;&#21017;&#12290;&#31532;&#19968;&#20010;&#21407;&#21017;&#19987;&#27880;&#20110;&#36890;&#36807;&#22312;&#21407;&#22987;&#22810;&#27169;&#24577;&#31354;&#38388;&#20013;&#36827;&#34892;&#30452;&#25509;&#37319;&#26679;&#26469;&#29983;&#25104;&#35821;&#35328;&#21644;&#22270;&#20687;&#21518;&#39564;&#30340;&#29983;&#25104;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#20687;CLIP&#36825;&#26679;&#30340;&#22806;&#37096;&#29305;&#24449;&#25552;&#21462;&#22120;&#25152;&#22266;&#26377;&#30340;&#38480;&#21046;&#21644;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#20854;&#27425;&#65292;DreamLLM&#20419;&#36827;&#20102;&#21407;&#22987;&#30340;&#12289;&#20132;&#32455;&#30340;&#25991;&#20214;&#29983;&#25104;&#65292;&#23545;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#20197;&#21450;&#38750;&#32467;&#26500;&#21270;&#24067;&#23616;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;DreamLLM&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#25152;&#26377;&#26465;&#20214;&#12289;&#36793;&#32536;&#21644;&#32852;&#21512;&#22810;&#27169;&#24577;&#20998;&#24067;&#12290;&#20316;&#20026;&#32467;&#26524;&#65292;DreamLLM&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#20132;&#32455;&#20869;&#23481;&#30340;MLLM&#12290;&#32508;&#21512;&#23454;&#39564;&#31361;&#26174;&#20102;DreamLLM&#20316;&#20026;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#39564;&#35777;&#26041;&#27861;&#65288;CoVe&#65289;&#65292;&#36890;&#36807;&#22312;&#22238;&#31572;&#20043;&#21069;&#36827;&#34892;&#22791;&#26597;&#38382;&#39064;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;&#23454;&#39564;&#35777;&#26126;CoVe&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#33021;&#26377;&#25928;&#38477;&#20302;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2309.11495</link><description>&lt;p&gt;
&#38142;&#24335;&#39564;&#35777;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Verification Reduces Hallucination in Large Language Models. (arXiv:2309.11495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11495
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#39564;&#35777;&#26041;&#27861;&#65288;CoVe&#65289;&#65292;&#36890;&#36807;&#22312;&#22238;&#31572;&#20043;&#21069;&#36827;&#34892;&#22791;&#26597;&#38382;&#39064;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;&#23454;&#39564;&#35777;&#26126;CoVe&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#33021;&#26377;&#25928;&#38477;&#20302;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#29983;&#25104;&#21512;&#29702;&#20294;&#19981;&#27491;&#30830;&#30340;&#20107;&#23454;&#20449;&#24687;&#65288;&#21363;&#24187;&#35273;&#65289;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#20986;&#22238;&#22797;&#26102;&#36827;&#34892;&#24605;&#32771;&#20197;&#32416;&#27491;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38142;&#24335;&#39564;&#35777;&#65288;CoVe&#65289;&#26041;&#27861;&#65292;&#27169;&#22411;&#39318;&#20808;&#65288;i&#65289;&#36215;&#33609;&#21021;&#22987;&#22238;&#22797;&#65307;&#28982;&#21518;&#65288;ii&#65289;&#35745;&#21010;&#39564;&#35777;&#38382;&#39064;&#26469;&#20107;&#23454;&#26816;&#26597;&#33609;&#31295;&#65307;&#65288;iii&#65289;&#29420;&#31435;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#31572;&#26696;&#21463;&#20854;&#20182;&#22238;&#22797;&#30340;&#24433;&#21709;&#65307;&#26368;&#21518;&#65288;iv&#65289;&#29983;&#25104;&#26368;&#32456;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#22238;&#31572;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CoVe&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;&#24187;&#35273;&#30340;&#24773;&#20917;&#65292;&#21253;&#25324;&#26469;&#33258;&#32500;&#22522;&#25968;&#25454;&#30340;&#21015;&#34920;&#38382;&#39064;&#12289;&#23553;&#38381;&#20070;&#31821;MultiSpanQA&#21644;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.
&lt;/p&gt;</description></item><item><title>Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11489</link><description>&lt;p&gt;
Text2Reward&#65306;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11489
&lt;/p&gt;
&lt;p&gt;
Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#65307;&#23427;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#25110;&#39046;&#22495;&#25968;&#25454;&#65292;&#23548;&#33268;&#24320;&#21457;&#25104;&#26412;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text2Reward&#65292;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21487;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#30446;&#26631;&#65292;Text2Reward&#29983;&#25104;&#20316;&#20026;&#29615;&#22659;&#32039;&#20945;&#34920;&#31034;&#30340;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#36817;&#20351;&#29992;LLM&#32534;&#20889;&#31232;&#30095;&#22870;&#21169;&#20195;&#30721;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;Text2Reward&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20195;&#30721;&#65292;&#21487;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#29616;&#26377;&#36719;&#20214;&#21253;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;ManiSkill2&#65292;MetaWorld&#65289;&#21644;&#20004;&#20010;MuJoCo&#30340;&#36816;&#21160;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;Text2Reward&#12290;&#22312;17&#20010;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;13&#20010;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#22870;&#21169;&#20195;&#30721;&#35757;&#32451;&#30340;&#25919;&#31574;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Prompt Insertion (PI)&#30340;&#25511;&#21046;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2309.11439</link><description>&lt;p&gt;
&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20013;&#36890;&#36807;&#25552;&#31034;&#25554;&#20837;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21487;&#25511;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction. (arXiv:2309.11439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Prompt Insertion (PI)&#30340;&#25511;&#21046;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#20013;&#65292;&#30830;&#20445;&#29992;&#25143;&#29702;&#35299;&#20462;&#27491;&#21407;&#22240;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20110;&#20462;&#27491;&#21407;&#22240;&#30340;&#26631;&#35760;&#12289;&#31034;&#20363;&#21644;&#25552;&#31034;&#65292;&#20294;&#26410;&#30452;&#25509;&#35299;&#37322;&#20462;&#27491;&#21407;&#22240;&#12290;&#34429;&#28982;&#24050;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#21508;&#31181;&#20219;&#21153;&#25552;&#20379;&#30452;&#25509;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;GEC&#39046;&#22495;&#23578;&#26080;&#27492;&#31867;&#26041;&#27861;&#12290;&#29983;&#25104;GEC&#20462;&#27491;&#30340;&#35299;&#37322;&#28041;&#21450;&#23545;&#40784;&#36755;&#20837;&#21644;&#36755;&#20986;&#26631;&#35760;&#65292;&#35782;&#21035;&#20462;&#27491;&#28857;&#65292;&#24182;&#22987;&#32456;&#21576;&#29616;&#30456;&#24212;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25552;&#31034;&#38590;&#20197;&#26174;&#24335;&#22320;&#25511;&#21046;&#29983;&#25104;&#65292;&#22240;&#27492;&#24456;&#38590;&#25351;&#23450;&#22797;&#26434;&#30340;&#26684;&#24335;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Prompt Insertion (PI)&#30340;&#25511;&#21046;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#20351;LLMs&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20462;&#27491;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Grammatical Error Correction (GEC), it is crucial to ensure the user's comprehension of a reason for correction. Existing studies present tokens, examples, and hints as to the basis for correction but do not directly explain the reasons for corrections. Although methods that use Large Language Models (LLMs) to provide direct explanations in natural language have been proposed for various tasks, no such method exists for GEC. Generating explanations for GEC corrections involves aligning input and output tokens, identifying correction points, and presenting corresponding explanations consistently. However, it is not straightforward to specify a complex format to generate explanations, because explicit control of generation is difficult with prompts. This study introduces a method called controlled generation with Prompt Insertion (PI) so that LLMs can explain the reasons for corrections in natural language. In PI, LLMs first correct the input text, and then we automatically extract th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.11436</link><description>&lt;p&gt;
&#20320;&#20165;&#20851;&#27880;&#23631;&#24149;&#65306;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#26426;&#22120;&#20154;&#26088;&#22312;&#36890;&#36807;&#19982;&#29992;&#25143;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#65292;&#26080;&#38656;&#25163;&#21160;&#24178;&#39044;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22312;&#22810;&#26679;&#29615;&#22659;&#20013;&#26377;&#25928;&#21442;&#19982;&#12290;&#20026;&#20102;&#31526;&#21512;LLM&#30340;&#36755;&#20837;-&#36755;&#20986;&#35201;&#27714;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#27801;&#30418;&#29615;&#22659;&#20013;&#24320;&#21457;&#65292;&#20381;&#36182;&#20110;&#22806;&#37096;&#24037;&#20855;&#21644;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#30340;API&#23558;&#29615;&#22659;&#35299;&#26512;&#20026;&#25991;&#26412;&#20803;&#32032;&#65292;&#24182;&#35299;&#37322;&#39044;&#27979;&#30340;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#21463;&#21040;&#25512;&#29702;&#25928;&#29575;&#20302;&#21644;&#38169;&#35823;&#20256;&#25773;&#39118;&#38505;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Auto-UI&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#23545;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#30340;API&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#20316;&#38142;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#31995;&#21015;&#20013;&#38388;&#20808;&#21069;&#21160;&#20316;&#21382;&#21490;&#21644;&#26410;&#26469;&#21160;&#20316;&#35745;&#21010;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -leveraging a series of intermediate previous action histories and future action plans -- to help the age
&lt;/p&gt;</description></item><item><title>Kosmos-2.5&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25991;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#38405;&#35835;&#25991;&#26412;&#23494;&#38598;&#22411;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#31354;&#38388;&#24863;&#30340;&#25991;&#26412;&#22359;&#21644;&#32467;&#26500;&#21270;&#25991;&#26412;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#25552;&#31034;&#19979;&#20219;&#20309;&#25991;&#26412;&#23494;&#38598;&#22411;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#25193;&#23637;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.11419</link><description>&lt;p&gt;
Kosmos-2.5: &#19968;&#20010;&#22810;&#27169;&#24577;&#25991;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Kosmos-2.5: A Multimodal Literate Model. (arXiv:2309.11419v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11419
&lt;/p&gt;
&lt;p&gt;
Kosmos-2.5&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25991;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#38405;&#35835;&#25991;&#26412;&#23494;&#38598;&#22411;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#31354;&#38388;&#24863;&#30340;&#25991;&#26412;&#22359;&#21644;&#32467;&#26500;&#21270;&#25991;&#26412;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#25552;&#31034;&#19979;&#20219;&#20309;&#25991;&#26412;&#23494;&#38598;&#22411;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#25193;&#23637;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Kosmos-2.5&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;&#25991;&#26412;&#23494;&#38598;&#22411;&#22270;&#20687;&#36827;&#34892;&#26426;&#22120;&#38405;&#35835;&#30340;&#22810;&#27169;&#24577;&#25991;&#23398;&#27169;&#22411;&#12290;Kosmos-2.5&#22312;&#20004;&#20010;&#19981;&#21516;&#20294;&#30456;&#20114;&#21512;&#20316;&#30340;&#36716;&#24405;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65306;(1) &#29983;&#25104;&#20855;&#26377;&#31354;&#38388;&#24863;&#30340;&#25991;&#26412;&#22359;&#65292;&#20854;&#20013;&#27599;&#20010;&#25991;&#26412;&#22359;&#37117;&#34987;&#36171;&#20104;&#20854;&#22312;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#22352;&#26631;&#65292;&#20197;&#21450;(2) &#29983;&#25104;&#20197;Markdown&#26684;&#24335;&#25429;&#25417;&#26679;&#24335;&#21644;&#32467;&#26500;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#36755;&#20986;&#12290;&#36825;&#31181;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#25991;&#23398;&#33021;&#21147;&#26159;&#36890;&#36807;&#20849;&#20139;&#30340;Transformer&#26550;&#26500;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#21644;&#28789;&#27963;&#30340;&#25991;&#26412;&#34920;&#31034;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#22312;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#25991;&#26412;&#35782;&#21035;&#21644;&#22270;&#20687;&#21040;Markdown&#25991;&#26412;&#29983;&#25104;&#19978;&#35780;&#20272;&#20102;Kosmos-2.5&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#36731;&#26494;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#25552;&#31034;&#30340;&#20219;&#20309;&#25991;&#26412;&#23494;&#38598;&#22411;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#65292;&#20351;&#20854;&#25104;&#20026;&#28041;&#21450;&#25991;&#26412;&#20016;&#23500;&#22270;&#20687;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;&#36825;&#39033;&#24037;&#20316;&#36824;&#20026;&#26410;&#26469;&#30340;&#25193;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Kosmos-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. This work also paves the way for the future scaling o
&lt;/p&gt;</description></item><item><title>Safurai-001&#26159;&#19968;&#31181;&#26032;&#30340;&#20195;&#30721;LLM&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#24037;&#31243;&#36827;&#23637;&#21644;&#25351;&#20196;&#35843;&#20248;&#65292;&#23427;&#33021;&#22815;&#25552;&#20379;&#19982;&#20854;&#20182;&#26368;&#26032;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#23545;&#35805;&#24335;&#20132;&#20114;&#20307;&#39564;&#26041;&#38754;&#26377;&#25152;&#25552;&#21319;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;GPT4&#30340;&#22810;&#21442;&#25968;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#25552;&#20379;&#20102;&#20840;&#38754;&#27934;&#23519;&#27169;&#22411;&#30340;&#21151;&#33021;&#21644;&#24615;&#33021;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;Safurai-001&#36229;&#36807;&#20102;GPT-3.5 1.58%&#21644;WizardCoder 18.78%&#12290;</title><link>http://arxiv.org/abs/2309.11385</link><description>&lt;p&gt;
Safurai 001:&#20195;&#30721;LLM&#35780;&#20272;&#30340;&#26032;&#23450;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safurai 001: New Qualitative Approach for Code LLM Evaluation. (arXiv:2309.11385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11385
&lt;/p&gt;
&lt;p&gt;
Safurai-001&#26159;&#19968;&#31181;&#26032;&#30340;&#20195;&#30721;LLM&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#24037;&#31243;&#36827;&#23637;&#21644;&#25351;&#20196;&#35843;&#20248;&#65292;&#23427;&#33021;&#22815;&#25552;&#20379;&#19982;&#20854;&#20182;&#26368;&#26032;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#23545;&#35805;&#24335;&#20132;&#20114;&#20307;&#39564;&#26041;&#38754;&#26377;&#25152;&#25552;&#21319;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;GPT4&#30340;&#22810;&#21442;&#25968;&#35780;&#20272;&#22522;&#20934;&#65292;&#23427;&#25552;&#20379;&#20102;&#20840;&#38754;&#27934;&#23519;&#27169;&#22411;&#30340;&#21151;&#33021;&#21644;&#24615;&#33021;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;Safurai-001&#36229;&#36807;&#20102;GPT-3.5 1.58%&#21644;WizardCoder 18.78%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Safurai-001&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#30340;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#32534;&#30721;&#36741;&#21161;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#22522;&#20110;&#26368;&#26032;&#30340;&#32534;&#30721;LLM&#36827;&#23637;&#65292;Safurai-001&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#26032;&#27169;&#22411;&#22914;WizardCoder&#12289;PanguCoder&#21644;Phi-1&#30456;&#23218;&#32654;&#65292;&#20294;&#26088;&#22312;&#25552;&#20379;&#26356;&#21152;&#23545;&#35805;&#24335;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#24037;&#31243;&#30340;&#36827;&#23637;&#65288;&#21253;&#25324;&#26368;&#26032;&#30340;&#25968;&#25454;&#36716;&#25442;&#21644;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65289;&#21644;&#25351;&#20196;&#35843;&#20248;&#65292;&#36825;&#20010;&#26032;&#27169;&#22411;&#25215;&#35834;&#33021;&#22815;&#19982;&#26368;&#36817;&#30340;&#38381;&#28304;&#21644;&#24320;&#28304;&#21457;&#23637;&#24182;&#39550;&#40784;&#39537;&#12290;&#37492;&#20110;&#23545;&#32534;&#30721;LLM&#30340;&#39640;&#25928;&#35780;&#20272;&#25351;&#26631;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#22522;&#20110;GPT4&#30340;&#22810;&#21442;&#25968;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#22810;&#26679;&#30340;&#21442;&#25968;&#26469;&#20840;&#38754;&#20102;&#35299;&#27169;&#22411;&#30340;&#21151;&#33021;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;Safurai-001&#22312;&#24615;&#33021;&#19978;&#33021;&#22815;&#36229;&#36807;GPT-3.5 1.58%&#21644;WizardCoder 18.78%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Safurai-001, a new Large Language Model (LLM) with significant potential in the domain of coding assistance. Driven by recent advancements in coding LLMs, Safurai-001 competes in performance with the latest models like WizardCoder [Xu et al., 2023], PanguCoder [Shen et al., 2023] and Phi-1 [Gunasekar et al., 2023] but aims to deliver a more conversational interaction. By capitalizing on the progress in data engineering (including latest techniques of data transformation and prompt engineering) and instruction tuning, this new model promises to stand toe-to-toe with recent closed and open source developments. Recognizing the need for an efficacious evaluation metric for coding LLMs, this paper also introduces GPT4-based MultiParameters, an evaluation benchmark that harnesses varied parameters to present a comprehensive insight into the models functioning and performance. Our assessment shows that Safurai-001 can outperform GPT-3.5 by 1.58% and WizardCoder by 18.78% i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#23545;&#40784;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#32763;&#35793;&#19982;&#20998;&#21106;&#25918;&#22312;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65292;&#35813;&#26041;&#27861;&#26159;&#39318;&#27425;&#23454;&#29616;&#20102;&#36825;&#31181;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.11384</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#23545;&#40784;&#20998;&#21106;&#23454;&#29616;&#38271;&#25991;&#26412;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Long-Form End-to-End Speech Translation via Latent Alignment Segmentation. (arXiv:2309.11384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#23545;&#40784;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#32763;&#35793;&#19982;&#20998;&#21106;&#25918;&#22312;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65292;&#35813;&#26041;&#27861;&#26159;&#39318;&#27425;&#23454;&#29616;&#20102;&#36825;&#31181;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#21482;&#33021;&#22788;&#29702;&#20960;&#31186;&#38047;&#30340;&#38899;&#39057;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#21487;&#20197;&#26681;&#25454;&#20154;&#24037;&#26631;&#27880;&#30340;&#36716;&#24405;&#21644;&#32763;&#35793;&#25552;&#20379;&#21477;&#23376;&#32423;&#21035;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#24182;&#19981;&#23384;&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;&#20998;&#21106;&#12290;&#30446;&#21069;&#30340;&#35821;&#38899;&#20998;&#21106;&#26041;&#27861;&#35201;&#20040;&#25552;&#20379;&#36739;&#24046;&#30340;&#20998;&#21106;&#36136;&#37327;&#65292;&#35201;&#20040;&#38656;&#35201;&#20197;&#24310;&#36831;&#25442;&#21462;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#24310;&#36831;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#35821;&#38899;&#32763;&#35793;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#19982; ST CTC&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#30417;&#30563;&#25110;&#39069;&#22806;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20998;&#21106;&#20219;&#21153;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23454;&#38469;&#30340;&#31471;&#21040;&#31471;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#21516;&#19968;&#20010;&#27169;&#22411;&#21516;&#26102;&#29992;&#20110;&#32763;&#35793;&#21644;&#20998;&#21106;&#12290;&#22312;&#21508;&#31181;&#35821;&#35328;&#23545;&#21644;&#22495;&#20869;&#22806;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current simultaneous speech translation models can process audio only up to a few seconds long. Contemporary datasets provide an oracle segmentation into sentences based on human-annotated transcripts and translations. However, the segmentation into sentences is not available in the real world. Current speech segmentation approaches either offer poor segmentation quality or have to trade latency for quality. In this paper, we propose a novel segmentation approach for a low-latency end-to-end speech translation. We leverage the existing speech translation encoder-decoder architecture with ST CTC and show that it can perform the segmentation task without supervision or additional parameters. To the best of our knowledge, our method is the first that allows an actual end-to-end simultaneous speech translation, as the same model is used for translation and segmentation at the same time. On a diverse set of language pairs and in- and out-of-domain data, we show that the proposed approach ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#23556;&#20987;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#26694;&#26550;&#65292;&#20854;&#20013;&#36890;&#36807;&#19982;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#35752;&#35770;&#26469;&#25910;&#38598;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#20197;&#25552;&#21319;&#23548;&#33322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11382</link><description>&lt;p&gt;
&#35758;&#35770;&#20877;&#34892;&#21160;&#65306;&#22810;&#19987;&#23478;&#35752;&#35770;&#19979;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions. (arXiv:2309.11382v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#23556;&#20987;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#26694;&#26550;&#65292;&#20854;&#20013;&#36890;&#36807;&#19982;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#35752;&#35770;&#26469;&#25910;&#38598;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#20197;&#25552;&#21319;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#26159;&#19968;&#39033;&#38656;&#35201;&#21253;&#25324;&#29702;&#35299;&#12289;&#24863;&#30693;&#21644;&#35268;&#21010;&#22312;&#20869;&#30340;&#24191;&#27867;&#25216;&#33021;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#36825;&#26679;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;VLN&#26041;&#27861;&#23436;&#20840;&#20381;&#36182;&#20110;&#19968;&#20010;&#27169;&#22411;&#33258;&#24049;&#30340;&#24605;&#32771;&#22312;&#19968;&#20010;&#22238;&#21512;&#20869;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#29978;&#33267;&#26159;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT4&#65292;&#20173;&#28982;&#38590;&#20197;&#36890;&#36807;&#21333;&#22238;&#21512;&#33258;&#25105;&#24605;&#32771;&#26469;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#19987;&#23478;&#21672;&#35810;&#20250;&#35758;&#20013;&#24471;&#21040;&#28789;&#24863;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#23556;&#20987;VLN&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#30340;&#22823;&#22411;&#27169;&#22411;&#34987;&#20316;&#20026;&#39046;&#22495;&#19987;&#23478;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23548;&#33322;&#20195;&#29702;DiscussNav&#21487;&#20197;&#22312;&#27599;&#19968;&#27493;&#20043;&#21069;&#19982;&#36825;&#20123;&#19987;&#23478;&#31215;&#26497;&#35752;&#35770;&#65292;&#25910;&#38598;&#24517;&#35201;&#30340;&#20449;&#24687;&#20877;&#34892;&#21160;&#12290;&#36825;&#20123;&#35752;&#35770;&#28085;&#30422;&#20102;&#20851;&#38190;&#30340;&#23548;&#33322;&#23376;&#20219;&#21153;&#65292;&#22914;&#25351;&#20196;&#29702;&#35299;&#12289;&#29615;&#22659;&#24863;&#30693;&#21644;&#23436;&#25104;&#20272;&#35745;&#12290;&#36890;&#36807;&#32508;&#21512;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#35752;&#35770;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;VLN&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#22810;&#26041;&#38754;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model's own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting, we introduce a novel zero-shot VLN framework. Within this framework, large models possessing distinct abilities are served as domain experts. Our proposed navigation agent, namely DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks like instruction understanding, environment perception, and completion estimation. Through comprehensive experiments, we demonstrate that discussions w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#27431;&#27954;&#35758;&#20250;&#20013;&#21033;&#30410;&#38598;&#22242;&#65288;&#28216;&#35828;&#22242;&#20307;&#65289;&#30340;&#24433;&#21709;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#21033;&#30410;&#38598;&#22242;&#30340;&#31435;&#22330;&#25991;&#20214;&#21644;&#35758;&#21592;&#30340;&#35762;&#35805;&#65292;&#21457;&#29616;&#20102;&#35758;&#21592;&#21644;&#28216;&#35828;&#22242;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20123;&#32852;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#25581;&#31034;&#35758;&#21592;&#19982;&#28216;&#35828;&#22242;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23545;&#20110;&#29702;&#35299;&#35758;&#20250;&#31435;&#27861;&#36807;&#31243;&#20013;&#28216;&#35828;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.11381</link><description>&lt;p&gt;
&#30740;&#31350;&#27431;&#27954;&#35758;&#20250;&#20013;&#28216;&#35828;&#24433;&#21709;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Studying Lobby Influence in the European Parliament. (arXiv:2309.11381v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#27431;&#27954;&#35758;&#20250;&#20013;&#21033;&#30410;&#38598;&#22242;&#65288;&#28216;&#35828;&#22242;&#20307;&#65289;&#30340;&#24433;&#21709;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#21033;&#30410;&#38598;&#22242;&#30340;&#31435;&#22330;&#25991;&#20214;&#21644;&#35758;&#21592;&#30340;&#35762;&#35805;&#65292;&#21457;&#29616;&#20102;&#35758;&#21592;&#21644;&#28216;&#35828;&#22242;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20123;&#32852;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#25581;&#31034;&#35758;&#21592;&#19982;&#28216;&#35828;&#22242;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23545;&#20110;&#29702;&#35299;&#35758;&#20250;&#31435;&#27861;&#36807;&#31243;&#20013;&#28216;&#35828;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#27431;&#27954;&#35758;&#20250;&#20013;&#21033;&#30410;&#38598;&#22242;&#65288;&#28216;&#35828;&#22242;&#20307;&#65289;&#22312;&#31435;&#27861;&#36807;&#31243;&#20013;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#25910;&#38598;&#21644;&#20998;&#26512;&#20102;&#28216;&#35828;&#22242;&#20307;&#30340;&#31435;&#22330;&#25991;&#20214;&#21644;&#35758;&#21592;&#30340;&#35762;&#35805;&#65292;&#36890;&#36807;&#27604;&#36739;&#36825;&#20123;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#34164;&#21547;&#20851;&#31995;&#65292;&#25105;&#20204;&#33021;&#22815;&#21457;&#29616;&#35758;&#21592;&#21644;&#28216;&#35828;&#22242;&#20307;&#20043;&#38388;&#30340;&#21487;&#35299;&#37322;&#32852;&#31995;&#12290;&#22312;&#32570;&#20047;&#36825;&#31181;&#32852;&#31995;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#21457;&#29616;&#30340;&#32852;&#31995;&#19982;&#25105;&#20204;&#31574;&#21010;&#30340;&#19968;&#20221;&#20851;&#20110;&#35758;&#21592;&#21644;&#28216;&#35828;&#22242;&#20307;&#20043;&#38388;&#36716;&#21457;&#38142;&#25509;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#35758;&#21592;&#20844;&#24320;&#25259;&#38706;&#30340;&#20250;&#35758;&#36827;&#34892;&#38388;&#25509;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#36798;&#21040;&#20102;0.77&#30340;AUC&#20998;&#25968;&#65292;&#24182;&#19988;&#27604;&#20960;&#20010;&#22522;&#20934;&#26041;&#27861;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#21457;&#29616;&#30340;&#38142;&#25509;&#36827;&#34892;&#30340;&#32508;&#21512;&#20998;&#26512;&#65292;&#28041;&#21450;&#30456;&#20851;&#28216;&#35828;&#22242;&#20307;&#21644;&#35758;&#21592;&#30340;&#25919;&#27835;&#22242;&#20307;&#65292;&#19982;&#36825;&#20123;&#22242;&#20307;&#30340;&#24847;&#35782;&#24418;&#24577;&#30340;&#39044;&#26399;&#30456;&#19968;&#33268;&#65288;&#20363;&#22914;&#65292;&#20013;&#24038;&#32764;&#22242;&#20307;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method based on natural language processing (NLP), for studying the influence of interest groups (lobbies) in the law-making process in the European Parliament (EP). We collect and analyze novel datasets of lobbies' position papers and speeches made by members of the EP (MEPs). By comparing these texts on the basis of semantic similarity and entailment, we are able to discover interpretable links between MEPs and lobbies. In the absence of a ground-truth dataset of such links, we perform an indirect validation by comparing the discovered links with a dataset, which we curate, of retweet links between MEPs and lobbies, and with the publicly disclosed meetings of MEPs. Our best method achieves an AUC score of 0.77 and performs significantly better than several baselines. Moreover, an aggregate analysis of the discovered links, between groups of related lobbies and political groups of MEPs, correspond to the expectations from the ideology of the groups (e.g., center-left grou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#20998;&#22359;&#26463;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25511;&#21046;&#36136;&#37327;&#19982;&#24310;&#36831;&#30340;&#26435;&#34913;&#65292;&#36890;&#36807;&#24341;&#20837;&#26412;&#22320;&#19968;&#33268;&#24615;&#25110;&#20445;&#25345;-n&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#19981;&#25913;&#21464;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;0.6-3.6 BLEU&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#25110;&#32773;&#22312;&#19981;&#25913;&#21464;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;0.8-1.4&#31186;&#30340;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2309.11379</link><description>&lt;p&gt;
&#22686;&#37327;&#20998;&#22359;&#26463;&#25628;&#32034;&#29992;&#20110;&#20855;&#26377;&#21487;&#25511;&#30340;&#36136;&#37327;-&#24310;&#36831;&#26435;&#34913;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff. (arXiv:2309.11379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#20998;&#22359;&#26463;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25511;&#21046;&#36136;&#37327;&#19982;&#24310;&#36831;&#30340;&#26435;&#34913;&#65292;&#36890;&#36807;&#24341;&#20837;&#26412;&#22320;&#19968;&#33268;&#24615;&#25110;&#20445;&#25345;-n&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#19981;&#25913;&#21464;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;0.6-3.6 BLEU&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#25110;&#32773;&#22312;&#19981;&#25913;&#21464;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;0.8-1.4&#31186;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#20998;&#22359;&#33258;&#27880;&#24847;&#32534;&#30721;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#22312;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#39046;&#22495;&#23853;&#38706;&#22836;&#35282;&#12290;&#36825;&#20123;&#27169;&#22411;&#37319;&#29992;&#20998;&#22359;&#26463;&#25628;&#32034;&#21644;&#20551;&#35774;&#21487;&#38752;&#24615;&#35780;&#20998;&#26469;&#30830;&#23450;&#20309;&#26102;&#31561;&#24453;&#26356;&#22810;&#36755;&#20837;&#35821;&#38899;&#20197;&#20415;&#36827;&#19968;&#27493;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30452;&#21040;&#25972;&#20010;&#35821;&#38899;&#36755;&#20837;&#34987;&#28040;&#32791;&#25481;&#25165;&#33021;&#23637;&#31034;&#20986;&#21333;&#20010;&#30340;&#22686;&#37327;&#32763;&#35793;&#65292;&#26080;&#27861;&#30452;&#25509;&#21521;&#29992;&#25143;&#23637;&#31034;&#20986;&#21333;&#20010;&#22686;&#37327;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#25511;&#21046;&#36136;&#37327;&#19982;&#24310;&#36831;&#26435;&#34913;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22686;&#37327;&#20998;&#22359;&#26463;&#25628;&#32034;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#19968;&#33268;&#24615;&#25110;&#20445;&#25345;-n&#31574;&#30053;&#29992;&#20110;&#36136;&#37327;-&#24310;&#36831;&#25511;&#21046;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#22312;&#32447;&#25110;&#31163;&#32447;&#32763;&#35793;&#27169;&#22411;&#20013;&#65292;&#24182;&#35777;&#26126;&#36825;&#20004;&#31181;&#31867;&#22411;&#37117;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#22312;&#32447;&#27169;&#24335;&#20013;&#20351;&#29992;&#12290;MuST-C&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;0.6-3.6 BLEU&#30340;&#25552;&#21319;&#32780;&#19981;&#25913;&#21464;&#24310;&#36831;&#65292;&#25110;0.8-1.4&#31186;&#30340;&#24310;&#36831;&#25552;&#21319;&#32780;&#19981;&#25913;&#21464;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \textit{incremental} translation to users. Further, this method lacks mechanisms for \textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode.  Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.
&lt;/p&gt;</description></item><item><title>GECTurk&#26159;&#19968;&#20010;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#21644;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#23427;&#37319;&#29992;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#35206;&#30422;&#20102;20&#22810;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#35821;&#27861;&#21644;&#25340;&#20889;&#35268;&#21017;&#65292;&#24182;&#19988;&#36890;&#36807;&#25163;&#21160;&#27880;&#37322;&#30005;&#24433;&#35780;&#35770;&#21019;&#36896;&#20102;&#26356;&#30495;&#23454;&#30340;&#27979;&#35797;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.11346</link><description>&lt;p&gt;
GECTurk&#65306;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#21644;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GECTurk: Grammatical Error Correction and Detection Dataset for Turkish. (arXiv:2309.11346v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11346
&lt;/p&gt;
&lt;p&gt;
GECTurk&#26159;&#19968;&#20010;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#21644;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#23427;&#37319;&#29992;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#35206;&#30422;&#20102;20&#22810;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#35821;&#27861;&#21644;&#25340;&#20889;&#35268;&#21017;&#65292;&#24182;&#19988;&#36890;&#36807;&#25163;&#21160;&#27880;&#37322;&#30005;&#24433;&#35780;&#35770;&#21019;&#36896;&#20102;&#26356;&#30495;&#23454;&#30340;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#65288;GEC&#65289;&#24037;&#20855;&#24050;&#32463;&#34987;&#35777;&#26126;&#23545;&#20110;&#27597;&#35821;&#20351;&#29992;&#32773;&#21644;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#38750;&#24120;&#26377;&#29992;&#12290;&#24320;&#21457;&#36825;&#26679;&#30340;&#24037;&#20855;&#38656;&#35201;&#22823;&#37327;&#24179;&#34892;&#30340;&#12289;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#20294;&#26159;&#23545;&#20110;&#22823;&#22810;&#25968;&#35821;&#35328;&#26469;&#35828;&#65292;&#36825;&#31181;&#25968;&#25454;&#26159;&#19981;&#21487;&#24471;&#21040;&#30340;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26159;&#20811;&#26381;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#30340;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22303;&#32819;&#20854;&#35821;&#36825;&#26679;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#26469;&#35828;&#65292;&#24182;&#19981;&#30452;&#25509;&#65292;&#22240;&#20026;&#22797;&#26434;&#30340;&#20889;&#20316;&#35268;&#21017;&#38656;&#35201;&#38899;&#38901;&#12289;&#24418;&#24577;&#21644;&#21477;&#27861;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#22303;&#32819;&#20854;&#35821;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#28085;&#30422;&#20102;20&#22810;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#35821;&#27861;&#21644;&#25340;&#20889;&#35268;&#21017;&#65288;&#21363;&#20889;&#20316;&#35268;&#21017;&#65289;&#65292;&#36890;&#36807;&#22797;&#26434;&#30340;&#36716;&#25442;&#20989;&#25968;&#23454;&#29616;&#12290;&#20351;&#29992;&#36825;&#20010;&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#20174;&#19987;&#19994;&#32534;&#36753;&#30340;&#25991;&#31456;&#20013;&#27966;&#29983;&#20986;&#20102;13&#19975;&#26465;&#39640;&#36136;&#37327;&#30340;&#24179;&#34892;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25163;&#21160;&#27880;&#37322;&#19968;&#32452;&#30005;&#24433;&#35780;&#35770;&#26469;&#21019;&#24314;&#19968;&#20010;&#26356;&#30495;&#23454;&#30340;&#27979;&#35797;&#38598;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19977;&#20010;&#22522;&#20934;&#32447;&#65292;&#21046;&#23450;&#20102;&#20219;&#21153;&#30340;
&lt;/p&gt;
&lt;p&gt;
Grammatical Error Detection and Correction (GEC) tools have proven useful for native speakers and second language learners. Developing such tools requires a large amount of parallel, annotated data, which is unavailable for most languages. Synthetic data generation is a common practice to overcome the scarcity of such data. However, it is not straightforward for morphologically rich languages like Turkish due to complex writing rules that require phonological, morphological, and syntactic information. In this work, we present a flexible and extensible synthetic data generation pipeline for Turkish covering more than 20 expert-curated grammar and spelling rules (a.k.a., writing rules) implemented through complex transformation functions. Using this pipeline, we derive 130,000 high-quality parallel sentences from professionally edited articles. Additionally, we create a more realistic test set by manually annotating a set of movie reviews. We implement three baselines formulating the tas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#25991;&#31456;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#39640;&#38454;&#35821;&#20041;&#30340;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11341</link><description>&lt;p&gt;
&#29992;&#36793;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#25991;&#31456;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Article Classification with Edge-Heterogeneous Graph Neural Networks. (arXiv:2309.11341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#25991;&#31456;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#39640;&#38454;&#35821;&#20041;&#30340;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29616;&#26377;&#21644;&#26032;&#21457;&#24067;&#30340;&#25991;&#31456;&#25968;&#37327;&#24222;&#22823;&#65292;&#23558;&#30740;&#31350;&#25104;&#26524;&#20998;&#31867;&#21040;&#29305;&#23450;&#19978;&#19979;&#25991;&#26631;&#31614;&#20307;&#31995;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36793;&#24322;&#26500;&#22270;&#34920;&#31034;&#26469;&#20016;&#23500;&#31616;&#21333;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27969;&#27700;&#32447;&#65292;&#20197;&#25552;&#39640;&#25991;&#31456;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;SciBERT&#26469;&#29983;&#25104;&#33410;&#28857;&#29305;&#24449;&#65292;&#20197;&#25429;&#25417;&#25991;&#31456;&#30340;&#25991;&#26412;&#20803;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;Open Graph Benchmark&#65288;OGB&#65289;ogbn-arxiv&#25968;&#25454;&#38598;&#21644;PubMed&#31958;&#23615;&#30149;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23436;&#20840;&#30417;&#30563;&#30340;&#20256;&#23548;&#24335;&#33410;&#28857;&#20998;&#31867;&#23454;&#39564;&#65292;&#20998;&#21035;&#36890;&#36807;Microsoft Academic Graph&#65288;MAG&#65289;&#21644;PubMed Central&#28155;&#21152;&#20102;&#38468;&#21152;&#20803;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36793;&#24322;&#26500;&#22270;&#30456;&#23545;&#20110;&#36793;&#21516;&#26500;&#22270;&#65292;&#33021;&#22815;&#22987;&#32456;&#25552;&#39640;&#25152;&#26377;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#20351;&#31616;&#21333;&#19988;&#27973;&#23618;&#30340;GNN&#27969;&#27700;&#32447;&#33021;&#22815;&#19982;&#26356;&#22797;&#26434;&#30340;&#26550;&#26500;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifying research output into context-specific label taxonomies is a challenging and relevant downstream task, given the volume of existing and newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Networks (GNN) pipelines with edge-heterogeneous graph representations. SciBERT is used for node feature generation to capture higher-order semantics within the articles' textual metadata. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate that edge-heterogeneous graphs consistently improve the performance of all GNN models compared to the edge-homogeneous graphs. The transformed data enable simple and shallow GNN pipelines to achieve results on par with more complex architect
&lt;/p&gt;</description></item><item><title>TRAVID&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#32763;&#35793;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#32763;&#35793;&#21475;&#35821;&#65292;&#36824;&#21487;&#20197;&#23558;&#32763;&#35793;&#30340;&#35821;&#38899;&#19982;&#35828;&#35805;&#32773;&#30340;&#22068;&#21767;&#21160;&#20316;&#21516;&#27493;&#65292;&#20026;&#23398;&#29983;&#21644;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#35270;&#39057;&#32763;&#35793;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.11338</link><description>&lt;p&gt;
TRAVID&#65306;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#32763;&#35793;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TRAVID: An End-to-End Video Translation Framework. (arXiv:2309.11338v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11338
&lt;/p&gt;
&lt;p&gt;
TRAVID&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#32763;&#35793;&#26694;&#26550;&#65292;&#19981;&#20165;&#21487;&#20197;&#32763;&#35793;&#21475;&#35821;&#65292;&#36824;&#21487;&#20197;&#23558;&#32763;&#35793;&#30340;&#35821;&#38899;&#19982;&#35828;&#35805;&#32773;&#30340;&#22068;&#21767;&#21160;&#20316;&#21516;&#27493;&#65292;&#20026;&#23398;&#29983;&#21644;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#35270;&#39057;&#32763;&#35793;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#20840;&#29699;&#21270;&#30340;&#19990;&#30028;&#20013;&#65292;&#19982;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#20154;&#26377;&#25928;&#27807;&#36890;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#35821;&#35328;&#32763;&#35793;&#26041;&#27861;&#65292;&#22914;&#25991;&#23383;&#25110;&#20165;&#22768;&#38899;&#30340;&#32763;&#35793;&#65292;&#21487;&#20197;&#23436;&#25104;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#36890;&#36807;&#38754;&#37096;&#34920;&#24773;&#21644;&#22068;&#21767;&#21160;&#20316;&#20256;&#36798;&#30340;&#23436;&#25972;&#19978;&#19979;&#25991;&#21644;&#24494;&#22937;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#32763;&#35793;&#31995;&#32479;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#32763;&#35793;&#21475;&#35821;&#65292;&#36824;&#21487;&#20197;&#23558;&#32763;&#35793;&#30340;&#35821;&#38899;&#19982;&#35828;&#35805;&#32773;&#30340;&#22068;&#21767;&#21160;&#20316;&#21516;&#27493;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#19987;&#27880;&#20110;&#32763;&#35793;&#21508;&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;&#25945;&#32946;&#35762;&#24231;&#65292;&#24182;&#19988;&#23427;&#34987;&#35774;&#35745;&#25104;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#31995;&#32479;&#35774;&#32622;&#20013;&#20063;&#33021;&#26377;&#25928;&#12290;&#36890;&#36807;&#23558;&#19982;&#30446;&#26631;&#35821;&#35328;&#19968;&#33268;&#30340;&#22068;&#21767;&#21160;&#20316;&#19982;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#36827;&#34892;&#21305;&#37197;&#65292;&#20351;&#29992;&#35821;&#38899;&#20811;&#38534;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#24212;&#29992;&#20026;&#23398;&#29983;&#21644;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20307;&#39564;&#12290;&#36825;&#20010;&#38468;&#21152;&#21151;&#33021;&#21019;&#36896;&#20102;&#19968;&#20010;&#26356;&#23436;&#25972;&#30340;&#35270;&#39057;&#32763;&#35793;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's globalized world, effective communication with people from diverse linguistic backgrounds has become increasingly crucial. While traditional methods of language translation, such as written text or voice-only translations, can accomplish the task, they often fail to capture the complete context and nuanced information conveyed through nonverbal cues like facial expressions and lip movements. In this paper, we present an end-to-end video translation system that not only translates spoken language but also synchronizes the translated speech with the lip movements of the speaker. Our system focuses on translating educational lectures in various Indian languages, and it is designed to be effective even in low-resource system settings. By incorporating lip movements that align with the target language and matching them with the speaker's voice using voice cloning techniques, our application offers an enhanced experience for students and users. This additional feature creates a mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#25968;&#25454;&#20197;&#21450;&#25506;&#32034;&#20999;&#25442;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31361;&#23612;&#26031;&#26041;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26469;&#28040;&#38500;&#25340;&#20889;&#19981;&#21512;&#36866;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2309.11327</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#25910;&#38598;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20999;&#25442;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition. (arXiv:2309.11327v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#25968;&#25454;&#20197;&#21450;&#25506;&#32034;&#20999;&#25442;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31361;&#23612;&#26031;&#26041;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26469;&#28040;&#38500;&#25340;&#20889;&#19981;&#21512;&#36866;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#26041;&#35328;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#35201;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#36824;&#35201;&#22788;&#29702;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#31361;&#23612;&#26031;&#26041;&#35328;&#65292;&#35299;&#20915;&#20102;&#19978;&#36848;ASR&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25910;&#38598;&#20102;&#25991;&#26412;&#21644;&#38899;&#39057;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25506;&#32034;&#33258;&#25105;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#23569;&#26679;&#26412;&#20999;&#25442;&#26041;&#27861;&#65292;&#20197;&#22312;&#19981;&#21516;&#31361;&#23612;&#26031;&#27979;&#35797;&#38598;&#19978;&#25512;&#21160;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65307;&#28085;&#30422;&#19981;&#21516;&#30340;&#22768;&#23398;&#12289;&#35821;&#35328;&#21644;&#38901;&#24459;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#37492;&#20110;&#24120;&#35268;&#25340;&#20889;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#23545;&#36716;&#24405;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#36991;&#20813;&#27979;&#35797;&#21442;&#32771;&#20013;&#30340;&#25340;&#20889;&#19981;&#21512;&#36866;&#25152;&#24102;&#26469;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36716;&#24405;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#12289;&#33521;&#35821;&#21644;&#27861;&#35821;&#28151;&#21512;&#35821;&#35328;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#25152;&#26377;&#35757;&#32451;&#21644;&#27979;&#35797;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#20379;&#20844;&#20247;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crafting an effective Automatic Speech Recognition (ASR) solution for dialects demands innovative approaches that not only address the data scarcity issue but also navigate the intricacies of linguistic diversity. In this paper, we address the aforementioned ASR challenge, focusing on the Tunisian dialect. First, textual and audio data is collected and in some cases annotated. Second, we explore self-supervision, semi-supervision and few-shot code-switching approaches to push the state-of-the-art on different Tunisian test sets; covering different acoustic, linguistic and prosodic conditions. Finally, and given the absence of conventional spelling, we produce a human evaluation of our transcripts to avoid the noise coming from spelling inadequacies in our testing references. Our models, allowing to transcribe audio samples in a linguistic mix involving Tunisian Arabic, English and French, and all the data used during training and testing are released for public use and further improvem
&lt;/p&gt;</description></item><item><title>DISC-LawLLM&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20026;&#26234;&#33021;&#27861;&#24459;&#26381;&#21153;&#32454;&#35843;&#30340;&#26234;&#33021;&#27861;&#24459;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#27861;&#24459;&#25512;&#29702;&#25552;&#31034;&#31574;&#30053;&#21644;&#22686;&#24378;&#30340;&#26816;&#32034;&#27169;&#22359;&#65292;&#25552;&#20379;&#20102;&#22312;&#20013;&#22269;&#21496;&#27861;&#39046;&#22495;&#22810;&#26679;&#21270;&#27861;&#24459;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#27861;&#24459;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.11325</link><description>&lt;p&gt;
DISC-LawLLM:&#20026;&#26234;&#33021;&#27861;&#24459;&#26381;&#21153;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services. (arXiv:2309.11325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11325
&lt;/p&gt;
&lt;p&gt;
DISC-LawLLM&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20026;&#26234;&#33021;&#27861;&#24459;&#26381;&#21153;&#32454;&#35843;&#30340;&#26234;&#33021;&#27861;&#24459;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#27861;&#24459;&#25512;&#29702;&#25552;&#31034;&#31574;&#30053;&#21644;&#22686;&#24378;&#30340;&#26816;&#32034;&#27169;&#22359;&#65292;&#25552;&#20379;&#20102;&#22312;&#20013;&#22269;&#21496;&#27861;&#39046;&#22495;&#22810;&#26679;&#21270;&#27861;&#24459;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#27861;&#24459;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DISC-LawLLM&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20379;&#21508;&#31181;&#27861;&#24459;&#26381;&#21153;&#30340;&#26234;&#33021;&#27861;&#24459;&#31995;&#32479;&#12290;&#25105;&#20204;&#37319;&#29992;&#27861;&#24459;&#25512;&#29702;&#25552;&#31034;&#31574;&#30053;&#65292;&#22312;&#20013;&#22269;&#21496;&#27861;&#39046;&#22495;&#26500;&#24314;&#20102;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20855;&#22791;&#27861;&#24459;&#25512;&#29702;&#33021;&#21147;&#30340;LLMs&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21033;&#29992;&#26816;&#32034;&#27169;&#22359;&#22686;&#24378;&#20102;LLMs&#30340;&#33021;&#21147;&#65292;&#20197;&#35775;&#38382;&#21644;&#21033;&#29992;&#22806;&#37096;&#27861;&#24459;&#30693;&#35782;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27861;&#24459;&#22522;&#20934;&#35780;&#20272;&#31995;&#32479;&#65292;DISC-Law-Eval&#65292;&#20174;&#23458;&#35266;&#21644;&#20027;&#35266;&#20004;&#20010;&#32500;&#24230;&#35780;&#20272;&#26234;&#33021;&#27861;&#24459;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22312;DISC-Law-Eval&#19978;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20026;&#19981;&#21516;&#27861;&#24459;&#22330;&#26223;&#19979;&#30340;&#21508;&#31181;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;&#35814;&#32454;&#30340;&#36164;&#28304;&#21487;&#20197;&#22312;https://github.com/FudanDISC/DISC-LawLLM&#19978;&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose DISC-LawLLM, an intelligent legal system utilizing large language models (LLMs) to provide a wide range of legal services. We adopt legal syllogism prompting strategies to construct supervised fine-tuning datasets in the Chinese Judicial domain and fine-tune LLMs with legal reasoning capability. We augment LLMs with a retrieval module to enhance models' ability to access and utilize external legal knowledge. A comprehensive legal benchmark, DISC-Law-Eval, is presented to evaluate intelligent legal systems from both objective and subjective dimensions. Quantitative and qualitative results on DISC-Law-Eval demonstrate the effectiveness of our system in serving various users across diverse legal scenarios. The detailed resources are available at https://github.com/FudanDISC/DISC-LawLLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#23545;&#35805;&#20219;&#21153;&#21161;&#25163;&#30340;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#27969;&#29305;&#24449;&#21644;&#29992;&#25143;&#34892;&#20026;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#20154;-&#26426;&#23545;&#35805;&#21644;&#35780;&#20998;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#23545;&#35805;&#27969;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#29305;&#24449;&#32771;&#34385;&#22312;&#20869;&#30340;&#27169;&#22411;&#22312;&#31163;&#32447;&#35780;&#20998;&#39044;&#27979;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#23545;CTA&#29305;&#23450;&#34892;&#20026;&#29305;&#24449;&#30340;&#20998;&#26512;&#21487;&#20197;&#20026;&#26410;&#26469;&#30340;&#31995;&#32479;&#25552;&#20379;&#21551;&#21457;&#12290;</title><link>http://arxiv.org/abs/2309.11307</link><description>&lt;p&gt;
&#20351;&#29992;&#34892;&#20026;&#21644;&#23545;&#35805;&#27969;&#29305;&#24449;&#39044;&#27979;&#23545;&#35805;&#20219;&#21153;&#21161;&#25163;&#30340;&#35780;&#20998;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features. (arXiv:2309.11307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#23545;&#35805;&#20219;&#21153;&#21161;&#25163;&#30340;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23545;&#35805;&#27969;&#29305;&#24449;&#21644;&#29992;&#25143;&#34892;&#20026;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#20154;-&#26426;&#23545;&#35805;&#21644;&#35780;&#20998;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#23545;&#35805;&#27969;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#29305;&#24449;&#32771;&#34385;&#22312;&#20869;&#30340;&#27169;&#22411;&#22312;&#31163;&#32447;&#35780;&#20998;&#39044;&#27979;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#23545;CTA&#29305;&#23450;&#34892;&#20026;&#29305;&#24449;&#30340;&#20998;&#26512;&#21487;&#20197;&#20026;&#26410;&#26469;&#30340;&#31995;&#32479;&#25552;&#20379;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#23545;&#35805;&#20219;&#21153;&#21161;&#25163;&#65288;CTA&#65289;&#30340;&#25104;&#21151;&#23545;&#20110;&#29702;&#35299;&#29992;&#25143;&#34892;&#20026;&#24182;&#30456;&#24212;&#22320;&#37319;&#21462;&#34892;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TB-Rater&#65292;&#36825;&#26159;&#19968;&#20010;Transformer&#27169;&#22411;&#65292;&#23427;&#23558;&#23545;&#35805;&#27969;&#29305;&#24449;&#19982;&#29992;&#25143;&#34892;&#20026;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39044;&#27979;CTA&#22330;&#26223;&#20013;&#30340;&#29992;&#25143;&#35780;&#20998;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22312;Alexa TaskBot&#25361;&#25112;&#20013;&#25910;&#38598;&#30340;&#30495;&#23454;&#20154;-&#26426;&#23545;&#35805;&#21644;&#35780;&#20998;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21644;&#22810;&#36718;&#23545;&#35805;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#23545;&#35805;&#27969;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#29305;&#24449;&#27169;&#22411;&#21270;&#20026;&#21333;&#19968;&#27169;&#22411;&#23545;&#20110;&#31163;&#32447;&#35780;&#20998;&#39044;&#27979;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23545;CTA&#29305;&#23450;&#34892;&#20026;&#29305;&#24449;&#30340;&#20998;&#26512;&#20026;&#36825;&#31181;&#35774;&#32622;&#24102;&#26469;&#20102;&#27934;&#23519;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#21551;&#21160;&#26410;&#26469;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the success of Conversational Task Assistants (CTA) can be critical to understand user behavior and act accordingly. In this paper, we propose TB-Rater, a Transformer model which combines conversational-flow features with user behavior features for predicting user ratings in a CTA scenario. In particular, we use real human-agent conversations and ratings collected in the Alexa TaskBot challenge, a novel multimodal and multi-turn conversational context. Our results show the advantages of modeling both the conversational-flow and behavioral aspects of the conversation in a single model for offline rating prediction. Additionally, an analysis of the CTA-specific behavioral features brings insights into this setting and can be used to bootstrap future systems.
&lt;/p&gt;</description></item><item><title>CPLLM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#26469;&#39044;&#27979;&#30446;&#26631;&#30142;&#30149;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CPLLM&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11295</link><description>&lt;p&gt;
CPLLM: &#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPLLM: Clinical Prediction with Large Language Models. (arXiv:2309.11295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11295
&lt;/p&gt;
&lt;p&gt;
CPLLM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#26469;&#39044;&#27979;&#30446;&#26631;&#30142;&#30149;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CPLLM&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21033;&#29992;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;LLM&#65292;&#20219;&#21153;&#26159;&#39044;&#27979;&#24739;&#32773;&#22312;&#19979;&#19968;&#27425;&#23601;&#35786;&#25110;&#38543;&#21518;&#30340;&#35786;&#26029;&#20013;&#26159;&#21542;&#20250;&#34987;&#35786;&#26029;&#20026;&#30446;&#26631;&#30142;&#30149;&#65292;&#24182;&#21033;&#29992;&#20182;&#20204;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;RETAIN&#21644;Med-BERT&#65292;&#21518;&#32773;&#26159;&#20351;&#29992;&#32467;&#26500;&#21270;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#30142;&#30149;&#39044;&#27979;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CPLLM&#22312;PR-AUC&#21644;ROC-AUC&#25351;&#26631;&#19978;&#22343;&#36229;&#36807;&#20102;&#25152;&#26377;&#27979;&#35797;&#27169;&#22411;&#65292;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models.
&lt;/p&gt;</description></item><item><title>AuTexTification&#26159;IberLEF 2023&#30740;&#35752;&#20250;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#24402;&#23646;&#22810;&#39046;&#22495;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;160,000&#22810;&#26465;&#25991;&#26412;&#65292;&#28085;&#30422;&#20102;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20197;&#21450;&#25512;&#25991;&#12289;&#35780;&#35770;&#12289;&#26032;&#38395;&#12289;&#27861;&#24459;&#21644;&#25805;&#20316;&#25351;&#21335;&#31561;&#20116;&#20010;&#39046;&#22495;&#12290;&#20849;&#26377;114&#20010;&#22242;&#38431;&#21442;&#19982;&#65292;&#25552;&#20132;&#20102;175&#27425;&#36816;&#34892;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11285</link><description>&lt;p&gt;
&#12298;IberLEF 2023&#30340;AuTexTification&#27010;&#36848;&#65306;&#22810;&#39046;&#22495;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#21644;&#24402;&#23646;&#12299;
&lt;/p&gt;
&lt;p&gt;
Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains. (arXiv:2309.11285v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11285
&lt;/p&gt;
&lt;p&gt;
AuTexTification&#26159;IberLEF 2023&#30740;&#35752;&#20250;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#24402;&#23646;&#22810;&#39046;&#22495;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;160,000&#22810;&#26465;&#25991;&#26412;&#65292;&#28085;&#30422;&#20102;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20197;&#21450;&#25512;&#25991;&#12289;&#35780;&#35770;&#12289;&#26032;&#38395;&#12289;&#27861;&#24459;&#21644;&#25805;&#20316;&#25351;&#21335;&#31561;&#20116;&#20010;&#39046;&#22495;&#12290;&#20849;&#26377;114&#20010;&#22242;&#38431;&#21442;&#19982;&#65292;&#25552;&#20132;&#20102;175&#27425;&#36816;&#34892;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20316;&#20026;IberLEF 2023&#30740;&#35752;&#20250;&#19968;&#37096;&#20998;&#30340;AuTexTification&#20849;&#20139;&#20219;&#21153;&#30340;&#27010;&#36848;&#65292;&#35813;&#30740;&#35752;&#20250;&#26159;&#22312;SEPLN 2023&#20250;&#35758;&#26694;&#26550;&#20869;&#30340;&#20234;&#27604;&#21033;&#20122;&#35821;&#35328;&#35780;&#20272;&#35770;&#22363;&#20013;&#36827;&#34892;&#30340;&#12290;AuTexTification&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#22312;&#23376;&#20219;&#21153;1&#20013;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#30830;&#23450;&#19968;&#27573;&#25991;&#26412;&#26159;&#20154;&#24037;&#25776;&#20889;&#36824;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#12290;&#22312;&#23376;&#20219;&#21153;2&#20013;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#24402;&#23646;&#20110;&#20845;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;AuTexTification 2023&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20004;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65289;&#21644;&#20116;&#20010;&#39046;&#22495;&#65288;&#25512;&#25991;&#12289;&#35780;&#35770;&#12289;&#26032;&#38395;&#12289;&#27861;&#24459;&#21644;&#25805;&#20316;&#25351;&#21335;&#65289;&#65292;&#20849;&#21253;&#21547;&#36229;&#36807;160,000&#26465;&#25991;&#26412;&#12290;&#20849;&#26377;114&#20010;&#22242;&#38431;&#25253;&#21517;&#21442;&#19982;&#65292;&#20854;&#20013;36&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;175&#27425;&#36816;&#34892;&#32467;&#26524;&#65292;&#20854;&#20013;20&#20010;&#22242;&#38431;&#36824;&#25552;&#20132;&#20102;&#24037;&#20316;&#31508;&#35760;&#12290;&#22312;&#36825;&#20010;&#27010;&#36848;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AuTexTification&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#65292;&#20197;&#21450;&#21442;&#19982;&#31995;&#32479;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the overview of the AuTexTification shared task as part of the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the framework of the SEPLN 2023 conference. AuTexTification consists of two subtasks: for Subtask 1, participants had to determine whether a text is human-authored or has been generated by a large language model. For Subtask 2, participants had to attribute a machine-generated text to one of six different text generation models. Our AuTexTification 2023 dataset contains more than 160.000 texts across two languages (English and Spanish) and five domains (tweets, reviews, news, legal, and how-to articles). A total of 114 teams signed up to participate, of which 36 sent 175 runs, and 20 of them sent their working notes. In this overview, we present the AuTexTification dataset and task, the submitted participating systems, and the results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#23545;&#35805;&#20013;&#24341;&#20837;&#36259;&#21619;&#20107;&#23454;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#23545;&#35805;&#31995;&#32479;&#30340;&#21442;&#19982;&#24230;&#21644;&#25972;&#20307;&#20307;&#39564;&#65292;&#36890;&#36807;A/B&#27979;&#35797;&#26174;&#31034;&#35813;&#26041;&#27861;&#30456;&#23545;&#35780;&#20998;&#25552;&#39640;&#20102;9.7%&#12290;</title><link>http://arxiv.org/abs/2309.11283</link><description>&lt;p&gt;
&#36259;&#21619;&#20107;&#23454;&#19990;&#30028;&#30340;&#24043;&#24072;&#65306;&#29992;&#36259;&#38395;&#23089;&#20048;&#26041;&#24335;&#20016;&#23500;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
The Wizard of Curiosities: Enriching Dialogues with Fun Facts. (arXiv:2309.11283v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11283
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#23545;&#35805;&#20013;&#24341;&#20837;&#36259;&#21619;&#20107;&#23454;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#23545;&#23545;&#35805;&#31995;&#32479;&#30340;&#21442;&#19982;&#24230;&#21644;&#25972;&#20307;&#20307;&#39564;&#65292;&#36890;&#36807;A/B&#27979;&#35797;&#26174;&#31034;&#35813;&#26041;&#27861;&#30456;&#23545;&#35780;&#20998;&#25552;&#39640;&#20102;9.7%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#24341;&#20837;&#36259;&#21619;&#20107;&#23454;&#26159;&#20197;&#19968;&#31181;&#24841;&#24555;&#21644;&#26377;&#36259;&#30340;&#26041;&#24335;&#21521;&#23545;&#26041;&#20256;&#25480;&#26032;&#30693;&#35782;&#12290;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#36259;&#21619;&#20107;&#23454;&#34701;&#20837;&#23545;&#35805;&#20013;&#65292;&#21487;&#20197;&#25552;&#21319;&#29992;&#25143;&#23545;&#23545;&#35805;&#31995;&#32479;&#30340;&#24863;&#30693;&#21644;&#25972;&#20307;&#29992;&#25143;&#20307;&#39564;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#23450;&#21046;&#30340;&#36259;&#21619;&#20107;&#23454;&#65292;&#38024;&#23545;&#28921;&#39274;&#21644;DIY&#39046;&#22495;&#30340;&#23545;&#35805;&#36827;&#34892;&#20016;&#23500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22312;&#20122;&#39532;&#36874;Alexa TaskBot&#25361;&#25112;&#30340;&#22810;&#27169;&#24577;&#21644;&#22810;&#36718;&#23545;&#35805;&#29615;&#22659;&#20013;&#25910;&#38598;&#21040;&#30340;&#30495;&#23454;&#20154;&#19982;&#26426;&#22120;&#20154;&#23545;&#35805;&#12290;&#26681;&#25454;&#36229;&#36807;1000&#20010;&#23545;&#35805;&#30340;A/B&#27979;&#35797;&#32467;&#26524;&#65292;&#36259;&#21619;&#20107;&#23454;&#19981;&#20165;&#22686;&#21152;&#20102;&#29992;&#25143;&#30340;&#21442;&#19982;&#24230;&#65292;&#36824;&#25552;&#20379;&#20102;&#24179;&#22343;&#30456;&#23545;&#35780;&#20998;&#25552;&#39640;&#20102;9.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Introducing curiosities in a conversation is a way to teach something new to the person in a pleasant and enjoyable way. Enriching dialogues with contextualized curiosities can improve the users' perception of a dialog system and their overall user experience. In this paper, we introduce a set of curated curiosities, targeting dialogues in the cooking and DIY domains. In particular, we use real human-agent conversations collected in the context of the Amazon Alexa TaskBot challenge, a multimodal and multi-turn conversational setting. According to an A/B test with over 1000 conversations, curiosities not only increase user engagement, but provide an average relative rating improvement of 9.7%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#25351;&#20196;&#36716;&#21270;&#20026;&#20250;&#35805;&#24335;&#25351;&#20196;&#65292;&#38024;&#23545;&#20250;&#35805;&#21161;&#25163;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#20998;&#21106;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#26631;&#35760;&#30340;Transformer&#27169;&#22411;&#22312;&#35745;&#31639;&#20250;&#35805;&#27493;&#39588;&#29305;&#24449;&#26041;&#38754;&#25928;&#26524;&#26368;&#22909;&#65292;&#24182;&#19988;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#21407;&#22987;&#25351;&#20196;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11271</link><description>&lt;p&gt;
&#20026;&#20250;&#35805;&#21161;&#25163;&#30340;&#22797;&#26434;&#20219;&#21153;&#20998;&#21106;&#24314;&#31435;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Grounded Complex Task Segmentation for Conversational Assistants. (arXiv:2309.11271v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#25351;&#20196;&#36716;&#21270;&#20026;&#20250;&#35805;&#24335;&#25351;&#20196;&#65292;&#38024;&#23545;&#20250;&#35805;&#21161;&#25163;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#20998;&#21106;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#26631;&#35760;&#30340;Transformer&#27169;&#22411;&#22312;&#35745;&#31639;&#20250;&#35805;&#27493;&#39588;&#29305;&#24449;&#26041;&#38754;&#25928;&#26524;&#26368;&#22909;&#65292;&#24182;&#19988;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#21407;&#22987;&#25351;&#20196;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#38405;&#35835;&#30456;&#21516;&#30340;&#25351;&#20196;&#30456;&#27604;&#65292;&#20250;&#35805;&#21161;&#25163;&#22312;&#25191;&#34892;&#22797;&#26434;&#25351;&#20196;&#26102;&#24448;&#24448;&#20250;&#38754;&#20020;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#21147;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24403;&#20250;&#35805;&#21161;&#25163;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#30340;&#27493;&#39588;&#26102;&#65292;&#38656;&#35201;&#23558;&#20219;&#21153;&#32467;&#26500;&#21270;&#20026;&#21512;&#36866;&#38271;&#24230;&#21644;&#22797;&#26434;&#24230;&#30340;&#21487;&#31649;&#29702;&#20449;&#24687;&#12290;&#26412;&#25991;&#38024;&#23545;&#39135;&#35889;&#39046;&#22495;&#65292;&#23558;&#32467;&#26500;&#21270;&#25351;&#20196;&#36716;&#21270;&#20026;&#20250;&#35805;&#24335;&#25351;&#20196;&#12290;&#25105;&#20204;&#26681;&#25454;&#20250;&#35805;&#22330;&#26223;&#27880;&#37322;&#20102;&#25351;&#20196;&#30340;&#32467;&#26500;&#65292;&#20197;&#20102;&#35299;&#35813;&#22330;&#26223;&#20013;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;&#20026;&#20102;&#35745;&#31639;&#19978;&#36848;&#20250;&#35805;&#27493;&#39588;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;&#26631;&#35760;&#30340;&#26041;&#27861;&#25928;&#26524;&#26368;&#22909;&#12290;&#36827;&#19968;&#27493;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#25143;&#20542;&#21521;&#20110;&#25509;&#21463;&#38271;&#24230;&#21644;&#22797;&#26434;&#24230;&#36866;&#20013;&#30340;&#27493;&#39588;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#21407;&#22987;&#22522;&#20110;&#32593;&#39029;&#30340;&#25351;&#20196;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following complex instructions in conversational assistants can be quite daunting due to the shorter attention and memory spans when compared to reading the same instructions. Hence, when conversational assistants walk users through the steps of complex tasks, there is a need to structure the task into manageable pieces of information of the right length and complexity. In this paper, we tackle the recipes domain and convert reading structured instructions into conversational structured ones. We annotated the structure of instructions according to a conversational scenario, which provided insights into what is expected in this setting. To computationally model the conversational step's characteristics, we tested various Transformer-based architectures, showing that a token-based approach delivers the best results. A further user study showed that users tend to favor steps of manageable complexity and length, and that the proposed methodology can improve the original web-based instructi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;BART&#12289;T5&#21644;BERT2BERT-style&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11259</link><description>&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-Sequence Spanish Pre-trained Language Models. (arXiv:2309.11259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;BART&#12289;T5&#21644;BERT2BERT-style&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#22823;&#36827;&#23637;&#20026;&#35768;&#22810;&#38750;&#33521;&#35821;&#35821;&#35328;&#29256;&#26412;&#30340;&#24320;&#21457;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#20102;&#20165;&#32534;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#30340;&#26550;&#26500;&#12290;&#34429;&#28982;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21253;&#25324;BERT&#12289;RoBERTa&#21644;GPT&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20248;&#21183;&#65292;&#20294;&#22312;&#28041;&#21450;&#36755;&#20837;&#36755;&#20986;&#23545;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#32570;&#20047;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23454;&#26045;&#21644;&#35780;&#20272;&#33879;&#21517;&#30340;&#20165;&#22312;&#35199;&#29677;&#29273;&#35821;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24320;&#21019;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BART&#12289;T5&#21644;BERT2BERT&#39118;&#26684;&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#35821;&#29256;&#26412;&#65292;&#24182;&#23545;&#23427;&#20204;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#37325;&#36848;&#21644;&#29983;&#25104;&#24335;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#20854;&#20013;BART&#21644;T5&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, substantial advancements in pre-trained language models have paved the way for the development of numerous non-English language versions, with a particular focus on encoder-only and decoder-only architectures. While Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited prowess in natural language understanding and generation, there remains a scarcity of encoder-decoder models designed for sequence-to-sequence tasks involving input-output pairs. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder-decoder architectures, exclusively pre-trained on Spanish corpora. Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across a diverse range of sequence-to-sequence tasks, spanning summarization, rephrasing, and generative question answering. Our findings underscore the competitive performance of all models, with BART and T5 emerging a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#65288;V&#65286;L&#65289;&#27169;&#22411;&#22312;&#24418;&#24577;&#23398;&#23618;&#38754;&#19978;&#26159;&#21542;&#25429;&#25417;&#21040;&#34893;&#29983;&#30456;&#20851;&#35789;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#23588;&#20854;&#34920;&#29616;&#20986;&#19968;&#31181;&#35821;&#27861;&#20559;&#35265;&#12290;&#35813;&#30740;&#31350;&#21487;&#20026;&#27979;&#35797;V&#65286;L&#27169;&#22411;&#22312;&#25429;&#25417;&#20854;&#20182;&#24494;&#22937;&#35821;&#35328;&#29305;&#24449;&#26041;&#38754;&#30340;&#33021;&#21147;&#25552;&#20379;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11252</link><description>&lt;p&gt;
&#22330;&#26223;&#32454;&#21270;&#22120;&#65306;&#22312;&#24418;&#24577;&#23398;&#23618;&#38754;&#19978;&#23558;&#20027;&#39064;&#19982;&#22270;&#20687;&#30456;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
The Scenario Refiner: Grounding subjects in images at the morphological level. (arXiv:2309.11252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#65288;V&#65286;L&#65289;&#27169;&#22411;&#22312;&#24418;&#24577;&#23398;&#23618;&#38754;&#19978;&#26159;&#21542;&#25429;&#25417;&#21040;&#34893;&#29983;&#30456;&#20851;&#35789;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#23588;&#20854;&#34920;&#29616;&#20986;&#19968;&#31181;&#35821;&#27861;&#20559;&#35265;&#12290;&#35813;&#30740;&#31350;&#21487;&#20026;&#27979;&#35797;V&#65286;L&#27169;&#22411;&#22312;&#25429;&#25417;&#20854;&#20182;&#24494;&#22937;&#35821;&#35328;&#29305;&#24449;&#26041;&#38754;&#30340;&#33021;&#21147;&#25552;&#20379;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34893;&#29983;&#30340;&#30456;&#20851;&#35789;&#65292;&#22914;"runner"&#21644;"running"&#65292;&#23637;&#31034;&#20102;&#35821;&#20041;&#19978;&#30340;&#24046;&#24322;&#65292;&#20063;&#24341;&#21457;&#20102;&#19981;&#21516;&#30340;&#35270;&#35273;&#22330;&#26223;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#65288;V&#65286;L&#65289;&#27169;&#22411;&#22312;&#24418;&#24577;&#23398;&#23618;&#38754;&#19978;&#26159;&#21542;&#25429;&#25417;&#21040;&#36825;&#20123;&#21306;&#21035;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;V&#65286;L&#27169;&#22411;&#30340;&#32467;&#26524;&#19982;&#20154;&#31867;&#21028;&#26029;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#19981;&#21516;&#65292;&#23588;&#20854;&#34920;&#29616;&#20986;&#19968;&#31181;&#35821;&#27861;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20154;&#31867;&#27169;&#22411;&#19981;&#23545;&#40784;&#19982;&#27169;&#22411;&#26550;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25512;&#24191;&#29992;&#20110;&#27979;&#35797;&#27169;&#22411;&#22312;&#25429;&#25417;&#20854;&#20182;&#24494;&#22937;&#35821;&#35328;&#29305;&#24449;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Derivationally related words, such as "runner" and "running", exhibit semantic differences which also elicit different visual scenarios. In this paper, we ask whether Vision and Language (V\&amp;L) models capture such distinctions at the morphological level, using a a new methodology and dataset. We compare the results from V\&amp;L models to human judgements and find that models' predictions differ from those of human participants, in particular displaying a grammatical bias. We further investigate whether the human-model misalignment is related to model architecture. Our methodology, developed on one specific morphological contrast, can be further extended for testing models on capturing other nuanced language features.
&lt;/p&gt;</description></item><item><title>OpenChat&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#24182;&#31616;&#21270;RLFT&#26041;&#27861;&#30340;&#27714;&#35299;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.11235</link><description>&lt;p&gt;
OpenChat: &#29992;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenChat: Advancing Open-source Language Models with Mixed-Quality Data. (arXiv:2309.11235v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11235
&lt;/p&gt;
&lt;p&gt;
OpenChat&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#24182;&#31616;&#21270;RLFT&#26041;&#27861;&#30340;&#27714;&#35299;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20687;LLaMA&#36825;&#26679;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20986;&#29616;&#12290;&#26368;&#36817;&#30340;&#21457;&#23637;&#20013;&#20351;&#29992;&#20102;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#65288;RLFT&#65289;&#26469;&#20351;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;SFT&#26041;&#27861;&#23558;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#28151;&#21512;&#36136;&#37327;&#31561;&#21516;&#23545;&#24453;&#65292;&#32780;RLFT&#26041;&#27861;&#21017;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#25110;&#22522;&#20110;&#25490;&#21517;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;OpenChat&#65292;&#29992;&#20110;&#21033;&#29992;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#25512;&#36827;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#33324;&#30340;SFT&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#23569;&#37327;&#30340;&#19987;&#23478;&#25968;&#25454;&#21644;&#22823;&#37327;&#30340;&#27425;&#20248;&#25968;&#25454;&#65292;&#27809;&#26377;&#20219;&#20309;&#20248;&#20808;&#32423;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C(onditioned)-RLFT&#65292;&#23558;&#19981;&#21516;&#30340;&#25968;&#25454;&#28304;&#35270;&#20026;&#31895;&#31890;&#24230;&#30340;&#22870;&#21169;&#26631;&#31614;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#26465;&#20214;&#21270;&#31574;&#30053;&#65292;&#20197;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#36136;&#37327;&#20449;&#24687;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;C-RLFT&#20013;&#65292;&#26368;&#20248;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#21333;&#38454;&#27573;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#30417;&#30563;&#23398;&#20064;&#36731;&#26494;&#27714;&#35299;&#65292;&#20351;&#24471;&#35813;&#38382;&#39064;&#24471;&#21040;&#20102;&#31616;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, whic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM2Speech&#65292;&#19968;&#31181;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#36793;&#21512;&#25104;&#35821;&#38899;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#34255;&#23884;&#20837;&#26469;&#23454;&#29616;&#27969;&#24335;&#21512;&#25104;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#29983;&#25104;&#35821;&#38899;&#30340;&#24310;&#36831;&#65292;&#24182;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11210</link><description>&lt;p&gt;
&#36793;&#29983;&#25104;&#25991;&#26412;&#36793;&#21512;&#25104;&#35821;&#38899;&#65306;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#27969;&#24335;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Speak While You Think: Streaming Speech Synthesis During Text Generation. (arXiv:2309.11210v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM2Speech&#65292;&#19968;&#31181;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#36793;&#21512;&#25104;&#35821;&#38899;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#34255;&#23884;&#20837;&#26469;&#23454;&#29616;&#27969;&#24335;&#21512;&#25104;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#29983;&#25104;&#35821;&#38899;&#30340;&#24310;&#36831;&#65292;&#24182;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#19982;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#22823;&#22810;&#36890;&#36807;&#25991;&#26412;&#36827;&#34892;&#12290;&#20351;&#29992;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;Text-To-Speech&#65289;&#26469;&#21512;&#25104;LLM&#36755;&#20986;&#36890;&#24120;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#24310;&#36831;&#65292;&#36825;&#23545;&#20110;&#27969;&#21033;&#30340;&#35821;&#38899;&#23545;&#35805;&#26469;&#35828;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM2Speech&#65292;&#19968;&#31181;&#22312;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21516;&#26102;&#21512;&#25104;&#35821;&#38899;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24310;&#36831;&#12290;LLM2Speech&#27169;&#20223;&#38750;&#27969;&#24335;&#25945;&#24072;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#38480;&#21046;&#23545;&#26410;&#26469;&#19978;&#19979;&#25991;&#30340;&#26292;&#38706;&#65292;&#20197;&#23454;&#29616;&#27969;&#24335;&#21512;&#25104;&#12290;&#23427;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#36825;&#26159;&#25991;&#26412;&#29983;&#25104;&#30340;&#21103;&#20135;&#21697;&#65292;&#21253;&#21547;&#26377;&#20449;&#24687;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLM2Speech&#20445;&#25345;&#20102;&#25945;&#24072;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#24310;&#36831;&#65292;&#20351;&#33258;&#28982;&#23545;&#35805;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate impressive capabilities, yet interaction with these models is mostly facilitated through text. Using Text-To-Speech to synthesize LLM outputs typically results in notable latency, which is impractical for fluent voice conversations. We propose LLM2Speech, an architecture to synthesize speech while text is being generated by an LLM which yields significant latency reduction. LLM2Speech mimics the predictions of a non-streaming teacher model while limiting the exposure to future context in order to enable streaming. It exploits the hidden embeddings of the LLM, a by-product of the text generation that contains informative semantic context. Experimental results show that LLM2Speech maintains the teacher's quality while reducing the latency to enable natural conversations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#21270;KG&#30693;&#35782;&#20026;&#25991;&#26412;&#21270;&#38472;&#36848;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31572;&#26696;&#25935;&#24863;&#30340;KG-to-Text&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11206</link><description>&lt;p&gt;
&#25552;&#21462;-&#25913;&#20889;-&#22238;&#31572;&#65306;&#19968;&#31181;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering. (arXiv:2309.11206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11206
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#21270;KG&#30693;&#35782;&#20026;&#25991;&#26412;&#21270;&#38472;&#36848;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31572;&#26696;&#25935;&#24863;&#30340;KG-to-Text&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#22312;&#35760;&#24518;&#25152;&#26377;&#19990;&#30028;&#30693;&#35782;&#65292;&#23588;&#20854;&#26159;&#38271;&#23614;&#30693;&#35782;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38656;&#35201;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KGQA&#65289;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26816;&#32034;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20197;&#22686;&#24378;LLMs&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;KGQA&#20013;LLMs&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;&#32570;&#20047;&#22522;&#20110;&#25991;&#26412;&#30340;&#21512;&#29702;&#34920;&#36848;KG&#30693;&#35782;&#65292;&#21363;&#24573;&#30053;&#20102;KG&#34920;&#31034;&#21644;&#25991;&#26412;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31572;&#26696;&#25935;&#24863;&#30340;KG-to-Text&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;KG&#30693;&#35782;&#36716;&#21270;&#20026;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25991;&#26412;&#21270;&#38472;&#36848;&#65292;&#29992;&#20110;KGQA&#12290;&#22522;&#20110;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;KGQA&#20219;&#21153;&#30340;&#22686;&#24378;&#22411;KG-to-Text LLMS&#26694;&#26550;&#12290;&#22312;&#20960;&#20010;KGQA&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;KG-to-Text&#22686;&#24378;LLMs&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperfor
&lt;/p&gt;</description></item><item><title>Languini Kitchen&#26159;&#19968;&#20010;&#30740;&#31350;&#38598;&#20307;&#21644;&#20195;&#30721;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#31561;&#25928;&#35745;&#31639;&#26469;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#35201;&#28857;&#65306;&#23454;&#39564;&#21327;&#35758;&#12289;&#27169;&#22411;&#27604;&#36739;&#12289;&#31561;&#25928;&#35745;&#31639;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.11197</link><description>&lt;p&gt;
Languini Kitchen: &#22312;&#19981;&#21516;&#35745;&#31639;&#35268;&#27169;&#19978;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute. (arXiv:2309.11197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11197
&lt;/p&gt;
&lt;p&gt;
Languini Kitchen&#26159;&#19968;&#20010;&#30740;&#31350;&#38598;&#20307;&#21644;&#20195;&#30721;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#31561;&#25928;&#35745;&#31639;&#26469;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#35201;&#28857;&#65306;&#23454;&#39564;&#21327;&#35758;&#12289;&#27169;&#22411;&#27604;&#36739;&#12289;&#31561;&#25928;&#35745;&#31639;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Languini Kitchen&#26082;&#26159;&#19968;&#20010;&#30740;&#31350;&#38598;&#20307;&#65292;&#21448;&#26159;&#19968;&#20010;&#20195;&#30721;&#24211;&#65292;&#26088;&#22312;&#36171;&#20104;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#20570;&#20986;&#26377;&#24847;&#20041;&#36129;&#29486;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#39564;&#21327;&#35758;&#65292;&#20351;&#24471;&#21487;&#20197;&#22522;&#20110;&#31561;&#25928;&#35745;&#31639;&#65288;&#20197;&#21152;&#36895;&#22120;&#23567;&#26102;&#35745;&#37327;&#65289;&#26469;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;&#12290;&#27169;&#22411;&#35757;&#32451;&#30340;&#20196;&#29260;&#25968;&#37327;&#30001;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#21644;&#36873;&#25321;&#30340;&#35745;&#31639;&#31867;&#21035;&#26469;&#23450;&#20041;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#23545;&#24433;&#21709;&#24635;&#21442;&#25968;&#25110;&#28014;&#28857;&#25805;&#20316;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#39044;&#22788;&#29702;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#19988;&#39640;&#36136;&#37327;&#30340;&#22270;&#20070;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#25991;&#26723;&#38271;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#23398;&#26415;&#22522;&#20934;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#35745;&#31639;&#27700;&#24179;&#19978;&#30340;&#23454;&#39564;&#26469;&#20272;&#35745;&#26041;&#27861;&#30340;&#32463;&#39564;&#24615;&#25193;&#23637;&#36235;&#21183;&#12290;&#36825;&#39033;&#24037;&#20316;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#20934;&#27169;&#22411;&#65306;&#20174;GPT-2&#26550;&#26500;&#25512;&#23548;&#20986;&#30340;&#21069;&#39304;&#27169;&#22411;&#21450;...
&lt;/p&gt;
&lt;p&gt;
The Languini Kitchen serves as both a research collective and codebase designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modelling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;LLM&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2309.11166</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21333;&#35789;&#32423;&#25200;&#21160;&#30495;&#30340;&#20855;&#26377;&#40065;&#26834;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Really Robust to Word-Level Perturbations?. (arXiv:2309.11166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;LLM&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#27169;&#21644;&#33021;&#21147;&#19978;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#25104;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#38500;&#20102;&#36861;&#27714;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36991;&#20813;&#23545;&#29305;&#23450;&#25552;&#31034;&#30340;&#28608;&#28872;&#21453;&#39304;&#22806;&#65292;&#30830;&#20445;LLM&#30340;&#36131;&#20219;&#24615;&#36824;&#38656;&#35201;&#20851;&#27880;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#20855;&#26377;&#39044;&#23450;&#20041;&#30417;&#30563;&#26631;&#31614;&#30340;&#20256;&#32479;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36825;&#19982;&#24403;&#20195;LLMs&#30340;&#20986;&#33394;&#29983;&#25104;&#33021;&#21147;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#29702;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#26469;&#35780;&#20272;LLMs&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#21512;&#29702;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;TREvaL&#65289;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;TREval&#25552;&#20379;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;LLM&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38754;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24335;&#38382;&#39064;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
The swift advancement in the scale and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary LLMs. To address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the robustness of LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Our extensive empirical experiments have demonstrated that TREval provides an accurate method for evaluating the robustness of an LLM, especially when faced with more challenging open 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#21644;&#35821;&#27861;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#35789;&#21521;&#37327;&#23545;&#21477;&#27861;&#34920;&#31034;&#27809;&#26377;&#20559;&#22909;&#65292;&#32780;&#35821;&#35328;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20986;&#29616;&#39057;&#29575;&#27604;&#20219;&#21153;&#25968;&#25454;&#30340;&#25968;&#37327;&#26356;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.11165</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#21644;&#35821;&#27861;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessment of Pre-Trained Models Across Languages and Grammars. (arXiv:2309.11165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#21644;&#35821;&#27861;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#35789;&#21521;&#37327;&#23545;&#21477;&#27861;&#34920;&#31034;&#27809;&#26377;&#20559;&#22909;&#65292;&#32780;&#35821;&#35328;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20986;&#29616;&#39057;&#29575;&#27604;&#20219;&#21153;&#25968;&#25454;&#30340;&#25968;&#37327;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#24418;&#24335;&#30340;&#21477;&#27861;&#32467;&#26500;&#26041;&#38754;&#23398;&#20064;&#21477;&#27861;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35299;&#26512;&#35270;&#20026;&#24207;&#21015;&#26631;&#35760;&#26469;&#24674;&#22797;&#32452;&#25104;&#21644;&#20381;&#36182;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20960;&#20010;LLMs&#65292;&#24182;&#22312;13&#20010;&#19981;&#21516;&#30340;UD&#26641;&#24211;&#29992;&#20110;&#20381;&#36182;&#35299;&#26512;&#21644;10&#20010;&#26641;&#24211;&#29992;&#20110;&#32452;&#25104;&#35299;&#26512;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;i&#65289;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#32534;&#30721;&#20043;&#38388;&#20855;&#26377;&#19968;&#33268;&#24615;&#65292;&#65288;ii&#65289;&#39044;&#35757;&#32451;&#35789;&#21521;&#37327;&#19981;&#20559;&#22909;&#20381;&#36182;&#35821;&#27861;&#32780;&#38750;&#32452;&#25104;&#35821;&#27861;&#34920;&#31034;&#65292;&#65288;iii&#65289;&#23376;&#35789;&#26631;&#35760;&#21270;&#26159;&#34920;&#31034;&#21477;&#27861;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#19982;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#30456;&#21453;&#65292;&#65288;iv&#65289;&#20174;&#35789;&#21521;&#37327;&#20013;&#24674;&#22797;&#21477;&#27861;&#26102;&#65292;&#35821;&#35328;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20986;&#29616;&#39057;&#29575;&#27604;&#20219;&#21153;&#25968;&#25454;&#30340;&#25968;&#37327;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an approach for assessing how multilingual large language models (LLMs) learn syntax in terms of multi-formalism syntactic structures. We aim to recover constituent and dependency structures by casting parsing as sequence labeling. To do so, we select a few LLMs and study them on 13 diverse UD treebanks for dependency parsing and 10 treebanks for constituent parsing. Our results show that: (i) the framework is consistent across encodings, (ii) pre-trained word vectors do not favor constituency representations of syntax over dependencies, (iii) sub-word tokenization is needed to represent syntax, in contrast to character-based models, and (iv) occurrence of a language in the pretraining data is more important than the amount of task data when recovering syntax from the word vectors.
&lt;/p&gt;</description></item><item><title>CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11143</link><description>&lt;p&gt;
CoT-BERT: &#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11143
&lt;/p&gt;
&lt;p&gt;
CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#36755;&#20837;&#21477;&#23376;&#36716;&#21270;&#20026;&#23500;&#21547;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#30340;&#22266;&#23450;&#38271;&#24230;&#21521;&#37327;&#65292;&#21516;&#26102;&#28040;&#38500;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#25512;&#21160;&#19979;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#36712;&#36857;&#20013;&#65292;&#20173;&#28982;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#24605;&#32500;&#38142;&#26465;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#20026;&#20102;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20013;&#30340;&#28508;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21477;&#23376;&#34920;&#31034;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#29702;&#35299;&#21644;&#25688;&#35201;&#12290;&#38543;&#21518;&#65292;&#21518;&#19968;&#38454;&#27573;&#30340;&#36755;&#20986;&#34987;&#21033;&#29992;&#20026;&#36755;&#20837;&#21477;&#23376;&#30340;&#21521;&#37327;&#21270;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#21644;&#27169;&#26495;&#21435;&#22122;&#25216;&#26415;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#12290;&#20005;&#26684;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CoT-BERT&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#25991;&#26412;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#21407;&#22411;&#65292;&#21487;&#29992;&#20110;&#36741;&#21161;&#33521;&#35821;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#35813;&#31995;&#32479;&#20132;&#20114;&#30340;&#23398;&#20064;&#32773;&#30340;&#35821;&#27861;&#33539;&#22260;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.11142</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;DNN&#36890;&#36807;&#25991;&#26412;&#29983;&#25104;&#36741;&#21161;&#33521;&#35821;&#23398;&#20064;&#36807;&#31243;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#21407;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prototype of a robotic system to assist the learning process of English language with text-generation through DNN. (arXiv:2309.11142v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#25991;&#26412;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#21407;&#22411;&#65292;&#21487;&#29992;&#20110;&#36741;&#21161;&#33521;&#35821;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#35813;&#31995;&#32479;&#20132;&#20114;&#30340;&#23398;&#20064;&#32773;&#30340;&#35821;&#27861;&#33539;&#22260;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#33521;&#35821;&#25945;&#23398;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#26159;&#20351;&#29992;&#20132;&#20114;&#24335;&#35774;&#22791;&#26469;&#21560;&#24341;&#23398;&#20064;&#32773;&#21442;&#19982;&#33258;&#20027;&#23398;&#20064;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24037;&#20316;&#21407;&#22411;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#25991;&#26412;&#26469;&#24110;&#21161;&#23398;&#20064;&#33521;&#35821;&#30340;&#20154;&#12290;&#23398;&#20064;&#32773;&#36890;&#36807;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#35813;&#30028;&#38754;&#26681;&#25454;&#29992;&#25143;&#30340;&#33521;&#35821;&#27700;&#24179;&#29983;&#25104;&#25991;&#26412;&#12290;&#23454;&#39564;&#20351;&#29992;&#33521;&#35821;&#23398;&#20064;&#32773;&#36827;&#34892;&#65292;&#24182;&#26681;&#25454;&#22269;&#38469;&#33521;&#35821;&#35821;&#35328;&#27979;&#35797;&#31995;&#32479;&#65288;IELTS&#65289;&#35780;&#20998;&#26631;&#20934;&#36827;&#34892;&#27979;&#37327;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#35813;&#31995;&#32479;&#30340;&#23398;&#20064;&#32773;&#30340;&#35821;&#27861;&#33539;&#22260;&#26377;&#25152;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last ongoing years, there has been a significant ascending on the field of Natural Language Processing (NLP) for performing multiple tasks including English Language Teaching (ELT). An effective strategy to favor the learning process uses interactive devices to engage learners in their self-learning process. In this work, we present a working prototype of a humanoid robotic system to assist English language self-learners through text generation using Long Short Term Memory (LSTM) Neural Networks. The learners interact with the system using a Graphic User Interface that generates text according to the English level of the user. The experimentation was conducted using English learners and the results were measured accordingly to International English Language Testing System (IELTS) rubric. Preliminary results show an increment in the Grammatical Range of learners who interacted with the system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#32534;&#30721;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#21040;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#20013;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#28040;&#24687;&#36827;&#34892;&#36890;&#20449;&#30340;&#25928;&#29575;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#35821;&#20041;&#28304;&#32534;&#30721;&#12289;&#35821;&#20041;&#20449;&#36947;&#32534;&#30721;&#21644;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#31561;&#21019;&#26032;&#31639;&#27861;&#26469;&#25552;&#39640;&#36890;&#20449;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11127</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#32534;&#30721;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#36890;&#20449;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation. (arXiv:2309.11127v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#32534;&#30721;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#21040;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#20013;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#28040;&#24687;&#36827;&#34892;&#36890;&#20449;&#30340;&#25928;&#29575;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#35821;&#20041;&#28304;&#32534;&#30721;&#12289;&#35821;&#20041;&#20449;&#36947;&#32534;&#30721;&#21644;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#31561;&#21019;&#26032;&#31639;&#27861;&#26469;&#25552;&#39640;&#36890;&#20449;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#21040;&#26032;&#20852;&#30340;&#35821;&#20041;&#36890;&#20449;&#65288;SC&#65289;&#33539;&#24335;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#35821;&#20041;&#36890;&#20449;&#65288;LSC&#65289;&#26694;&#26550;&#12290;&#22312;LSC&#20013;&#65292;&#26426;&#22120;&#20351;&#29992;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#35299;&#37322;&#21644;&#25805;&#20316;&#30340;&#20154;&#31867;&#35821;&#35328;&#28040;&#24687;&#36827;&#34892;&#36890;&#20449;&#65292;&#20197;&#25552;&#39640;SC&#25928;&#29575;&#12290;&#20026;&#20102;&#23637;&#31034;LSC&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#21019;&#26032;&#31639;&#27861;&#65306;1&#65289;&#35821;&#20041;&#28304;&#32534;&#30721;&#65288;SSC&#65289;&#65292;&#23558;&#25991;&#26412;&#25552;&#31034;&#21387;&#32553;&#25104;&#25429;&#25417;&#25552;&#31034;&#30340;&#21477;&#27861;&#26412;&#36136;&#30340;&#20851;&#38190;&#22836;&#35789;&#65292;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#20986;&#29616;&#39034;&#24207;&#20197;&#20445;&#25345;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#65307;2&#65289;&#35821;&#20041;&#20449;&#36947;&#32534;&#30721;&#65288;SCC&#65289;&#65292;&#36890;&#36807;&#29992;&#26356;&#38271;&#30340;&#21516;&#20041;&#35789;&#26367;&#25442;&#22836;&#35789;&#26469;&#25552;&#39640;&#23545;&#38169;&#35823;&#30340;&#23481;&#38169;&#24615;&#65307;3&#65289;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;SKD&#65289;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#21548;&#20247;&#30340;&#35821;&#35328;&#39118;&#26684;&#26469;&#29983;&#25104;&#23450;&#21046;&#30340;&#25552;&#31034;&#12290;&#22312;&#36880;&#27493;&#36827;&#34892;&#30340;&#36890;&#20449;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;LSC&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
By integrating recent advances in large language models (LLMs) and generative models into the emerging semantic communication (SC) paradigm, in this article we put forward to a novel framework of language-oriented semantic communication (LSC). In LSC, machines communicate using human language messages that can be interpreted and manipulated via natural language processing (NLP) techniques for SC efficiency. To demonstrate LSC's potential, we introduce three innovative algorithms: 1) semantic source coding (SSC) which compresses a text prompt into its key head words capturing the prompt's syntactic essence while maintaining their appearance order to keep the prompt's context; 2) semantic channel coding (SCC) that improves robustness against errors by substituting head words with their lenghthier synonyms; and 3) semantic knowledge distillation (SKD) that produces listener-customized prompts via in-context learning the listener's language style. In a communication task for progressive te
&lt;/p&gt;</description></item><item><title>AttentionMix&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#12290;&#22312;&#19977;&#20010;&#26631;&#20934;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AttentionMix&#22312;NLP&#39046;&#22495;&#30340;&#25968;&#25454;&#22686;&#24378;&#20013;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;Mixup&#26426;&#21046;&#30340;&#20004;&#31181;&#22522;&#20934;&#26041;&#27861;&#21644;&#26222;&#36890;BERT&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11104</link><description>&lt;p&gt;
AttentionMix: &#19968;&#31181;&#22522;&#20110;BERT&#27880;&#24847;&#26426;&#21046;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AttentionMix: Data augmentation method that relies on BERT attention mechanism. (arXiv:2309.11104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11104
&lt;/p&gt;
&lt;p&gt;
AttentionMix&#26159;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#12290;&#22312;&#19977;&#20010;&#26631;&#20934;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AttentionMix&#22312;NLP&#39046;&#22495;&#30340;&#25968;&#25454;&#22686;&#24378;&#20013;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;Mixup&#26426;&#21046;&#30340;&#20004;&#31181;&#22522;&#20934;&#26041;&#27861;&#21644;&#26222;&#36890;BERT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#19988;&#26377;&#35768;&#22810;&#20197;&#24341;&#23548;&#26041;&#24335;&#25191;&#34892;&#22270;&#20687;&#28151;&#21512;&#30340;&#21518;&#32487;&#26041;&#27861;&#12290;&#23558;Mixup&#24605;&#24819;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23613;&#31649;&#24050;&#32463;&#23384;&#22312;&#19968;&#20123;&#23558;Mixup&#24212;&#29992;&#20110;&#25991;&#26412;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AttentionMix&#65292;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#34429;&#28982;&#26412;&#25991;&#20851;&#27880;BERT&#27880;&#24847;&#26426;&#21046;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#12290;AttentionMix&#22312;3&#20010;&#26631;&#20934;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#25152;&#26377;&#19977;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#21033;&#29992;Mixup&#26426;&#21046;&#21644;&#26222;&#36890;BERT&#26041;&#27861;&#30340;&#20004;&#31181;&#22522;&#20934;&#26041;&#27861;&#12290;&#32467;&#26524;&#35777;&#23454;&#65292;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;NLP&#39046;&#22495;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mixup method has proven to be a powerful data augmentation technique in Computer Vision, with many successors that perform image mixing in a guided manner. One of the interesting research directions is transferring the underlying Mixup idea to other domains, e.g. Natural Language Processing (NLP). Even though there already exist several methods that apply Mixup to textual data, there is still room for new, improved approaches. In this work, we introduce AttentionMix, a novel mixing method that relies on attention-based information. While the paper focuses on the BERT attention mechanism, the proposed approach can be applied to generally any attention-based model. AttentionMix is evaluated on 3 standard sentiment classification datasets and in all three cases outperforms two benchmark approaches that utilize Mixup mechanism, as well as the vanilla BERT method. The results confirm that the attention-based information can be effectively used for data augmentation in the NLP domain.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11093</link><description>&lt;p&gt;
K-pop&#27468;&#35789;&#32763;&#35793;&#65306;&#25968;&#25454;&#38598;&#12289;&#20998;&#26512;&#19982;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11093
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#35789;&#32763;&#35793;&#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#20102;&#19968;&#20010;&#19990;&#32426;&#30340;&#39046;&#22495;&#65292;&#22914;&#20170;&#21560;&#24341;&#30528;&#35745;&#31639;&#35821;&#35328;&#23398;&#30740;&#31350;&#32773;&#30340;&#27880;&#24847;&#12290;&#25105;&#20204;&#22312;&#20197;&#24448;&#30740;&#31350;&#20013;&#21457;&#29616;&#20102;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22312;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#23613;&#31649;K-pop&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#35199;&#26041;&#27969;&#27966;&#21644;&#35821;&#35328;&#65292;&#27809;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;K-pop&#19978;&#12290;&#20854;&#27425;&#65292;&#27468;&#35789;&#32763;&#35793;&#39046;&#22495;&#32570;&#20047;&#21487;&#20844;&#24320;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65307;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26080;&#27492;&#31867;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25299;&#23485;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#30340;&#27969;&#27966;&#21644;&#35821;&#35328;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21809;&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#32422;89%&#20026;K-pop&#27468;&#35789;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#36880;&#34892;&#21644;&#36880;&#33410;&#23545;&#40784;&#20102;&#38889;&#35821;&#21644;&#33521;&#35821;&#27468;&#35789;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#19982;&#20854;&#20182;&#24191;&#27867;&#30740;&#31350;&#30340;&#27969;&#27966;&#21306;&#20998;&#24320;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34913;&#37327;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#21644;&#25366;&#25496;&#38590;&#36127;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11082</link><description>&lt;p&gt;
&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#19982;&#19977;&#20803;&#37096;&#20998;&#36793;&#38469;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning. (arXiv:2309.11082v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34913;&#37327;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#21644;&#25366;&#25496;&#38590;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32593;&#32476;&#35270;&#39057;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#20351;&#24471;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#23545;&#20110;&#35270;&#39057;&#36807;&#28388;&#12289;&#25512;&#33616;&#21644;&#25628;&#32034;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#21644;&#27969;&#34892;&#12290;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26088;&#22312;&#23558;&#30456;&#20851;&#30340;&#25991;&#26412;/&#35270;&#39057;&#25490;&#22312;&#19981;&#30456;&#20851;&#30340;&#25991;&#26412;/&#35270;&#39057;&#20043;&#21069;&#12290;&#35813;&#20219;&#21153;&#30340;&#26680;&#24515;&#26159;&#20934;&#30830;&#34913;&#37327;&#25991;&#26412;&#21644;&#35270;&#39057;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#12290;&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#20391;&#37325;&#20110;&#26500;&#24314;&#27491;&#36127;&#26679;&#26412;&#23545;&#20197;&#23398;&#20064;&#25991;&#26412;&#21644;&#35270;&#39057;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#20851;&#27880;&#38590;&#36127;&#26679;&#26412;&#21644;&#27169;&#25311;&#19981;&#21516;&#23618;&#27425;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#19981;&#22815;&#65292;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20351;&#29992;&#20004;&#20010;&#26032;&#26041;&#27861;&#25913;&#36827;&#20102;&#23545;&#27604;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#21033;&#29992;&#33392;&#38590;&#30340;&#20363;&#23376;&#26469;&#25552;&#39640;&#40065;&#26834;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#27169;&#22359;(DMAE)&#65292;&#20174;&#25991;&#26412;&#21644;&#35270;&#35273;&#32447;&#32034;&#20013;&#25366;&#25496;&#38590;&#36127;&#26679;&#26412;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#24341;&#20837;&#19968;&#20010;&#36127;&#38754;&#26679;&#26412;&#31579;&#36873;&#26426;&#21046;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24314;&#27169;&#19981;&#21516;&#32423;&#21035;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the explosion of web videos makes text-video retrieval increasingly essential and popular for video filtering, recommendation, and search. Text-video retrieval aims to rank relevant text/video higher than irrelevant ones. The core of this task is to precisely measure the cross-modal similarity between texts and videos. Recently, contrastive learning methods have shown promising results for text-video retrieval, most of which focus on the construction of positive and negative pairs to learn text and video representations. Nevertheless, they do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity. To address these two issues, this paper improves contrastive learning using two novel techniques. First, to exploit hard examples for robust discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs from textual and visual clues. By further introducing a Negat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#24863;&#30693;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#26469;&#25193;&#23637;&#39044;&#35757;&#32451;&#23545;&#35805;&#27169;&#22411;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#36890;&#29992;&#39044;&#35757;&#32451;&#23545;&#35805;&#27169;&#22411;&#65288;UniPCM&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UniPCM&#20855;&#26377;&#40065;&#26834;&#24615;&#12289;&#24378;&#22823;&#30340;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#23545;&#35805;&#20219;&#21153;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11065</link><description>&lt;p&gt;
UniPCM: &#20855;&#26377;&#20219;&#21153;&#24863;&#30693;&#33258;&#21160;&#25552;&#31034;&#30340;&#36890;&#29992;&#39044;&#35757;&#32451;&#23545;&#35805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt. (arXiv:2309.11065v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#24863;&#30693;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#26469;&#25193;&#23637;&#39044;&#35757;&#32451;&#23545;&#35805;&#27169;&#22411;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#36890;&#29992;&#39044;&#35757;&#32451;&#23545;&#35805;&#27169;&#22411;&#65288;UniPCM&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UniPCM&#20855;&#26377;&#40065;&#26834;&#24615;&#12289;&#24378;&#22823;&#30340;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#23545;&#35805;&#20219;&#21153;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#36801;&#31227;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#20851;&#20110;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#30340;&#24037;&#20316;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#20026;&#23450;&#20041;&#30340;&#36755;&#20837;&#26684;&#24335;&#25110;&#25552;&#31034;&#65292;&#36825;&#22312;&#36136;&#37327;&#21644;&#25968;&#37327;&#19978;&#37117;&#19981;&#26159;&#26368;&#20339;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#20219;&#21153;&#30340;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#65288;TAP&#65289;&#26469;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#12290;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#23545;&#35805;&#27169;&#22411;&#30340;&#35821;&#26009;&#24211;&#25193;&#23637;&#21040;&#20102;&#26469;&#33258;15&#20010;&#19982;&#23545;&#35805;&#30456;&#20851;&#20219;&#21153;&#30340;122&#20010;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#36890;&#29992;&#39044;&#35757;&#32451;&#23545;&#35805;&#27169;&#22411;&#65288;UniPCM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#20110;&#21508;&#31181;&#23545;&#35805;&#20219;&#21153;&#21644;&#19981;&#21516;&#23545;&#35805;&#31995;&#32479;&#20855;&#26377;&#24378;&#22823;&#22522;&#30784;&#30340;&#27169;&#22411;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;UniPCM&#23545;&#36755;&#20837;&#25552;&#31034;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#23436;&#25104;&#21508;&#31181;&#23545;&#35805;&#30456;&#20851;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;UniPCM&#20855;&#26377;&#24456;&#24378;&#30340;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;9&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that multi-task pre-training greatly improves the model's robustness and transfer ability, which is crucial for building a high-quality dialog system. However, most previous works on multi-task pre-training rely heavily on human-defined input format or prompt, which is not optimal in quality and quantity. In this work, we propose to use Task-based Automatic Prompt generation (TAP) to automatically generate high-quality prompts. Using the high-quality prompts generated, we scale the corpus of the pre-trained conversation model to 122 datasets from 15 dialog-related tasks, resulting in Universal Pre-trained Conversation Model (UniPCM), a powerful foundation model for various conversational tasks and different dialog systems. Extensive experiments have shown that UniPCM is robust to input prompts and capable of various dialog-related tasks. Moreover, UniPCM has strong transfer ability and excels at low resource scenarios, achieving SOTA results on 9 different dat
&lt;/p&gt;</description></item><item><title>XATU&#26159;&#31532;&#19968;&#20010;&#32454;&#31890;&#24230;&#22522;&#20110;&#25351;&#20196;&#30340;&#21487;&#35299;&#37322;&#24615;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#24191;&#27867;&#30340;&#32534;&#36753;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#25351;&#20196;&#21644;&#40644;&#37329;&#26631;&#20934;&#32534;&#36753;&#35828;&#26126;&#26469;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11063</link><description>&lt;p&gt;
XATU: &#38754;&#21521;&#21487;&#35299;&#37322;&#24615;&#25991;&#26412;&#26356;&#26032;&#30340;&#32454;&#31890;&#24230;&#22522;&#20110;&#25351;&#20196;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates. (arXiv:2309.11063v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11063
&lt;/p&gt;
&lt;p&gt;
XATU&#26159;&#31532;&#19968;&#20010;&#32454;&#31890;&#24230;&#22522;&#20110;&#25351;&#20196;&#30340;&#21487;&#35299;&#37322;&#24615;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#24191;&#27867;&#30340;&#32534;&#36753;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#25351;&#20196;&#21644;&#40644;&#37329;&#26631;&#20934;&#32534;&#36753;&#35828;&#26126;&#26469;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32534;&#36753;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#20462;&#25913;&#25991;&#26412;&#20197;&#26356;&#22909;&#22320;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#25552;&#20379;&#31895;&#31890;&#24230;&#25351;&#20196;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#32534;&#36753;&#21518;&#30340;&#36755;&#20986;&#20284;&#20046;&#21512;&#29702;&#65292;&#20294;&#24448;&#24448;&#20559;&#31163;&#20102;&#40644;&#37329;&#21442;&#32771;&#20013;&#21015;&#20986;&#30340;&#39044;&#26399;&#26356;&#25913;&#65292;&#23548;&#33268;&#35780;&#20272;&#20998;&#25968;&#36739;&#20302;&#12290;&#20026;&#20102;&#20840;&#38754;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#32534;&#36753;&#33021;&#21147;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;XATU&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#32454;&#31890;&#24230;&#22522;&#20110;&#25351;&#20196;&#30340;&#21487;&#35299;&#37322;&#24615;&#25991;&#26412;&#32534;&#36753;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;XATU&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20027;&#39064;&#21644;&#25991;&#26412;&#31867;&#22411;&#65292;&#21253;&#25324;&#35789;&#27719;&#12289;&#21477;&#27861;&#12289;&#35821;&#20041;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#30340;&#32534;&#36753;&#12290;&#20026;&#20102;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#28304;&#21644;&#20154;&#24037;&#27880;&#37322;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;&#32454;&#31890;&#24230;&#25351;&#20196;&#21644;&#40644;&#37329;&#26631;&#20934;&#32534;&#36753;&#35828;&#26126;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#35780;&#20272;&#29616;&#26377;&#30340;&#24320;&#25918;&#21644;&#23553;&#38381;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
Text editing is a crucial task that involves modifying text to better align with user intents. However, existing text editing benchmark datasets have limitations in providing only coarse-grained instructions. Consequently, although the edited output may seem reasonable, it often deviates from the intended changes outlined in the gold reference, resulting in low evaluation scores. To comprehensively investigate the text editing capabilities of large language models, this paper introduces XATU, the first benchmark specifically designed for fine-grained instruction-based explainable text editing. XATU covers a wide range of topics and text types, incorporating lexical, syntactic, semantic, and knowledge-intensive edits. To enhance interpretability, we leverage high-quality data sources and human annotation, resulting in a benchmark that includes fine-grained instructions and gold-standard edit explanations. By evaluating existing open and closed large language models against our benchmark
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#24605;&#36335;&#38142;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#36890;&#24120;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#26356;&#21152;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#20855;&#26377;&#26356;&#22823;&#22810;&#26679;&#24615;&#19988;&#24615;&#33021;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#36739;&#22909;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#24605;&#36335;&#38142;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.11054</link><description>&lt;p&gt;
&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#24605;&#36335;&#38142;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Design of Chain-of-Thought in Math Problem Solving. (arXiv:2309.11054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#24605;&#36335;&#38142;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#36890;&#24120;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#26356;&#21152;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#20855;&#26377;&#26356;&#22823;&#22810;&#26679;&#24615;&#19988;&#24615;&#33021;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#36739;&#22909;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#24605;&#36335;&#38142;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#36335;&#38142;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#23545;&#35774;&#35745;&#24605;&#36335;&#38142;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32771;&#23519;&#65292;&#27604;&#36739;&#20102;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#21508;&#31181;&#31243;&#24207;&#24605;&#36335;&#38142;&#65292;&#21253;&#25324;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#12289;&#27880;&#37322;&#25551;&#36848;&#31243;&#24207;&#21644;&#38750;&#25551;&#36848;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#32534;&#31243;&#35821;&#35328;&#23545;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#24433;&#21709;&#65292;&#27604;&#36739;&#20102;Python&#21644;Wolfram&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;GSM8K&#12289;MATHQA&#21644;SVAMP&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#36890;&#24120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20855;&#26377;30B&#21442;&#25968;&#30340;&#26368;&#20339;&#32452;&#21512;&#26126;&#26174;&#36229;&#36807;&#20102;GPT-3.5-turbo&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#22810;&#26679;&#24615;&#65292;&#22240;&#27492;&#36890;&#24120;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#26356;&#22909;&#36873;&#25321;&#27604;Wolfram&#35821;&#35328;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#32771;&#34385;&#22240;&#32032;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solving. We conduct a comprehensive examination of methods for designing CoT, comparing conventional natural language CoT with various program CoTs, including the self-describing program, the comment-describing program, and the non-describing program. Furthermore, we investigate the impact of programming language on program CoTs, comparing Python and Wolfram Language. Through extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs often have superior effectiveness in math problem solving. Notably, the best performing combination with 30B parameters beats GPT-3.5-turbo by a significant margin. The results show that self-describing program offers greater diversity and thus can generally achieve higher performance. We also find that Python is a better choice of language than Wolfram for program CoTs. The experimental results provide a valuable guideline for future CoT designs that take into acco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#19978;&#30340;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#65292;&#22312;&#36873;&#21462;&#30456;&#20851;&#30340;&#34920;&#26684;&#21333;&#20803;&#12289;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#12289;&#20197;&#21450;&#25512;&#29702;&#38598;&#25104;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.11049</link><description>&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#34920;&#26684;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#65306;&#23450;&#20301;&#12289;&#26816;&#32034;&#21644;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables. (arXiv:2309.11049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#19978;&#30340;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#65292;&#22312;&#36873;&#21462;&#30456;&#20851;&#30340;&#34920;&#26684;&#21333;&#20803;&#12289;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#12289;&#20197;&#21450;&#25512;&#29702;&#38598;&#25104;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#38382;&#31572;&#65288;TableQA&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20542;&#21521;&#20110;&#20174;&#19968;&#20010;&#25110;&#20960;&#20010;&#34920;&#26684;&#21333;&#20803;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#29983;&#25104;&#20107;&#23454;&#24615;&#30340;&#31616;&#30701;&#31572;&#26696;&#65292;&#32780;&#32570;&#20047;&#23545;&#36873;&#23450;&#34920;&#26684;&#21333;&#20803;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#33258;&#30001;&#24418;&#24335;&#30340;TableQA&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#30456;&#20851;&#34920;&#26684;&#21333;&#20803;&#36873;&#25321;&#31574;&#30053;&#21644;&#29420;&#31435;&#20449;&#24687;&#30340;&#22797;&#26434;&#38598;&#25104;&#21644;&#25512;&#29702;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#36824;&#19981;&#20805;&#20998;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#65306;&#34920;&#26684;&#21040;&#22270;&#30340;&#36716;&#25442;&#21644;&#21333;&#20803;&#23450;&#20301;&#12289;&#22806;&#37096;&#30693;&#35782;&#26816;&#32034;&#21644;&#34920;&#26684;-&#25991;&#26412;&#34701;&#21512;&#65288;&#31216;&#20026;TAG-QA&#65289;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#24335;TableQA&#20013;&#38024;&#23545;&#38271;&#33258;&#30001;&#24418;&#24335;&#31572;&#26696;&#30340;&#25512;&#29702;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TAG-QA (1) &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23450;&#20301;&#30456;&#20851;&#34920;&#26684;&#21333;&#20803;&#65292;&#20197;&#25910;&#38598;&#30456;&#20851;&#34892;&#21644;&#21015;&#20043;&#38388;&#30340;&#20132;&#21449;&#21333;&#20803;&#65307;(2) &#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#30340;&#22806;&#37096;&#30693;&#35782;&#65307;(3)...
&lt;/p&gt;
&lt;p&gt;
Question answering on tabular data (TableQA), which aims at generating answers to questions grounded on a given table, has attracted increasing attention in recent years. Existing work tends to generate factual short-form answers by extracting information from one or a few table cells without reasoning over selected table cells. However, the free-form TableQA, requiring a more complex relevant table cell selection strategy and the complex integration and inference of separate pieces of information, has been under-explored. To this end, this paper proposes a generalized three-stage approach: Table-to-Graph conversion and cell localizing, external knowledge retrieval and table-text fusion (called TAG-QA), addressing the challenge of inferring long free-form answer for generative TableQA. In particular, TAG-QA (1) locates relevant table cells using a graph neural network to gather intersecting cells between relevant rows and columns; (2) leverages external knowledge from Wikipedia and (3)
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;BERT&#21644;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20307;&#21305;&#37197;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#26684;&#24335;&#21644;&#22797;&#26434;&#23646;&#24615;&#20851;&#32852;&#30340;&#24322;&#26500;&#23454;&#20307;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.11046</link><description>&lt;p&gt;
&#20351;&#29992;BERT&#21644;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22797;&#26434;&#23646;&#24615;&#20851;&#32852;&#30340;&#24322;&#26500;&#23454;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Entity Matching with Complex Attribute Associations using BERT and Neural Networks. (arXiv:2309.11046v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11046
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;BERT&#21644;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20307;&#21305;&#37197;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#26684;&#24335;&#21644;&#22797;&#26434;&#23646;&#24615;&#20851;&#32852;&#30340;&#24322;&#26500;&#23454;&#20307;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#26469;&#33258;&#30334;&#24230;&#30334;&#31185;&#21644;&#32500;&#22522;&#30334;&#31185;&#31561;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#20986;&#19981;&#21516;&#30340;&#24418;&#24335;&#12290;&#30446;&#21069;&#30340;&#23454;&#20307;&#21305;&#37197;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21516;&#36136;&#21270;&#25968;&#25454;&#19978;&#65292;&#20854;&#29305;&#28857;&#26159;&#23646;&#24615;&#20855;&#26377;&#30456;&#21516;&#30340;&#32467;&#26500;&#21644;&#31616;&#27905;&#30340;&#23646;&#24615;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21462;&#21521;&#22312;&#22788;&#29702;&#20855;&#26377;&#22810;&#26679;&#21270;&#26684;&#24335;&#30340;&#25968;&#25454;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#32858;&#21512;&#30456;&#24212;&#23646;&#24615;&#20043;&#38388;&#30340;&#23646;&#24615;&#20540;&#30456;&#20284;&#24230;&#26469;&#30830;&#23450;&#23454;&#20307;&#30456;&#20284;&#24230;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#23646;&#24615;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#20854;&#20013;&#19968;&#20010;&#23646;&#24615;&#21487;&#33021;&#20855;&#26377;&#22810;&#31181;&#20851;&#32852;&#12290;&#31616;&#21333;&#30340;&#36880;&#23545;&#23646;&#24615;&#27604;&#36739;&#26041;&#27861;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#20013;&#25152;&#34164;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20307;&#21305;&#37197;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#20110;BERT&#21644;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#22797;&#26434;&#23646;&#24615;&#20851;&#31995;&#30340;&#23454;&#20307;&#21305;&#37197;&#27169;&#22411;&#65288;EMM-CCAR&#65289;&#65292;&#23427;&#26500;&#24314;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Across various domains, data from different sources such as Baidu Baike and Wikipedia often manifest in distinct forms. Current entity matching methodologies predominantly focus on homogeneous data, characterized by attributes that share the same structure and concise attribute values. However, this orientation poses challenges in handling data with diverse formats. Moreover, prevailing approaches aggregate the similarity of attribute values between corresponding attributes to ascertain entity similarity. Yet, they often overlook the intricate interrelationships between attributes, where one attribute may have multiple associations. The simplistic approach of pairwise attribute comparison fails to harness the wealth of information encapsulated within entities.To address these challenges, we introduce a novel entity matching model, dubbed Entity Matching Model for Capturing Complex Attribute Relationships(EMM-CCAR),built upon pre-trained models. Specifically, this model transforms the m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#28151;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#26500;&#24314;&#28151;&#21512;&#20219;&#21153;&#36866;&#37197;&#22120;&#65292;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;NLP&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#20248;&#21270;&#36866;&#37197;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25903;&#25345;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.11042</link><description>&lt;p&gt;
&#20351;&#29992;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#28151;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters. (arXiv:2309.11042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#28151;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#26500;&#24314;&#28151;&#21512;&#20219;&#21153;&#36866;&#37197;&#22120;&#65292;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;NLP&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#20248;&#21270;&#36866;&#37197;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25903;&#25345;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#23610;&#23544;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#35757;&#32451;&#21644;&#22312;&#32447;&#37096;&#32626;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ALTER&#65292;&#19968;&#20010;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21442;&#25968;&lt;1B&#65289;&#19978;&#26377;&#25928;&#26500;&#24314;&#28151;&#21512;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#31995;&#32479;&#65292;&#20197;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;NLP&#20219;&#21153;&#65292;&#24182;&#25429;&#25417;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#21516;&#28857;&#21644;&#24046;&#24322;&#65292;&#20197;&#25903;&#25345;&#29305;&#23450;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;ALTER&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20219;&#21153;&#36866;&#37197;&#22120;&#28151;&#21512;&#65288;MTA&#65289;&#27169;&#22359;&#65292;&#20316;&#20026;&#24213;&#23618;&#27169;&#22411;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#25429;&#25417;&#20219;&#21153;&#20869;&#37096;&#30340;&#30693;&#35782;&#21644;&#20219;&#21153;&#38388;&#30340;&#30693;&#35782;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#22312;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#20248;&#21270;&#36866;&#37197;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#22312;&#28151;&#21512;NLP&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#19978;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have achieved amazing zero-shot learning performance over a variety of Natural Language Processing (NLP) tasks, especially for text generative tasks. Yet, the large size of LLMs often leads to the high computational cost of model training and online deployment. In our work, we present ALTER, a system that effectively builds the multi-tAsk Learners with mixTure-of-task-adaptERs upon small language models (with &lt;1B parameters) to address multiple NLP tasks simultaneously, capturing the commonalities and differences between tasks, in order to support domain-specific applications. Specifically, in ALTER, we propose the Mixture-of-Task-Adapters (MTA) module as an extension to the transformer architecture for the underlying model to capture the intra-task and inter-task knowledge. A two-stage training method is further proposed to optimize the collaboration between adapters at a small computational cost. Experimental results over a mixture of NLP tasks 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#22522;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#65292;&#36890;&#36807;&#25429;&#33719;&#26631;&#31614;&#20381;&#36182;&#20851;&#31995;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.11027</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#23454;&#29616;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach. (arXiv:2309.11027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11027
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#22522;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#65292;&#36890;&#36807;&#25429;&#33719;&#26631;&#31614;&#20381;&#36182;&#20851;&#31995;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21644;&#20998;&#31867;&#23454;&#20307;&#25552;&#21450;&#65292;&#24182;&#23558;&#20854;&#24402;&#31867;&#20026;&#39044;&#23450;&#20041;&#30340;&#31867;&#22411;(&#20363;&#22914;&#65292;&#32452;&#32455;&#25110;&#20010;&#20154;&#21517;&#31216;)&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#24037;&#20316;&#23558;NER&#22609;&#36896;&#20026;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;(&#20063;&#31216;&#20026;&#22522;&#20110;MRC&#30340;NER)&#65292;&#20854;&#20013;&#36890;&#36807;MRC&#22238;&#31572;&#19982;&#39044;&#23450;&#20041;&#23454;&#20307;&#31867;&#22411;&#30456;&#20851;&#30340;&#38382;&#39064;&#26469;&#23454;&#29616;&#23454;&#20307;&#35782;&#21035;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#24573;&#35270;&#20102;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#26631;&#31614;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#26631;&#31614;&#20381;&#36182;&#20851;&#31995;&#32435;&#20837;&#21040;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#22522;&#20110;MRC&#30340;NER&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;MRC&#30340;NER&#20998;&#35299;&#20026;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#25429;&#33719;&#26631;&#31614;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#23884;&#22871;NER&#21644;&#25153;&#24179;NER&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#22810;&#20219;&#21153;NER&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#20219;&#21153;NER&#33021;&#22815;&#36798;&#21040;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) aims to extract and classify entity mentions in the text into pre-defined types (e.g., organization or person name). Recently, many works have been proposed to shape the NER as a machine reading comprehension problem (also termed MRC-based NER), in which entity recognition is achieved by answering the formulated questions related to pre-defined entity types through MRC, based on the contexts. However, these works ignore the label dependencies among entity types, which are critical for precisely recognizing named entities. In this paper, we propose to incorporate the label dependencies among entity types into a multi-task learning framework for better MRC-based NER. We decompose MRC-based NER into multiple tasks and use a self-attention module to capture label dependencies. Comprehensive experiments on both nested NER and flat NER datasets are conducted to validate the effectiveness of the proposed Multi-NER. Experimental results show that Multi-NER can ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21516;&#26102;&#23545;&#23545;&#35805;&#22238;&#24212;&#21644;&#35821;&#35328;&#29305;&#24449;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#65292;&#25506;&#32034;&#20102;&#26500;&#24314;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.11000</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#22238;&#24212;&#21644;&#35821;&#38899;&#21512;&#25104;&#30340;&#32852;&#21512;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large Language Model. (arXiv:2309.11000v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21516;&#26102;&#23545;&#23545;&#35805;&#22238;&#24212;&#21644;&#35821;&#35328;&#29305;&#24449;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#65292;&#25506;&#32034;&#20102;&#26500;&#24314;&#32479;&#19968;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#26500;&#24314;&#19968;&#20010;&#21516;&#26102;&#8220;&#24605;&#32771;&#22914;&#20309;&#22238;&#31572;&#8221;&#21644;&#8220;&#24605;&#32771;&#22914;&#20309;&#35828;&#35805;&#8221;&#30340;AI&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#36825;&#19982;&#24403;&#21069;&#29420;&#31435;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22359;&#30340;&#32423;&#32852;&#27969;&#31243;&#30456;&#27604;&#26356;&#36148;&#36817;&#20154;&#31867;&#35821;&#38899;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#20551;&#35774;&#20855;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#37325;&#35201;&#30340;&#35821;&#38899;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#23545;&#23545;&#35805;&#22238;&#24212;&#21644;&#35821;&#35328;&#29305;&#24449;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#32452;&#23454;&#39564;&#65306;1&#65289;&#38901;&#24459;&#32467;&#26500;&#39044;&#27979;&#65292;&#36825;&#26159;TTS&#20013;&#30340;&#20856;&#22411;&#21069;&#31471;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#35821;&#38899;&#29702;&#35299;&#33021;&#21147;&#65307;2&#65289;&#36827;&#19968;&#27493;&#25972;&#21512;&#23545;&#35805;&#22238;&#24212;&#21644;&#21508;&#31181;&#35821;&#35328;&#29305;&#24449;&#65292;&#20351;&#29992;&#32479;&#19968;&#30340;&#32534;&#30721;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#23545;&#20110;&#26500;&#24314;&#32479;&#19968;&#30340;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#26159;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the potential of constructing an AI spoken dialogue system that "thinks how to respond" and "thinks how to speak" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules. We hypothesize that Large Language Models (LLMs) with billions of parameters possess significant speech understanding capabilities and can jointly model dialogue responses and linguistic features. We conduct two sets of experiments: 1) Prosodic structure prediction, a typical front-end task in TTS, demonstrating the speech understanding ability of LLMs, and 2) Further integrating dialogue response and a wide array of linguistic features using a unified encoding format. Our results indicate that the LLM-based approach is a promising direction for building unified spoken dialogue systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#26102;&#30340;&#36136;&#37327;&#25552;&#21319;&#33976;&#39311;&#21040;&#22522;&#20934;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10966</link><description>&lt;p&gt;
MBR&#21644;QE&#24494;&#35843;&#65306;&#23545;&#26368;&#20339;&#21644;&#26368;&#26114;&#36149;&#30340;&#35299;&#30721;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#26102;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods. (arXiv:2309.10966v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#26102;&#30340;&#36136;&#37327;&#25552;&#21319;&#33976;&#39311;&#21040;&#22522;&#20934;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#30340;&#35299;&#30721;&#26041;&#27861;&#30740;&#31350;&#20013;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#27874;&#26463;&#25628;&#32034;&#21644;&#36138;&#23146;&#35299;&#30721;&#31639;&#27861;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#27169;&#22411;&#27010;&#29575;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#27169;&#22411;&#22256;&#24785;&#24230;&#19982;&#36136;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26356;&#24378;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#21253;&#25324;&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#37325;&#25490;&#24207;&#21644;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#12290;&#23613;&#31649;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MBR&#24494;&#35843;&#21644;QE&#24494;&#35843;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#33976;&#39311;&#20102;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#30340;&#36136;&#37327;&#25552;&#21319;&#65292;&#22312;&#25512;&#26029;&#26102;&#20351;&#29992;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;NLG&#20219;&#21153;&#65292;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#36827;&#34892;&#33258;&#35757;&#32451;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#30340;&#24615;&#33021;&#20173;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#22806;&#37096;LLM&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#24494;&#35843;&#26041;&#27861;&#20063;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that the traditional beam search and greedy decoding algorithms are not optimal, because model probabilities do not always align with human preferences. Stronger decoding methods, including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR) decoding, have since been proposed to mitigate the model-perplexity-vs-quality mismatch. While these decoding methods achieve state-of-the-art performance, they are prohibitively expensive to compute. In this work, we propose MBR finetuning and QE finetuning which distill the quality gains from these decoding methods at training time, while using an efficient decoding algorithm at inference time. Using the canonical NLG task of Neural Machine Translation (NMT), we show that even with self-training, these finetuning methods significantly outperform the base model. Moreover, when using an external LLM as a teacher model, these finetuning methods outp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#24847;&#22270;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#24494;&#35843;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#23545;&#20110;&#26377;&#25928;&#21033;&#29992;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.10954</link><description>&lt;p&gt;
&#24102;&#26377;&#22810;&#20010;&#26631;&#31614;&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning for Text Classification with Many Labels. (arXiv:2309.10954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#24847;&#22270;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#24494;&#35843;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#23545;&#20110;&#26377;&#25928;&#21033;&#29992;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20855;&#26377;&#35768;&#22810;&#26631;&#31614;&#30340;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#20351;&#24471;&#22312;&#25552;&#31034;&#20013;&#38590;&#20197;&#36866;&#24212;&#36275;&#22815;&#25968;&#37327;&#30340;&#31034;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32469;&#36807;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#27599;&#27425;&#25512;&#29702;&#35843;&#29992;&#21482;&#32473;&#27169;&#22411;&#25552;&#20379;&#20102;&#23545;&#23436;&#25972;&#26631;&#31614;&#31354;&#38388;&#30340;&#37096;&#20998;&#35270;&#22270;&#12290;&#22312;&#26368;&#36817;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;(OPT, LLaMA)&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#24847;&#22270;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#36229;&#36234;&#20102;&#24494;&#35843;&#24615;&#33021;&#22312;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#25968;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20197;&#21450;&#19981;&#21516;&#27169;&#22411;&#35268;&#27169;&#19979;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#23545;&#20110;&#26377;&#25928;&#32780;&#19968;&#33268;&#22320;&#21033;&#29992;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#12290;&#36890;&#36807;&#36816;&#34892;&#20960;&#20010;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27169;&#22411;&#23545;&#20197;&#19979;&#20869;&#23481;&#30340;&#20351;&#29992;&#65306;a)&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#24403;&#21069;&#36755;&#20837;&#30340;&#30456;&#20284;&#24230;, b) &#21363;&#26102;&#26597;&#35810;&#35821;&#21477;&#30340;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) using large language models for tasks with many labels is challenging due to the limited context window, which makes it difficult to fit a sufficient number of examples in the prompt. In this paper, we use a pre-trained dense retrieval model to bypass this limitation, giving the model only a partial view of the full label space for each inference call. Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art performance in few-shot settings for three common intent classification datasets, with no finetuning. We also surpass fine-tuned performance on fine-grained sentiment classification in certain cases. We analyze the performance across number of in-context examples and different model scales, showing that larger models are necessary to effectively and consistently make use of larger context lengths for ICL. By running several ablations, we analyze the model's use of: a) the similarity of the in-context examples to the current input, b) 
&lt;/p&gt;</description></item><item><title>LMDX&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#24067;&#23616;&#32534;&#30721;&#21644;&#31572;&#26696;&#34394;&#26500;&#30340;&#22256;&#38590;&#65292;&#33021;&#22815;&#22312;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.10952</link><description>&lt;p&gt;
LMDX&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
LMDX: Language Model-based Document Information Extraction and Localization. (arXiv:2309.10952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10952
&lt;/p&gt;
&lt;p&gt;
LMDX&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#24067;&#23616;&#32534;&#30721;&#21644;&#31572;&#26696;&#34394;&#26500;&#30340;&#22256;&#38590;&#65292;&#33021;&#22815;&#22312;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;&#35768;&#22810;&#29616;&#26377;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#20852;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLM&#23578;&#26410;&#25104;&#21151;&#24212;&#29992;&#20110;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#65292;&#36825;&#26159;&#35768;&#22810;&#25991;&#26723;&#22788;&#29702;&#24037;&#20316;&#27969;&#30340;&#26680;&#24515;&#65292;&#21253;&#25324;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#65288;VRD&#65289;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#65292;&#32473;&#23450;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#27169;&#24335;&#12290;LLM&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;LLM&#20013;&#32570;&#20047;&#24067;&#23616;&#32534;&#30721;&#65292;&#36825;&#23545;&#20110;&#39640;&#36136;&#37327;&#30340;&#25552;&#21462;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21450;&#32570;&#20047;&#19968;&#20010;&#22522;&#20110;&#29702;&#35770;&#30340;&#26426;&#21046;&#65292;&#30830;&#20445;&#31572;&#26696;&#19981;&#26159;&#34394;&#26500;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#65288;LMDX&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;LLM&#36866;&#24212;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#12290;LMDX&#21487;&#20197;&#25552;&#21462;&#21333;&#19968;&#12289;&#37325;&#22797;&#21644;&#23618;&#27425;&#32467;&#26500;&#23454;&#20307;&#65292;&#26080;&#35770;&#26159;&#21542;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#22522;&#20110;&#29702;&#35770;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art on many existing tasks and exhibiting emergent capabilities. However, LLMs have not yet been successfully applied on semi-structured document information extraction, which is at the core of many document processing workflows and consists of extracting key entities from a visually rich document (VRD) given a predefined target schema. The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated. In this paper, we introduce Language Model-based Document Information Extraction and Localization (LMDX), a methodology to adapt arbitrary LLMs for document information extraction. LMDX can do extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and lo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#20026;Pir\'a 2.0&#25968;&#25454;&#38598;&#23450;&#20041;&#20102;&#20845;&#20010;&#19981;&#21516;&#30340;&#38382;&#31572;&#20219;&#21153;&#30340;&#27979;&#35797;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#38405;&#35835;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10945</link><description>&lt;p&gt;
Pir\'a 2.0&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;&#19968;&#20010;&#20851;&#20110;&#28023;&#27915;&#12289;&#24052;&#35199;&#28023;&#23736;&#21644;&#27668;&#20505;&#21464;&#21270;&#30340;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Benchmarks for Pir\'a 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change. (arXiv:2309.10945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10945
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#20026;Pir\'a 2.0&#25968;&#25454;&#38598;&#23450;&#20041;&#20102;&#20845;&#20010;&#19981;&#21516;&#30340;&#38382;&#31572;&#20219;&#21153;&#30340;&#27979;&#35797;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#38405;&#35835;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pir\'a&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#28023;&#27915;&#12289;&#24052;&#35199;&#28023;&#23736;&#21644;&#27668;&#20505;&#21464;&#21270;&#30340;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#26377;&#20851;&#36825;&#20123;&#20027;&#39064;&#30340;&#31185;&#23398;&#25688;&#35201;&#21644;&#25253;&#21578;&#30340;&#25910;&#34255;&#20013;&#26500;&#24314;&#32780;&#25104;&#30340;&#12290;&#35813;&#25968;&#25454;&#38598;&#20195;&#34920;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35821;&#35328;&#36164;&#28304;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27979;&#35797;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#21462;&#19987;&#23478;&#31185;&#23398;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;Pir\'a&#23578;&#26410;&#24320;&#21457;&#20986;&#35814;&#32454;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#21019;&#24314;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#21033;&#29992;Pir\'a&#20316;&#20026;&#27979;&#35797;&#21508;&#31181;&#38382;&#31572;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;Pir\'a&#25968;&#25454;&#38598;&#23450;&#20041;&#20102;&#20845;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#23553;&#38381;&#29983;&#25104;&#38382;&#31572;&#12289;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#24320;&#25918;&#24335;&#38382;&#31572;&#12289;&#31572;&#26696;&#35302;&#21457;&#21644;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#12290;&#20316;&#20026;&#36825;&#19968;&#21162;&#21147;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#21046;&#20316;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#31934;&#36873;&#29256;&#26412;&#65292;&#20854;&#20013;&#20462;&#27491;&#20102;&#19968;&#20123;&#35821;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pir\'a is a reading comprehension dataset focused on the ocean, the Brazilian coast, and climate change, built from a collection of scientific abstracts and reports on these topics. This dataset represents a versatile language resource, particularly useful for testing the ability of current machine learning models to acquire expert scientific knowledge. Despite its potential, a detailed set of baselines has not yet been developed for Pir\'a. By creating these baselines, researchers can more easily utilize Pir\'a as a resource for testing machine learning models across a wide range of question answering tasks. In this paper, we define six benchmarks over the Pir\'a dataset, covering closed generative question answering, machine reading comprehension, information retrieval, open question answering, answer triggering, and multiple choice question answering. As part of this effort, we have also produced a curated version of the original dataset, where we fixed a number of grammar issues, r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#19987;&#38376;&#38024;&#23545;&#20420;&#35821;&#30340;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20420;&#35821;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24076;&#26395;&#33021;&#22815;&#25512;&#21160;&#20420;&#35821;&#39046;&#22495;&#30340;NLP&#30740;&#31350;&#21644;&#24037;&#19994;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.10931</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#20420;&#35821;&#30340;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;
&lt;/p&gt;
&lt;p&gt;
A Family of Pretrained Transformer Language Models for Russian. (arXiv:2309.10931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#19987;&#38376;&#38024;&#23545;&#20420;&#35821;&#30340;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20420;&#35821;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24076;&#26395;&#33021;&#22815;&#25512;&#21160;&#20420;&#35821;&#39046;&#22495;&#30340;NLP&#30740;&#31350;&#21644;&#24037;&#19994;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;Transformer&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#19987;&#38376;&#38024;&#23545;&#20420;&#35821;&#30340;&#36825;&#31181;&#27169;&#22411;&#30340;&#21457;&#23637;&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#22522;&#20110;&#32534;&#30721;&#22120;&#65288;ruBERT, ruRoBERTa, ruELECTRA&#65289;&#12289;&#35299;&#30721;&#22120;&#65288;ruGPT-3&#65289;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;ruT5, FRED-T5&#65289;&#27169;&#22411;&#30340;13&#20010;&#20420;&#35821;Transformer LMs&#65292;&#20855;&#26377;&#22810;&#31181;&#23610;&#23544;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#36890;&#36807;HuggingFace&#24179;&#21488;&#36731;&#26494;&#33719;&#21462;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#21644;&#39044;&#35757;&#32451;&#30340;&#25253;&#21578;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20420;&#35821;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#21457;&#24067;&#36825;&#20123;&#19987;&#38376;&#30340;Transformer LMs&#65292;&#25105;&#20204;&#24076;&#26395;&#25299;&#23485;NLP&#30740;&#31350;&#26041;&#21521;&#30340;&#33539;&#22260;&#65292;&#24182;&#20419;&#36827;&#38024;&#23545;&#20420;&#35821;&#30340;&#24037;&#19994;&#35299;&#20915;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, Transformer language models (LMs) represent a fundamental component of the NLP research methodologies and applications. However, the development of such models specifically for the Russian language has received little attention. This paper presents a collection of 13 Russian Transformer LMs based on the encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) models in multiple sizes. Access to these models is readily available via the HuggingFace platform. We provide a report of the model architecture design and pretraining, and the results of evaluating their generalization abilities on Russian natural language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs, we hope to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22797;&#26434;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#20197;&#21450;&#38544;&#24335;&#39118;&#26684;&#39044;&#35757;&#32451;&#36827;&#34892;&#23454;&#39564;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2309.10929</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#23646;&#24615;&#39044;&#35757;&#32451;&#23558;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29305;&#21270;&#20110;&#22797;&#26434;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training. (arXiv:2309.10929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22797;&#26434;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#20197;&#21450;&#38544;&#24335;&#39118;&#26684;&#39044;&#35757;&#32451;&#36827;&#34892;&#23454;&#39564;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22797;&#26434;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#30340;&#27010;&#24565;&#65292;&#24182;&#22522;&#20110;&#20004;&#20010;&#24191;&#27867;&#36866;&#29992;&#30340;&#22330;&#26223;&#26500;&#24314;&#20102;&#22797;&#26434;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#20854;&#31867;&#21035;&#20013;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;700&#20010;&#37325;&#26032;&#34920;&#36848;&#30340;&#21477;&#23376;&#21644;&#26469;&#33258;&#28216;&#25103;&#21407;&#31070;&#30340;1,000&#20010;&#21477;&#23376;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22797;&#26434;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12289;&#32593;&#32476;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#37096;&#32626;&#25104;&#26412;&#31561;&#32570;&#28857;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25506;&#32034;&#20102;&#23567;&#22411;&#27169;&#22411;&#65288;&#23567;&#20110;T5-3B&#65289;&#22312;&#38544;&#24335;&#39118;&#26684;&#39044;&#35757;&#32451;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19982;ChatGPT&#30340;&#20154;&#24037;&#35780;&#20215;&#23545;&#40784;&#30340;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#27169;&#22411;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce the concept of complex text style transfer tasks, and constructed complex text datasets based on two widely applicable scenarios. Our dataset is the first large-scale data set of its kind, with 700 rephrased sentences and 1,000 sentences from the game Genshin Impact. While large language models (LLM) have shown promise in complex text style transfer, they have drawbacks such as data privacy concerns, network instability, and high deployment costs. To address these issues, we explore the effectiveness of small models (less than T5-3B) with implicit style pre-training through contrastive learning. We also propose a method for automated evaluation of text generation quality based on alignment with human evaluations using ChatGPT. Finally, we compare our approach with existing methods and show that our model achieves state-of-art performances of few-shot text style transfer models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26631;&#31614;&#19978;&#19979;&#25991;&#30340;&#21322;&#33258;&#22238;&#24402;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23376;&#32593;&#32476;&#65292;&#23558;&#20808;&#21069;&#22359;&#20013;&#30340;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10926</link><description>&lt;p&gt;
&#24102;&#26631;&#31614;&#19978;&#19979;&#25991;&#30340;&#21322;&#33258;&#22238;&#24402;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Semi-Autoregressive Streaming ASR With Label Context. (arXiv:2309.10926v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10926
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26631;&#31614;&#19978;&#19979;&#25991;&#30340;&#21322;&#33258;&#22238;&#24402;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23376;&#32593;&#32476;&#65292;&#23558;&#20808;&#21069;&#22359;&#20013;&#30340;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;(NAR)&#24314;&#27169;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#38388;&#26041;&#38754;&#27604;&#33258;&#22238;&#24402;(AR)&#27169;&#22411;&#22823;&#22823;&#38477;&#20302;&#65292;&#21516;&#26102;&#20063;&#36798;&#21040;&#20102;&#36739;&#22909;&#30340;&#36716;&#24405;&#20934;&#30830;&#29575;&#12290;&#30001;&#20110;NAR&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#27169;&#22411;&#24517;&#39035;&#31561;&#24453;&#25972;&#20010;&#35805;&#35821;&#30340;&#23436;&#25972;&#23436;&#25104;&#25165;&#33021;&#36827;&#34892;&#22788;&#29702;&#65292;&#22240;&#27492;&#19968;&#20123;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;&#22359;&#29366;&#27880;&#24847;&#21147;&#30340;&#27969;&#24335;NAR&#27169;&#22411;&#65292;&#20197;&#29992;&#20110;&#20302;&#24310;&#36831;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#19982;&#27969;&#24335;AR&#21644;&#38750;&#27969;&#24335;NAR&#27169;&#22411;&#30456;&#27604;&#65292;&#27969;&#24335;NAR&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#26126;&#26174;&#28382;&#21518;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;&#30340;&#8220;&#21322;&#33258;&#22238;&#24402;&#8221;ASR&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;(LM)&#23376;&#32593;&#32476;&#23558;&#20808;&#21069;&#22359;&#20013;&#21457;&#20986;&#30340;&#26631;&#31614;&#20316;&#20026;&#38468;&#21152;&#19978;&#19979;&#25991;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36138;&#23146;&#35299;&#30721;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22359;&#20043;&#38388;&#38468;&#36817;&#22788;&#29702;&#25554;&#20837;&#21644;&#21024;&#38500;&#38169;&#35823;&#65292;&#21516;&#26102;&#19981;&#26174;&#33879;&#22686;&#21152;&#25512;&#26029;&#26102;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#27969;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive (NAR) modeling has gained significant interest in speech processing since these models achieve dramatically lower inference time than autoregressive (AR) models while also achieving good transcription accuracy. Since NAR automatic speech recognition (ASR) models must wait for the completion of the entire utterance before processing, some works explore streaming NAR models based on blockwise attention for low-latency applications. However, streaming NAR models significantly lag in accuracy compared to streaming AR and non-streaming NAR models. To address this, we propose a streaming "semi-autoregressive" ASR model that incorporates the labels emitted in previous blocks as additional context using a Language Model (LM) subnetwork. We also introduce a novel greedy decoding algorithm that addresses insertion and deletion errors near block boundaries while not significantly increasing the inference time. Experiments show that our method outperforms the existing streaming 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;&#24179;&#21488;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36229;&#23548;&#20307;&#23454;&#39564;&#25968;&#25454;&#12290;&#35813;&#24179;&#21488;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#31243;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26356;&#26032;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#12290;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#35813;&#20998;&#21306;&#24179;&#21488;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#31649;&#29702;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10923</link><description>&lt;p&gt;
&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;: &#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#39640;&#36136;&#37327;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Semi-automatic staging area for high-quality structured data extraction from scientific literature. (arXiv:2309.10923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10923
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;&#24179;&#21488;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36229;&#23548;&#20307;&#23454;&#39564;&#25968;&#25454;&#12290;&#35813;&#24179;&#21488;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#31243;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26356;&#26032;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#12290;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#35813;&#20998;&#21306;&#24179;&#21488;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#31649;&#29702;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#37319;&#38598;&#36229;&#23548;&#20307;&#23454;&#39564;&#25968;&#25454;&#30340; SuperCon &#25968;&#25454;&#24211;&#30340;&#20998;&#21306;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#26356;&#26032; SuperCon &#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#31243;&#32452;&#25104;&#30340;&#24037;&#20316;&#27969;&#39537;&#21160;&#30340;&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;&#24179;&#21488;&#65292;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#25968;&#25454;&#24211;&#20013;&#23545;&#25968;&#25454;&#36827;&#34892;&#26657;&#39564;&#21644;&#32416;&#38169;&#12290;&#24322;&#24120;&#26816;&#27979;&#33258;&#21160;&#36807;&#31243;&#29992;&#20110;&#39044;&#20808;&#31579;&#36873;&#37319;&#38598;&#21040;&#30340;&#25968;&#25454;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#23450;&#21046;&#30340;&#29992;&#25143;&#30028;&#38754;&#22312;&#21407;&#22987; PDF &#25991;&#26723;&#19978;&#36827;&#34892;&#25968;&#25454;&#39564;&#35777;&#21644;&#32416;&#38169;&#12290;&#27492;&#22806;&#65292;&#24403;&#35760;&#24405;&#34987;&#32416;&#38169;&#26102;&#65292;&#20854;&#21407;&#22987;&#25968;&#25454;&#34987;&#25910;&#38598;&#24182;&#29992;&#20110;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#20998;&#21306;&#24179;&#21488;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#31649;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#23558;&#30028;&#38754;&#19982;&#20256;&#32479;&#30340;&#25163;&#21160;&#38405;&#35835; PDF &#25991;&#26723;&#24182;&#22312; Excel &#25991;&#26723;&#20013;&#35760;&#24405;&#20449;&#24687;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a staging area for ingesting new superconductors' experimental data in SuperCon that is machine-collected from scientific articles. Our objective is to enhance the efficiency of updating SuperCon while maintaining or enhancing the data quality. We present a semi-automatic staging area driven by a workflow combining automatic and manual processes on the extracted database. An anomaly detection automatic process aims to pre-screen the collected data. Users can then manually correct any errors through a user interface tailored to simplify the data verification on the original PDF documents. Additionally, when a record is corrected, its raw data is collected and utilised to improve machine learning models as training data. Evaluation experiments demonstrate that our staging area significantly improves curation quality. We compare the interface with the traditional manual approach of reading PDF documents and recording information in an Excel document. Using the in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#25552;&#20379;&#38899;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#19978;&#19979;&#25991;&#65292;&#20351;&#31995;&#32479;&#38544;&#21547;&#22320;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2309.10917</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#19978;&#19979;&#25991;&#21270;
&lt;/p&gt;
&lt;p&gt;
End-to-End Speech Recognition Contextualization with Large Language Models. (arXiv:2309.10917v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#25552;&#20379;&#38899;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#19978;&#19979;&#25991;&#65292;&#20351;&#31995;&#32479;&#38544;&#21547;&#22320;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#32780;&#21463;&#21040;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#34701;&#20837;LLMs&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35821;&#38899;&#35782;&#21035;&#35270;&#20026;&#22522;&#20110;&#39044;&#35757;&#32451;LLM&#30340;&#28151;&#21512;&#27169;&#24577;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#38899;&#39057;&#29305;&#24449;&#20197;&#21450;&#21487;&#36873;&#30340;&#25991;&#26412;&#26631;&#35760;&#26469;&#35757;&#32451;&#31995;&#32479;&#20197;&#35299;&#30721;&#26041;&#24335;&#23436;&#25104;&#36716;&#24405;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#31995;&#32479;&#20250;&#38544;&#21547;&#22320;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#38750;&#32467;&#26500;&#21270;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65292;&#24403;&#25552;&#20379;&#39069;&#22806;&#30340;&#25991;&#26412;&#19978;&#19979;&#25991;&#26102;&#65292;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#20102;6%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31454;&#20105;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#25972;&#20307;&#19978;&#23558;WER&#25552;&#39640;&#20102;7.5%&#65292;&#23545;&#20110;&#32597;&#35265;&#35789;&#35821;&#30340;WER&#25552;&#39640;&#20102;17%&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#19978;&#19979;&#25991;&#21270;RNN-T&#31995;&#32479;&#30340;&#35757;&#32451;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We provide audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system is implicitly incentivized to learn how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively and improve by 7.5% WER overall and 17% WER on rare words against a baseline contextualized RNN-T system that has been trained on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#25216;&#26415;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.10916</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#21644;&#24433;&#21709;&#20989;&#25968;&#65292;&#25105;&#20204;&#33021;&#20174;&#23545;&#25239;&#26679;&#26412;&#20013;&#33719;&#24471;&#20160;&#20040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#25216;&#26415;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#26159;&#36890;&#36807;&#24494;&#23567;&#25200;&#21160;&#26469;&#27450;&#39575;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#65292;&#36215;&#21021;&#22312;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#30740;&#31350;&#65292;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20063;&#24320;&#22987;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26816;&#27979;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36755;&#20837;&#25200;&#21160;&#30340;&#25628;&#32034;&#65292;&#20294;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#24050;&#32463;&#21457;&#23637;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#34920;&#24449;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#12290;&#26412;&#25991;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#65292;&#19968;&#31181;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#12290;&#29305;&#21035;&#26159;&#21069;&#32773;&#30456;&#27604;&#20960;&#20010;&#24378;&#22522;&#20934;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65307;&#27492;&#22806;&#65292;&#23545;&#24433;&#21709;&#20989;&#25968;&#30340;&#26032;&#39062;&#20351;&#29992;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#26681;&#25454;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples, deliberately crafted using small perturbations to fool deep neural networks, were first studied in image processing and more recently in NLP. While approaches to detecting adversarial examples in NLP have largely relied on search over input perturbations, image processing has seen a range of techniques that aim to characterise adversarial subspaces over the learned representations.  In this paper, we adapt two such approaches to NLP, one based on nearest neighbors and influence functions and one on Mahalanobis distances. The former in particular produces a state-of-the-art detector when compared against several strong baselines; moreover, the novel use of influence functions provides insight into how the nature of adversarial example subspaces in NLP relate to those in image processing, and also how they differ depending on the kind of NLP task.
&lt;/p&gt;</description></item><item><title>RedPenNet&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#20013;&#30340;&#26550;&#26500;&#21644;&#21442;&#25968;&#20887;&#20313;&#38382;&#39064;&#65292;&#20855;&#26377;&#21322;&#33258;&#22238;&#24402;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.10898</link><description>&lt;p&gt;
RedPenNet&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#65306;&#23558;&#36755;&#20986;&#36716;&#21270;&#20026;&#21333;&#35789;&#65292;&#23558;&#27880;&#24847;&#21147;&#24212;&#29992;&#21040;&#29255;&#27573;&#19978;
&lt;/p&gt;
&lt;p&gt;
RedPenNet for Grammatical Error Correction: Outputs to Tokens, Attentions to Spans. (arXiv:2309.10898v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10898
&lt;/p&gt;
&lt;p&gt;
RedPenNet&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#20013;&#30340;&#26550;&#26500;&#21644;&#21442;&#25968;&#20887;&#20313;&#38382;&#39064;&#65292;&#20855;&#26377;&#21322;&#33258;&#22238;&#24402;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#21253;&#25324;&#21477;&#23376;&#21512;&#24182;&#12289;&#21477;&#23376;&#20998;&#21106;&#21644;&#25913;&#20889;&#12289;&#25991;&#26412;&#31616;&#21270;&#20197;&#21450;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#65288;GEC&#65289;&#65292;&#23427;&#20204;&#37117;&#38754;&#20020;&#30528;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#20043;&#38388;&#39640;&#24230;&#30456;&#20284;&#30340;&#29305;&#28857;&#12290;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#22788;&#20110;&#20004;&#20010;&#24050;&#32463;&#25104;&#29087;&#30340;&#39046;&#22495;&#30340;&#20132;&#21449;&#28857;&#19978;&#65306;&#65288;i&#65289;&#20840;&#33258;&#22238;&#24402;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#24120;&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31561;&#20219;&#21153;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#24207;&#21015;&#26631;&#27880;&#25216;&#26415;&#65292;&#24120;&#29992;&#20110;&#35789;&#24615;&#26631;&#27880;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#31561;&#20219;&#21153;&#12290;&#20026;&#20102;&#35774;&#35745;&#19968;&#20010;&#24179;&#34913;&#30340;&#26550;&#26500;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#35768;&#22810;&#23500;&#26377;&#24819;&#35937;&#21147;&#21644;&#38750;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#22312;&#30456;&#20851;&#24037;&#20316;&#37096;&#20998;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;RedPenNet&#65292;&#26088;&#22312;&#20943;&#23569;&#29305;&#23450;&#30340;&#24207;&#21015;&#21040;&#32534;&#36753;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#26550;&#26500;&#21644;&#21442;&#25968;&#20887;&#20313;&#65292;&#20445;&#30041;&#23427;&#20204;&#30340;&#21322;&#33258;&#22238;&#24402;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;$F_{0
&lt;/p&gt;
&lt;p&gt;
The text editing tasks, including sentence fusion, sentence splitting and rephrasing, text simplification, and Grammatical Error Correction (GEC), share a common trait of dealing with highly similar input and output sequences. This area of research lies at the intersection of two well-established fields: (i) fully autoregressive sequence-to-sequence approaches commonly used in tasks like Neural Machine Translation (NMT) and (ii) sequence tagging techniques commonly used to address tasks such as Part-of-speech tagging, Named-entity recognition (NER), and similar. In the pursuit of a balanced architecture, researchers have come up with numerous imaginative and unconventional solutions, which we're discussing in the Related Works section. Our approach to addressing text editing tasks is called RedPenNet and is aimed at reducing architectural and parametric redundancies presented in specific Sequence-To-Edits models, preserving their semi-autoregressive advantages. Our models achieve $F_{0
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SALT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20195;&#30721;&#20999;&#25442;&#21644;&#23884;&#20837;&#28151;&#21512;&#19982;&#33258;&#25105;&#22686;&#24378;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10891</link><description>&lt;p&gt;
&#33258;&#25105;&#22686;&#24378;&#25913;&#36827;&#20102;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer. (arXiv:2309.10891v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SALT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20195;&#30721;&#20999;&#25442;&#21644;&#23884;&#20837;&#28151;&#21512;&#19982;&#33258;&#25105;&#22686;&#24378;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#26159;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26680;&#24515;&#20219;&#21153;&#65292;&#20801;&#35768;&#22312;&#20855;&#26377;&#26356;&#20805;&#36275;&#35757;&#32451;&#36164;&#28304;&#30340;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#25512;&#24191;&#21040;&#20854;&#20182;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#20808;&#21069;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#21162;&#21147;&#20351;&#29992;&#24179;&#34892;&#35821;&#26009;&#24211;&#12289;&#21452;&#35821;&#35789;&#20856;&#25110;&#20854;&#20182;&#26631;&#27880;&#23545;&#40784;&#25968;&#25454;&#26469;&#25552;&#39640;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#65292;&#36825;&#20123;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#33719;&#21462;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;SALT&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#65292;&#32780;&#19981;&#38656;&#35201;&#36825;&#20123;&#22806;&#37096;&#25968;&#25454;&#30340;&#24110;&#21161;&#12290;&#36890;&#36807;&#32467;&#21512;&#20195;&#30721;&#20999;&#25442;&#21644;&#23884;&#20837;&#28151;&#21512;&#19982;&#33258;&#25105;&#22686;&#24378;&#65292;SALT&#26377;&#25928;&#22320;&#33976;&#39311;&#20102;&#22810;&#35821;&#35328;PLM&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#65292;&#24182;&#22686;&#24378;&#20102;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;&#22312;XNLI&#21644;PAWS-X&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#22806;&#37096;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/luka-group/SALT&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-lingual transfer is a central task in multilingual NLP, allowing models trained in languages with more sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain. In this paper, we propose a simple yet effective method, SALT, to improve the zero-shot cross-lingual transfer of the multilingual pretrained language models without the help of such external data. By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM and enhances its transferability on downstream tasks. Experimental results on XNLI and PAWS-X show that our method is able to improve zero-shot cross-lingual transferability without external data. Our code is available at https://github.com/luka-group/SALT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#33258;&#21160;&#23545;&#39135;&#29289;&#31995;&#32479;&#26412;&#20307;&#20013;&#30340;&#32452;&#32455;&#36827;&#34892;&#20998;&#31867;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;NLP&#27169;&#22411;&#21487;&#20197;&#22312;&#36825;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10880</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23545;&#39135;&#29289;&#31995;&#32479;&#26412;&#20307;&#36827;&#34892;&#32452;&#32455;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying Organizations for Food System Ontologies using Natural Language Processing. (arXiv:2309.10880v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#33258;&#21160;&#23545;&#39135;&#29289;&#31995;&#32479;&#26412;&#20307;&#20013;&#30340;&#32452;&#32455;&#36827;&#34892;&#20998;&#31867;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;NLP&#27169;&#22411;&#21487;&#20197;&#22312;&#36825;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#33258;&#21160;&#23545;&#23454;&#20307;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#36798;&#21040;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#19982;&#39135;&#29289;&#31995;&#32479;&#26412;&#20307;&#30340;&#38598;&#25104;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#33021;&#22815;&#33258;&#21160;&#23558;&#32452;&#32455;&#26681;&#25454;&#19982;&#29615;&#22659;&#38382;&#39064;&#30456;&#20851;&#30340;&#31867;&#21035;&#20197;&#21450;&#32654;&#22269;&#25919;&#24220;&#29992;&#20110;&#25551;&#36848;&#21830;&#19994;&#27963;&#21160;&#30340;&#26631;&#20934;&#20135;&#19994;&#20998;&#31867;&#65288;SIC&#65289;&#20195;&#30721;&#36827;&#34892;&#20998;&#31867;&#30340;NLP&#27169;&#22411;&#12290;NLP&#27169;&#22411;&#30340;&#36755;&#20837;&#20026;&#27599;&#20010;&#32452;&#32455;&#36890;&#36807;Google&#25628;&#32034;&#24341;&#25806;&#26816;&#32034;&#21040;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#35813;&#25991;&#26412;&#29255;&#27573;&#29992;&#20316;&#29992;&#20110;&#23398;&#20064;&#30340;&#32452;&#32455;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;NLP&#27169;&#22411;&#21487;&#20197;&#22312;&#36825;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#30456;&#24403;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23427;&#20204;&#20381;&#36182;&#20110;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#35768;&#22810;&#20854;&#20182;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;NLP&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#25910;&#38598;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research explores the use of natural language processing (NLP) methods to automatically classify entities for the purpose of knowledge graph population and integration with food system ontologies. We have created NLP models that can automatically classify organizations with respect to categories associated with environmental issues as well as Standard Industrial Classification (SIC) codes, which are used by the U.S. government to characterize business activities. As input, the NLP models are provided with text snippets retrieved by the Google search engine for each organization, which serves as a textual description of the organization that is used for learning. Our experimental results show that NLP models can achieve reasonably good performance for these two classification tasks, and they rely on a general framework that could be applied to many other classification problems as well. We believe that NLP models represent a promising approach for automatically harvesting informatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MelodyGLM&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#38271;&#26399;&#32467;&#26500;&#30340;&#26059;&#24459;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#38899;&#20048;n-gram&#21644;&#38271;&#36328;&#24230;&#25277;&#26679;&#31574;&#30053;&#26469;&#25429;&#25417;&#26059;&#24459;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22823;&#35268;&#27169;&#31526;&#21495;&#26059;&#24459;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.10738</link><description>&lt;p&gt;
MelodyGLM: &#38899;&#20048;&#31526;&#21495;&#26059;&#24459;&#29983;&#25104;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation. (arXiv:2309.10738v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MelodyGLM&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#38271;&#26399;&#32467;&#26500;&#30340;&#26059;&#24459;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#38899;&#20048;n-gram&#21644;&#38271;&#36328;&#24230;&#25277;&#26679;&#31574;&#30053;&#26469;&#25429;&#25417;&#26059;&#24459;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22823;&#35268;&#27169;&#31526;&#21495;&#26059;&#24459;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#38899;&#20048;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#31526;&#21495;&#26059;&#24459;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#25429;&#25417;&#38899;&#31526;&#24207;&#21015;&#20013;&#30340;&#22810;&#23610;&#24230;&#12289;&#22810;&#32500;&#32467;&#26500;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#25991;&#26412;&#21644;&#38899;&#20048;&#20043;&#38388;&#39046;&#22495;&#30693;&#35782;&#24046;&#24322;&#30340;&#32536;&#25925;&#12290;&#27492;&#22806;&#65292;&#21487;&#29992;&#22823;&#35268;&#27169;&#31526;&#21495;&#26059;&#24459;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#38480;&#21046;&#20102;&#39044;&#35757;&#32451;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MelodyGLM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#38271;&#26399;&#32467;&#26500;&#26059;&#24459;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#38899;&#20048;n-gram&#21644;&#38271;&#36328;&#24230;&#25277;&#26679;&#31574;&#30053;&#65292;&#20026;&#26059;&#24459;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#24314;&#31435;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#31354;&#30333;&#22635;&#20805;&#20219;&#21153;&#65292;&#20197;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38899;&#39640;n-gram&#12289;&#33410;&#22863;n-gram&#21450;&#20854;&#32452;&#21512;&#30340;n-gram&#32435;&#20837;&#38899;&#20048;n-gram&#31354;&#30333;&#22635;&#20805;&#20219;&#21153;&#20013;&#65292;&#20197;&#24314;&#27169;&#26059;&#24459;&#30340;&#22810;&#32500;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#31526;&#21495;&#26059;&#24459;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have achieved impressive results in various music understanding and generation tasks. However, existing pre-training methods for symbolic melody generation struggle to capture multi-scale, multi-dimensional structural information in note sequences, due to the domain knowledge discrepancy between text and music. Moreover, the lack of available large-scale symbolic melody datasets limits the pre-training improvement. In this paper, we propose MelodyGLM, a multi-task pre-training framework for generating melodies with long-term structure. We design the melodic n-gram and long span sampling strategies to create local and global blank infilling tasks for modeling the local and global structures in melodies. Specifically, we incorporate pitch n-grams, rhythm n-grams, and their combined n-grams into the melodic n-gram blank infilling tasks for modeling the multi-dimensional structures in melodies. To this end, we have constructed a large-scale symbolic melody datas
&lt;/p&gt;</description></item><item><title>NusaWrites&#39033;&#30446;&#36890;&#36807;&#27597;&#35821;&#32773;&#27573;&#33853;&#25776;&#20889;&#26500;&#24314;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#65292;&#24357;&#34917;&#20102;&#22312;&#32447;&#29228;&#21462;&#21644;&#32763;&#35793;&#25991;&#26723;&#25152;&#24102;&#26469;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#38480;&#21046;&#12290;&#35813;&#30740;&#31350;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#22320;&#26041;&#35821;&#35328;&#19978;&#36827;&#34892;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;datasetname&#8221;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;12&#31181;&#34987;&#20302;&#20272;&#21644;&#26497;&#24230;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2309.10661</link><description>&lt;p&gt;
NusaWrites: &#20026;&#34987;&#20302;&#20272;&#21644;&#26497;&#24230;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#26500;&#24314;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages. (arXiv:2309.10661v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10661
&lt;/p&gt;
&lt;p&gt;
NusaWrites&#39033;&#30446;&#36890;&#36807;&#27597;&#35821;&#32773;&#27573;&#33853;&#25776;&#20889;&#26500;&#24314;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#65292;&#24357;&#34917;&#20102;&#22312;&#32447;&#29228;&#21462;&#21644;&#32763;&#35793;&#25991;&#26723;&#25152;&#24102;&#26469;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#38480;&#21046;&#12290;&#35813;&#30740;&#31350;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#22320;&#26041;&#35821;&#35328;&#19978;&#36827;&#34892;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;datasetname&#8221;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;12&#31181;&#34987;&#20302;&#20272;&#21644;&#26497;&#24230;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27665;&#20027;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#35775;&#38382;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#34987;&#20302;&#20272;&#21644;&#26497;&#24230;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#36890;&#36807;&#22312;&#32447;&#29228;&#21462;&#21644;&#25991;&#20214;&#32763;&#35793;&#20026;&#36825;&#20123;&#35821;&#35328;&#24320;&#21457;&#24102;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#30340;&#35821;&#26009;&#24211;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#21644;&#36153;&#29992;&#25928;&#30410;&#30340;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25152;&#24471;&#21040;&#30340;&#35821;&#26009;&#24211;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#32570;&#20047;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#19982;&#24403;&#22320;&#31038;&#21306;&#30340;&#25991;&#21270;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#30340;&#22320;&#26041;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22312;&#32447;&#29228;&#21462;&#12289;&#20154;&#24037;&#32763;&#35793;&#21644;&#27597;&#35821;&#32773;&#27573;&#33853;&#25776;&#20889;&#22312;&#26500;&#24314;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#27597;&#35821;&#32773;&#27573;&#33853;&#25776;&#20889;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#22312;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#25991;&#21270;&#20869;&#23481;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#8220;datasetname &#8221;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;12&#20010;&#34987;&#20302;&#20272;&#21644;&#26497;&#24230;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Democratizing access to natural language processing (NLP) technology is crucial, especially for underrepresented and extremely low-resource languages. Previous research has focused on developing labeled and unlabeled corpora for these languages through online scraping and document translation. While these methods have proven effective and cost-efficient, we have identified limitations in the resulting corpora, including a lack of lexical diversity and cultural relevance to local communities. To address this gap, we conduct a case study on Indonesian local languages. We compare the effectiveness of online scraping, human translation, and paragraph writing by native speakers in constructing datasets. Our findings demonstrate that datasets generated through paragraph writing by native speakers exhibit superior quality in terms of lexical diversity and cultural content. In addition, we present the \datasetname{} benchmark, encompassing 12 underrepresented and extremely low-resource languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#65288;REI&#65289;&#23454;&#29616;&#20102;&#32479;&#19968;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#25351;&#20196;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#32422;&#26463;&#65292;&#26080;&#38656;&#23545;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#65292;&#24182;&#23545;&#21508;&#31181;&#32422;&#26463;&#32452;&#21512;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10447</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#23454;&#29616;&#32479;&#19968;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Toward Unified Controllable Text Generation via Regular Expression Instruction. (arXiv:2309.10447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#65288;REI&#65289;&#23454;&#29616;&#20102;&#32479;&#19968;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#25351;&#20196;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#32422;&#26463;&#65292;&#26080;&#38656;&#23545;&#26550;&#26500;&#36827;&#34892;&#20462;&#25913;&#65292;&#24182;&#23545;&#21508;&#31181;&#32422;&#26463;&#32452;&#21512;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22522;&#26412;&#26041;&#38754;&#20043;&#19968;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#38024;&#23545;&#19981;&#21516;&#32422;&#26463;&#31867;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#37325;&#22823;&#30340;&#26550;&#26500;&#25110;&#35299;&#30721;&#20462;&#25913;&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#38468;&#21152;&#32422;&#26463;&#25110;&#35299;&#20915;&#19981;&#21516;&#32422;&#26463;&#32452;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#27491;&#21017;&#34920;&#36798;&#24335;&#25351;&#20196;&#65288;REI&#65289;&#65292;&#21033;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#26426;&#21046;&#20805;&#20998;&#21033;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#20248;&#21183;&#65292;&#32479;&#19968;&#24314;&#27169;&#21508;&#31181;&#32422;&#26463;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;REI&#36890;&#36807;&#27491;&#21017;&#34920;&#36798;&#24335;&#39118;&#26684;&#30340;&#25351;&#20196;&#25903;&#25345;&#25152;&#26377;&#27969;&#34892;&#30340;&#32454;&#31890;&#24230;&#21487;&#25511;&#29983;&#25104;&#32422;&#26463;&#65292;&#21363;&#35789;&#27719;&#12289;&#20301;&#32622;&#21644;&#38271;&#24230;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#22312;&#20013;&#31561;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#24494;&#35843;&#25110;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#23569;&#26679;&#26412;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#20110;&#21508;&#31181;&#32422;&#26463;&#32452;&#21512;&#26102;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#25972;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable text generation is a fundamental aspect of natural language generation, with numerous methods proposed for different constraint types. However, these approaches often require significant architectural or decoding modifications, making them challenging to apply to additional constraints or resolve different constraint combinations. To address this, our paper introduces Regular Expression Instruction (REI), which utilizes an instruction-based mechanism to fully exploit regular expressions' advantages to uniformly model diverse constraints. Specifically, our REI supports all popular fine-grained controllable generation constraints, i.e., lexical, positional, and length, as well as their complex combinations, via regular expression-style instructions. Our method only requires fine-tuning on medium-scale language models or few-shot, in-context learning on large language models, and requires no further adjustment when applied to various constraint combinations. Experiments demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2309.10400</link><description>&lt;p&gt;
PoSE: &#36890;&#36807;&#20301;&#32622;&#36339;&#36291;&#24335;&#35757;&#32451;&#25552;&#39640;LLMs&#23545;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#26377;&#25928;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Positional Skip-wise (PoSE)&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20110;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;PoSE&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#27169;&#25311;&#38271;&#36755;&#20837;&#65292;&#23558;&#35757;&#32451;&#38271;&#24230;&#19982;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#20998;&#31163;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#38271;&#36755;&#20837;&#24207;&#21015;&#20013;&#36873;&#25321;&#33509;&#24178;&#30701;&#22359;&#65292;&#24182;&#24341;&#20837;&#19981;&#21516;&#30340;&#36339;&#36291;&#20559;&#32622;&#39033;&#26469;&#20462;&#25913;&#27599;&#20010;&#22359;&#30340;&#20301;&#32622;&#32034;&#24341;&#12290;&#36825;&#20123;&#36339;&#36291;&#20559;&#32622;&#39033;&#20197;&#21450;&#27599;&#20010;&#22359;&#30340;&#38271;&#24230;&#22312;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#37117;&#20250;&#21464;&#21270;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#25152;&#26377;&#20301;&#32622;&#65292;&#32780;&#26080;&#38656;&#23545;&#23436;&#25972;&#38271;&#24230;&#30340;&#36755;&#20837;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#23545;&#23436;&#25972;&#38271;&#24230;&#36827;&#34892;&#24494;&#35843;&#30456;&#27604;&#65292;PoSE&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;&#21033;&#29992;&#36825;&#19968;&#20248;&#21183;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65292;PoSE&#19982;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. Furthermore, we empirically confirm that PoSE is compatible with 
&lt;/p&gt;</description></item><item><title>QASnowball&#26159;&#19968;&#20010;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#26377;&#30417;&#30563;&#30340;&#26679;&#26412;&#31181;&#23376;&#38598;&#29983;&#25104;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#31181;&#23376;&#21270;&#36827;&#34892;&#33258;&#25105;&#22686;&#24378;&#12290;&#22312;&#39640;&#36164;&#28304;&#33521;&#25991;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2309.10326</link><description>&lt;p&gt;
QASnowball: &#19968;&#20010;&#29992;&#20110;&#39640;&#36136;&#37327;&#38382;&#31572;&#25968;&#25454;&#29983;&#25104;&#30340;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
QASnowball: An Iterative Bootstrapping Framework for High-Quality Question-Answering Data Generation. (arXiv:2309.10326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10326
&lt;/p&gt;
&lt;p&gt;
QASnowball&#26159;&#19968;&#20010;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#26377;&#30417;&#30563;&#30340;&#26679;&#26412;&#31181;&#23376;&#38598;&#29983;&#25104;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#31181;&#23376;&#21270;&#36827;&#34892;&#33258;&#25105;&#22686;&#24378;&#12290;&#22312;&#39640;&#36164;&#28304;&#33521;&#25991;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#20854;&#22312;&#24212;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#31283;&#23450;&#30340;QA&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36845;&#20195;&#33258;&#20030;&#26694;&#26550;QASnowball&#65292;&#29992;&#20110;QA&#25968;&#25454;&#22686;&#24378;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#26377;&#30417;&#30563;&#30340;&#26679;&#26412;&#31181;&#23376;&#38598;&#36845;&#20195;&#22320;&#29983;&#25104;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;QASnowball&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#22238;&#31572;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#20174;&#26080;&#26631;&#31614;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20505;&#36873;&#31572;&#26696;&#30340;&#26680;&#24515;&#30701;&#35821;&#65307;&#38382;&#39064;&#29983;&#25104;&#22120;&#65292;&#26681;&#25454;&#25991;&#26723;&#21644;&#20505;&#36873;&#31572;&#26696;&#29983;&#25104;&#38382;&#39064;&#65307;QA&#25968;&#25454;&#36807;&#28388;&#22120;&#65292;&#29992;&#20110;&#36807;&#28388;&#20986;&#39640;&#36136;&#37327;&#30340;QA&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;QASnowball&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#31181;&#23376;&#21270;&#31181;&#23376;&#38598;&#22312;&#19981;&#21516;&#36845;&#20195;&#20013;&#36827;&#34892;&#33258;&#25105;&#22686;&#24378;&#65292;&#20174;&#32780;&#19981;&#26029;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#39640;&#36164;&#28304;&#33521;&#25991;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the success of question answering (QA), especially its potential to be a foundation paradigm for tackling diverse NLP tasks. However, obtaining sufficient data to build an effective and stable QA system still remains an open problem. For this problem, we introduce an iterative bootstrapping framework for QA data augmentation (named QASnowball), which can iteratively generate large-scale high-quality QA data based on a seed set of supervised examples. Specifically, QASnowball consists of three modules, an answer extractor to extract core phrases in unlabeled documents as candidate answers, a question generator to generate questions based on documents and candidate answers, and a QA data filter to filter out high-quality QA data. Moreover, QASnowball can be self-enhanced by reseeding the seed set to fine-tune itself in different iterations, leading to continual improvements in the generation quality. We conduct experiments in the high-resource English scenario
&lt;/p&gt;</description></item><item><title>Baichuan 2&#26159;&#19968;&#31995;&#21015;&#24320;&#25918;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25317;&#26377;70&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#35757;&#32451;&#33258;26&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;Baichuan 2&#22312;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22402;&#30452;&#39046;&#22495;&#22914;&#21307;&#23398;&#21644;&#27861;&#24459;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.10305</link><description>&lt;p&gt;
Baichuan 2: &#24320;&#25918;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Baichuan 2: Open Large-scale Language Models. (arXiv:2309.10305v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10305
&lt;/p&gt;
&lt;p&gt;
Baichuan 2&#26159;&#19968;&#31995;&#21015;&#24320;&#25918;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25317;&#26377;70&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#35757;&#32451;&#33258;26&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;Baichuan 2&#22312;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22402;&#30452;&#39046;&#22495;&#22914;&#21307;&#23398;&#21644;&#27861;&#24459;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20165;&#26377;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#20943;&#23569;&#20102;&#23545;&#24191;&#27867;&#29305;&#24449;&#24037;&#31243;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24378;&#22823;&#30340;LLMs&#26159;&#23553;&#38381;&#28304;&#20195;&#30721;&#30340;&#65292;&#25110;&#32773;&#22312;&#38500;&#20102;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#36825;&#31687;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Baichuan 2&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#21547;70&#20159;&#21644;130&#20159;&#20010;&#21442;&#25968;&#65292;&#20351;&#29992;26&#19975;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#12290;Baichuan 2&#22312;MMLU&#12289;CMMLU&#12289;GSM8K&#21644;HumanEval&#31561;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#20854;&#20182;&#30456;&#21516;&#35268;&#27169;&#30340;&#24320;&#28304;&#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#12290;&#27492;&#22806;&#65292;Baichuan 2&#22312;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#22402;&#30452;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;&#25152;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#26816;&#26597;&#28857;&#65292;&#20197;&#20351;&#30740;&#31350;&#30028;&#26356;&#22909;&#22320;&#29702;&#35299;Baichuan 2&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(&#22914;GPT3.5)&#21487;&#20197;&#35299;&#20915;&#21644;&#35299;&#37322;&#29289;&#29702;&#35789;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#35745;&#31639;&#21644;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#35299;&#20915;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#33021;&#22815;&#24635;&#32467;&#28041;&#21450;&#30340;&#30693;&#35782;&#12289;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#21019;&#36896;&#26032;&#30340;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08182</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#21644;&#35299;&#37322;&#29289;&#29702;&#35789;&#38382;&#39064;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level. (arXiv:2309.08182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(&#22914;GPT3.5)&#21487;&#20197;&#35299;&#20915;&#21644;&#35299;&#37322;&#29289;&#29702;&#35789;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#35745;&#31639;&#21644;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#35299;&#20915;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#33021;&#22815;&#24635;&#32467;&#28041;&#21450;&#30340;&#30693;&#35782;&#12289;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#21019;&#36896;&#26032;&#30340;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22522;&#20110;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#19981;&#20165;&#21487;&#20197;&#35299;&#20915;&#32431;&#25968;&#23398;&#39064;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#29289;&#29702;&#35789;&#38382;&#39064;-&#21363;&#22522;&#20110;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#35745;&#31639;&#21644;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#27880;&#37322;&#20102;&#31532;&#19968;&#20010;&#29289;&#29702;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;-PhysQA&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;1000&#20010;&#21021;&#20013;&#29289;&#29702;&#35789;&#38382;&#39064;&#65288;&#21253;&#25324;&#36816;&#21160;&#23398;&#12289;&#36136;&#37327;&#21644;&#23494;&#24230;&#12289;&#21147;&#23398;&#12289;&#28909;&#23398;&#21644;&#30005;&#23398;&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;GPT3.5&#26469;&#29983;&#25104;&#36825;&#20123;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#21457;&#29616;GPT3.5&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#19978;&#33258;&#21160;&#35299;&#20915;49.3%&#30340;&#38382;&#39064;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#19978;&#21017;&#20026;73.2%&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#31867;&#20284;&#38382;&#39064;&#21450;&#20854;&#31572;&#26696;&#20316;&#20026;&#25552;&#31034;&#65292;LLM&#21487;&#20197;&#35299;&#20915;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#22522;&#30784;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;&#38500;&#20102;&#33258;&#21160;&#35299;&#20915;&#38382;&#39064;&#65292;GPT3.5&#36824;&#21487;&#20197;&#24635;&#32467;&#38382;&#39064;&#28041;&#21450;&#30340;&#30693;&#35782;&#25110;&#20027;&#39064;&#65292;&#29983;&#25104;&#30456;&#20851;&#35299;&#37322;&#65292;&#24182;&#26681;&#25454;&#36755;&#20837;&#38382;&#39064;&#32508;&#21512;&#20986;&#26032;&#30340;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work demonstrates that large language model (LLM) pre-trained on texts can not only solve pure math word problems, but also physics word problems-problems to be solved by calculation and inference based on some prior physical knowledge. We collect and annotate the first physics word problem dataset-PhysQA, which contains over 1000 junior high school physics word problems (on Kinematics, Mass&amp;Density, Mechanics, Heat, Electricity). Then we use OpenAI' s GPT3.5 to generate the answer of these problems and found that GPT3.5 could automatically solve 49.3% of the problems on zero-shot learning and 73.2% on few-shot learning. This result show that by using similar problem and its answer as prompt, LLM could solve elementary physics word problems approaching human level. Besides automatically solving problems, GPT3.5 could also summarize the knowledge or topic examined by the problem, generate the relevant explanation, and synthesis new physics word problems according tothe input problem
&lt;/p&gt;</description></item><item><title>PromptASR&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#25552;&#31034;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#25511;&#39118;&#26684;&#30340;&#35821;&#22659;&#21270;&#35821;&#38899;&#36716;&#24405;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#21069;&#19968;&#35805;&#35821;&#30340;&#30495;&#23454;&#25991;&#26412;&#20316;&#20026;&#20869;&#23481;&#25552;&#31034;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;ASR&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#38405;&#35835;&#22270;&#20070;&#25968;&#25454;&#38598;&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;21.9&#65285;&#21644;6.8&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#37319;&#29992;&#21333;&#35789;&#32423;&#20559;&#32622;&#21015;&#34920;&#20316;&#20026;&#25552;&#31034;&#26469;&#25552;&#39640;&#23545;&#32597;&#35265;&#21333;&#35789;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#20351;&#29992;&#39069;&#22806;&#30340;&#26679;&#24335;&#25552;&#31034;&#26469;&#24341;&#23548;ASR&#31995;&#32479;&#36755;&#20986;&#19981;&#21516;&#39118;&#26684;&#30340;&#36716;&#24405;&#12290;</title><link>http://arxiv.org/abs/2309.07414</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#25511;&#39118;&#26684;&#30340;&#35821;&#22659;&#21270;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;PromptASR
&lt;/p&gt;
&lt;p&gt;
PromptASR for contextualized ASR with controllable style. (arXiv:2309.07414v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07414
&lt;/p&gt;
&lt;p&gt;
PromptASR&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#25552;&#31034;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#25511;&#39118;&#26684;&#30340;&#35821;&#22659;&#21270;&#35821;&#38899;&#36716;&#24405;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#21069;&#19968;&#35805;&#35821;&#30340;&#30495;&#23454;&#25991;&#26412;&#20316;&#20026;&#20869;&#23481;&#25552;&#31034;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;ASR&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#38405;&#35835;&#22270;&#20070;&#25968;&#25454;&#38598;&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;21.9&#65285;&#21644;6.8&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#37319;&#29992;&#21333;&#35789;&#32423;&#20559;&#32622;&#21015;&#34920;&#20316;&#20026;&#25552;&#31034;&#26469;&#25552;&#39640;&#23545;&#32597;&#35265;&#21333;&#35789;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#20351;&#29992;&#39069;&#22806;&#30340;&#26679;&#24335;&#25552;&#31034;&#26469;&#24341;&#23548;ASR&#31995;&#32479;&#36755;&#20986;&#19981;&#21516;&#39118;&#26684;&#30340;&#36716;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#65292;&#25552;&#31034;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#20027;&#39064;&#25110;&#36923;&#36753;&#20851;&#31995;&#31561;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptASR&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#25552;&#31034;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;E2E ASR&#65289;&#31995;&#32479;&#20013;&#65292;&#20197;&#23454;&#29616;&#20855;&#26377;&#21487;&#25511;&#39118;&#26684;&#30340;&#35821;&#22659;&#21270;&#35821;&#38899;&#36716;&#24405;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#19987;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#23545;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#36328;&#20004;&#31181;&#27169;&#24577;&#30340;&#29305;&#24449;&#20132;&#20114;&#23558;&#32534;&#30721;&#27880;&#20837;&#21040;&#35821;&#38899;&#32534;&#30721;&#22120;&#20013;&#12290;&#24403;&#20351;&#29992;&#21069;&#38754;&#35805;&#35821;&#30340;&#30495;&#23454;&#25991;&#26412;&#20316;&#20026;&#20869;&#23481;&#25552;&#31034;&#26102;&#65292;&#19982;&#22522;&#32447;ASR&#31995;&#32479;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#38405;&#35835;&#22270;&#20070;&#25968;&#25454;&#38598;&#21644;&#20869;&#37096;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;21.9&#65285;&#21644;6.8&#65285;&#30340;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;&#31995;&#32479;&#36824;&#21487;&#20197;&#37319;&#29992;&#21333;&#35789;&#32423;&#20559;&#32622;&#21015;&#34920;&#20316;&#20026;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;&#23545;&#32597;&#35265;&#21333;&#35789;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#36824;&#21487;&#20197;&#32473;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#20379;&#39069;&#22806;&#30340;&#26679;&#24335;&#25552;&#31034;&#65292;&#24182;&#24341;&#23548;ASR&#31995;&#32479;&#36755;&#20986;&#19981;&#21516;&#39118;&#26684;&#30340;&#36716;&#24405;&#12290;&#20195;&#30721;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompts are crucial to large language models as they provide context information such as topic or logical relationships. Inspired by this, we propose PromptASR, a framework that integrates prompts in end-to-end automatic speech recognition (E2E ASR) systems to achieve contextualized ASR with controllable style of transcriptions. Specifically, a dedicated text encoder encodes the text prompts and the encodings are injected into the speech encoder by cross-attending the features from two modalities. When using the ground truth text from preceding utterances as content prompt, the proposed system achieves 21.9% and 6.8% relative word error rate reductions on a book reading dataset and an in-house dataset compared to a baseline ASR system. The system can also take word-level biasing lists as prompt to improve recognition accuracy on rare words. An additional style prompt can be given to the text encoder and guide the ASR system to output different styles of transcriptions. The code is avai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.15363</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#25991;&#26412;&#21040;SQL&#30340;&#30740;&#31350;&#65306;&#19968;&#20010;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#19968;&#31181;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#22522;&#20934;&#38459;&#30861;&#20102;&#35774;&#35745;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#21644;&#24191;&#27867;&#30340;&#27604;&#36739;&#65292;&#21253;&#25324;&#38382;&#39064;&#34920;&#31034;&#12289;&#31034;&#20363;&#36873;&#25321;&#21644;&#31034;&#20363;&#32452;&#32455;&#65292;&#24182;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#35814;&#32454;&#38416;&#36848;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#65292;&#36798;&#21040;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#26438;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24378;&#35843;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#65292;&#24182;&#22312;&#27492;&#24230;&#37327;&#19979;&#27604;&#36739;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24320;&#28304;LLMs&#65292;&#24182;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#30417;&#30563;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborates their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. Additionally, we investigate open-source LLMs in in-context learning, and further enhance their performance with task-specific superv
&lt;/p&gt;</description></item><item><title>MT4CrossOIE&#26159;&#19968;&#31181;&#22810;&#38454;&#27573;&#35843;&#20248;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#35821;&#31181;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#12290;&#23427;&#36890;&#36807;&#21521;&#20849;&#20139;&#27169;&#22411;&#27880;&#20837;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#20302;&#31209;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#36827;&#34892;&#27169;&#22411;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2308.06552</link><description>&lt;p&gt;
MT4CrossOIE: &#22810;&#38454;&#27573;&#35843;&#20248;&#29992;&#20110;&#36328;&#35821;&#31181;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
MT4CrossOIE: Multi-stage Tuning for Cross-lingual Open Information Extraction. (arXiv:2308.06552v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06552
&lt;/p&gt;
&lt;p&gt;
MT4CrossOIE&#26159;&#19968;&#31181;&#22810;&#38454;&#27573;&#35843;&#20248;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#35821;&#31181;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#12290;&#23427;&#36890;&#36807;&#21521;&#20849;&#20139;&#27169;&#22411;&#27880;&#20837;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#20302;&#31209;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#36827;&#34892;&#27169;&#22411;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#31181;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#26088;&#22312;&#20174;&#22810;&#35821;&#35328;&#30340;&#21407;&#22987;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#20849;&#20139;&#30340;&#36328;&#35821;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#35821;&#35328;&#65292;&#20294;&#26410;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#29305;&#23450;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MT4CrossIE&#30340;&#26377;&#25928;&#22810;&#38454;&#27573;&#35843;&#20248;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21521;&#20849;&#20139;&#27169;&#22411;&#27880;&#20837;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#26469;&#22686;&#24378;&#36328;&#35821;&#31181;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#22312;&#22266;&#23450;&#32534;&#30721;&#22120;&#20013;&#35843;&#25972;&#36328;&#35821;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20849;&#20139;&#35821;&#20041;&#31354;&#38388;&#65288;&#20363;&#22914;&#23884;&#20837;&#30697;&#38453;&#65289;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#20248;&#21270;&#20854;&#20182;&#32452;&#20214;&#12290;&#32463;&#36807;&#36275;&#22815;&#30340;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#28151;&#21512;LoRAs&#20248;&#21270;&#22810;&#20010;&#39069;&#22806;&#30340;&#20302;&#31209;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#65292;&#20197;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#36328;&#35821;&#31181;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#38454;&#27573;&#25552;&#31034;&#26469;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27880;&#37322;&#22810;&#35821;&#31181;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual open information extraction aims to extract structured information from raw text across multiple languages. Previous work uses a shared cross-lingual pre-trained model to handle the different languages but underuses the potential of the language-specific representation. In this paper, we propose an effective multi-stage tuning framework called MT4CrossIE, designed for enhancing cross-lingual open information extraction by injecting language-specific knowledge into the shared model. Specifically, the cross-lingual pre-trained model is first tuned in a shared semantic space (e.g., embedding matrix) in the fixed encoder and then other components are optimized in the second stage. After enough training, we freeze the pre-trained model and tune the multiple extra low-rank language-specific modules using mixture-of-LoRAs for model-based cross-lingual transfer. In addition, we leverage two-stage prompting to encourage the large language model (LLM) to annotate the multi-lingual 
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20174;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#21644;&#27010;&#24565;&#22270;&#12290;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.17089</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Concept-Oriented Deep Learning with Large Language Models. (arXiv:2306.17089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20174;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#21644;&#27010;&#24565;&#22270;&#12290;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#25991;&#26412;&#29983;&#25104;&#21644;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#23427;&#20204;&#20063;&#26159;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#65288;CODL&#65289;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#21069;&#25552;&#26159;LLMs&#35201;&#29702;&#35299;&#27010;&#24565;&#24182;&#30830;&#20445;&#27010;&#24565;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#21450;LLMs&#22312;CODL&#20013;&#30340;&#20027;&#35201;&#29992;&#36884;&#65292;&#21253;&#25324;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27010;&#24565;&#12289;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27010;&#24565;&#22270;&#21644;&#27010;&#24565;&#23398;&#20064;&#12290;&#20154;&#31867;&#30693;&#35782;&#21253;&#25324;&#31526;&#21495;&#65288;&#27010;&#24565;&#24615;&#65289;&#30693;&#35782;&#21644;&#20855;&#20307;&#65288;&#24863;&#24615;&#65289;&#30693;&#35782;&#12290;&#32780;&#20165;&#25991;&#26412;&#30340;LLMs&#21482;&#33021;&#34920;&#31034;&#31526;&#21495;&#65288;&#27010;&#24565;&#24615;&#65289;&#30693;&#35782;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22810;&#27169;&#24577;LLMs&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#30693;&#35782;&#30340;&#23436;&#25972;&#33539;&#22260;&#65288;&#27010;&#24565;&#24615;&#21644;&#24863;&#24615;&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#35270;&#35273;-&#35821;&#35328;LLMs&#20013;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#36825;&#26159;&#26368;&#37325;&#35201;&#30340;&#22810;&#27169;&#24577;LLMs&#65292;&#24182;&#20171;&#32461;&#20102;&#23427;&#20204;&#22312;CODL&#20013;&#30340;&#20027;&#35201;&#29992;&#36884;&#65292;&#21253;&#25324;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#12289;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been successfully used in many natural-language tasks and applications including text generation and AI chatbots. They also are a promising new technology for concept-oriented deep learning (CODL). However, the prerequisite is that LLMs understand concepts and ensure conceptual consistency. We discuss these in this paper, as well as major uses of LLMs for CODL including concept extraction from text, concept graph extraction from text, and concept learning. Human knowledge consists of both symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal LLMs, on the other hand, are capable of representing the full range (conceptual and sensory) of human knowledge. We discuss conceptual understanding in visual-language LLMs, the most important multimodal LLMs, and major uses of them for CODL including concept extraction from image, concept graph extraction from image
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#25968;&#25454;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#22686;&#24378;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#23545;&#35805;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;&#26550;&#26500;&#36827;&#34892;&#22810;&#35821;&#35328;&#23545;&#40784;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14949</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#20013;&#30340;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Data Augmentation for Document-grounded Dialog Systems in Low Resource Languages. (arXiv:2305.14949v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#25968;&#25454;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#22686;&#24378;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#23545;&#35805;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;&#26550;&#26500;&#36827;&#34892;&#22810;&#35821;&#35328;&#23545;&#40784;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#26469;&#22686;&#24378;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#23545;&#35805;&#29983;&#25104;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27969;&#31243;CLEM&#65288;&#36328;&#35821;&#35328;&#22686;&#24378;&#27169;&#22411;&#65289;&#65292;&#21253;&#25324;&#23545;&#25239;&#35757;&#32451;&#30340;&#26816;&#32034;&#22120;&#21644;&#37325;&#26032;&#25490;&#24207;&#22120;&#20197;&#21450;&#34701;&#21512;&#35299;&#30721;&#22120;&#30340;&#29983;&#25104;&#22120;Fid&#65288;fusion-in-decoder&#65289;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#32763;&#35793;&#35757;&#32451;&#25968;&#25454;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#22312;DialDoc 2023&#31454;&#36187;&#20013;&#33719;&#24471;&#31532;&#22235;&#21517;&#12290;&#22240;&#27492;&#65292;CLEM&#21487;&#20197;&#20316;&#20026;&#35299;&#20915;DGDS&#20013;&#36164;&#28304;&#31232;&#32570;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#20026;&#22810;&#35821;&#35328;&#23545;&#40784;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework to address the issue of data scarcity in Document-Grounded Dialogue Systems(DGDS). Our model leverages high-resource languages to enhance the capability of dialogue generation in low-resource languages. Specifically, We present a novel pipeline CLEM (Cross-Lingual Enhanced Model) including adversarial training retrieval (Retriever and Re-ranker), and Fid (fusion-in-decoder) generator. To further leverage high-resource language, we also propose an innovative architecture to conduct alignment across different languages with translated training. Extensive experiment results demonstrate the effectiveness of our model and we achieved 4th place in the DialDoc 2023 Competition. Therefore, CLEM can serve as a solution to resource scarcity in DGDS and provide useful guidance for multi-lingual alignment tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#23545;GPT-3.5&#12289;GPT-4&#21644;BARD&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#38646;&#26679;&#26412;&#24773;&#22659;&#20013;&#65292;ChatGPT-4&#34920;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;GPT-4&#30456;&#23545;&#20110;GPT-3.5&#30340;&#20248;&#21183;&#21487;&#33021;&#26159;&#30001;&#20854;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;NLP&#25928;&#29575;&#25152;&#24341;&#36215;&#30340;&#65292;&#20294;&#23545;&#20110;BARD&#26469;&#35828;&#24182;&#19981;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#36825;&#19977;&#20010;&#27169;&#22411;&#22312;&#24402;&#32435;&#12289;&#25968;&#23398;&#21644;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.12477</link><description>&lt;p&gt;
GPT-3.5&#12289;GPT-4&#36824;&#26159;BARD&#65311;&#23545;LLM&#22312;&#38646;&#26679;&#26412;&#24773;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#36890;&#36807;&#25552;&#31034;&#25552;&#21319;&#24615;&#33021;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts. (arXiv:2305.12477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#23545;GPT-3.5&#12289;GPT-4&#21644;BARD&#36827;&#34892;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#38646;&#26679;&#26412;&#24773;&#22659;&#20013;&#65292;ChatGPT-4&#34920;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;GPT-4&#30456;&#23545;&#20110;GPT-3.5&#30340;&#20248;&#21183;&#21487;&#33021;&#26159;&#30001;&#20854;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;NLP&#25928;&#29575;&#25152;&#24341;&#36215;&#30340;&#65292;&#20294;&#23545;&#20110;BARD&#26469;&#35828;&#24182;&#19981;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#36825;&#19977;&#20010;&#27169;&#22411;&#22312;&#24402;&#32435;&#12289;&#25968;&#23398;&#21644;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#23384;&#22312;&#30528;&#24403;&#21069;&#30340;&#28909;&#35758;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#21313;&#19968;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#19981;&#21516;&#30340;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#24443;&#24213;&#25216;&#26415;&#35780;&#20272;&#65292;&#30740;&#31350;&#20102;GPT-3.5&#12289;GPT-4&#21644;BARD&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#22312;&#38646;&#26679;&#26412;&#24773;&#22659;&#19979;&#65292;ChatGPT-4&#30456;&#23545;&#20110;ChatGPT-3.5&#21644;BARD&#22312;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;GPT-4&#30456;&#23545;&#20110;GPT-3.5&#30340;&#20248;&#21183;&#21487;&#33021;&#21487;&#20197;&#36890;&#36807;&#20854;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;NLP&#25928;&#29575;&#26469;&#35299;&#37322;&#65292;&#20294;&#23545;&#20110;BARD&#26469;&#35828;&#24182;&#19981;&#26126;&#26174;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#19977;&#20010;&#27169;&#22411;&#22312;&#24402;&#32435;&#12289;&#25968;&#23398;&#21644;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24378;&#21270;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#23545;&#36825;&#19977;&#20010;&#27169;&#22411;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#35814;&#32454;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#24037;&#31243;&#21270;&#30340;&#25552;&#31034;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable performance on various Natural Language Processing (NLP) tasks. However, there is a current hot debate regarding their reasoning capacity. In this paper, we examine the performance of GPT-3.5, GPT-4, and BARD models, by performing a thorough technical evaluation on different reasoning tasks across eleven distinct datasets. Our paper provides empirical evidence showcasing the superior performance of ChatGPT-4 in comparison to both ChatGPT-3.5 and BARD in zero-shot setting throughout almost all evaluated tasks. While the superiority of GPT-4 compared to GPT-3.5 might be explained by its larger size and NLP efficiency, this was not evident for BARD. We also demonstrate that the three models show limited proficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks. To bolster our findings, we present a detailed and comprehensive analysis of the results from these three models. Furthermore, we propose a set of engineered prompt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;MasakhaNEWS&#65292;&#23427;&#26159;&#19968;&#20010;&#35206;&#30422;16&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#26032;&#38395;&#20027;&#39064;&#20998;&#31867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#35780;&#20272;&#22522;&#32447;&#27169;&#22411;&#22806;&#65292;&#36824;&#25506;&#32034;&#20102;&#36866;&#29992;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20840;&#38754;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.09972</link><description>&lt;p&gt;
MasakhaNEWS&#65306;&#38750;&#27954;&#35821;&#35328;&#26032;&#38395;&#20027;&#39064;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MasakhaNEWS: News Topic Classification for African languages. (arXiv:2304.09972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09972
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;MasakhaNEWS&#65292;&#23427;&#26159;&#19968;&#20010;&#35206;&#30422;16&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#26032;&#38395;&#20027;&#39064;&#20998;&#31867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#35780;&#20272;&#22522;&#32447;&#27169;&#22411;&#22806;&#65292;&#36824;&#25506;&#32034;&#20102;&#36866;&#29992;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20840;&#38754;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#35206;&#30422;&#22810;&#20010;NLP&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#38750;&#27954;&#35821;&#35328;&#22312;NLP&#30740;&#31350;&#20013;&#20005;&#37325;&#21463;&#21040;&#24573;&#35270;&#12290;&#34429;&#28982;&#23384;&#22312;&#19968;&#20123;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;NLP&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#26426;&#22120;&#32763;&#35793;&#65289;&#20855;&#26377;&#35206;&#30422;&#22810;&#20010;&#22320;&#29702;&#21644;&#20998;&#31867;&#22810;&#26679;&#30340;&#38750;&#27954;&#35821;&#35328;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;MasakhaNEWS - &#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#28085;&#30422;&#38750;&#27954;&#24191;&#27867;&#20351;&#29992;&#30340;16&#31181;&#35821;&#35328;&#30340;&#26032;&#38395;&#20027;&#39064;&#20998;&#31867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#24494;&#35843;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#26469;&#35780;&#20272;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#20123;&#36866;&#29992;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#24494;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20363;&#22914;&#36328;&#35821;&#35328;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;&#22914;MAD-X&#65289;&#12289;&#27169;&#24335;&#21033;&#29992;&#35757;&#32451;&#65288;PET&#65289;&#12289;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#21644;&#26080;&#25552;&#31034;&#21477;&#23376;&#35757;&#32451;&#65288;ELECTRA&#65289;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
African languages are severely under-represented in NLP research due to lack of datasets covering several NLP tasks. While there are individual language specific datasets that are being expanded to different tasks, only a handful of NLP tasks (e.g. named entity recognition and machine translation) have standardized benchmark datasets covering several geographical and typologically-diverse African languages. In this paper, we develop MasakhaNEWS -- a new benchmark dataset for news topic classification covering 16 languages widely spoken in Africa. We provide an evaluation of baseline models by training classical machine learning models and fine-tuning several language models. Furthermore, we explore several alternatives to full fine-tuning of language models that are better suited for zero-shot and few-shot learning such as cross-lingual parameter-efficient fine-tuning (like MAD-X), pattern exploiting training (PET), prompting language models (like ChatGPT), and prompt-free sentence tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;ChatGPT&#27169;&#22411;&#30340;&#38382;&#31572;&#31995;&#32479;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#20998;&#31867;&#26694;&#26550;&#23545;&#28508;&#22312;&#30340;&#38382;&#39064;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#40657;&#30418;&#27979;&#35797;&#35268;&#33539;CheckList&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07992</link><description>&lt;p&gt;
&#35780;&#20272; ChatGPT &#20316;&#20026;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions. (arXiv:2303.07992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;ChatGPT&#27169;&#22411;&#30340;&#38382;&#31572;&#31995;&#32479;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#20998;&#31867;&#26694;&#26550;&#23545;&#28508;&#22312;&#30340;&#38382;&#39064;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#40657;&#30418;&#27979;&#35797;&#35268;&#33539;CheckList&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT &#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#23616;&#38480;&#24615;&#20173;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#12290;&#30001;&#20110; ChatGPT &#35206;&#30422;&#32500;&#22522;&#30334;&#31185;&#31561;&#36164;&#28304;&#24182;&#25903;&#25345;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#22240;&#27492;&#23427;&#24341;&#36215;&#20102;&#20316;&#20026;&#20256;&#32479;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#27169;&#22411;&#26367;&#20195;&#21697;&#30340;&#20851;&#27880;&#12290;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#26159; KBQA &#30340;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#20840;&#38754;&#27979;&#35797;&#20102;&#27169;&#22411;&#22312;&#35821;&#20041;&#35299;&#26512;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272; ChatGPT &#20316;&#20026;&#19968;&#20010;&#20351;&#29992;&#33258;&#24049;&#30693;&#35782;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#38382;&#31572;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#20854;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#22797;&#26434;&#38382;&#39064;&#30340;&#28508;&#22312;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#26631;&#31614;&#25551;&#36848;&#27599;&#20010;&#27979;&#35797;&#38382;&#39064;&#65292;&#20197;&#35782;&#21035;&#32452;&#21512;&#25512;&#29702;&#12290;&#26681;&#25454; Ribeir &#25552;&#20986;&#30340; CheckList &#30340;&#40657;&#30418;&#27979;&#35797;&#35268;&#33539;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a powerful large language model (LLM) that has made remarkable progress in natural language understanding. Nevertheless, the performance and limitations of the model still need to be extensively evaluated. As ChatGPT covers resources such as Wikipedia and supports natural language question answering, it has garnered attention as a potential replacement for traditional knowledge based question answering (KBQA) models. Complex question answering is a challenge task of KBQA, which comprehensively tests the ability of models in semantic parsing and reasoning. To assess the performance of ChatGPT as a question answering system (QAS) using its own knowledge, we present a framework that evaluates its ability to answer complex questions. Our approach involves categorizing the potential features of complex questions and describing each test question with multiple labels to identify combinatorial reasoning. Following the black-box testing specifications of CheckList proposed by Ribeir
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#31070;&#32463;&#35828;&#35805;&#20154;&#23884;&#20837;&#24212;&#29992;&#20110;ASR&#31995;&#32479;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#23884;&#20837;&#25552;&#21462;&#27969;&#31243;&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#22768;&#23398;&#27169;&#22411;&#21644;&#28155;&#21152;&#31070;&#32463;&#35828;&#35805;&#20154;&#23884;&#20837;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;WER&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.04571</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#25913;&#36827;&#31070;&#32463;&#35828;&#35805;&#20154;&#23884;&#20837;&#29992;&#20110;ASR
&lt;/p&gt;
&lt;p&gt;
Analyzing And Improving Neural Speaker Embeddings for ASR. (arXiv:2301.04571v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#31070;&#32463;&#35828;&#35805;&#20154;&#23884;&#20837;&#24212;&#29992;&#20110;ASR&#31995;&#32479;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#23884;&#20837;&#25552;&#21462;&#27969;&#31243;&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#22768;&#23398;&#27169;&#22411;&#21644;&#28155;&#21152;&#31070;&#32463;&#35828;&#35805;&#20154;&#23884;&#20837;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;WER&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35828;&#35805;&#20154;&#23884;&#20837;&#36890;&#36807;DNN&#27169;&#22411;&#32534;&#30721;&#35828;&#35805;&#32773;&#30340;&#35821;&#38899;&#29305;&#24449;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#35828;&#35805;&#32773;&#39564;&#35777;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#31070;&#32463;&#35828;&#35805;&#20154;&#23884;&#20837;&#22312;ASR&#31995;&#32479;&#20013;&#30340;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#31070;&#32463;&#35828;&#35805;&#20154;&#23884;&#20837;&#38598;&#25104;&#21040;&#22522;&#20110;Conformer&#30340;&#28151;&#21512;HMM ASR&#31995;&#32479;&#20013;&#65292;&#24182;&#36827;&#34892;&#25913;&#36827;&#12290;&#23545;&#20110;ASR&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#23884;&#20837;&#25552;&#21462;&#27969;&#31243;&#65292;&#24182;&#32467;&#21512;&#21152;&#26435;&#31616;&#21333;&#21152;&#27861;&#38598;&#25104;&#26041;&#27861;&#65292;&#20351;&#24471;x-vector&#21644;c-vector&#36798;&#21040;&#19982;i-vector&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27604;&#36739;&#21644;&#20998;&#26512;&#19981;&#21516;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;newbob&#23398;&#20064;&#29575;&#35843;&#24230;&#36716;&#25442;&#20026;&#21333;&#21608;&#26399;&#23398;&#20064;&#35843;&#24230;&#26469;&#25913;&#36827;&#22768;&#23398;&#27169;&#22411;&#65292;&#22312;Switchboard&#19978;&#30456;&#23545;WER&#38477;&#20302;&#20102;&#32422;3%&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;17%&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#28155;&#21152;&#31070;&#32463;&#35828;&#35805;&#20154;&#23884;&#20837;&#65292;&#25105;&#20204;&#22312;Hub5'00&#19978;&#33719;&#24471;&#20102;&#39069;&#22806;&#32422;3%&#30340;&#30456;&#23545;WER&#25913;&#36827;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#22522;&#20110;Conformer&#30340;&#28151;&#21512;A
&lt;/p&gt;
&lt;p&gt;
Neural speaker embeddings encode the speaker's speech characteristics through a DNN model and are prevalent for speaker verification tasks. However, few studies have investigated the usage of neural speaker embeddings for an ASR system. In this work, we present our efforts w.r.t integrating neural speaker embeddings into a conformer based hybrid HMM ASR system. For ASR, our improved embedding extraction pipeline in combination with the Weighted-Simple-Add integration method results in x-vector and c-vector reaching on par performance with i-vectors. We further compare and analyze different speaker embeddings. We present our acoustic model improvements obtained by switching from newbob learning rate schedule to one cycle learning schedule resulting in a ~3% relative WER reduction on Switchboard, additionally reducing the overall training time by 17%. By further adding neural speaker embeddings, we gain additional ~3% relative WER improvement on Hub5'00. Our best Conformer-based hybrid A
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19977;&#20803;&#32452;&#25277;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#26410;&#35265;&#20851;&#31995;&#30340;&#25277;&#21462;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#19988;&#19968;&#33268;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10708</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#27169;&#26495;&#22635;&#20805;&#19979;&#30340;&#19977;&#20803;&#32452;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Triplet Extraction by Template Infilling. (arXiv:2212.10708v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19977;&#20803;&#32452;&#25277;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#26410;&#35265;&#20851;&#31995;&#30340;&#25277;&#21462;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#19988;&#19968;&#33268;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#20803;&#32452;&#25277;&#21462;&#30340;&#20219;&#21153;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#23454;&#20307;&#23545;&#21450;&#20854;&#23545;&#24212;&#30340;&#20851;&#31995;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#26041;&#27861;&#22312;&#29305;&#23450;&#30446;&#26631;&#20851;&#31995;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#25277;&#21462;&#27169;&#22411;&#65292;&#26080;&#27861;&#25552;&#21462;&#35757;&#32451;&#26102;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#20851;&#31995;&#12290;&#23558;&#27169;&#22411;&#25512;&#24191;&#21040;&#26410;&#35265;&#20851;&#31995;&#36890;&#24120;&#38656;&#35201;&#23545;&#24120;&#24120;&#19981;&#21487;&#38752;&#19988;&#22122;&#22768;&#36739;&#22823;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#19977;&#20803;&#32452;&#25552;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#25277;&#21462;&#27169;&#22411;&#20855;&#22791;&#38646;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#28040;&#38500;&#23545;&#39069;&#22806;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;ZETT&#65288;Zero-shot Triplet extraction by Template infilling&#65289;&#65292;&#23558;&#20219;&#21153;&#30446;&#26631;&#19982;&#29983;&#25104;&#24335;Transformer&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#23545;&#40784;&#65292;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#20851;&#31995;&#12290;&#22312;FewRel&#21644;Wiki-ZSL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ZETT&#26174;&#31034;&#20986;&#19968;&#33268;&#19988;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of triplet extraction aims to extract pairs of entities and their corresponding relations from unstructured text. Most existing methods train an extraction model on training data involving specific target relations, and are incapable of extracting new relations that were not observed at training time. Generalizing the model to unseen relations typically requires fine-tuning on synthetic training data which is often noisy and unreliable. We show that by reducing triplet extraction to a template infilling task over a pre-trained language model (LM), we can equip the extraction model with zero-shot learning capabilities and eliminate the need for additional training data. We propose a novel framework, ZETT (ZEro-shot Triplet extraction by Template infilling), that aligns the task objective to the pre-training objective of generative transformers to generalize to unseen relations. Experiments on FewRel and Wiki-ZSL datasets demonstrate that ZETT shows consistent and stable perform
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#21307;&#30103;&#39046;&#22495;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#30340;&#28151;&#21512;ASR&#31995;&#32479;&#65292;&#26088;&#22312;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#24182;&#20811;&#26381;&#35821;&#35328;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2210.13397</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#21307;&#30103;&#39046;&#22495;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#30340;&#28151;&#21512;ASR&#31995;&#32479;&#30340;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Development of Hybrid ASR Systems for Low Resource Medical Domain Conversational Telephone Speech. (arXiv:2210.13397v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#21307;&#30103;&#39046;&#22495;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#30340;&#28151;&#21512;ASR&#31995;&#32479;&#65292;&#26088;&#22312;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#24182;&#20811;&#26381;&#35821;&#35328;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#38556;&#30861;&#22312;&#25105;&#20204;&#26085;&#30410;&#36830;&#25509;&#21644;&#20840;&#29699;&#21270;&#30340;&#19990;&#30028;&#20013;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#22914;&#21307;&#38498;&#25110;&#24613;&#35786;&#23460;&#65292;&#27807;&#36890;&#22256;&#38590;&#21644;&#24310;&#35823;&#21487;&#33021;&#23548;&#33268;&#21307;&#30103;&#22833;&#35823;&#21644;&#38750;&#26368;&#20339;&#30340;&#24739;&#32773;&#25252;&#29702;&#12290;&#22312;HYKIST&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#24503;&#35821;&#21307;&#29983;&#21644;&#38463;&#25289;&#20271;&#35821;&#25110;&#36234;&#21335;&#35821;&#24739;&#32773;&#20043;&#38388;&#30340;&#20132;&#27969;&#12290;&#30446;&#21069;&#65292;&#21307;&#29983;&#21487;&#20197;&#25171;&#30005;&#35805;&#32473;Triaphon&#26381;&#21153;&#20197;&#33719;&#24471;&#26469;&#33258;&#32763;&#35793;&#21592;&#30340;&#24110;&#21161;&#65292;&#20197;&#20419;&#36827;&#27807;&#36890;&#12290;HYKIST&#30340;&#30446;&#26631;&#26159;&#20026;&#36890;&#24120;&#27809;&#26377;&#19987;&#19994;&#32972;&#26223;&#30340;&#21452;&#35821;&#32763;&#35793;&#21592;&#25552;&#20379;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#20197;&#25913;&#36827;&#24739;&#32773;&#25252;&#29702;&#24182;&#35299;&#20915;&#35821;&#35328;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#38024;&#23545;&#21307;&#30103;&#39046;&#22495;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#30340;ASR&#31995;&#32479;&#24320;&#21457;&#24037;&#20316;&#65292;&#28041;&#21450;&#20004;&#31181;&#35821;&#35328;&#23545;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#21508;&#31181;&#22768;&#23398;&#27169;&#22411;&#26550;&#26500;&#21644;&#26041;&#35328;&#24341;&#36215;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language barriers present a great challenge in our increasingly connected and global world. Especially within the medical domain, e.g. hospital or emergency room, communication difficulties and delays may lead to malpractice and non-optimal patient care. In the HYKIST project, we consider patient-physician communication, more specifically between a German-speaking physician and an Arabic- or Vietnamese-speaking patient. Currently, a doctor can call the Triaphon service to get assistance from an interpreter in order to help facilitate communication. The HYKIST goal is to support the usually non-professional bilingual interpreter with an automatic speech translation system to improve patient care and help overcome language barriers. In this work, we present our ASR system development efforts for this conversational telephone speech translation task in the medical domain for two languages pairs, data collection, various acoustic model architectures and dialect-induced difficulties.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#23558;&#38169;&#35823;&#20449;&#24687;&#27880;&#20837;&#38382;&#31572;&#31995;&#32479;&#30340;&#25915;&#20987;&#65292;&#24182;&#21457;&#29616;&#38382;&#31572;&#27169;&#22411;&#23545;&#20110;&#38169;&#35823;&#20449;&#24687;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#21363;&#20351;&#23569;&#37327;&#38169;&#35823;&#20449;&#24687;&#30340;&#27745;&#26579;&#20063;&#20250;&#23548;&#33268;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2110.07803</link><description>&lt;p&gt;
&#23558;&#38169;&#35823;&#20449;&#24687;&#27880;&#20837;&#36827;&#34892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Attacking Open-domain Question Answering by Injecting Misinformation. (arXiv:2110.07803v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.07803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#23558;&#38169;&#35823;&#20449;&#24687;&#27880;&#20837;&#38382;&#31572;&#31995;&#32479;&#30340;&#25915;&#20987;&#65292;&#24182;&#21457;&#29616;&#38382;&#31572;&#27169;&#22411;&#23545;&#20110;&#38169;&#35823;&#20449;&#24687;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#21363;&#20351;&#23569;&#37327;&#38169;&#35823;&#20449;&#24687;&#30340;&#27745;&#26579;&#20063;&#20250;&#23548;&#33268;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23459;&#20256;&#12289;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#20013;&#34394;&#20551;&#12289;&#19981;&#20934;&#30830;&#21644;&#35823;&#23548;&#24615;&#20449;&#24687;&#30340;&#22686;&#21152;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#31572;&#31995;&#32479;&#38754;&#20020;&#30528;&#22312;&#34987;&#38169;&#35823;&#20449;&#24687;&#27745;&#26579;&#30340;&#35821;&#22659;&#20013;&#32508;&#21512;&#21644;&#25512;&#29702;&#20197;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#32039;&#36843;&#24615;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#20351;&#38382;&#31572;&#31995;&#32479;&#23545;&#38169;&#35823;&#20449;&#24687;&#20855;&#22791;&#40065;&#26834;&#24615;&#65292;&#32780;&#36825;&#26159;&#20043;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#19968;&#20010;&#20027;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#27169;&#22411;&#23545;&#24102;&#26377;&#38169;&#35823;&#20449;&#24687;&#25991;&#26723;&#30340;&#35821;&#26009;&#24211;&#27745;&#26579;&#30340;&#25935;&#24863;&#24615;&#65292;&#26469;&#30740;&#31350;&#38169;&#35823;&#20449;&#24687;&#23545;&#38382;&#31572;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#25972;&#29702;&#20102;&#20154;&#24037;&#25776;&#20889;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#34394;&#20551;&#25991;&#26723;&#65292;&#23558;&#20854;&#27880;&#20837;&#21040;&#38382;&#31572;&#27169;&#22411;&#30340;&#35777;&#25454;&#35821;&#26009;&#24211;&#20013;&#65292;&#24182;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#23569;&#37327;&#30340;&#38169;&#35823;&#20449;&#24687;&#27745;&#26579;&#20063;&#20250;&#20351;&#38382;&#31572;&#27169;&#22411;&#33030;&#24369;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#20250;&#22823;&#24133;&#24230;&#19979;&#38477;&#12290;&#24403;&#31070;&#32463;&#27169;&#22411;&#25209;&#37327;&#29983;&#25104;&#34394;&#20551;&#25991;&#26723;&#26102;&#25110;&#25915;&#20987;&#32773;&#38024;&#23545;&#38382;&#31572;&#27169;&#22411;&#26102;&#65292;&#38169;&#35823;&#20449;&#24687;&#25915;&#20987;&#30340;&#23041;&#32961;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a rise in false, inaccurate, and misleading information in propaganda, news, and social media, real-world Question Answering (QA) systems face the challenges of synthesizing and reasoning over misinformation-polluted contexts to derive correct answers. This urgency gives rise to the need to make QA systems robust to misinformation, a topic previously unexplored. We study the risk of misinformation to QA models by investigating the sensitivity of open-domain QA models to corpus pollution with misinformation documents. We curate both human-written and model-generated false documents that we inject into the evidence corpus of QA models and assess the impact on the performance of these systems. Experiments show that QA models are vulnerable to even small amounts of evidence contamination brought by misinformation, with large absolute performance drops on all models. Misinformation attack brings more threat when fake documents are produced at scale by neural models or the attacker targ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#31243;&#24207;&#20026;&#36807;&#31243;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#25351;&#20196;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;&#24314;&#31435;&#23618;&#32423;&#27169;&#22359;&#21270;&#32593;&#32476;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#39044;&#27979;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2109.08214</link><description>&lt;p&gt;
&#20197;&#31243;&#24207;&#20026;&#36807;&#31243;&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#22788;&#22659;&#20195;&#29702;&#36827;&#34892;&#23618;&#32423;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Procedures as Programs: Hierarchical Control of Situated Agents through Natural Language. (arXiv:2109.08214v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.08214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#31243;&#24207;&#20026;&#36807;&#31243;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#25351;&#20196;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;&#24314;&#31435;&#23618;&#32423;&#27169;&#22359;&#21270;&#32593;&#32476;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#39044;&#27979;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#26500;&#24605;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#26102;&#65292;&#20182;&#20204;&#20250;&#20197;&#23618;&#27425;&#32467;&#26500;&#30340;&#26041;&#24335;&#36827;&#34892;&#65306;&#23558;&#39640;&#32423;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#23545;&#22788;&#22659;&#20195;&#29702;&#30340;&#25991;&#29486;&#20013;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#23558;&#35201;&#25191;&#34892;&#30340;&#36807;&#31243;&#35270;&#20026;&#31616;&#21333;&#21160;&#20316;&#30340;&#24179;&#22374;&#24207;&#21015;&#65292;&#25110;&#32773;&#26368;&#22810;&#21482;&#26377;&#27973;&#23618;&#30340;&#31243;&#24207;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31243;&#24207;&#20316;&#20026;&#36807;&#31243;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#32780;&#30452;&#35266;&#30340;&#34920;&#31034;&#23618;&#32423;&#36807;&#31243;&#30693;&#35782;&#20197;&#36827;&#34892;&#20195;&#29702;&#25351;&#20196;&#21644;&#25511;&#21046;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#27169;&#22359;&#21270;&#32593;&#32476;&#30340;&#24314;&#27169;&#33539;&#24335;&#65292;&#35813;&#32593;&#32476;&#30001;&#35268;&#21010;&#32773;&#21644;&#21453;&#24212;&#22120;&#32452;&#25104;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#39044;&#27979;&#65292;&#24182;&#25506;&#27979;&#29615;&#22659;&#20197;&#33719;&#21462;&#23436;&#25104;&#31243;&#24207;&#25191;&#34892;&#25152;&#38656;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;IQA&#21644;ALFRED&#25968;&#25454;&#38598;&#19978;&#23454;&#20363;&#21270;&#36825;&#20010;&#26694;&#26550;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36319;&#38543;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;&#21453;&#24212;&#24335;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;o...
&lt;/p&gt;
&lt;p&gt;
When humans conceive how to perform a particular task, they do so hierarchically: splitting higher-level tasks into smaller sub-tasks. However, in the literature on natural language (NL) command of situated agents, most works have treated the procedures to be executed as flat sequences of simple actions, or any hierarchies of procedures have been shallow at best. In this paper, we propose a formalism of procedures as programs, a powerful yet intuitive method of representing hierarchical procedural knowledge for agent command and control. We further propose a modeling paradigm of hierarchical modular networks, which consist of a planner and reactors that convert NL intents to predictions of executable programs and probe the environment for information necessary to complete the program execution. We instantiate this framework on the IQA and ALFRED datasets for NL instruction following. Our model outperforms reactive baselines by a large margin on both datasets. We also demonstrate that o
&lt;/p&gt;</description></item></channel></rss>