<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#20998;&#31867;&#26041;&#26696;&#23545;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01364</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;: &#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Continual Learning for Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01364
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#20998;&#31867;&#26041;&#26696;&#23545;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25351;&#20986;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#23548;&#33268;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19981;&#26131;&#39057;&#32321;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26356;&#26032;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#36171;&#20104;LLMs&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#20351;&#20854;&#19982;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#31867;&#30693;&#35782;&#20445;&#25345;&#21516;&#27493;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#25345;&#32493;&#23398;&#20064;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#37492;&#20110;LLMs&#30340;&#29420;&#29305;&#24615;&#65292;&#25105;&#20204;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#20998;&#31867;&#26041;&#26696;&#23545;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#28041;&#21450;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#35843;&#25972;&#21644;&#23545;&#40784;&#31561;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#25345;&#32493;&#23398;&#20064;&#19982;&#22312;&#35268;&#27169;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#31616;&#21333;&#36866;&#24212;&#26041;&#27861;&#20197;&#21450;&#20854;&#20182;&#22686;&#24378;&#31574;&#30053;(&#22914;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#27169;&#22411;&#32534;&#36753;)&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#23545;&#22522;&#20934;&#21644;&#35780;&#20272;&#30340;&#35752;&#35770;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#19968;&#37325;&#35201;&#20219;&#21153;&#38754;&#20020;&#30340;&#20960;&#20010;&#25361;&#25112;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;&#19987;&#29992;&#20998;&#35789;&#22120;&#65292;&#23545;&#20998;&#35789;&#22120;&#35774;&#35745;&#36827;&#34892;&#20102;&#28040;&#27602;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#20998;&#35789;&#22120;&#30340;&#22823;&#23567;&#12289;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#24314;&#35758;&#21644;&#20999;&#25442;&#20998;&#35789;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01035</link><description>&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#20998;&#35789;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Getting the most out of your tokenizer for pre-training and domain adaptation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;&#19987;&#29992;&#20998;&#35789;&#22120;&#65292;&#23545;&#20998;&#35789;&#22120;&#35774;&#35745;&#36827;&#34892;&#20102;&#28040;&#27602;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#20998;&#35789;&#22120;&#30340;&#22823;&#23567;&#12289;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#24314;&#35758;&#21644;&#20999;&#25442;&#20998;&#35789;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35789;&#26159;&#29616;&#20195;LLM&#20013;&#40092;&#20026;&#20154;&#30693;&#19988;&#24120;&#34987;&#24573;&#35270;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22823;&#22810;&#25968;&#24050;&#21457;&#34920;&#30340;&#20316;&#21697;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#37117;&#20351;&#29992;&#21516;&#19968;&#20010;&#20998;&#35789;&#22120;&#65292;&#36890;&#24120;&#26159;&#20174;&#21478;&#19968;&#20010;&#27169;&#22411;&#20511;&#29992;&#32780;&#26469;&#30340;&#65292;&#24182;&#27809;&#26377;&#36827;&#34892;&#28040;&#34701;&#25110;&#20998;&#26512;&#26469;&#20248;&#21270;&#20998;&#35789;&#12290;&#27492;&#22806;&#65292;&#22312;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#26102;&#65292;&#20998;&#35789;&#22120;&#36890;&#24120;&#20445;&#25345;&#19981;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#35789;&#22120;&#30340;&#22823;&#23567;&#12289;&#39044;&#26631;&#35760;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#30340;&#29983;&#25104;&#36895;&#24230;&#12289;&#26377;&#25928;&#19978;&#19979;&#25991;&#22823;&#23567;&#12289;&#20869;&#23384;&#20351;&#29992;&#21644;&#19979;&#28216;&#24615;&#33021;&#22343;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19987;&#29992;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#20998;&#35789;&#22120;&#65292;&#24182;&#23545;&#20998;&#35789;&#22120;&#35774;&#35745;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65288;&#22914;HumanEval&#21644;MBPP&#65289;&#20013;LLM&#24615;&#33021;&#24433;&#21709;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#65292;&#25552;&#20379;&#20102;&#20998;&#35789;&#22120;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#22312;&#39044;&#35757;&#32451;LLM&#20013;&#20999;&#25442;&#20998;&#35789;&#22120;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21644;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to 
&lt;/p&gt;</description></item><item><title>Edu-ConvoKit&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#30340;&#24320;&#28304;&#24211;&#65292;&#23427;&#35299;&#20915;&#20102;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#20998;&#26512;&#36164;&#28304;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25991;&#26723;&#21644;&#38468;&#21152;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.05111</link><description>&lt;p&gt;
Edu-ConvoKit:&#19968;&#31181;&#29992;&#20110;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#30340;&#24320;&#28304;&#24211;
&lt;/p&gt;
&lt;p&gt;
Edu-ConvoKit: An Open-Source Library for Education Conversation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05111
&lt;/p&gt;
&lt;p&gt;
Edu-ConvoKit&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#30340;&#24320;&#28304;&#24211;&#65292;&#23427;&#35299;&#20915;&#20102;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#20998;&#26512;&#36164;&#28304;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25991;&#26723;&#21644;&#38468;&#21152;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Edu-ConvoKit&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#30340;&#39044;&#22788;&#29702;&#12289;&#27880;&#37322;&#21644;&#20998;&#26512;&#30340;&#24320;&#28304;&#24211;&#12290;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#30340;&#20998;&#26512;&#36164;&#28304;&#31232;&#32570;&#65292;&#20351;&#24471;&#30740;&#31350;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#27492;&#38590;&#20197;&#33719;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;Edu-ConvoKit&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;Edu-ConvoKit&#26159;&#24320;&#28304;&#30340;&#65288;https://github.com/stanfordnlp/edu-convokit&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;pip&#36827;&#34892;&#23433;&#35013;&#65288;https://pypi.org/project/edu-convokit/&#65289;&#65292;&#24182;&#20855;&#26377;&#20840;&#38754;&#30340;&#25991;&#26723;&#65288;https://edu-convokit.readthedocs.io/en/latest/&#65289;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#35270;&#39057;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#35266;&#30475;&#65306;https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8-&#12290;&#25105;&#20204;&#36824;&#22312;GitHub&#20179;&#24211;&#20013;&#25552;&#20379;&#20102;&#38468;&#21152;&#36164;&#28304;&#65292;&#20363;&#22914;Edu-ConvoKit&#22312;&#19977;&#20010;&#19981;&#21516;&#25945;&#32946;&#25968;&#25454;&#38598;&#19978;&#30340;Colab&#24212;&#29992;&#31243;&#24207;&#20197;&#21450;Edu-ConvoKit&#30456;&#20851;&#35770;&#25991;&#30340;&#23384;&#20648;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Edu-ConvoKit, an open-source library designed to handle pre-processing, annotation and analysis of conversation data in education. Resources for analyzing education conversation data are scarce, making the research challenging to perform and therefore hard to access. We address these challenges with Edu-ConvoKit. Edu-ConvoKit is open-source (https://github.com/stanfordnlp/edu-convokit ), pip-installable (https://pypi.org/project/edu-convokit/ ), with comprehensive documentation (https://edu-convokit.readthedocs.io/en/latest/ ). Our demo video is available at: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8- . We include additional resources, such as Colab applications of Edu-ConvoKit to three diverse education datasets and a repository of Edu-ConvoKit related papers, that can be found in our GitHub repository.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26089;&#26399;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;GRIT&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#20197;&#36866;&#24212;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05106</link><description>&lt;p&gt;
&#20351;&#29992;GRIT&#27169;&#22411;&#36827;&#34892;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Image captioning for Brazilian Portuguese using GRIT model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26089;&#26399;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;GRIT&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#20197;&#36866;&#24212;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#30340;&#26089;&#26399;&#24320;&#21457;&#24037;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;GRIT&#65288;&#22522;&#20110;&#32593;&#26684;&#21644;&#21306;&#22495;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;Transformer&#65289;&#27169;&#22411;&#26469;&#23436;&#25104;&#27492;&#24037;&#20316;&#12290;GRIT&#26159;&#19968;&#20010;&#20165;&#20351;&#29992;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#20004;&#20010;&#35270;&#35273;&#29305;&#24449;&#26469;&#29983;&#25104;&#26356;&#22909;&#30340;&#23383;&#24149;&#12290;GRIT&#26041;&#27861;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;GRIT&#27169;&#22411;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20197;&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents the early development of a model of image captioning for the Brazilian Portuguese language. We used the GRIT (Grid - and Region-based Image captioning Transformer) model to accomplish this work. GRIT is a Transformer-only neural architecture that effectively utilizes two visual features to generate better captions. The GRIT method emerged as a proposal to be a more efficient way to generate image captioning. In this work, we adapt the GRIT model to be trained in a Brazilian Portuguese dataset to have an image captioning method for the Brazilian Portuguese Language.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;AI&#31995;&#32479;&#33021;&#22815;&#26381;&#21153;&#20110;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#38656;&#27714;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#40784;&#23450;&#20041;&#21644;&#23454;&#29616;&#22810;&#20803;&#20027;&#20041;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#26469;&#35780;&#20272;&#21644;&#27979;&#35797;&#22810;&#20803;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05070</link><description>&lt;p&gt;
&#36890;&#24448;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
A Roadmap to Pluralistic Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05070
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;AI&#31995;&#32479;&#33021;&#22815;&#26381;&#21153;&#20110;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#38656;&#27714;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#40784;&#23450;&#20041;&#21644;&#23454;&#29616;&#22810;&#20803;&#20027;&#20041;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#26469;&#35780;&#20272;&#21644;&#27979;&#35797;&#22810;&#20803;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26435;&#21147;&#21644;&#26222;&#21450;&#31243;&#24230;&#30340;&#22686;&#21152;&#65292;&#35774;&#35745;&#33021;&#22815;&#20026;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#20154;&#26381;&#21153;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#20197;&#26381;&#21153;&#22810;&#20803;&#20154;&#31867;&#20215;&#20540;&#35266;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20855;&#20307;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#30830;&#23450;&#21644;&#24418;&#24335;&#21270;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#26041;&#24335;&#26469;&#23450;&#20041;&#21644;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#22810;&#20803;&#20027;&#20041;&#65306;1&#65289;Overton&#22810;&#20803;&#27169;&#22411;&#65292;&#23637;&#31034;&#21512;&#29702;&#21453;&#24212;&#30340;&#20809;&#35889;&#65307;2&#65289;&#21487;&#25805;&#25511;&#30340;&#22810;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#35843;&#25972;&#20197;&#21453;&#26144;&#29305;&#23450;&#30340;&#35266;&#28857;&#65307;3&#65289;&#20998;&#24067;&#22810;&#20803;&#27169;&#22411;&#65292;&#22312;&#20998;&#24067;&#20013;&#24456;&#22909;&#22320;&#26657;&#20934;&#32473;&#23450;&#20154;&#32676;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#21644;&#24418;&#24335;&#21270;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#65306;1&#65289;&#22810;&#30446;&#26631;&#22522;&#20934;&#65307;2&#65289;&#26435;&#34913;&#21487;&#25805;&#25511;&#22522;&#20934;&#65292;&#40723;&#21169;&#27169;&#22411;&#23545;&#20219;&#24847;&#26435;&#34913;&#36827;&#34892;&#35843;&#25972;&#65307;3&#65289;&#38506;&#23457;&#22242;&#22810;&#20803;&#22522;&#20934;&#65292;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#19981;&#21516;&#38506;&#23457;&#22242;&#30340;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly m
&lt;/p&gt;</description></item><item><title>SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2402.05044</link><description>&lt;p&gt;
SALAD-Bench: &#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#21270;&#21644;&#20840;&#38754;&#24615;&#23433;&#20840;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05044
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23433;&#20840;&#22522;&#20934;&#65292;&#31216;&#20026;SALAD-Bench&#12290;SALAD-Bench&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#36328;&#19977;&#20010;&#23618;&#27425;&#30340;&#32454;&#33268;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20934;&#12290;SALAD-Bench&#36890;&#36807;&#23545;&#26631;&#20934;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#65288;&#21253;&#25324;&#25915;&#20987;&#12289;&#38450;&#24481;&#20462;&#25913;&#21644;&#22810;&#39033;&#36873;&#25321;&#65289;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#26377;&#25928;&#31649;&#29702;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#26080;&#32541;&#21487;&#38752;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#22120;&#65306;&#22522;&#20110;LLM&#30340;MD-Judge&#65292;&#19987;&#27880;&#20110;&#25915;&#20987;&#22686;&#24378;&#26597;&#35810;&#30340;&#38382;&#31572;&#23545;&#35780;&#20272;&#12290;&#20197;&#19978;&#32452;&#20214;&#23558;SALAD-Bench&#20174;&#26631;&#20934;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#25193;&#23637;&#21040;&#20102;LLM&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#35780;&#20272;&#65292;&#30830;&#20445;&#20102;&#32852;&#21512;&#30446;&#26631;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;BERT&#30340;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#23545;&#26089;&#26399;&#29616;&#20195;&#33521;&#35821;&#21644;&#29616;&#20195;&#33521;&#35821;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#20013;&#30340;&#21382;&#21490;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.05034</link><description>&lt;p&gt;
BERT&#22914;&#20309;&#35828;&#33678;&#22763;&#27604;&#20122;&#33521;&#35821;&#65311;&#35780;&#20272;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21382;&#21490;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
How BERT Speaks Shakespearean English? Evaluating Historical Bias in Contextual Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;BERT&#30340;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#23545;&#26089;&#26399;&#29616;&#20195;&#33521;&#35821;&#21644;&#29616;&#20195;&#33521;&#35821;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#20013;&#30340;&#21382;&#21490;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;BERT&#30340;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#21382;&#21490;&#20559;&#35265;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#27979;&#37327;&#20854;&#23545;&#26089;&#26399;&#29616;&#20195;&#33521;&#35821;&#65288;EME&#65289;&#21644;&#29616;&#20195;&#33521;&#35821;&#65288;ME&#65289;&#30340;&#36866;&#24212;&#24615;&#26469;&#35780;&#20272;&#20854;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22635;&#31354;&#27979;&#35797;&#65292;&#25105;&#20204;&#23545;60&#20010;&#25513;&#30422;&#21477;&#23376;&#65288;20&#20010;EME&#29305;&#23450;&#12289;20&#20010;ME&#29305;&#23450;&#21644;20&#20010;&#36890;&#29992;&#65289;&#20197;&#21450;&#19977;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#65288;BERT Base&#12289;MacBERTh&#12289;English HLM&#65289;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20004;&#31181;&#35821;&#35328;&#21464;&#20307;&#20043;&#38388;&#30340;5&#28857;&#21452;&#26497;&#21051;&#24230;&#23545;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#35745;&#31639;&#21152;&#26435;&#24471;&#20998;&#26469;&#27979;&#37327;&#27599;&#20010;&#27169;&#22411;&#23545;EME&#21644;ME&#33521;&#35821;&#21464;&#20307;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the idea of analysing the historical bias of contextual language models based on BERT by measuring their adequacy with respect to Early Modern (EME) and Modern (ME) English. In our preliminary experiments, we perform fill-in-the-blank tests with 60 masked sentences (20 EME-specific, 20 ME-specific and 20 generic) and three different models (i.e., BERT Base, MacBERTh, English HLM). We then rate the model predictions according to a 5-point bipolar scale between the two language varieties and derive a weighted score to measure the adequacy of each model to EME and ME varieties of English.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25945;&#23398;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#25506;&#35752;&#20102;&#36890;&#36807;&#24314;&#35774;&#24615;&#21453;&#39304;&#21644;&#25552;&#31034;&#25351;&#23548;&#23398;&#29983;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#31181;&#23545;&#40784;&#26041;&#27861;&#20197;&#21450;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23545;&#40784;LLM&#65292;&#25552;&#20379;&#26356;&#20248;&#36136;&#30340;&#25945;&#32946;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.05000</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25945;&#23398;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Pedagogical Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25945;&#23398;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#25506;&#35752;&#20102;&#36890;&#36807;&#24314;&#35774;&#24615;&#21453;&#39304;&#21644;&#25552;&#31034;&#25351;&#23548;&#23398;&#29983;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#31181;&#23545;&#40784;&#26041;&#27861;&#20197;&#21450;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23545;&#40784;LLM&#65292;&#25552;&#20379;&#26356;&#20248;&#36136;&#30340;&#25945;&#32946;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25945;&#23398;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#36825;&#22312;&#25945;&#32946;&#32972;&#26223;&#19979;&#24212;&#29992;LLM&#20855;&#26377;&#36716;&#21464;&#24615;&#30340;&#24847;&#20041;&#12290;&#19982;&#30452;&#25509;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#19981;&#21516;&#65292;&#25945;&#23398;&#23545;&#40784;&#30340;LLM&#20316;&#20026;&#36741;&#21161;&#24037;&#20855;&#65292;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#21487;&#31649;&#29702;&#30340;&#23376;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24314;&#35774;&#24615;&#30340;&#21453;&#39304;&#21644;&#25552;&#31034;&#25351;&#23548;&#23398;&#29983;&#25214;&#21040;&#26368;&#32456;&#31572;&#26696;&#12290;&#20854;&#30446;&#26631;&#26159;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#35299;&#20915;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#20197;&#21152;&#28145;&#20182;&#20204;&#23545;&#20027;&#39064;&#30340;&#29702;&#35299;&#21644;&#20869;&#21270;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20102;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#65292;&#27809;&#26377;&#23558;&#30446;&#26631;&#23450;&#20041;&#20026;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#26410;&#20351;&#29992;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLHF&#65289;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#40784;&#30340;&#35270;&#35282;&#37325;&#26032;&#35299;&#37322;&#20102;&#36825;&#19968;&#35770;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;RLHF&#26041;&#27861;&#20316;&#20026;&#23545;&#40784;LLM&#30340;&#20248;&#36234;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the novel concept of pedagogically aligned Large Language Models (LLMs) that signifies a transformative shift in the application of LLMs within educational contexts. Rather than providing direct responses to user queries, pedagogically-aligned LLMs function as scaffolding tools, breaking complex problems into manageable subproblems and guiding students towards the final answer through constructive feedback and hints. The objective is to equip learners with problem-solving strategies that deepen their understanding and internalization of the subject matter. Previous research in this field has primarily applied the supervised finetuning approach without framing the objective as an alignment problem, hence not employing reinforcement learning through human feedback (RLHF) methods. This study reinterprets the narrative by viewing the task through the lens of alignment and demonstrates how RLHF methods emerge naturally as a superior alternative for aligning LLM b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#21512;&#20316;&#35757;&#32451;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12289;&#30693;&#35782;&#26356;&#26032;&#19981;&#36275;&#20197;&#21450;&#25512;&#29702;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#30693;&#35782;&#25512;&#29702;&#21644;&#25512;&#29702;&#32467;&#26524;&#36861;&#36394;&#12290;</title><link>https://arxiv.org/abs/2402.04978</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21327;&#20316;&#30340;&#22686;&#24378;&#22411;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#25512;&#29702;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#21512;&#20316;&#35757;&#32451;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12289;&#30693;&#35782;&#26356;&#26032;&#19981;&#36275;&#20197;&#21450;&#25512;&#29702;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#30693;&#35782;&#25512;&#29702;&#21644;&#25512;&#29702;&#32467;&#26524;&#36861;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36935;&#21040;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#34394;&#26500;&#38382;&#39064;&#12289;&#30693;&#35782;&#26356;&#26032;&#19981;&#36275;&#20197;&#21450;&#25512;&#29702;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#33258;&#30001;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#20854;&#20013;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#21644;LLMs&#20043;&#38388;&#23494;&#20999;&#21512;&#20316;&#12290;&#35813;&#26041;&#26696;&#39318;&#20808;&#20351;&#29992;LLMs&#36845;&#20195;&#22320;&#25506;&#32034;KG&#65292;&#36873;&#25321;&#24615;&#22320;&#26816;&#32034;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#30693;&#35782;&#23376;&#22270;&#20197;&#25903;&#25345;&#25512;&#29702;&#12290;&#28982;&#21518;&#24341;&#23548;LLMs&#36827;&#19968;&#27493;&#32452;&#21512;&#20869;&#22312;&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#22312;&#23376;&#22270;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#26126;&#30830;&#38416;&#36848;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#36825;&#31181;&#21327;&#20316;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#65292;&#24182;&#20415;&#20110;&#36861;&#36394;&#25512;&#29702;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20013;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#20167;&#24680;&#36855;&#22240;&#30340;&#25991;&#26412;&#37096;&#20998;&#23545;&#20110;&#27867;&#21270;&#33267;&#19981;&#21516;&#39046;&#22495;&#30340;&#20998;&#31867;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#22270;&#20687;&#37096;&#20998;&#23545;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#25935;&#24863;&#12290;&#23454;&#39564;&#35777;&#26126;&#20167;&#24680;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#20167;&#24680;&#36855;&#22240;&#20998;&#31867;&#22120;&#12290;&#40657;&#30418;&#35299;&#37322;&#34920;&#26126;&#25991;&#26412;&#27169;&#24577;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#36739;&#22823;&#65292;&#20294;&#24341;&#20837;&#22270;&#20687;&#26631;&#39064;&#21518;&#36129;&#29486;&#38477;&#20302;&#12290;&#26032;&#30340;&#28151;&#28102;&#25968;&#25454;&#38598;&#35780;&#20272;&#26174;&#31034;&#25991;&#26412;&#28151;&#28102;&#22240;&#32032;&#34920;&#29616;&#36739;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04967</link><description>&lt;p&gt;
&#25991;&#26412;&#36824;&#26159;&#22270;&#20687;&#65311;&#23545;&#20110;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#30340;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#27169;&#22411;&#26469;&#35828;&#65292;&#20160;&#20040;&#26356;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20013;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#20167;&#24680;&#36855;&#22240;&#30340;&#25991;&#26412;&#37096;&#20998;&#23545;&#20110;&#27867;&#21270;&#33267;&#19981;&#21516;&#39046;&#22495;&#30340;&#20998;&#31867;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#22270;&#20687;&#37096;&#20998;&#23545;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#25935;&#24863;&#12290;&#23454;&#39564;&#35777;&#26126;&#20167;&#24680;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#20167;&#24680;&#36855;&#22240;&#20998;&#31867;&#22120;&#12290;&#40657;&#30418;&#35299;&#37322;&#34920;&#26126;&#25991;&#26412;&#27169;&#24577;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#36739;&#22823;&#65292;&#20294;&#24341;&#20837;&#22270;&#20687;&#26631;&#39064;&#21518;&#36129;&#29486;&#38477;&#20302;&#12290;&#26032;&#30340;&#28151;&#28102;&#25968;&#25454;&#38598;&#35780;&#20272;&#26174;&#31034;&#25991;&#26412;&#28151;&#28102;&#22240;&#32032;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22810;&#27169;&#24335;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20013;&#36328;&#39046;&#22495;&#27867;&#21270;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#35777;&#25454;&#25903;&#25345;&#20551;&#35774;&#65306;&#21482;&#26377;&#20167;&#24680;&#36855;&#22240;&#20013;&#30340;&#25991;&#26412;&#25104;&#20998;&#20351;&#24471;&#29616;&#26377;&#30340;&#22810;&#27169;&#24335;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36827;&#34892;&#27867;&#21270;&#65292;&#32780;&#22270;&#20687;&#25104;&#20998;&#21017;&#23545;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#38750;&#24120;&#25935;&#24863;&#12290;&#35777;&#25454;&#21253;&#25324;&#28436;&#31034;&#65292;&#35777;&#26126;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#20167;&#24680;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#34920;&#29616;&#19982;&#20167;&#24680;&#36855;&#22240;&#20998;&#31867;&#22120;&#30456;&#20284;&#12290;&#21516;&#26102;&#65292;&#23558;&#30001;&#36855;&#22240;&#22270;&#20687;&#29983;&#25104;&#30340;&#26631;&#39064;&#24341;&#20837;&#21040;&#20167;&#24680;&#36855;&#22240;&#20998;&#31867;&#22120;&#20013;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#24179;&#22343;F1&#25351;&#26631;&#32422;&#20026;0.02&#12290;&#36890;&#36807;&#40657;&#30418;&#35299;&#37322;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25991;&#26412;&#27169;&#24577;&#30340;&#37325;&#35201;&#36129;&#29486;&#65288;&#24179;&#22343;83%&#65289;&#65292;&#20294;&#24341;&#20837;&#36855;&#22240;&#22270;&#20687;&#26631;&#39064;&#21518;&#36129;&#29486;&#38477;&#20302;&#33267;52%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#26032;&#21019;&#24314;&#30340;&#28151;&#28102;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#25991;&#26412;&#28151;&#28102;&#22240;&#32032;&#30340;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the formidable challenge of cross-domain generalization in multimodal hate meme detection, presenting compelling findings. We provide enough pieces of evidence supporting the hypothesis that only the textual component of hateful memes enables the existing multimodal classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset. The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a zero-shot setting. Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02. Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme's image captions (52%). Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24515;&#38382;&#39064;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#30001;&#20110;&#20998;&#32452;&#25439;&#22833;&#23548;&#33268;&#30340;&#39044;&#27979;&#20998;&#25968;&#19982;&#23454;&#38469;&#27010;&#29575;&#20559;&#31163;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#37325;&#26032;&#30830;&#23450;LLMs&#65292;&#25913;&#21892;&#23427;&#20204;&#30340;&#33258;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04957</link><description>&lt;p&gt;
&#20174;&#20998;&#32452;&#25439;&#22833;&#30340;&#35282;&#24230;&#37325;&#26500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24515;
&lt;/p&gt;
&lt;p&gt;
Reconfidencing LLMs from the Grouping Loss Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24515;&#38382;&#39064;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#30001;&#20110;&#20998;&#32452;&#25439;&#22833;&#23548;&#33268;&#30340;&#39044;&#27979;&#20998;&#25968;&#19982;&#23454;&#38469;&#27010;&#29575;&#20559;&#31163;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#37325;&#26032;&#30830;&#23450;LLMs&#65292;&#25913;&#21892;&#23427;&#20204;&#30340;&#33258;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;ChatGPT&#21644;LLaMA&#65292;&#22312;&#33258;&#20449;&#30340;&#21475;&#21563;&#20013;&#23481;&#26131;&#29983;&#25104;&#34394;&#20551;&#31572;&#26696;&#12290;&#23613;&#31649;&#24341;&#23548;&#21644;&#26657;&#20934;&#20449;&#24515;&#20998;&#25968;&#30340;&#21162;&#21147;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25511;&#21046;&#19981;&#30830;&#23450;&#24615;&#24517;&#39035;&#36229;&#36234;&#26657;&#20934;: &#30001;&#20110;&#20998;&#32452;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#39044;&#27979;&#20998;&#25968;&#21487;&#33021;&#26126;&#26174;&#20559;&#31163;&#23454;&#38469;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20174;&#30693;&#35782;&#24211;&#20013;&#33719;&#21462;&#65292;&#20197;&#35780;&#20272;&#23545;Mistral&#21644;LLaMA&#30340;&#31572;&#26696;&#32473;&#20986;&#30340;&#20449;&#24515;&#20998;&#25968;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#36807;&#20110;&#33258;&#20449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#26576;&#20123;&#31572;&#26696;&#19978;&#27604;&#20854;&#20182;&#31572;&#26696;&#26356;&#36807;&#20110;&#33258;&#20449;&#65292;&#20363;&#22914;&#21462;&#20915;&#20110;&#26597;&#35810;&#20013;&#20154;&#30340;&#22269;&#31821;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#29702;&#35770;&#20013;&#65292;&#36825;&#23601;&#26159;&#20998;&#32452;&#25439;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#30830;&#23450;LLMs&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20165;&#21462;&#28040;&#26657;&#20934;&#65292;&#36824;&#21462;&#28040;&#20998;&#32452;&#25439;&#22833;&#12290;&#32463;&#36807;&#37325;&#26032;&#30830;&#23450;&#30340;LLMs&#32463;&#36807;&#22788;&#29702;&#21518;&#65292;&#34920;&#31034;&#25913;&#36827;&#30340;&#33258;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone. While efforts to elicit and calibrate confidence scores have proven useful, recent findings show that controlling uncertainty must go beyond calibration: predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss. In this work, we construct a new evaluation dataset derived from a knowledge base to assess confidence scores given to answers of Mistral and LLaMA. Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \emph{eg} depending on the nationality of the person in the query. In uncertainty-quantification theory, this is grouping loss. To address this, we propose a solution to reconfidence LLMs, canceling not only calibration but also grouping loss. The LLMs, after the reconfidencing process, indicate improved confidenc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;ChatGPT&#23545;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#30340;&#35782;&#21035;&#65292;&#23581;&#35797;&#20102;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#20294;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#36827;&#34892;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#20998;&#31867;&#22312;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#20173;&#38590;&#20197;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.04918</link><description>&lt;p&gt;
&#20419;&#20351;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#27880;&#37322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Prompting Implicit Discourse Relation Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;ChatGPT&#23545;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#30340;&#35782;&#21035;&#65292;&#23581;&#35797;&#20102;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#20294;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#36827;&#34892;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#20998;&#31867;&#22312;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#20173;&#38590;&#20197;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26080;&#38656;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#34987;&#21457;&#29616;&#36229;&#36807;&#20102;&#20247;&#21253;&#24037;&#20316;&#32773;&#12290;&#28982;&#32780;&#65292;ChatGPT&#22312;&#36890;&#36807;&#26631;&#20934;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#24341;&#23548;&#30340;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#36828;&#36828;&#19981;&#22914;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#23545;&#20960;&#31181;&#32463;&#36807;&#39564;&#35777;&#30340;&#25552;&#31034;&#25216;&#26415;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#25913;&#21892;ChatGPT&#23545;&#35805;&#35821;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23581;&#35797;&#23558;&#28041;&#21450;&#22823;&#37327;&#25277;&#35937;&#26631;&#31614;&#30340;&#20998;&#31867;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25512;&#29702;&#20934;&#30830;&#29575;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65292;&#36825;&#34920;&#26126;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#20998;&#31867;&#22312;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#23578;&#19981;&#21487;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models, such as ChatGPT, archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches. This work investigates several proven prompting techniques to improve ChatGPT's recognition of discourse relations. In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks. Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering, suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32454;&#31890;&#24230;&#35821;&#35328;&#25511;&#21046;&#19979;&#30340;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#20010;&#32454;&#31890;&#24230;&#35821;&#35328;&#23646;&#24615;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#32034;&#20102;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#20844;&#24320;&#20102;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.04914</link><description>&lt;p&gt;
&#32454;&#31890;&#24230;&#35821;&#35328;&#25511;&#21046;&#19979;&#30340;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Personalized Text Generation with Fine-Grained Linguistic Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32454;&#31890;&#24230;&#35821;&#35328;&#25511;&#21046;&#19979;&#30340;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#20010;&#32454;&#31890;&#24230;&#35821;&#35328;&#23646;&#24615;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#32034;&#20102;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#20844;&#24320;&#20102;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#36234;&#26469;&#36234;&#31361;&#20986;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25511;&#21046;&#29983;&#25104;&#25991;&#26412;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#20351;&#20854;&#26356;&#21152;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#25511;&#21046;&#20869;&#23481;&#25110;&#24314;&#27169;&#29305;&#23450;&#30340;&#39640;&#32423;/&#31895;&#31890;&#24230;&#23646;&#24615;&#65292;&#22914;&#27491;&#24335;&#31243;&#24230;&#12289;&#39046;&#22495;&#25110;&#24773;&#24863;&#65292;&#20197;&#21453;&#26144;&#20316;&#32773;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#25511;&#21046;&#36328;&#22810;&#20010;&#35821;&#35328;&#32500;&#24230;&#30340;&#32454;&#31890;&#24230;&#23646;&#24615;&#65292;&#22914;&#35789;&#27719;&#21644;&#21477;&#27861;&#23646;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#26469;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#26681;&#25454;&#22810;&#20010;&#32454;&#31890;&#24230;&#35821;&#35328;&#23646;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#20174;&#24433;&#21709;&#23427;&#20204;&#24615;&#33021;&#30340;&#22240;&#32032;&#20013;&#24471;&#20986;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the text generation capabilities of large language models become increasingly prominent, recent studies have focused on controlling particular aspects of the generated text to make it more personalized. However, most research on controllable text generation focuses on controlling the content or modeling specific high-level/coarse-grained attributes that reflect authors' writing styles, such as formality, domain, or sentiment. In this paper, we focus on controlling fine-grained attributes spanning multiple linguistic dimensions, such as lexical and syntactic attributes. We introduce a novel benchmark to train generative models and evaluate their ability to generate personalized text based on multiple fine-grained linguistic attributes. We systematically investigate the performance of various large language models on our benchmark and draw insights from the factors that impact their performance. We make our code, data, and pretrained models publicly available.
&lt;/p&gt;</description></item><item><title>L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04902</link><description>&lt;p&gt;
L4Q: &#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#37327;&#21270;&#35757;&#32451;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04902
&lt;/p&gt;
&lt;p&gt;
L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#26041;&#27861;&#27491;&#22312;&#27969;&#34892;&#36215;&#26469;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25152;&#24102;&#26469;&#30340;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#20943;&#23569;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#36890;&#24120;&#39318;&#36873;&#21518;&#35757;&#32451;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#24182;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#37327;&#21270;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#37327;&#21270;&#27169;&#22411;&#30340;&#37197;&#32622;&#12290;&#30001;&#38750;&#32447;&#24615;&#37327;&#21270;&#25110;&#28151;&#21512;&#31934;&#24230;&#26435;&#37325;&#24341;&#36215;&#30340;&#25928;&#26524;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#24182;&#19988;&#37325;&#26032;&#35757;&#32451;&#29305;&#23450;&#37327;&#21270;&#21442;&#25968;&#21487;&#33021;&#20250;&#24433;&#21709;&#26368;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L4Q&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#12290;L4Q&#21033;&#29992;&#20102;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#26159;&#21542;&#21487;&#20197;&#29992;&#20316;&#23545;&#25239;&#29983;&#25104;&#24335;&#21407;&#29983;&#24191;&#21578;&#30340;&#23545;&#31574;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#24191;&#21578;&#20542;&#21521;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#24102;&#33258;&#21160;&#25972;&#21512;&#24191;&#21578;&#30340;&#29983;&#25104;&#31572;&#26696;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.04889</link><description>&lt;p&gt;
&#21457;&#29616;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#30340;&#29983;&#25104;&#24335;&#21407;&#29983;&#24191;&#21578;
&lt;/p&gt;
&lt;p&gt;
Detecting Generated Native Ads in Conversational Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#26159;&#21542;&#21487;&#20197;&#29992;&#20316;&#23545;&#25239;&#29983;&#25104;&#24335;&#21407;&#29983;&#24191;&#21578;&#30340;&#23545;&#31574;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#24191;&#21578;&#20542;&#21521;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#24102;&#33258;&#21160;&#25972;&#21512;&#24191;&#21578;&#30340;&#29983;&#25104;&#31572;&#26696;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25628;&#32034;&#24341;&#25806;&#22914;YouChat&#21644;Microsoft Copilot&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#26597;&#35810;&#29983;&#25104;&#31572;&#26696;&#12290;&#23558;&#27492;&#25216;&#26415;&#29992;&#20110;&#29983;&#25104;&#24182;&#25972;&#21512;&#24191;&#21578;&#65292;&#32780;&#19981;&#26159;&#23558;&#24191;&#21578;&#19982;&#26377;&#26426;&#25628;&#32034;&#32467;&#26524;&#20998;&#24320;&#25918;&#32622;&#65292;&#21482;&#26159;&#19968;&#23567;&#27493;&#12290;&#36825;&#31181;&#31867;&#22411;&#30340;&#24191;&#21578;&#31867;&#20284;&#20110;&#21407;&#29983;&#24191;&#21578;&#21644;&#20135;&#21697;&#25918;&#32622;&#65292;&#20004;&#32773;&#37117;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#24494;&#22937;&#21644;&#25805;&#32437;&#24615;&#24191;&#21578;&#24418;&#24335;&#12290;&#22312;&#32771;&#34385;&#21040;&#19982;LLM&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#26102;&#65292;&#20449;&#24687;&#25628;&#32034;&#32773;&#23558;&#24456;&#21487;&#33021;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#38754;&#20020;&#36825;&#31181;LLM&#25216;&#26415;&#30340;&#20351;&#29992;&#65292;&#22240;&#27492;&#20379;&#24212;&#21830;&#38656;&#35201;&#24320;&#21457;&#21487;&#25345;&#32493;&#30340;&#21830;&#19994;&#27169;&#24335;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#26159;&#21542;&#20063;&#21487;&#20197;&#29992;&#20316;&#23545;&#25239;&#29983;&#25104;&#24335;&#21407;&#29983;&#24191;&#21578;&#30340;&#23545;&#31574;&#65292;&#21363;&#38459;&#27490;&#23427;&#20204;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#24191;&#21578;&#20542;&#21521;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#24102;&#33258;&#21160;&#25972;&#21512;&#24191;&#21578;&#30340;&#29983;&#25104;&#31572;&#26696;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search engines such as YouChat and Microsoft Copilot use large language models (LLMs) to generate answers to queries. It is only a small step to also use this technology to generate and integrate advertising within these answers - instead of placing ads separately from the organic search results. This type of advertising is reminiscent of native advertising and product placement, both of which are very effective forms of subtle and manipulative advertising. It is likely that information seekers will be confronted with such use of LLM technology in the near future, especially when considering the high computational costs associated with LLMs, for which providers need to develop sustainable business models. This paper investigates whether LLMs can also be used as a countermeasure against generated native ads, i.e., to block them. For this purpose we compile a large dataset of ad-prone queries and of generated answers with automatically integrated ads to experiment with fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.04875</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Provable Length and Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#26356;&#38271;&#24207;&#21015;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#20196;&#29260;&#32452;&#21512;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#26159;&#37325;&#35201;&#30340;&#38750;&#20998;&#24067;&#21270;&#27867;&#21270;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#26550;&#26500;&#20013;&#65292;&#26397;&#30528;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#26681;&#25454;&#26550;&#26500;&#30340;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#30340;&#24517;&#35201;&#24615;&#65292;&#20363;&#22914;&#19982;&#30495;&#23454;&#34920;&#31034;&#20855;&#26377;&#32447;&#24615;&#25110;&#25490;&#21015;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
&lt;/p&gt;</description></item><item><title>CodeIt&#26159;&#19968;&#31181;&#20855;&#22791;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#37325;&#26631;&#35760;&#20026;&#37319;&#26679;&#31243;&#24207;&#30340;&#23454;&#38469;&#36755;&#20986;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31243;&#24207;&#21512;&#25104;&#20013;&#22870;&#21169;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04858</link><description>&lt;p&gt;
CodeIt&#65306;&#20855;&#26377;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04858
&lt;/p&gt;
&lt;p&gt;
CodeIt&#26159;&#19968;&#31181;&#20855;&#22791;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#37325;&#26631;&#35760;&#20026;&#37319;&#26679;&#31243;&#24207;&#30340;&#23454;&#38469;&#36755;&#20986;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31243;&#24207;&#21512;&#25104;&#20013;&#22870;&#21169;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#33021;&#22815;&#35299;&#20915;&#36890;&#24120;&#34987;&#35748;&#20026;&#38656;&#35201;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36890;&#29992;&#26234;&#33021;&#22522;&#20934;&#27979;&#35797;&#20363;&#22914;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#34920;&#29616;&#20173;&#28982;&#38750;&#24120;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ARC&#35270;&#20026;&#19968;&#20010;&#20197;&#32534;&#31243;&#31034;&#20363;&#20026;&#22522;&#30784;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Code Iteration&#65288;CodeIt&#65289;&#30340;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;1&#65289;&#31243;&#24207;&#25277;&#26679;&#21644;&#22238;&#39038;&#37325;&#26631;&#35760;&#20197;&#21450;2&#65289;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#32463;&#39564;&#22238;&#25918;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;episode&#30340;&#30446;&#26631;&#65288;&#21363;&#32473;&#23450;&#36755;&#20837;&#30340;&#30446;&#26631;&#31243;&#24207;&#36755;&#20986;&#65289;&#37325;&#26631;&#35760;&#20026;&#37319;&#26679;&#31243;&#24207;&#20135;&#29983;&#30340;&#23454;&#38469;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#31243;&#24207;&#21512;&#25104;&#20013;&#22870;&#21169;&#26497;&#24230;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;&#24212;&#29992;CodeIt&#20110;ARC&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#12289;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#12290;CodeIt&#26159;&#31532;&#19968;&#20010;&#31070;&#32463;&#20803;-&#21512;&#25104;&#26426;&#21046;&#19968;&#20307;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-sy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;</title><link>https://arxiv.org/abs/2402.04854</link><description>&lt;p&gt;
&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#23398;&#26415;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32570;&#20047;&#30740;&#31350;&#22521;&#35757;&#30340;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#30740;&#31350;&#35843;&#26597;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#30740;&#31350;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#24456;&#38590;&#29702;&#35299;&#20182;&#20204;&#30740;&#31350;&#20027;&#39064;&#20869;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#21457;&#29616;&#26032;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#20026;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#25552;&#20379;&#30452;&#35266;&#30340;&#24110;&#21161;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#25552;&#20379;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#24182;&#25512;&#33616;&#30456;&#20851;&#30340;&#23398;&#26415;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#20027;&#35201;&#20381;&#36182;&#20110;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#23383;&#65292;&#24120;&#24120;&#26080;&#27861;&#28165;&#26970;&#22320;&#21576;&#29616;&#22810;&#20010;&#30456;&#20851;&#35770;&#25991;&#20043;&#38388;&#30340;&#36923;&#36753;&#23618;&#27425;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20165;&#20165;&#20381;&#36182;&#20110;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#35753;&#30740;&#31350;&#20154;&#21592;&#22256;&#24785;&#20026;&#20160;&#20040;&#25512;&#33616;&#20102;&#29305;&#23450;&#30340;&#25991;&#31456;&#12290;&#20182;&#20204;&#21487;&#33021;&#32570;&#20047;&#20102;&#35299;&#20851;&#20110;&#20182;&#20204;&#24076;&#26395;&#33719;&#24471;&#30340;"&#38382;&#39064;&#35299;&#20915;"&#21644;"&#38382;&#39064;&#21457;&#29616;"&#20043;&#38388;&#30340;&#35265;&#35299;&#36830;&#25509;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25903;&#25345;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04838</link><description>&lt;p&gt;
PaDeLLM-NER&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24182;&#34892;&#35299;&#30721;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;LLMs&#30340;&#39640;&#24310;&#36831;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#39034;&#24207;&#35299;&#30721;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;NER&#30340;&#25152;&#26377;&#26631;&#31614;&#21644;&#25552;&#21450;&#65292;&#26174;&#33879;&#22686;&#21152;&#20102;&#24207;&#21015;&#38271;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PaDeLLM-NER&#65288;Parallel Decoding in LLM for NE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#27169;&#22359;&#25110;&#26550;&#26500;&#20462;&#25913;&#21363;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#12290;PaDeLLM-NER&#20801;&#35768;&#21516;&#26102;&#35299;&#30721;&#25152;&#26377;&#25552;&#21450;&#65292;&#20174;&#32780;&#20943;&#23569;&#29983;&#25104;&#24310;&#36831;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PaDeLLM-NER&#30340;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#23545;&#33521;&#35821;&#21644;&#20013;&#25991;&#26469;&#35828;&#27604;&#33258;&#22238;&#24402;&#26041;&#27861;&#24555;1.76&#21040;10.22&#20493;&#12290;&#19982;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#32500;&#25345;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36873;&#25321;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#21709;&#24212;&#26368;&#38271;&#30340;1,000&#26465;&#25351;&#31034;&#20316;&#20026;&#22522;&#20934;&#32447;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#23545;&#40784;&#30340;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#23545;&#38271;&#25351;&#31034;&#36827;&#34892;&#36731;&#37327;&#32423;&#25913;&#36827;&#36827;&#19968;&#27493;&#25552;&#21319;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04833</link><description>&lt;p&gt;
&#38271;&#24230;&#26356;&#38271;&#23545;&#40784;&#26356;&#22909;&#65306;&#19968;&#31181;&#31616;&#21333;&#20294;&#38590;&#20197;&#26395;&#20854;&#39033;&#32972;&#30340;&#25351;&#23548;&#24494;&#35843;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04833
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#21709;&#24212;&#26368;&#38271;&#30340;1,000&#26465;&#25351;&#31034;&#20316;&#20026;&#22522;&#20934;&#32447;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#23545;&#40784;&#30340;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#23545;&#38271;&#25351;&#31034;&#36827;&#34892;&#36731;&#37327;&#32423;&#25913;&#36827;&#36827;&#19968;&#27493;&#25552;&#21319;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20849;&#35782;&#35748;&#20026;&#65292;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#24494;&#35843;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#20294;&#20855;&#20307;&#26159;&#20160;&#20040;&#21602;&#65311;LIMA&#65288;NeurIPS 2023&#65289;&#21644;AlpaGasus&#65288;ICLR 2024&#65289;&#26159;&#36873;&#25321;&#36825;&#31867;&#39640;&#36136;&#37327;&#31034;&#20363;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23427;&#20204;&#35201;&#20040;&#36890;&#36807;&#25163;&#21160;&#25972;&#29702;&#35201;&#20040;&#20351;&#29992;GPT-3.5-Turbo&#20316;&#20026;&#36136;&#37327;&#35780;&#20998;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#21709;&#24212;&#26368;&#38271;&#30340;1,000&#26465;&#25351;&#31034;&#30340;&#26497;&#31616;&#22522;&#20934;&#32447;&#22312;GPT-4&#21644;PaLM-2&#30340;&#35780;&#21028;&#19979;&#22987;&#32456;&#33021;&#22815;&#32988;&#36807;&#36825;&#20123;&#22797;&#26434;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#22522;&#20110;&#20107;&#23454;&#30693;&#35782;&#30340;OpenLLM&#22522;&#20934;&#19978;&#20445;&#25345;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#22312;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;Llama-2-7B&#65292;Llama-2-13B&#21644;Mistral-7B&#65289;&#21644;&#25968;&#25454;&#38598;&#65288;Alpaca-52k&#21644;Evol-Instruct-70k&#65289;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#23545;&#36825;&#26679;&#30340;&#38271;&#25351;&#31034;&#36827;&#34892;&#36731;&#37327;&#32423;&#25913;&#36827;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#25105;&#20204;&#22312;&#21482;&#35757;&#32451;&#20102;1,000&#20010;&#20363;&#23376;&#19988;&#27809;&#26377;&#22806;&#37096;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;AlpacaEval 2.0&#19978;&#33719;&#24471;&#20102;&#22522;&#20110;Llama-2-7B&#30340;&#27169;&#22411;&#30340;&#31532;&#20108;&#39640;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a consensus that instruction fine-tuning of LLMs requires high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-of-the-art methods for selecting such high-quality examples, either via manual curation or using GPT-3.5-Turbo as a quality scorer. We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses from standard datasets can consistently outperform these sophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining competitive on the OpenLLM benchmarks that test factual knowledge. We demonstrate this for several state-of-the-art LLMs (Llama-2-7B, Llama-2-13B, and Mistral-7B) and datasets (Alpaca-52k and Evol-Instruct-70k). In addition, a lightweight refinement of such long instructions can further improve the abilities of the fine-tuned LLMs, and allows us to obtain the 2nd highest-ranked Llama-2-7B-based model on AlpacaEval 2.0 while training on only 1,000 examples and no ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21327;&#21516;&#21442;&#32771;&#28216;&#25103;&#20013;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#36861;&#38543;&#32773;&#34892;&#20026;&#30340;&#36890;&#20449;&#31574;&#30053;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO&#65289;&#65292;&#35777;&#26126;&#20102;&#31070;&#32463;&#20195;&#29702;&#33021;&#22815;&#19982;&#20855;&#26377;&#19981;&#21516;&#33258;&#20449;&#24230;&#21644;&#33258;&#20027;&#24615;&#30340;&#36861;&#38543;&#32773;&#33391;&#22909;&#22320;&#21327;&#20316;&#65292;&#24182;&#19988;&#22312;&#23398;&#20064;&#20449;&#21495;&#20013;&#32771;&#34385;&#20102;&#27807;&#36890;&#21162;&#21147;&#30340;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.04824</link><description>&lt;p&gt;
&#22312;&#21327;&#21516;&#21442;&#32771;&#28216;&#25103;&#20013;&#23398;&#20064;&#19981;&#21516;&#36861;&#38543;&#32773;&#34892;&#20026;&#30340;&#36890;&#20449;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Communication Policies for Different Follower Behaviors in a Collaborative Reference Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21327;&#21516;&#21442;&#32771;&#28216;&#25103;&#20013;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#36861;&#38543;&#32773;&#34892;&#20026;&#30340;&#36890;&#20449;&#31574;&#30053;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PPO&#65289;&#65292;&#35777;&#26126;&#20102;&#31070;&#32463;&#20195;&#29702;&#33021;&#22815;&#19982;&#20855;&#26377;&#19981;&#21516;&#33258;&#20449;&#24230;&#21644;&#33258;&#20027;&#24615;&#30340;&#36861;&#38543;&#32773;&#33391;&#22909;&#22320;&#21327;&#20316;&#65292;&#24182;&#19988;&#22312;&#23398;&#20064;&#20449;&#21495;&#20013;&#32771;&#34385;&#20102;&#27807;&#36890;&#21162;&#21147;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Albrecht&#21644;Stone&#65288;2018&#65289;&#25351;&#20986;&#65292;&#24314;&#27169;&#19981;&#26029;&#21464;&#21270;&#30340;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#8220;&#30001;&#20110;&#20854;&#20182;&#26234;&#33021;&#20307;&#21487;&#33021;&#20570;&#20986;&#30340;&#34892;&#20026;&#26412;&#36136;&#19978;&#26159;&#26080;&#38480;&#21046;&#30340;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23545;&#21327;&#21516;&#21442;&#32771;&#28216;&#25103;&#20013;&#20551;&#35774;&#20249;&#20276;&#34892;&#20026;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#36825;&#20010;&#28216;&#25103;&#20013;&#65292;&#24403;&#26377;&#30693;&#35782;&#30340;&#23548;&#28216;&#33021;&#22815;&#36890;&#36807;&#21475;&#22836;&#24341;&#23548;&#36861;&#38543;&#32773;&#20174;&#22810;&#20010;&#24178;&#25200;&#32773;&#20013;&#36873;&#25321;&#29305;&#23450;&#30340;&#25340;&#22270;&#26102;&#65292;&#25165;&#33021;&#21462;&#24471;&#25104;&#21151;&#12290;&#25105;&#20204;&#25226;&#36825;&#20010;&#35821;&#35328;&#22522;&#30784;&#21644;&#21327;&#35843;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26469;&#35299;&#20915;&#65292;&#24182;&#27979;&#37327;&#20102;&#19968;&#20010;&#24120;&#35265;&#30340;&#24378;&#21270;&#35757;&#32451;&#31639;&#27861;&#65288;PPO&#65289;&#22312;&#22810;&#31181;&#21551;&#21457;&#24335;&#36861;&#38543;&#32773;&#34892;&#20026;&#19978;&#20135;&#29983;&#30340;&#31070;&#32463;&#20195;&#29702;&#65288;&#23548;&#28216;&#65289;&#30340;&#34920;&#29616;&#31243;&#24230;&#65292;&#36825;&#20123;&#34892;&#20026;&#22312;&#33258;&#20449;&#24230;&#21644;&#33258;&#20027;&#24615;&#26041;&#38754;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19968;&#31181;&#23398;&#20064;&#20449;&#21495;&#65292;&#38500;&#20102;&#30446;&#26631;&#26465;&#20214;&#22806;&#36824;&#23562;&#37325;&#20551;&#35774;&#30340;&#27807;&#36890;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;&#22240;&#32032;&#20250;&#23548;&#33268;&#27807;&#36890;&#31574;&#30053;&#30340;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Albrecht and Stone (2018) state that modeling of changing behaviors remains an open problem "due to the essentially unconstrained nature of what other agents may do". In this work we evaluate the adaptability of neural artificial agents towards assumed partner behaviors in a collaborative reference game. In this game success is achieved when a knowledgeable Guide can verbally lead a Follower to the selection of a specific puzzle piece among several distractors. We frame this language grounding and coordination task as a reinforcement learning problem and measure to which extent a common reinforcement training algorithm (PPO) is able to produce neural agents (the Guides) that perform well with various heuristic Follower behaviors that vary along the dimensions of confidence and autonomy. We experiment with a learning signal that in addition to the goal condition also respects an assumed communicative effort. Our results indicate that this novel ingredient leads to communicative strategi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#24335;&#20154;&#21147;&#36164;&#28304;&#35843;&#26597;&#22238;&#24212;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#22238;&#24212;&#32858;&#31867;&#35782;&#21035;&#20851;&#38190;&#26041;&#38754;&#65292;&#24182;&#37319;&#29992;&#33655;&#20848;BERT&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#25903;&#25345;&#21592;&#24037;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04812</link><description>&lt;p&gt;
&#38754;&#21521;&#24320;&#25918;&#24335;&#20154;&#21147;&#36164;&#28304;&#35843;&#26597;&#22238;&#24212;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Aspect-Based Sentiment Analysis for Open-Ended HR Survey Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#24335;&#20154;&#21147;&#36164;&#28304;&#35843;&#26597;&#22238;&#24212;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#22238;&#24212;&#32858;&#31867;&#35782;&#21035;&#20851;&#38190;&#26041;&#38754;&#65292;&#24182;&#37319;&#29992;&#33655;&#20848;BERT&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#25903;&#25345;&#21592;&#24037;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21592;&#24037;&#21916;&#22909;&#12289;&#24847;&#35265;&#21644;&#24773;&#24863;&#23545;&#20110;&#26377;&#25928;&#30340;&#21592;&#24037;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#24320;&#25918;&#24335;&#35843;&#26597;&#22238;&#24212;&#20316;&#20026;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21592;&#24037;&#28385;&#24847;&#24230;&#35843;&#26597;&#20013;&#33655;&#20848;&#24320;&#25918;&#24335;&#22238;&#24212;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;(ABSA)&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#22238;&#24212;&#20013;&#22266;&#26377;&#30340;&#22122;&#22768;&#21644;&#21464;&#24322;&#24615;&#65292;&#23454;&#29616;&#23545;&#24773;&#24863;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#20197;&#25903;&#25345;&#21592;&#24037;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;&#36890;&#36807;&#22238;&#24212;&#32858;&#31867;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20845;&#20010;&#20851;&#38190;&#26041;&#38754;(&#34218;&#27700;&#12289;&#26102;&#38388;&#23433;&#25490;&#12289;&#32852;&#31995;&#12289;&#27807;&#36890;&#12289;&#20010;&#20154;&#20851;&#27880;&#12289;&#21327;&#35758;)&#65292;&#24182;&#30001;&#39046;&#22495;&#19987;&#23478;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#30001;1,458&#20010;&#33655;&#20848;&#35843;&#26597;&#22238;&#24212;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26041;&#38754;&#21644;&#24773;&#24863;&#30340;&#26631;&#31614;&#19981;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33655;&#20848;BERT&#27169;&#22411;&#30340;&#20960;&#20010;&#23567;&#25209;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#35789;&#34955;&#27169;&#22411;&#21644;&#38646;&#26679;&#26412;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;ABSA&#39046;&#22495;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding preferences, opinions, and sentiment of the workforce is paramount for effective employee lifecycle management. Open-ended survey responses serve as a valuable source of information. This paper proposes a machine learning approach for aspect-based sentiment analysis (ABSA) of Dutch open-ended responses in employee satisfaction surveys. Our approach aims to overcome the inherent noise and variability in these responses, enabling a comprehensive analysis of sentiments that can support employee lifecycle management. Through response clustering we identify six key aspects (salary, schedule, contact, communication, personal attention, agreements), which we validate by domain experts. We compile a dataset of 1,458 Dutch survey responses, revealing label imbalance in aspects and sentiments. We propose few-shot approaches for ABSA based on Dutch BERT models, and compare them against bag-of-words and zero-shot baselines. Our work significantly contributes to the field of ABSA by d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OAIF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#21453;&#39304;&#26469;&#25913;&#21892;DAP&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;LLM&#20316;&#20026;&#26631;&#27880;&#22120;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#31163;&#32447;DAP&#21644;RLHF&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.04792</link><description>&lt;p&gt;
&#26469;&#33258;&#22312;&#32447;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#30340;&#30452;&#25509;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Language Model Alignment from Online AI Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OAIF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#21453;&#39304;&#26469;&#25913;&#21892;DAP&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;LLM&#20316;&#20026;&#26631;&#27880;&#22120;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#31163;&#32447;DAP&#21644;RLHF&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30452;&#25509;&#23545;&#40784;&#20559;&#22909;&#65288;DAP&#65289;&#26041;&#27861;&#22914;DPO&#24050;&#25104;&#20026;&#23545;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#30340;&#39640;&#25928;&#26367;&#20195;&#26041;&#27861;&#65292;&#19981;&#35201;&#27714;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DAP&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#36890;&#24120;&#22312;&#35757;&#32451;&#20043;&#21069;&#25910;&#38598;&#65292;&#24182;&#19988;&#20174;&#19981;&#26356;&#26032;&#65292;&#22240;&#27492;&#21453;&#39304;&#32431;&#31929;&#26159;&#31163;&#32447;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#22238;&#24212;&#36890;&#24120;&#26159;&#20174;&#19968;&#20010;&#19982;&#34987;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#65292;&#30001;&#20110;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#21464;&#21270;&#65292;&#23545;&#40784;&#38454;&#27573;&#24517;&#28982;&#26159;&#38750;&#31574;&#30053;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#32447;&#21453;&#39304;&#26159;&#20851;&#38190;&#65292;&#21487;&#20197;&#25913;&#21892;DAP&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#32447;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#65288;OAIF&#65289;&#65292;&#20351;&#29992;LLM&#20316;&#20026;&#26631;&#27880;&#22120;&#65306;&#22312;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#20174;&#24403;&#21069;&#27169;&#22411;&#20013;&#37319;&#26679;&#20004;&#20010;&#22238;&#24212;&#65292;&#24182;&#25552;&#31034;LLM&#26631;&#27880;&#22120;&#36873;&#25321;&#21738;&#20010;&#26356;&#21463;&#27426;&#36814;&#65292;&#20174;&#32780;&#25552;&#20379;&#22312;&#32447;&#21453;&#39304;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#20294;&#36890;&#36807;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;OAIF&#20248;&#20110;&#31163;&#32447;DAP&#21644;RLHF
&lt;/p&gt;
&lt;p&gt;
Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#36741;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#24120;&#31867;&#20154;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#22914;GPT-4V&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04788</link><description>&lt;p&gt;
MLLM&#20316;&#20026;&#27861;&#23448;&#65306;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#35780;&#20272;&#22810;&#27169;&#24577;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#36741;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#24120;&#31867;&#20154;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#22914;GPT-4V&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLM&#30340;&#23454;&#29992;&#24615;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#31526;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#12290;&#26412;&#25991;&#21463;&#21040;LLM&#27169;&#22411;&#20013;LLM&#20316;&#20026;&#27861;&#23448;&#30340;&#21551;&#21457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#34987;&#31216;&#20026;MLLM&#20316;&#20026;&#27861;&#23448;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#21327;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#35780;&#20998;&#35780;&#20272;&#12289;&#23545;&#27604;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#31867;&#20154;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20687;GPT-4V&#36825;&#26679;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22810;&#26679;&#30340;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#30340;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#23545;MLLM&#30340;&#25913;&#36827;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further rese
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#32479;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;LLM&#27169;&#22411;&#30340;&#33258;&#25105;&#21512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;LLM&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#35780;&#21028;&#20004;&#32773;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#19982;GPT-3.5&#19981;&#30456;&#20284;&#12290;&#36825;&#19968;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29702;&#35299;LLL&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.04787</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#33258;&#25105;&#21512;&#29702;&#27169;&#22411;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#32479;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;LLM&#27169;&#22411;&#30340;&#33258;&#25105;&#21512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;LLM&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#35780;&#21028;&#20004;&#32773;&#30340;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#19982;GPT-3.5&#19981;&#30456;&#20284;&#12290;&#36825;&#19968;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29702;&#35299;LLL&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#27169;&#22411;&#30340;&#33258;&#25105;&#21512;&#29702;&#33021;&#21147;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#29983;&#25104;&#30340;&#35299;&#37322;&#21487;&#20197;&#25581;&#31034;&#39044;&#27979;&#30340;&#21512;&#29702;&#24615;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#19982;&#39044;&#27979;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#20854;&#20013;&#30340;&#27169;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#30340;&#32479;&#35745;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#23454;&#29616;&#20851;&#20110;&#20219;&#21153;&#65288;&#22312;&#25105;&#20204;&#30340;&#20363;&#23376;&#20013;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65289;&#22914;&#20309;&#35299;&#20915;&#30340;&#20551;&#35774;&#65292;&#24182;&#23558;&#20854;&#20869;&#37096;&#29366;&#24577;&#36890;&#36807;&#27169;&#26495;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#28982;&#21518;&#20351;&#29992;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#23558;&#36825;&#20123;&#35299;&#37322;&#19982;LLM&#29983;&#25104;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#30456;&#27604;&#36739;&#65292;&#20197;&#35780;&#21028;LLM&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#20363;&#23376;&#20551;&#35774;&#21644;&#20004;&#31181;&#36125;&#21494;&#26031;&#32593;&#32476;&#23454;&#29616;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#24212;&#29992;&#12290;&#24471;&#21040;&#30340;&#27169;&#22411;&#19982;GPT-3.5&#27809;&#26377;&#26126;&#26174;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#28857;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-rationalising capabilities of LLMs are appealing because the generated explanations can give insights into the plausibility of the predictions. However, how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further. To this end, we propose a hypothesis-driven statistical framework. We use a Bayesian network to implement a hypothesis about how a task (in our example, natural language inference) is solved, and its internal states are translated into natural language with templates. Those explanations are then compared to LLM-generated free-text explanations using automatic and human evaluations. This allows us to judge how similar the LLM's and the Bayesian network's decision processes are. We demonstrate the usage of our framework with an example hypothesis and two realisations in Bayesian networks. The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well 
&lt;/p&gt;</description></item><item><title>StableMask&#26159;&#19968;&#31181;&#22312;&#20165;&#35299;&#30721;Transformer&#20013;&#25913;&#36827;&#22240;&#26524;&#23631;&#34109;&#30340;&#26080;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20266;&#27880;&#24847;&#21147;&#20540;&#26469;&#24179;&#34913;&#27880;&#24847;&#21147;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#20943;&#23567;&#30340;&#23631;&#34109;&#27604;&#29575;&#26469;&#32534;&#30721;&#32477;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.04779</link><description>&lt;p&gt;
StableMask: &#22312;&#20165;&#35299;&#30721;Transformer&#20013;&#25913;&#36827;&#22240;&#26524;&#23631;&#34109;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StableMask: Refining Causal Masking in Decoder-only Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04779
&lt;/p&gt;
&lt;p&gt;
StableMask&#26159;&#19968;&#31181;&#22312;&#20165;&#35299;&#30721;Transformer&#20013;&#25913;&#36827;&#22240;&#26524;&#23631;&#34109;&#30340;&#26080;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20266;&#27880;&#24847;&#21147;&#20540;&#26469;&#24179;&#34913;&#27880;&#24847;&#21147;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#20943;&#23567;&#30340;&#23631;&#34109;&#27604;&#29575;&#26469;&#32534;&#30721;&#32477;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#65292;&#20165;&#35299;&#30721;Transformer&#26550;&#26500;&#20013;&#37319;&#29992;&#22240;&#26524;&#23631;&#34109;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPE&#65289;&#24050;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#36873;&#25321;&#12290;&#23613;&#31649;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#38480;&#21046;&#65306;&#39318;&#20808;&#65292;&#21363;&#20351;&#24403;&#21069;&#23884;&#20837;&#20855;&#26377;&#36275;&#22815;&#30340;&#33258;&#21253;&#21547;&#20449;&#24687;&#65292;&#23427;&#35201;&#27714;&#25152;&#26377;&#27880;&#24847;&#21147;&#20998;&#25968;&#37117;&#20026;&#38750;&#38646;&#19988;&#24635;&#21644;&#20026;1&#12290;&#36825;&#24378;&#36843;&#27169;&#22411;&#23545;&#29305;&#23450;&#30340;&#26631;&#35760;&#20998;&#37197;&#19981;&#25104;&#27604;&#20363;&#30340;&#36807;&#24230;&#20851;&#27880;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;RPE&#30340;Transformer&#22312;&#32534;&#30721;&#32477;&#23545;&#20301;&#32622;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#22240;&#27492;&#22312;&#20301;&#32622;&#20851;&#38190;&#20219;&#21153;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;StableMask&#65306;&#19968;&#31181;&#26080;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22240;&#26524;&#23631;&#34109;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#23427;&#24341;&#20837;&#20102;&#20266;&#27880;&#24847;&#21147;&#20540;&#26469;&#24179;&#34913;&#27880;&#24847;&#21147;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#20943;&#23567;&#30340;&#23631;&#34109;&#27604;&#29575;&#26469;&#32534;&#30721;&#32477;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;StableMask&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04678</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#20449;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As Faithful Explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#37096;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24050;&#32463;&#33021;&#22815;&#29087;&#32451;&#35299;&#20915;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#20256;&#32479;&#30340;&#20197;&#36755;&#20837;&#20026;&#37325;&#28857;&#30340;&#35299;&#37322;&#31639;&#27861;&#26469;&#35299;&#37322;LLMs&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#26426;&#21046;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#36827;&#34892;&#21333;&#21521;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;LLMs&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#32463;&#24120;&#22240;&#20026;&#32570;&#20047;&#21487;&#20449;&#24230;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#22240;&#20026;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#19981;&#20934;&#30830;&#22320;&#21453;&#26144;LLMs&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;xLLM&#65292;&#20197;&#25552;&#39640;LLMs&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;xLLM&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20013;&#30340;&#28304;&#20449;&#24687;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#26469;&#25506;&#32034;&#25277;&#35937;&#25688;&#35201;&#30340;&#29983;&#25104;&#26041;&#24335;&#12290;&#20182;&#20204;&#36890;&#36807;&#27880;&#37322;&#28304;&#21477;&#23376;&#21644;&#27604;&#36739;&#22810;&#31181;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#65292;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#34920;&#29616;&#36739;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04677</link><description>&lt;p&gt;
&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20013;&#30340;&#28304;&#20449;&#24687;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Source Identification in Abstractive Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20013;&#30340;&#28304;&#20449;&#24687;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#26469;&#25506;&#32034;&#25277;&#35937;&#25688;&#35201;&#30340;&#29983;&#25104;&#26041;&#24335;&#12290;&#20182;&#20204;&#36890;&#36807;&#27880;&#37322;&#28304;&#21477;&#23376;&#21644;&#27604;&#36739;&#22810;&#31181;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22522;&#32447;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#65292;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#29983;&#25104;&#25688;&#35201;&#65292;&#20294;&#30446;&#21069;&#23545;&#28304;&#20449;&#24687;&#22914;&#20309;&#36716;&#21270;&#20026;&#25688;&#35201;&#30340;&#36807;&#31243;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#21253;&#21547;&#29983;&#25104;&#25688;&#35201;&#20013;&#22522;&#26412;&#20449;&#24687;&#30340;&#36755;&#20837;&#21477;&#23376;&#23450;&#20041;&#20026;&#8220;&#28304;&#21477;&#23376;&#8221;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#28304;&#21477;&#23376;&#26469;&#30740;&#31350;&#25277;&#35937;&#25688;&#35201;&#30340;&#29983;&#25104;&#26041;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;CNN/DailyMail&#21644;XSum&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26723;-&#25688;&#35201;&#23545;&#36827;&#34892;&#20102;&#31995;&#32479;&#29983;&#25104;&#21644;&#21442;&#32771;&#25688;&#35201;&#30340;&#28304;&#21477;&#23376;&#27880;&#37322;&#65292;&#24182;&#21046;&#23450;&#20102;&#33258;&#21160;&#28304;&#21477;&#23376;&#26816;&#27979;&#26041;&#27861;&#24182;&#27604;&#36739;&#20102;&#22810;&#31181;&#26041;&#27861;&#20197;&#24314;&#31435;&#24378;&#22522;&#32447;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#24230;&#25277;&#35937;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#22312;&#30456;&#23545;&#25552;&#21462;&#24335;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20197;&#22312;https://github.com/suhara/sourcesum&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural abstractive summarization models make summaries in an end-to-end manner, and little is known about how the source information is actually converted into summaries. In this paper, we define input sentences that contain essential information in the generated summary as $\textit{source sentences}$ and study how abstractive summaries are made by analyzing the source sentences. To this end, we annotate source sentences for reference summaries and system summaries generated by PEGASUS on document-summary pairs sampled from the CNN/DailyMail and XSum datasets. We also formulate automatic source sentence detection and compare multiple methods to establish a strong baseline for the task. Experimental results show that the perplexity-based method performs well in highly abstractive settings, while similarity-based methods perform robustly in relatively extractive settings. Our code and data are available at https://github.com/suhara/sourcesum.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19968;&#20010;&#23567;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#24320;&#28304;LLM&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#25511;&#21046;&#36755;&#20837;&#20998;&#27573;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#29420;&#31435;&#25919;&#31574;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#26368;&#26032;&#22522;&#32447;&#30456;&#24403;&#30340;BLEU&#20998;&#25968;&#12290;&#21516;&#26102;&#65292;&#38381;&#28304;&#27169;&#22411;GPT-4&#22312;&#38646;-shot&#19979;&#20063;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20026;&#25552;&#21319;&#26410;&#26469;&#30340;SiMT&#31995;&#32479;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.04636</link><description>&lt;p&gt;
TransLLaMa: &#22522;&#20110;LLM&#30340;&#21516;&#20256;&#32763;&#35793;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TransLLaMa: LLM-based Simultaneous Translation System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19968;&#20010;&#23567;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#24320;&#28304;LLM&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#25511;&#21046;&#36755;&#20837;&#20998;&#27573;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#29420;&#31435;&#25919;&#31574;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#26368;&#26032;&#22522;&#32447;&#30456;&#24403;&#30340;BLEU&#20998;&#25968;&#12290;&#21516;&#26102;&#65292;&#38381;&#28304;&#27169;&#22411;GPT-4&#22312;&#38646;-shot&#19979;&#20063;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20026;&#25552;&#21319;&#26410;&#26469;&#30340;SiMT&#31995;&#32479;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26368;&#36817;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#25512;&#29702;&#26041;&#38754;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#65288;SiMT&#65289;&#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#24212;&#29992;&#26377;&#38480;&#65292;&#30446;&#21069;&#30001;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#20027;&#23548;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19968;&#20010;&#21253;&#21547;&#22240;&#26524;&#23545;&#40784;&#30340;&#28304;&#21477;&#23376;&#21644;&#30446;&#26631;&#21477;&#23376;&#23545;&#30340;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#24320;&#28304;LLM&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;&#29305;&#27530;&#30340;&#8220;&#31561;&#24453;&#8221;&#26631;&#35760;&#26469;&#30452;&#25509;&#25511;&#21046;&#36755;&#20837;&#20998;&#27573;&#12290;&#36825;&#28040;&#38500;&#20102;&#29420;&#31435;&#25919;&#31574;&#30340;&#38656;&#35201;&#65292;&#20351;LLM&#33021;&#22815;&#25191;&#34892;&#19982;&#29305;&#23450;&#26368;&#26032;&#22522;&#32447;&#30340;BLEU&#20998;&#25968;&#30456;&#24403;&#30340;&#33521;&#24503;&#21644;&#33521;&#20420;SiMT&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#38381;&#28304;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#22312;&#27809;&#26377;&#20808;&#21069;&#35757;&#32451;&#65288;&#38646;-shot&#65289;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;&#20102;&#25913;&#36827;&#26410;&#26469;SiMT&#31995;&#32479;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoder-only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning. Nonetheless, they have limited applications in simultaneous machine translation (SiMT), currently dominated by encoder-decoder transformers. This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special "wait" token. This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines. We also evaluated closed-source models such as GPT-4, which displayed encouraging results in performing the SiMT task without prior training (zero-shot), indicating a promising avenue for enhancing future SiMT systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#35748;&#30693;&#31574;&#30053;&#22686;&#24378;&#30340;&#35828;&#26381;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#26410;&#26469;&#21457;&#23637;&#65292;&#24378;&#35843;&#20102;&#23545;&#20110;&#23454;&#29616;&#26234;&#33021;&#21644;&#20154;&#31867;&#21270;&#23545;&#35805;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20351;&#23545;&#35805;&#26426;&#22120;&#20154;&#22312;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#22238;&#24212;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;&#20154;&#31867;&#33324;&#30340;&#35828;&#26381;&#27700;&#24179;&#65292;&#35828;&#26381;&#23545;&#35805;&#26426;&#22120;&#20154;&#38656;&#35201;&#34701;&#20837;&#35748;&#30693;&#24515;&#29702;&#23398;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.04631</link><description>&lt;p&gt;
&#35748;&#30693;&#31574;&#30053;&#22686;&#24378;&#30340;&#35828;&#26381;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#26410;&#26469;&#65306;&#26032;&#30340;&#35266;&#28857;&#21644;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04631
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#35748;&#30693;&#31574;&#30053;&#22686;&#24378;&#30340;&#35828;&#26381;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#26410;&#26469;&#21457;&#23637;&#65292;&#24378;&#35843;&#20102;&#23545;&#20110;&#23454;&#29616;&#26234;&#33021;&#21644;&#20154;&#31867;&#21270;&#23545;&#35805;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20351;&#23545;&#35805;&#26426;&#22120;&#20154;&#22312;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#22238;&#24212;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;&#20154;&#31867;&#33324;&#30340;&#35828;&#26381;&#27700;&#24179;&#65292;&#35828;&#26381;&#23545;&#35805;&#26426;&#22120;&#20154;&#38656;&#35201;&#34701;&#20837;&#35748;&#30693;&#24515;&#29702;&#23398;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35498;&#26381;&#26159;&#20154;&#31867;&#20132;&#27969;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#33021;&#21147;&#20043;&#19968;&#65292;&#22312;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#39046;&#22495;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#27492;&#32473;&#20104;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#20154;&#31867;&#24448;&#24448;&#36890;&#36807;&#23545;&#35805;&#30340;&#26041;&#24335;&#26469;&#35828;&#26381;&#20182;&#20154;&#25913;&#21464;&#20182;&#20204;&#30340;&#35266;&#28857;&#12289;&#24577;&#24230;&#25110;&#34892;&#20026;&#65292;&#36825;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#37117;&#26159;&#22914;&#27492;&#65288;&#20363;&#22914;&#25512;&#21160;&#31038;&#20250;&#31119;&#21033;&#12289;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#20105;&#35770;&#65289;&#12290;&#24320;&#21457;&#33021;&#22815;&#35828;&#26381;&#20182;&#20154;&#25509;&#21463;&#26576;&#20123;&#31435;&#22330;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#23545;&#20110;&#23454;&#29616;&#30495;&#27491;&#26234;&#33021;&#21644;&#20855;&#26377;&#20154;&#31867;&#29305;&#28857;&#30340;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#23545;&#35805;&#26426;&#22120;&#20154;&#24050;&#32463;&#33719;&#24471;&#20102;&#22312;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#22238;&#24212;&#29983;&#25104;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#20856;&#22411;&#19988;&#22797;&#26434;&#30340;&#35748;&#30693;&#24515;&#29702;&#31995;&#32479;&#65292;&#35828;&#26381;&#23545;&#35805;&#26426;&#22120;&#20154;&#36824;&#38656;&#35201;&#33719;&#24471;&#26469;&#33258;&#35748;&#30693;&#24515;&#29702;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#20197;&#36798;&#21040;&#20154;&#31867;&#33324;&#30340;&#35828;&#26381;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#35748;&#30693;&#31574;&#30053;&#22686;&#24378;&#30340;&#35828;&#26381;&#23545;&#35805;&#26426;&#22120;&#20154;&#65288;&#23450;&#20041;&#20026;CogAgent&#65289;
&lt;/p&gt;
&lt;p&gt;
Persuasion, as one of the crucial abilities in human communication, has garnered extensive attention from researchers within the field of intelligent dialogue systems. We humans tend to persuade others to change their viewpoints, attitudes or behaviors through conversations in various scenarios (e.g., persuasion for social good, arguing in online platforms). Developing dialogue agents that can persuade others to accept certain standpoints is essential to achieving truly intelligent and anthropomorphic dialogue system. Benefiting from the substantial progress of Large Language Models (LLMs), dialogue agents have acquired an exceptional capability in context understanding and response generation. However, as a typical and complicated cognitive psychological system, persuasive dialogue agents also require knowledge from the domain of cognitive psychology to attain a level of human-like persuasion. Consequently, the cognitive strategy-enhanced persuasive dialogue agent (defined as CogAgent
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#29983;&#21629;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#23545;OpenLlama LLM&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#26597;&#35810;&#38598;&#21512;&#65292;&#20174;&#32780;&#33021;&#22815;&#36827;&#34892;&#24494;&#35843;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#26597;&#35810;&#20013;&#35821;&#20041;&#32447;&#32034;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04627</link><description>&lt;p&gt;
SPARQL&#29983;&#25104;&#65306;&#23545;OpenLLaMA&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#24494;&#35843;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04627
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#29983;&#21629;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#23545;OpenLlama LLM&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#26597;&#35810;&#38598;&#21512;&#65292;&#20174;&#32780;&#33021;&#22815;&#36827;&#34892;&#24494;&#35843;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#26597;&#35810;&#20013;&#35821;&#20041;&#32447;&#32034;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#30340;&#25104;&#21151;&#20026;&#22522;&#20110;LLM&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#20854;&#23454;&#26045;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#22312;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;SPARQL&#26597;&#35810;&#30340;&#20219;&#21153;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#38024;&#23545;&#29983;&#21629;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#23545;OpenLlama LLM&#36827;&#34892;&#24494;&#35843;&#30340;&#20960;&#31181;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#23637;&#24050;&#26377;&#26597;&#35810;&#38598;&#21512;&#65292;&#20174;&#32780;&#33719;&#24471;&#19968;&#32452;&#26356;&#22823;&#30340;&#35821;&#20041;&#20016;&#23500;&#30340;&#38382;&#39064;-SPARQL&#26597;&#35810;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#20351;&#22312;&#36825;&#20123;&#23545;&#31232;&#32570;&#30340;&#25968;&#25454;&#38598;&#20013;&#20063;&#33021;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#26597;&#35810;&#20013;&#35821;&#20041;&#8220;&#32447;&#32034;&#8221;&#30340;&#20316;&#29992;&#65292;&#20363;&#22914;&#26377;&#24847;&#20041;&#30340;&#21464;&#37327;&#21517;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The recent success of Large Language Models (LLM) in a wide range of Natural Language Processing applications opens the path towards novel Question Answering Systems over Knowledge Graphs leveraging LLMs. However, one of the main obstacles preventing their implementation is the scarcity of training data for the task of translating questions into corresponding SPARQL queries, particularly in the case of domain-specific KGs. To overcome this challenge, in this study, we evaluate several strategies for fine-tuning the OpenLlama LLM for question answering over life science knowledge graphs. In particular, we propose an end-to-end data augmentation approach for extending a set of existing queries over a given knowledge graph towards a larger dataset of semantically enriched question-to-SPARQL query pairs, enabling fine-tuning even for datasets where these pairs are scarce. In this context, we also investigate the role of semantic "clues" in the queries, such as meaningful variable names and
&lt;/p&gt;</description></item><item><title>MEMORYLLM&#26159;&#19968;&#20010;&#33258;&#26356;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;Transformer&#21644;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#20869;&#23384;&#27744;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25972;&#21512;&#26032;&#30693;&#35782;&#24182;&#20445;&#25345;&#38271;&#26399;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#12290;&#21363;&#20351;&#22312;&#36817;&#30334;&#19975;&#27425;&#20869;&#23384;&#26356;&#26032;&#21518;&#65292;MEMORYLLM&#20173;&#33021;&#20445;&#25345;&#25805;&#20316;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04624</link><description>&lt;p&gt;
MEMORYLLM&#65306;&#38754;&#21521;&#33258;&#26356;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MEMORYLLM: Towards Self-Updatable Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04624
&lt;/p&gt;
&lt;p&gt;
MEMORYLLM&#26159;&#19968;&#20010;&#33258;&#26356;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;Transformer&#21644;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#20869;&#23384;&#27744;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25972;&#21512;&#26032;&#30693;&#35782;&#24182;&#20445;&#25345;&#38271;&#26399;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#12290;&#21363;&#20351;&#22312;&#36817;&#30334;&#19975;&#27425;&#20869;&#23384;&#26356;&#26032;&#21518;&#65292;MEMORYLLM&#20173;&#33021;&#20445;&#25345;&#25805;&#20316;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37096;&#32626;&#21518;&#36890;&#24120;&#20445;&#25345;&#38745;&#24577;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#21521;&#27169;&#22411;&#20013;&#27880;&#20837;&#26032;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#26088;&#22312;&#26500;&#24314;&#21253;&#21547;&#30456;&#24403;&#27604;&#20363;&#30340;&#21487;&#33258;&#26356;&#26032;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#25972;&#21512;&#26032;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MEMORYLLM&#65292;&#23427;&#26159;&#19968;&#20010;&#30001;Transformer&#21644;&#22266;&#23450;&#22823;&#23567;&#30340;&#20869;&#23384;&#27744;&#32452;&#25104;&#30340;&#27169;&#22411;&#65292;&#20301;&#20110;Transformer&#30340;&#28508;&#31354;&#38388;&#20869;&#12290;MEMORYLLM&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#30693;&#35782;&#36827;&#34892;&#33258;&#25105;&#26356;&#26032;&#24182;&#35760;&#24518;&#20808;&#21069;&#27880;&#20837;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;MEMORYLLM&#26377;&#25928;&#22320;&#25972;&#21512;&#26032;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#20854;&#24615;&#33021;&#22312;&#27169;&#22411;&#32534;&#36753;&#22522;&#20934;&#19978;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#36824;&#34920;&#29616;&#20986;&#38271;&#26399;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#65292;&#36825;&#22312;&#25105;&#20204;&#33258;&#23450;&#20041;&#30340;&#35780;&#20272;&#21644;&#38271;&#19978;&#19979;&#25991;&#22522;&#20934;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;MEMORYLLM&#22312;&#36827;&#34892;&#20102;&#36817;&#30334;&#19975;&#27425;&#20869;&#23384;&#26356;&#26032;&#21518;&#65292;&#27809;&#26377;&#20219;&#20309;&#24615;&#33021;&#19979;&#38477;&#30340;&#36857;&#35937;&#65292;&#26174;&#31034;&#20986;&#25805;&#20316;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently. To this end, we introduce MEMORYLLM, a model that comprises a transformer and a fixed-size memory pool within the latent space of the transformer. MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks. Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks. MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates.
&lt;/p&gt;</description></item><item><title>InfLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;LLMs&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#23384;&#20648;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35745;&#31639;&#26426;&#21046;&#65292;&#20801;&#35768;LLMs&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#27969;&#24335;&#36755;&#20837;&#30340;&#38271;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2402.04617</link><description>&lt;p&gt;
InfLLM: &#25581;&#31034;LLMs&#23545;&#20110;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#26080;&#38656;&#35757;&#32451;&#30340;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04617
&lt;/p&gt;
&lt;p&gt;
InfLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;LLMs&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#23384;&#20648;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35745;&#31639;&#26426;&#21046;&#65292;&#20801;&#35768;LLMs&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#27969;&#24335;&#36755;&#20837;&#30340;&#38271;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#20855;&#26377;&#28459;&#38271;&#20256;&#36755;&#36755;&#20837;&#30340;&#29616;&#23454;&#24212;&#29992;&#30340;&#22522;&#30707;&#65292;&#22914;LLM&#39537;&#21160;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22312;&#21463;&#38480;&#26368;&#22823;&#38271;&#24230;&#24207;&#21015;&#19978;&#39044;&#35757;&#32451;&#30340;LLMs&#26080;&#27861;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#22240;&#20026;&#23384;&#22312;&#39046;&#22495;&#22806;&#21644;&#20998;&#25955;&#27880;&#24847;&#21147;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#37319;&#29992;&#28369;&#21160;&#27880;&#24847;&#21147;&#31383;&#21475;&#21644;&#20002;&#24323;&#36828;&#36317;&#31163;&#26631;&#35760;&#65292;&#20197;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#33719;&#24207;&#21015;&#20869;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#35821;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;InfLLM&#65292;&#26469;&#25581;&#31034;LLMs&#22788;&#29702;&#27969;&#24335;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;InfLLM&#23558;&#36828;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#23384;&#20648;&#21040;&#38468;&#21152;&#30340;&#20869;&#23384;&#21333;&#20803;&#20013;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#26426;&#21046;&#26469;&#26597;&#25214;&#19982;&#27880;&#24847;&#35745;&#31639;&#30456;&#20851;&#30340;&#26631;&#35760;&#21333;&#20803;&#12290;&#22240;&#27492;&#65292;InfLLM&#20801;&#35768;LLMs&#39640;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;&#35821;&#20041;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining
&lt;/p&gt;</description></item><item><title>TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04616</link><description>&lt;p&gt;
TinyLLM: &#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TinyLLM: Learning a Small Student from Multiple Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04616
&lt;/p&gt;
&lt;p&gt;
TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26356;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#36739;&#23567;&#30340;LLMs&#26356;&#28789;&#27963;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;&#22312;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#30693;&#35782;&#33976;&#39311;&#22240;&#20854;&#20986;&#33394;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#20419;&#36827;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TinyLLM&#65292;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#25945;&#24072;LLMs&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;LLM&#30340;&#26032;&#22411;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#40723;&#21169;&#23398;&#29983;LLM&#19981;&#20165;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#32780;&#19988;&#29702;&#35299;&#36825;&#20123;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;&#37492;&#20110;&#19981;&#21516;&#30340;LLMs&#20855;&#26377;&#19981;&#21516;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#21560;&#25910;&#26469;&#33258;&#22810;&#20010;&#25945;&#24072;LLMs&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#32769;&#24072;&#24378;&#21046;&#27169;&#22359;...
&lt;/p&gt;
&lt;p&gt;
Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#30340;&#20449;&#23454;&#24615;&#19982;&#21487;&#20449;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#36825;&#20123;&#35299;&#37322;&#22312;&#36923;&#36753;&#19978;&#26159;&#21512;&#20046;&#24773;&#29702;&#19988;&#36830;&#36143;&#30340;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#19982;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#19968;&#33268;&#65292;&#24341;&#21457;&#23545;&#20854;&#20449;&#23454;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2402.04614</link><description>&lt;p&gt;
&#20449;&#23454;&#24615;&#19982;&#21487;&#20449;&#24230;: &#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#30340;(&#19981;)&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#30340;&#20449;&#23454;&#24615;&#19982;&#21487;&#20449;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#36825;&#20123;&#35299;&#37322;&#22312;&#36923;&#36753;&#19978;&#26159;&#21512;&#20046;&#24773;&#29702;&#19988;&#36830;&#36143;&#30340;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#19982;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#19968;&#33268;&#65292;&#24341;&#21457;&#23545;&#20854;&#20449;&#23454;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#37096;&#32626;&#20026;&#20960;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#29616;&#20195;LLMs&#21487;&#20197;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#65288;SEs&#65289;&#65292;&#36825;&#20123;SEs&#25581;&#31034;&#20102;&#23427;&#20204;&#35299;&#37322;&#20854;&#34892;&#20026;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#30001;&#20110;&#20854;&#23545;&#35805;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#29305;&#28857;&#65292;&#33258;&#25105;&#35299;&#37322;&#24050;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#20449;&#23454;&#24615;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLMs&#29983;&#25104;&#30340;SEs&#20013;&#20449;&#23454;&#24615;&#21644;&#21487;&#20449;&#24230;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;LLMs&#25797;&#38271;&#29983;&#25104;&#21487;&#20449;&#30340;&#35299;&#37322;-&#23545;&#20154;&#31867;&#29992;&#25143;&#26469;&#35828;&#20284;&#20046;&#36923;&#36753;&#21644;&#36830;&#36143;-&#20294;&#36825;&#20123;&#35299;&#37322;&#26410;&#24517;&#19982;LLMs&#30340;&#25512;&#29702;&#36807;&#31243;&#30456;&#19968;&#33268;&#65292;&#24341;&#21457;&#23545;&#20854;&#20449;&#23454;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#24403;&#21069;&#36235;&#21183;&#26159;&#20026;&#20102;&#29992;&#25143;&#21451;&#22909;&#30028;&#38754;&#30340;&#38656;&#27714;&#32780;&#22686;&#21152;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#21487;&#33021;&#20250;&#20197;&#38477;&#20302;&#35299;&#37322;&#30340;&#20449;&#23454;&#24615;&#20026;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31243;&#24207;&#21592;-&#35299;&#37322;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#21518;&#32534;&#36753;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20132;&#21449;&#39046;&#22495;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#36229;&#36234;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#21518;&#32534;&#36753;&#26041;&#27861;&#65292;&#25552;&#21319;&#20102;GPT-3.5&#22312;&#36923;&#36753;&#24418;&#24335;&#21040;&#25991;&#26412;&#36716;&#25442;&#21644;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04609</link><description>&lt;p&gt;
&#36890;&#36807;LLM&#21518;&#32534;&#36753;&#25913;&#36827;&#36328;&#39046;&#22495;&#20302;&#36164;&#28304;&#25991;&#26412;&#29983;&#25104;&#65306;&#19968;&#31181;&#31243;&#24207;&#21592;-&#35299;&#37322;&#22120;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31243;&#24207;&#21592;-&#35299;&#37322;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#21518;&#32534;&#36753;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20132;&#21449;&#39046;&#22495;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#36229;&#36234;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#21518;&#32534;&#36753;&#26041;&#27861;&#65292;&#25552;&#21319;&#20102;GPT-3.5&#22312;&#36923;&#36753;&#24418;&#24335;&#21040;&#25991;&#26412;&#36716;&#25442;&#21644;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#32534;&#36753;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;GPT-3.5&#25110;GPT-4&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#26032;&#20854;&#21442;&#25968;&#20197;&#25552;&#21319;&#25991;&#26412;&#36136;&#37327;&#19981;&#21487;&#34892;&#25110;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21518;&#32534;&#36753;&#21487;&#33021;&#20250;&#38480;&#21046;LLM&#22312;&#39046;&#22495;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#32534;&#36753;&#31574;&#30053;&#23545;&#20110;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#24182;&#19981;&#26159;&#26368;&#20339;&#35774;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31243;&#24207;&#21592;-&#35299;&#37322;&#22120;&#26041;&#27861;&#65292;&#22312;&#32534;&#36753;LLM&#30340;&#36755;&#20986;&#26102;&#20445;&#30041;&#20102;&#20854;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20013;&#30340;&#32534;&#36753;&#25805;&#20316;&#19987;&#38376;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20132;&#21449;&#39046;&#22495;&#29615;&#22659;&#20013;&#65292;&#31243;&#24207;&#21592;-&#35299;&#37322;&#22120;&#26174;&#33879;&#25552;&#21319;&#20102;GPT-3.5&#22312;&#36923;&#36753;&#24418;&#24335;&#21040;&#25991;&#26412;&#36716;&#25442;&#21644;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;LLM&#21518;&#32534;&#36753;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive. However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text-generation tasks. To address these limitations, we propose a neural programmer-interpreter approach that preserves the domain generalization ability of LLMs when editing their output. The editing actions in this framework are specifically devised for text generation. Extensive experiments demonstrate that the programmer-interpreter significantly enhances GPT-3.5's performance in logical form-to-text conversion and low-resource machine translation, surpassing other state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#23545;&#40784;&#30340;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22120;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#36807;&#24230;&#20462;&#27491;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#20462;&#27491;&#65292;&#25552;&#39640;&#20462;&#27491;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04601</link><description>&lt;p&gt;
Alirector: &#25552;&#21319;&#23545;&#40784;&#30340;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22120;
&lt;/p&gt;
&lt;p&gt;
Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#23545;&#40784;&#30340;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22120;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#36807;&#24230;&#20462;&#27491;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#20462;&#27491;&#65292;&#25552;&#39640;&#20462;&#27491;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;CGEC&#65289;&#22312;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#26102;&#38754;&#20020;&#20005;&#37325;&#30340;&#36807;&#24230;&#20462;&#27491;&#25361;&#25112;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#20462;&#27491;&#38382;&#39064;&#65292;&#20294;&#24456;&#38590;&#36866;&#24212;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#23545;&#40784;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#35299;&#20915;&#36807;&#24230;&#20462;&#27491;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#20462;&#27491;&#27169;&#22411;&#65292;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#21021;&#22987;&#20462;&#27491;&#12290;&#28982;&#21518;&#65292;&#23558;&#28304;&#21477;&#23376;&#19982;&#21021;&#22987;&#20462;&#27491;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#21478;&#19968;&#36718;&#20462;&#27491;&#65292;&#20197;&#20419;&#20351;&#23545;&#40784;&#27169;&#22411;&#19987;&#27880;&#20110;&#28508;&#22312;&#30340;&#36807;&#24230;&#20462;&#27491;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#35782;&#21035;&#32454;&#24494;&#24046;&#21035;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#28304;&#21477;&#23376;&#21644;&#21021;&#22987;&#20462;&#27491;&#30340;&#36870;&#21521;&#23545;&#40784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#23545;&#40784;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;CGEC&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#20462;&#27491;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs). While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs. In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs. Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment kn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;UltraLink&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#30693;&#35782;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#21270;&#29305;&#23450;&#30693;&#35782;&#19978;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#21457;&#29616;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#20943;&#23569;&#20102;&#35821;&#35328;&#26080;&#20851;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.04588</link><description>&lt;p&gt;
UltraLink: &#19968;&#20010;&#24320;&#28304;&#30340;&#30693;&#35782;&#22686;&#24378;&#22810;&#35821;&#35328;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;UltraLink&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#30693;&#35782;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#21270;&#29305;&#23450;&#30693;&#35782;&#19978;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#21457;&#29616;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#20943;&#23569;&#20102;&#35821;&#35328;&#26080;&#20851;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#25991;&#19978;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#30417;&#30563;&#24494;&#35843;&#30340;&#30740;&#31350;&#36824;&#30456;&#23545;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#19982;&#20043;&#21069;&#31616;&#21333;&#32763;&#35793;&#33521;&#25991;&#25351;&#20196;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;LLMs&#30340;&#35821;&#35328;&#29305;&#23450;&#21644;&#35821;&#35328;&#26080;&#20851;&#33021;&#21147;&#12290;&#23545;&#20110;&#35821;&#35328;&#29305;&#23450;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25552;&#21462;LLMs&#26356;&#22810;&#30340;&#25991;&#21270;&#29305;&#23450;&#30693;&#35782;&#65292;&#25552;&#39640;&#23427;&#20204;&#20026;&#19981;&#21516;&#22269;&#23478;&#29992;&#25143;&#26381;&#21153;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#35821;&#35328;&#26080;&#20851;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#29616;&#20195;LLMs&#23637;&#29616;&#20986;&#24456;&#24378;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#22240;&#27492;&#22810;&#27425;&#23398;&#20064;&#30456;&#21516;&#20869;&#23481;&#30340;&#22810;&#31181;&#35821;&#35328;&#24182;&#19981;&#24517;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#35821;&#35328;&#26080;&#20851;SFT&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-source large language models (LLMs) have gained significant strength across diverse fields. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual supervised fine-tuning. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. For language-specific abilities, we introduce a knowledge-grounded data augmentation approach to elicit more culture-specific knowledge of LLMs, improving their ability to serve users from different countries. For language-agnostic abilities, we find through experiments that modern LLMs exhibit strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic SFT data w
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#22312;&#20449;&#20219;&#28216;&#25103;&#20013;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#20559;&#35265;&#21644;&#23545;&#20195;&#29702;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.04559</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Model Agents Simulate Human Trust Behaviors?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04559
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#22312;&#20449;&#20219;&#28216;&#25103;&#20013;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#20559;&#35265;&#21644;&#23545;&#20195;&#29702;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#32463;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#20316;&#20026;&#27169;&#25311;&#24037;&#20855;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#22312;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;LLM&#20195;&#29702;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20154;&#31867;&#20114;&#21160;&#20013;&#26368;&#20851;&#38190;&#30340;&#34892;&#20026;&#20043;&#19968;&#65292;&#20449;&#20219;&#65292;&#26088;&#22312;&#35843;&#26597;LLM&#20195;&#29702;&#26159;&#21542;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#65292;&#22312;&#34987;&#34892;&#20026;&#32463;&#27982;&#23398;&#24191;&#27867;&#25509;&#21463;&#30340;&#20449;&#20219;&#28216;&#25103;&#26694;&#26550;&#19979;&#65292;LLM&#20195;&#29702;&#36890;&#24120;&#34920;&#29616;&#20986;&#20449;&#20219;&#34892;&#20026;&#65292;&#31216;&#20026;&#20195;&#29702;&#20449;&#20219;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#20195;&#29702;&#22312;&#20449;&#20219;&#34892;&#20026;&#26041;&#38754;&#19982;&#20154;&#31867;&#20855;&#26377;&#36739;&#39640;&#30340;&#34892;&#20026;&#19968;&#33268;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;LLM&#20195;&#29702;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20195;&#29702;&#20449;&#20219;&#20013;&#30340;&#20559;&#35265;&#20197;&#21450;&#20195;&#29702;&#20449;&#20219;&#22312;&#23545;&#20195;&#29702;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#26041;&#38754;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21253;&#25324;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#22312;&#20869;&#30340;&#26465;&#20214;&#19979;&#20195;&#29702;&#20449;&#20219;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;-&#33050;&#26412;&#30693;&#35782;&#20849;&#20139;&#26550;&#26500;&#65292;&#21033;&#29992;&#21508;&#31181;&#35821;&#35328;&#33050;&#26412;&#20013;&#25991;&#26412;&#34920;&#31034;&#30340;&#36328;&#20851;&#27880;&#21644;&#23545;&#40784;&#65292;&#20197;&#25552;&#39640;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#20013;&#30340;&#24773;&#24863;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04542</link><description>&lt;p&gt;
&#20998;&#20139;&#24744;&#24050;&#32463;&#30693;&#36947;&#30340;&#19996;&#35199;&#65306;&#29992;&#20110;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#30340;&#24773;&#24863;&#26816;&#27979;&#30340;&#36328;&#35821;&#35328;-&#33050;&#26412;&#36716;&#31227;&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Share What You Already Know: Cross-Language-Script Transfer and Alignment for Sentiment Detection in Code-Mixed Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;-&#33050;&#26412;&#30693;&#35782;&#20849;&#20139;&#26550;&#26500;&#65292;&#21033;&#29992;&#21508;&#31181;&#35821;&#35328;&#33050;&#26412;&#20013;&#25991;&#26412;&#34920;&#31034;&#30340;&#36328;&#20851;&#27880;&#21644;&#23545;&#40784;&#65292;&#20197;&#25552;&#39640;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#20013;&#30340;&#24773;&#24863;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#28151;&#21512;&#28041;&#21450;&#22810;&#31181;&#35821;&#35328;&#30340;&#28151;&#21512;&#12290;&#36825;&#26159;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#26085;&#30410;&#21457;&#29983;&#30340;&#29616;&#35937;&#12290;&#36890;&#24120;&#65292;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#26159;&#29992;&#21333;&#19968;&#33050;&#26412;&#32534;&#20889;&#30340;&#65292;&#21363;&#20351;&#28041;&#21450;&#30340;&#35821;&#35328;&#26377;&#19981;&#21516;&#30340;&#33050;&#26412;&#12290;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#20027;&#35201;&#21033;&#29992;&#35821;&#35328;&#30340;&#26412;&#26426;&#33050;&#26412;&#20013;&#30340;&#25968;&#25454;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#20195;&#30721;&#28151;&#21512;&#30340;&#25991;&#26412;&#34987;&#30452;&#25509;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#27599;&#31181;&#35821;&#35328;&#30340;&#26412;&#26426;&#33050;&#26412;&#21487;&#20197;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#32780;&#29983;&#25104;&#26356;&#22909;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;-&#33050;&#26412;&#30693;&#35782;&#20849;&#20139;&#26550;&#26500;&#65292;&#21033;&#29992;&#21508;&#31181;&#35821;&#35328;&#33050;&#26412;&#20013;&#25991;&#26412;&#34920;&#31034;&#30340;&#36328;&#20851;&#27880;&#21644;&#23545;&#40784;&#12290;&#22312;&#21253;&#21547;&#23612;&#27850;&#23572;&#35821;-&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27169;&#22411;&#35299;&#37322;&#25216;&#26415;&#23545;&#27169;&#22411;&#30340;&#35299;&#37322;&#26174;&#31034;&#20102;&#30693;&#35782;&#20849;&#20139;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-switching entails mixing multiple languages. It is an increasingly occurring phenomenon in social media texts. Usually, code-mixed texts are written in a single script, even though the languages involved have different scripts. Pre-trained multilingual models primarily utilize the data in the native script of the language. In existing studies, the code-switched texts are utilized as they are. However, using the native script for each language can generate better representations of the text owing to the pre-trained knowledge. Therefore, a cross-language-script knowledge sharing architecture utilizing the cross attention and alignment of the representations of text in individual language scripts was proposed in this study. Experimental results on two different datasets containing Nepali-English and Hindi-English code-switched texts, demonstrate the effectiveness of the proposed method. The interpretation of the model using model explainability technique illustrates the sharing of la
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;SumRec&#65292;&#29992;&#20110;&#20174;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#20013;&#25512;&#33616;&#20010;&#24615;&#21270;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#23545;&#35805;&#20013;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#25688;&#35201;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#31867;&#22411;&#25512;&#33616;&#29289;&#21697;&#20449;&#24687;&#65292;&#23454;&#39564;&#35777;&#26126;SumRec&#26694;&#26550;&#27604;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2402.04523</link><description>&lt;p&gt;
SumRec: &#20351;&#29992;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#36827;&#34892;&#25512;&#33616;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SumRec: A Framework for Recommendation using Open-Domain Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;SumRec&#65292;&#29992;&#20110;&#20174;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#20013;&#25512;&#33616;&#20010;&#24615;&#21270;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#23545;&#35805;&#20013;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#25688;&#35201;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#31867;&#22411;&#25512;&#33616;&#29289;&#21697;&#20449;&#24687;&#65292;&#23454;&#39564;&#35777;&#26126;SumRec&#26694;&#26550;&#27604;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#23545;&#35805;&#21253;&#21547;&#30528;&#20851;&#20110;&#35828;&#35805;&#32773;&#20852;&#36259;&#12289;&#20559;&#22909;&#21644;&#32463;&#39564;&#30340;&#22823;&#37327;&#26377;&#29992;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#21033;&#29992;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#23545;&#35805;&#20013;&#30340;&#30693;&#35782;&#26469;&#20010;&#24615;&#21270;&#21508;&#31181;&#31995;&#32479;&#24182;&#20026;&#39640;&#32423;&#20449;&#24687;&#25552;&#20379;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#23545;&#35805;&#20013;&#25512;&#33616;&#20449;&#24687;&#30340;&#26032;&#39062;&#26694;&#26550;SumRec&#12290;&#35813;&#30740;&#31350;&#36824;&#20351;&#29992;ChatRec&#36825;&#20010;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26469;&#26816;&#39564;&#35813;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#21462;&#35828;&#35805;&#32773;&#21644;&#29289;&#21697;&#30340;&#29305;&#24449;&#65292;SumRec&#26694;&#26550;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#23545;&#35805;&#20013;&#29983;&#25104;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#25688;&#35201;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#31867;&#22411;&#25512;&#33616;&#29289;&#21697;&#20449;&#24687;&#12290;&#28982;&#21518;&#23558;&#35828;&#35805;&#32773;&#21644;&#29289;&#21697;&#20449;&#24687;&#36755;&#20837;&#21040;&#35780;&#20998;&#20272;&#35745;&#27169;&#22411;&#20013;&#65292;&#29983;&#25104;&#25512;&#33616;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SumRec&#26694;&#26550;&#27604;&#20351;&#29992;&#23545;&#35805;&#21644;&#21407;&#22987;&#29289;&#21697;&#25551;&#36848;&#30340;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chat dialogues contain considerable useful information about a speaker's interests, preferences, and experiences.Thus, knowledge from open-domain chat dialogue can be used to personalize various systems and offer recommendations for advanced information.This study proposed a novel framework SumRec for recommending information from open-domain chat dialogue.The study also examined the framework using ChatRec, a newly constructed dataset for training and evaluation. To extract the speaker and item characteristics, the SumRec framework employs a large language model (LLM) to generate a summary of the speaker information from a dialogue and to recommend information about an item according to the type of user.The speaker and item information are then input into a score estimation model, generating a recommendation score.Experimental results show that the SumRec framework provides better recommendations than the baseline method of using dialogues and item descriptions in their original form.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#21040;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#25512;&#36831;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27969;&#25968;&#25454;&#22788;&#29702;&#20013;&#21516;&#26102;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.04513</link><description>&lt;p&gt;
&#22312;&#27969;&#25968;&#25454;&#19978;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#30340;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Cascade Learning for Efficient Inference over Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04513
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#21040;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#25512;&#36831;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27969;&#25968;&#25454;&#22788;&#29702;&#20013;&#21516;&#26102;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#22238;&#31572;&#20851;&#20110;&#25968;&#25454;&#27969;&#30340;&#22797;&#26434;&#26597;&#35810;&#26041;&#38754;&#20855;&#26377;&#22825;&#28982;&#30340;&#20248;&#21183;&#65292;&#20294;&#26159; LLM &#25512;&#29702;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#65292;&#36825;&#26159;&#39318;&#20010;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#36825;&#37324;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#22120;&#65289;&#24320;&#22987;&#65292;&#21040;&#24378;&#22823;&#30340; LLM &#32467;&#26463;&#65292;&#24182;&#37197;&#22791;&#19968;&#20010;&#20915;&#23450;&#22312;&#32473;&#23450;&#36755;&#20837;&#19978;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#30340;&#25512;&#36831;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#22312;&#32447;&#23398;&#20064;&#32423;&#32852;&#30340;&#20219;&#21153;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20026;&#35813;&#38382;&#39064;&#25552;&#20379;&#20102;&#26080;&#36951;&#25022;&#31639;&#27861;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982; LLM &#30456;&#24403;&#65292;&#21516;&#26102;&#23558;&#25512;&#29702;&#25104;&#26412;&#21066;&#20943;&#20102;&#22810;&#36798; 90%&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#27969;&#22788;&#29702;&#20013;&#30340;&#25928;&#33021;&#21644;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25193;&#23637;&#20102;&#23618;&#29702;&#35770;&#27169;&#22411;&#20174;&#35789;&#27719;&#27495;&#20041;&#21040;&#31687;&#31456;&#27495;&#20041;&#65292;&#36890;&#36807;&#35745;&#31639;&#26032;&#30340;&#19978;&#19979;&#25991;&#24615;&#24230;&#37327;&#65292;&#21457;&#29616;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#27604;&#20363;&#22823;&#24133;&#22686;&#21152;&#65292;&#24182;&#36890;&#36807;&#23558;Winograd Schema&#24314;&#27169;&#20026;Bell-CHSH&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#23618;&#29702;&#35770;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#25351;&#20195;&#27495;&#20041;&#30340;&#33258;&#28982;&#35821;&#35328;&#25361;&#25112;&#19978;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04505</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#27495;&#20041;&#30340;&#23618;&#29702;&#35770;&#27169;&#22411;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Developments in Sheaf-Theoretic Models of Natural Language Ambiguities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25193;&#23637;&#20102;&#23618;&#29702;&#35770;&#27169;&#22411;&#20174;&#35789;&#27719;&#27495;&#20041;&#21040;&#31687;&#31456;&#27495;&#20041;&#65292;&#36890;&#36807;&#35745;&#31639;&#26032;&#30340;&#19978;&#19979;&#25991;&#24615;&#24230;&#37327;&#65292;&#21457;&#29616;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#27604;&#20363;&#22823;&#24133;&#22686;&#21152;&#65292;&#24182;&#36890;&#36807;&#23558;Winograd Schema&#24314;&#27169;&#20026;Bell-CHSH&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#23618;&#29702;&#35770;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#25351;&#20195;&#27495;&#20041;&#30340;&#33258;&#28982;&#35821;&#35328;&#25361;&#25112;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#26159;&#25968;&#23398;&#23545;&#35937;&#65292;&#30001;&#22522;&#30784;&#26500;&#25104;&#30340;&#25299;&#25169;&#31354;&#38388;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#25968;&#25454;&#32452;&#25104;&#65292;&#20363;&#22914;&#23450;&#20041;&#22312;&#24320;&#38598;&#19978;&#30340;&#36830;&#32493;&#20989;&#25968;&#12290;&#23618;&#26368;&#21021;&#29992;&#20110;&#20195;&#25968;&#25299;&#25169;&#21644;&#36923;&#36753;&#20013;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#20063;&#29992;&#20110;&#24314;&#27169;&#29289;&#29702;&#23454;&#39564;&#21644;&#33258;&#28982;&#35821;&#35328;&#28040;&#23696;&#36807;&#31243;&#31561;&#20107;&#20214;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#20174;&#35789;&#27719;&#27495;&#20041;&#25193;&#23637;&#21040;&#30001;&#25351;&#20195;&#20135;&#29983;&#30340;&#31687;&#31456;&#27495;&#20041;&#12290;&#39318;&#20808;&#65292;&#23545;&#19968;&#32452;&#22522;&#26412;&#30340;&#25351;&#20195;&#31687;&#31456;&#25968;&#25454;&#35745;&#31639;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#19979;&#25991;&#24615;&#24230;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#27604;&#20363;&#26356;&#39640;&#65292;&#20026;82.9%&#65292;&#32780;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#26377;3.17%&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#21253;&#21547;&#25351;&#20195;&#27495;&#20041;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25361;&#25112;&#8212;&#8212;Winograd Schema&#24314;&#27169;&#20026;Bell-CHSH&#22330;&#26223;&#65292;&#20854;&#19978;&#19979;&#25991;&#27604;&#20363;&#20026;0.096&#12290;
&lt;/p&gt;
&lt;p&gt;
Sheaves are mathematical objects consisting of a base which constitutes a topological space and the data associated with each open set thereof, e.g. continuous functions defined on the open sets. Sheaves have originally been used in algebraic topology and logic. Recently, they have also modelled events such as physical experiments and natural language disambiguation processes. We extend the latter models from lexical ambiguities to discourse ambiguities arising from anaphora. To begin, we calculated a new measure of contextuality for a dataset of basic anaphoric discourses, resulting in a higher proportion of contextual models--82.9%--compared to previous work which only yielded 3.17% contextual models. Then, we show how an extension of the natural language processing challenge, known as the Winograd Schema, which involves anaphoric ambiguities can be modelled on the Bell-CHSH scenario with a contextual fraction of 0.096.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26799;&#24230;&#35745;&#31639;&#30340;&#22797;&#26434;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#21442;&#25968;&#21306;&#22495;&#20869;&#21487;&#20197;&#20197;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#36827;&#34892;&#21069;&#21521;&#35745;&#31639;&#65292;&#20294;&#22312;&#20854;&#20313;&#21442;&#25968;&#21306;&#22495;&#20869;&#38656;&#35201;&#36229;&#36807;&#20108;&#27425;&#26102;&#38388;&#65292;&#36825;&#23545;&#20110;LLM&#35757;&#32451;&#30340;&#27599;&#20010;&#27493;&#39588;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.04497</link><description>&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26799;&#24230;&#35745;&#31639;&#30340;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Fine-Grained Complexity of Gradient Computation for Training Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26799;&#24230;&#35745;&#31639;&#30340;&#22797;&#26434;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#21442;&#25968;&#21306;&#22495;&#20869;&#21487;&#20197;&#20197;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#36827;&#34892;&#21069;&#21521;&#35745;&#31639;&#65292;&#20294;&#22312;&#20854;&#20313;&#21442;&#25968;&#21306;&#22495;&#20869;&#38656;&#35201;&#36229;&#36807;&#20108;&#27425;&#26102;&#38388;&#65292;&#36825;&#23545;&#20110;LLM&#35757;&#32451;&#30340;&#27599;&#20010;&#27493;&#39588;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#20316;&#20986;&#20102;&#22522;&#26412;&#36129;&#29486;&#12290;&#35201;&#35757;&#32451;&#19968;&#20010;LLM&#65292;&#20154;&#20204;&#38656;&#35201;&#20132;&#26367;&#36816;&#34892;&#8220;&#21069;&#21521;&#35745;&#31639;&#8221;&#21644;&#8220;&#21453;&#21521;&#35745;&#31639;&#8221;&#12290;&#21069;&#21521;&#35745;&#31639;&#21487;&#20197;&#35270;&#20026;&#27880;&#24847;&#21147;&#20989;&#25968;&#30340;&#35780;&#20272;&#65292;&#32780;&#21518;&#21521;&#35745;&#31639;&#21487;&#20197;&#35270;&#20026;&#26799;&#24230;&#35745;&#31639;&#12290;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;[Alman&#21644;Song&#65292;NeurIPS 2023]&#35777;&#26126;&#22312;&#26576;&#20123;&#21442;&#25968;&#21306;&#22495;&#20013;&#21069;&#21521;&#27493;&#39588;&#21487;&#20197;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#25191;&#34892;&#65292;&#20294;&#22312;&#20854;&#20313;&#21442;&#25968;&#21306;&#22495;&#20869;&#65292;&#38500;&#38750;&#27969;&#34892;&#30340;&#20551;&#35774;SETH&#19981;&#25104;&#31435;&#65292;&#21542;&#21017;&#27809;&#26377;&#30495;&#27491;&#30340;&#20122;&#20108;&#27425;&#26102;&#38388;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#35745;&#31639;&#21333;&#23618;&#27880;&#24847;&#21147;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#20197;&#21450;LLM&#35757;&#32451;&#30340;&#25972;&#20010;&#36807;&#31243;&#20013;&#20284;&#20046;&#26356;&#38590;&#30340;&#38382;&#39064;&#20960;&#20046;&#23436;&#20840;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;&#36825;&#23436;&#20840;&#21051;&#30011;&#20102;LLM&#35757;&#32451;&#27599;&#20010;&#27493;&#39588;&#30340;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made fundamental contributions over the last a few years. To train an LLM, one needs to alternatingly run `forward' computations and `backward' computations. The forward computation can be viewed as attention function evaluation, and the backward computation can be viewed as a gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it was proved that the forward step can be performed in almost-linear time in certain parameter regimes, but that there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis SETH is false. In this work, we show nearly identical results for the harder-seeming problem of computing the gradient of loss function of one layer attention network, and thus for the entire process of LLM training. This completely characterizes the fine-grained complexity of every step of LLM training.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26469;&#33258;&#19977;&#20010;OpenAI&#35821;&#35328;&#27169;&#22411;&#30340;&#25925;&#20107;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-3&#30340;&#26356;&#26032;&#29256;&#26412;&#36880;&#28176;&#20986;&#29616;&#20102;&#8220;&#27169;&#24335;&#23849;&#28291;&#8221;&#30340;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.04477</link><description>&lt;p&gt;
&#36890;&#36807;&#21465;&#36848;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27169;&#24335;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Detecting Mode Collapse in Language Models via Narration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04477
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26469;&#33258;&#19977;&#20010;OpenAI&#35821;&#35328;&#27169;&#22411;&#30340;&#25925;&#20107;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-3&#30340;&#26356;&#26032;&#29256;&#26412;&#36880;&#28176;&#20986;&#29616;&#20102;&#8220;&#27169;&#24335;&#23849;&#28291;&#8221;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#20004;&#20010;&#20316;&#32773;&#20889;&#20316;&#26041;&#24335;&#30456;&#21516;&#12290;&#20174;&#35789;&#27719;&#21040;&#20462;&#36766;&#25163;&#27861;&#65292;&#22312;&#20070;&#38754;&#21465;&#36848;&#20013;&#21576;&#29616;&#20986;&#30340;&#20010;&#20154;&#29305;&#33394;&#65292;&#26263;&#31034;&#20102;&#19968;&#20301;&#29305;&#23450;&#30340;&#20316;&#32773;&#65292;&#25991;&#23398;&#29702;&#35770;&#23478;&#23558;&#20854;&#31216;&#20026;&#38544;&#21547;&#25110;&#34394;&#25311;&#20316;&#32773;&#65292;&#19982;&#25991;&#26412;&#30340;&#23454;&#38469;&#20316;&#32773;&#25110;&#21465;&#36848;&#32773;&#19981;&#21516;&#12290;&#26089;&#26399;&#20351;&#29992;&#26469;&#33258;&#21508;&#31181;&#19981;&#21327;&#35843;&#26469;&#28304;&#30340;&#26410;&#32463;&#36807;&#28388;&#30340;&#35757;&#32451;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#20102;&#19981;&#36830;&#36143;&#30340;&#20010;&#24615;&#65292;&#36825;&#23545;&#20110;&#23545;&#35805;&#20219;&#21153;&#26469;&#35828;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#20294;&#23545;&#20110;&#20174;&#22810;&#20010;&#35266;&#28857;&#37319;&#26679;&#25991;&#23398;&#21364;&#26159;&#26377;&#29992;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23545;&#40784;&#30740;&#31350;&#26041;&#38754;&#21462;&#24471;&#30340;&#25104;&#21151;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23545;&#35821;&#35328;&#27169;&#22411;&#26045;&#21152;&#20027;&#35266;&#19968;&#33268;&#30340;&#20154;&#29289;&#24418;&#35937;&#65292;&#20294;&#23545;&#40784;&#27169;&#22411;&#26159;&#21542;&#20445;&#30041;&#20102;&#23545;&#27169;&#25311;&#20219;&#24847;&#34394;&#25311;&#20316;&#32773;&#30340;&#33021;&#21147;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#23457;&#26597;&#12290;&#36890;&#36807;&#30740;&#31350;&#26469;&#33258;&#19977;&#20010;OpenAI&#35821;&#35328;&#27169;&#22411;&#30340;4,374&#20010;&#25925;&#20107;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#30528;GPT-3&#30340;&#26356;&#26032;&#29256;&#26412;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#8220;&#27169;&#24335;&#23849;&#28291;&#8221;&#29616;&#35937;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
No two authors write alike. Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author--what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text. Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively consistent personae on language models via instruction tuning and reinforcement learning from human feedback (RLHF), but whether aligned models retain the ability to model an arbitrary virtual author has received little scrutiny. By studying 4,374 stories sampled from three OpenAI language models, we show successive versions of GPT-3 suffer from increasing degrees of "mode collapse" whereby overf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32593;&#39029;&#25130;&#22270;&#20013;&#20803;&#32032;&#30340;&#8220;&#21452;&#35270;&#22270;&#8221;&#26469;&#36827;&#34892; HTML &#20803;&#32032;&#30340;&#32972;&#26223;&#21270;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#27599;&#20010;&#20803;&#32032;&#19982;&#20854;&#37051;&#23621;&#20803;&#32032;&#36827;&#34892;&#32972;&#26223;&#21270;&#65292;&#20351;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20351;&#24471;HTML&#20803;&#32032;&#30340;&#32467;&#26524;&#34920;&#31034;&#23545;&#20110;&#20195;&#29702;&#25191;&#34892;&#25805;&#20316;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#12290;</title><link>https://arxiv.org/abs/2402.04476</link><description>&lt;p&gt;
&#21452;&#35270;&#22270;&#35270;&#35273;&#32972;&#26223;&#21270;&#30340;&#32593;&#39029;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Dual-View Visual Contextualization for Web Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32593;&#39029;&#25130;&#22270;&#20013;&#20803;&#32032;&#30340;&#8220;&#21452;&#35270;&#22270;&#8221;&#26469;&#36827;&#34892; HTML &#20803;&#32032;&#30340;&#32972;&#26223;&#21270;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#27599;&#20010;&#20803;&#32032;&#19982;&#20854;&#37051;&#23621;&#20803;&#32032;&#36827;&#34892;&#32972;&#26223;&#21270;&#65292;&#20351;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20351;&#24471;HTML&#20803;&#32032;&#30340;&#32467;&#26524;&#34920;&#31034;&#23545;&#20110;&#20195;&#29702;&#25191;&#34892;&#25805;&#20316;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32593;&#39029;&#23548;&#33322;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#22312;&#23454;&#38469;&#32593;&#31449;&#19978;&#25191;&#34892;&#22797;&#26434;&#21644;&#22810;&#26679;&#20219;&#21153;&#30340;&#32593;&#32476;&#20195;&#29702;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#26159;&#20197; HTML &#25991;&#26723;&#20316;&#20026;&#36755;&#20837;&#65292;HTML &#25991;&#26723;&#23450;&#20041;&#20102;&#32593;&#39029;&#30340;&#20869;&#23481;&#21644;&#25805;&#20316;&#31354;&#38388;&#65288;&#21363;&#21487;&#25805;&#20316;&#20803;&#32032;&#21644;&#25805;&#20316;&#65289;&#12290;&#28982;&#32780;&#65292;HTML &#25991;&#26723;&#21487;&#33021;&#26080;&#27861;&#20026;&#27599;&#20010;&#20803;&#32032;&#25552;&#20379;&#28165;&#26224;&#30340;&#20219;&#21153;&#30456;&#20851;&#32972;&#26223;&#65292;&#20351;&#24471;&#36873;&#25321;&#27491;&#30830;&#30340;&#65288;&#19968;&#31995;&#21015;&#30340;&#65289;&#25805;&#20316;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#32593;&#39029;&#25130;&#22270;&#20013;&#20803;&#32032;&#30340;&#8220;&#21452;&#35270;&#22270;&#8221;&#26469;&#36827;&#34892; HTML &#20803;&#32032;&#30340;&#32972;&#26223;&#21270;&#65306;&#27599;&#20010; HTML &#20803;&#32032;&#22312;&#25130;&#22270;&#20013;&#26377;&#20854;&#23545;&#24212;&#30340;&#36793;&#30028;&#26694;&#21644;&#35270;&#35273;&#20869;&#23481;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#27934;&#23519;&#21147;&#8212;&#8212;&#32593;&#39029;&#24320;&#21457;&#32773;&#20542;&#21521;&#20110;&#22312;&#32593;&#39029;&#19978;&#23558;&#20219;&#21153;&#30456;&#20851;&#20803;&#32032;&#25918;&#32622;&#22312;&#38468;&#36817;&#20197;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#65292;&#24182;&#25552;&#20986;&#23558;&#27599;&#20010;&#20803;&#32032;&#19982;&#20854;&#37051;&#23621;&#20803;&#32032;&#36827;&#34892;&#32972;&#26223;&#21270;&#65292;&#20351;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;HTML &#20803;&#32032;&#30340;&#32467;&#26524;&#34920;&#31034;&#23545;&#20110;&#20195;&#29702;&#25191;&#34892;&#25805;&#20316;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic web navigation aims to build a web agent that can follow language instructions to execute complex and diverse tasks on real-world websites. Existing work primarily takes HTML documents as input, which define the contents and action spaces (i.e., actionable elements and operations) of webpages. Nevertheless, HTML documents may not provide a clear task-related context for each element, making it hard to select the right (sequence of) actions. In this paper, we propose to contextualize HTML elements through their "dual views" in webpage screenshots: each HTML element has its corresponding bounding box and visual content in the screenshot. We build upon the insight -- web developers tend to arrange task-related elements nearby on webpages to enhance user experiences -- and propose to contextualize each element with its neighbor elements, using both textual and visual features. The resulting representations of HTML elements are more informative for the agent to take action. We val
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29992;&#20110;&#21307;&#29983;-AI&#20250;&#35786;&#30340;&#21333;&#27425;&#20998;&#31867;&#30340;&#23884;&#20837;&#25216;&#26415;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#23884;&#20837;&#25216;&#26415;&#33021;&#22815;&#21487;&#38752;&#19988;&#28789;&#27963;&#22320;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;Word2Vec&#12289;GloVe&#21644;&#23383;&#31526;ngram&#23884;&#20837;&#25216;&#26415;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04442</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#21307;&#29983;-AI&#20250;&#35786;&#30340;&#21333;&#27425;&#20998;&#31867;&#30340;&#23884;&#20837;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Evaluating Embeddings for One-Shot Classification of Doctor-AI Consultations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29992;&#20110;&#21307;&#29983;-AI&#20250;&#35786;&#30340;&#21333;&#27425;&#20998;&#31867;&#30340;&#23884;&#20837;&#25216;&#26415;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#23884;&#20837;&#25216;&#26415;&#33021;&#22815;&#21487;&#38752;&#19988;&#28789;&#27963;&#22320;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;Word2Vec&#12289;GloVe&#21644;&#23383;&#31526;ngram&#23884;&#20837;&#25216;&#26415;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#21307;&#24739;&#27807;&#36890;&#23545;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#24739;&#32773;&#25252;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#25216;&#26415;&#21644;&#21333;&#27425;&#20998;&#31867;&#31995;&#32479;&#26469;&#23545;&#21307;&#29983;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#21307;&#30103;&#20250;&#35786;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#20998;&#26512;&#35832;&#22914;&#35789;&#34955;&#27169;&#22411;&#12289;&#23383;&#31526;ngram&#12289;Word2Vec&#12289;GloVe&#12289;fastText&#21644;GPT2&#31561;&#23884;&#20837;&#25216;&#26415;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#21333;&#27425;&#20998;&#31867;&#31995;&#32479;&#22312;&#21307;&#30103;&#20250;&#35786;&#20013;&#22914;&#20309;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#23884;&#20837;&#25216;&#26415;&#33021;&#22815;&#21487;&#38752;&#19988;&#28789;&#27963;&#22320;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;Word2Vec&#12289;GloVe&#21644;&#23383;&#31526;ngram&#23884;&#20837;&#25216;&#26415;&#34920;&#29616;&#33391;&#22909;&#65292;&#34920;&#26126;&#23427;&#20204;&#36866;&#29992;&#20110;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#24314;&#27169;&#12290;GPT2&#23884;&#20837;&#25216;&#26415;&#20063;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#23427;&#20063;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#26174;&#33879;&#25552;&#39640;&#20102;&#21307;&#30103;&#23545;&#35805;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective communication between healthcare providers and patients is crucial to providing high-quality patient care. In this work, we investigate how Doctor-written and AI-generated texts in healthcare consultations can be classified using state-of-the-art embeddings and one-shot classification systems. By analyzing embeddings such as bag-of-words, character n-grams, Word2Vec, GloVe, fastText, and GPT2 embeddings, we examine how well our one-shot classification systems capture semantic information within medical consultations. Results show that the embeddings are capable of capturing semantic features from text in a reliable and adaptable manner. Overall, Word2Vec, GloVe and Character n-grams embeddings performed well, indicating their suitability for modeling targeted to this task. GPT2 embedding also shows notable performance, indicating its suitability for models tailored to this task as well. Our machine learning architectures significantly improved the quality of health conversati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.04437</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Structured Entity Extraction Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#24433;&#21709;&#20102;&#20449;&#24687;&#25552;&#21462;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#21644;&#35268;&#33539;&#21270;&#20102;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#65288;SEE&#65289;&#20219;&#21153;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#36817;&#20284;&#23454;&#20307;&#38598;&#37325;&#21472;&#65288;AESOP&#65289;&#24230;&#37327;&#65292;&#20197;&#36866;&#24403;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#24182;&#34892;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#39046;&#22495;&#30340;&#26410;&#26469;&#36827;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DFA-LLM&#65288;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#24378;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23884;&#20837;&#20174;&#23545;&#35805;&#20013;&#23398;&#20064;&#21040;&#30340;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#24471;&#23545;&#35805;&#20195;&#29702;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#35268;&#33539;&#21512;&#35268;&#24615;&#30340;&#22238;&#22797;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04411</link><description>&lt;p&gt;
Chatbot&#36935;&#35265;&#31649;&#36947;&#65306;&#21033;&#29992;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#36827;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DFA-LLM&#65288;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#24378;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23884;&#20837;&#20174;&#23545;&#35805;&#20013;&#23398;&#20064;&#21040;&#30340;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#24471;&#23545;&#35805;&#20195;&#29702;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#35268;&#33539;&#21512;&#35268;&#24615;&#30340;&#22238;&#22797;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#22686;&#24378;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;DFA-LLM&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#21319;&#23545;&#35805;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#20256;&#32479;&#30340;LLM&#22312;&#29305;&#23450;&#24773;&#26223;&#65288;&#22914;&#24773;&#24863;&#25903;&#25345;&#21644;&#23458;&#25143;&#26381;&#21153;&#65289;&#20013;&#29983;&#25104;&#35268;&#33539;&#21512;&#35268;&#30340;&#22238;&#22797;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;&#20174;&#35757;&#32451;&#23545;&#35805;&#20013;&#23398;&#20064;&#21040;&#30340;&#30830;&#23450;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;DFA&#65289;&#23884;&#20837;&#21040;LLM&#20013;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#20351;&#24471;LLM&#33021;&#22815;&#25353;&#29031;DFA&#25351;&#23548;&#30340;&#30830;&#23450;&#24615;&#22238;&#24212;&#36335;&#24452;&#26469;&#22238;&#24212;&#12290;DFA-LLM&#30340;&#20248;&#21183;&#21253;&#25324;&#21487;&#35299;&#37322;&#24615;&#32467;&#26500;&#65292;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#22238;&#22797;&#26816;&#32034;&#20197;&#21450;&#19982;&#29616;&#26377;LLM&#30340;&#21363;&#25554;&#21363;&#29992;&#20860;&#23481;&#24615;&#12290;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#39564;&#35777;&#20102;DFA-LLM&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#26377;&#28508;&#21147;&#25104;&#20026;&#23545;&#35805;&#20195;&#29702;&#39046;&#22495;&#30340;&#26377;&#20215;&#20540;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Definite Finite Automaton augmented large language model (DFA-LLM), a novel framework designed to enhance the capabilities of conversational agents using large language models (LLMs). Traditional LLMs face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the LLM. This structured approach enables the LLM to adhere to a deterministic response pathway, guided by the DFA. The advantages of DFA-LLM include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing LLMs. Extensive benchmarks validate DFA-LLM's effectiveness, indicating its potential as a valuable contribution to the conversational agent.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20010;&#24615;&#21270;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#27169;&#22359;&#65288;PEFT&#65289;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27665;&#20027;&#21270;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25317;&#26377;&#21644;&#20351;&#29992;&#20182;&#20204;&#33258;&#24049;&#30340;LLM&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#23450;&#21046;&#33021;&#21147;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04401</link><description>&lt;p&gt;
&#36890;&#36807;&#20010;&#24615;&#21270;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04401
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20010;&#24615;&#21270;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#27169;&#22359;&#65288;PEFT&#65289;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27665;&#20027;&#21270;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25317;&#26377;&#21644;&#20351;&#29992;&#20182;&#20204;&#33258;&#24049;&#30340;LLM&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#23450;&#21046;&#33021;&#21147;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#20010;&#24615;&#21270;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#26088;&#22312;&#20351;LLM&#30340;&#20132;&#20114;&#12289;&#20869;&#23481;&#21644;&#25512;&#33616;&#19982;&#20010;&#20307;&#29992;&#25143;&#20559;&#22909;&#30456;&#19968;&#33268;&#12290;&#26368;&#36817;LLM&#20010;&#24615;&#21270;&#30340;&#36827;&#23637;&#32858;&#28966;&#20110;&#26377;&#25928;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#36890;&#36807;&#20351;&#29992;&#34892;&#20026;&#21382;&#21490;&#26816;&#32034;&#21644;&#25991;&#26412;&#27010;&#35201;&#31561;&#38750;&#21442;&#25968;&#21270;&#30693;&#35782;&#20016;&#23500;&#29992;&#25143;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#27169;&#22411;&#25152;&#26377;&#26435;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#20102;&#19968;&#23450;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#23450;&#21046;&#33021;&#21147;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#29992;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;OPPU&#30340;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20010;&#24615;&#21270;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PEFT&#65289;&#27169;&#22359;&#26469;&#23384;&#20648;&#29992;&#25143;&#29305;&#23450;&#30340;&#34892;&#20026;&#27169;&#24335;&#21644;&#20559;&#22909;&#12290;&#36890;&#36807;&#25554;&#20837;&#29992;&#25143;&#30340;&#20010;&#20154;PEFT&#21442;&#25968;&#65292;&#20182;&#20204;&#21487;&#20197;&#25317;&#26377;&#21644;&#20351;&#29992;&#20182;&#20204;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization in large language models (LLMs) is increasingly important, aiming to align LLM's interactions, content, and recommendations with individual user preferences. Recent advances in LLM personalization have spotlighted effective prompt design, by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues. Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic. To address these shortcomings, we introduce One PEFT Per User (OPPU), which employs personalized parameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior patterns and preferences. By plugging in users' personal PEFT parameters, they can own and use their LLMs personally. OPPU integrates parametric user knowledge in the personal PEFT parame
&lt;/p&gt;</description></item><item><title>QuIP#&#26159;&#19968;&#31181;&#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.04396</link><description>&lt;p&gt;
QuIP#: &#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#36827;&#34892;&#26356;&#22909;&#30340;LLM&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04396
&lt;/p&gt;
&lt;p&gt;
QuIP#&#26159;&#19968;&#31181;&#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#36890;&#36807;&#23558;LLM&#30340;&#26435;&#37325;&#37327;&#21270;&#20026;&#20302;&#31934;&#24230;&#26469;&#20943;&#23569;&#20854;&#20869;&#23384;&#21344;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QuIP#&#65292;&#19968;&#31181;&#20165;&#22522;&#20110;&#26435;&#37325;&#30340;PTQ&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#26032;&#25216;&#26415;&#65292;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;($\le$ 4&#27604;&#29305;&#27599;&#20010;&#26435;&#37325;)&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;QuIP#&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#21704;&#36798;&#29595;&#24503;&#21464;&#25442;&#25913;&#36827;&#20102;QuIP&#20013;&#30340;&#38750;&#30456;&#24178;&#22788;&#29702;&#65292;&#35813;&#26041;&#27861;&#26356;&#24555;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35770;&#29305;&#24615;&#12290;&#20854;&#27425;&#65292;QuIP#&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21033;&#29992;&#20102;&#38750;&#30456;&#24178;&#26435;&#37325;&#20855;&#26377;&#30340;&#29699;&#24418;&#20122;&#39640;&#26031;&#20998;&#24067;&#29305;&#24615;&#65306;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#22522;&#20110;&#39640;&#24230;&#23545;&#31216;$E_8$&#26684;&#20070;&#30340;&#30828;&#20214;&#39640;&#25928;&#20195;&#30721;&#20070;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;8&#32500;&#21333;&#20301;&#29699;&#35013;&#22635;&#12290;&#31532;&#19977;&#65292;QuIP#&#20351;&#29992;&#24494;&#35843;&#26469;&#25552;&#39640;&#23545;&#21407;&#22987;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;QuIP#&#20248;&#20110;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#26032;&#30340;PTQ&#25193;&#23637;&#34892;&#20026;&#65292;&#24182;&#25903;&#25345;&#24555;&#36895;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21253;&#25324;Deepfakes&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;Deepfakes&#33021;&#22815;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#25913;&#21464;&#20107;&#23454;&#65292;&#32780;LLMs&#21017;&#20855;&#26377;&#29983;&#25104;&#36890;&#29992;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#36947;&#24503;&#20351;&#29992;&#26159;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.04373</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#19990;&#30028;&#65306;Deepfakes&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The World of Generative AI: Deepfakes and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04373
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21253;&#25324;Deepfakes&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;Deepfakes&#33021;&#22815;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#25913;&#21464;&#20107;&#23454;&#65292;&#32780;LLMs&#21017;&#20855;&#26377;&#29983;&#25104;&#36890;&#29992;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#36947;&#24503;&#20351;&#29992;&#26159;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#29983;&#27963;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#26102;&#20195;&#12290;Deepfakes&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;GenAI&#30340;&#20004;&#20010;&#20363;&#23376;&#12290;&#23588;&#20854;&#26159;Deepfakes&#23545;&#31038;&#20250;&#26500;&#25104;&#20102;&#20196;&#20154;&#25285;&#24551;&#30340;&#23041;&#32961;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#24182;&#25913;&#21464;&#20107;&#23454;&#12290;LLM&#26159;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#36890;&#29992;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#29983;&#25104;&#24615;&#36136;&#65292;&#22914;&#26524;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#65292;&#20063;&#21487;&#33021;&#23545;&#20154;&#20204;&#26500;&#25104;&#39118;&#38505;&#12290;&#36825;&#31687;&#31616;&#30701;&#30340;&#25991;&#31456;&#35797;&#22270;&#25506;&#32034;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We live in the era of Generative Artificial Intelligence (GenAI). Deepfakes and Large Language Models (LLMs) are two examples of GenAI. Deepfakes, in particular, pose an alarming threat to society as they are capable of spreading misinformation and changing the truth. LLMs are powerful language models that generate general-purpose language. However due to its generative aspect, it can also be a risk for people if used with ill intentions. The ethical use of these technologies is a big concern. This short article tries to find out the interrelationship between them.
&lt;/p&gt;</description></item><item><title>Hedgehog&#26159;&#19968;&#31181;&#20855;&#26377;Softmax&#27169;&#20223;&#30340;&#21487;&#23398;&#20064;&#32447;&#24615;&#27880;&#24847;&#21147;&#65292;&#36890;&#36807;&#20445;&#25345;&#23574;&#38160;&#21644;&#21333;&#35843;&#24615;&#26469;&#24357;&#34917;&#32447;&#24615;&#27880;&#24847;&#21147;&#22312;&#36136;&#37327;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.04347</link><description>&lt;p&gt;
&#21050;&#29484;&#19982;&#35946;&#29482;&#65306;&#20855;&#26377;Softmax&#27169;&#20223;&#30340;&#34920;&#36798;&#24615;&#32447;&#24615;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Hedgehog &amp; the Porcupine: Expressive Linear Attentions with Softmax Mimicry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04347
&lt;/p&gt;
&lt;p&gt;
Hedgehog&#26159;&#19968;&#31181;&#20855;&#26377;Softmax&#27169;&#20223;&#30340;&#21487;&#23398;&#20064;&#32447;&#24615;&#27880;&#24847;&#21147;&#65292;&#36890;&#36807;&#20445;&#25345;&#23574;&#38160;&#21644;&#21333;&#35843;&#24615;&#26469;&#24357;&#34917;&#32447;&#24615;&#27880;&#24847;&#21147;&#22312;&#36136;&#37327;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#27880;&#24847;&#21147;&#24050;&#32463;&#26174;&#31034;&#20986;&#25552;&#39640;Transformer&#25928;&#29575;&#30340;&#28508;&#21147;&#65292;&#23558;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38477;&#20302;&#20026;&#19982;&#24207;&#21015;&#38271;&#24230;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#36825;&#23545;&#20110;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#20855;&#26377;&#28608;&#21160;&#20154;&#24515;&#30340;&#21069;&#26223;&#65306;&#65288;1&#65289;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#32447;&#24615;Transformer&#65292;&#65288;2&#65289;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;Transformer&#36827;&#34892;&#8220;&#24494;&#35843;-&#36716;&#25442;&#8221;&#20026;&#32447;&#24615;&#29256;&#26412;&#65292;&#24182;&#24674;&#22797;&#20219;&#21153;&#24615;&#33021;&#65292;&#20197;&#21450;&#65288;3&#65289;&#23558;&#35832;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31561;Transformer&#36827;&#34892;&#8220;&#39044;&#35757;&#32451;-&#36716;&#25442;&#8221;&#65292;&#20197;&#23454;&#29616;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;&#22312;&#36136;&#37327;&#19978;&#24120;&#24120;&#19981;&#22914;&#26631;&#20934;&#30340;softmax&#27880;&#24847;&#21147;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#32570;&#20047;&#19982;&#33391;&#22909;&#24615;&#33021;&#30456;&#20851;&#30340;softmax&#27880;&#24847;&#21147;&#30340;&#20851;&#38190;&#23646;&#24615;&#65306;&#20302;&#29109;&#65288;&#25110;&#8220;&#23574;&#23792;&#8221;&#65289;&#26435;&#37325;&#21644;&#28857;&#31215;&#21333;&#35843;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#31616;&#21333;&#29305;&#24449;&#26144;&#23556;&#65292;&#20445;&#30041;&#20102;&#36825;&#20123;&#23646;&#24615;&#65292;&#24182;&#19982;softmax&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#65292;&#20294;&#22312;&#32447;&#24615;&#27880;&#24847;&#21147;&#20013;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hedgehog&#65292;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#65292;&#20445;&#25345;&#20102;&#23574;&#23792;&#21644;&#21333;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) "finetuned-conversion" of task-specific Transformers into linear versions that recover task performance, and (3) "pretrained-conversion" of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or "spiky") weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monoton
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;LLMs&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21644;&#20351;&#29992;&#38381;&#28304;LLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#23454;&#39564;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#27861;&#24459;&#36829;&#35268;&#34892;&#20026;&#30340;&#26816;&#27979;&#21644;&#19982;&#21463;&#23475;&#32773;&#30340;&#20851;&#32852;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#35774;&#32622;&#36866;&#29992;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;F1&#20998;&#25968;&#20998;&#21035;&#20026;62.69&#65285;&#21644;81.02&#65285;&#12290;&#30740;&#31350;&#26368;&#32456;&#20844;&#24320;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#20197;&#25512;&#21160;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.04335</link><description>&lt;p&gt;
LegalLens: &#21033;&#29992;LLMs&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#35782;&#21035;&#27861;&#24459;&#36829;&#35268;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;LLMs&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21644;&#20351;&#29992;&#38381;&#28304;LLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#23454;&#39564;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#27861;&#24459;&#36829;&#35268;&#34892;&#20026;&#30340;&#26816;&#27979;&#21644;&#19982;&#21463;&#23475;&#32773;&#30340;&#20851;&#32852;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#35774;&#32622;&#36866;&#29992;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;F1&#20998;&#25968;&#20998;&#21035;&#20026;62.69&#65285;&#21644;81.02&#65285;&#12290;&#30740;&#31350;&#26368;&#32456;&#20844;&#24320;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#20197;&#25512;&#21160;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#65292;&#31532;&#19968;&#20010;&#26159;&#26816;&#27979;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#27861;&#24459;&#36829;&#35268;&#34892;&#20026;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#36825;&#20123;&#36829;&#35268;&#34892;&#20026;&#19982;&#21487;&#33021;&#21463;&#24433;&#21709;&#30340;&#20010;&#20154;&#20851;&#32852;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#30001;&#39046;&#22495;&#19987;&#23478;&#27880;&#37322;&#36827;&#34892;&#39564;&#35777;&#12290;&#20004;&#20010;&#20219;&#21153;&#37117;&#26159;&#20026;&#38598;&#20307;&#35785;&#35772;&#26696;&#24773;&#22659;&#29305;&#21035;&#35774;&#35745;&#30340;&#12290;&#23454;&#39564;&#35774;&#35745;&#37319;&#29992;&#20102;&#26469;&#33258;BERT&#31995;&#21015;&#21644;&#24320;&#28304;LLMs&#30340;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38381;&#28304;LLMs&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#21487;&#29992;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#20854;&#36829;&#35268;&#34892;&#20026;&#35782;&#21035;&#30340;F1&#20998;&#25968;&#20026;62.69&#65285;&#65292;&#19982;&#21463;&#23475;&#32773;&#30456;&#20851;&#30340;&#20998;&#25968;&#20026;81.02&#65285;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#29992;&#20110;&#23454;&#39564;&#30340;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#20197;&#25512;&#21160;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on two main tasks, the first for detecting legal violations within unstructured textual data, and the second for associating these violations with potentially affected individuals. We constructed two datasets using Large Language Models (LLMs) which were subsequently validated by domain expert annotators. Both tasks were designed specifically for the context of class-action cases. The experimental design incorporated fine-tuning models from the BERT family and open-source LLMs, and conducting few-shot experiments using closed-source LLMs. Our results, with an F1-score of 62.69\% (violation identification) and 81.02\% (associating victims), show that our datasets and setups can be used for both tasks. Finally, we publicly release the datasets and the code used for the experiments in order to advance further research in the area of legal natural language processing (NLP).
&lt;/p&gt;</description></item><item><title>LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.04333</link><description>&lt;p&gt;
LESS&#65306;&#29992;&#20110;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#30340;&#36873;&#25321;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LESS: Selecting Influential Data for Targeted Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04333
&lt;/p&gt;
&lt;p&gt;
LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20351;&#29992;&#32452;&#21512;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#36890;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#24448;&#24448;&#38656;&#35201;&#19968;&#22871;&#19987;&#38376;&#30340;&#25216;&#33021;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#12290;&#25361;&#25112;&#22312;&#20110;&#20174;&#36825;&#20123;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#26368;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#20197;&#26377;&#25928;&#24320;&#21457;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#20272;&#35745;&#25968;&#25454;&#24433;&#21709;&#24182;&#25191;&#34892;&#36866;&#29992;&#20110;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#30340;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#20851;&#38190;&#22312;&#20110;LESS&#23558;&#29616;&#26377;&#30340;&#24433;&#21709;&#20844;&#24335;&#35843;&#25972;&#20026;&#19982;Adam&#20248;&#21270;&#22120;&#21644;&#21487;&#21464;&#38271;&#24230;&#25351;&#20196;&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#12290;LESS&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#32500;&#26799;&#24230;&#29305;&#24449;&#30340;&#39640;&#24230;&#21487;&#37325;&#29992;&#21644;&#21487;&#20256;&#36882;&#30340;&#26799;&#24230;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20855;&#26377;&#29305;&#23450;&#33021;&#21147;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#30456;&#20284;&#24230;&#36873;&#25321;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;t
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#24341;&#29992;&#30340;&#26377;&#25928;&#26694;&#26550;&#65292;&#24182;&#22312;&#24120;&#35265;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.04315</link><description>&lt;p&gt;
&#20351;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#24341;&#29992;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Training Language Models to Generate Text with Citations via Fine-grained Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#24341;&#29992;&#30340;&#26377;&#25928;&#26694;&#26550;&#65292;&#24182;&#22312;&#24120;&#35265;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#26041;&#38754;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#22238;&#31572;&#24120;&#24120;&#32570;&#20047;&#21487;&#38752;&#26469;&#28304;&#30340;&#24341;&#29992;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#30452;&#35266;&#26041;&#27861;&#26159;&#23558;&#22806;&#37096;&#25991;&#26723;&#30340;&#24341;&#29992;&#20316;&#20026;&#35777;&#25454;&#21253;&#21547;&#22312;&#25991;&#26412;&#20013;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#30452;&#25509;&#20419;&#20351;LLMs&#29983;&#25104;&#24341;&#29992;&#25991;&#26412;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#36828;&#38750;&#20196;&#20154;&#28385;&#24847;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36739;&#23567;&#30340;LLMs&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#25945;&#25480;LLMs&#29983;&#25104;&#39640;&#24230;&#25903;&#25345;&#21644;&#30456;&#20851;&#30340;&#24341;&#29992;&#65292;&#21516;&#26102;&#30830;&#20445;&#20854;&#21709;&#24212;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#23558;&#36825;&#20123;&#32454;&#31890;&#24230;&#22870;&#21169;&#24212;&#29992;&#20110;&#24120;&#35265;&#30340;LLMs&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#20256;&#32479;&#20570;&#27861;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#20174;ALCE&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#21462;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources. An intuitive solution to these issues would be to include in-text citations referring to external documents as evidence. While previous works have directly prompted LLMs to generate in-text citations, their performances are far from satisfactory, especially when it comes to smaller LLMs. In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses. We also conduct a systematic analysis of applying these fine-grained rewards to common LLM training strategies, demonstrating its advantage over conventional practices. We conduct extensive experiments on Question Answering (QA) datasets taken from the ALCE benchmark and validate the model's gene
&lt;/p&gt;</description></item><item><title>BiLLM&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#30340;&#26435;&#37325;&#21644;&#20248;&#21270;&#20108;&#20540;&#21270;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04291</link><description>&lt;p&gt;
BiLLM: &#25512;&#21160;LLMs&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04291
&lt;/p&gt;
&lt;p&gt;
BiLLM&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#30340;&#26435;&#37325;&#21644;&#20248;&#21270;&#20108;&#20540;&#21270;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#36890;&#29992;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20294;&#23545;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#26377;&#24456;&#22823;&#30340;&#38656;&#27714;&#12290;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20108;&#20540;&#21270;&#21487;&#20197;&#23558;&#27169;&#22411;&#26435;&#37325;&#26497;&#22823;&#22320;&#20943;&#23569;&#21040;&#20165;1&#20301;&#65292;&#38477;&#20302;&#20102;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37327;&#21270;&#25216;&#26415;&#22312;&#36229;&#20302;&#20301;&#23485;&#19979;&#26080;&#27861;&#20445;&#25345;LLM&#30340;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BiLLM&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLM&#23450;&#21046;&#30340;&#24320;&#21019;&#24615;&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#12290;&#22522;&#20110;LLMs&#30340;&#26435;&#37325;&#20998;&#24067;&#65292;BiLLM&#39318;&#20808;&#35782;&#21035;&#21644;&#32467;&#26500;&#36873;&#25321;&#37325;&#35201;&#30340;&#26435;&#37325;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#20108;&#20540;&#21270;&#27531;&#24046;&#36924;&#36817;&#31574;&#30053;&#26469;&#26368;&#23567;&#21270;&#21387;&#32553;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#38750;&#37325;&#35201;&#26435;&#37325;&#30340;&#38047;&#24418;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#20998;&#21106;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#22320;&#23558;&#23427;&#20204;&#20998;&#32452;&#21644;&#20108;&#20540;&#21270;&#12290;BiLLM&#39318;&#27425;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy infere
&lt;/p&gt;</description></item><item><title>ProtAgents&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24179;&#21488;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#35299;&#20915;&#22797;&#26434;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#35774;&#35745;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04268</link><description>&lt;p&gt;
ProtAgents: &#36890;&#36807;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#36827;&#34892;&#34507;&#30333;&#36136;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04268
&lt;/p&gt;
&lt;p&gt;
ProtAgents&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24179;&#21488;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#35299;&#20915;&#22797;&#26434;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#35774;&#35745;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#36229;&#36234;&#33258;&#28982;&#30028;&#20013;&#24050;&#26377;&#30340;&#34507;&#30333;&#36136;&#23545;&#20110;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#30340;&#36827;&#23637;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#30446;&#21069;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27169;&#22411;&#65292;&#22914;&#23558;&#34507;&#30333;&#36136;&#32467;&#26500;&#19982;&#26448;&#26009;&#24615;&#36136;&#30456;&#20114;&#20851;&#32852;&#30340;&#26367;&#20195;&#27169;&#22411;&#26469;&#35299;&#20915;&#31471;&#21040;&#31471;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#38598;&#20013;&#20110;&#29305;&#23450;&#30340;&#26448;&#26009;&#30446;&#26631;&#25110;&#32467;&#26500;&#29305;&#24615;&#65292;&#24403;&#38656;&#35201;&#23558;&#39046;&#22495;&#22806;&#30340;&#30693;&#35782;&#32435;&#20837;&#21040;&#35774;&#35745;&#36807;&#31243;&#25110;&#36827;&#34892;&#32508;&#21512;&#25968;&#25454;&#20998;&#26512;&#26102;&#65292;&#23427;&#20204;&#30340;&#28789;&#27963;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;ProtAgents&#24179;&#21488;&#65292;&#29992;&#20110;&#26032;&#22411;&#34507;&#30333;&#36136;&#35774;&#35745;&#65292;&#20854;&#20013;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#30340;AI&#26234;&#33021;&#20307;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#21327;&#20316;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#36890;&#36807;&#26234;&#33021;&#20307;&#30340;&#22810;&#26679;&#21457;&#23637;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25317;&#26377;&#19987;&#38271;&#65292;&#21253;&#25324;&#30693;&#35782;&#26816;&#32034;&#12289;&#34507;&#30333;&#36136;&#32467;&#26500;&#20998;&#26512;&#12289;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#27169;&#25311;&#21644;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;p&gt;
&#20248;&#20808;&#23433;&#20840;&#20445;&#38556;&#32780;&#38750;&#33258;&#27835;&#65306;&#31185;&#23398;&#20013;LLM&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33258;&#20027;&#36827;&#34892;&#23454;&#39564;&#21644;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#30340;&#28431;&#27934;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#23578;&#26410;&#23545;&#36825;&#20123;&#28431;&#27934;&#36827;&#34892;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#35823;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#27010;&#36848;&#20102;&#31185;&#23398;LLM&#26426;&#22120;&#20154;&#22266;&#26377;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#24847;&#22270;&#12289;&#29305;&#23450;&#30340;&#31185;&#23398;&#39046;&#22495;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#37096;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#28431;&#27934;&#30340;&#36215;&#28304;&#21644;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#26032;&#20107;&#20214;&#26102;&#24773;&#32490;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#27604;&#36739;&#26032;&#32463;&#39564;&#21644;&#36807;&#21435;&#35760;&#24518;&#26469;&#33719;&#24471;&#29702;&#35299;&#26032;&#32463;&#39564;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24773;&#24863;&#27979;&#35797;&#25429;&#25417;&#26234;&#33021;&#20307;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.04232</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#33021;&#22815;&#39044;&#27979;&#24773;&#24863;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Generative Agents Predict Emotion?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#26032;&#20107;&#20214;&#26102;&#24773;&#32490;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#27604;&#36739;&#26032;&#32463;&#39564;&#21644;&#36807;&#21435;&#35760;&#24518;&#26469;&#33719;&#24471;&#29702;&#35299;&#26032;&#32463;&#39564;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24773;&#24863;&#27979;&#35797;&#25429;&#25417;&#26234;&#33021;&#20307;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#35768;&#22810;&#31867;&#20284;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;LLMs&#30340;&#20849;&#24773;&#29702;&#35299;&#21644;&#24773;&#32490;&#29366;&#24577;&#23578;&#26410;&#19982;&#20154;&#31867;&#23545;&#40784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;LLM&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#26032;&#20107;&#20214;&#26102;&#24773;&#32490;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#27604;&#36739;&#26032;&#32463;&#39564;&#21644;&#36807;&#21435;&#35760;&#24518;&#26469;&#33719;&#24471;&#29702;&#35299;&#26032;&#32463;&#39564;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#31181;&#27604;&#36739;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#24773;&#22659;&#20013;&#29702;&#35299;&#26032;&#32463;&#39564;&#65292;&#26681;&#25454;&#24773;&#32490;&#35780;&#20272;&#29702;&#35770;&#65292;&#36825;&#23545;&#20110;&#24773;&#32490;&#29983;&#25104;&#33267;&#20851;&#37325;&#35201;&#12290;&#39318;&#20808;&#65292;&#26234;&#33021;&#20307;&#23558;&#26032;&#32463;&#39564;&#24863;&#30693;&#20026;&#26102;&#38388;&#24207;&#21015;&#25991;&#26412;&#25968;&#25454;&#12290;&#22312;&#24863;&#30693;&#27599;&#20010;&#26032;&#36755;&#20837;&#21518;&#65292;&#26234;&#33021;&#20307;&#29983;&#25104;&#36807;&#21435;&#30456;&#20851;&#35760;&#24518;&#30340;&#25688;&#35201;&#65292;&#34987;&#31216;&#20026;&#8220;&#35268;&#33539;&#8221;&#65292;&#24182;&#23558;&#26032;&#32463;&#39564;&#19982;&#27492;&#35268;&#33539;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#27604;&#36739;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#26234;&#33021;&#20307;&#22914;&#20309;&#22312;&#24773;&#22659;&#20013;&#23545;&#26032;&#32463;&#39564;&#20570;&#20986;&#21453;&#24212;&#12290;&#20351;&#29992;&#24773;&#24863;&#27979;&#35797;PANAS&#23545;&#26234;&#33021;&#20307;&#36827;&#34892;&#27979;&#35797;&#65292;&#25429;&#25417;&#26234;&#33021;&#20307;&#22312;&#26032;&#32463;&#39564;&#21518;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#20154;&#20204;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#20154;&#29305;&#36136;&#36807;&#24230;&#24402;&#22240;&#30340;&#29616;&#35937;&#65292;&#24182;&#21628;&#21505;&#23398;&#26415;&#30028;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20869;&#23481;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;</title><link>https://arxiv.org/abs/2402.03962</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#21453;&#23545;&#34394;&#20551;&#30340;AI&#33192;&#32960;&#24615;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03962
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#20154;&#20204;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#20154;&#29305;&#36136;&#36807;&#24230;&#24402;&#22240;&#30340;&#29616;&#35937;&#65292;&#24182;&#21628;&#21505;&#23398;&#26415;&#30028;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20869;&#23481;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#19968;&#31181;&#20542;&#21521;&#65292;&#30475;&#21040;&#21608;&#22260;&#30340;&#29289;&#20307;&#20855;&#26377;&#31867;&#20284;"&#20154;&#31867;"&#30340;&#29305;&#36136;&#12290;&#25105;&#20204;&#32473;&#27773;&#36710;&#21462;&#21517;&#23383;&#65292;&#21644;&#23456;&#29289;&#29978;&#33267;&#23478;&#29992;&#30005;&#22120;&#20132;&#35848;&#65292;&#20223;&#20315;&#23427;&#20204;&#33021;&#20687;&#20854;&#20182;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#25105;&#20204;&#12290;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#25311;&#20154;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20154;&#20204;&#22768;&#31216;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#33021;&#22815;&#23519;&#35273;&#21040;&#31867;&#20284;&#20110;&#20154;&#31867;&#26234;&#33021;&#30340;&#29305;&#36136;&#12290;&#22312;&#36825;&#31687;&#20301;&#32622;&#35770;&#25991;&#20013;&#65292;&#32771;&#34385;&#21040;&#19987;&#19994;&#28608;&#21169;&#12289;&#20154;&#31867;&#20559;&#35265;&#21644;&#19968;&#33324;&#26041;&#27861;&#35770;&#35774;&#32622;&#65292;&#25105;&#20204;&#35752;&#35770;&#22914;&#20309;&#24403;&#21069;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36861;&#27714;&#20026;&#23558;&#31867;&#20154;&#29305;&#36136;&#24402;&#22240;&#20110;LLM&#25171;&#24320;&#20102;&#28389;&#35294;&#20043;&#38376;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#21457;&#29616;&#21487;&#35299;&#37322;&#20026;&#20154;&#31867;&#30340;&#27169;&#24335;&#24182;&#19981;&#24212;&#35813;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;&#32771;&#34385;&#21040;&#23186;&#20307;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#36941;&#25551;&#20889;&#65292;&#25105;&#20204;&#21628;&#21505;&#23398;&#26415;&#30028;&#29305;&#21035;&#35880;&#24910;&#65292;&#24182;&#23545;&#23398;&#26415;&#35802;&#20449;&#21407;&#21017;&#26377;&#39069;&#22806;&#30340;&#24847;&#35782;&#65292;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20449;&#24687;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI rese
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03780</link><description>&lt;p&gt;
&#25581;&#31034;&#23459;&#20256;&#65306;&#22522;&#20110;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#35821;&#35328;&#21450;&#20854;&#25991;&#20307;&#29305;&#24449;&#12290;&#25552;&#20986;&#20102;PPN&#25968;&#25454;&#38598;&#65292;&#21363;&#23459;&#20256;&#24615;&#20266;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26426;&#26500;&#30830;&#23450;&#30340;&#23459;&#20256;&#26469;&#28304;&#32593;&#31449;&#19978;&#30340;&#26032;&#38395;&#25991;&#31456;&#32452;&#25104;&#12290;&#20174;&#35813;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#20102;&#19968;&#37096;&#20998;&#26679;&#26412;&#19982;&#26469;&#33258;&#24120;&#35268;&#27861;&#22269;&#26032;&#38395;&#30340;&#25991;&#31456;&#28151;&#21512;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;URL&#36827;&#34892;&#20102;&#25513;&#30422;&#65292;&#20197;&#36827;&#34892;&#20154;&#31867;&#27880;&#37322;&#23454;&#39564;&#65292;&#20351;&#29992;11&#20010;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#20004;&#31181;&#31867;&#22411;&#30340;&#26032;&#38395;&#32440;&#23545;&#27599;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#35782;&#21035;&#27880;&#37322;&#32773;&#20351;&#29992;&#30340;&#32447;&#32034;&#65292;&#24182;&#19982;&#26426;&#22120;&#20998;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;VAGO&#20998;&#26512;&#22120;&#36827;&#34892;&#36766;&#36848;&#27169;&#31946;&#21644;&#20027;&#35266;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#29992;TF-IDF&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#21450;&#22235;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;&#20004;&#20010;&#22522;&#20110;RoBERTa&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#21477;&#27861;&#30340;CATS&#65292;&#20197;&#21450;&#32467;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#19968;&#20010;XGBoost&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03659</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21453;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20256;&#32479;&#30340;&#38750;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#35299;&#37322;&#32929;&#31080;&#39044;&#27979;&#36890;&#24120;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#35299;&#37322;&#20165;&#38480;&#20110;&#21487;&#35270;&#21270;&#37325;&#35201;&#25991;&#26412;&#19978;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32929;&#31080;&#39044;&#27979;&#23545;LLM&#26469;&#35828;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33021;&#22815;&#26435;&#34913;&#28151;&#20081;&#31038;&#20250;&#25991;&#26412;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#38543;&#30528;&#24341;&#20837;&#35299;&#37322;&#32452;&#20214;&#65292;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#38656;&#35201;LLM&#33021;&#22815;&#29992;&#21475;&#22836;&#26041;&#24335;&#35299;&#37322;&#20026;&#20160;&#20040;&#26576;&#20123;&#22240;&#32032;&#27604;&#20854;&#20182;&#22240;&#32032;&#26356;&#37325;&#35201;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35201;&#20026;&#36825;&#26679;&#30340;&#20219;&#21153;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#38656;&#35201;&#19987;&#23478;&#26631;&#27880;&#30340;&#26679;&#26412;&#26469;&#35299;&#37322;&#35757;&#32451;&#38598;&#20013;&#30340;&#27599;&#27425;&#32929;&#31080;&#27874;&#21160;&#65292;&#36825;&#22312;&#25104;&#26412;&#21644;&#23454;&#38469;&#21487;&#25193;&#23637;&#24615;&#19978;&#26159;&#26114;&#36149;&#19988;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) 
&lt;/p&gt;</description></item><item><title>VLN-Video&#21033;&#29992;&#34892;&#36710;&#35270;&#39057;&#30340;&#22810;&#26679;&#23460;&#22806;&#29615;&#22659;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#23548;&#33322;&#25351;&#20196;&#19982;&#21160;&#20316;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03561</link><description>&lt;p&gt;
VLN-Video: &#21033;&#29992;&#34892;&#36710;&#35270;&#39057;&#36827;&#34892;&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03561
&lt;/p&gt;
&lt;p&gt;
VLN-Video&#21033;&#29992;&#34892;&#36710;&#35270;&#39057;&#30340;&#22810;&#26679;&#23460;&#22806;&#29615;&#22659;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#23548;&#33322;&#25351;&#20196;&#19982;&#21160;&#20316;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#35201;&#27714;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#36924;&#30495;&#30340;&#19977;&#32500;&#23460;&#22806;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;&#29616;&#26377;&#30340;VLN&#26041;&#27861;&#22312;&#23548;&#33322;&#29615;&#22659;&#22810;&#26679;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VLN-Video&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22312;&#32654;&#22269;&#22810;&#20010;&#22478;&#24066;&#30340;&#34892;&#36710;&#35270;&#39057;&#20013;&#23384;&#22312;&#30340;&#22810;&#26679;&#21270;&#23460;&#22806;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#23548;&#33322;&#25351;&#20196;&#21644;&#21160;&#20316;&#26469;&#25552;&#39640;&#23460;&#22806;VLN&#24615;&#33021;&#12290;VLN-Video&#32467;&#21512;&#20102;&#30452;&#35266;&#32463;&#20856;&#26041;&#27861;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#21033;&#29992;&#27169;&#26495;&#22635;&#20805;&#29983;&#25104;&#26377;&#23454;&#38469;&#22522;&#30784;&#30340;&#23548;&#33322;&#25351;&#20196;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#22270;&#20687;&#26059;&#36716;&#30456;&#20284;&#24230;&#30340;&#23548;&#33322;&#21160;&#20316;&#39044;&#27979;&#22120;&#20174;&#34892;&#36710;&#35270;&#39057;&#20013;&#33719;&#21462;VLN&#39118;&#26684;&#30340;&#25968;&#25454;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;VLN&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Touchdown&#25968;&#25454;&#38598;&#21644;&#30001;&#34892;&#36710;&#35270;&#39057;&#21019;&#24314;&#30340;&#35270;&#39057;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.02619</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Increasing Trust in Language Models through the Reuse of Verified Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#32463;&#24120;&#24573;&#30053;&#32597;&#35265;&#30340;&#36793;&#30028;&#24773;&#20917;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#21487;&#20449;&#24230;&#26631;&#20934;&#65292;&#21363;&#20219;&#21153;&#31639;&#27861;&#21644;&#30005;&#36335;&#23454;&#29616;&#24517;&#39035;&#32463;&#36807;&#39564;&#35777;&#65292;&#32771;&#34385;&#21040;&#36793;&#30028;&#24773;&#20917;&#65292;&#24182;&#19988;&#27809;&#26377;&#24050;&#30693;&#30340;&#25925;&#38556;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#28385;&#36275;&#36825;&#19968;&#26631;&#20934;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#20102;&#23436;&#20840;&#39564;&#35777;&#12290;&#20026;&#20102;&#23637;&#31034;&#32463;&#36807;&#39564;&#35777;&#30340;&#27169;&#22359;&#30340;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#25554;&#20837;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#21516;&#26102;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#26356;&#22797;&#26434;&#30340;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20219;&#21153;&#27169;&#22359;&#25554;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#21033;&#29992;&#27169;&#22411;&#30340;&#37325;&#22797;&#20351;&#29992;&#26469;&#25552;&#39640;&#21487;&#39564;&#35777;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#12289;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.01748</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#12289;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#22522;&#30784;&#27169;&#22411;&#34987;&#23459;&#31216;&#20026;6G&#31995;&#32479;&#30340;&#25913;&#21464;&#32773;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#26080;&#32447;&#32593;&#32476;&#30340;LLMs&#30340;&#21162;&#21147;&#20165;&#38480;&#20110;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#21019;&#24314;&#20197;&#26080;&#32447;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#37326;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#35774;&#35745;&#38024;&#23545;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;(AI)&#21407;&#29983;&#32593;&#32476;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#12290;&#19982;&#22522;&#20110;NLP&#30340;&#22522;&#30784;&#27169;&#22411;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#20419;&#36827;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#30340;&#35774;&#35745;&#65306;1) &#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#65292;2) &#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;3) &#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20197;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language mod- els that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network a
&lt;/p&gt;</description></item><item><title>CFTM&#26159;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#26469;&#35782;&#21035;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#30340;&#27491;&#36127;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#36319;&#36394;&#20027;&#39064;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01734</link><description>&lt;p&gt;
CFTM: &#36830;&#32493;&#26102;&#38388;&#20998;&#25968;&#35805;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CFTM: Continuous time fractional topic model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01734
&lt;/p&gt;
&lt;p&gt;
CFTM&#26159;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#26469;&#35782;&#21035;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#30340;&#27491;&#36127;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#36319;&#36394;&#20027;&#39064;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#26102;&#38388;&#20998;&#25968;&#35805;&#39064;&#27169;&#22411;&#65288;cFTM&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#65288;fBm&#65289;&#26377;&#25928;&#22320;&#35782;&#21035;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#38543;&#26102;&#38388;&#30340;&#27491;&#36127;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;cFTM&#21487;&#20197;&#25429;&#25417;&#21040;&#20027;&#39064;&#21644;&#35789;&#20998;&#24067;&#20013;&#30340;&#36825;&#20123;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#65292;&#21453;&#26144;&#20102;fBm&#30340;&#20027;&#35201;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;cFTM&#30340;&#21442;&#25968;&#20272;&#35745;&#36807;&#31243;&#19982;&#20256;&#32479;&#20027;&#39064;&#27169;&#22411;LDA&#30340;&#30456;&#24403;&#12290;&#20026;&#20102;&#35777;&#26126;cFTM&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#27982;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#36825;&#20123;&#27979;&#35797;&#30340;&#32467;&#26524;&#25903;&#25345;&#35813;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#21644;&#36319;&#36394;&#20027;&#39064;&#38543;&#26102;&#38388;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#25110;&#31895;&#31961;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the Continuous Time Fractional Topic Model (cFTM), a new method for dynamic topic modeling. This approach incorporates fractional Brownian motion~(fBm) to effectively identify positive or negative correlations in topic and word distribution over time, revealing long-term dependency or roughness. Our theoretical analysis shows that the cFTM can capture these long-term dependency or roughness in both topic and word distributions, mirroring the main characteristics of fBm. Moreover, we prove that the parameter estimation process for the cFTM is on par with that of LDA, traditional topic models. To demonstrate the cFTM's property, we conduct empirical study using economic news articles. The results from these tests support the model's ability to identify and track long-term dependency or roughness in topics over time.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#20811;&#26381;&#29616;&#26377;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01729</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Contextualization Distillation from Large Language Model for Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#20811;&#26381;&#29616;&#26377;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25991;&#26412;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#35821;&#26009;&#24211;&#20174;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#25110;&#21516;&#20041;&#35789;&#23450;&#20041;&#20013;&#25910;&#38598;&#30340;&#38745;&#24577;&#21644;&#22122;&#22768;&#24615;&#36136;&#24120;&#24120;&#38480;&#21046;&#20102;&#22522;&#20110;PLM&#30340;KGC&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#33976;&#39311;&#31574;&#30053;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21487;&#25554;&#20837;&#21644;&#21487;&#25773;&#25918;&#30340;&#26041;&#27861;&#65292;&#19982;&#21028;&#21035;&#21644;&#29983;&#25104;&#30340;KGC&#26694;&#26550;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#32039;&#20945;&#30340;&#32467;&#26500;&#21270;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#27573;&#33853;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#37325;&#24314;&#21644;&#19978;&#19979;&#25991;&#21270;&#65292;&#20351;&#36739;&#23567;&#30340;KGC&#27169;&#22411;&#33021;&#22815;&#21560;&#25910;&#36825;&#20123;&#20016;&#23500;&#30340;&#19977;&#20803;&#32452;&#20013;&#30340;&#35265;&#35299;&#12290;&#23545;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;KGC&#25216;&#26415;&#30340;&#20840;&#38754;&#35780;&#20272;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#21644;&#36866;&#24212;&#24615;&#65292;&#25581;&#31034;&#20102;&#26080;&#35770;&#22522;&#30784;&#31649;&#36947;&#22914;&#20309;&#65292;&#22987;&#32456;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#21442;&#19982;&#32773;&#23545;18&#26465;&#38543;&#26426;&#26631;&#35760;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#28040;&#24687;&#25776;&#20889;&#20013;&#30340;&#36741;&#21161;&#20351;&#29992;&#21487;&#33021;&#23545;&#24863;&#30693;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01726</link><description>&lt;p&gt;
AI&#20013;&#20171;&#20132;&#27969;&#30340;&#25351;&#23548;&#65306;AI&#19981;&#25913;&#21464;&#23545;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of Text Messages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#21442;&#19982;&#32773;&#23545;18&#26465;&#38543;&#26426;&#26631;&#35760;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#28040;&#24687;&#25776;&#20889;&#20013;&#30340;&#36741;&#21161;&#20351;&#29992;&#21487;&#33021;&#23545;&#24863;&#30693;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#20154;&#26469;&#35828;&#65292;&#28966;&#34385;&#12289;&#25233;&#37057;&#21644;&#20854;&#20182;&#31038;&#20132;&#21644;&#24515;&#29702;&#22240;&#32032;&#21487;&#33021;&#20351;&#25776;&#20889;&#25991;&#26412;&#28040;&#24687;&#25104;&#20026;&#19968;&#39033;&#31215;&#26497;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#26159;&#24110;&#21161;&#37027;&#20123;&#26412;&#26469;&#20250;&#35273;&#24471;&#21457;&#36865;&#30701;&#20449;&#22256;&#38590;&#25110;&#26377;&#21387;&#21147;&#30340;&#29992;&#25143;&#30340;&#23436;&#32654;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#24555;&#36895;&#26222;&#21450;&#65292;&#20294;&#23545;&#20854;&#22312;&#25991;&#26412;&#28040;&#24687;&#25776;&#20889;&#20013;&#30340;&#36741;&#21161;&#20351;&#29992;&#30340;&#32771;&#34385;&#36824;&#26410;&#34987;&#25506;&#32034;&#12290;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#30340;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#65292;AI&#30340;&#20844;&#20247;&#24773;&#32490;&#36739;&#24046;&#21487;&#33021;&#23548;&#33268;&#20854;&#36741;&#21161;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#20351;&#29992;&#23545;&#24863;&#30693;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#20351;&#29992;&#36866;&#24471;&#20854;&#21453;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20154;&#20204;&#26159;&#21542;&#35748;&#20026;&#19968;&#26465;&#25991;&#26412;&#28040;&#24687;&#26159;&#21542;&#22312;&#25776;&#20889;&#36807;&#31243;&#20013;&#24471;&#21040;&#20102;AI&#30340;&#36741;&#21161;&#65292;&#20250;&#25913;&#21464;&#20854;&#24863;&#30693;&#30340;&#35821;&#35843;&#12289;&#28165;&#26224;&#24230;&#21644;&#34920;&#36798;&#24847;&#22270;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;26&#21517;&#21442;&#19982;&#32773;&#23545;18&#26465;&#38543;&#26426;&#26631;&#35760;&#30340;&#39044;&#20808;&#25776;&#20889;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;&#12290;&#36890;&#36807;&#20998;&#26512;&#21442;&#19982;&#32773;&#23545;&#28040;&#24687;&#35821;&#35843;&#30340;&#35780;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many people, anxiety, depression, and other social and mental factors can make composing text messages an active challenge. To remedy this problem, large language models (LLMs) may yet prove to be the perfect tool to assist users that would otherwise find texting difficult or stressful. However, despite rapid uptake in LLM usage, considerations for their assistive usage in text message composition have not been explored. A primary concern regarding LLM usage is that poor public sentiment regarding AI introduces the possibility that its usage may harm perceptions of AI-assisted text messages, making usage counter-productive. To (in)validate this possibility, we explore how the belief that a text message did or did not receive AI assistance in composition alters its perceived tone, clarity, and ability to convey intent. In this study, we survey the perceptions of 26 participants on 18 randomly labeled pre-composed text messages. In analyzing the participants' ratings of message tone,
&lt;/p&gt;</description></item><item><title>APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets.</title><link>https://arxiv.org/abs/2402.01697</link><description>&lt;p&gt;
APT-Pipe: &#29992;&#20110;&#31038;&#20132;&#35745;&#31639;&#25968;&#25454;&#26631;&#27880;&#30340;&#33258;&#21160;&#25552;&#31034;&#35843;&#25972;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01697
&lt;/p&gt;
&lt;p&gt;
APT-Pipe is an automated prompt-tuning tool that enhances ChatGPT's text classification performance by automatically tuning prompts on any given dataset, resulting in a significant improvement in weighted F1-score across twelve experimented datasets.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#24212;&#29992;&#22312;&#31038;&#20132;&#35745;&#31639;&#25991;&#26412;&#26631;&#27880;&#20013;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#20154;&#20204;&#24050;&#32463;&#30693;&#36947;&#24615;&#33021;&#21462;&#20915;&#20110;&#36755;&#20837;&#25552;&#31034;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#26377;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#26469;&#25506;&#32034;&#25552;&#31034;&#35843;&#25972;&#30340;&#25216;&#26415;&#21644;&#25351;&#21335;&#65292;&#35797;&#22270;&#25913;&#21892;&#25552;&#31034;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#25163;&#24037;&#21162;&#21147;&#21644;&#23545;&#27491;&#22312;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#25552;&#31034;&#35843;&#25972;&#27969;&#27700;&#32447;APT-Pipe&#12290;APT-Pipe&#26088;&#22312;&#33258;&#21160;&#35843;&#25972;&#25552;&#31034;&#65292;&#20197;&#25552;&#39640;ChatGPT&#22312;&#20219;&#20309;&#32473;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;APT-Pipe&#65292;&#24182;&#22312;12&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;APT-Pipe&#35843;&#25972;&#30340;&#25552;&#31034;&#26377;&#21161;&#20110;ChatGPT&#22312;12&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#20013;&#26377;9&#20010;&#33719;&#24471;&#26356;&#39640;&#30340;&#21152;&#26435;F1&#20998;&#25968;&#65292;&#24179;&#22343;&#25913;&#36827;&#20102;7.01&#65285;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#31361;&#20986;&#20102;APT-Pipe&#20316;&#20026;&#19968;&#20010;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has highlighted the potential of LLM applications, like ChatGPT, for performing label annotation on social computing text. However, it is already well known that performance hinges on the quality of the input prompts. To address this, there has been a flurry of research into prompt tuning -- techniques and guidelines that attempt to improve the quality of prompts. Yet these largely rely on manual effort and prior knowledge of the dataset being annotated. To address this limitation, we propose APT-Pipe, an automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts to enhance ChatGPT's text classification performance on any given dataset. We implement APT-Pipe and test it across twelve distinct text classification datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher weighted F1-score on nine out of twelve experimented datasets, with an improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as a framework by 
&lt;/p&gt;</description></item><item><title>OLMo&#26159;&#19968;&#31181;&#30495;&#27491;&#24320;&#25918;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#23545;&#35821;&#35328;&#27169;&#22411;&#31185;&#23398;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.00838</link><description>&lt;p&gt;
OLMo: &#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
OLMo: Accelerating the Science of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00838
&lt;/p&gt;
&lt;p&gt;
OLMo&#26159;&#19968;&#31181;&#30495;&#27491;&#24320;&#25918;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#23545;&#35821;&#35328;&#27169;&#22411;&#31185;&#23398;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#21644;&#21830;&#19994;&#20135;&#21697;&#12290;&#38543;&#30528;&#21830;&#19994;&#37325;&#35201;&#24615;&#30340;&#22686;&#21152;&#65292;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#24050;&#32463;&#23553;&#38381;&#36215;&#26469;&#65292;&#21482;&#33021;&#36890;&#36807;&#19987;&#26377;&#25509;&#21475;&#35775;&#38382;&#65292;&#20854;&#35757;&#32451;&#25968;&#25454;&#12289;&#26550;&#26500;&#21644;&#24320;&#21457;&#32454;&#33410;&#27809;&#26377;&#36879;&#38706;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#32454;&#33410;&#23545;&#20110;&#31185;&#23398;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#20854;&#20559;&#35265;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#25105;&#20204;&#35748;&#20026;&#30740;&#31350;&#31038;&#21306;&#26377;&#26435;&#35775;&#38382;&#24378;&#22823;&#32780;&#30495;&#27491;&#24320;&#25918;&#30340;LM&#12290;&#20026;&#27492;&#65292;&#26412;&#25216;&#26415;&#25253;&#21578;&#35814;&#32454;&#20171;&#32461;&#20102;OLMo&#30340;&#39318;&#20010;&#29256;&#26412;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#12289;&#30495;&#27491;&#24320;&#25918;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#26500;&#24314;&#21644;&#30740;&#31350;&#35821;&#35328;&#24314;&#27169;&#31185;&#23398;&#30340;&#26694;&#26550;&#12290;&#19982;&#20043;&#21069;&#21482;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#21644;&#25512;&#29702;&#20195;&#30721;&#30340;&#21162;&#21147;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#24067;OLMo&#21644;&#25972;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#30721;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#21457;&#24067;&#33021;&#22686;&#24378;&#24320;&#25918;&#30740;&#31350;&#31038;&#21306;&#30340;&#33021;&#21147;&#65292;&#24182;&#28608;&#21457;&#26356;&#22810;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#30524;&#21160;&#36861;&#36394;&#25216;&#26415;&#30740;&#31350;&#20102;&#27597;&#35821;&#20026;&#27721;&#35821;&#30340;&#20154;&#23545;&#35821;&#35328;&#26223;&#35266;&#30340;&#27880;&#24847;&#28857;&#65292;&#21457;&#29616;&#20854;&#20851;&#27880;&#31243;&#24230;&#39640;&#20110;&#19968;&#33324;&#26223;&#35266;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#35821;&#35328;&#26223;&#35266;&#30340;&#20449;&#24687;&#23494;&#24230;&#36739;&#39640;&#12290;</title><link>https://arxiv.org/abs/2312.08906</link><description>&lt;p&gt;
&#20351;&#29992;&#30524;&#21160;&#36861;&#36394;&#30740;&#31350;&#27597;&#35821;&#20026;&#27721;&#35821;&#30340;&#20154;&#23545;&#35821;&#35328;&#26223;&#35266;&#22270;&#20687;&#30340;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Using eye tracking to investigate what native Chinese speakers notice about linguistic landscape images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#30524;&#21160;&#36861;&#36394;&#25216;&#26415;&#30740;&#31350;&#20102;&#27597;&#35821;&#20026;&#27721;&#35821;&#30340;&#20154;&#23545;&#35821;&#35328;&#26223;&#35266;&#30340;&#27880;&#24847;&#28857;&#65292;&#21457;&#29616;&#20854;&#20851;&#27880;&#31243;&#24230;&#39640;&#20110;&#19968;&#33324;&#26223;&#35266;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#35821;&#35328;&#26223;&#35266;&#30340;&#20449;&#24687;&#23494;&#24230;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26223;&#35266;&#26159;&#31038;&#20250;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#30524;&#21160;&#36861;&#36394;&#25216;&#26415;&#26159;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#25216;&#26415;&#12290;&#22312;&#23545;&#35821;&#35328;&#26223;&#35266;&#30340;&#30740;&#31350;&#20013;&#65292;&#24456;&#23569;&#20351;&#29992;&#30524;&#21160;&#36861;&#36394;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#21033;&#29992;&#30524;&#21160;&#36861;&#36394;&#25216;&#26415;&#30740;&#31350;&#20102;&#27597;&#35821;&#20026;&#27721;&#35821;&#30340;&#20154;&#23545;&#35821;&#35328;&#26223;&#35266;&#30340;&#23454;&#38469;&#27880;&#24847;&#28857;&#65292;&#24182;&#21457;&#29616;&#22312;&#20957;&#35270;&#26102;&#38388;&#21644;&#20957;&#35270;&#27425;&#25968;&#36825;&#20004;&#20010;&#32500;&#24230;&#19978;&#65292;&#27597;&#35821;&#20026;&#27721;&#35821;&#30340;&#20154;&#23545;&#35821;&#35328;&#26223;&#35266;&#30340;&#20851;&#27880;&#31243;&#24230;&#39640;&#20110;&#19968;&#33324;&#26223;&#35266;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#35821;&#35328;&#26223;&#35266;&#30340;&#20449;&#24687;&#23494;&#24230;&#36739;&#39640;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#36825;&#19968;&#29616;&#35937;&#30340;&#20854;&#20182;&#21487;&#33021;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linguistic landscape is an important field in sociolinguistic research. Eye tracking technology is a common technology in psychological research. There are few cases of using eye movement to study linguistic landscape. This paper uses eye tracking technology to study the actual fixation of the linguistic landscape and finds that in the two dimensions of fixation time and fixation times, the fixation of native Chinese speakers to the linguistic landscape is higher than that of the general landscape. This paper argues that this phenomenon is due to the higher information density of linguistic landscapes. At the same time, the article also discusses other possible reasons for this phenomenon.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2312.04828</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;
&lt;/p&gt;
&lt;p&gt;
Human-Readable Fingerprint for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#21644;&#37197;&#22871;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#35768;&#21487;&#35777;&#65292;&#20445;&#25252;LLM&#30340;&#29256;&#26435;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#30340;&#21442;&#25968;&#20462;&#25913;&#65292;&#30830;&#23450;LLM&#30340;&#21407;&#22987;&#22522;&#26412;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;LLM&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#22320;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#65292;&#32780;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#27169;&#22411;&#25910;&#25947;&#21518;&#65292;LLM&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#20445;&#25345;&#31283;&#23450;&#65292;&#36890;&#36807;&#21518;&#32493;&#30340;&#35757;&#32451;&#27493;&#39588;&#65292;&#21253;&#25324;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;RLHF&#65292;&#20960;&#20046;&#27809;&#26377;&#25200;&#21160;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#36275;&#22815;&#26465;&#20214;&#12290;&#36890;&#36807;&#32487;&#32493;&#35757;&#32451;LLM&#24182;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#39033;&#26469;&#25512;&#24320;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#21521;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#24517;&#35201;&#24615;&#65292;&#32467;&#26524;&#20351;&#24471;&#27169;&#22411;&#21463;&#25439;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26041;&#21521;&#23481;&#26131;&#21463;&#21040;&#31616;&#21333;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22914;&#32500;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension 
&lt;/p&gt;</description></item><item><title>CDEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#21270;&#32500;&#24230;&#12290;&#36890;&#36807;&#23545;&#20845;&#20010;&#25991;&#21270;&#32500;&#24230;&#21644;&#19971;&#20010;&#39046;&#22495;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#21270;&#29305;&#24449;&#12289;&#19968;&#33268;&#24615;&#21644;&#24046;&#24322;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#26102;&#34701;&#20837;&#25991;&#21270;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.16421</link><description>&lt;p&gt;
CDEval&#65306;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#32500;&#24230;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16421
&lt;/p&gt;
&lt;p&gt;
CDEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#21270;&#32500;&#24230;&#12290;&#36890;&#36807;&#23545;&#20845;&#20010;&#25991;&#21270;&#32500;&#24230;&#21644;&#19971;&#20010;&#39046;&#22495;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#21270;&#29305;&#24449;&#12289;&#19968;&#33268;&#24615;&#21644;&#24046;&#24322;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#26102;&#34701;&#20837;&#25991;&#21270;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25193;&#23637;&#26174;&#33879;&#22686;&#24378;&#20854;&#33021;&#21147;&#65292;&#23545;&#30830;&#20445;&#20854;&#36127;&#36131;&#20219;&#21644;&#20262;&#29702;&#20351;&#29992;&#30340;&#23545;&#40784;&#38382;&#39064;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#23545;&#40784;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26222;&#19990;&#20215;&#20540;&#35266;&#65288;&#22914;HHH&#21407;&#21017;&#65289;&#19978;&#65292;&#20294;&#25991;&#21270;&#36825;&#19968;&#26412;&#36136;&#19978;&#26159;&#20247;&#22810;&#22810;&#20803;&#21270;&#30340;&#32500;&#24230;&#21364;&#26410;&#24471;&#21040;&#20805;&#20998;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;CDEval&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#25991;&#21270;&#32500;&#24230;&#12290;CDEval&#36890;&#36807;&#32467;&#21512;GPT-4&#30340;&#33258;&#21160;&#29983;&#25104;&#21644;&#20154;&#24037;&#39564;&#35777;&#26500;&#24314;&#65292;&#22312;&#19971;&#20010;&#39046;&#22495;&#28085;&#30422;&#20102;&#20845;&#20010;&#25991;&#21270;&#32500;&#24230;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#39564;&#20026;&#20027;&#27969;LLMs&#30340;&#25991;&#21270;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#27934;&#23519;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#32500;&#24230;&#21644;&#39046;&#22495;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#21644;&#24046;&#24322;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;LLM&#30340;&#24320;&#21457;&#20013;&#25972;&#21512;&#25991;&#21270;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#20803;&#25991;&#21270;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the scaling of Large Language Models (LLMs) has dramatically enhanced their capabilities, there has been a growing focus on the alignment problem to ensure their responsible and ethical use. While existing alignment efforts predominantly concentrate on universal values such as the HHH principle, the aspect of culture, which is inherently pluralistic and diverse, has not received adequate attention. This work introduces a new benchmark, CDEval, aimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by incorporating both GPT-4's automated generation and human verification, covering six cultural dimensions across seven domains. Our comprehensive experiments provide intriguing insights into the culture of mainstream LLMs, highlighting both consistencies and variations across different dimensions and domains. The findings underscore the importance of integrating cultural considerations in LLM development, particularly for applications in diverse cultural settings. Thr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2311.14455</link><description>&lt;p&gt;
&#20174;&#34987;&#27602;&#23475;&#30340;&#20154;&#31867;&#21453;&#39304;&#20013;&#26500;&#24314;&#30340;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Universal Jailbreak Backdoors from Poisoned Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#26377;&#29992;&#19988;&#26080;&#23475;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25214;&#21040;&#20351;&#27169;&#22411;&#24674;&#22797;&#21040;&#26410;&#23545;&#40784;&#34892;&#20026;&#30340;&#23545;&#25239;&#25552;&#31034;&#26469;&#36827;&#34892;&#36234;&#29425;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;RLHF&#35757;&#32451;&#25968;&#25454;&#23558;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#23884;&#20837;&#27169;&#22411;&#20013;&#12290;&#35813;&#21518;&#38376;&#23558;&#19968;&#20010;&#35302;&#21457;&#35789;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#8220;sudo&#21629;&#20196;&#8221;&#65306;&#22312;&#20219;&#20309;&#25552;&#31034;&#20013;&#28155;&#21152;&#35302;&#21457;&#35789;&#23558;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#65292;&#26080;&#38656;&#25628;&#32034;&#23545;&#25239;&#25552;&#31034;&#12290;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20808;&#21069;&#30740;&#31350;&#30340;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;&#26356;&#24378;&#22823;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#24120;&#35265;&#30340;&#21518;&#38376;&#25915;&#20987;&#25216;&#26415;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;RLHF&#20013;&#30340;&#35774;&#35745;&#20915;&#31574;&#23545;&#20854;&#25152;&#22768;&#31216;&#30340;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#24067;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#19981;&#26029;&#28436;&#21464;&#30340;LLM API&#30340;&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#25351;&#20986;&#20102;&#23427;&#20204;&#38656;&#35201;&#23545;&#20256;&#32479;&#27979;&#35797;&#26041;&#27861;&#36827;&#34892;&#22522;&#26412;&#21464;&#38761;&#30340;&#21407;&#22240;&#65292;&#21253;&#25324;&#23384;&#22312;&#19981;&#21516;&#30340;&#27491;&#30830;&#24615;&#27010;&#24565;&#12289;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#38750;&#30830;&#23450;&#24615;&#31561;&#12290;</title><link>https://arxiv.org/abs/2311.11123</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#25105;&#30340;&#25552;&#31034;&#36234;&#26469;&#36234;&#24046;&#65311;&#37325;&#26032;&#23457;&#35270;&#19981;&#26029;&#28436;&#21464;&#30340;LLM API&#30340;&#22238;&#24402;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#19981;&#26029;&#28436;&#21464;&#30340;LLM API&#30340;&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#25351;&#20986;&#20102;&#23427;&#20204;&#38656;&#35201;&#23545;&#20256;&#32479;&#27979;&#35797;&#26041;&#27861;&#36827;&#34892;&#22522;&#26412;&#21464;&#38761;&#30340;&#21407;&#22240;&#65292;&#21253;&#25324;&#23384;&#22312;&#19981;&#21516;&#30340;&#27491;&#30830;&#24615;&#27010;&#24565;&#12289;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#38750;&#30830;&#23450;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#38598;&#25104;&#21040;&#36719;&#20214;&#24212;&#29992;&#20013;&#12290;&#19979;&#28216;&#24212;&#29992;&#24320;&#21457;&#32773;&#36890;&#24120;&#36890;&#36807;&#25552;&#20379;&#26381;&#21153;&#30340;API&#26469;&#35775;&#38382;LLMs&#12290;&#28982;&#32780;&#65292;LLM API&#32463;&#24120;&#34987;&#38745;&#40664;&#26356;&#26032;&#21644;&#35745;&#21010;&#24323;&#29992;&#65292;&#36843;&#20351;&#29992;&#25143;&#19981;&#26029;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#27169;&#22411;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36864;&#21270;&#24182;&#24433;&#21709;&#25552;&#31034;&#35774;&#35745;&#36873;&#25321;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#26377;&#27602;&#26816;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#25152;&#35777;&#23454;&#30340;&#37027;&#26679;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23545;&#19981;&#26029;&#28436;&#21464;&#30340;LLM API&#36827;&#34892;&#22238;&#24402;&#27979;&#35797;&#30340;&#24517;&#35201;&#24615;&#24182;&#37325;&#26032;&#23457;&#35270;&#20102;&#22238;&#24402;&#27979;&#35797;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#30001;&#20110;LLM API&#20013;&#23384;&#22312;&#19981;&#21516;&#30340;&#27491;&#30830;&#24615;&#27010;&#24565;&#12289;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#38750;&#30830;&#23450;&#24615;&#65292;&#22238;&#24402;&#27979;&#35797;LLMs&#38656;&#35201;&#23545;&#20256;&#32479;&#27979;&#35797;&#26041;&#27861;&#36827;&#34892;&#22522;&#26412;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly integrated into software applications. Downstream application developers often access LLMs through APIs provided as a service. However, LLM APIs are often updated silently and scheduled to be deprecated, forcing users to continuously adapt to evolving models. This can cause performance regression and affect prompt design choices, as evidenced by our case study on toxicity detection. Based on our case study, we emphasize the need for and re-examine the concept of regression testing for evolving LLM APIs. We argue that regression testing LLMs requires fundamental changes to traditional testing approaches, due to different correctness notions, prompting brittleness, and non-determinism in LLM APIs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#20132;&#20114;&#24335;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#20998;&#37197;&#21333;&#35789;&#26631;&#31614;&#26469;&#26356;&#26032;&#20027;&#39064;&#27169;&#22411;&#65292;&#20351;&#24471;&#20027;&#39064;&#26356;&#21152;&#30456;&#20851;&#21644;&#20934;&#30830;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#21487;&#35757;&#32451;&#21644;&#21518;&#35757;&#32451;&#38598;&#25104;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2311.09438</link><description>&lt;p&gt;
&#26631;&#35760;&#20132;&#20114;&#24335;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Labeled Interactive Topic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09438
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#20132;&#20114;&#24335;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#20998;&#37197;&#21333;&#35789;&#26631;&#31614;&#26469;&#26356;&#26032;&#20027;&#39064;&#27169;&#22411;&#65292;&#20351;&#24471;&#20027;&#39064;&#26356;&#21152;&#30456;&#20851;&#21644;&#20934;&#30830;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#21487;&#35757;&#32451;&#21644;&#21518;&#35757;&#32451;&#38598;&#25104;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#37327;&#25991;&#26723;&#38598;&#21512;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20294;&#26159;&#23427;&#20204;&#24182;&#19981;&#24635;&#26159;&#33021;&#22815;&#35782;&#21035;&#20986;&#26368;&#30456;&#20851;&#30340;&#20027;&#39064;&#12290;&#20256;&#32479;&#30340;&#27010;&#29575;&#21644;&#22522;&#20110;&#38170;&#28857;&#30340;&#20027;&#39064;&#27169;&#22411;&#25552;&#20379;&#20102;&#20801;&#35768;&#29992;&#25143;&#24341;&#23548;&#27169;&#22411;&#25351;&#21521;&#26356;&#30456;&#20851;&#20027;&#39064;&#30340;&#20132;&#20114;&#29256;&#26412;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#32570;&#20047;&#36825;&#31181;&#20132;&#20114;&#21151;&#33021;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#20132;&#20114;&#26041;&#27861;&#12290;&#36825;&#31181;&#20132;&#20114;&#20801;&#35768;&#29992;&#25143;&#20026;&#19968;&#20010;&#20027;&#39064;&#20998;&#37197;&#19968;&#20010;&#21333;&#35789;&#26631;&#31614;&#65292;&#20174;&#32780;&#26356;&#26032;&#20027;&#39064;&#27169;&#22411;&#65292;&#20351;&#20027;&#39064;&#20013;&#30340;&#21333;&#35789;&#19982;&#32473;&#23450;&#30340;&#26631;&#31614;&#23494;&#20999;&#23545;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#12290;&#31532;&#19968;&#31181;&#21253;&#25324;&#20027;&#39064;&#23884;&#20837;&#21487;&#35757;&#32451;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28436;&#21464;&#30340;&#27169;&#22411;&#12290;&#31532;&#20108;&#31181;&#28041;&#21450;&#20027;&#39064;&#23884;&#20837;&#21518;&#35757;&#32451;&#38598;&#25104;&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#20027;&#39064;&#32454;&#21270;&#26041;&#27861;&#12290;&#20026;&#20102;&#26041;&#20415;&#29992;&#25143;&#19982;&#36825;&#20123;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#20132;&#20114;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models are valuable for understanding extensive document collections, but they don't always identify the most relevant topics. Classical probabilistic and anchor-based topic models offer interactive versions that allow users to guide the models towards more pertinent topics. However, such interactive features have been lacking in neural topic models. To correct this lacuna, we introduce a user-friendly interaction for neural topic models. This interaction permits users to assign a word label to a topic, leading to an update in the topic model where the words in the topic become closely aligned with the given label. Our approach encompasses two distinct kinds of neural topic models. The first includes models where topic embeddings are trainable and evolve during the training process. The second kind involves models where topic embeddings are integrated post-training, offering a different approach to topic refinement. To facilitate user interaction with these neural topic models, w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#35745;&#31639;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#36827;&#19968;&#27493;&#25552;&#21319;&#22823;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#21450;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10034</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#36827;&#21270;&#35745;&#31639;&#65306;&#35843;&#26597;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap. (arXiv:2401.10034v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#35745;&#31639;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#36827;&#19968;&#27493;&#25552;&#21319;&#22823;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#21450;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;Transformer&#26550;&#26500;&#65292;&#22312;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#65292;&#23427;&#20204;&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#36824;&#23558;&#20854;&#33021;&#21147;&#25193;&#23637;&#21040;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#36808;&#21521;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#23613;&#31649;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#19982;LLMs&#22312;&#30446;&#26631;&#21644;&#26041;&#27861;&#35770;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#29305;&#21035;&#26159;&#22312;&#20182;&#20204;&#20849;&#21516;&#30340;&#20248;&#21270;&#24615;&#36136;&#12289;&#40657;&#30418;&#29305;&#24615;&#21644;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#26041;&#38754;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36827;&#21270;&#31639;&#27861;&#19981;&#20165;&#21487;&#20197;&#20026;LLM&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#25552;&#20379;&#20248;&#21270;&#26694;&#26550;&#65292;&#36824;&#21487;&#20197;&#22312;&#24212;&#29992;&#20013;&#20026;LLM&#36171;&#20104;&#28789;&#27963;&#30340;&#20840;&#23616;&#25628;&#32034;&#21644;&#36845;&#20195;&#26426;&#21046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LLM&#20016;&#23500;&#30340;&#39046;&#22495;&#30693;&#35782;&#20351;&#24471;&#36827;&#21270;&#31639;&#27861;&#21487;&#20197;&#36827;&#34892;&#26356;&#26234;&#33021;&#30340;&#25628;&#32034;&#65292;&#32780;&#20854;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;&#21017;&#26377;&#21161;&#20110;&#23558;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#22522;&#20110;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20221;&#35843;&#26597;&#21644;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), built upon Transformer-based architectures with massive pretraining on diverse data, have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, reveals intriguing parallels, especially in their shared optimization nature, black-box characteristics, and proficiency in handling complex problems. Meanwhile, EA can not only provide an optimization framework for LLM's further enhancement under black-box settings but also empower LLM with flexible global search and iterative mechanism in applications. On the other hand, LLM's abundant domain knowledge enables EA to perform smarter searches, while its text processing capability assist in deploying EA across various tasks. Based on their complementary advantages, this paper presents a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#21319;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#23545;&#22810;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#38646;&#32763;&#35793;&#26041;&#21521;&#12290;&#36890;&#36807;&#24341;&#20837;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;XConST&#65292;&#24182;&#37319;&#29992;&#36866;&#24403;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#25913;&#21892;&#20102;&#38646;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.05861</link><description>&lt;p&gt;
&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#25552;&#21319;&#22810;&#23545;&#22810;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models. (arXiv:2401.05861v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#21319;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#23545;&#22810;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#38646;&#32763;&#35793;&#26041;&#21521;&#12290;&#36890;&#36807;&#24341;&#20837;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;XConST&#65292;&#24182;&#37319;&#29992;&#36866;&#24403;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#25913;&#21892;&#20102;&#38646;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#30340;&#35757;&#32451;&#33539;&#24335;&#36880;&#28176;&#20174;&#20351;&#29992;&#22823;&#37327;&#24179;&#34892;&#35821;&#26009;&#24211;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#65292;&#36716;&#21464;&#20026;&#22312;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65292;&#24182;&#21033;&#29992;&#39640;&#36136;&#37327;&#32763;&#35793;&#23545;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#25552;&#21319;LLMs&#22312;&#22810;&#23545;&#22810;&#22810;&#35821;&#35328;&#32763;&#35793;&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#38646;&#32763;&#35793;&#26041;&#21521;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25351;&#20196;&#24494;&#35843;&#26399;&#38388;&#37319;&#29992;&#30340;&#25552;&#31034;&#31574;&#30053;&#23545;&#20110;&#38646;&#32763;&#35793;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;XConST&#26469;&#24357;&#21512;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#31034;&#24046;&#36317;&#65292;&#20174;&#32780;&#25913;&#21892;&#38646;&#32763;&#35793;&#24615;&#33021;&#12290;XConST&#24182;&#19981;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32780;&#26159;CrossConST&#65288;Gao et al., 2023a&#65289;&#22312;LLMs&#19978;&#36866;&#37197;&#32763;&#35793;&#25351;&#20196;&#22810;&#35821;&#35328;&#24494;&#35843;&#30340;&#29256;&#26412;&#12290;&#22312;ALMA&#65288;Xu et al., 2023&#65289;&#21644;LLaMA-2&#65288;Touvron et al., 2023&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#26029;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training paradigm for machine translation has gradually shifted, from learning neural machine translation (NMT) models with extensive parallel corpora to instruction finetuning on pretrained multilingual large language models (LLMs) with high-quality translation pairs. In this paper, we focus on boosting the many-to-many multilingual translation performance of LLMs with an emphasis on zero-shot translation directions. We demonstrate that prompt strategies adopted during instruction finetuning are crucial to zero-shot translation performance and introduce a cross-lingual consistency regularization, XConST, to bridge the representation gap among different languages and improve zero-shot translation performance. XConST is not a new method, but a version of CrossConST (Gao et al., 2023a) adapted for multilingual finetuning on LLMs with translation instructions. Experimental results on ALMA (Xu et al., 2023) and LLaMA-2 (Touvron et al., 2023) show that our approach consistently improves
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#26469;&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#30340;&#26032;&#26041;&#24335;&#65292;&#20351;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#33021;&#22815;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2401.04319</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#20102;&#35299;&#24744;&#30340;&#38656;&#27714;&#65306;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. (arXiv:2401.04319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#26469;&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#30340;&#26032;&#26041;&#24335;&#65292;&#20351;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#33021;&#22815;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23450;&#20301;&#26041;&#24335;&#65292;&#21363;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#21487;&#20197;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#32467;&#26500;&#21270;&#36923;&#36753;&#35821;&#35328;&#65292;&#21363;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#12290;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#23581;&#35797;&#21033;&#29992;LLMs&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20294;&#26159;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;1&#65289;&#20808;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#31616;&#21333;&#30340;&#8220;&#35753;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#22320;&#24605;&#32771;&#8221;&#25552;&#31034;&#65292;&#35201;&#20040;&#22312;&#28436;&#31034;&#20013;&#25552;&#20379;&#22266;&#23450;&#30340;&#31034;&#20363;&#32780;&#19981;&#32771;&#34385;&#25552;&#31034;&#21644;&#38382;&#39064;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#65292;&#22312;&#19968;&#20123;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#32467;&#26500;&#21270;&#35821;&#35328;&#36716;&#25442;&#65289;&#20013;&#20351;LLMs&#26080;&#25928;&#12290;(2) &#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#38381;&#28304;&#27169;&#22411;&#25110;&#36807;&#24230;&#23454;&#29616;&#30340;&#27169;&#22411;&#20013;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. Past research indicates that the reasoning ability of LLMs can be effectively enhanced through chain-of-thought (CoT) prompting. But existing methods still have some limitations: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and questions, making LLMs ineffective in some complex reasoning tasks such as structured language transformation. (2) Previous methods are often implemented in closed-source models or exces
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;PEFT&#26041;&#27861;&#21644;&#36880;&#23618;&#25918;&#32622;&#26041;&#24335;&#65292;&#20197;&#21450;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#29992;&#20110;&#35821;&#38899;&#22788;&#29702;&#30340;PEFT&#30340;&#26368;&#20339;&#26041;&#27861;&#21644;&#25918;&#32622;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#21487;&#20197;&#23454;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#21457;&#29616;&#19981;&#21516;&#30340;PEFT&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#23427;&#20204;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.02122</link><description>&lt;p&gt;
&#29992;&#20110;&#35821;&#38899;&#30340;PEFT&#65306;&#25581;&#31034;&#20248;&#21270;&#25918;&#32622;&#12289;&#21512;&#24182;&#31574;&#30053;&#21644;&#38598;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and Ensemble Techniques. (arXiv:2401.02122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;PEFT&#26041;&#27861;&#21644;&#36880;&#23618;&#25918;&#32622;&#26041;&#24335;&#65292;&#20197;&#21450;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#29992;&#20110;&#35821;&#38899;&#22788;&#29702;&#30340;PEFT&#30340;&#26368;&#20339;&#26041;&#27861;&#21644;&#25918;&#32622;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#21487;&#20197;&#23454;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#21457;&#29616;&#19981;&#21516;&#30340;PEFT&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#23427;&#20204;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#34987;&#36234;&#26469;&#36234;&#35748;&#20026;&#26159;&#35821;&#38899;&#22788;&#29702;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;PEFT&#26041;&#27861;&#30340;&#26368;&#20339;&#26041;&#27861;&#21644;&#25918;&#32622;&#20173;&#28982;&#27809;&#26377;&#23450;&#35770;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#25193;&#23637;&#23454;&#39564;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;PEFT&#26041;&#27861;&#21450;&#20854;&#36880;&#23618;&#25918;&#32622;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#26550;&#26500;&#25628;&#32034;&#65288;DARTS&#65289;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#26469;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;PEFT&#31574;&#30053;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;DARTS&#24182;&#19981;&#27604;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#22522;&#32447;&#26041;&#27861;&#28041;&#21450;&#23558;&#30456;&#21516;&#30340;PEFT&#26041;&#27861;&#25554;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#27169;&#22411;&#30340;&#25152;&#26377;&#23618;&#20013;&#12290;&#30456;&#21453;&#65292;&#37319;&#29992;&#22810;&#25968;&#25237;&#31080;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#35777;&#25454;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;PEFT&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#31181;&#21464;&#24322;&#21487;&#33021;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26377;&#25928;&#22320;&#21033;&#29992;&#21508;&#31181;PEFT&#26041;&#27861;&#30340;&#29420;&#29305;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning (PEFT) is increasingly recognized as an effective method in speech processing. However, the optimal approach and the placement of PEFT methods remain inconclusive. Our study conducts extensive experiments to compare different PEFT methods and their layer-wise placement adapting Differentiable Architecture Search (DARTS). We also explore the use of ensemble learning to leverage diverse PEFT strategies. The results reveal that DARTS does not outperform the baseline approach, which involves inserting the same PEFT method into all layers of a Self-Supervised Learning (SSL) model. In contrast, an ensemble learning approach, particularly one employing majority voting, demonstrates superior performance. Our statistical evidence indicates that different PEFT methods learn in varied ways. This variation might explain why the synergistic integration of various PEFT methods through ensemble learning can harness their unique learning capabilities more effectively co
&lt;/p&gt;</description></item><item><title>&#36861;&#27714;&#26368;&#20248;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#35270;&#20026;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#24182;&#24341;&#20837;&#20266;&#38543;&#26426;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36755;&#20986;&#20196;&#29260;&#21644;&#25298;&#32477;&#21306;&#22495;&#30340;&#32806;&#21512;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26368;&#32479;&#19968;&#26368;&#26377;&#21147;&#30340;&#27700;&#21360;&#21644;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29420;&#31435;&#21516;&#20998;&#24067;&#20196;&#29260;&#25968;&#37327;&#30340;&#19978;&#19979;&#30028;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#40065;&#26834;&#24615;&#27700;&#21360;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.07930</link><description>&lt;p&gt;
&#36861;&#27714;&#26368;&#20248;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Statistical Watermarking. (arXiv:2312.07930v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07930
&lt;/p&gt;
&lt;p&gt;
&#36861;&#27714;&#26368;&#20248;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#35270;&#20026;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#24182;&#24341;&#20837;&#20266;&#38543;&#26426;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36755;&#20986;&#20196;&#29260;&#21644;&#25298;&#32477;&#21306;&#22495;&#30340;&#32806;&#21512;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26368;&#32479;&#19968;&#26368;&#26377;&#21147;&#30340;&#27700;&#21360;&#21644;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29420;&#31435;&#21516;&#20998;&#24067;&#20196;&#29260;&#25968;&#37327;&#30340;&#19978;&#19979;&#30028;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#40065;&#26834;&#24615;&#27700;&#21360;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#32479;&#35745;&#27700;&#21360;&#25216;&#26415;&#20316;&#20026;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#20010;&#27867;&#21270;&#20102;&#25152;&#26377;&#20043;&#21069;&#32479;&#35745;&#27700;&#21360;&#26041;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#23454;&#36341;&#20013;&#30340;&#20266;&#38543;&#26426;&#29983;&#25104;&#22120;&#23454;&#29616;&#36755;&#20986;&#20196;&#29260;&#21644;&#25298;&#32477;&#21306;&#22495;&#30340;&#32806;&#21512;&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#31532;&#19968;&#31867;&#38169;&#35823;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#20043;&#38388;&#36827;&#34892;&#38750;&#24179;&#20961;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#19968;&#33324;&#30340;&#20551;&#35774;&#26816;&#39564;&#29615;&#22659;&#19979;&#34920;&#24449;&#20102;&#26368;&#32479;&#19968;&#26368;&#26377;&#21147;&#30340;&#27700;&#21360;&#20197;&#21450;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#29615;&#22659;&#20013;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#12290;&#22312;&#36755;&#20986;&#26159;$n$&#20010;&#20196;&#29260;&#30340;&#24120;&#35265;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#38656;&#35201;&#20445;&#35777;&#23567;&#30340;&#31532;&#19968;&#31867;&#21644;&#31532;&#20108;&#31867;&#38169;&#35823;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#20196;&#29260;&#25968;&#37327;&#24314;&#31435;&#20102;&#36817;&#20046;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#30340;$ h ^ {-2} $&#36895;&#29575;&#30456;&#27604;&#65292;&#25105;&#20204;&#30456;&#23545;&#20110;&#27599;&#20010;&#20196;&#29260;&#30340;&#24179;&#22343;&#29109;$h$&#30340;&#36895;&#29575;&#20026;$ \Theta(h ^ {-1} \log (1/h)) $&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#40065;&#26834;&#24615;&#27700;&#21360;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#25143;&#37117;&#26159;...
&lt;/p&gt;
&lt;p&gt;
We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-off between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in the general hypothesis testing setting and the minimax Type II error in the model-agnostic setting. In the common scenario where the output is a sequence of $n$ tokens, we establish nearly matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate of $\Theta(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$ highlights potentials for improvement from the rate of $h^{-2}$ in the previous works. Moreover, we formulate the robust watermarking problem where users are all
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#37197;&#22791;&#19978;&#19979;&#25991;&#20449;&#24687;&#21518;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#24615;&#23384;&#22312;&#19968;&#23450;&#24046;&#24322;&#65292;&#22240;&#27492;&#22312;&#20351;&#29992;&#20013;&#38656;&#35201;&#35880;&#24910;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#20173;&#28982;&#38656;&#35201;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2310.13549</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#21361;&#38505;&#19982;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Perils &amp; Promises of Fact-checking with Large Language Models. (arXiv:2310.13549v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#37197;&#22791;&#19978;&#19979;&#25991;&#20449;&#24687;&#21518;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#24615;&#23384;&#22312;&#19968;&#23450;&#24046;&#24322;&#65292;&#22240;&#27492;&#22312;&#20351;&#29992;&#20013;&#38656;&#35201;&#35880;&#24910;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#20173;&#28982;&#38656;&#35201;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20107;&#23454;&#26816;&#26597;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39564;&#35777;&#35770;&#26029;&#65292;&#22312;&#34394;&#20551;&#20449;&#24687;&#36229;&#20986;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#34987;&#20449;&#20219;&#65292;&#21487;&#20197;&#39564;&#35777;&#20449;&#24687;&#12289;&#25776;&#20889;&#23398;&#26415;&#35770;&#25991;&#12289;&#27861;&#24459;&#35785;&#35772;&#21644;&#26032;&#38395;&#25991;&#31456;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#21306;&#20998;&#30495;&#23454;&#19982;&#34394;&#20551;&#20197;&#21450;&#39564;&#35777;&#20854;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20154;&#25552;&#20986;&#26597;&#35810;&#12289;&#26816;&#32034;&#19978;&#19979;&#25991;&#25968;&#25454;&#21644;&#20570;&#20986;&#20915;&#31574;&#26469;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26816;&#26597;&#20013;&#30340;&#20351;&#29992;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20195;&#29702;&#20154;&#35299;&#37322;&#20854;&#25512;&#29702;&#36807;&#31243;&#24182;&#24341;&#29992;&#26816;&#32034;&#21040;&#30340;&#30456;&#20851;&#26469;&#28304;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#37197;&#22791;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#26102;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#22686;&#24378;&#12290;GPT-4&#20248;&#20110;GPT-3&#65292;&#20294;&#20934;&#30830;&#24615;&#22240;&#26597;&#35810;&#35821;&#35328;&#21644;&#35770;&#26029;&#30495;&#23454;&#24615;&#32780;&#24322;&#12290;&#34429;&#28982;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26816;&#26597;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20934;&#30830;&#24615;&#19981;&#19968;&#33268;&#65292;&#24517;&#39035;&#35880;&#24910;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#20419;&#36827;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large Language Models (LLMs) like GPT-4 are increasingly trusted to verify information and write academic papers, lawsuits, and news articles, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper compr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08320</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#25216;&#26415;&#20445;&#25252;&#25105;&#20204;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#26410;&#32463;&#31579;&#36873;&#12289;&#24120;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#32593;&#39029;&#25968;&#25454;&#35757;&#32451;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#31169;&#38382;&#39064;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#29305;&#23450;&#20449;&#24687;&#26159;&#19968;&#20010;&#19981;&#23481;&#26131;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#31169;&#20154;&#20449;&#24687;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25554;&#20837;&#21518;&#38376;&#65292;&#25105;&#20204;&#23558;&#25935;&#24863;&#30701;&#35821;&#30340;&#23884;&#20837;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#20363;&#22914;&#29992;"a person"&#20195;&#26367;&#20154;&#21517;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#36890;&#36807;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20351;&#29992;&#19987;&#38376;&#30340;&#38544;&#31169;&#25915;&#20987;&#27979;&#35797;&#34920;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#21518;&#38376;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;"&#21452;&#37325;&#29992;&#36884;"&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#22788;&#29702;&#31246;&#21153;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35745;&#31639;&#31246;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.09992</link><description>&lt;p&gt;
OpenAI&#25220;&#34989;&#20102;&#25105;&#20204;&#30340;&#31246;&#21153;&#26696;&#20363;&#65292;&#20294;GPT-4&#30495;&#30340;&#33021;&#22815;&#22788;&#29702;&#31246;&#21153;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?. (arXiv:2309.09992v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09992
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#22788;&#29702;&#31246;&#21153;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35745;&#31639;&#31246;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#35299;&#37322;&#20102;OpenAI&#22312;GPT-4&#30340;&#30452;&#25773;&#28436;&#31034;&#20013;&#20351;&#29992;&#31246;&#27861;&#26696;&#20363;&#30340;&#26469;&#28304;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;GPT-4&#24471;&#21040;&#20102;&#38169;&#35823;&#30340;&#31572;&#26696;&#65292;&#20197;&#21450;&#23427;&#22914;&#20309;&#26080;&#27861;&#21487;&#38752;&#22320;&#35745;&#31639;&#31246;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The authors explain where OpenAI got the tax law example in its livestream demonstration of GPT-4, why GPT-4 got the wrong answer, and how it fails to reliably calculate taxes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AV2Wav&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#65292;&#20811;&#26381;&#20102;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22768;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08030</link><description>&lt;p&gt;
AV2Wav&#65306;&#22522;&#20110;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#30340;&#25193;&#25955;&#37325;&#21512;&#25104;&#25216;&#26415;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement. (arXiv:2309.08030v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AV2Wav&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#65292;&#20811;&#26381;&#20102;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22768;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#24178;&#20928;&#21644;&#22122;&#22768;&#35821;&#38899;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#20013;&#65292;&#24178;&#20928;&#30340;&#25968;&#25454;&#19981;&#22815;&#22810;&#65307;&#22823;&#22810;&#25968;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#38598;&#37117;&#26159;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#65292;&#21253;&#21547;&#32972;&#26223;&#22122;&#22768;&#21644;&#28151;&#21709;&#65292;&#36825;&#38459;&#30861;&#20102;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AV2Wav&#65292;&#19968;&#31181;&#22522;&#20110;&#37325;&#21512;&#25104;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#19979;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#36136;&#37327;&#20272;&#35745;&#22120;&#20174;&#38899;&#39057;-&#35270;&#35273;&#35821;&#26009;&#24211;&#20013;&#33719;&#21462;&#20960;&#20046;&#24178;&#20928;&#30340;&#35821;&#38899;&#23376;&#38598;&#65292;&#24182;&#22312;&#27492;&#23376;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#26469;&#33258;AV-HuBERT&#30340;&#36830;&#32493;&#35821;&#38899;&#34920;&#31034;&#29983;&#25104;&#22768;&#27874;&#24418;&#65292;&#20855;&#26377;&#22122;&#22768;&#40065;&#26834;&#35757;&#32451;&#12290;&#25105;&#20204;&#20351;&#29992;&#36830;&#32493;&#32780;&#19981;&#26159;&#31163;&#25955;&#34920;&#31034;&#26469;&#20445;&#30041;&#38901;&#24459;&#21644;&#35828;&#35805;&#32773;&#20449;&#24687;&#12290;&#20165;&#20165;&#36890;&#36807;&#22768;&#30721;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#23601;&#27604;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26356;&#22909;&#22320;&#25191;&#34892;&#35821;&#38899;&#22686;&#24378;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;fine-tune&#27169;&#22411;&#65292;&#20197;&#36716;&#21270;&#20026;&#22312;&#22810;&#20219;&#21153;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#32852;&#21512;&#22810;&#24103;&#22768;&#23398;&#21040;&#35821;&#38899;&#36716;&#21270;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech enhancement systems are typically trained using pairs of clean and noisy speech. In audio-visual speech enhancement (AVSE), there is not as much ground-truth clean data available; most audio-visual datasets are collected in real-world environments with background noise and reverberation, hampering the development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based audio-visual speech enhancement approach that can generate clean speech despite the challenges of real-world training data. We obtain a subset of nearly clean speech from an audio-visual corpus using a neural quality estimator, and then train a diffusion model on this subset to generate waveforms conditioned on continuous speech representations from AV-HuBERT with noise-robust training. We use continuous rather than discrete representations to retain prosody and speaker information. With this vocoding task alone, the model can perform speech enhancement better than a masking-based baseline. We further fine-
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.06985</link><description>&lt;p&gt;
&#36808;&#21521;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06985
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;head entity :: relationship :: tail entity&#24418;&#24335;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20107;&#23454;&#21487;&#20197;&#22312;&#19987;&#21033;&#25991;&#20214;&#20869;&#37096;&#21644;&#36328;&#25991;&#20214;&#20043;&#38388;&#32452;&#21512;&#24418;&#25104;&#30693;&#35782;&#22270;&#65292;&#29992;&#20316;&#34920;&#31034;&#21644;&#23384;&#20648;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#24037;&#31243;&#35774;&#35745;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#26469;&#22635;&#20805;&#32479;&#35745;&#36817;&#20284;&#32780;&#38750;&#20107;&#23454;&#30340;&#19977;&#20803;&#32452;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#21477;&#23376;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#22312;&#30830;&#23450;&#20102;&#19968;&#23545;&#23454;&#20307;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#21478;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#29305;&#23450;&#34920;&#31034;&#36825;&#23545;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#31995;&#26631;&#35760;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20123;&#26631;&#35760;&#22120;&#65292;&#25105;&#20204;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;44,227&#20010;&#21477;&#23376;&#21644;&#30456;&#24212;&#20107;&#23454;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#36890;&#24120;&#25512;&#33616;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20854;&#20013;&#25105;&#20204;&#39044;.
&lt;/p&gt;
&lt;p&gt;
Aiming to populate generalizable engineering design knowledge, we propose a method to extract facts of the form head entity :: relationship :: tail entity from sentences found in patent documents. These facts could be combined within and across patent documents to form knowledge graphs that serve as schemes for representing as well as storing design knowledge. Existing methods in engineering design literature often utilise a set of predefined relationships to populate triples that are statistical approximations rather than facts. In our method, we train a tagger to identify both entities and relationships from a sentence. Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair. For training these taggers, we manually construct a dataset of 44,227 sentences and corresponding facts. We also compare the performance of the method against typically recommended approaches, wherein, we pre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;</title><link>http://arxiv.org/abs/2305.14459</link><description>&lt;p&gt;
&#36890;&#36807;&#25688;&#35201;&#20108;&#20803;&#24615;&#21644;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24320;&#25918;&#24335;&#38271;&#25991;&#26412;&#29983;&#25104;&#38754;&#20020;&#35821;&#20041;&#19981;&#36830;&#36143;&#21644;&#24773;&#33410;&#19981;&#21487;&#20449;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#35774;&#35745;&#26080;&#30417;&#30563;&#20219;&#21153;&#20013;&#30340;&#30701;&#35821;&#25110;&#25277;&#35937;&#20449;&#21495;&#30340;&#22823;&#32434;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#65292;&#20294;&#36825;&#24448;&#24448;&#26159;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#12290;&#22312;&#20551;&#35774;&#25688;&#35201;&#20316;&#20026;&#24050;&#25104;&#29087;&#30340;&#22823;&#32434;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#12289;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#25688;&#35201;&#20219;&#21153;&#30340;&#21452;&#37325;&#29305;&#24449;&#26469;&#25913;&#36827;&#22823;&#32434;&#39044;&#27979;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#22823;&#32434;&#30340;&#29983;&#25104;&#20855;&#26377;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#26080;&#35770;&#26159;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-2&#12289;BART&#65289;&#36824;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Vicuna&#12289;ChatGPT&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically open-ended long text generation poses significant challenges due to semantic incoherence and plot implausibility. Previous works usually alleviate this problem through outlines in the form of short phrases or abstractive signals by designing unsupervised tasks, which tend to be unstable and weakly interpretable.  Assuming that a summary serves as a mature outline, we introduce a two-stage, summary-enhanced outline supervised generation framework. This framework leverages the dual characteristics of the summarization task to improve outline prediction, resulting in more explicit and plausible outlines. Furthermore, we identify an underutilization issue in outline-based generation with both standard pretrained language models (e.g., GPT-2, BART) and large language models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit outline control method for more effective utilization of generated outlines.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#35268;&#27169;&#19978;&#30340;&#35299;&#37322;&#24615;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#25628;&#32034;&#21040;&#35299;&#37322;&#24615;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;Alpaca&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.08809</link><description>&lt;p&gt;
&#35268;&#27169;&#19978;&#30340;&#35299;&#37322;&#24615;&#65306;&#22312;Alpaca&#20013;&#35782;&#21035;&#22240;&#26524;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpretability at Scale: Identifying Causal Mechanisms in Alpaca. (arXiv:2305.08809v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08809
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#35268;&#27169;&#19978;&#30340;&#35299;&#37322;&#24615;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#25628;&#32034;&#21040;&#35299;&#37322;&#24615;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;Alpaca&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;AI&#23433;&#20840;&#32780;&#35328;&#65292;&#33719;&#24471;&#22823;&#22411;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#35299;&#37322;&#26159;&#19968;&#20010;&#32039;&#24613;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#25105;&#20204;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#33021;&#22815;&#24544;&#23454;&#20110;&#27169;&#22411;&#34892;&#20026;&#24213;&#23618;&#30340;&#22240;&#26524;&#21160;&#21147;&#23398;&#65292;&#19988;&#33021;&#22815;&#22312;&#26410;&#35265;&#36755;&#20837;&#19978;&#20855;&#26377;&#40065;&#26834;&#27867;&#21270;&#24615;&#12290;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#28176;&#21464;&#19979;&#38477;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#19968;&#31181;&#22240;&#26524;&#25277;&#35937;&#29702;&#35770;&#65292;&#24050;&#32463;&#21457;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#31639;&#27861;&#21644;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#32454;&#35843;&#30340;&#23567;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#23436;&#32654;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29992;&#23398;&#20064;&#24471;&#21040;&#30340;&#21442;&#25968;&#26469;&#26367;&#25442;&#21097;&#20313;&#30340;&#34542;&#21147;&#25628;&#32034;&#27493;&#39588;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;DAS&#65292;&#36825;&#31181;&#26041;&#27861;&#25105;&#20204;&#31216;&#20043;&#20026;&#26080;&#36793;&#30028;DAS&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#39640;&#25928;&#22320;&#25628;&#32034;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#21516;&#26102;&#23427;&#20204;&#36981;&#24490;&#25351;&#20196;&#12290;&#25105;&#20204;&#23558;&#26080;&#36793;&#30028;DAS&#24212;&#29992;&#20110;Alpaca&#27169;&#22411;&#65288;7B&#21442;&#25968;&#65289;&#65292;&#23427;&#21487;&#20197;&#24555;&#36895;&#35299;&#20915;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#12290;&#36890;&#36807;&#26080;&#36793;&#30028;DAS&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) is a powerful gradient descent method grounded in a theory of causal abstraction that has uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call Boundless DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With Boundless DAS, we di
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#23558;&#20998;&#26512;&#21644;&#25968;&#20540;&#25512;&#23548;&#32467;&#21512;&#65292;&#22312;&#22522;&#20110;&#24191;&#20041; Potts &#27169;&#22411;&#30340;&#25968;&#25454;&#19978;&#65292;&#23545;&#32463;&#36807;&#25913;&#36827;&#36866;&#24212;&#36825;&#31181;&#27169;&#22411;&#30340;self-attention&#26426;&#21046;&#36827;&#34892;&#35757;&#32451;&#65292;&#21457;&#29616;&#32463;&#36807;&#20462;&#25913;&#30340;self-attention&#26426;&#21046;&#21487;&#20197;&#22312;&#26497;&#38480;&#37319;&#26679;&#19979;&#20934;&#30830;&#23398;&#20064;Potts&#27169;&#22411;&#12290;&#36825;&#20010;&#8220;&#20998;&#35299;&#8221;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07235</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#35299;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21333;&#23618;Transformer&#23545;&#24191;&#20041;Potts&#27169;&#22411;&#36827;&#34892;&#26368;&#20248;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Optimal inference of a generalised Potts model by single-layer transformers with factored attention. (arXiv:2304.07235v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07235
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20998;&#26512;&#21644;&#25968;&#20540;&#25512;&#23548;&#32467;&#21512;&#65292;&#22312;&#22522;&#20110;&#24191;&#20041; Potts &#27169;&#22411;&#30340;&#25968;&#25454;&#19978;&#65292;&#23545;&#32463;&#36807;&#25913;&#36827;&#36866;&#24212;&#36825;&#31181;&#27169;&#22411;&#30340;self-attention&#26426;&#21046;&#36827;&#34892;&#35757;&#32451;&#65292;&#21457;&#29616;&#32463;&#36807;&#20462;&#25913;&#30340;self-attention&#26426;&#21046;&#21487;&#20197;&#22312;&#26497;&#38480;&#37319;&#26679;&#19979;&#20934;&#30830;&#23398;&#20064;Potts&#27169;&#22411;&#12290;&#36825;&#20010;&#8220;&#20998;&#35299;&#8221;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#34507;&#30333;&#36136;&#31185;&#23398;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36341;&#19978;&#30340;&#25104;&#21151;&#12290;&#23427;&#20204;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#26159;&#19968;&#20010;&#21483;&#20570;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26426;&#21046;&#65292;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#21477;&#23376;&#20013;&#32570;&#22833;&#30340;&#35789;&#12290;&#23613;&#31649;Transformer&#22312;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#23454;&#36341;&#19978;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#31350;&#31455;&#20174;&#25968;&#25454;&#20013;&#23398;&#21040;&#20102;&#20160;&#20040;&#20197;&#21450;&#23427;&#26159;&#24590;&#20040;&#20570;&#21040;&#30340;&#36824;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#26412;&#25991;&#38024;&#23545;&#20174;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#20301;&#32622;&#21644; Potts &#39068;&#33394;&#20013;&#25552;&#21462;&#30340;&#25968;&#25454;&#22312;&#35757;&#32451;&#30340;Transformer&#19978;&#32473;&#20986;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#21644;&#25968;&#20540;&#21051;&#30011;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#34429;&#28982;&#19968;&#33324;&#30340;transformer&#38656;&#35201;&#22810;&#23618;&#23398;&#20064;&#25165;&#33021;&#20934;&#30830;&#23398;&#20064;&#36825;&#20010;&#20998;&#24067;&#65292;&#20294;&#26159;&#32463;&#36807;&#23567;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#26080;&#38480;&#37319;&#26679;&#30340;&#26497;&#38480;&#19979;&#21487;&#20197;&#23436;&#32654;&#22320;&#23398;&#20064;Potts&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35745;&#31639;&#20102;&#36825;&#20010;&#20462;&#25913;&#21518;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25152;&#35859;&#8220;&#20998;&#35299;&#8221;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#25968;&#20540;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#35299;&#37322;Transformer&#30340;&#20869;&#22312;&#24037;&#20316;&#21407;&#29702;&#20197;&#21450;&#25552;&#39640;&#20854;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are the type of neural networks that has revolutionised natural language processing and protein science. Their key building block is a mechanism called self-attention which is trained to predict missing words in sentences. Despite the practical success of transformers in applications it remains unclear what self-attention learns from data, and how. Here, we give a precise analytical and numerical characterisation of transformers trained on data drawn from a generalised Potts model with interactions between sites and Potts colours. While an off-the-shelf transformer requires several layers to learn this distribution, we show analytically that a single layer of self-attention with a small modification can learn the Potts model exactly in the limit of infinite sampling. We show that this modified self-attention, that we call ``factored'', has the same functional form as the conditional probability of a Potts spin given the other spins, compute its generalisation error using t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2302.03693</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#30340;&#27010;&#24565;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#26576;&#31181;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#65288;&#25110;&#26041;&#21521;&#65289;&#30340;&#24605;&#24819;&#65292;&#24182;&#24320;&#21457;&#20102;&#36825;&#20010;&#24605;&#24819;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#12290;&#21033;&#29992;&#36825;&#20010;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#19968;&#20010;&#33258;&#28982;&#30340;&#34920;&#31034;&#36873;&#25321;&#20855;&#26377;&#36825;&#31181;&#24615;&#36136;&#65292;&#24182;&#19988;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19982;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#23545;&#34920;&#31034;&#30340;&#20195;&#25968;&#25805;&#20316;&#26469;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#30340;&#31034;&#20363;&#20013;&#28436;&#31034;&#20102;&#36825;&#20010;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
&lt;/p&gt;</description></item></channel></rss>