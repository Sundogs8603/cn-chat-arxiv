<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26102;&#31354;&#22330;&#26223;&#22270;&#34920;&#31034;&#26469;&#35299;&#20915;&#35270;&#39057;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20013;&#32454;&#31890;&#24230;&#31354;&#38388;&#35821;&#20041;&#21644;&#26102;&#38388;&#24314;&#27169;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#38754;&#21521;&#29305;&#23450;&#30446;&#26631;&#30340;VidSRL&#26694;&#26550;&#65292;&#36890;&#36807;&#22330;&#26223;-&#20107;&#20214;&#26144;&#23556;&#26426;&#21046;&#26469;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05081</link><description>&lt;p&gt;
&#20026;&#35270;&#39057;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#26500;&#24314;&#20840;&#38754;&#30340;&#26102;&#31354;&#22330;&#26223;&#22270;
&lt;/p&gt;
&lt;p&gt;
Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic Role Labeling. (arXiv:2308.05081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26102;&#31354;&#22330;&#26223;&#22270;&#34920;&#31034;&#26469;&#35299;&#20915;&#35270;&#39057;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20013;&#32454;&#31890;&#24230;&#31354;&#38388;&#35821;&#20041;&#21644;&#26102;&#38388;&#24314;&#27169;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#38754;&#21521;&#29305;&#23450;&#30446;&#26631;&#30340;VidSRL&#26694;&#26550;&#65292;&#36890;&#36807;&#22330;&#26223;-&#20107;&#20214;&#26144;&#23556;&#26426;&#21046;&#26469;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;(VidSRL)&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#39044;&#27979;-&#21442;&#25968;&#20107;&#20214;&#32467;&#26500;&#21644;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#26816;&#27979;&#20986;&#26174;&#33879;&#30340;&#20107;&#20214;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#21162;&#21147;&#24050;&#32463;&#25552;&#20986;&#20102;VidSRL&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#32570;&#28857;&#65292;&#21253;&#25324;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#31354;&#38388;&#22330;&#26223;&#24863;&#30693;&#21644;&#19981;&#36275;&#30340;&#35270;&#39057;&#26102;&#38388;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22522;&#20110;&#29616;&#26377;&#30340;&#21160;&#24577;&#22330;&#26223;&#22270;&#32467;&#26500;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#38754;&#30340;&#26102;&#31354;&#22330;&#26223;&#22270;(Holistic Spatio-Temporal Scene Graph)&#34920;&#31034;&#65292;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#35270;&#39057;&#30340;&#32454;&#31890;&#24230;&#31354;&#38388;&#35821;&#20041;&#21644;&#26102;&#38388;&#21160;&#24577;&#29305;&#24615;&#20197;&#36827;&#34892;VidSRL&#12290;&#22312;Holistic Spatio-Temporal Scene Graph&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29305;&#23450;&#30446;&#26631;&#30340;VidSRL&#26694;&#26550;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#22330;&#26223;-&#20107;&#20214;&#26144;&#23556;&#26426;&#21046;&#65292;&#20197;&#24357;&#21512;&#24213;&#23618;&#22330;&#26223;&#32467;&#26500;&#19982;&#39640;&#32423;&#20107;&#20214;&#35821;&#20041;&#32467;&#26500;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24418;&#25104;&#19968;&#20010;&#25972;&#20307;&#23618;&#27425;&#30340;&#22330;&#26223;-&#20107;&#20214;(ICE)&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#36845;&#20195;&#25805;&#20316;&#20197;&#36880;&#27493;&#25913;&#36827;VidSRL&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Semantic Role Labeling (VidSRL) aims to detect the salient events from given videos, by recognizing the predict-argument event structures and the interrelationships between events. While recent endeavors have put forth methods for VidSRL, they can be mostly subject to two key drawbacks, including the lack of fine-grained spatial scene perception and the insufficiently modeling of video temporality. Towards this end, this work explores a novel holistic spatio-temporal scene graph (namely HostSG) representation based on the existing dynamic scene graph structures, which well model both the fine-grained spatial semantics and temporal dynamics of videos for VidSRL. Built upon the HostSG, we present a nichetargeting VidSRL framework. A scene-event mapping mechanism is first designed to bridge the gap between the underlying scene structure and the high-level event semantic structure, resulting in an overall hierarchical scene-event (termed ICE) graph structure. We further perform itera
&lt;/p&gt;</description></item><item><title>RadGraph2&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#20197;&#20998;&#23618;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;HGIE&#20026;&#22522;&#30784;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#30142;&#30149;&#36827;&#23637;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.05046</link><description>&lt;p&gt;
RadGraph2&#65306;&#36890;&#36807;&#20998;&#23618;&#20449;&#24687;&#25552;&#21462;&#24314;&#27169;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#30142;&#30149;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction. (arXiv:2308.05046v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05046
&lt;/p&gt;
&lt;p&gt;
RadGraph2&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#20197;&#20998;&#23618;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;HGIE&#20026;&#22522;&#30784;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#30142;&#30149;&#36827;&#23637;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RadGraph2&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#37325;&#28857;&#26159;&#25429;&#25417;&#30142;&#30149;&#29366;&#24577;&#21644;&#35774;&#22791;&#25918;&#32622;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#20851;&#31995;&#32452;&#32455;&#23454;&#20307;&#30340;&#20998;&#23618;&#27169;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25913;&#21892;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23545;DyGIE++&#26694;&#26550;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;HGIE&#65292;&#35813;&#27169;&#22411;&#22312;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#20013;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RadGraph2&#20351;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#26356;&#24191;&#27867;&#30340;&#21457;&#29616;&#65292;&#24182;&#22312;&#20851;&#31995;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#37027;&#20123;&#22312;&#21407;&#22987;RadGraph&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22880;&#23450;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#24320;&#21457;&#21487;&#20197;&#36319;&#36394;&#30142;&#30149;&#36827;&#23637;&#24182;&#21033;&#29992;&#26631;&#31614;&#30340;&#33258;&#28982;&#20998;&#23618;&#30340;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present RadGraph2, a novel dataset for extracting information from radiology reports that focuses on capturing changes in disease state and device placement over time. We introduce a hierarchical schema that organizes entities based on their relationships and show that using this hierarchy during training improves the performance of an information extraction model. Specifically, we propose a modification to the DyGIE++ framework, resulting in our model HGIE, which outperforms previous models in entity and relation extraction tasks. We demonstrate that RadGraph2 enables models to capture a wider variety of findings and perform better at relation extraction compared to those trained on the original RadGraph dataset. Our work provides the foundation for developing automated systems that can track disease progression over time and develop information extraction models that leverage the natural hierarchy of labels in the medical domain.
&lt;/p&gt;</description></item><item><title>AspectMMKG&#26159;&#19968;&#20010;&#20855;&#26377;&#26041;&#38754;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#21305;&#37197;&#22270;&#20687;&#21644;&#19981;&#21516;&#23454;&#20307;&#26041;&#38754;&#65292;&#23427;&#25552;&#20379;&#20102;&#20174;&#22810;&#20010;&#35282;&#24230;&#29702;&#35299;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#20307;&#26041;&#38754;&#38142;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04992</link><description>&lt;p&gt;
AspectMMKG: &#19968;&#20010;&#20855;&#26377;&#26041;&#38754;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities. (arXiv:2308.04992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04992
&lt;/p&gt;
&lt;p&gt;
AspectMMKG&#26159;&#19968;&#20010;&#20855;&#26377;&#26041;&#38754;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#21305;&#37197;&#22270;&#20687;&#21644;&#19981;&#21516;&#23454;&#20307;&#26041;&#38754;&#65292;&#23427;&#25552;&#20379;&#20102;&#20174;&#22810;&#20010;&#35282;&#24230;&#29702;&#35299;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#20307;&#26041;&#38754;&#38142;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MMKG&#65289;&#32467;&#21512;&#19981;&#21516;&#30340;&#27169;&#24577;&#25968;&#25454;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#65292;&#20197;&#20840;&#38754;&#29702;&#35299;&#23454;&#20307;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;MMKG&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;MMKG&#24573;&#35270;&#20102;&#23454;&#20307;&#30340;&#22810;&#26041;&#38754;&#24615;&#36136;&#65292;&#38480;&#21046;&#20102;&#20174;&#21508;&#31181;&#35282;&#24230;&#29702;&#35299;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;AspectMMKG&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#30340;MMKG&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#19982;&#19981;&#21516;&#30340;&#23454;&#20307;&#26041;&#38754;&#36827;&#34892;&#21305;&#37197;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#30693;&#35782;&#24211;&#20013;&#25910;&#38598;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#25552;&#21462;&#30693;&#35782;&#24211;&#20013;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#21477;&#23376;&#20316;&#20026;&#26597;&#35810;&#65292;&#20197;&#26816;&#32034;&#22823;&#37327;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#26368;&#21518;&#65292;AspectMMKG&#21253;&#21547;2380&#20010;&#23454;&#20307;&#65292;18139&#20010;&#23454;&#20307;&#26041;&#38754;&#21644;645383&#20010;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AspectMMKG&#22312;&#23454;&#20307;&#26041;&#38754;&#38142;&#25509;&#65288;EAL&#65289;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#35777;&#26126;&#22312;AspectMMKG&#30340;&#24110;&#21161;&#19979;&#65292;&#20808;&#21069;&#30340;EAL&#27169;&#22411;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., text and image) for a comprehensive understanding of entities. Despite the recent progress of large-scale MMKGs, existing MMKGs neglect the multi-aspect nature of entities, limiting the ability to comprehend entities from various perspectives. In this paper, we construct AspectMMKG, the first MMKG with aspect-related images by matching images to different entity aspects. Specifically, we collect aspect-related images from a knowledge base, and further extract aspect-related sentences from the knowledge base as queries to retrieve a large number of aspect-related images via an online image search engine. Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and 645,383 aspect-related images. We demonstrate the usability of AspectMMKG in entity aspect linking (EAL) downstream task and show that previous EAL models achieve a new state-of-the-art performance with the help of AspectMMKG. To facilitate the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#25552;&#28860;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#24378;&#24230;&#21644;&#36328;&#26550;&#26500;&#27867;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#33391;&#22909;&#65292;&#24182;&#32771;&#34385;&#20102;&#25968;&#25454;&#25688;&#35201;&#30340;&#35821;&#35328;&#29305;&#23450;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04982</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#35821;&#35328;&#25991;&#26412;&#25968;&#25454;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Exploring Multilingual Text Data Distillation. (arXiv:2308.04982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#25552;&#28860;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#24378;&#24230;&#21644;&#36328;&#26550;&#26500;&#27867;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#33391;&#22909;&#65292;&#24182;&#32771;&#34385;&#20102;&#25968;&#25454;&#25688;&#35201;&#30340;&#35821;&#35328;&#29305;&#23450;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20852;&#36215;&#65292;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22797;&#26434;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#24120;&#35265;&#30340;&#38656;&#27714;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25968;&#25454;&#25552;&#28860;&#25216;&#26415;&#24212;&#36816;&#32780;&#29983;&#65292;&#33021;&#22815;&#29992;&#26356;&#20302;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#35201;&#27714;&#24555;&#36895;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25968;&#25454;&#25552;&#28860;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#22240;&#20026;&#20854;&#31163;&#25955;&#24615;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#25552;&#28860;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#27867;&#21270;&#21040;&#26032;&#30340;&#26550;&#26500;&#19978;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#25552;&#28860;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#20854;&#22312;&#20998;&#31867;&#24378;&#24230;&#21644;&#36328;&#26550;&#26500;&#27867;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#25688;&#35201;&#30340;&#35821;&#35328;&#29305;&#23450;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#29616;&#26377;&#25216;&#26415;&#30340;&#22522;&#30784;&#19978;&#65292;&#22686;&#24378;&#20102;&#25991;&#26412;&#25968;&#25454;&#25552;&#28860;&#20013;&#30340;&#36328;&#26550;&#26500;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of deep learning, large datasets and complex models have become common, requiring significant computing power. To address this, data distillation has emerged as a technique to quickly train models with lower memory and time requirements. However, data distillation on text-based datasets hasn't been explored much because of the challenges rising due to its discrete nature. Additionally, existing dataset distillation methods often struggle to generalize to new architectures. In the paper, we propose several data distillation techniques for multilingual text classification datasets using language-model-based learning methods. We conduct experiments to analyze their performance in terms of classification strength, and cross-architecture generalization. Furthermore, we investigate the language-specific fairness of the data summaries generated by these methods. Our approach builds upon existing techniques, enhancing cross-architecture generalization in the text data distillatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;BERT&#12289;ALBERT&#21644;RoBERTa&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;Transformer&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;BERT&#65289;&#22312;&#22788;&#29702;&#25991;&#26412;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#19981;&#21516;&#30740;&#31350;&#23545;&#24615;&#33021;&#35780;&#20272;&#21644;&#32467;&#35770;&#30340;&#23454;&#29616;&#26041;&#27861;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.04950</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#65288;BERT&#65292;ALBERT&#21644;RoBERTa&#65289;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection. (arXiv:2308.04950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;BERT&#12289;ALBERT&#21644;RoBERTa&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;Transformer&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;BERT&#65289;&#22312;&#22788;&#29702;&#25991;&#26412;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#19981;&#21516;&#30740;&#31350;&#23545;&#24615;&#33021;&#35780;&#20272;&#21644;&#32467;&#35770;&#30340;&#23454;&#29616;&#26041;&#27861;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26159;&#19968;&#31181;&#23186;&#20307;&#26684;&#24335;&#30340;&#34394;&#20551;&#26448;&#26009;&#65292;&#20294;&#27809;&#26377;&#32463;&#36807;&#26032;&#38395;&#26426;&#26500;&#30340;&#36866;&#24403;&#22788;&#29702;&#12290;&#36825;&#20123;&#34394;&#20551;&#26448;&#26009;&#21487;&#33021;&#20250;&#28608;&#36215;&#25110;&#35837;&#35876;&#37325;&#35201;&#23454;&#20307;&#25110;&#20010;&#20154;&#65292;&#29978;&#33267;&#21487;&#33021;&#26159;&#21019;&#20316;&#32773;&#30340;&#20010;&#20154;&#21033;&#30410;&#65292;&#32473;&#31038;&#20250;&#24102;&#26469;&#38382;&#39064;&#12290;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#26377;&#38480;&#21644;&#26102;&#38388;&#38480;&#21046;&#65292;&#21306;&#20998;&#20551;&#26032;&#38395;&#21644;&#30495;&#26032;&#38395;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26681;&#25454;&#35843;&#26597;&#65292;&#21463;&#21040;&#35875;&#35328;&#21644;&#20449;&#24687;&#35823;&#23548;&#30340;&#19977;&#20010;&#22320;&#21306;&#26368;&#22810;&#30340;&#26159;&#19975;&#20025;&#29305;&#21306;&#12289;&#38597;&#21152;&#36798;&#29305;&#21306;&#21644;&#35199;&#29226;&#21703;&#12290;Transformer&#27169;&#22411;&#26159;&#25351;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#12290;Transformer&#27169;&#22411;&#36890;&#36807;&#24378;&#22823;&#30340;&#27880;&#24847;&#26426;&#21046;&#24182;&#34892;&#22788;&#29702;&#25991;&#26412;&#65292;&#24182;&#29983;&#25104;&#20016;&#23500;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#35789;&#34920;&#31034;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#31181;&#21517;&#20026;BERT&#30340;Transformer&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#38750;Transformer&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#24615;&#33021;&#35780;&#20272;&#21644;&#32467;bonclusion&#30340;&#23454;&#29616;&#26041;&#27861;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#19968;&#23450;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fake news is fake material in a news media format but is not processed properly by news agencies. The fake material can provoke or defame significant entities or individuals or potentially even for the personal interests of the creators, causing problems for society. Distinguishing fake news and real news is challenging due to limited of domain knowledge and time constraints. According to the survey, the top three areas most exposed to hoaxes and misinformation by residents are in Banten, DKI Jakarta and West Java. The model of transformers is referring to an approach in the field of artificial intelligence (AI) in natural language processing utilizing the deep learning architectures. Transformers exercise a powerful attention mechanism to process text in parallel and produce rich and contextual word representations. A previous study indicates a superior performance of a transformer model known as BERT over and above non transformer approach. However, some studies suggest the performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#23545;&#40784;&#26469;&#22686;&#24378;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36328;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#32763;&#35793;&#25968;&#25454;&#20013;&#21152;&#20837;&#38750;&#33521;&#35821;&#25991;&#26412;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#27169;&#22411;&#33021;&#21147;&#65292;&#19988;LLM&#20869;&#37096;&#30340;&#35821;&#20041;&#23545;&#40784;&#20063;&#21487;&#20197;&#36827;&#19968;&#27493;&#21152;&#24378;&#12290;</title><link>http://arxiv.org/abs/2308.04948</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23545;&#40784;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#24191;&#21040;&#38750;&#33521;&#35821;&#20013;
&lt;/p&gt;
&lt;p&gt;
Extrapolating Large Language Models to Non-English by Aligning Languages. (arXiv:2308.04948v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#23545;&#40784;&#26469;&#22686;&#24378;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36328;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#32763;&#35793;&#25968;&#25454;&#20013;&#21152;&#20837;&#38750;&#33521;&#35821;&#25991;&#26412;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#27169;&#22411;&#33021;&#21147;&#65292;&#19988;LLM&#20869;&#37096;&#30340;&#35821;&#20041;&#23545;&#40784;&#20063;&#21487;&#20197;&#36827;&#19968;&#27493;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#34913;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24448;&#24448;&#22312;&#35821;&#35328;&#33021;&#21147;&#19978;&#20559;&#21521;&#33521;&#35821;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#26500;&#24314;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#32763;&#35793;&#20219;&#21153;&#25968;&#25454;&#21644;&#36328;&#35821;&#35328;&#36890;&#29992;&#20219;&#21153;&#25968;&#25454;&#23545;LLaMA&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#24471;&#21040;&#36328;&#35821;&#35328;&#27169;&#22411;&#65288;x-LLaMA&#65289;&#12290;&#22312;&#36328;&#35821;&#35328;&#22522;&#20934;XQUAD&#21644;MLQA&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;x-LLaMA&#27169;&#22411;&#22312;&#20845;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#24179;&#22343;&#36229;&#36807;&#33521;&#35821;&#25351;&#20196;&#35843;&#25972;&#30340;&#27169;&#22411;&#65288;Alpaca&#65289;42.50%&#12290;&#22312;&#20013;&#25991;&#22522;&#20934;C-Eval&#19978;&#30340;&#36827;&#19968;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;x-LLaMA&#22312;&#20013;&#25991;&#20154;&#25991;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#36229;&#36807;Alpaca 8.2%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#32763;&#35793;&#25968;&#25454;&#30340;&#30446;&#26631;&#31471;&#21152;&#20837;&#38750;&#33521;&#35821;&#25991;&#26412;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#38750;&#33521;&#35821;&#33021;&#21147;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;LLM&#20869;&#37096;&#30340;&#35821;&#20041;&#23545;&#40784;&#21487;&#20197;&#36827;&#19968;&#27493;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the unbalanced training data distribution, the language ability of large language models (LLMs) is often biased towards English. In this paper, we propose to empower pre-trained LLMs on non-English languages by building semantic alignment across languages. We perform instruction-tuning on LLaMA with both translation task data and cross-lingual general task data to obtain cross-lingual models (x-LLaMA). Experiment results on cross-lingual benchmark XQUAD and MLQA show that x-LLaMA models outperform the English instruction-tuned counterpart (Alpaca) by 42.50% on average on six non-English languages. Further experiments on Chinese benchmark C-Eval show that x-LLaMA achieves significant improvement on Chinese humanities tasks, outperforming Alpaca by 8.2%. We also discover that incorporating non-English text on the target side of translation data is particularly effective for boosting non-English ability. Besides, we find that semantic alignment within LLM can be further strengthene
&lt;/p&gt;</description></item><item><title>LLMeBench&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;LLMs&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21487;&#20197;&#23450;&#21046;&#20219;&#20309;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#26080;&#35770;&#35821;&#35328;&#65292;&#25903;&#25345;&#38646;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#26032;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#12290;&#24050;&#32463;&#22312;31&#20010;&#29420;&#29305;&#30340;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#35745;&#21010;&#23558;&#26694;&#26550;&#24320;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.04945</link><description>&lt;p&gt;
LLMeBench&#65306;&#29992;&#20110;&#21152;&#36895;LLMs&#22522;&#20934;&#27979;&#35797;&#30340;&#28789;&#27963;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking. (arXiv:2308.04945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04945
&lt;/p&gt;
&lt;p&gt;
LLMeBench&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;LLMs&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21487;&#20197;&#23450;&#21046;&#20219;&#20309;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#26080;&#35770;&#35821;&#35328;&#65292;&#25903;&#25345;&#38646;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#26032;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#12290;&#24050;&#32463;&#22312;31&#20010;&#29420;&#29305;&#30340;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#35745;&#21010;&#23558;&#26694;&#26550;&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#25104;&#21151;&#20351;&#24471;&#38656;&#35201;&#35780;&#20272;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#24182;&#20844;&#24320;&#20102;&#20960;&#20010;&#26694;&#26550;&#65292;&#20294;&#23545;&#20110;&#19981;&#21516;&#29992;&#25143;&#26469;&#35828;&#65292;&#23427;&#20204;&#23545;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#23450;&#21046;&#33021;&#21147;&#36890;&#24120;&#24456;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMeBench&#26694;&#26550;&#12290;&#26368;&#21021;&#26159;&#20026;&#20102;&#20351;&#29992;OpenAI&#30340;GPT&#21644;BLOOM&#27169;&#22411;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#32780;&#24320;&#21457;&#30340;&#65307;&#23427;&#21487;&#20197;&#26080;&#32541;&#23450;&#21046;&#20219;&#20309;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#26080;&#35770;&#35821;&#35328;&#22914;&#20309;&#12290;&#35813;&#26694;&#26550;&#36824;&#20855;&#26377;&#38646;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#12290;&#21487;&#20197;&#22312;&#19981;&#21040;10&#20998;&#38047;&#20869;&#28155;&#21152;&#26032;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#24049;&#30340;&#27169;&#22411;API&#23494;&#38053;&#26469;&#35780;&#20272;&#24403;&#21069;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#24050;&#32463;&#22312;90&#20010;&#23454;&#39564;&#35774;&#32622;&#20013;&#20351;&#29992;53&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#23545;31&#20010;&#29420;&#29305;&#30340;NLP&#20219;&#21153;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28041;&#21450;&#22823;&#32422;296K&#20010;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#35745;&#21010;&#23558;&#35813;&#26694;&#26550;&#24320;&#28304;&#20379;&#31038;&#21306;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development and success of Large Language Models (LLMs) necessitate an evaluation of their performance across diverse NLP tasks in different languages. Although several frameworks have been developed and made publicly available, their customization capabilities for specific tasks and datasets are often complex for different users. In this study, we introduce the LLMeBench framework. Initially developed to evaluate Arabic NLP tasks using OpenAI's GPT and BLOOM models; it can be seamlessly customized for any NLP task and model, regardless of language. The framework also features zero- and few-shot learning settings. A new custom dataset can be added in less than 10 minutes, and users can use their own model API keys to evaluate the task at hand. The developed framework has been already tested on 31 unique NLP tasks using 53 publicly available datasets within 90 experimental setups, involving approximately 296K data points. We plan to open-source the framework for the community
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#38405;&#35835;&#36807;&#31243;&#20013;&#30340;&#30524;&#21160;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#25512;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#33021;&#22815;&#27169;&#25311;&#38405;&#35835;&#38556;&#30861;&#20013;&#19981;&#36866;&#24212;&#25512;&#29702;&#25928;&#26524;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.04941</link><description>&lt;p&gt;
&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#25512;&#29702;&#20197;&#29702;&#35299;&#38405;&#35835;&#21644;&#38405;&#35835;&#38556;&#30861;&#20013;&#30340;&#30524;&#21160;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Integrating large language models and active inference to understand eye movements in reading and dyslexia. (arXiv:2308.04941v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#38405;&#35835;&#36807;&#31243;&#20013;&#30340;&#30524;&#21160;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#25512;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#33021;&#22815;&#27169;&#25311;&#38405;&#35835;&#38556;&#30861;&#20013;&#19981;&#36866;&#24212;&#25512;&#29702;&#25928;&#26524;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#37319;&#29992;&#23618;&#27425;&#21270;&#20027;&#21160;&#25512;&#29702;&#26469;&#27169;&#25311;&#38405;&#35835;&#21644;&#30524;&#21160;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#23558;&#35821;&#35328;&#22788;&#29702;&#25551;&#36848;&#20026;&#23545;&#23618;&#27425;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#65292;&#20174;&#38899;&#33410;&#21040;&#21477;&#23376;&#30340;&#19981;&#21516;&#31890;&#24230;&#23454;&#29616;&#39044;&#27979;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#29992;&#20110;&#23454;&#29616;&#36924;&#30495;&#30340;&#25991;&#26412;&#39044;&#27979;&#65292;&#20197;&#21450;&#20027;&#21160;&#25512;&#29702;&#29992;&#20110;&#24341;&#23548;&#30524;&#21160;&#21040;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#24471;&#23545;&#39044;&#27979;&#36827;&#34892;&#27979;&#35797;&#25104;&#20026;&#21487;&#33021;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29087;&#32451;&#38405;&#35835;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#21333;&#35789;&#21644;&#21477;&#23376;&#65292;&#24182;&#36981;&#24490;&#38405;&#35835;&#21452;&#36335;&#29702;&#35770;&#20013;&#30340;&#35789;&#27719;&#21644;&#38750;&#35789;&#27719;&#36335;&#24452;&#30340;&#21306;&#20998;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20801;&#35768;&#27169;&#25311;&#38405;&#35835;&#36807;&#31243;&#20013;&#23545;&#30524;&#21160;&#34892;&#20026;&#20135;&#29983;&#19981;&#36866;&#24212;&#25512;&#29702;&#25928;&#26524;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#38405;&#35835;&#38556;&#30861;&#12290;&#20026;&#20102;&#27169;&#25311;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#20943;&#24369;&#20102;&#20808;&#39564;&#30340;&#36129;&#29486;&#65292;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#21644;&#26356;&#21152;&#26029;&#29255;&#21270;&#30340;&#38405;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel computational model employing hierarchical active inference to simulate reading and eye movements. The model characterizes linguistic processing as inference over a hierarchical generative model, facilitating predictions and inferences at various levels of granularity, from syllables to sentences.  Our approach combines the strengths of large language models for realistic textual predictions and active inference for guiding eye movements to informative textual information, enabling the testing of predictions. The model exhibits proficiency in reading both known and unknown words and sentences, adhering to the distinction between lexical and nonlexical routes in dual-route theories of reading. Notably, our model permits the exploration of maladaptive inference effects on eye movements during reading, such as in dyslexia. To simulate this condition, we attenuate the contribution of priors during the reading process, leading to incorrect inferences and a more fragmented
&lt;/p&gt;</description></item><item><title>LLaMA-E&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#12290;</title><link>http://arxiv.org/abs/2308.04913</link><description>&lt;p&gt;
LLaMA-E&#65306;&#22810;&#26041;&#38754;&#25351;&#23548;&#19979;&#30340;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#22686;&#24378;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following. (arXiv:2308.04913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04913
&lt;/p&gt;
&lt;p&gt;
LLaMA-E&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#28041;&#21450;&#21019;&#24314;&#21560;&#24341;&#20154;&#12289;&#20016;&#23500;&#19988;&#26377;&#38024;&#23545;&#24615;&#30340;&#20419;&#38144;&#20869;&#23481;&#65292;&#20197;&#25512;&#21160;&#20135;&#21697;&#38144;&#21806;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33539;&#20363;&#65292;&#20026;&#35299;&#20915;&#36825;&#31181;&#24773;&#26223;&#20013;&#30340;&#21508;&#31181;&#21019;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36890;&#29992;&#35821;&#26009;&#24211;&#21644;&#24120;&#35782;&#30693;&#35782;&#35757;&#32451;&#30340;&#20027;&#27969;LLM&#22312;&#36866;&#24212;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#21644;&#23458;&#25143;&#29420;&#29305;&#30340;&#22797;&#26434;&#21644;&#20010;&#24615;&#21270;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#20687;GPT-3.5&#36825;&#26679;&#30340;LLM&#38656;&#35201;&#36827;&#34892;&#36828;&#31243;&#35775;&#38382;&#65292;&#24341;&#21457;&#20102;&#22312;&#20256;&#36755;&#36807;&#31243;&#20013;&#20445;&#25252;&#22823;&#37327;&#23458;&#25143;&#38544;&#31169;&#25968;&#25454;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-E&#65292;&#38024;&#23545;&#22810;&#26679;&#21270;&#30340;&#30005;&#23376;&#21830;&#21153;&#21019;&#20316;&#20219;&#21153;&#30340;&#32479;&#19968;&#19988;&#23450;&#21046;&#30340;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39046;&#22495;&#19987;&#23478;&#20174;&#24191;&#21578;&#29983;&#25104;&#12289;&#26597;&#35810;&#22686;&#24378;&#30340;&#20135;&#21697;&#26631;&#39064;&#25913;&#20889;&#12289;&#20135;&#21697;&#20998;&#31867;&#12289;&#36141;&#20080;&#24847;&#22270;&#25512;&#27979;&#21644;&#24120;&#35268;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#21019;&#24314;&#20102;&#31181;&#23376;&#25351;&#23548;&#38598;&#21512;&#12290;&#36825;&#20123;&#20219;&#21153;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
E-commerce authoring involves creating attractive, abundant, and targeted promotional content to drive product sales. The emergence of large language models (LLMs) introduces an innovative paradigm, offering a unified solution to address various authoring tasks within this scenario. However, mainstream LLMs trained on general corpora with common sense knowledge reveal limitations in fitting complex and personalized features unique to e-commerce products and customers. Furthermore, LLMs like GPT-3.5 necessitate remote accessibility, raising concerns about safeguarding voluminous customer privacy data during transmission. This paper proposes the LLaMA-E, the unified and customized instruction-following language models focusing on diverse e-commerce authoring tasks. Specifically, the domain experts create the seed instruction set from the tasks of ads generation, query-enhanced product title rewriting, product classification, purchase intent speculation, and general Q&amp;A. These tasks enabl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#26041;&#35328;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#20174;wav2vec 2.0&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#28508;&#22312;&#23884;&#20837;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#65292;&#24182;&#22312;&#35813;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04886</link><description>&lt;p&gt;
&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#26041;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance. (arXiv:2308.04886v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04886
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#27663;&#36317;&#31163;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#26041;&#35328;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#20174;wav2vec 2.0&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#28508;&#22312;&#23884;&#20837;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#65292;&#24182;&#22312;&#35813;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#35328;&#20998;&#31867;&#34987;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#22810;&#31181;&#24212;&#29992;&#31995;&#32479;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37096;&#32626;&#30340;&#26041;&#35328;&#20998;&#31867;&#27169;&#22411;&#21487;&#33021;&#20250;&#36935;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#24322;&#24120;&#36755;&#20837;&#65292;&#20063;&#31216;&#20026;&#31163;&#32676;&#26679;&#26412;&#65288;OOD &#26679;&#26412;&#65289;&#12290;&#36825;&#20123; OOD &#26679;&#26412;&#21487;&#33021;&#23548;&#33268;&#24847;&#22806;&#30340;&#36755;&#20986;&#65292;&#22240;&#20026;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#26679;&#26412;&#25152;&#23646;&#30340;&#26041;&#35328;&#12290;&#22312;&#26041;&#35328;&#20998;&#31867;&#39046;&#22495;&#65292;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#26159;&#19968;&#20010;&#40092;&#20026;&#20154;&#30693;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#39532;&#27663;&#36317;&#31163;&#29305;&#24449;&#26041;&#27861;&#26469;&#26816;&#27979;&#31163;&#32676;&#26679;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22522;&#20110; wav2vec 2.0 &#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#27169;&#22411;&#30340;&#25152;&#26377;&#20013;&#38388;&#23618;&#30340;&#28508;&#22312;&#23884;&#20837;&#26469;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31163;&#32676;&#26679;&#26412;&#26816;&#27979;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialect classification is used in a variety of applications, such as machine translation and speech recognition, to improve the overall performance of the system. In a real-world scenario, a deployed dialect classification model can encounter anomalous inputs that differ from the training data distribution, also called out-of-distribution (OOD) samples. Those OOD samples can lead to unexpected outputs, as dialects of those samples are unseen during model training. Out-of-distribution detection is a new research area that has received little attention in the context of dialect classification. Towards this, we proposed a simple yet effective unsupervised Mahalanobis distance feature-based method to detect out-of-distribution samples. We utilize the latent embeddings from all intermediate layers of a wav2vec 2.0 transformer-based dialect classifier model for multi-task learning. Our proposed approach outperforms other state-of-the-art OOD detection methods significantly.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35745;&#31639;&#24314;&#27169;&#65292;&#35813;&#36328;&#35821;&#35328;&#30740;&#31350;&#21033;&#29992;&#20449;&#24687;&#35770;&#37327;&#21270;&#20102;&#20803;&#38899;&#21644;&#35856;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#35789;&#26681;&#24418;&#24335;&#32780;&#19981;&#26159;&#23624;&#25240;&#21464;&#24418;&#30340;&#35789;&#24418;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#28085;&#30422;&#20102;&#26356;&#22810;&#26410;&#34987;&#30740;&#31350;&#30340;&#35821;&#35328;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#31070;&#32463;PLMs&#33021;&#22815;&#25429;&#25417;&#21040;&#23384;&#22312;&#20803;&#38899;&#21644;&#35856;&#30340;&#35821;&#35328;&#20013;&#30340;&#21644;&#35856;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.04885</link><description>&lt;p&gt;
&#20803;&#38899;&#21644;&#35856;&#30340;&#20449;&#24687;&#35770;&#29305;&#24449;&#65306;&#35789;&#27719;&#21015;&#34920;&#30340;&#36328;&#35821;&#35328;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Characterization of Vowel Harmony: A Cross-Linguistic Study on Word Lists. (arXiv:2308.04885v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04885
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#24314;&#27169;&#65292;&#35813;&#36328;&#35821;&#35328;&#30740;&#31350;&#21033;&#29992;&#20449;&#24687;&#35770;&#37327;&#21270;&#20102;&#20803;&#38899;&#21644;&#35856;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#35789;&#26681;&#24418;&#24335;&#32780;&#19981;&#26159;&#23624;&#25240;&#21464;&#24418;&#30340;&#35789;&#24418;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#28085;&#30422;&#20102;&#26356;&#22810;&#26410;&#34987;&#30740;&#31350;&#30340;&#35821;&#35328;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#31070;&#32463;PLMs&#33021;&#22815;&#25429;&#25417;&#21040;&#23384;&#22312;&#20803;&#38899;&#21644;&#35856;&#30340;&#35821;&#35328;&#20013;&#30340;&#21644;&#35856;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#36328;&#35821;&#35328;&#30740;&#31350;&#65292;&#26088;&#22312;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#35745;&#31639;&#24314;&#27169;&#26469;&#37327;&#21270;&#20803;&#38899;&#21644;&#35856;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#35789;&#27719;&#20013;&#20803;&#38899;&#30340;&#21487;&#39044;&#27979;&#24615;&#23450;&#20041;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#30340;&#21644;&#35856;&#24230;&#37327;&#65292;&#25105;&#20204;&#36890;&#36807;&#38899;&#32032;&#32423;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26469;&#20272;&#35745;&#12290;&#27492;&#21069;&#30340;&#23450;&#37327;&#30740;&#31350;&#22312;&#20998;&#26512;&#20803;&#38899;&#21644;&#35856;&#26102;&#20027;&#35201;&#20381;&#36182;&#20110;&#23624;&#25240;&#21464;&#24418;&#30340;&#35789;&#24418;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#36328;&#35821;&#35328;&#21487;&#27604;&#36739;&#30340;&#35789;&#26681;&#24418;&#24335;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#36825;&#20123;&#24418;&#24335;&#24456;&#23569;&#25110;&#27809;&#26377;&#23624;&#25240;&#21464;&#24418;&#65292;&#36825;&#26679;&#21487;&#20197;&#28085;&#30422;&#26356;&#22810;&#26410;&#34987;&#30740;&#31350;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;PLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#21253;&#25324;&#27599;&#31181;&#35821;&#35328;&#26368;&#22810;1000&#20010;&#35789;&#27719;&#26465;&#30446;&#30340;&#35789;&#27719;&#21015;&#34920;&#12290;&#23613;&#31649;&#25105;&#20204;&#20351;&#29992;&#30340;&#25968;&#25454;&#27604;&#20197;&#21069;&#20351;&#29992;&#30340;&#35821;&#26009;&#24211;&#35201;&#23567;&#24471;&#22810;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#31070;&#32463;PLMs&#33021;&#22815;&#25429;&#25417;&#21040;&#19968;&#32452;&#23384;&#22312;&#36825;&#31181;&#29616;&#35937;&#30340;&#35821;&#35328;&#20013;&#30340;&#20803;&#38899;&#21644;&#35856;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#35777;&#26126;&#20102;&#35789;&#27719;&#21015;&#34920;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#31867;&#22411;&#23398;&#30740;&#31350;&#36164;&#28304;&#65292;
&lt;/p&gt;
&lt;p&gt;
We present a cross-linguistic study that aims to quantify vowel harmony using data-driven computational modeling. Concretely, we define an information-theoretic measure of harmonicity based on the predictability of vowels in a natural language lexicon, which we estimate using phoneme-level language models (PLMs). Prior quantitative studies have relied heavily on inflected word-forms in the analysis of vowel harmony. We instead train our models using cross-linguistically comparable lemma forms with little or no inflection, which enables us to cover more under-studied languages. Training data for our PLMs consists of word lists with a maximum of 1000 entries per language. Despite the fact that the data we employ are substantially smaller than previously used corpora, our experiments demonstrate the neural PLMs capture vowel harmony patterns in a set of languages that exhibit this phenomenon. Our work also demonstrates that word lists are a valuable resource for typological research, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24773;&#24863;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#25913;&#21464;&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#25991;&#26412;&#20013;&#26465;&#20214;&#21464;&#37327;&#30340;&#23454;&#29616;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#27982;&#39640;&#25928;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04857</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#36827;&#34892;&#24773;&#24863;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Emotion-Conditioned Text Generation through Automatic Prompt Optimization. (arXiv:2308.04857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24773;&#24863;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#25913;&#21464;&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#25991;&#26412;&#20013;&#26465;&#20214;&#21464;&#37327;&#30340;&#23454;&#29616;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#27982;&#39640;&#25928;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#32791;&#36153;&#24040;&#22823;&#30340;&#24494;&#35843;&#25110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#24456;&#38590;&#22312;&#27809;&#26377;&#22823;&#37327;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#22909;&#30340;&#32467;&#26524;&#12290;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#32780;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#21017;&#26159;&#19968;&#20010;&#20855;&#26377;&#28508;&#21147;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36825;&#26159;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#21448;&#33021;&#22815;&#36798;&#21040;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#21644;&#32467;&#26500;&#21270;&#39044;&#27979;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#20013;&#21463;&#21040;&#30340;&#20851;&#27880;&#21364;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#24773;&#24863;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#28155;&#21152;&#12289;&#21024;&#38500;&#25110;&#26367;&#25442;&#26631;&#35760;&#26469;&#25913;&#21464;&#25552;&#31034;&#12290;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#19968;&#20010;&#25991;&#26412;&#20998;&#31867;&#22120;&#26469;&#34913;&#37327;&#29983;&#25104;&#25991;&#26412;&#20013;&#26465;&#20214;&#21464;&#37327;&#30340;&#23454;&#29616;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional natural language generation methods often require either expensive fine-tuning or training a large language model from scratch. Both are unlikely to lead to good results without a substantial amount of data and computational resources. Prompt learning without changing the parameters of a large language model presents a promising alternative. It is a cost-effective approach, while still achieving competitive results. While this procedure is now established for zero- and few-shot text classification and structured prediction, it has received limited attention in conditional text generation. We present the first automatic prompt optimization approach for emotion-conditioned text generation with instruction-fine-tuned models. Our method uses an iterative optimization procedure that changes the prompt by adding, removing, or replacing tokens. As objective function, we only require a text classifier that measures the realization of the conditional variable in the generated text. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968; TSSR&#65292;&#20855;&#26377;&#22855;&#25968;&#12289;&#38750;&#32447;&#24615;&#12289;&#21333;&#35843;&#21644;&#21487;&#24494;&#20998;&#30340;&#29305;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;TSSR&#30456;&#27604;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.04832</link><description>&lt;p&gt;
TSSR&#65306;&#19968;&#31181;&#25130;&#26029;&#21644;&#24102;&#31526;&#21495;&#30340;&#24179;&#26041;&#26681;&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TSSR: A Truncated and Signed Square Root Activation Function for Neural Networks. (arXiv:2308.04832v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968; TSSR&#65292;&#20855;&#26377;&#22855;&#25968;&#12289;&#38750;&#32447;&#24615;&#12289;&#21333;&#35843;&#21644;&#21487;&#24494;&#20998;&#30340;&#29305;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;TSSR&#30456;&#27604;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#26159;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#31216;&#20026;&#25130;&#26029;&#21644;&#24102;&#31526;&#21495;&#24179;&#26041;&#26681;&#65288;TSSR&#65289;&#20989;&#25968;&#12290;&#35813;&#20989;&#25968;&#20855;&#26377;&#22855;&#25968;&#12289;&#38750;&#32447;&#24615;&#12289;&#21333;&#35843;&#21644;&#21487;&#24494;&#20998;&#30340;&#29305;&#24615;&#12290;&#20854;&#26799;&#24230;&#26159;&#36830;&#32493;&#19988;&#22987;&#32456;&#20026;&#27491;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#24615;&#65292;&#23427;&#26377;&#28508;&#21147;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#12290;&#22810;&#20010;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#30340;TSSR&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#28608;&#27963;&#20989;&#25968;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#35813;&#20989;&#25968;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation functions are essential components of neural networks. In this paper, we introduce a new activation function called the Truncated and Signed Square Root (TSSR) function. This function is distinctive because it is odd, nonlinear, monotone and differentiable. Its gradient is continuous and always positive. Thanks to these properties, it has the potential to improve the numerical stability of neural networks. Several experiments confirm that the proposed TSSR has better performance than other stat-of-the-art activation functions. The proposed function has significant implications for the development of neural network models and can be applied to a wide range of applications in fields such as computer vision, natural language processing, and speech recognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;Gscore&#20316;&#20026;&#34913;&#37327;&#29983;&#25104;&#32467;&#26524;&#36136;&#37327;&#30340;&#32508;&#21512;&#25351;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.04823</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Generation Capabilities of Large Chinese Language Models. (arXiv:2308.04823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;Gscore&#20316;&#20026;&#34913;&#37327;&#29983;&#25104;&#32467;&#26524;&#36136;&#37327;&#30340;&#32508;&#21512;&#25351;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CG-Eval&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23545;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#31185;&#23398;&#24037;&#31243;&#12289;&#20154;&#25991;&#31038;&#31185;&#12289;&#25968;&#23398;&#35745;&#31639;&#12289;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#12289;&#21496;&#27861;&#32771;&#35797;&#21644;&#27880;&#20876;&#20250;&#35745;&#24072;&#32771;&#35797;&#20845;&#20010;&#23398;&#31185;&#20013;&#29983;&#25104;&#20934;&#30830;&#21644;&#30456;&#20851;&#30340;&#22238;&#31572;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;Gscore&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#21152;&#26435;&#27714;&#21644;&#24471;&#21040;&#30340;&#32508;&#21512;&#25351;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#19982;&#21442;&#32771;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#27979;&#35797;&#25968;&#25454;&#21644;&#27979;&#35797;&#32467;&#26524;&#21487;&#22312;&#27492;http URL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents CG-Eval, the first comprehensive evaluation of the generation capabilities of large Chinese language models across a wide range of academic disciplines. The models' performance was assessed based on their ability to generate accurate and relevant responses to different types of questions in six disciplines, namely, Science and Engineering, Humanities and Social Sciences, Mathematical Calculations, Medical Practitioner Qualification Examination, Judicial Examination, and Certified Public Accountant Examination. This paper also presents Gscore, a composite index derived from the weighted sum of multiple metrics to measure the quality of model's generation against a reference. The test data and test results can be found at this http URL
&lt;/p&gt;</description></item><item><title>CLEVA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#21451;&#22909;&#24179;&#21488;&#65292;&#36890;&#36807;&#26631;&#20934;&#21270;&#24037;&#20316;&#27969;&#31243;&#12289;&#31454;&#20105;&#25490;&#34892;&#27036;&#21644;&#20943;&#23569;&#27745;&#26579;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.04813</link><description>&lt;p&gt;
CLEVA&#65306;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
CLEVA: Chinese Language Models EVAluation Platform. (arXiv:2308.04813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04813
&lt;/p&gt;
&lt;p&gt;
CLEVA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#21451;&#22909;&#24179;&#21488;&#65292;&#36890;&#36807;&#26631;&#20934;&#21270;&#24037;&#20316;&#27969;&#31243;&#12289;&#31454;&#20105;&#25490;&#34892;&#27036;&#21644;&#20943;&#23569;&#27745;&#26579;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#26029;&#20986;&#29616;&#65292;&#22914;&#20309;&#35780;&#20272;&#27169;&#22411;&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#35780;&#20272;&#20013;&#25991;LLMs&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#22522;&#20934;&#12289;&#38750;&#26631;&#20934;&#21270;&#21644;&#26080;&#27861;&#27604;&#36739;&#30340;&#25552;&#31034;&#36807;&#31243;&#65292;&#20197;&#21450;&#26222;&#36941;&#23384;&#22312;&#30340;&#27745;&#26579;&#39118;&#38505;&#31561;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CLEVA&#65292;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#20013;&#25991;LLMs&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#37319;&#29992;&#26631;&#20934;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#23450;&#26399;&#26356;&#26032;&#31454;&#20105;&#25490;&#34892;&#27036;&#65292;&#35780;&#20272;LLMs&#22312;&#21508;&#20010;&#32500;&#24230;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#23569;&#27745;&#26579;&#65292;CLEVA&#31934;&#36873;&#20102;&#22823;&#37327;&#26032;&#25968;&#25454;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#65292;&#20445;&#35777;&#27599;&#20010;&#25490;&#34892;&#27036;&#36718;&#27425;&#37117;&#26377;&#29420;&#29305;&#30340;&#23376;&#38598;&#12290;&#29992;&#25143;&#21482;&#38656;&#28857;&#20987;&#20960;&#19979;&#40736;&#26631;&#24182;&#20351;&#29992;&#27169;&#22411;API&#21363;&#21487;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#26080;&#38656;&#32534;&#20889;&#22823;&#37327;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous emergence of Chinese Large Language Models (LLMs), how to evaluate a model's capabilities has become an increasingly significant issue. The absence of a comprehensive Chinese benchmark that thoroughly assesses a model's performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted to holistically evaluate Chinese LLMs. Our platform employs a standardized workflow to assess LLMs' performance across various dimensions, regularly updating a competitive leaderboard. To alleviate contamination, CLEVA curates a significant proportion of new data and develops a sampling strategy that guarantees a unique subset for each leaderboard round. Empowered by an easy-to-use interface that requires just a few mouse clicks and a model API, users can conduct a thorough evaluation with minimal coding. Large-scale experiments
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20108;&#20998;&#24322;&#26500;&#22270;(BHG)&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20855;&#26377;&#24120;&#35782;&#30693;&#35782;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#24120;&#35265;&#30340;&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#30693;&#35782;&#20016;&#23500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.04811</link><description>&lt;p&gt;
&#25105;&#20204;&#21482;&#38656;&#35201;&#19968;&#20010;&#20108;&#20998;&#22270;&#26469;&#22686;&#24378;&#20855;&#26377;&#24120;&#35782;&#30693;&#35782;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Bipartite Graph is All We Need for Enhancing Emotional Reasoning with Commonsense Knowledge. (arXiv:2308.04811v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04811
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20108;&#20998;&#24322;&#26500;&#22270;(BHG)&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20855;&#26377;&#24120;&#35782;&#30693;&#35782;&#30340;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#24120;&#35265;&#30340;&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#30693;&#35782;&#20016;&#23500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#22312;&#24773;&#24863;&#25512;&#29702;&#26041;&#38754;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35805;&#20013;&#65292;&#23545;&#20110;&#35832;&#22914;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#36827;&#34892;&#22312;&#32447;&#24847;&#35265;&#25366;&#25496;&#21644;&#20849;&#24773;&#23545;&#35805;&#31995;&#32479;&#31561;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#30001;&#20110;&#35768;&#22810;&#24773;&#22659;&#20013;&#24773;&#24863;&#30340;&#20256;&#36882;&#20855;&#26377;&#38544;&#21547;&#24615;&#36136;&#65292;&#24120;&#35782;&#30693;&#35782;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20016;&#23500;&#35805;&#35821;&#35821;&#20041;&#21644;&#22686;&#24378;&#23545;&#35805;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#25191;&#34892;&#32463;&#39564;&#24615;&#30340;&#30693;&#35782;&#36807;&#28388;&#65292;&#24182;&#35774;&#35745;&#39640;&#24230;&#23450;&#21046;&#30340;&#26550;&#26500;&#20197;&#19982;&#35805;&#35821;&#36827;&#34892;&#30693;&#35782;&#20132;&#20114;&#65292;&#36825;&#21487;&#33021;&#20002;&#24323;&#26377;&#29992;&#30340;&#30693;&#35782;&#26041;&#38754;&#24182;&#38480;&#21046;&#20854;&#23545;&#19981;&#21516;&#30693;&#35782;&#26469;&#28304;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#20855;&#26377;&#24120;&#35782;&#30693;&#35782;&#30340;&#24773;&#24863;&#25512;&#29702;&#30340;&#20108;&#20998;&#24322;&#26500;&#22270;(BHG)&#26041;&#27861;&#12290;&#22312;BHG&#20013;&#65292;&#25552;&#21462;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#35805;&#35821;&#34920;&#31034;&#21644;&#30693;&#35782;&#34920;&#31034;&#34987;&#24314;&#27169;&#20026;&#24322;&#26500;&#33410;&#28857;&#12290;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26356;&#22810;&#30340;&#30693;&#35782;&#32858;&#21512;&#33410;&#28857;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The context-aware emotional reasoning ability of AI systems, especially in conversations, is of vital importance in applications such as online opinion mining from social media and empathetic dialogue systems. Due to the implicit nature of conveying emotions in many scenarios, commonsense knowledge is widely utilized to enrich utterance semantics and enhance conversation modeling. However, most previous knowledge infusion methods perform empirical knowledge filtering and design highly customized architectures for knowledge interaction with the utterances, which can discard useful knowledge aspects and limit their generalizability to different knowledge sources. Based on these observations, we propose a Bipartite Heterogeneous Graph (BHG) method for enhancing emotional reasoning with commonsense knowledge. In BHG, the extracted context-aware utterance representations and knowledge representations are modeled as heterogeneous nodes. Two more knowledge aggregation node types are proposed 
&lt;/p&gt;</description></item><item><title>ADMUS&#26159;&#19968;&#31181;&#28176;&#36827;&#24335;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#30693;&#35782;&#28304;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#12289;&#22810;&#31181;&#30693;&#35782;&#24211;&#21644;&#19981;&#21516;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#23427;&#36890;&#36807;&#35299;&#32806;&#30693;&#35782;&#24211;&#38382;&#31572;&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#25968;&#25454;&#38598;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26032;&#25968;&#25454;&#38598;&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#24182;&#19988;&#25104;&#26412;&#26497;&#20302;&#12290;</title><link>http://arxiv.org/abs/2308.04800</link><description>&lt;p&gt;
ADMUS: &#19968;&#31181;&#36866;&#24212;&#22810;&#31181;&#30693;&#35782;&#28304;&#30340;&#28176;&#36827;&#24335;&#38382;&#31572;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ADMUS: A Progressive Question Answering Framework Adaptable to Multiple Knowledge Sources. (arXiv:2308.04800v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04800
&lt;/p&gt;
&lt;p&gt;
ADMUS&#26159;&#19968;&#31181;&#28176;&#36827;&#24335;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#30693;&#35782;&#28304;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#12289;&#22810;&#31181;&#30693;&#35782;&#24211;&#21644;&#19981;&#21516;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#23427;&#36890;&#36807;&#35299;&#32806;&#30693;&#35782;&#24211;&#38382;&#31572;&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#25968;&#25454;&#38598;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26032;&#25968;&#25454;&#38598;&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#24182;&#19988;&#25104;&#26412;&#26497;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24341;&#20837;&#65292;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#31995;&#32479;&#22312;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20110;&#25552;&#39640;&#27169;&#22411;&#22312;&#21508;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#65292;&#24573;&#35270;&#20102;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65288;&#20363;&#22914;&#22810;&#31199;&#25143;&#24179;&#21488;&#65289;&#23558;&#31995;&#32479;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#39640;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ADMUS&#65292;&#19968;&#31181;&#36866;&#24212;&#22810;&#31181;&#25968;&#25454;&#38598;&#30340;&#28176;&#36827;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#26694;&#26550;&#65292;&#21253;&#25324;&#22810;&#31181;&#35821;&#35328;&#65292;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#21644;&#19981;&#21516;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#35299;&#32806;&#24120;&#35268;&#30693;&#35782;&#24211;&#38382;&#31572;&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#31181;&#19982;&#25968;&#25454;&#38598;&#26080;&#20851;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25903;&#25345;&#26080;&#32541;&#38598;&#25104;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21482;&#38656;&#35201;&#20197;&#26497;&#23567;&#30340;&#25104;&#26412;&#21019;&#24314;&#19968;&#20010;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#24494;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the introduction of deep learning models, semantic parsingbased knowledge base question answering (KBQA) systems have achieved high performance in handling complex questions. However, most existing approaches primarily focus on enhancing the model's effectiveness on individual benchmark datasets, disregarding the high costs of adapting the system to disparate datasets in real-world scenarios (e.g., multi-tenant platform). Therefore, we present ADMUS, a progressive knowledge base question answering framework designed to accommodate a wide variety of datasets, including multiple languages, diverse backbone knowledge bases, and disparate question answering datasets. To accomplish the purpose, we decouple the architecture of conventional KBQA systems and propose this dataset-independent framework. Our framework supports the seamless integration of new datasets with minimal effort, only requiring creating a dataset-related micro-service at a negligible cost. To enhance the usability of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#26391;&#35835;&#35821;&#38899;&#25968;&#25454;&#65292;&#35780;&#20272;&#20102;&#19968;&#31181;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#22312;&#33258;&#21160;&#27979;&#37327;&#22833;&#35821;&#30151;&#24739;&#32773;&#35328;&#35821;&#27969;&#21033;&#24230;&#26041;&#38754;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#23545;&#27604;&#19987;&#19994;&#35328;&#35821;&#27835;&#30103;&#24072;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#21457;&#29616;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#35328;&#35821;&#27969;&#21033;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.04763</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#27979;&#37327;&#22833;&#35821;&#30151;&#24739;&#32773;&#35762;&#35805;&#27969;&#21033;&#24230;&#30340;&#21021;&#27493;&#25104;&#26524;&#65306;&#22522;&#20110;&#26391;&#35835;&#35821;&#38899;&#25968;&#25454;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Automatically measuring speech fluency in people with aphasia: first achievements using read-speech data. (arXiv:2308.04763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#26391;&#35835;&#35821;&#38899;&#25968;&#25454;&#65292;&#35780;&#20272;&#20102;&#19968;&#31181;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#22312;&#33258;&#21160;&#27979;&#37327;&#22833;&#35821;&#30151;&#24739;&#32773;&#35328;&#35821;&#27969;&#21033;&#24230;&#26041;&#38754;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#23545;&#27604;&#19987;&#19994;&#35328;&#35821;&#27835;&#30103;&#24072;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#21457;&#29616;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#35328;&#35821;&#27969;&#21033;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#35328;&#35821;&#21644;&#35821;&#35328;&#30149;&#29702;&#23398;&#23478;&#24120;&#24120;&#20381;&#36182;&#23545;&#20110;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#35328;&#35821;&#27969;&#21033;&#24615;&#30340;&#35780;&#21028;&#26469;&#35786;&#26029;&#25110;&#30417;&#27979;&#24739;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20027;&#35266;&#26041;&#27861;&#22240;&#20854;&#21487;&#38752;&#24615;&#19981;&#36275;&#20197;&#21450;&#22312;&#20020;&#24202;&#19978;&#33457;&#36153;&#36807;&#22810;&#30340;&#26102;&#38388;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#30446;&#30340;&#65306;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#19968;&#31181;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#22312;&#33258;&#21160;&#27979;&#37327;&#22833;&#35821;&#30151;&#24739;&#32773;&#35328;&#35821;&#27969;&#21033;&#24230;&#26041;&#38754;&#30340;&#30456;&#20851;&#24615;&#65292;&#35813;&#31639;&#27861;&#26368;&#21021;&#26159;&#22312;&#35821;&#35328;&#20064;&#24471;&#39046;&#22495;&#24320;&#21457;&#30340;&#12290;&#26041;&#27861;&#19982;&#36807;&#31243;&#65306;&#36890;&#36807;&#38750;&#33829;&#21033;&#32452;&#32455;&#21644;&#35328;&#35821;&#27835;&#30103;&#24072;&#32593;&#32476;&#25307;&#21215;&#20102;29&#21517;&#22833;&#35821;&#30151;&#24739;&#32773;&#21644;5&#21517;&#23545;&#29031;&#21442;&#19982;&#32773;&#12290;&#25152;&#26377;&#21442;&#19982;&#32773;&#22312;&#26391;&#35835;&#27861;&#35821;&#29256;&#27874;&#22763;&#39039;&#35786;&#26029;&#24615;&#22833;&#35821;&#30151;&#27979;&#35797;&#20013;&#30340;&#19968;&#32452;&#21477;&#23376;&#26102;&#34987;&#24405;&#21046;&#19979;&#26469;&#12290;&#19977;&#21517;&#21463;&#36807;&#35757;&#32451;&#30340;&#35328;&#35821;&#27835;&#30103;&#24072;&#26681;&#25454;&#19968;&#20010;&#20116;&#28857;&#20020;&#24202;&#24615;&#36136;&#35780;&#20215;&#23610;&#24230;&#35780;&#20272;&#20102;&#27599;&#20010;&#21477;&#23376;&#30340;&#27969;&#21033;&#24230;&#12290;&#20351;&#29992;&#27491;&#21453;&#21521;&#20998;&#27495;&#20998;&#21106;&#21644;&#32858;&#31867;&#31639;&#27861;&#26469;&#35745;&#31639;&#27599;&#20010;&#21477;&#23376;&#30340;&#22235;&#20010;&#33258;&#21160;&#39044;&#27979;&#22240;&#23376;&#65292;&#21363;&#35821;&#38899;&#30456;&#20851;&#37096;&#20998;&#30340;&#38271;&#24230;&#12289;&#27491;&#21453;&#21521;&#20998;&#27495;&#37096;&#20998;&#30340;&#38271;&#24230;&#12289;&#27491;&#21453;&#21521;&#20998;&#27495;&#37096;&#20998;&#30340;&#25968;&#37327;&#21644;&#21477;&#23376;&#20013;&#20849;&#25391;&#31163;&#25955;&#31243;&#24230;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Speech and language pathologists (SLPs) often relyon judgements of speech fluency for diagnosing or monitoringpatients with aphasia. However, such subjective methods havebeen criticised for their lack of reliability and their clinical cost interms of time. Aims: This study aims at assessing the relevance of a signalprocessingalgorithm, initially developed in the field of language acquisition, for the automatic measurement of speech fluency in people with aphasia (PWA). Methods &amp; Procedures: Twenty-nine PWA and five control participantswere recruited via non-profit organizations and SLP networks. All participants were recorded while reading out loud a set ofsentences taken from the French version of the Boston Diagnostic Aphasia Examination. Three trained SLPs assessed the fluency of each sentence on a five-point qualitative scale. A forward-backward divergence segmentation and a clustering algorithm were used to compute, for each sentence, four automatic predictors of speec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#26816;&#32034;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;/&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#21644;&#26597;&#35810;&#20998;&#35299;&#27169;&#22411;&#65292;&#26356;&#20934;&#30830;&#22320;&#32858;&#28966;&#20110;&#26597;&#35810;&#30340;&#19981;&#21516;&#20449;&#24687;&#21333;&#20803;&#12290;&#36825;&#31181;&#27969;&#31243;&#22312;&#27573;&#33853;&#35206;&#30422;&#21644;&#21629;&#21517;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#25913;&#36827;&#65292;&#21516;&#26102;&#20063;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04756</link><description>&lt;p&gt;
&#20026;&#26032;&#30340;&#39046;&#22495;&#26500;&#24314;&#21487;&#35299;&#37322;&#21644;&#21487;&#38752;&#30340;&#24320;&#25918;&#20449;&#24687;&#26816;&#32034;&#22120;
&lt;/p&gt;
&lt;p&gt;
Building Interpretable and Reliable Open Information Retriever for New Domains Overnight. (arXiv:2308.04756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#26816;&#32034;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;/&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#21644;&#26597;&#35810;&#20998;&#35299;&#27169;&#22411;&#65292;&#26356;&#20934;&#30830;&#22320;&#32858;&#28966;&#20110;&#26597;&#35810;&#30340;&#19981;&#21516;&#20449;&#24687;&#21333;&#20803;&#12290;&#36825;&#31181;&#27969;&#31243;&#22312;&#27573;&#33853;&#35206;&#30422;&#21644;&#21629;&#21517;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#25913;&#36827;&#65292;&#21516;&#26102;&#20063;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#25110;&#30693;&#35782;&#26816;&#32034;&#26159;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65289;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#31616;&#27905;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#31264;&#23494;&#21521;&#37327;&#34920;&#31034;&#26597;&#35810;&#21644;&#30693;&#35782;&#27573;&#33853;&#65292;&#24182;&#23398;&#20064;&#35789;&#27719;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;IR&#21644;QA&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#21333;&#19968;&#30340;&#31264;&#23494;&#21521;&#37327;&#21644;&#31471;&#21040;&#31471;&#30417;&#30563;&#24182;&#19981;&#24635;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#26597;&#35810;&#21487;&#33021;&#38656;&#35201;&#27880;&#24847;&#22810;&#20010;&#26041;&#38754;&#21644;&#38544;&#21547;&#30340;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#26816;&#32034;&#27969;&#31243;&#65292;&#21033;&#29992;&#23454;&#20307;/&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#21644;&#26597;&#35810;&#20998;&#35299;&#27169;&#22411;&#65292;&#26356;&#20934;&#30830;&#22320;&#32858;&#28966;&#20110;&#26597;&#35810;&#30340;&#19981;&#21516;&#20449;&#24687;&#21333;&#20803;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23613;&#31649;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27969;&#31243;&#22312;&#27573;&#33853;&#35206;&#30422;&#21644;&#21629;&#21517;&#20934;&#30830;&#24615;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval (IR) or knowledge retrieval, is a critical component for many down-stream tasks such as open-domain question answering (QA). It is also very challenging, as it requires succinctness, completeness, and correctness. In recent works, dense retrieval models have achieved state-of-the-art (SOTA) performance on in-domain IR and QA benchmarks by representing queries and knowledge passages with dense vectors and learning the lexical and semantic similarity. However, using single dense vectors and end-to-end supervision are not always optimal because queries may require attention to multiple aspects and event implicit knowledge. In this work, we propose an information retrieval pipeline that uses entity/event linking model and query decomposition model to focus more accurately on different information units of the query. We show that, while being more interpretable and reliable, our proposed pipeline significantly improves passage coverages and denotation accuracies across
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#65292;&#22312;&#27133;&#20301;&#24402;&#32435;&#20219;&#21153;&#20013;&#65292;&#25104;&#21151;&#22320;&#35825;&#23548;&#20102;&#27133;&#20301;&#36793;&#30028;&#65292;&#24182;&#22312;&#20004;&#20010;NLU&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#20196;&#29260;&#32423;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#33021;&#25552;&#20379;&#22686;&#24378;&#30340;&#27133;&#20301;&#26631;&#31614;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.04712</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#21644;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#27133;&#20301;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning. (arXiv:2308.04712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#65292;&#22312;&#27133;&#20301;&#24402;&#32435;&#20219;&#21153;&#20013;&#65292;&#25104;&#21151;&#22320;&#35825;&#23548;&#20102;&#27133;&#20301;&#36793;&#30028;&#65292;&#24182;&#22312;&#20004;&#20010;NLU&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#20196;&#29260;&#32423;&#30417;&#30563;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#33021;&#25552;&#20379;&#22686;&#24378;&#30340;&#27133;&#20301;&#26631;&#31614;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#65288;&#22914;&#24847;&#22270;&#35782;&#21035;&#21644;&#27133;&#20301;&#22635;&#20805;&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#25165;&#33021;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#22312;&#29616;&#23454;&#20013;&#65292;&#26631;&#35760;&#30340;&#26102;&#38388;&#32423;&#21035;&#65288;&#27133;&#20301;&#26631;&#31614;&#65289;&#32791;&#26102;&#19988;&#38590;&#20197;&#33719;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27133;&#20301;&#24402;&#32435;(SI)&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#27809;&#26377;&#26174;&#24335;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#35825;&#23548;&#27133;&#20301;&#36793;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#25506;&#27979;&#21644;&#23545;&#27604;&#23398;&#20064;&#26426;&#21046;&#26469;&#21033;&#29992;(1)&#20174;PLM&#20013;&#25552;&#21462;&#30340;&#26080;&#30417;&#30563;&#35821;&#20041;&#30693;&#35782;&#65292;&#21644;(2)&#20174;TOD&#20013;&#21487;&#29992;&#30340;&#39069;&#22806;&#21477;&#23376;&#32423;&#24847;&#22270;&#26631;&#31614;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27133;&#20301;&#24402;&#32435;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#33021;&#22815;&#24357;&#34917;&#19982;&#22522;&#20110;&#20196;&#29260;&#32423;&#30417;&#30563;&#27169;&#22411;&#22312;&#20004;&#20010;NLU&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24046;&#36317;&#12290;&#24403;&#25512;&#24191;&#21040;&#26032;&#20986;&#29616;&#30340;&#24847;&#22270;&#26102;&#65292;&#25105;&#20204;&#30340;SI&#30446;&#26631;&#20063;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#27133;&#20301;&#26631;&#31614;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advanced methods in Natural Language Understanding for Task-oriented Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a large amount of annotated data to achieve competitive performance. In reality, token-level annotations (slot labels) are time-consuming and difficult to acquire. In this work, we study the Slot Induction (SI) task whose objective is to induce slot boundaries without explicit knowledge of token-level slot annotations. We propose leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised semantic knowledge extracted from PLM, and (2) additional sentence-level intent label signals available from TOD. Our approach is shown to be effective in SI task and capable of bridging the gaps with token-level supervised models on two NLU benchmark datasets. When generalized to emerging intents, our SI objectives also provide enhanced slot label representations, leading to improved per
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25913;&#36827;&#22312;&#20855;&#26377;&#20805;&#20998;&#35299;&#37322;&#24615;&#32972;&#26223;&#19979;&#65292;&#20351;&#29992;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#35757;&#32451;&#20013;&#26410;&#35265;&#30340;&#25361;&#25112;&#24615;&#30701;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#29702;&#25454;&#29983;&#25104;&#21644;&#23494;&#38598;&#26816;&#32034;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#29702;&#25454;&#25490;&#21517;&#27169;&#22411;&#36827;&#34892;&#35780;&#20998;&#21644;&#32452;&#21512;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#22686;&#24378;&#26816;&#32034;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#36739;&#23567;&#30340;&#25512;&#29702;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#38271;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.04711</link><description>&lt;p&gt;
&#20351;&#29992;&#29702;&#30001;&#29983;&#25104;&#21644;&#23494;&#38598;&#26816;&#32034;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Answering Unseen Questions With Smaller Language\\Models Using Rationale Generation and Dense Retrieval. (arXiv:2308.04711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25913;&#36827;&#22312;&#20855;&#26377;&#20805;&#20998;&#35299;&#37322;&#24615;&#32972;&#26223;&#19979;&#65292;&#20351;&#29992;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#35757;&#32451;&#20013;&#26410;&#35265;&#30340;&#25361;&#25112;&#24615;&#30701;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#29702;&#25454;&#29983;&#25104;&#21644;&#23494;&#38598;&#26816;&#32034;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#29702;&#25454;&#25490;&#21517;&#27169;&#22411;&#36827;&#34892;&#35780;&#20998;&#21644;&#32452;&#21512;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#22686;&#24378;&#26816;&#32034;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#36739;&#23567;&#30340;&#25512;&#29702;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#38271;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#20379;&#36275;&#22815;&#30340;&#35299;&#37322;&#24615;&#32972;&#26223;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#35777;&#26126;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25361;&#25112;&#24615;&#30340;&#26080;&#27861;&#22312;&#35757;&#32451;&#20013;&#35265;&#36807;&#30340;&#30701;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#36827;&#19968;&#27493;&#25913;&#36827;&#35813;&#22330;&#26223;&#30340;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#27880;&#37325;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#29702;&#30001;&#19982;&#36890;&#36807;&#22810;&#36718;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#21019;&#24314;&#30340;&#26356;&#38271;&#19978;&#19979;&#25991;&#32467;&#21512;&#36215;&#26469;&#12290;&#31532;&#19968;&#20010;&#26041;&#27861;&#65288;$RR$&#65289;&#28041;&#21450;&#35757;&#32451;&#19968;&#20010;&#29702;&#25454;&#25490;&#21517;&#27169;&#22411;&#65292;&#20197;&#35780;&#20998;&#30340;&#26041;&#24335;&#34913;&#37327;&#29983;&#25104;&#30340;&#29702;&#30001;&#21644;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#35780;&#20998;&#20351;&#29992;&#22810;&#31181;&#32452;&#21512;&#31574;&#30053;&#20174;&#20004;&#20010;&#30693;&#35782;&#28304;&#20013;&#33719;&#24471;&#32452;&#21512;&#19978;&#19979;&#25991;&#12290;&#23545;&#20110;&#31532;&#20108;&#31181;&#26041;&#27861;&#65288;$RATD$&#65289;&#65292;&#25105;&#20204;&#20351;&#29992;&#22686;&#24378;&#26816;&#32034;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#36739;&#23567;&#30340;&#25512;&#29702;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#29087;&#32451;&#22320;&#21033;&#29992;&#26469;&#33258;&#26356;&#38271;&#25991;&#26412;&#24207;&#21015;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#33021;&#37096;&#20998;&#20855;&#26377;&#35777;&#25454;&#24615;&#19988;&#39057;&#32321;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
When provided with sufficient explanatory context, smaller Language Models have been shown to exhibit strong reasoning ability on challenging short-answer question-answering tasks where the questions are unseen in training. We evaluate two methods for further improvement in this setting. Both methods focus on combining rationales generated by a larger Language Model with longer contexts created from a multi-hop dense retrieval system. The first method ($\textit{RR}$) involves training a Rationale Ranking model to score both generated rationales and retrieved contexts with respect to relevance and truthfulness. We then use the scores to derive combined contexts from both knowledge sources using a number of combinatory strategies. For the second method ($\textit{RATD}$) we train a smaller Reasoning model using retrieval-augmented training datasets such that it becomes proficient at utilising relevant information from longer text sequences that may be only partially evidential and frequen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#20010;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;GPT-4&#12289;Claude 2&#22312;&#32958;&#30149;&#23398;&#20869;&#31185;&#22810;&#39033;&#36873;&#25321;&#39064;&#32771;&#35797;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#26410;&#26469;&#26377;&#28508;&#21147;&#25104;&#20026;&#21307;&#23398;&#22521;&#35757;&#12289;&#21307;&#30103;&#21327;&#21161;&#21644;&#24739;&#32773;&#20132;&#20114;&#30340;&#19968;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.04709</link><description>&lt;p&gt;
&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#21644;Claude 2&#30340;&#27604;&#36739;&#30740;&#31350;&#65306;&#32958;&#30149;&#23398;&#20013;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#32771;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Open-Source Large Language Models, GPT-4 and Claude 2: Multiple-Choice Test Taking in Nephrology. (arXiv:2308.04709v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#20010;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;GPT-4&#12289;Claude 2&#22312;&#32958;&#30149;&#23398;&#20869;&#31185;&#22810;&#39033;&#36873;&#25321;&#39064;&#32771;&#35797;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#26410;&#26469;&#26377;&#28508;&#21147;&#25104;&#20026;&#21307;&#23398;&#22521;&#35757;&#12289;&#21307;&#30103;&#21327;&#21161;&#21644;&#24739;&#32773;&#20132;&#20114;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;LLM&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102; remarkable &#33021;&#21147;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;LLM&#21644;&#20854;&#20182;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25152;&#25198;&#28436;&#30340;&#30830;&#20999;&#35282;&#33394;&#20173;&#19981;&#28165;&#26970;&#12290;&#23558;&#26469;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#21487;&#33021;&#25104;&#20026;&#36866;&#24212;&#24615;&#21307;&#24072;&#22521;&#35757;&#12289;&#21307;&#30103;&#21327;&#21161;&#24212;&#29992;&#21644;&#25968;&#23383;&#21270;&#24739;&#32773;&#20132;&#20114;&#22330;&#26223;&#30340;&#19968;&#37096;&#20998;&#12290;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21442;&#19982;&#21307;&#23398;&#22521;&#35757;&#21644;&#24739;&#32773;&#25252;&#29702;&#30340;&#33021;&#21147;&#23558;&#37096;&#20998;&#21462;&#20915;&#20110;&#23427;&#20204;&#26159;&#21542;&#25484;&#25569;&#29305;&#23450;&#21307;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#22312;&#20869;&#31185;&#19987;&#19994;&#22810;&#39033;&#36873;&#25321;&#39064;&#32771;&#35797;&#33021;&#21147;&#30340;&#32972;&#26223;&#19979;&#65292;&#35843;&#26597;&#20102;LLM&#30340;&#21307;&#23398;&#30693;&#35782;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#24320;&#28304;&#30340;LLM&#65288;Koala 7B&#12289;Falcon 7B&#12289;Stable-Vicuna 13B&#21644;Orca Mini 13B&#65289;&#19982;GPT-4&#21644;Claude 2&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been significant breakthroughs in the field of natural language processing, particularly with the development of large language models (LLMs). These LLMs have showcased remarkable capabilities on various benchmarks. In the healthcare field, the exact role LLMs and other future AI models will play remains unclear. There is a potential for these models in the future to be used as part of adaptive physician training, medical co-pilot applications, and digital patient interaction scenarios. The ability of AI models to participate in medical training and patient care will depend in part on their mastery of the knowledge content of specific medical fields. This study investigated the medical knowledge capability of LLMs, specifically in the context of internal medicine subspecialty multiple-choice test-taking ability. We compared the performance of several open-source LLMs (Koala 7B, Falcon 7B, Stable-Vicuna 13B, and Orca Mini 13B), to GPT-4 and Claude 2 on multip
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#38754;&#21521;&#26032;&#38395;&#30340;&#22635;&#23383;&#28216;&#25103;&#12290;&#36890;&#36807;&#28155;&#21152;&#23613;&#21487;&#33021;&#22810;&#30340;&#26032;&#38395;&#34893;&#29983;&#35789;&#27719;&#65292;&#21487;&#20197;&#22686;&#24378;&#25945;&#32946;&#30446;&#30340;&#21644;&#20154;&#20204;&#23545;&#26032;&#38395;&#30340;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2308.04688</link><description>&lt;p&gt;
&#29983;&#25104;&#38754;&#21521;&#26032;&#38395;&#30340;&#22635;&#23383;&#28216;&#25103;&#20316;&#20026;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Generating News-Centric Crossword Puzzles As A Constraint Satisfaction and Optimization Problem. (arXiv:2308.04688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#38754;&#21521;&#26032;&#38395;&#30340;&#22635;&#23383;&#28216;&#25103;&#12290;&#36890;&#36807;&#28155;&#21152;&#23613;&#21487;&#33021;&#22810;&#30340;&#26032;&#38395;&#34893;&#29983;&#35789;&#27719;&#65292;&#21487;&#20197;&#22686;&#24378;&#25945;&#32946;&#30446;&#30340;&#21644;&#20154;&#20204;&#23545;&#26032;&#38395;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22635;&#23383;&#28216;&#25103;&#19981;&#20165;&#20165;&#20316;&#20026;&#23089;&#20048;&#27963;&#21160;&#65292;&#36824;&#26159;&#19968;&#31181;&#21487;&#20197;&#29992;&#26469;&#33719;&#21462;&#35789;&#27719;&#21644;&#35821;&#35328;&#33021;&#21147;&#30340;&#25945;&#32946;&#24037;&#20855;&#12290;&#22686;&#24378;&#25945;&#32946;&#30446;&#30340;&#30340;&#19968;&#31181;&#31574;&#30053;&#26159;&#20010;&#24615;&#21270;&#65292;&#27604;&#22914;&#28155;&#21152;&#26356;&#22810;&#20851;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#21333;&#35789;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#40723;&#21169;&#20154;&#20204;&#23545;&#26032;&#38395;&#24863;&#20852;&#36259;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#38754;&#21521;&#26032;&#38395;&#30340;&#22635;&#23383;&#28216;&#25103;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#21487;&#33021;&#30340;&#22330;&#26223;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21407;&#22411;&#20316;&#20026;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#21253;&#21547;&#23613;&#21487;&#33021;&#22810;&#30340;&#26032;&#38395;&#34893;&#29983;&#35789;&#27719;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25253;&#21578;&#20102;&#22312;&#20960;&#20010;&#26465;&#20214;&#19979;&#25152;&#38656;&#30340;&#29983;&#25104;&#27010;&#29575;&#21644;&#26102;&#38388;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#25968;&#26032;&#38395;&#34893;&#29983;&#35789;&#27719;&#65292;&#20063;&#21487;&#20197;&#29983;&#25104;&#38754;&#21521;&#26032;&#38395;&#30340;&#22635;&#23383;&#28216;&#25103;&#12290;&#36890;&#36807;&#23545;&#21407;&#22411;&#30340;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#20851;&#20110;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#30340;&#25552;&#26696;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crossword puzzles have traditionally served not only as entertainment but also as an educational tool that can be used to acquire vocabulary and language proficiency. One strategy to enhance the educational purpose is personalization, such as including more words on a particular topic. This paper focuses on the case of encouraging people's interest in news and proposes a framework for automatically generating news-centric crossword puzzles. We designed possible scenarios and built a prototype as a constraint satisfaction and optimization problem, that is, containing as many news-derived words as possible. Our experiments reported the generation probabilities and time required under several conditions. The results showed that news-centric crossword puzzles can be generated even with few news-derived words. We summarize the current issues and future research directions through a qualitative evaluation of the prototype. This is the first proposal that a formulation of a constraint satisfa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;Sci-CoT&#26694;&#26550;&#65292;&#20998;&#31163;&#20102;&#29983;&#25104;&#29702;&#30001;&#21644;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.04679</link><description>&lt;p&gt;
Sci-CoT: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#23567;&#22411;&#31185;&#23398;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA. (arXiv:2308.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;Sci-CoT&#26694;&#26550;&#65292;&#20998;&#31163;&#20102;&#29983;&#25104;&#29702;&#30001;&#21644;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#24191;&#27867;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#33021;&#21147;&#24402;&#21151;&#20110;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#35268;&#27169;&#21644;&#23545;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;LLMs&#23637;&#29616;&#20986;&#22686;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#23545;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#36825;&#24402;&#21151;&#20110;&#19968;&#31181;&#21517;&#20026;"&#24605;&#32500;&#38142; (CoT)&#25552;&#31034;"&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#24341;&#23548;&#26368;&#32456;&#31572;&#26696;&#25512;&#29702;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#24378;&#35843;&#30340;&#26159;&#65292;&#36825;&#20123;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#20284;&#20046;&#21482;&#22312;&#20855;&#26377;&#33267;&#23569;100&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23558;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sci-CoT&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#20998;&#31163;&#20102;&#29983;&#25104;&#29702;&#30001;&#21644;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown outstanding performance across wide range of downstream tasks. This competency is attributed to their substantial parameter size and pre-training on extensive corpus. Moreover, LLMs have exhibited enhanced reasoning capabilities in tackling complex reasoning tasks, owing to the utilization of a method named ``Chain-of-Thought (CoT) prompting''. This method is designed to generate intermediate reasoning steps that guide the inference of the final answer. However, it is essential to highlight that these advanced reasoning abilities appear to emerge in models with a minimum of 10 billion parameters, thereby limiting its efficacy in situations where computational resources are constrained. In this paper, we investigate the possibility of transferring the reasoning capabilities of LLMs to smaller models via knowledge distillation. Specifically, we propose Sci-CoT, a two-stage framework that separates the processes of generating rationales and inferrin
&lt;/p&gt;</description></item><item><title>Sudowoodo&#26159;&#19968;&#20010;&#20013;&#25991;&#27468;&#35789;&#27169;&#20223;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#24179;&#34892;&#35821;&#26009;&#24211;&#21644;&#21033;&#29992;&#21518;&#22788;&#29702;&#27169;&#22359;&#26469;&#29983;&#25104;&#26032;&#30340;&#39640;&#36136;&#37327;&#27468;&#35789;&#65292;&#23454;&#29616;&#20102;&#27169;&#20223;&#28304;&#27468;&#35789;&#30340;&#39118;&#26684;&#21644;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.04665</link><description>&lt;p&gt;
Sudowoodo: &#19968;&#31181;&#24102;&#26377;&#28304;&#27468;&#35789;&#30340;&#20013;&#25991;&#25234;&#24773;&#27169;&#20223;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sudowoodo: a Chinese Lyric Imitation System with Source Lyrics. (arXiv:2308.04665v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04665
&lt;/p&gt;
&lt;p&gt;
Sudowoodo&#26159;&#19968;&#20010;&#20013;&#25991;&#27468;&#35789;&#27169;&#20223;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#24179;&#34892;&#35821;&#26009;&#24211;&#21644;&#21033;&#29992;&#21518;&#22788;&#29702;&#27169;&#22359;&#26469;&#29983;&#25104;&#26032;&#30340;&#39640;&#36136;&#37327;&#27468;&#35789;&#65292;&#23454;&#29616;&#20102;&#27169;&#20223;&#28304;&#27468;&#35789;&#30340;&#39118;&#26684;&#21644;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#35789;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#20013;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#24212;&#29992;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20851;&#38190;&#35789;&#12289;&#25276;&#38901;&#31561;&#31934;&#30830;&#25511;&#21046;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#27468;&#35789;&#12290;&#28982;&#32780;&#65292;&#27468;&#35789;&#27169;&#20223;&#65292;&#21363;&#36890;&#36807;&#27169;&#20223;&#28304;&#27468;&#35789;&#30340;&#39118;&#26684;&#21644;&#20869;&#23481;&#26469;&#21019;&#20316;&#26032;&#27468;&#35789;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#32570;&#20047;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;"Sudowoodo"&#30340;&#20013;&#25991;&#27468;&#35789;&#27169;&#20223;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#28304;&#27468;&#35789;&#30340;&#25991;&#26412;&#29983;&#25104;&#26032;&#30340;&#27468;&#35789;&#12290;&#20026;&#20102;&#35299;&#20915;&#27468;&#35789;&#27169;&#20223;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#20047;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#28304;&#27468;&#35789;&#30340;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#27468;&#35789;&#27169;&#22411;&#26500;&#24314;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#28982;&#21518;&#20351;&#29992;&#26032;&#27468;&#35789;&#19982;&#28304;&#27468;&#35789;&#30340;&#23545;&#26469;&#35757;&#32451;&#27468;&#35789;&#27169;&#20223;&#27169;&#22411;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21518;&#22788;&#29702;&#27169;&#22359;&#26469;&#36807;&#28388;&#21644;&#25490;&#24207;&#29983;&#25104;&#30340;&#27468;&#35789;&#65292;&#36873;&#25321;&#26368;&#39640;&#36136;&#37327;&#30340;&#27468;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lyrics generation is a well-known application in natural language generation research, with several previous studies focusing on generating accurate lyrics using precise control such as keywords, rhymes, etc. However, lyrics imitation, which involves writing new lyrics by imitating the style and content of the source lyrics, remains a challenging task due to the lack of a parallel corpus. In this paper, we introduce \textbf{\textit{Sudowoodo}}, a Chinese lyrics imitation system that can generate new lyrics based on the text of source lyrics. To address the issue of lacking a parallel training corpus for lyrics imitation, we propose a novel framework to construct a parallel corpus based on a keyword-based lyrics model from source lyrics. Then the pairs \textit{(new lyrics, source lyrics)} are used to train the lyrics imitation model. During the inference process, we utilize a post-processing module to filter and rank the generated lyrics, selecting the highest-quality ones. We incorpora
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20013;&#21476;&#39640;&#22320;&#24503;&#35821;&#21644;&#29616;&#20195;&#24503;&#35821;&#30340;&#35821;&#35328;&#36830;&#32493;&#24615;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#29616;&#26377;&#30340;&#29616;&#20195;&#24503;&#35821;&#26641;&#24211;&#36164;&#28304;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20013;&#21476;&#39640;&#22320;&#24503;&#35821;&#30340;&#30701;&#35821;&#32467;&#26500;&#20998;&#26512;&#22120;&#65292;&#26080;&#38656;&#20381;&#36182;&#26631;&#27880;&#30340;MHG&#26641;&#24211;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.04645</link><description>&lt;p&gt;
&#20013;&#21476;&#39640;&#22320;&#24503;&#35821;&#30340;&#36328;&#35821;&#35328;&#30701;&#35821;&#32467;&#26500;&#20998;&#26512;&#65306;&#19968;&#31181;&#21435;&#35789;&#27861;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Constituency Parsing for Middle High German: A Delexicalized Approach. (arXiv:2308.04645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20013;&#21476;&#39640;&#22320;&#24503;&#35821;&#21644;&#29616;&#20195;&#24503;&#35821;&#30340;&#35821;&#35328;&#36830;&#32493;&#24615;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#29616;&#26377;&#30340;&#29616;&#20195;&#24503;&#35821;&#26641;&#24211;&#36164;&#28304;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20013;&#21476;&#39640;&#22320;&#24503;&#35821;&#30340;&#30701;&#35821;&#32467;&#26500;&#20998;&#26512;&#22120;&#65292;&#26080;&#38656;&#20381;&#36182;&#26631;&#27880;&#30340;MHG&#26641;&#24211;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35821;&#32467;&#26500;&#20998;&#26512;&#22312;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;&#26631;&#27880;&#30340;&#35299;&#26512;&#25968;&#25454;&#35757;&#32451;&#21476;&#20195;&#35821;&#35328;&#30340;&#33258;&#21160;&#21477;&#27861;&#20998;&#26512;&#31995;&#32479;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#26500;&#24314;&#36825;&#20123;&#35821;&#35328;&#30340;&#26641;&#24211;&#23384;&#22312;&#22266;&#26377;&#25361;&#25112;&#12290;&#36825;&#38656;&#35201;&#20016;&#23500;&#30340;&#35821;&#35328;&#19987;&#19994;&#30693;&#35782;&#65292;&#23548;&#33268;&#21487;&#29992;&#36164;&#28304;&#30340;&#31232;&#32570;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38556;&#30861;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#25216;&#26415;&#20026;&#20302;&#36164;&#28304;&#30446;&#26631;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#25216;&#26415;&#38656;&#35201;&#26368;&#23569;&#29978;&#33267;&#27809;&#26377;&#26631;&#27880;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#26500;&#24314;&#36866;&#29992;&#20110;&#20013;&#21476;&#39640;&#22320;&#24503;&#35821;&#65288;MHG&#65289;&#30340;&#30701;&#35821;&#32467;&#26500;&#20998;&#26512;&#22120;&#65292;&#22312;&#32570;&#20047;&#26631;&#27880;&#30340;MHG&#26641;&#24211;&#36827;&#34892;&#35757;&#32451;&#30340;&#29616;&#23454;&#26465;&#20214;&#19979;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;MHG&#21644;&#29616;&#20195;&#24503;&#35821;&#65288;MG&#65289;&#20043;&#38388;&#30340;&#35821;&#35328;&#36830;&#32493;&#24615;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#20016;&#23500;&#30340;MG&#26641;&#24211;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constituency parsing plays a fundamental role in advancing natural language processing (NLP) tasks. However, training an automatic syntactic analysis system for ancient languages solely relying on annotated parse data is a formidable task due to the inherent challenges in building treebanks for such languages. It demands extensive linguistic expertise, leading to a scarcity of available resources. To overcome this hurdle, cross-lingual transfer techniques which require minimal or even no annotated data for low-resource target languages offer a promising solution. In this study, we focus on building a constituency parser for $\mathbf{M}$iddle $\mathbf{H}$igh $\mathbf{G}$erman $\mathbf{MHG}$ under realistic conditions, where no annotated MHG treebank is available for training. In our approach, we leverage the linguistic continuity and structural similarity between MHG and $\mathbf{M}$odern $\mathbf{G}$erman $\mathbf{MG}$, along with the abundance of MG treebank resources. Specifically, b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#36817;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#22312;&#20998;&#26512;&#35821;&#20041;&#21464;&#21270;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#25991;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.04625</link><description>&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#22312;&#35780;&#20272;&#35821;&#20041;&#21464;&#21270;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Sentence Embedding Models for Assessing Semantic Variation. (arXiv:2308.04625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#36817;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#22312;&#20998;&#26512;&#35821;&#20041;&#21464;&#21270;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#25991;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#38271;&#31687;&#30495;&#23454;&#19990;&#30028;&#25991;&#26412;&#65288;&#22914;&#20070;&#31821;&#25110;&#35760;&#24405;&#65289;&#20013;&#35821;&#20041;&#21464;&#21270;&#30340;&#27169;&#24335;&#22312;&#25991;&#20307;&#12289;&#35748;&#30693;&#21644;&#35821;&#35328;&#23398;&#30340;&#35282;&#24230;&#19978;&#24456;&#26377;&#36259;&#12290;&#23427;&#20063;&#26377;&#21161;&#20110;&#35832;&#22914;&#25991;&#26412;&#20998;&#27573;&#12289;&#25991;&#26723;&#25688;&#35201;&#21644;&#35821;&#20041;&#26032;&#39062;&#24615;&#26816;&#27979;&#31561;&#24212;&#29992;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#20960;&#31181;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#20351;&#24471;&#36825;&#31181;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#19981;&#21516;&#26041;&#27861;&#20135;&#29983;&#30340;&#35821;&#20041;&#34920;&#31034;&#22312;&#33258;&#36523;&#19978;&#26159;&#21542;&#19968;&#33268;&#21644;&#26377;&#24847;&#20041;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#21477;&#23376;&#35821;&#20041;&#30456;&#20284;&#24230;&#21644;&#22810;&#26412;&#25991;&#23398;&#20316;&#21697;&#30340;&#21477;&#23376;&#23545;&#35821;&#20041;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#27604;&#36739;&#20960;&#31181;&#26368;&#36817;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#12290;&#19982;&#20197;&#21069;&#20351;&#29992;&#30446;&#26631;&#20219;&#21153;&#21644;&#31574;&#21010;&#25968;&#25454;&#38598;&#26469;&#27604;&#36739;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;"&#37326;&#22806;"&#25552;&#20379;&#20102;&#23545;&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#32771;&#34385;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#30830;&#23454;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing the pattern of semantic variation in long real-world texts such as books or transcripts is interesting from the stylistic, cognitive, and linguistic perspectives. It is also useful for applications such as text segmentation, document summarization, and detection of semantic novelty. The recent emergence of several vector-space methods for sentence embedding has made such analysis feasible. However, this raises the issue of how consistent and meaningful the semantic representations produced by various methods are in themselves. In this paper, we compare several recent sentence embedding methods via time-series of semantic similarity between successive sentences and matrices of pairwise sentence similarity for multiple books of literature. In contrast to previous work using target tasks and curated datasets to compare sentence embedding methods, our approach provides an evaluation of the methods 'in the wild'. We find that most of the sentence embedding methods considered do in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;E2E&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#30001;LLM&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#25351;&#26631;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04624</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#25216;&#26415;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;&#26041;&#27861;&#21644;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Benchmarking LLM powered Chatbots: Methods and Metrics. (arXiv:2308.04624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;E2E&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#30001;LLM&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#25351;&#26631;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#23545;&#35805;&#20195;&#29702;&#65292;&#21363;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#27491;&#25104;&#20026;&#20225;&#19994;&#20026;&#23458;&#25143;&#21644;&#21512;&#20316;&#20249;&#20276;&#25552;&#20379;&#25903;&#25345;&#30340;&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#26426;&#21046;&#12290;&#20026;&#20102;&#35780;&#20272;&#29305;&#21035;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#38656;&#35201;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#36825;&#23601;&#26159;&#32842;&#22825;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#25152;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;E2E&#65288;&#31471;&#21040;&#31471;&#65289;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;E2E&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#30001;LLM&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;E2E&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#20854;&#20182;&#24120;&#29992;&#30340;&#29616;&#26377;&#25351;&#26631;&#35780;&#20272;&#20102;&#19968;&#20010;&#31034;&#20363;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#19981;&#21516;&#22797;&#26434;&#31243;&#24230;&#65292;&#24182;&#35266;&#23519;&#21040;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#20854;&#20182;&#25351;&#26631;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#25351;&#26631;&#34987;&#35777;&#26126;&#26159;&#19981;&#21487;&#39044;&#27979;&#30340;&#65292;&#32780;&#19982;E2E&#22522;&#20934;&#27979;&#35797;&#30456;&#20851;&#30340;&#25351;&#26631;&#20351;&#29992;&#20102;&#20313;&#24358;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous conversational agents, i.e. chatbots, are becoming an increasingly common mechanism for enterprises to provide support to customers and partners. In order to rate chatbots, especially ones powered by Generative AI tools like Large Language Models (LLMs) we need to be able to accurately assess their performance. This is where chatbot benchmarking becomes important. In this paper, we propose the use of a novel benchmark that we call the E2E (End to End) benchmark, and show how the E2E benchmark can be used to evaluate accuracy and usefulness of the answers provided by chatbots, especially ones powered by LLMs. We evaluate an example chatbot at different levels of sophistication based on both our E2E benchmark, as well as other available metrics commonly used in the state of art, and observe that the proposed benchmark show better results compared to others. In addition, while some metrics proved to be unpredictable, the metric associated with the E2E benchmark, which uses cosi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20998;&#38454;&#27573;&#25512;&#27979;&#35299;&#30721;&#65292;&#29992;&#20110;&#21152;&#36895;&#22312;&#23567;&#25209;&#37327;&#12289;&#35774;&#22791;&#19978;&#36827;&#34892;LLM&#25512;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#26641;&#24418;&#32467;&#26500;&#30340;&#25209;&#27425;&#37325;&#32452;&#21644;&#22686;&#21152;&#31532;&#20108;&#38454;&#27573;&#30340;&#25512;&#27979;&#35299;&#30721;&#65292;&#23558;&#21333;&#25209;&#35299;&#30721;&#24310;&#36831;&#38477;&#20302;&#20102;3.16&#20493;&#65292;&#32780;&#36755;&#20986;&#36136;&#37327;&#20445;&#25345;&#23436;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.04623</link><description>&lt;p&gt;
&#37319;&#29992;&#20998;&#38454;&#27573;&#25512;&#27979;&#35299;&#30721;&#21152;&#36895;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Accelerating LLM Inference with Staged Speculative Decoding. (arXiv:2308.04623v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20998;&#38454;&#27573;&#25512;&#27979;&#35299;&#30721;&#65292;&#29992;&#20110;&#21152;&#36895;&#22312;&#23567;&#25209;&#37327;&#12289;&#35774;&#22791;&#19978;&#36827;&#34892;LLM&#25512;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#26641;&#24418;&#32467;&#26500;&#30340;&#25209;&#27425;&#37325;&#32452;&#21644;&#22686;&#21152;&#31532;&#20108;&#38454;&#27573;&#30340;&#25512;&#27979;&#35299;&#30721;&#65292;&#23558;&#21333;&#25209;&#35299;&#30721;&#24310;&#36831;&#38477;&#20302;&#20102;3.16&#20493;&#65292;&#32780;&#36755;&#20986;&#36136;&#37327;&#20445;&#25345;&#23436;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;LLM&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#22810;&#26679;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21363;&#20998;&#38454;&#27573;&#25512;&#27979;&#35299;&#30721;&#65292;&#26469;&#21152;&#36895;&#22312;&#23567;&#25209;&#37327;&#12289;&#35774;&#22791;&#19978;&#36827;&#34892;LLM&#25512;&#29702;&#12290;&#36890;&#36807;&#25913;&#36827;&#20808;&#21069;&#30340;&#25512;&#27979;&#35299;&#30721;&#24037;&#20316;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23567;&#25209;&#37327;&#25512;&#29702;&#30340;&#20302;&#31639;&#26415;&#24378;&#24230;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25512;&#27979;&#25209;&#27425;&#37325;&#26032;&#32452;&#32455;&#25104;&#26641;&#24418;&#32467;&#26500;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#29983;&#25104;&#25104;&#26412;&#65292;&#24182;&#22686;&#21152;&#20102;&#27599;&#25209;&#39044;&#26399;&#30340;&#26631;&#35760;&#25968;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#31532;&#20108;&#38454;&#27573;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;&#32508;&#21512;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20445;&#25345;&#36755;&#20986;&#36136;&#37327;&#23436;&#32654;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#21333;&#25209;&#35299;&#30721;&#24310;&#36831;&#38477;&#20302;&#20102;3.16&#20493;&#65292;&#20351;&#29992;&#20102;762M&#21442;&#25968;&#30340;GPT-2-L&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances with large language models (LLM) illustrate their diverse capabilities. We propose a novel algorithm, staged speculative decoding, to accelerate LLM inference in small-batch, on-device scenarios. We address the low arithmetic intensity of small-batch inference by improving upon previous work in speculative decoding. First, we restructure the speculative batch as a tree, which reduces generation costs and increases the expected tokens per batch. Second, we add a second stage of speculative decoding. Taken together, we reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L model while perfectly preserving output quality.
&lt;/p&gt;</description></item><item><title>Shepherd&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#35780;&#35770;&#21644;&#25552;&#20986;&#25913;&#36827;&#24314;&#35758;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#21644;&#20462;&#22797;&#19981;&#21516;&#30340;&#38169;&#35823;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;Shepherd&#30340;&#24615;&#33021;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2308.04592</link><description>&lt;p&gt;
"Shepherd: &#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#35780;&#35770;&#32773;"
&lt;/p&gt;
&lt;p&gt;
Shepherd: A Critic for Language Model Generation. (arXiv:2308.04592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04592
&lt;/p&gt;
&lt;p&gt;
Shepherd&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#35780;&#35770;&#21644;&#25552;&#20986;&#25913;&#36827;&#24314;&#35758;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#21644;&#20462;&#22797;&#19981;&#21516;&#30340;&#38169;&#35823;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;Shepherd&#30340;&#24615;&#33021;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#25216;&#26415;&#24320;&#22987;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#20248;&#21270;&#20854;&#36755;&#20986;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Shepherd&#65292;&#19968;&#31181;&#29305;&#23450;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35780;&#35770;&#22238;&#22797;&#24182;&#25552;&#20986;&#25913;&#36827;&#24314;&#35758;&#65292;&#36229;&#36234;&#20102;&#26410;&#35843;&#25972;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#35782;&#21035;&#19981;&#21516;&#30340;&#38169;&#35823;&#24182;&#25552;&#20379;&#24314;&#35758;&#26469;&#20462;&#22797;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;&#31038;&#21306;&#21453;&#39304;&#21644;&#20154;&#24037;&#27880;&#37322;&#20013;&#31574;&#21010;&#25972;&#29702;&#32780;&#25104;&#12290;&#23613;&#31649;Shepherd&#35268;&#27169;&#36739;&#23567;&#65288;7B&#20010;&#21442;&#25968;&#65289;&#65292;&#20294;&#20854;&#35780;&#35770;&#35201;&#20040;&#19982;ChatGPT&#31561;&#24050;&#24314;&#31435;&#30340;&#27169;&#22411;&#31561;&#25928;&#65292;&#35201;&#20040;&#26356;&#20248;&#12290;&#36890;&#36807;&#20351;&#29992;GPT-4&#36827;&#34892;&#35780;&#20272;&#65292;Shepherd&#30456;&#23545;&#20110;&#31454;&#20105;&#23545;&#25163;&#24179;&#22343;&#20855;&#26377;53-87%&#30340;&#32988;&#29575;&#12290;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;Shepherd&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#19988;&#24179;&#22343;&#19982;ChatGPT&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models improve, there is increasing interest in techniques that leverage these models' capabilities to refine their own outputs. In this work, we introduce Shepherd, a language model specifically tuned to critique responses and suggest refinements, extending beyond the capabilities of an untuned model to identify diverse errors and provide suggestions to remedy them. At the core of our approach is a high quality feedback dataset, which we curate from community feedback and human annotations. Even though Shepherd is small (7B parameters), its critiques are either equivalent or preferred to those from established models including ChatGPT. Using GPT-4 for evaluation, Shepherd reaches an average win-rate of 53-87% compared to competitive alternatives. In human evaluation, Shepherd strictly outperforms other models and on average closely ties with ChatGPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#21477;&#38405;&#35835;&#22120;&#19982;&#20256;&#32479;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20960;&#20046;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04566</link><description>&lt;p&gt;
&#21333;&#21477;&#38405;&#35835;&#22120;&#65306;&#35299;&#20915;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Single-Sentence Reader: A Novel Approach for Addressing Answer Position Bias. (arXiv:2308.04566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#21477;&#38405;&#35835;&#22120;&#19982;&#20256;&#32479;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20960;&#20046;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#27169;&#22411;&#24448;&#24448;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#65288;&#20063;&#31216;&#20026;&#25968;&#25454;&#38598;&#20559;&#24046;&#25110;&#30740;&#31350;&#30028;&#30340;&#26631;&#27880;&#24037;&#20214;&#65289;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#22312;&#19981;&#23436;&#20840;&#29702;&#35299;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;MRC&#20219;&#21153;&#65292;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23548;&#33268;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#20302;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#35757;&#32451;&#38382;&#39064;&#20013;&#26377;&#30456;&#24403;&#27604;&#20363;&#30340;&#31572;&#26696;&#20165;&#20301;&#20110;&#19978;&#19979;&#25991;&#30340;&#31532;&#19968;&#21477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;MRC&#20013;&#30340;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#32467;&#26524;&#20960;&#20046;&#19982;&#20256;&#32479;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#21333;&#21477;&#38405;&#35835;&#22120;&#36935;&#21040;&#30340;&#20960;&#20010;&#25361;&#25112;&#21644;&#25552;&#20986;&#30340;&#24212;&#23545;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Reading Comprehension (MRC) models tend to take advantage of spurious correlations (also known as dataset bias or annotation artifacts in the research community). Consequently, these models may perform the MRC task without fully comprehending the given context and question, which is undesirable since it may result in low robustness against distribution shift. This paper delves into the concept of answer-position bias, where a significant percentage of training questions have answers located solely in the first sentence of the context. We propose a Single-Sentence Reader as a new approach for addressing answer position bias in MRC. We implement this approach using six different models and thoroughly analyze their performance. Remarkably, our proposed Single-Sentence Readers achieve results that nearly match those of models trained on conventional training sets, proving their effectiveness. Our study also discusses several challenges our Single-Sentence Readers encounter and prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;ACM KDF-SIGIR 2023&#31454;&#36187;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23454;&#20307;&#20171;&#35789;&#36827;&#34892;&#36130;&#21153;&#20851;&#31995;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#22312;&#27604;&#36187;&#20013;&#36194;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.04534</link><description>&lt;p&gt;
&#25552;&#21069;&#25991;&#26412;&#65306;&#21033;&#29992;&#23454;&#20307;&#20171;&#35789;&#36827;&#34892;&#36130;&#21153;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Ahead of the Text: Leveraging Entity Preposition for Financial Relation Extraction. (arXiv:2308.04534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;ACM KDF-SIGIR 2023&#31454;&#36187;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23454;&#20307;&#20171;&#35789;&#36827;&#34892;&#36130;&#21153;&#20851;&#31995;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#22312;&#27604;&#36187;&#20013;&#36194;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;ACM KDF-SIGIR 2023&#31454;&#36187;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;REFind&#30340;&#36130;&#21153;&#23454;&#20307;&#20851;&#31995;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#19968;&#20010;&#23454;&#20307;&#20851;&#31995;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#19968;&#20010;&#22810;&#27493;&#39588;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25552;&#20379;&#30340;&#23454;&#20307;&#25554;&#20837;&#21040;&#25991;&#26412;&#20013;&#23545;&#24212;&#30340;&#20301;&#32622;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#24102;&#26631;&#31614;&#30340;&#35757;&#32451;&#38598;&#23545;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;roberta-large&#36827;&#34892;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#24494;&#35843;&#65292;&#20197;&#39044;&#27979;&#23454;&#20307;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#21518;&#22788;&#29702;&#38454;&#27573;&#65292;&#20197;&#35782;&#21035;&#21644;&#22788;&#29702;&#27169;&#22411;&#29983;&#25104;&#30340;&#19981;&#22826;&#21487;&#33021;&#30340;&#39044;&#27979;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#27604;&#36187;&#30340;&#20844;&#20849;&#25490;&#34892;&#27036;&#19978;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of the ACM KDF-SIGIR 2023 competition, we undertook an entity relation task on a dataset of financial entity relations called REFind. Our top-performing solution involved a multi-step approach. Initially, we inserted the provided entities at their corresponding locations within the text. Subsequently, we fine-tuned the transformer-based language model roberta-large for text classification by utilizing a labeled training set to predict the entity relations. Lastly, we implemented a post-processing phase to identify and handle improbable predictions generated by the model. As a result of our methodology, we achieved the 1st place ranking on the competition's public leaderboard.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21512;&#20316;&#23545;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#20986;&#29256;&#29289;&#25968;&#37327;&#26377;&#22686;&#38271;&#36235;&#21183;&#65292;&#24182;&#19988;&#36825;&#20123;&#20986;&#29256;&#29289;&#24448;&#24448;&#27604;&#20165;&#30001;&#23398;&#26415;&#30028;&#20135;&#29983;&#30340;&#20986;&#29256;&#29289;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04524</link><description>&lt;p&gt;
&#25105;&#24212;&#35813;&#21644;&#35841;&#21512;&#20316;? &#20851;&#20110;NLP&#23398;&#26415;&#30028;&#19982;&#24037;&#19994;&#30028;&#21512;&#20316;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP. (arXiv:2308.04524v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21512;&#20316;&#23545;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#20986;&#29256;&#29289;&#25968;&#37327;&#26377;&#22686;&#38271;&#36235;&#21183;&#65292;&#24182;&#19988;&#36825;&#20123;&#20986;&#29256;&#29289;&#24448;&#24448;&#27604;&#20165;&#30001;&#23398;&#26415;&#30028;&#20135;&#29983;&#30340;&#20986;&#29256;&#29289;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#35843;&#26597;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21512;&#20316;&#23545;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;NLP&#35770;&#25991;&#20013;&#25552;&#21462;&#26426;&#26500;&#21644;&#24341;&#29992;&#20449;&#24687;&#30340;&#27969;&#31243;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#19977;&#31867;&#65306;&#23398;&#26415;&#30028;&#12289;&#24037;&#19994;&#30028;&#21644;&#28151;&#21512;&#22411;&#65288;&#23398;&#26415;&#30028;&#19982;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#65292;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;-&#24037;&#19994;&#30028;&#21512;&#20316;&#20986;&#29256;&#29289;&#30340;&#25968;&#37327;&#21576;&#22686;&#38271;&#36235;&#21183;&#65292;&#24182;&#19988;&#36825;&#20123;&#31867;&#22411;&#30340;&#20986;&#29256;&#29289;&#19982;&#20165;&#22312;&#23398;&#26415;&#30028;&#20135;&#29983;&#30340;&#20986;&#29256;&#29289;&#30456;&#27604;&#65292;&#24448;&#24448;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of our research was to investigate the effects of collaboration between academia and industry on Natural Language Processing (NLP). To do this, we created a pipeline to extract affiliations and citations from NLP papers and divided them into three categories: academia, industry, and hybrid (collaborations between academia and industry). Our empirical analysis found that there is a trend towards an increase in industry and academia-industry collaboration publications and that these types of publications tend to have a higher impact compared to those produced solely within academia.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#32452;&#21512;&#20998;&#24067;&#24335;&#24847;&#20041;&#27169;&#22411;&#20013;&#35299;&#26512;Donkey&#21477;&#23376;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#22411;&#36923;&#36753;&#35821;&#27861;&#21644;&#20851;&#31995;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.04519</link><description>&lt;p&gt;
DisCoCat&#29992;&#20110;Donkey&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;
DisCoCat for Donkey Sentences. (arXiv:2308.04519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04519
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#32452;&#21512;&#20998;&#24067;&#24335;&#24847;&#20041;&#27169;&#22411;&#20013;&#35299;&#26512;Donkey&#21477;&#23376;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#22411;&#36923;&#36753;&#35821;&#27861;&#21644;&#20851;&#31995;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#32452;&#21512;&#20998;&#24067;&#24335;&#24847;&#20041;&#27169;&#22411;&#20013;&#35299;&#26512;Geach&#30340;Donkey&#21477;&#23376;&#12290;&#25105;&#20204;&#22312;&#20043;&#21069;&#20851;&#20110;DisCoCat&#65288;&#20998;&#24067;&#24335;&#32452;&#21512;&#33539;&#30068;&#65289;&#26694;&#26550;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#36827;&#34892;&#25193;&#23637;&#65292;&#21253;&#25324;&#23545;&#35805;&#35821;&#12289;&#38480;&#23450;&#35789;&#21644;&#20851;&#31995;&#20195;&#35789;&#30340;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#22411;&#36923;&#36753;&#35821;&#27861;&#26469;&#35299;&#26512;donkey&#21477;&#23376;&#65292;&#21516;&#26102;&#23450;&#20041;&#20102;&#20851;&#31995;&#21644;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how to parse Geach's Donkey sentences in a compositional distributional model of meaning. We build on previous work on the DisCoCat (Distributional Compositional Categorical) framework, including extensions that model discourse, determiners, and relative pronouns. We present a type-logical syntax for parsing donkey sentences, for which we define both relational and vector space semantics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32452;&#21512;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#19982;HuBERT transformer&#30456;&#32467;&#21512;&#65292;&#20197;&#25429;&#25417;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#20809;&#35889;&#21644;&#38271;&#26399;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;GCN&#21033;&#29992;&#22270;&#34920;&#31034;&#25991;&#26412;&#25968;&#25454;&#65292;&#25429;&#25417;&#38271;&#26399;&#19978;&#19979;&#25991;&#20381;&#36182;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#32780;HuBERT&#21033;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#21644;&#35821;&#38899;&#20013;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#32467;&#21512;&#36825;&#20004;&#20010;&#26041;&#27861;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.04517</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25429;&#25417;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#20809;&#35889;&#21644;&#38271;&#26399;&#19978;&#19979;&#25991;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Capturing Spectral and Long-term Contextual Information for Speech Emotion Recognition Using Deep Learning Techniques. (arXiv:2308.04517v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32452;&#21512;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#19982;HuBERT transformer&#30456;&#32467;&#21512;&#65292;&#20197;&#25429;&#25417;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#20809;&#35889;&#21644;&#38271;&#26399;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;GCN&#21033;&#29992;&#22270;&#34920;&#31034;&#25991;&#26412;&#25968;&#25454;&#65292;&#25429;&#25417;&#38271;&#26399;&#19978;&#19979;&#25991;&#20381;&#36182;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#32780;HuBERT&#21033;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#21644;&#35821;&#38899;&#20013;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#32467;&#21512;&#36825;&#20004;&#20010;&#26041;&#27861;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#65292;&#22914;LSTM&#12289;CNN&#12289;RNN&#12289;SVM&#21644;MLP&#65292;&#22312;&#25429;&#25417;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12289;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#24615;&#20197;&#21450;&#25429;&#25417;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#29992;&#20110;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#65292;HuBERT transformer&#29992;&#20110;&#20998;&#26512;&#38899;&#39057;&#20449;&#21495;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GCN&#22312;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#25991;&#26412;&#34920;&#31034;&#25429;&#25417;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#20381;&#36182;&#21644;&#20851;&#31995;&#20197;&#21450;&#26816;&#27979;&#35789;&#20043;&#38388;&#30340;&#35821;&#22659;&#24847;&#20041;&#21644;&#35821;&#20041;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;HuBERT&#21033;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#65292;&#33021;&#22815;&#23545;&#35821;&#38899;&#20013;&#30340;&#26102;&#38388;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#25429;&#25417;&#23545;&#24773;&#24863;&#35782;&#21035;&#20135;&#29983;&#36129;&#29486;&#30340;&#24494;&#22937;&#24046;&#24322;&#21644;&#21464;&#21270;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional approaches in speech emotion recognition, such as LSTM, CNN, RNN, SVM, and MLP, have limitations such as difficulty capturing long-term dependencies in sequential data, capturing the temporal dynamics, and struggling to capture complex patterns and relationships in multimodal data. This research addresses these shortcomings by proposing an ensemble model that combines Graph Convolutional Networks (GCN) for processing textual data and the HuBERT transformer for analyzing audio signals. We found that GCNs excel at capturing Long-term contextual dependencies and relationships within textual data by leveraging graph-based representations of text and thus detecting the contextual meaning and semantic relationships between words. On the other hand, HuBERT utilizes self-attention mechanisms to capture long-range dependencies, enabling the modeling of temporal dynamics present in speech and capturing subtle nuances and variations that contribute to emotion recognition. By combining
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#23545;&#35805;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#27169;&#24577;&#21644;&#19978;&#19979;&#25991;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#35299;&#32544;&#26426;&#21046;&#26469;&#21516;&#26102;&#24314;&#27169;&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#24615;&#21644;&#23545;&#35805;&#30340;&#35821;&#22659;&#21270;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04502</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#22810;&#27169;&#24773;&#24863;&#35782;&#21035;&#20013;&#37325;&#26032;&#23457;&#35270;&#27169;&#24577;&#21644;&#19978;&#19979;&#25991;&#30340;&#35299;&#32544;&#21644;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition. (arXiv:2308.04502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#23545;&#35805;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#27169;&#24577;&#21644;&#19978;&#19979;&#25991;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#35299;&#32544;&#26426;&#21046;&#26469;&#21516;&#26102;&#24314;&#27169;&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#24615;&#21644;&#23545;&#35805;&#30340;&#35821;&#22659;&#21270;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#22330;&#26223;&#19979;&#65292;&#20351;&#26426;&#22120;&#33021;&#22815;&#29702;&#35299;&#20154;&#31867;&#24773;&#24863;&#22312;&#22810;&#27169;&#24577;&#35821;&#22659;&#19979;&#30340;&#30740;&#31350;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#65292;&#36825;&#20010;&#20219;&#21153;&#34987;&#31216;&#20026;&#23545;&#35805;&#24335;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65288;MM-ERC&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;MM-ERC&#19968;&#30452;&#21463;&#21040;&#20851;&#27880;&#65292;&#35768;&#22810;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#23558;MM-ERC&#35270;&#20026;&#26631;&#20934;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#32544;&#21644;&#34701;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#26469;&#26368;&#22823;&#21270;&#29305;&#24449;&#30340;&#25928;&#29992;&#12290;&#28982;&#32780;&#22312;&#37325;&#26032;&#23457;&#35270;MM-ERC&#30340;&#29305;&#28857;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#29305;&#24449;&#35299;&#32544;&#21644;&#34701;&#21512;&#30340;&#27493;&#39588;&#20013;&#65292;&#26082;&#24212;&#35813;&#36866;&#24403;&#22320;&#24314;&#27169;&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#20063;&#24212;&#35813;&#24314;&#27169;&#23545;&#35805;&#30340;&#35821;&#22659;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20805;&#20998;&#32771;&#34385;&#19978;&#36848;&#35266;&#28857;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;&#19968;&#26041;&#38754;&#65292;&#22312;&#29305;&#24449;&#35299;&#32544;&#38454;&#27573;&#65292;&#25105;&#20204;&#26681;&#25454;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#23618;&#35299;&#32544;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been a hot research topic to enable machines to understand human emotions in multimodal contexts under dialogue scenarios, which is tasked with multimodal emotion analysis in conversation (MM-ERC). MM-ERC has received consistent attention in recent years, where a diverse range of methods has been proposed for securing better task performance. Most existing works treat MM-ERC as a standard multimodal classification problem and perform multimodal feature disentanglement and fusion for maximizing feature utility. Yet after revisiting the characteristic of MM-ERC, we argue that both the feature multimodality and conversational contextualization should be properly modeled simultaneously during the feature disentanglement and fusion steps. In this work, we target further pushing the task performance by taking full consideration of the above insights. On the one hand, during feature disentanglement, based on the contrastive learning technique, we devise a Dual-level Disentanglement Mec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#26680;&#25351;&#20195;&#35299;&#20915;&#26041;&#26696;&#24341;&#20837;&#21040;&#23545;&#35805;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#38598;DialogRE^C+&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#26680;&#25351;&#20195;&#30693;&#35782;&#21487;&#20197;&#22686;&#24378;&#21442;&#25968;&#20851;&#31995;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#25552;&#21319;DRE&#20219;&#21153;&#20013;&#36215;&#21040;&#31215;&#26497;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.04498</link><description>&lt;p&gt;
DialogRE^C+&#65306;DialogRE&#22312;&#23545;&#35805;&#20013;&#20851;&#31995;&#25277;&#21462;&#20013;&#26680;&#25351;&#20195;&#24110;&#21161;&#30340;&#25193;&#23637;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
DialogRE^C+: An Extension of DialogRE to Investigate How Much Coreference Helps Relation Extraction in Dialogs. (arXiv:2308.04498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#26680;&#25351;&#20195;&#35299;&#20915;&#26041;&#26696;&#24341;&#20837;&#21040;&#23545;&#35805;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#38598;DialogRE^C+&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#26680;&#25351;&#20195;&#30693;&#35782;&#21487;&#20197;&#22686;&#24378;&#21442;&#25968;&#20851;&#31995;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#25552;&#21319;DRE&#20219;&#21153;&#20013;&#36215;&#21040;&#31215;&#26497;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20851;&#31995;&#25277;&#21462;(DRE)&#26159;&#35782;&#21035;&#23545;&#35805;&#25991;&#26412;&#20013;&#21442;&#25968;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#20294;&#24120;&#35265;&#30340;&#38382;&#39064;&#26159;&#20154;&#31216;&#20195;&#35789;&#12289;&#23454;&#20307;&#21644;&#21457;&#35328;&#32773;&#30340;&#26680;&#25351;&#20195;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;DialogRE^C+&#65292;&#23558;&#26680;&#25351;&#20195;&#35299;&#20915;&#26041;&#26696;&#24341;&#20837;&#21040;DRE&#22330;&#26223;&#20013;&#12290;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#26680;&#25351;&#20195;&#30693;&#35782;&#65292;&#26399;&#26395;&#22686;&#24378;&#21442;&#25968;&#20851;&#31995;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;DialogRE^C+&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#29616;&#26377;&#30340;DialogRE&#25968;&#25454;&#25163;&#21160;&#27880;&#37322;&#20102;&#24635;&#20849;5,068&#20010;&#26680;&#25351;&#20195;&#38142;&#65292;&#28085;&#30422;&#20102;36,369&#20010;&#21442;&#25968;&#25552;&#21450;&#12290;&#20854;&#20013;&#65292;&#26126;&#30830;&#26631;&#35760;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26680;&#25351;&#20195;&#38142;&#31867;&#22411;&#65292;&#20998;&#21035;&#26159;&#21457;&#35328;&#32773;&#38142;&#12289;&#20010;&#20154;&#38142;&#12289;&#22320;&#28857;&#38142;&#21644;&#32452;&#32455;&#38142;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;4&#20010;&#22522;&#20110;&#22270;&#30340;DRE&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#26377;&#25928;&#30340;&#26680;&#25351;&#20195;&#34920;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;DRE&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#22522;&#20110;&#25105;&#20204;&#30340;&#27880;&#37322;&#35757;&#32451;&#20102;&#19968;&#20010;&#26680;&#25351;&#20195;&#35299;&#20915;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#33258;&#21160;&#25552;&#21462;&#30340;&#26680;&#25351;&#20195;&#23545;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue relation extraction (DRE) that identifies the relations between argument pairs in dialogue text, suffers much from the frequent occurrence of personal pronouns, or entity and speaker coreference. This work introduces a new benchmark dataset DialogRE^C+, introducing coreference resolution into the DRE scenario. With the aid of high-quality coreference knowledge, the reasoning of argument relations is expected to be enhanced. In DialogRE^C+ dataset, we manually annotate total 5,068 coreference chains over 36,369 argument mentions based on the existing DialogRE data, where four different coreference chain types namely speaker chain, person chain, location chain and organization chain are explicitly marked. We further develop 4 coreference-enhanced graph-based DRE models, which learn effective coreference representations for improving the DRE task. We also train a coreference resolution model based on our annotations and evaluate the effect of automatically extracted coreference c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#24503;&#22269;&#25512;&#29305;&#19978;COVID&#30123;&#24773;&#26399;&#38388;&#25919;&#31574;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#32454;&#31890;&#24230;&#25919;&#27835;&#20559;&#22909;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#25919;&#27835;&#35266;&#28857;&#22312;&#30123;&#24773;&#26399;&#38388;&#26377;&#25152;&#22686;&#21152;&#65292;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#19981;&#21516;&#25919;&#27835;&#31867;&#21035;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.04444</link><description>&lt;p&gt;
&#24503;&#22269;&#25512;&#29305;&#19978;COVID&#30123;&#24773;&#26399;&#38388;&#25919;&#31574;&#20559;&#22909;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Changes in Policy Preferences in German Tweets during the COVID Pandemic. (arXiv:2308.04444v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#24503;&#22269;&#25512;&#29305;&#19978;COVID&#30123;&#24773;&#26399;&#38388;&#25919;&#31574;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#32454;&#31890;&#24230;&#25919;&#27835;&#20559;&#22909;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#25919;&#27835;&#35266;&#28857;&#22312;&#30123;&#24773;&#26399;&#38388;&#26377;&#25152;&#22686;&#21152;&#65292;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#19981;&#21516;&#25919;&#27835;&#31867;&#21035;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#24050;&#25104;&#20026;&#20132;&#27969;&#25919;&#27835;&#35266;&#28857;&#30340;&#37325;&#35201;&#35770;&#22363;&#12290;&#38024;&#23545;COVID&#25514;&#26045;&#65292;&#20844;&#27665;&#30452;&#25509;&#22312;&#36825;&#20123;&#24179;&#21488;&#19978;&#34920;&#36798;&#20102;&#33258;&#24049;&#30340;&#25919;&#31574;&#20559;&#22909;&#12290;&#22312;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#20013;&#37327;&#21270;&#25919;&#27835;&#20559;&#22909;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#22823;&#37327;&#30340;&#20869;&#23481;&#38656;&#35201;&#21487;&#25193;&#23637;&#30340;&#33258;&#21160;&#25552;&#21462;&#25919;&#27835;&#20559;&#22909;--&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25919;&#27835;&#20559;&#22909;&#25552;&#21462;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#26377;&#32454;&#31890;&#24230;&#25919;&#27835;&#20559;&#22909;&#27880;&#37322;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#35757;&#32451;&#22312;&#36825;&#20010;&#25968;&#25454;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20174;2019&#24180;&#21040;2022&#24180;&#30340;&#24503;&#22269;Twitter&#35821;&#26009;&#24211;&#20013;&#30340;&#25919;&#31574;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;COVID&#30123;&#24773;&#65292;&#25919;&#27835;&#35266;&#28857;&#30340;&#34920;&#36798;&#22686;&#21152;&#20102;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#25104;&#29087;&#30340;&#25919;&#31574;&#20559;&#22909;&#20998;&#31867;&#27861;&#20998;&#26512;&#20102;&#32454;&#31890;&#24230;&#30340;&#25919;&#27835;&#35266;&#28857;&#65292;&#24182;&#31361;&#20986;&#20102;&#19981;&#21516;&#25919;&#27835;&#31867;&#21035;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online social media have become an important forum for exchanging political opinions. In response to COVID measures citizens expressed their policy preferences directly on these platforms. Quantifying political preferences in online social media remains challenging: The vast amount of content requires scalable automated extraction of political preferences -- however fine grained political preference extraction is difficult with current machine learning (ML) technology, due to the lack of data sets. Here we present a novel data set of tweets with fine grained political preference annotations. A text classification model trained on this data is used to extract policy preferences in a German Twitter corpus ranging from 2019 to 2022. Our results indicate that in response to the COVID pandemic, expression of political opinions increased. Using a well established taxonomy of policy preferences we analyse fine grained political views and highlight changes in distinct political categories. The
&lt;/p&gt;</description></item><item><title>OpinionConv&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#21644;&#20915;&#31574;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2308.04226</link><description>&lt;p&gt;
OpinionConv: &#36890;&#36807;&#22522;&#20110;&#30495;&#23454;&#20027;&#35266;&#20307;&#39564;&#30340;&#35266;&#28857;&#23454;&#29616;&#23545;&#35805;&#24335;&#20135;&#21697;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
OpinionConv: Conversational Product Search with Grounded Opinions. (arXiv:2308.04226v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04226
&lt;/p&gt;
&lt;p&gt;
OpinionConv&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#21644;&#20915;&#31574;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25628;&#32034;&#20135;&#21697;&#26102;&#65292;&#20182;&#20154;&#30340;&#35266;&#28857;&#22312;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23545;&#20135;&#21697;&#30340;&#20027;&#35266;&#20307;&#39564;&#21487;&#20197;&#26159;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#36825;&#22312;&#38144;&#21806;&#23545;&#35805;&#20013;&#20063;&#26159;&#22914;&#27492;&#65292;&#22312;&#36825;&#31181;&#23545;&#35805;&#20013;&#65292;&#23458;&#25143;&#21644;&#38144;&#21806;&#21161;&#25163;&#20132;&#25442;&#26377;&#20851;&#20135;&#21697;&#30340;&#20107;&#23454;&#21644;&#35266;&#28857;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#27492;&#31867;&#23545;&#35805;&#30340;AI&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30340;&#32463;&#39564;&#27809;&#26377;&#30495;&#23454;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#35780;&#35770;&#20316;&#20026;&#20135;&#21697;&#35266;&#28857;&#30340;&#20016;&#23500;&#26469;&#28304;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#30495;&#23454;&#20027;&#35266;&#21465;&#36848;&#25903;&#25345;&#23545;&#35805;&#24335;AI&#12290;&#36890;&#36807;OpinionConv&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#38144;&#21806;&#23545;&#35805;&#30340;&#23545;&#35805;&#24335;AI&#12290;&#20026;&#20102;&#39564;&#35777;&#29983;&#25104;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;&#35266;&#28857;&#34987;&#35748;&#20026;&#26159;&#30495;&#23454;&#30340;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21592;&#20063;&#30830;&#35748;&#20102;&#35266;&#28857;&#23545;&#20110;&#20915;&#31574;&#30340;&#20449;&#24687;&#22522;&#30784;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an AI for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of real-world experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives. With OpinionConv, we develop the first conversational AI for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#26469;&#22686;&#24378;&#21305;&#37197;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;WMT Metrics&#22522;&#20934;&#20013;&#65292;&#22810;&#21442;&#32771;F200spBLEU&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#21442;&#32771;&#26041;&#27861;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;7.2\%&#65292;&#36229;&#36807;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;BERTscore&#30340;3.9\%&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03131</link><description>&lt;p&gt;
&#36808;&#21521;&#22810;&#21442;&#32771;&#26102;&#20195; &#8212;&#8212; &#35299;&#20915;NLG&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#21644;&#21442;&#32771;&#22810;&#26679;&#24615;&#26377;&#38480;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation. (arXiv:2308.03131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#26469;&#22686;&#24378;&#21305;&#37197;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;WMT Metrics&#22522;&#20934;&#20013;&#65292;&#22810;&#21442;&#32771;F200spBLEU&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#21442;&#32771;&#26041;&#27861;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;7.2\%&#65292;&#36229;&#36807;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;BERTscore&#30340;3.9\%&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
N-gram&#21305;&#37197;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;BLEU&#21644;chrF&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#22522;&#20110;&#21305;&#37197;&#30340;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#36739;&#24369;&#30340;&#30456;&#20851;&#24615;&#65292;&#23588;&#20854;&#26159;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#26631;&#22914;BLEURT&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#27979;&#21305;&#37197;&#25351;&#26631;&#24615;&#33021;&#29942;&#39048;&#30340;&#21407;&#22240;&#21487;&#33021;&#26159;&#21442;&#32771;&#36164;&#26009;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;"&#22810;&#20010;&#21442;&#32771;"&#26469;&#22686;&#24378;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;WMT Metrics&#22522;&#20934;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22810;&#21442;&#32771;F200spBLEU&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#21442;&#32771;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;7.2\%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#36824;&#36229;&#36807;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;BERTscore&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;3.9\%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24471;&#21040;&#32531;&#35299;&#36890;&#36807;&#25105;&#20204;&#30340;&#22810;&#21442;&#32771;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely utilized across a range of natural language generation (NLG) tasks. However, recent studies have revealed a weak correlation between these matching-based metrics and human evaluations, especially when compared with neural-based metrics like BLEURT. In this paper, we conjecture that the performance bottleneck in matching-based metrics may be caused by the limited diversity of references. To address this issue, we propose to utilize \textit{multiple references} to enhance the consistency between these metrics and human evaluations. Within the WMT Metrics benchmarks, we observe that the multi-references F200spBLEU surpasses the conventional single-reference one by an accuracy improvement of 7.2\%. Remarkably, it also exceeds the neural-based BERTscore by an accuracy enhancement of 3.9\%. Moreover, we observe that the data leakage issue in large language models (LLMs) can be mitigated to a large extent by our multi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.02582</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#30340;&#27867;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#20174;&#35757;&#32451;&#38598;&#20013;&#25512;&#29702;&#20986;&#23569;&#37327;&#26679;&#26412;&#65292;&#20197;&#21512;&#25104;&#27599;&#20010;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#27979;&#35797;&#26597;&#35810;&#30340;&#36816;&#34892;&#26102;&#25552;&#31034;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#23436;&#20840;&#35206;&#30422;SQL&#23376;&#21477;&#12289;&#36816;&#31639;&#31526;&#21644;&#20989;&#25968;&#65292;&#24182;&#22312;&#20801;&#35768;&#30340;&#20196;&#29260;&#38271;&#24230;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#22823;&#39046;&#22495;&#35206;&#30422;&#12290;&#36825;&#26679;&#21487;&#20197;&#21512;&#25104;&#19968;&#20010;&#22266;&#23450;&#30340;&#36890;&#29992;&#25552;&#31034;&#65288;GP&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;NL&#27979;&#35797;&#26597;&#35810;&#20043;&#38388;&#20849;&#29992;&#30340;&#22810;&#26679;&#21270;&#26679;&#26412;&#38598;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#12290;&#25105;&#20204;&#36824;&#23558;GP&#33258;&#36866;&#24212;&#21040;&#30446;&#26631;&#25968;&#25454;&#24211;&#39046;&#22495;&#65288;DA-GP&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#27867;&#21270;&#65307;&#28982;&#21518;&#37319;&#29992;&#20998;&#35299;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#65288;LTMP-DA-GP&#65289;&#26469;&#22788;&#29702;&#36328;&#32452;&#21512;&#27867;&#21270;&#12290;LTMP-DA-GP&#30340;&#21512;&#25104;&#26159;&#31163;&#32447;&#20219;&#21153;&#65292;
&lt;/p&gt;
&lt;p&gt;
Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.12267</link><description>&lt;p&gt;
&#38754;&#21521;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#35770;&#25991;&#30340;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#33021;&#22815;&#22312;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#27969;&#30021;&#22238;&#31572;&#12290;&#23613;&#31649;&#25215;&#35748;&#25216;&#26415;&#36827;&#27493;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#25945;&#32946;&#32773;&#20063;&#25285;&#24515;&#23398;&#29983;&#21487;&#33021;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#24182;&#23558;&#20854;&#20551;&#20882;&#20026;&#33258;&#24049;&#30340;&#21407;&#21019;&#20316;&#21697;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;AI&#20869;&#23481;&#26816;&#27979;&#30740;&#31350;&#26159;&#22522;&#20110;&#36825;&#20123;&#25285;&#24551;&#36827;&#34892;&#30340;&#65292;&#20294;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;AI&#20869;&#23481;&#26816;&#27979;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#20551;&#35774;&#19968;&#20010;&#25991;&#26412;&#35201;&#20040;&#23436;&#20840;&#30001;&#20154;&#31867;&#32534;&#20889;&#65292;&#35201;&#20040;&#23436;&#20840;&#30001;AI&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#20869;&#23481;&#26816;&#27979;&#22312;&#19968;&#20010;&#23569;&#26377;&#25506;&#32034;&#20294;&#21364;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26816;&#27979;&#30340;&#25991;&#26412;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#28151;&#21512;&#25991;&#26412;&#65289;&#21327;&#20316;&#32534;&#20889;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#26816;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20174;&#32473;&#23450;&#30340;&#28151;&#21512;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#31867;&#32534;&#20889;&#20869;&#23481;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#36716;&#25442;&#28857;&#65288;&#36793;&#30028;&#26816;&#27979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
&lt;/p&gt;</description></item><item><title>Retentive Network&#65288;RetNet&#65289;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;RetNet&#20855;&#26377;&#35757;&#32451;&#24182;&#34892;&#21270;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.08621</link><description>&lt;p&gt;
Retentive Network: &#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Transformer&#30340;&#32487;&#20219;&#32773;
&lt;/p&gt;
&lt;p&gt;
Retentive Network: A Successor to Transformer for Large Language Models. (arXiv:2307.08621v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08621
&lt;/p&gt;
&lt;p&gt;
Retentive Network&#65288;RetNet&#65289;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;RetNet&#20855;&#26377;&#35757;&#32451;&#24182;&#34892;&#21270;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Retentive Network (RetNet)&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#20102;&#24490;&#29615;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#24314;&#27169;&#30340;&#20445;&#30041;&#26426;&#21046;&#65292;&#25903;&#25345;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;&#21363;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24182;&#34892;&#34920;&#31034;&#20801;&#35768;&#36827;&#34892;&#35757;&#32451;&#24182;&#34892;&#21270;&#12290;&#24490;&#29615;&#34920;&#31034;&#33021;&#22815;&#23454;&#29616;&#20302;&#25104;&#26412;&#30340;$O(1)$&#25512;&#29702;&#65292;&#20174;&#32780;&#25552;&#39640;&#35299;&#30721;&#21534;&#21520;&#37327;&#12289;&#24310;&#36831;&#21644;GPU&#20869;&#23384;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;&#20998;&#22359;&#24490;&#29615;&#34920;&#31034;&#20415;&#20110;&#20351;&#29992;&#32447;&#24615;&#22797;&#26434;&#24230;&#36827;&#34892;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#21487;&#20197;&#24182;&#34892;&#32534;&#30721;&#65292;&#21516;&#26102;&#36827;&#34892;&#24490;&#29615;&#25688;&#35201;&#12290;&#35821;&#35328;&#24314;&#27169;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RetNet&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25193;&#23637;&#32467;&#26524;&#12289;&#24182;&#34892;&#35757;&#32451;&#12289;&#20302;&#25104;&#26412;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;AutoHint&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#21644;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#29983;&#25104;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20248;&#21270;&#21407;&#22987;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.07415</link><description>&lt;p&gt;
AutoHint: &#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#19982;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoHint: Automatic Prompt Optimization with Hint Generation. (arXiv:2307.07415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AutoHint&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#21644;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#29983;&#25104;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#20248;&#21270;&#21407;&#22987;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AutoHint&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#21644;&#20248;&#21270;&#30340;&#26032;&#26694;&#26550;&#12290;&#34429;&#28982;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27880;&#37322;&#33021;&#21147;&#65292;&#20294;&#23558;&#27492;&#33021;&#21147;&#24212;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#20851;&#38190;&#22312;&#20110;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#27966;&#29983;&#30340;&#20016;&#23500;&#25351;&#23548;&#32435;&#20837;&#21407;&#22987;&#25552;&#31034;&#65292;&#20197;&#32487;&#25215;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#20016;&#23500;&#31216;&#20026;&#8220;&#25552;&#31034;&#8221;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20174;&#19968;&#20010;&#21021;&#22987;&#25552;&#31034;&#24320;&#22987;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#25351;&#23548;LLM&#20174;&#38169;&#35823;&#39044;&#27979;&#20013;&#25512;&#26029;&#20986;&#36873;&#23450;&#26679;&#26412;&#30340;&#26032;&#25552;&#31034;&#65292;&#28982;&#21518;&#20174;&#27599;&#20010;&#26679;&#26412;&#30340;&#25552;&#31034;&#20013;&#36827;&#34892;&#24635;&#32467;&#65292;&#24182;&#23558;&#32467;&#26524;&#28155;&#21152;&#22238;&#21021;&#22987;&#25552;&#31034;&#65292;&#24418;&#25104;&#19968;&#20010;&#26032;&#30340;&#20016;&#23500;&#25351;&#23548;&#12290;&#35813;&#26041;&#27861;&#22312;BIG-Bench&#25351;&#20196;&#25512;&#23548;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents AutoHint, a novel framework for automatic prompt engineering and optimization for Large Language Models (LLM). While LLMs have demonstrated remarkable ability in achieving high-quality annotation in various tasks, the key to applying this ability to specific tasks lies in developing high-quality prompts. Thus we propose a framework to inherit the merits of both in-context learning and zero-shot learning by incorporating enriched instructions derived from input-output demonstrations to optimize original prompt. We refer to the enrichment as the hint and propose a framework to automatically generate the hint from labeled data. More concretely, starting from an initial prompt, our method first instructs a LLM to deduce new hints for selected samples from incorrect predictions, and then summarizes from per-sample hints and adds the results back to the initial prompt to form a new, enriched instruction. The proposed method is evaluated on the BIG-Bench Instruction Induct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;</title><link>http://arxiv.org/abs/2307.06775</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#39278;&#39135;&#32010;&#20081;&#30340;&#35786;&#26029;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27515;&#20129;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#12290;&#36825;&#31181;&#24040;&#22823;&#22686;&#38271;&#37096;&#20998;&#26469;&#28304;&#20110;&#30123;&#24773;&#30340;&#21387;&#21147;&#65292;&#20294;&#20063;&#19982;&#31038;&#20132;&#23186;&#20307;&#30340;&#26292;&#38706;&#22686;&#21152;&#26377;&#20851;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#35825;&#21457;&#35266;&#30475;&#32773;&#30340;&#39278;&#39135;&#32010;&#20081;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32452;&#21512;&#21028;&#26029;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#12290;&#20174;Twitter&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#21313;&#20108;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26681;&#25454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26368;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;RoBERTa&#21644;MaxViT&#34701;&#21512;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26080;&#30417;&#30563;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.06713</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26080;&#30417;&#30563;&#26657;&#20934;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30340;&#20808;&#39564;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26080;&#30417;&#30563;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26377;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#27491;&#22312;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#22823;&#37327;&#26080;&#30417;&#30563;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#12289;&#26657;&#20934;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#26041;&#27861;&#36827;&#34892;&#36866;&#24212;&#20197;&#25191;&#34892;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#23558;LLM&#35270;&#20026;&#40657;&#30418;&#65292;&#22312;&#27169;&#22411;&#23631;&#38556;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#38454;&#27573;&#65292;&#29992;&#20110;&#26657;&#20934;&#27169;&#22411;&#21518;&#39564;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#25552;&#31034;&#35757;&#32451;&#26679;&#26412;&#21644;&#26080;&#36866;&#24212;&#25968;&#25454;&#19979;&#30340;&#26657;&#20934;&#26041;&#27861;&#20013;&#20248;&#20110;&#26410;&#36866;&#24212;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach were calibration is performed without using any adaptation data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;LSR-Benchmark&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35828;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#26085;&#24120;&#29983;&#27963;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05113</link><description>&lt;p&gt;
&#36229;&#36234;&#26174;&#32780;&#26131;&#35265;&#65306;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#8212;&#8212;&#22522;&#20110;&#29983;&#27963;&#26223;&#35266;&#25512;&#29702;&#22522;&#20934;(LSR-Benchmark)&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Beyond the Obvious: Evaluating the Reasoning Ability In Real-life Scenarios of Language Models on Life Scapes Reasoning Benchmark~(LSR-Benchmark). (arXiv:2307.05113v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;LSR-Benchmark&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35828;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#26085;&#24120;&#29983;&#27963;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29983;&#27963;&#26223;&#35266;&#25512;&#29702;&#22522;&#20934; (LSR-Benchmark)&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#30495;&#23454;&#24773;&#22659;&#25512;&#29702;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24357;&#34917;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#26085;&#24120;&#32972;&#26223;&#19979;&#25512;&#29702;&#33021;&#21147;&#30340;&#24046;&#36317;&#12290;&#19982;&#39046;&#22495;&#30693;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;LSR-Benchmark&#21253;&#21547;&#33258;&#30001;&#25991;&#26412;&#26684;&#24335;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#26377;&#20851;&#30495;&#23454;&#29983;&#27963;&#24773;&#26223;&#12289;&#20154;&#31867;&#34892;&#20026;&#21644;&#35282;&#33394;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#26469;&#33258;&#24320;&#28304;&#22312;&#32447;&#26469;&#28304;&#30340;2162&#20010;&#38382;&#39064;&#32452;&#25104;&#65292;&#24182;&#36827;&#34892;&#25163;&#21160;&#27880;&#37322;&#20197;&#25552;&#39640;&#36136;&#37327;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;gpt3.5-turbo&#21644;instruction fine-tuned llama&#27169;&#22411;&#65292;&#27979;&#35797;&#20854;&#22312;LSR-Benchmark&#19978;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#26126;&#26174;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#36825;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#26085;&#24120;&#29983;&#27963;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), a novel dataset targeting real-life scenario reasoning, aiming to close the gap in artificial neural networks' ability to reason in everyday contexts. In contrast to domain knowledge reasoning datasets, LSR-Benchmark comprises free-text formatted questions with rich information on real-life scenarios, human behaviors, and character roles. The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve its quality. Experiments are conducted using state-of-the-art language models, such as gpt3.5-turbo and instruction fine-tuned llama models, to test the performance in LSR-Benchmark. The results reveal that humans outperform these models significantly, indicating a persisting challenge for machine learning models in comprehending daily human life.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#36947;&#22914;&#20309;&#19982;&#30693;&#36947;&#20160;&#20040;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#22238;&#31572;&#20851;&#20110;&#29992;&#25143;&#25163;&#20876;&#30340;&#22522;&#26412;&#20107;&#23454;&#12289;&#27969;&#31243;&#65292;&#24182;&#35299;&#20915;&#19968;&#20123;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;&#20182;&#20204;&#37319;&#29992;&#22270;(TARA)&#26469;&#32852;&#21512;&#34920;&#31034;&#27493;&#39588;&#21644;&#20107;&#23454;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#22312;&#22238;&#31572;&#23454;&#38469;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04187</link><description>&lt;p&gt;
&#30693;&#36947;&#22914;&#20309;&#19982;&#30693;&#36947;&#20160;&#20040;&#65306;&#29992;&#25143;&#25163;&#20876;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#26032;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Knowing-how &amp; Knowing-that: A New Task for Machine Reading Comprehension of User Manuals. (arXiv:2306.04187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#36947;&#22914;&#20309;&#19982;&#30693;&#36947;&#20160;&#20040;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#22238;&#31572;&#20851;&#20110;&#29992;&#25143;&#25163;&#20876;&#30340;&#22522;&#26412;&#20107;&#23454;&#12289;&#27969;&#31243;&#65292;&#24182;&#35299;&#20915;&#19968;&#20123;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;&#20182;&#20204;&#37319;&#29992;&#22270;(TARA)&#26469;&#32852;&#21512;&#34920;&#31034;&#27493;&#39588;&#21644;&#20107;&#23454;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#22312;&#22238;&#31572;&#23454;&#38469;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#25163;&#20876;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20855;&#26377;&#24040;&#22823;&#30340;&#23458;&#25143;&#26381;&#21153;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#30693;&#36947;&#22914;&#20309;&#19982;&#30693;&#36947;&#20160;&#20040;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#22238;&#31572;&#20851;&#20110;&#29992;&#25143;&#25163;&#20876;&#30340;&#22522;&#26412;&#20107;&#23454;&#12289;&#27969;&#31243;&#65292;&#24182;&#35299;&#20915;&#19968;&#20123;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;(TARA)&#20013;&#32852;&#21512;&#34920;&#31034;&#27493;&#39588;&#21644;&#20107;&#23454;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25903;&#25345;&#21508;&#31181;&#38382;&#39064;&#30340;&#32479;&#19968;&#25512;&#29702;&#12290;&#20026;&#20102;&#36827;&#34892;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#35780;&#20272;&#30740;&#31350;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#33258;&#21160;&#23558;&#29992;&#25143;&#25163;&#20876;&#35299;&#26512;&#25104;TARA&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#22312;&#22238;&#31572;&#23454;&#38469;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#29992;&#25143;&#25163;&#20876;&#34920;&#31034;&#20026;TARA&#26159;&#29992;&#25143;&#25163;&#20876;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#29702;&#24819;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;TARA&#30340;&#28145;&#20837;&#30740;&#31350;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#26410;&#26469;&#29992;&#25143;&#25163;&#20876;&#34920;&#31034;&#30340;&#38382;&#39064;&#21644;&#24191;&#27867;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#23558;&#29992;&#25143;&#25163;&#20876;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25512;&#21521;&#26356;&#23454;&#29992;&#21644;&#26377;&#25928;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine reading comprehension (MRC) of user manuals has huge potential in customer service. However,current methods have trouble answering complex questions. Therefore, we introduce the Knowing-how &amp; Knowing-that task that requires the model to answer factoid-style, procedure-style, and inconsistent questions about user manuals. We resolve this task by jointly representing the steps and facts in a graph (TARA), which supports a unified inference of various questions. Towards a systematical benchmarking study, we design a heuristic method to automatically parse user manuals into TARAs and build an annotated dataset to test the model's ability in answering real-world questions. Empirical results demonstrate that representing user manuals as TARAs is a desired solution for the MRC of user manuals. An in-depth investigation of TARA further sheds light on the issues and broader impacts of future representations of user manuals. We hope our work can move the MRC of user manuals to a more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35789;&#27719;&#31232;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#20316;&#20026;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30828;&#27491;&#20363;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.09287</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#35789;&#27719;&#31232;&#37322;&#20316;&#20026;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdversarialWord Dilution as Text Data Augmentation in Low-Resource Regime. (arXiv:2305.09287v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35789;&#27719;&#31232;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#20316;&#20026;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30828;&#27491;&#20363;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#22686;&#24378;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20013;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#29983;&#25104;&#30828;&#27491;&#20363;&#29992;&#20316;&#25968;&#25454;&#22686;&#24378;&#30340;&#26377;&#25928;&#26041;&#27861;&#21364;&#20173;&#26377;&#24453;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35789;&#27719;&#31232;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#30828;&#27491;&#20363;&#20316;&#20026;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35757;&#32451;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is widely used in text classification, especially in the low-resource regime where a few examples for each class are available during training. Despite the success, generating data augmentations as hard positive examples that may increase their effectiveness is under-explored. This paper proposes an Adversarial Word Dilution (AWD) method that can generate hard positive examples as text data augmentations to train the low-resource text classification model efficiently. Our idea of augmenting the text data is to dilute the embedding of strong positive words by weighted mixing with unknown-word embedding, making the augmented inputs hard to be recognized as positive by the classification model. We adversarially learn the dilution weights through a constrained min-max optimization process with the guidance of the labels. Empirical studies on three benchmark datasets show that AWD can generate more effective data augmentations and outperform the state-of-the-art text data 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AttentionViz&#65292;&#19968;&#31181;&#20197;&#32852;&#21512;&#23884;&#20837;&#20026;&#22522;&#30784;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#20840;&#23616;&#20998;&#26512;&#22810;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25552;&#39640;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#24182;&#36890;&#36807;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#21644;&#19987;&#23478;&#21453;&#39304;&#25552;&#20379;&#26032;&#30340;&#20132;&#20114;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.03210</link><description>&lt;p&gt;
AttentionViz&#65306;Transformer Attention&#30340;&#20840;&#23616;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
AttentionViz: A Global View of Transformer Attention. (arXiv:2305.03210v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AttentionViz&#65292;&#19968;&#31181;&#20197;&#32852;&#21512;&#23884;&#20837;&#20026;&#22522;&#30784;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#20840;&#23616;&#20998;&#26512;&#22810;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25552;&#39640;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#24182;&#36890;&#36807;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#21644;&#19987;&#23478;&#21453;&#39304;&#25552;&#20379;&#26032;&#30340;&#20132;&#20114;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#27491;&#22312;&#38761;&#26032;&#26426;&#22120;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#30340;&#20869;&#37096;&#36816;&#20316;&#20173;&#28982;&#31070;&#31192;&#33707;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#24207;&#21015;&#20013;&#20803;&#32032;&#20043;&#38388;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21487;&#35270;&#21270;Transformer&#27169;&#22411;&#29992;&#20110;&#35745;&#31639;&#27880;&#24847;&#21147;&#30340;&#26597;&#35810;&#21644;&#38190;&#21521;&#37327;&#30340;&#32852;&#21512;&#23884;&#20837;&#12290;&#19982;&#20197;&#21069;&#30340;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#26512;&#22810;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#20840;&#23616;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#32852;&#21512;&#26597;&#35810;-&#38190;&#23884;&#20837;&#21019;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;AttentionViz&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#30740;&#31350;&#35821;&#35328;&#21644;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#36890;&#36807;&#20960;&#20010;&#24212;&#29992;&#22330;&#26223;&#21644;&#19987;&#23478;&#21453;&#39304;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#21644;&#25552;&#20379;&#26377;&#20851;&#26597;&#35810;-&#38190;&#20132;&#20114;&#30340;&#26032;&#35265;&#35299;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz, based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35821;&#35328;&#22686;&#24378;Transformer&#32534;&#30721;&#22120;&#65292;&#24182;&#32467;&#21512;&#21307;&#23398;&#25552;&#31034;&#65292;&#23558;&#32467;&#26500;&#21270;&#12289;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#35821;&#35328;&#28508;&#31354;&#38388;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#21307;&#23398;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.17408</link><description>&lt;p&gt;
&#22522;&#20110;&#21307;&#23398;&#25552;&#31034;&#30340;&#35821;&#35328;&#22686;&#24378;Transformer&#32534;&#30721;&#22120;&#30340;&#21307;&#30103;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Medical Intervention Duration Estimation Using Language-enhanced Transformer Encoder with Medical Prompts. (arXiv:2303.17408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17408
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#22686;&#24378;Transformer&#32534;&#30721;&#22120;&#65292;&#24182;&#32467;&#21512;&#21307;&#23398;&#25552;&#31034;&#65292;&#23558;&#32467;&#26500;&#21270;&#12289;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#35821;&#35328;&#28508;&#31354;&#38388;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#21307;&#23398;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;(EHRs)&#20272;&#35745;&#21307;&#30103;&#24178;&#39044;&#30340;&#25345;&#32493;&#26102;&#38388;&#22312;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24573;&#30053;&#20102;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#33258;&#30001;&#25991;&#26412;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#35328;&#22686;&#24378;Transformer-based&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#23558;&#25152;&#26377;&#30456;&#20851;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#24577;&#65288;&#36830;&#32493;&#12289;&#20998;&#31867;&#12289;&#20108;&#36827;&#21046;&#21644;&#33258;&#30001;&#25991;&#26412;&#29305;&#24449;&#65289;&#25237;&#24433;&#21040;&#19968;&#20010;&#21327;&#35843;&#30340;&#35821;&#35328;&#28508;&#31354;&#38388;&#20013;&#65292;&#20511;&#21161;&#21307;&#23398;&#25552;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#24471;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#22312;&#21333;&#20803;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#20013;&#38598;&#25104;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#21307;&#23398;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#32654;&#22269;&#65288;ICU&#20303;&#38498;&#26102;&#38388;&#20272;&#35745;&#65289;&#21644;&#20122;&#27954;&#65288;&#25163;&#26415;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#65289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, estimating the duration of medical intervention based on electronic health records (EHRs) has gained significant attention in the filed of clinical decision support. However, current models largely focus on structured data, leaving out information from the unstructured clinical free-text data. To address this, we present a novel language-enhanced transformer-based framework, which projects all relevant clinical data modalities (continuous, categorical, binary, and free-text features) into a harmonized language latent space using a pre-trained sentence encoder with the help of medical prompts. The proposed method enables the integration of information from different modalities within the cell transformer encoder and leads to more accurate duration estimation for medical intervention. Our experimental results on both US-based (length of stay in ICU estimation) and Asian (surgical duration prediction) medical datasets demonstrate the effectiveness of our proposed framewor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaMMUT&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#65292;&#24182;&#22312;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16839</link><description>&lt;p&gt;
MaMMUT: &#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#32852;&#21512;&#23398;&#20064;&#30340;&#31616;&#21333;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaMMUT&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#65292;&#24182;&#22312;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24050;&#20174;&#32534;&#30721;-&#35299;&#30721;&#36716;&#21521;&#20165;&#35299;&#30721;&#30340;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#26222;&#36941;&#35748;&#20026;&#65292;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#29983;&#25104;&#20219;&#21153;&#21644;&#23545;&#27604;&#20219;&#21153;&#65292;&#24448;&#24448;&#20114;&#30456;&#20914;&#31361;&#65292;&#38590;&#20197;&#22312;&#19968;&#20010;&#26550;&#26500;&#20013;&#23481;&#32435;&#65292;&#24182;&#36827;&#19968;&#27493;&#38656;&#35201;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22797;&#26434;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#20165;&#35299;&#30721;&#27169;&#22411;&#65292;&#36825;&#22312;&#32852;&#21512;&#23398;&#20064;&#36825;&#20123;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22411;MaMMUT&#23454;&#29616;&#30340;&#12290;&#23427;&#30001;&#21333;&#19968;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#25991;&#26412;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#25991;&#26412;&#35299;&#30721;&#22120;&#19978;&#30340;&#26032;&#30340;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#19981;&#21516;&#30446;&#26631;&#20219;&#21153;&#30340;&#32852;&#21512;&#35757;&#32451;&#26159;&#31616;&#21333;&#30340;&#65292;&#26377;&#25928;&#30340;&#65292;&#24182;&#26368;&#22823;&#21270;&#20102;&#27169;&#22411;&#30340;&#26435;&#37325;&#20849;&#20139;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#26550;&#26500;&#20351;&#24471;&#23545;&#24320;&#25918;&#35789;&#27719;&#23545;&#35937;&#26816;&#27979;&#30340;&#31616;&#21333;&#25193;&#23637;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of language models have moved from encoder-decoder to decoder-only designs. In addition, the common knowledge has it that the two most popular multimodal tasks, the generative and contrastive tasks, tend to conflict with one another, are hard to accommodate in one architecture, and further need complex adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint training of these diverse-objective tasks is simple, effective, and maximizes the weight-sharing of the model. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#25104;&#26412;&#25928;&#30410;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#32463;&#27982;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#22522;&#20110;&#25104;&#26412;&#30340;&#20462;&#21098;&#65292;&#25552;&#20986;&#20102;EcoOptiGen&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20351;&#29992;GPT-3.5/GPT-4&#27169;&#22411;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.04673</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#25104;&#26412;&#25928;&#30410;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference. (arXiv:2303.04673v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#25104;&#26412;&#25928;&#30410;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#32463;&#27982;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#22522;&#20110;&#25104;&#26412;&#30340;&#20462;&#21098;&#65292;&#25552;&#20986;&#20102;EcoOptiGen&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20351;&#29992;GPT-3.5/GPT-4&#27169;&#22411;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20854;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#21508;&#31181;&#21830;&#19994;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#25104;&#26412;&#39537;&#20351;&#24212;&#29992;&#31243;&#24207;&#26500;&#24314;&#32773;&#22312;&#26377;&#38480;&#30340;&#25512;&#29702;&#39044;&#31639;&#19979;&#26368;&#22823;&#21270;&#29983;&#25104;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#20248;&#21270;&#25512;&#29702;&#36229;&#21442;&#25968;&#65288;&#22914;&#22238;&#22797;&#25968;&#37327;&#12289;&#28201;&#24230;&#21644;&#26368;&#22823;token&#25968;&#65289;&#30340;&#30740;&#31350;&#65292;&#36825;&#26174;&#33879;&#24433;&#21709;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#25928;&#29992;/&#25104;&#26412;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;EcoOptiGen&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#32463;&#27982;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#22522;&#20110;&#25104;&#26412;&#30340;&#20462;&#21098;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#20351;&#29992;GPT-3.5/GPT-4&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;EcoOptiGen&#24050;&#22312;FLAML&#24211;&#30340;`autogen'&#21253;&#20013;&#23454;&#29616;&#65306;\url{https://aka.ms/autogen}&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have sparked significant interest in their generative capabilities, leading to the development of various commercial applications. The high cost of using the models drives application builders to maximize the value of generation under a limited inference budget. This paper presents a study of optimizing inference hyperparameters such as the number of responses, temperature and max tokens, which significantly affects the utility/cost of text generation. We design a framework named EcoOptiGen which leverages economical hyperparameter optimization and cost-based pruning. Experiments with the GPT-3.5/GPT-4 models on a variety of tasks verify its effectiveness. EcoOptiGen is implemented in the `autogen' package of the FLAML library: \url{https://aka.ms/autogen}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KGQAn&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#26080;&#38656;&#38024;&#23545;&#27599;&#20010;&#30446;&#26631;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#23450;&#21046;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#36716;&#25442;&#20026;&#20013;&#38388;&#30340;&#25277;&#35937;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;SPARQL&#26597;&#35810;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2303.00595</link><description>&lt;p&gt;
&#19968;&#20010;&#36866;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#36890;&#29992;&#38382;&#31572;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
A Universal Question-Answering Platform for Knowledge Graphs. (arXiv:2303.00595v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KGQAn&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#26080;&#38656;&#38024;&#23545;&#27599;&#20010;&#30446;&#26631;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#23450;&#21046;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#36716;&#25442;&#20026;&#20013;&#38388;&#30340;&#25277;&#35937;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;SPARQL&#26597;&#35810;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#20197;RDF&#24341;&#25806;&#23384;&#20648;&#30340;&#26469;&#33258;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;SPARQL&#31471;&#28857;&#21487;&#20197;&#22312;Web&#19978;&#35775;&#38382;&#30693;&#35782;&#22270;&#35889;&#12290;&#20026;&#20102;&#27491;&#30830;&#34920;&#36798;&#19968;&#20010;&#31526;&#21512;&#35268;&#33539;&#30340;SPARQL&#26597;&#35810;&#65292;&#38656;&#35201;&#20102;&#35299;&#22270;&#32467;&#26500;&#21644;&#20854;&#32452;&#25104;&#37096;&#20998;&#30340;&#30830;&#20999;URI&#65292;&#36825;&#23545;&#20110;&#26222;&#36890;&#29992;&#25143;&#26469;&#35828;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#38382;&#31572;&#31995;&#32479;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;SPARQL&#26469;&#25552;&#20379;&#24110;&#21161;&#12290;&#29616;&#26377;&#30340;&#38382;&#31572;&#31995;&#32479;&#36890;&#24120;&#22522;&#20110;&#24212;&#29992;&#29305;&#23450;&#30340;&#20154;&#24037;&#31574;&#30053;&#65292;&#25110;&#32773;&#38656;&#35201;&#20808;&#39564;&#20449;&#24687;&#12289;&#26114;&#36149;&#30340;&#39044;&#22788;&#29702;&#21644;&#27169;&#22411;&#35843;&#25972;&#26469;&#36866;&#37197;&#27599;&#20010;&#30446;&#26631;&#30693;&#35782;&#22270;&#35889;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#24456;&#38590;&#25512;&#24191;&#21040;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#30693;&#35782;&#22270;&#35889;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KGQAn&#65292;&#19968;&#20010;&#19981;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#30446;&#26631;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#23450;&#21046;&#30340;&#36890;&#29992;&#38382;&#31572;&#31995;&#32479;&#12290;KGQAn&#19981;&#20351;&#29992;&#20154;&#24037;&#31574;&#30053;&#65292;&#32780;&#26159;&#23558;&#38382;&#39064;&#29702;&#35299;&#20316;&#20026;&#19968;&#20010;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#26469;&#36827;&#34892;&#26032;&#39062;&#30340;&#24418;&#24335;&#21270;&#65292;&#36890;&#36807;&#31070;&#32463;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#23558;&#38382;&#39064;&#36716;&#25442;&#20026;&#20013;&#38388;&#30340;&#25277;&#35937;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge from diverse application domains is organized as knowledge graphs (KGs) that are stored in RDF engines accessible in the web via SPARQL endpoints. Expressing a well-formed SPARQL query requires information about the graph structure and the exact URIs of its components, which is impractical for the average user. Question answering (QA) systems assist by translating natural language questions to SPARQL. Existing QA systems are typically based on application-specific human-curated rules, or require prior information, expensive pre-processing and model adaptation for each targeted KG. Therefore, they are hard to generalize to a broad set of applications and KGs.  In this paper, we propose KGQAn, a universal QA system that does not need to be tailored to each target KG. Instead of curated rules, KGQAn introduces a novel formalization of question understanding as a text generation problem to convert a question into an intermediate abstract representation via a neural sequence-to-se
&lt;/p&gt;</description></item></channel></rss>