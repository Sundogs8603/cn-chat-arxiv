<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#20307;&#39564;&#24335;&#36716;&#25442;&#30340;&#22810;&#38454;&#27573;&#22810;&#27969;&#22810;&#27169;&#24577;Transformer&#65292;&#36890;&#36807;&#23545;&#21516;&#27493;&#30340;&#22810;&#35282;&#24230;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#27169;&#22411;&#21644;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;14.01%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.14859</link><description>&lt;p&gt;
3M-TRANSFORMER&#65306;&#19968;&#31181;&#29992;&#20110;&#20307;&#39564;&#24335;&#36716;&#25442;&#39044;&#27979;&#30340;&#22810;&#38454;&#27573;&#22810;&#27969;&#22810;&#27169;&#24577;Transformer
&lt;/p&gt;
&lt;p&gt;
3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction. (arXiv:2310.14859v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#20307;&#39564;&#24335;&#36716;&#25442;&#30340;&#22810;&#38454;&#27573;&#22810;&#27969;&#22810;&#27169;&#24577;Transformer&#65292;&#36890;&#36807;&#23545;&#21516;&#27493;&#30340;&#22810;&#35282;&#24230;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#27169;&#22411;&#21644;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;14.01%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#39044;&#27979;&#20132;&#26367;&#23545;&#35805;&#22312;&#20154;&#26426;/&#26426;&#22120;&#20154;&#20132;&#20114;&#20013;&#20855;&#26377;&#24456;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#27807;&#36890;&#30340;&#22797;&#26434;&#24615;&#20351;&#36825;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36827;&#23637;&#34920;&#26126;&#65292;&#21516;&#27493;&#30340;&#22810;&#35282;&#24230;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19982;&#24322;&#27493;&#30340;&#21333;&#35282;&#24230;&#36716;&#24405;&#30456;&#27604;&#30340;&#20132;&#26367;&#23545;&#35805;&#39044;&#27979;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;Transformer&#30340;&#26032;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#20307;&#39564;&#24335;&#30340;&#12289;&#21516;&#27493;&#30340;&#22810;&#35282;&#24230;&#25968;&#25454;&#20013;&#30340;&#20132;&#26367;&#23545;&#35805;&#12290;&#25105;&#20204;&#22312;&#26368;&#36817;&#24341;&#20837;&#30340;EgoCom&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#22522;&#20934;&#32447;&#21644;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;3M-Transformer&#24179;&#22343;&#24615;&#33021;&#25552;&#39640;&#20102;14.01%&#12290;&#25105;&#20204;&#30340;3M-Transformer&#30340;&#28304;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#23558;&#22312;&#34987;&#25509;&#21463;&#21518;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting turn-taking in multiparty conversations has many practical applications in human-computer/robot interaction. However, the complexity of human communication makes it a challenging task. Recent advances have shown that synchronous multi-perspective egocentric data can significantly improve turn-taking prediction compared to asynchronous, single-perspective transcriptions. Building on this research, we propose a new multimodal transformer-based architecture for predicting turn-taking in embodied, synchronized multi-perspective data. Our experimental results on the recently introduced EgoCom dataset show a substantial performance improvement of up to 14.01% on average compared to existing baselines and alternative transformer-based approaches. The source code, and the pre-trained models of our 3M-Transformer will be available upon acceptance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#24067;&#24335;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;FedJudge&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#35774;&#22791;&#25110;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#65292;&#24182;&#23558;&#21442;&#25968;&#32858;&#21512;&#21644;&#20998;&#24067;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;&#36825;&#35299;&#20915;&#20102;&#38598;&#20013;&#24335;&#35757;&#32451;&#27861;&#24459;LLMs&#24341;&#21457;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#21644;&#20998;&#24067;&#20559;&#31227;&#23548;&#33268;&#30340;FL&#26041;&#27861;&#25928;&#26524;&#38477;&#20302;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.08173</link><description>&lt;p&gt;
FedJudge: &#20998;&#24067;&#24335;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FedJudge: Federated Legal Large Language Model. (arXiv:2309.08173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#24067;&#24335;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;FedJudge&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#35774;&#22791;&#25110;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#65292;&#24182;&#23558;&#21442;&#25968;&#32858;&#21512;&#21644;&#20998;&#24067;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;&#36825;&#35299;&#20915;&#20102;&#38598;&#20013;&#24335;&#35757;&#32451;&#27861;&#24459;LLMs&#24341;&#21457;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#21644;&#20998;&#24067;&#20559;&#31227;&#23548;&#33268;&#30340;FL&#26041;&#27861;&#25928;&#26524;&#38477;&#20302;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27861;&#24459;&#26234;&#33021;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#36741;&#21161;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#21644;&#26222;&#36890;&#20154;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27861;&#24459;LLMs&#30340;&#38598;&#20013;&#24335;&#35757;&#32451;&#24341;&#21457;&#20102;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#27861;&#24459;&#25968;&#25454;&#20998;&#25955;&#22312;&#21253;&#21547;&#25935;&#24863;&#20010;&#20154;&#20449;&#24687;&#30340;&#21508;&#20010;&#26426;&#26500;&#20043;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#23558;&#27861;&#24459;LLMs&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#65288;FL&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;FL&#65292;&#27861;&#24459;LLMs&#21487;&#20197;&#22312;&#35774;&#22791;&#25110;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#65292;&#20854;&#21442;&#25968;&#34987;&#32858;&#21512;&#24182;&#20998;&#24067;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#65292;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#32780;&#26080;&#38656;&#30452;&#25509;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#38459;&#30861;&#20102;LLMs&#22312;FL&#29615;&#22659;&#20013;&#30340;&#20840;&#38754;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#27861;&#24459;&#25968;&#25454;&#30340;&#20998;&#24067;&#20559;&#31227;&#20943;&#23569;&#20102;FL&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#24067;&#24335;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;FedJudge&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have gained prominence in the field of Legal Intelligence, offering potential applications in assisting legal professionals and laymen. However, the centralized training of these Legal LLMs raises data privacy concerns, as legal data is distributed among various institutions containing sensitive individual information. This paper addresses this challenge by exploring the integration of Legal LLMs with Federated Learning (FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally on devices or clients, and their parameters are aggregated and distributed on a central server, ensuring data privacy without directly sharing raw data. However, computation and communication overheads hinder the full fine-tuning of LLMs under the FL setting. Moreover, the distribution shift of legal data reduces the effectiveness of FL methods. To this end, in this paper, we propose the first Federated Legal Large Language Model (FedJudge) framework, which fine-tunes 
&lt;/p&gt;</description></item><item><title>ChatGPT&#21644;GPT-4&#22312;&#25169;&#20811;&#20013;&#26174;&#31034;&#20986;&#39640;&#32423;&#29702;&#35299;&#65292;&#20294;&#19981;&#26159;&#28216;&#25103;&#35770;&#29702;&#26368;&#20248;&#30340;&#25169;&#20811;&#29609;&#23478;&#12290;&#23545;&#27169;&#22411;&#21442;&#25968;&#21644;&#25552;&#31034;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#25169;&#20811;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.12466</link><description>&lt;p&gt;
ChatGPT&#21644;GPT-4&#26159;&#20248;&#31168;&#30340;&#25169;&#20811;&#29609;&#23478;&#21527;&#65311;&#8212;&#8212;&#19968;&#39033;Pre-Flop&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis. (arXiv:2308.12466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12466
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;GPT-4&#22312;&#25169;&#20811;&#20013;&#26174;&#31034;&#20986;&#39640;&#32423;&#29702;&#35299;&#65292;&#20294;&#19981;&#26159;&#28216;&#25103;&#35770;&#29702;&#26368;&#20248;&#30340;&#25169;&#20811;&#29609;&#23478;&#12290;&#23545;&#27169;&#22411;&#21442;&#25968;&#21644;&#25552;&#31034;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#25169;&#20811;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#21644;GPT-4&#38382;&#19990;&#20197;&#26469;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#29087;&#32451;&#31243;&#24230;&#26159;&#26174;&#32780;&#26131;&#35265;&#30340;&#65292;&#20294;&#23427;&#20204;&#22312;&#28216;&#25103;&#20013;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25169;&#20811;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#36824;&#26410;&#34987;&#25506;&#32034;&#12290;&#25169;&#20811;&#26159;&#19968;&#31181;&#38656;&#35201;&#22312;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#23436;&#20840;&#20449;&#24687;&#19979;&#20570;&#20986;&#20915;&#31574;&#30340;&#28216;&#25103;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#21644;GPT-4&#36827;&#34892;&#20102;&#25169;&#20811;&#27979;&#35797;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#25169;&#20811;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#36825;&#20004;&#20010;&#27169;&#22411;&#37117;&#23637;&#31034;&#20102;&#23545;&#25169;&#20811;&#30340;&#39640;&#32423;&#29702;&#35299;&#65292;&#21253;&#25324;&#36215;&#22987;&#25163;&#29260;&#30340;&#20272;&#20540;&#12289;&#25171;&#29260;&#20301;&#32622;&#20197;&#21450;&#28216;&#25103;&#35770;&#29702;&#26368;&#20248;(GTO)&#25169;&#20811;&#30340;&#20854;&#20182;&#22797;&#26434;&#24615;&#65292;&#20294;ChatGPT&#21644;GPT-4&#24182;&#19981;&#26159;&#28216;&#25103;&#35770;&#29702;&#26368;&#20248;&#30340;&#25169;&#20811;&#29609;&#23478;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#20102;&#19982;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#29609;&#25169;&#20811;&#30456;&#20851;&#30340;&#26368;&#20339;&#25552;&#31034;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#29305;&#24449;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#36825;&#20004;&#20010;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#25171;&#29260;&#39118;&#26684;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;GPT-4&#26159;
&lt;/p&gt;
&lt;p&gt;
Since the introduction of ChatGPT and GPT-4, these models have been tested across a large number of tasks. Their adeptness across domains is evident, but their aptitude in playing games and specifically their aptitude in the realm of poker has remained unexplored. Poker is a game that requires decision making under uncertainty and incomplete information. In this paper, we put ChatGPT and GPT-4 through the poker test and evaluate their poker skills. Our findings reveal that while both models display an advanced understanding of poker, encompassing concepts like the valuation of starting hands, playing positions and other intricacies of game theory optimal (GTO) poker, both ChatGPT and GPT-4 are NOT game theory optimal poker players.  Through a series of experiments, we first discover the characteristics of optimal prompts and model parameters for playing poker with these models. Our observations then unveil the distinct playing personas of the two models. We first conclude that GPT-4 is
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#20135;&#29983;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.15043</link><description>&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#36890;&#29992;&#21644;&#21487;&#36801;&#31227;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15043
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#20135;&#29983;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#24341;&#36215;&#21453;&#24863;&#30340;&#20869;&#23481;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#38450;&#27490;&#20135;&#29983;&#19981;&#33391;&#29983;&#25104;&#12290;&#23613;&#31649;&#22312;&#35268;&#36991;&#36825;&#20123;&#25514;&#26045;&#19978;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#25152;&#35859;&#30340;&#23545;LLMs&#30340;&#8220;&#36234;&#29425;&#8221;&#25915;&#20987;&#65292;&#20294;&#36825;&#20123;&#25915;&#20987;&#38656;&#35201;&#20154;&#20026;&#30340;&#24039;&#24605;&#65292;&#23454;&#38469;&#19978;&#24182;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#33391;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25214;&#21040;&#19968;&#20010;&#21518;&#32512;&#65292;&#24403;&#38468;&#21152;&#21040;&#21508;&#31181;&#26597;&#35810;&#19978;&#65292;&#20379;LLM&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#26102;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#27169;&#22411;&#20135;&#29983;&#32943;&#23450;&#22238;&#31572;&#65288;&#32780;&#19981;&#26159;&#25298;&#32477;&#22238;&#31572;&#65289;&#30340;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36138;&#23146;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#25628;&#32034;&#25216;&#26415;&#33258;&#21160;&#20135;&#29983;&#36825;&#20123;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#24182;&#19988;&#22312;&#36807;&#21435;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past autom
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prot2Text&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;GNNs&#21644;Transformers&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#32508;&#21512;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#20840;&#38754;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.14367</link><description>&lt;p&gt;
Prot2Text: &#22522;&#20110;GNNs&#21644;Transformers&#30340;&#22810;&#27169;&#24577;&#34507;&#30333;&#36136;&#21151;&#33021;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers. (arXiv:2307.14367v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14367
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prot2Text&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;GNNs&#21644;Transformers&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#32508;&#21512;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#20840;&#38754;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#20351;&#26576;&#20123;&#31185;&#23398;&#23478;&#23558;&#20854;&#29702;&#35299;&#24402;&#31867;&#20026;&#38590;&#20197;&#24819;&#35937;&#30340;&#20219;&#21153;&#12290;&#19981;&#21516;&#32423;&#21035;&#30340;&#25361;&#25112;&#20351;&#36825;&#39033;&#20219;&#21153;&#22797;&#26434;&#21270;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#24320;&#21457;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#23558;&#20219;&#21153;&#34920;&#36848;&#20026;&#22810;&#20998;&#31867;&#38382;&#39064;&#65292;&#21363;&#23558;&#39044;&#23450;&#20041;&#26631;&#31614;&#20998;&#37197;&#32473;&#34507;&#30333;&#36136;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;Prot2Text&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#20013;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#12290;&#36825;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#20801;&#35768;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#36827;&#34892;&#25972;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, en
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#34920;&#31034;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#21319;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22312;&#32452;&#21512;&#27867;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22686;&#24378;&#22522;&#20110;&#20196;&#29260;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20154;&#31867;&#37027;&#26679;&#36866;&#24403;&#22320;&#32452;&#21512;&#21644;&#20351;&#29992;&#24207;&#21015;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20174;&#36817;&#26399;&#30340;&#20851;&#20110;&#35757;&#32451;&#26356;&#28145;Transformer&#30340;&#30740;&#31350;&#32467;&#26524;&#26469;&#30475;&#65292;&#32416;&#32544;&#38382;&#39064;&#20027;&#35201;&#26159;&#30001;&#20110;&#27531;&#24046;&#36830;&#25509;&#30340;&#8220;&#27973;&#23618;&#8221;&#21644;&#31616;&#21333;&#30340;&#21333;&#27493;&#25805;&#20316;&#23548;&#33268;&#19981;&#33021;&#26377;&#25928;&#22320;&#34701;&#21512;&#21069;&#38754;&#23618;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.10799</link><description>&lt;p&gt;
Layer-wise Representation Fusion for Compositional Generalization. (arXiv:2307.10799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Layer-wise Representation Fusion for Compositional Generalization. (arXiv:2307.10799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#34920;&#31034;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#21319;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22312;&#32452;&#21512;&#27867;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22686;&#24378;&#22522;&#20110;&#20196;&#29260;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20154;&#31867;&#37027;&#26679;&#36866;&#24403;&#22320;&#32452;&#21512;&#21644;&#20351;&#29992;&#24207;&#21015;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20174;&#36817;&#26399;&#30340;&#20851;&#20110;&#35757;&#32451;&#26356;&#28145;Transformer&#30340;&#30740;&#31350;&#32467;&#26524;&#26469;&#30475;&#65292;&#32416;&#32544;&#38382;&#39064;&#20027;&#35201;&#26159;&#30001;&#20110;&#27531;&#24046;&#36830;&#25509;&#30340;&#8220;&#27973;&#23618;&#8221;&#21644;&#31616;&#21333;&#30340;&#21333;&#27493;&#25805;&#20316;&#23548;&#33268;&#19981;&#33021;&#26377;&#25928;&#22320;&#34701;&#21512;&#21069;&#38754;&#23618;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#26500;&#24314;&#30340;&#35299;&#20915;&#26041;&#26696;&#34987;&#35748;&#20026;&#22312;&#32452;&#21512;&#27867;&#21270;&#26041;&#38754;&#19981;&#22914;&#20154;&#31867;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#38459;&#30861;&#32452;&#21512;&#27867;&#21270;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26368;&#19978;&#23618;&#30340;&#34920;&#31034;&#34987;&#32416;&#32544;&#22312;&#19968;&#36215;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#24207;&#21015;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#34987;&#19981;&#36866;&#24403;&#22320;&#25197;&#26354;&#20102;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#22686;&#24378;&#22522;&#20110;&#20196;&#29260;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#32531;&#35299;&#34920;&#31034;&#32416;&#32544;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#20687;&#20154;&#31867;&#37027;&#26679;&#36866;&#24403;&#22320;&#32452;&#21512;&#21644;&#20351;&#29992;&#24207;&#21015;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#36817;&#26399;&#20851;&#20110;&#35757;&#32451;&#26356;&#28145;Transformer&#30340;&#30740;&#31350;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#32416;&#32544;&#38382;&#39064;&#23384;&#22312;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#8220;&#27973;&#23618;&#8221;&#27531;&#24046;&#36830;&#25509;&#21644;&#20854;&#31616;&#21333;&#30340;&#21333;&#27493;&#25805;&#20316;&#23548;&#33268;&#26080;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#21069;&#38754;&#23618;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite successes across a broad range of applications, sequence-to-sequence models' construct of solutions are argued to be less compositional than human-like generalization. There is mounting evidence that one of the reasons hindering compositional generalization is representations of the encoder and decoder uppermost layer are entangled. In other words, the syntactic and semantic representations of sequences are twisted inappropriately. However, most previous studies mainly concentrate on enhancing token-level semantic information to alleviate the representations entanglement problem, rather than composing and using the syntactic and semantic representations of sequences appropriately as humans do. In addition, we explain why the entanglement problem exists from the perspective of recent studies about training deeper Transformer, mainly owing to the ``shallow'' residual connections and its simple, one-step operations, which fails to fuse previous layers' information effectively. Sta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#65292;&#21457;&#29616;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.05722</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. (arXiv:2307.05722v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#65292;&#21457;&#29616;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#34892;&#20026;&#22270;&#30340;&#29702;&#35299;&#28508;&#21147;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34892;&#20026;&#22270;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#29702;&#35299;&#26469;&#25552;&#21319;&#22312;&#32447;&#25307;&#32856;&#20013;&#30340;&#25512;&#33616;&#65292;&#21253;&#25324;&#20419;&#36827;&#38750;&#20998;&#24067;&#24335;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35821;&#20041;&#34920;&#31034;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#24182;&#25581;&#31034;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#36335;&#24452;&#25552;&#31034;&#26500;&#36896;&#22120;&#65292;&#21033;&#29992;LLM&#25512;&#33616;&#22120;&#39318;&#27425;&#29702;&#35299;&#34892;&#20026;&#22270;&#65292;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#36335;&#24452;&#22686;&#24378;&#27169;&#22359;&#26469;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#24207;&#21015;&#36755;&#20837;&#24341;&#20837;&#30340;&#25552;&#31034;&#20559;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23558;LM&#30340;&#29305;&#28857;&#24341;&#20837;&#21040;&#34892;&#20026;&#22270;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks, demonstrating their exceptional capabilities in various domains. However, their potential for behavior graph understanding in job recommendations remains largely unexplored. This paper focuses on unveiling the capability of large language models in understanding behavior graphs and leveraging this understanding to enhance recommendations in online recruitment, including the promotion of out-of-distribution (OOD) application. We present a novel framework that harnesses the rich contextual information and semantic representations provided by large language models to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, we propose a meta-path prompt constructor that leverages LLM recommender to understand behavior graphs for the first time and design a corresponding path augmentation module to alleviate the prompt bias introduced by path-based sequence input. By leveragin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17564</link><description>&lt;p&gt;
BloombergGPT&#65306;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#37329;&#34701;&#25216;&#26415;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#32780;&#22797;&#26434;&#30340;&#24212;&#29992;&#65292;&#20174;&#24773;&#24863;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21040;&#38382;&#31572;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#65307;&#28982;&#32780;&#65292;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;LLM&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#25253;&#21578;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;&#25317;&#26377;500&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;&#37329;&#34701;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;3630&#20159;&#20010;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;&#24429;&#21338;&#31038;&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#33021;&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21448;&#22686;&#21152;&#20102;&#26469;&#33258;&#36890;&#29992;&#25968;&#25454;&#38598;&#30340;3450&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;LLM&#22522;&#20934;&#12289;&#24320;&#25918;&#24335;&#37329;&#34701;&#22522;&#20934;&#21644;&#19968;&#22871;&#26368;&#33021;&#20934;&#30830;&#21453;&#26144;&#25105;&#20204;&#39044;&#26399;&#29992;&#36884;&#30340;&#20869;&#37096;&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;BloombergGPT&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#20135;&#29983;&#20102;&#19968;&#20010;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#20250;&#29306;&#29298;&#26222;&#36890;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general 
&lt;/p&gt;</description></item><item><title>AdaLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#12290;&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#39044;&#31639;&#26681;&#25454;&#26435;&#37325;&#30697;&#38453;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#20998;&#37197;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#24494;&#35843;&#34920;&#29616;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.10512</link><description>&lt;p&gt;
&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. (arXiv:2303.10512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10512
&lt;/p&gt;
&lt;p&gt;
AdaLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39044;&#31639;&#20998;&#37197;&#26041;&#27861;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#12290;&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#39044;&#31639;&#26681;&#25454;&#26435;&#37325;&#30697;&#38453;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#20998;&#37197;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#24494;&#35843;&#34920;&#29616;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23545;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#37325;&#35201;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#21442;&#25968;&#65292;&#24403;&#23384;&#22312;&#22823;&#37327;&#19979;&#28216;&#20219;&#21153;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#24494;&#35843;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#20197;&#20197;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#35757;&#32451;&#21152;&#26435;&#30340;&#22686;&#37327;&#26356;&#26032;&#65292;&#20363;&#22914;&#20302;&#31209;&#22686;&#37327;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#39044;&#31639;&#22343;&#21248;&#20998;&#37197;&#21040;&#25152;&#26377;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#30697;&#38453;&#19978;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#26435;&#37325;&#21442;&#25968;&#30340;&#19981;&#21516;&#37325;&#35201;&#24615;&#12290;&#32467;&#26524;&#65292;&#24494;&#35843;&#30340;&#34920;&#29616;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaLoRA&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#33258;&#36866;&#24212;&#20998;&#37197;&#26435;&#37325;&#30697;&#38453;&#30340;&#21442;&#25968;&#39044;&#31639;&#12290;&#29305;&#21035;&#22320;&#65292;AdaLoRA&#23558;&#22686;&#37327;&#26356;&#26032;&#30340;&#21442;&#25968;&#21270;&#20026;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#24418;&#24335;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21098;&#26525;&#22855;&#24322;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;CVIB&#65289;&#65292;&#20197;&#20943;&#23569;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#35813;&#26694;&#26550;&#30001;&#19968;&#20010;&#21407;&#22987;&#32593;&#32476;&#21644;&#19968;&#20010;&#33258;&#21098;&#26525;&#32593;&#32476;&#32452;&#25104;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20002;&#24323;&#20102;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#20043;&#38388;&#30340;&#22810;&#20313;&#27169;&#24335;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02846</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#21644;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with Variational Information Bottleneck and Contrastive Learning. (arXiv:2303.02846v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;CVIB&#65289;&#65292;&#20197;&#20943;&#23569;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#35813;&#26694;&#26550;&#30001;&#19968;&#20010;&#21407;&#22987;&#32593;&#32476;&#21644;&#19968;&#20010;&#33258;&#21098;&#26525;&#32593;&#32476;&#32452;&#25104;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20002;&#24323;&#20102;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#20043;&#38388;&#30340;&#22810;&#20313;&#27169;&#24335;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel Contrastive Variational Information Bottleneck framework (CVIB) to reduce spurious correlations for aspect-based sentiment analysis (ABSA). The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning, which discards the superfluous patterns or spurious correlations between input features and prediction labels.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#30340;&#25991;&#29486;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28145;&#24230;&#27169;&#22411;&#36890;&#24120;&#22312;&#36755;&#20837;&#29305;&#24449;&#21644;&#36755;&#20986;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#36825;&#20250;&#32473;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#24102;&#26469;&#37325;&#22823;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;&#31216;&#20026;CVIB&#65289;&#65292;&#20197;&#20943;&#23569;ABSA&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;CVIB&#26694;&#26550;&#30001;&#19968;&#20010;&#21407;&#22987;&#32593;&#32476;&#21644;&#19968;&#20010;&#33258;&#21098;&#26525;&#32593;&#32476;&#32452;&#25104;&#65292;&#36825;&#20004;&#20010;&#32593;&#32476;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;VIB&#65289;&#21407;&#21017;&#20174;&#21407;&#22987;&#32593;&#32476;&#20013;&#23398;&#20064;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#19988;&#21387;&#32553;&#30340;&#32593;&#32476;&#65288;&#33258;&#21098;&#26525;&#32593;&#32476;&#65289;&#65292;&#35813;&#32593;&#32476;&#20002;&#24323;&#20102;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#26631;&#31614;&#20043;&#38388;&#30340;&#22810;&#20313;&#27169;&#24335;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#21098;&#26525;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23558;&#20004;&#20010;&#32593;&#32476;&#25289;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques have dominated the literature on aspect-based sentiment analysis (ABSA), yielding state-of-the-art results. However, these deep models generally suffer from spurious correlation problems between input features and output labels, which creates significant barriers to robustness and generalization capability. In this paper, we propose a novel Contrastive Variational Information Bottleneck framework (called CVIB) to reduce spurious correlations for ABSA. The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning. Concretely, we employ the Variational Information Bottleneck (VIB) principle to learn an informative and compressed network (self-pruned network) from the original network, which discards the superfluous patterns or spurious correlations between input features and prediction labels. Then, self-pruning contrastive learning is devised to pull together
&lt;/p&gt;</description></item></channel></rss>