<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65288;CADs&#65289;&#19982;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#20173;&#28982;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01270</link><description>&lt;p&gt;
&#20154;&#31867;&#36827;&#34892;&#26356;&#22909;&#30340;&#32534;&#36753;&#65306;&#34913;&#37327;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#22312;&#26377;&#23475;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection. (arXiv:2311.01270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65288;CADs&#65289;&#19982;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#20173;&#28982;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#31038;&#20250;&#35745;&#31639;&#20219;&#21153;&#20013;&#34987;&#20351;&#29992;&#65292;&#22914;&#26816;&#27979;&#24615;&#21035;&#27495;&#35270;&#12289;&#31181;&#26063;&#27495;&#35270;&#25110;&#20854;&#20182;&#20167;&#24680;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#30340;&#24037;&#20316;&#23581;&#35797;&#35299;&#20915;&#36825;&#20123;&#34394;&#20551;&#29305;&#24449;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65288;CADs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;CADs&#23545;&#29616;&#26377;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#26368;&#23567;&#25913;&#21160;&#24182;&#32763;&#36716;&#26631;&#31614;&#65307;&#22312;&#20854;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#29983;&#25104;CADs&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#33258;&#21160;&#21270;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;Polyjuice&#12289;ChatGPT&#21644;Flan-T5&#33258;&#21160;&#29983;&#25104;CADs&#65292;&#24182;&#19982;&#25163;&#21160;&#29983;&#25104;&#30340;CADs&#36827;&#34892;&#27604;&#36739;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#39046;&#22495;&#22806;&#27979;&#35797;&#38598;&#19978;&#27979;&#35797;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#25163;&#21160;CADs&#20173;&#28982;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. However, manually generating CADs can be time-consuming and expensive. Hence in this work, we assess if this task can be automated using generative NLP models. We automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate their usefulness in improving model robustness compared to manually-generated CADs. By testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual CADs are still the most e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.20246</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#20013;&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#35265;&#35299;&#19982;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#24182;&#35757;&#32451;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#32763;&#35793;&#26500;&#24314;&#20102;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#21457;&#29616;&#22312;&#22810;&#35821;&#35328;&#35757;&#32451;&#20013;&#65292;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#20197;&#21450;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#36866;&#29992;&#20110;&#21333;&#35821;&#35328;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#30340;&#24378;&#22823;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#20445;&#25345;&#25928;&#26524;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#21644;&#35757;&#32451;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#65288;xMR&#65289;LLM&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21033;&#29992;&#32763;&#35793;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#21253;&#21547;&#21313;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25968;&#23398;&#25512;&#29702;&#25351;&#23548;&#25968;&#25454;&#38598;MGSM8KInstruct&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;xMR&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;xMR LLMs&#65292;&#34987;&#21629;&#21517;&#20026;MathOctopus&#65292;&#22312;&#20960;&#27425;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#24320;&#28304;LLMs&#21644;ChatGPT&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MathOctopus-13B&#22312;MGSM&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;47.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;ChatGPT&#30340;46.3%&#12290;&#38500;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#20174;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#20013;&#21457;&#29616;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#35266;&#23519;&#21644;&#35265;&#35299;&#65306;&#65288;1&#65289;&#22312;&#22810;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26368;&#22909;&#23558;&#30446;&#26631;&#35821;&#35328;&#30340;&#32763;&#35793;&#19982;&#21407;&#22987;&#35821;&#35328;&#30340;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290; &#65288;2&#65289;&#20132;&#26367;&#35757;&#32451;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20030;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290; &#65288;3&#65289;&#27169;&#22411;&#23545;&#20110;&#20302;&#39057;&#35789;&#21644;&#38271;&#21477;&#23376;&#30340;&#22788;&#29702;&#26159;&#25361;&#25112;&#30340;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
&lt;/p&gt;</description></item><item><title>CodeChain&#26159;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08992</link><description>&lt;p&gt;
CodeChain: &#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#23454;&#29616;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules. (arXiv:2310.08992v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08992
&lt;/p&gt;
&lt;p&gt;
CodeChain&#26159;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#35299;&#20915;&#31616;&#21333;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#29087;&#32451;&#65292;&#27604;&#22914;&#22312;HumanEval&#25110;MBPP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#21644;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#32534;&#31243;&#20219;&#21153;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#20316;&#20026;&#25972;&#20307;&#20195;&#30721;&#22359;&#32780;&#19981;&#26159;&#23558;&#20854;&#20998;&#35299;&#20026;&#36923;&#36753;&#23376;&#20219;&#21153;&#21644;&#23376;&#27169;&#22359;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26377;&#32463;&#39564;&#30340;&#31243;&#24207;&#21592;&#26412;&#33021;&#22320;&#32534;&#20889;&#20855;&#26377;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22359;&#21270;&#20195;&#30721;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#36890;&#24120;&#20250;&#37325;&#22797;&#20351;&#29992;&#20043;&#21069;&#24320;&#21457;&#30340;&#27169;&#22359;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeChain&#65292;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CodeChain&#39318;&#20808;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25552;&#31034;&#25351;&#23548;LLM&#29983;&#25104;&#27169;&#22359;&#21270;&#20195;&#30721;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#20004;&#20010;&#27493;&#39588;&#23454;&#26045;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#65306;1&#65289;&#39069;&#22806;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08659</link><description>&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#24182;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;LoRA&#31934;&#35843;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21516;&#26102;&#24212;&#29992;&#37327;&#21270;&#21644;LoRA&#31934;&#35843;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#35266;&#23519;&#21040;&#23436;&#25972;&#31934;&#35843;&#21644;&#37327;&#21270;&#21152;LoRA&#31934;&#35843;&#26041;&#27861;&#20043;&#38388;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoftQ&#65288;LoRA-Fine-Tuning-aware Quantization&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#26469;&#36827;&#34892;LoRA&#31934;&#35843;&#12290;&#36825;&#31181;&#21021;&#22987;&#21270;&#20943;&#36731;&#20102;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#28436;&#36827;&#21382;&#31243;&#65292;&#21253;&#25324;&#26089;&#26399;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24341;&#20837;&#65292;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#20197;&#35299;&#20915;&#20559;&#35265;&#21644;&#26292;&#38706;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#36824;&#35752;&#35770;&#20102;&#24494;&#35843;&#31574;&#30053;&#12289;&#25511;&#21046;&#20195;&#30721;&#21644;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#30340;&#37325;&#22823;&#36129;&#29486;&#65292;&#20197;&#21450;&#20844;&#24179;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04438</link><description>&lt;p&gt;
&#19968;&#20221;&#20851;&#20110;&#25552;&#31034;&#24037;&#31243;&#30340;&#31616;&#35201;&#21382;&#21490;: &#21033;&#29992;&#35821;&#35328;&#27169;&#22411; (arXiv:2310.04438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
A Brief History of Prompt: Leveraging Language Models. (arXiv:2310.04438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04438
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#28436;&#36827;&#21382;&#31243;&#65292;&#21253;&#25324;&#26089;&#26399;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24341;&#20837;&#65292;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#20197;&#35299;&#20915;&#20559;&#35265;&#21644;&#26292;&#38706;&#20559;&#24046;&#31561;&#38382;&#39064;&#12290;&#36824;&#35752;&#35770;&#20102;&#24494;&#35843;&#31574;&#30053;&#12289;&#25511;&#21046;&#20195;&#30721;&#21644;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#30340;&#37325;&#22823;&#36129;&#29486;&#65292;&#20197;&#21450;&#20844;&#24179;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#21644;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#25552;&#31034;&#24037;&#31243;&#21644;&#29983;&#25104;&#30340;&#28436;&#36827;&#21382;&#31243;&#12290;&#20174;&#26089;&#26399;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#24320;&#22987;&#65292;&#25105;&#20204;&#36861;&#28335;&#20102;&#36825;&#20123;&#24180;&#26469;&#22609;&#36896;&#25552;&#31034;&#24037;&#31243;&#30340;&#20851;&#38190;&#21457;&#23637;&#12290;2015&#24180;&#24341;&#20837;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#21487;&#25511;&#24615;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36827;&#27493;&#12290;&#38543;&#21518;&#22312;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#26041;&#38754;&#30340;&#31361;&#30772;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25552;&#31034;&#24037;&#31243;&#65292;&#35299;&#20915;&#20102;&#26292;&#38706;&#20559;&#24046;&#21644;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#37325;&#28857;&#32771;&#23519;&#20102;2018&#24180;&#21644;2019&#24180;&#30340;&#37325;&#22823;&#36129;&#29486;&#65292;&#38598;&#20013;&#22312;&#24494;&#35843;&#31574;&#30053;&#12289;&#25511;&#21046;&#20195;&#30721;&#21644;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#19978;&#12290;&#26412;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#20197;&#21450;&#20302;&#36164;&#28304;&#36866;&#24212;&#30340;&#26085;&#30410;&#37325;&#35201;&#24615;&#12290;&#22312;2020&#24180;&#21644;2021&#24180;&#65292;&#19978;&#19979;&#25991;&#25552;&#31034;&#21644;&#36801;&#31227;&#23398;&#20064;&#21464;&#24471;&#31361;&#20986;&#65292;&#32780;2022&#24180;&#21644;2023&#24180;&#35265;&#35777;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive exploration of the evolution of prompt engineering and generation in the field of natural language processing (NLP). Starting from the early language models and information retrieval systems, we trace the key developments that have shaped prompt engineering over the years. The introduction of attention mechanisms in 2015 revolutionized language understanding, leading to advancements in controllability and context-awareness. Subsequent breakthroughs in reinforcement learning techniques further enhanced prompt engineering, addressing issues like exposure bias and biases in generated text. We examine the significant contributions in 2018 and 2019, focusing on fine-tuning strategies, control codes, and template-based generation. The paper also discusses the growing importance of fairness, human-AI collaboration, and low-resource adaptation. In 2020 and 2021, contextual prompting and transfer learning gained prominence, while 2022 and 2023 witnessed the e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22797;&#26434;&#25512;&#29702;&#12289;&#23545;&#35805;&#12289;&#22270;&#20687;&#25551;&#36848;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#23454;&#39564;&#65292;&#20026;&#23558;&#22810;&#27169;&#24577;&#33021;&#21147;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.03211</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Performance of Multimodal Language Models. (arXiv:2310.03211v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#22797;&#26434;&#25512;&#29702;&#12289;&#23545;&#35805;&#12289;&#22270;&#20687;&#25551;&#36848;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#23454;&#39564;&#65292;&#20026;&#23558;&#22810;&#27169;&#24577;&#33021;&#21147;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29420;&#31435;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#36890;&#36807;&#27169;&#22411;&#23233;&#25509;&#30340;&#26041;&#24335;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21518;&#65292;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26377;&#26395;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#19981;&#21516;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#12289;&#23545;&#35805;&#12289;&#22270;&#20687;&#25551;&#36848;&#12289;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#20108;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#23558;&#22810;&#27169;&#24577;&#33021;&#21147;&#34701;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#25351;&#23548;&#26550;&#26500;&#36873;&#25321;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#27809;&#26377;&#36275;&#22815;&#22320;&#35299;&#20915;&#22810;&#26679;&#21270;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) have demonstrated promising zero-shot generalization capabilities across various downstream tasks. Recent research has introduced multimodal capabilities to LLMs by integrating independently pretrained vision encoders through model grafting. These multimodal variants undergo instruction tuning, similar to LLMs, enabling effective zero-shot generalization for multimodal tasks. This study conducts a comparative analysis of different multimodal instruction tuning approaches and evaluates their performance across a range of tasks, including complex reasoning, conversation, image captioning, multiple-choice questions (MCQs), and binary classification. Through rigorous benchmarking and ablation experiments, we reveal key insights for guiding architectural choices when incorporating multimodal capabilities into LLMs. However, current approaches have limitations; they do not sufficiently address the need for a diverse multimodal instruction datase
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;PCA-EVAL&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;HOLMES&#65292;&#20197;&#25552;&#39640;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2310.02071</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond. (arXiv:2310.02071v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;PCA-EVAL&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;HOLMES&#65292;&#20197;&#25552;&#39640;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#25913;&#36827;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#30001;&#20110;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20687;GPT4-Vision&#36825;&#26679;&#30340;MLLM&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;MLLMs&#33021;&#21542;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#22788;&#29702;&#20855;&#36523;&#20915;&#31574;&#65292;&#24182;&#19988;LLMs&#21644;MLLMs&#20043;&#38388;&#30340;&#21327;&#20316;&#26159;&#21542;&#33021;&#22686;&#24378;&#20915;&#31574;&#33021;&#21147;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;PCA-EVAL&#30340;&#26032;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#20174;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#34892;&#21160;&#30340;&#35282;&#24230;&#35780;&#20272;&#20855;&#36523;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOLMES&#65292;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#21033;&#29992;MLLMs&#21644;APIs&#33719;&#21462;&#22810;&#27169;&#24577;&#20449;&#24687;&#20197;&#36827;&#34892;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#27604;&#36739;&#20102;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#21644;HOLMES&#65292;&#24182;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#12290;&#24403;&#21069;AI&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#21106;&#30340;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2310.01837</link><description>&lt;p&gt;
&#25193;&#23637;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation. (arXiv:2310.01837v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#22522;&#20110;CAM&#30340;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#12290;&#24403;&#21069;AI&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#19978;&#35757;&#32451;&#26102;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#35299;&#37322;&#22270;&#20687;&#20998;&#21106;&#30340;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#26080;&#27861;&#23545;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#12289;&#25552;&#21462;&#30340;&#29305;&#24449;&#21644;&#39044;&#27979;/&#25512;&#29702;&#25805;&#20316;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#29289;&#29702;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21482;&#33021;&#34987;&#35270;&#20026;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#19987;&#23478;&#38656;&#35201;&#24110;&#21161;&#29702;&#35299;AI&#27169;&#22411;&#30340;&#22797;&#26434;&#34892;&#20026;&#21644;&#22522;&#30784;&#20915;&#31574;&#36807;&#31243;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#30830;&#20445;AI&#27169;&#22411;&#31283;&#20581;&#12289;&#23454;&#29992;&#21644;&#21487;&#20449;&#36182;&#37096;&#32626;&#30340;&#25163;&#27573;&#12290;&#24050;&#26377;&#19968;&#20123;XAI&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#32780;&#23545;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#35299;&#37322;&#21017;&#22522;&#26412;&#19978;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;&#26368;&#26032;&#30340;XAI&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#20351;&#20854;&#36866;&#29992;&#20110;&#22810;&#31867;&#22270;&#20687;&#20998;&#21106;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#20854;&#20013;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#20013;&#30340;&#24314;&#31569;&#29289;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current AI-based methods do not provide comprehensible physical interpretations of the utilized data, extracted features, and predictions/inference operations. As a result, deep learning models trained using high-resolution satellite imagery lack transparency and explainability and can be merely seen as a black box, which limits their wide-level adoption. Experts need help understanding the complex behavior of AI models and the underlying decision-making process. The explainable artificial intelligence (XAI) field is an emerging field providing means for robust, practical, and trustworthy deployment of AI models. Several XAI techniques have been proposed for image classification tasks, whereas the interpretation of image segmentation remains largely unexplored. This paper offers to bridge this gap by adapting the recent XAI classification algorithms and making them usable for muti-class image segmentation, where we mainly focus on buildings' segmentation from high-resolution satellite 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;</title><link>http://arxiv.org/abs/2309.02705</link><description>&lt;p&gt;
&#35777;&#26126;LLM&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#23433;&#20840;&#65292;&#20844;&#24320;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#8220;&#27169;&#22411;&#23545;&#40784;&#8221;&#38450;&#25252;&#25514;&#26045;&#12290;&#19968;&#20010;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23433;&#20840;&#25514;&#26045;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#25932;&#23545;&#25552;&#31034;&#21253;&#21547;&#24694;&#24847;&#35774;&#35745;&#30340;&#26631;&#35760;&#24207;&#21015;&#65292;&#20197;&#35268;&#36991;&#27169;&#22411;&#30340;&#23433;&#20840;&#38450;&#25252;&#24182;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#31532;&#19968;&#20010;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#12290;&#25105;&#20204;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#12290;&#22914;&#26524;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#27979;&#21040;&#20219;&#20309;&#23376;&#24207;&#21015;&#25110;&#36755;&#20837;&#25552;&#31034;&#26377;&#23475;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#23558;&#23558;&#36755;&#20837;&#25552;&#31034;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#36825;&#20445;&#35777;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#22823;&#23567;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#30340;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#20063;&#23558;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#23545;&#25239;&#19977;&#31181;&#25915;&#20987;&#27169;&#24335;&#65306;i)&#25932;&#23545;&#21518;&#32512;&#65292;&#21363;&#38468;&#21152;&#25932;&#23545;&#24207;&#21015;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.06985</link><description>&lt;p&gt;
&#36808;&#21521;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06985
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#30693;&#35782;&#22270;&#26469;&#22635;&#20805;&#36890;&#29992;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22635;&#20805;&#36890;&#29992;&#24037;&#31243;&#35774;&#35745;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19987;&#21033;&#25991;&#20214;&#20013;&#25552;&#21462;head entity :: relationship :: tail entity&#24418;&#24335;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20107;&#23454;&#21487;&#20197;&#22312;&#19987;&#21033;&#25991;&#20214;&#20869;&#37096;&#21644;&#36328;&#25991;&#20214;&#20043;&#38388;&#32452;&#21512;&#24418;&#25104;&#30693;&#35782;&#22270;&#65292;&#29992;&#20316;&#34920;&#31034;&#21644;&#23384;&#20648;&#35774;&#35745;&#30693;&#35782;&#30340;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#24037;&#31243;&#35774;&#35745;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#26469;&#22635;&#20805;&#32479;&#35745;&#36817;&#20284;&#32780;&#38750;&#20107;&#23454;&#30340;&#19977;&#20803;&#32452;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#21477;&#23376;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#22312;&#30830;&#23450;&#20102;&#19968;&#23545;&#23454;&#20307;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#21478;&#19968;&#20010;&#26631;&#35760;&#22120;&#26469;&#35782;&#21035;&#29305;&#23450;&#34920;&#31034;&#36825;&#23545;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#31995;&#26631;&#35760;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20123;&#26631;&#35760;&#22120;&#65292;&#25105;&#20204;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;44,227&#20010;&#21477;&#23376;&#21644;&#30456;&#24212;&#20107;&#23454;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#36890;&#24120;&#25512;&#33616;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20854;&#20013;&#25105;&#20204;&#39044;.
&lt;/p&gt;
&lt;p&gt;
Aiming to populate generalizable engineering design knowledge, we propose a method to extract facts of the form head entity :: relationship :: tail entity from sentences found in patent documents. These facts could be combined within and across patent documents to form knowledge graphs that serve as schemes for representing as well as storing design knowledge. Existing methods in engineering design literature often utilise a set of predefined relationships to populate triples that are statistical approximations rather than facts. In our method, we train a tagger to identify both entities and relationships from a sentence. Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair. For training these taggers, we manually construct a dataset of 44,227 sentences and corresponding facts. We also compare the performance of the method against typically recommended approaches, wherein, we pre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#29109;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#26469;&#25552;&#39640;&#38646;-shot&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14726</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#21449;&#29109;&#24046;&#24322;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Demonstration Selection with Cross Entropy Difference. (arXiv:2305.14726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#29109;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#26469;&#25552;&#39640;&#38646;-shot&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#21033;&#29992;&#19978;&#19979;&#25991;&#28436;&#31034;&#26469;&#25552;&#39640;&#38646;-shot&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#26368;&#20339;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#22240;&#25152;&#36873;&#31034;&#20363;&#32780;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#29109;&#24046;&#24322;&#65288;CED&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#19978;&#19979;&#25991;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65306;&#22312;&#26377;&#38480;&#35843;&#25972;&#20102;&#36825;&#20123;&#28436;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#26377;&#25928;&#24615;&#19982;&#27979;&#35797;&#31034;&#20363;&#30340;&#22256;&#24785;&#24230;&#21576;&#36127;&#30456;&#20851;&#12290;&#25105;&#20204;&#21033;&#29992;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#23567;&#22411;&#27169;&#22411;&#65292;&#20197;&#35745;&#31639;&#27979;&#35797;&#31034;&#20363;&#21644;&#27599;&#20010;&#20505;&#36873;&#19978;&#19979;&#25991;&#28436;&#31034;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#24046;&#24322;&#12290;&#35813;&#25351;&#26631;&#29992;&#20110;&#20026;&#27599;&#20010;&#27979;&#35797;&#36755;&#20837;&#29420;&#31435;&#22320;&#25490;&#21517;&#21644;&#36873;&#25321;&#19978;&#19979;&#25991;&#28436;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#28151;&#21512;&#22495;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#32467;&#21512;&#20102;8&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20195;&#34920;4&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;CED&#22312;&#19978;&#19979;&#25991;&#28436;&#31034;&#36873;&#25321;&#26041;&#38754;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can use in-context demonstrations to improve performance on zero-shot tasks. However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples. We present a cross-entropy difference (CED) method for selecting in-context demonstrations. Our method is based on the observation that the effectiveness of in-context demonstrations negatively correlates with the perplexity of the test example by a language model that was finetuned on that demonstration. We utilize parameter efficient finetuning to train small models on training data that are used for computing the cross-entropy difference between a test example and every candidate in-context demonstration. This metric is used to rank and select in-context demonstrations independently for each test input. We evaluate our method on a mix-domain dataset that combines 8 benchmarks, representing 4 text generation tasks, showing that CED for in-co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#22312;&#27604;&#36739;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25991;&#26412;&#23454;&#20307;&#27604;&#36739;&#25968;&#25454;&#30340;&#26041;&#27861;&#21644;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14457</link><description>&lt;p&gt;
&#20026;&#27604;&#36739;&#25512;&#29702;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-training Language Models for Comparative Reasoning. (arXiv:2305.14457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#22312;&#27604;&#36739;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25991;&#26412;&#23454;&#20307;&#27604;&#36739;&#25968;&#25454;&#30340;&#26041;&#27861;&#21644;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20854;&#22312;&#25991;&#26412;&#27604;&#36739;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21487;&#25193;&#23637;&#30340;&#29992;&#20110;&#25910;&#38598;&#22522;&#20110;&#25991;&#26412;&#23454;&#20307;&#27604;&#36739;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19977;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#27604;&#36739;&#38382;&#31572;&#12289;&#38382;&#21477;&#29983;&#25104;&#21644;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#22823;&#22823;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#26412;&#24037;&#20316;&#36824;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#27604;&#36739;&#25512;&#29702;&#32508;&#21512;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel framework to pre-train language models for enhancing their abilities of comparative reasoning over texts. While recent research has developed models for NLP tasks that require comparative reasoning, they suffer from costly manual data labeling and limited generalizability to different tasks. Our approach involves a scalable method for collecting data for text-based entity comparison, which leverages both structured and unstructured data, and the design of three novel pre-training tasks. Evaluation on a range of downstream tasks including comparative question answering, question generation, and summarization shows that our pre-training framework significantly improves the comparative reasoning abilities of language models, especially under low-resource conditions. This work also releases the first integrated benchmark for comparative reasoning over texts.
&lt;/p&gt;</description></item></channel></rss>