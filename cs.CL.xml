<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21644;&#26356;&#26032;&#26089;&#20572;&#21644;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;&#30340;&#26041;&#27861;&#65292;&#20174;&#20559;&#24046;-&#26041;&#24046;&#30340;&#35282;&#24230;&#24341;&#23548;ASR&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#65292;&#21033;&#29992;&#36817;&#20284;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#26469;&#25351;&#23548;&#26089;&#20572;&#21644;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;&#65292;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;CER&#12290;</title><link>http://arxiv.org/abs/2308.02870</link><description>&lt;p&gt;
ApproBiVT: &#20351;&#29992;&#36817;&#20284;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#26469;&#24341;&#23548;ASR&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#30340;&#26089;&#20572;&#21644;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;
&lt;/p&gt;
&lt;p&gt;
ApproBiVT: Lead ASR Models to Generalize Better Using Approximated Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging. (arXiv:2308.02870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#21644;&#26356;&#26032;&#26089;&#20572;&#21644;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;&#30340;&#26041;&#27861;&#65292;&#20174;&#20559;&#24046;-&#26041;&#24046;&#30340;&#35282;&#24230;&#24341;&#23548;ASR&#27169;&#22411;&#26356;&#22909;&#22320;&#27867;&#21270;&#65292;&#21033;&#29992;&#36817;&#20284;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#26469;&#25351;&#23548;&#26089;&#20572;&#21644;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;&#65292;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;CER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#35757;&#32451;&#36890;&#24120;&#26159;&#22312;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#21516;&#26102;&#20381;&#36182;&#20110;&#39564;&#35777;&#38598;&#36890;&#36807;&#26089;&#20572;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#26368;&#21518;&#20960;&#20010;&#26816;&#26597;&#28857;&#25110;&#26368;&#20302;&#39564;&#35777;&#25439;&#22833;&#30340;&#24179;&#22343;&#20540;&#24471;&#21040;&#26368;&#32456;&#27169;&#22411;&#12290;&#26412;&#25991;&#20174;&#20559;&#24046;-&#26041;&#24046;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#21644;&#26356;&#26032;&#20102;&#26089;&#20572;&#21644;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;&#30340;&#26041;&#27861;&#12290;&#29702;&#35770;&#19978;&#65292;&#20559;&#24046;&#21644;&#26041;&#24046;&#20998;&#21035;&#34920;&#31034;&#27169;&#22411;&#30340;&#25311;&#21512;&#21644;&#21464;&#24322;&#24615;&#65292;&#23427;&#20204;&#30340;&#26435;&#34913;&#20915;&#23450;&#20102;&#25972;&#20307;&#27867;&#21270;&#35823;&#24046;&#12290;&#20294;&#26159;&#65292;&#31934;&#30830;&#35780;&#20272;&#23427;&#20204;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#25439;&#22833;&#21644;&#39564;&#35777;&#25439;&#22833;&#35270;&#20026;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#20195;&#29702;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#30340;&#26435;&#34913;&#65292;&#21363;&#36817;&#20284;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65288;ApproBiVT&#65289;&#65292;&#26469;&#24341;&#23548;&#26089;&#20572;&#21644;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;&#12290;&#22312;&#20351;&#29992;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;AISHELL-1&#21644;...
&lt;/p&gt;
&lt;p&gt;
The conventional recipe for Automatic Speech Recognition (ASR) models is to 1) train multiple checkpoints on a training set while relying on a validation set to prevent overfitting using early stopping and 2) average several last checkpoints or that of the lowest validation losses to obtain the final model. In this paper, we rethink and update the early stopping and checkpoint averaging from the perspective of the bias-variance tradeoff. Theoretically, the bias and variance represent the fitness and variability of a model and the tradeoff of them determines the overall generalization error. But, it's impractical to evaluate them precisely. As an alternative, we take the training loss and validation loss as proxies of bias and variance and guide the early stopping and checkpoint averaging using their tradeoff, namely an Approximated Bias-Variance Tradeoff (ApproBiVT). When evaluating with advanced ASR models, our recipe provides 2.5%-3.7% and 3.1%-4.6% CER reduction on the AISHELL-1 and
&lt;/p&gt;</description></item><item><title>EduChat&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#26088;&#22312;&#25903;&#25345;&#20010;&#24615;&#21270;&#12289;&#20844;&#24179;&#21644;&#26377;&#21516;&#24773;&#24515;&#30340;&#26234;&#33021;&#25945;&#32946;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#25945;&#32946;&#21151;&#33021;&#65292;&#22914;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#12289;&#25991;&#31456;&#35780;&#20272;&#12289;&#33487;&#26684;&#25289;&#24213;&#24335;&#25945;&#23398;&#21644;&#24773;&#24863;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2308.02773</link><description>&lt;p&gt;
EduChat:&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education. (arXiv:2308.02773v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02773
&lt;/p&gt;
&lt;p&gt;
EduChat&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#26088;&#22312;&#25903;&#25345;&#20010;&#24615;&#21270;&#12289;&#20844;&#24179;&#21644;&#26377;&#21516;&#24773;&#24515;&#30340;&#26234;&#33021;&#25945;&#32946;&#65292;&#24182;&#25552;&#20379;&#21508;&#31181;&#25945;&#32946;&#21151;&#33021;&#65292;&#22914;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#12289;&#25991;&#31456;&#35780;&#20272;&#12289;&#33487;&#26684;&#25289;&#24213;&#24335;&#25945;&#23398;&#21644;&#24773;&#24863;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
EduChat&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26234;&#33021;&#25945;&#32946;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#20854;&#30446;&#26631;&#26159;&#25903;&#25345;&#20010;&#24615;&#21270;&#12289;&#20844;&#24179;&#21644;&#26377;&#21516;&#24773;&#24515;&#30340;&#26234;&#33021;&#25945;&#32946;&#65292;&#20026;&#25945;&#24072;&#12289;&#23398;&#29983;&#21644;&#23478;&#38271;&#25552;&#20379;&#26381;&#21153;&#12290;&#22312;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#29702;&#35770;&#30340;&#25351;&#23548;&#19979;&#65292;&#23427;&#36827;&#19968;&#27493;&#21152;&#24378;&#20102;&#22522;&#26412;&#30340;LLM&#30340;&#25945;&#32946;&#21151;&#33021;&#65292;&#22914;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#65292;&#25991;&#31456;&#35780;&#20272;&#65292;&#33487;&#26684;&#25289;&#24213;&#24335;&#25945;&#23398;&#21644;&#24773;&#24863;&#25903;&#25345;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#25945;&#32946;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#26469;&#23398;&#20064;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#22312;&#35774;&#35745;&#30340;&#31995;&#32479;&#25552;&#31034;&#21644;&#25351;&#20196;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#28608;&#21457;&#21508;&#31181;&#25216;&#33021;&#12290;&#30446;&#21069;&#65292;EduChat&#20316;&#20026;&#19968;&#20010;&#24320;&#28304;&#39033;&#30446;&#22312;&#32447;&#21487;&#29992;&#65292;&#20854;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#21487;&#20197;&#22312;&#24179;&#21488;&#19978;&#33719;&#21462;&#65288;&#20363;&#22914;&#65292;GitHub https://github.com/icalk-nlp/EduChat&#65292;Hugging Face https://huggingface.co/ecnu-icalk&#65289;&#12290;&#25105;&#20204;&#36824;&#20934;&#22791;&#20102;&#19968;&#20010;&#22312;&#32447;&#28436;&#31034;&#20854;&#21151;&#33021;&#30340;&#35270;&#39057;&#65288;https://vimeo.com/851004454&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
EduChat (https://www.educhat.top/) is a large-scale language model (LLM)-based chatbot system in the education domain. Its goal is to support personalized, fair, and compassionate intelligent education, serving teachers, students, and parents. Guided by theories from psychology and education, it further strengthens educational functions such as open question answering, essay assessment, Socratic teaching, and emotional support based on the existing basic LLMs. Particularly, we learn domain-specific knowledge by pre-training on the educational corpus and stimulate various skills with tool use by fine-tuning on designed system prompts and instructions. Currently, EduChat is available online as an open-source project, with its code, data, and model parameters available on platforms (e.g., GitHub https://github.com/icalk-nlp/EduChat, Hugging Face https://huggingface.co/ecnu-icalk ). We also prepare a demonstration of its capabilities online (https://vimeo.com/851004454). This initiative ai
&lt;/p&gt;</description></item><item><title>&#20803;-Tsallis-&#29109;&#26368;&#23567;&#21270;&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#30446;&#26631;&#22495;&#19978;&#30340;&#23454;&#20363;&#33258;&#36866;&#24212;Tsallis&#29109;&#26469;&#35299;&#20915;&#33258;&#35757;&#32451;&#22312;&#22823;&#39046;&#22495;&#36716;&#31227;&#26102;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02746</link><description>&lt;p&gt;
&#20803;-Tsallis-&#29109;&#26368;&#23567;&#21270;&#65306;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#25991;&#26412;&#20998;&#31867;&#33258;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification. (arXiv:2308.02746v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02746
&lt;/p&gt;
&lt;p&gt;
&#20803;-Tsallis-&#29109;&#26368;&#23567;&#21270;&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#30446;&#26631;&#22495;&#19978;&#30340;&#23454;&#20363;&#33258;&#36866;&#24212;Tsallis&#29109;&#26469;&#35299;&#20915;&#33258;&#35757;&#32451;&#22312;&#22823;&#39046;&#22495;&#36716;&#31227;&#26102;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#36328;&#39046;&#22495;&#36866;&#24212;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#33258;&#35757;&#32451;&#36890;&#36807;&#20174;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#20013;&#29983;&#25104;&#20266;&#26679;&#26412;&#65292;&#24182;&#36845;&#20195;&#22312;&#20266;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#22312;&#28304;&#22495;&#19978;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#22312;&#30446;&#26631;&#22495;&#19978;&#26368;&#23567;&#21270;Gibbs&#29109;&#12290;&#28982;&#32780;&#65292;Gibbs&#29109;&#23545;&#39044;&#27979;&#35823;&#24046;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#24403;&#39046;&#22495;&#36716;&#31227;&#36739;&#22823;&#26102;&#65292;&#33258;&#35757;&#32451;&#24448;&#24448;&#20250;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20803;-Tsallis-&#29109;&#26368;&#23567;&#21270;&#65288;MTEM&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;&#30446;&#26631;&#22495;&#19978;&#30340;&#23454;&#20363;&#33258;&#36866;&#24212;Tsallis&#29109;&#12290;&#20026;&#20102;&#38477;&#20302;MTEM&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#25216;&#26415;&#26469;&#36817;&#20284;&#20803;&#23398;&#20064;&#20013;&#28041;&#21450;&#30340;&#20108;&#38454;&#23548;&#25968;&#12290;&#20026;&#20102;&#39640;&#25928;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36864;&#28779;&#37319;&#26679;&#26426;&#21046;&#26469;&#25506;&#32034;&#27169;&#22411;&#30340;&#39044;&#27979;&#27010;&#29575;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;m&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Text classification is a fundamental task for natural language processing, and adapting text classification models across domains has broad applications. Self-training generates pseudo-examples from the model's predictions and iteratively trains on the pseudo-examples, i.e., minimizes the loss on the source domain and the Gibbs entropy on the target domain. However, Gibbs entropy is sensitive to prediction errors, and thus, self-training tends to fail when the domain shift is large. In this paper, we propose Meta-Tsallis Entropy minimization (MTEM), which applies a meta-learning algorithm to optimize the instance adaptive Tsallis entropy on the target domain. To reduce the computation cost of MTEM, we propose an approximation technique to approximate the Second-order derivation involved in the meta-learning. To efficiently generate pseudo labels, we propose an annealing sampling mechanism for exploring the model's prediction probability. Theoretically, we prove the convergence of the m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#32479;&#21644;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#25928;&#26524;&#65292;&#21457;&#29616;&#20256;&#32479;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#19978;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#20294;&#26368;&#20339;&#27169;&#22411;&#30340;&#36873;&#25321;&#21462;&#20915;&#20110;&#20855;&#20307;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.02727</link><description>&lt;p&gt;
SOTA&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#25928;&#26524;&#22914;&#20309;
&lt;/p&gt;
&lt;p&gt;
How Good Are SOTA Fake News Detectors. (arXiv:2308.02727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#32479;&#21644;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#25928;&#26524;&#65292;&#21457;&#29616;&#20256;&#32479;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#19978;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#20294;&#26368;&#20339;&#27169;&#22411;&#30340;&#36873;&#25321;&#21462;&#20915;&#20110;&#20855;&#20307;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#33258;&#21160;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#21487;&#20197;&#38450;&#27490;&#34394;&#20551;&#38472;&#36848;&#22312;&#33719;&#24471;&#24456;&#22810;&#35266;&#28857;&#20043;&#21069;&#20256;&#25773;&#12290;&#33258;2016&#24180;&#32654;&#22269;&#24635;&#32479;&#36873;&#20030;&#20197;&#26469;&#65292;&#24050;&#32463;&#21019;&#24314;&#20102;&#20960;&#20010;&#23558;&#38472;&#36848;&#26631;&#35760;&#20026;&#21512;&#27861;&#25110;&#34394;&#20551;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20256;&#32479;&#21644;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#34913;&#37327;&#23427;&#20204;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#26368;&#36817;&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#20256;&#32479;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#20854;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#25968;&#25454;&#19978;&#65292;&#23613;&#31649;&#26368;&#20339;&#27169;&#22411;&#30340;&#36873;&#25321;&#21487;&#33021;&#21462;&#20915;&#20110;&#20855;&#20307;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic fake news detection with machine learning can prevent the dissemination of false statements before they gain many views. Several datasets labeling statements as legitimate or false have been created since the 2016 United States presidential election for the prospect of training machine learning models. We evaluate the robustness of both traditional and deep state-of-the-art models to gauge how well they may perform in the real world. We find that traditional models tend to generalize better to data outside the distribution it was trained on compared to more recently-developed large language models, though the best model to use may depend on the specific task at hand.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02618</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;GTFS: &#20174;&#25991;&#23383;&#21040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20132;&#36890;&#34892;&#25968;&#25454;&#21457;&#24067;&#26631;&#20934;General Transit Feed Specification&#65288;GTFS&#65289;&#26159;&#34920;&#26684;&#25968;&#25454;&#65292;&#20449;&#24687;&#20998;&#25955;&#22312;&#19981;&#21516;&#30340;&#25991;&#20214;&#20013;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#24037;&#20855;&#25110;&#21253;&#26469;&#26816;&#32034;&#20449;&#24687;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#36235;&#21183;&#20063;&#22312;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#30340;&#24819;&#27861;&#26159;&#30475;&#30475;&#24403;&#21069;&#24191;&#27867;&#37319;&#29992;&#30340;LLMs&#65288;ChatGPT&#65289;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20174;GTFS&#20013;&#26816;&#32034;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#27979;&#35797;ChatGPT&#65288;GPT-3.5&#65289;&#26159;&#21542;&#29702;&#35299;GTFS&#35268;&#33539;&#12290;GPT-3.5&#22312;&#25105;&#20204;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQ&#65289;&#20013;&#27491;&#30830;&#22238;&#31572;&#20102;77%&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#36807;&#28388;&#30340;GTFS&#25968;&#25454;&#38598;&#23545;LLM&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#12290;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#38646;-shot&#21644;&#31243;&#24207;&#21512;&#25104;&#12290;&#31243;&#24207;&#21512;&#25104;&#30340;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#31616;&#21333;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;90%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;40%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.02582</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#30340;&#27867;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#20174;&#35757;&#32451;&#38598;&#20013;&#25512;&#29702;&#20986;&#23569;&#37327;&#26679;&#26412;&#65292;&#20197;&#21512;&#25104;&#27599;&#20010;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#27979;&#35797;&#26597;&#35810;&#30340;&#36816;&#34892;&#26102;&#25552;&#31034;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#23436;&#20840;&#35206;&#30422;SQL&#23376;&#21477;&#12289;&#36816;&#31639;&#31526;&#21644;&#20989;&#25968;&#65292;&#24182;&#22312;&#20801;&#35768;&#30340;&#20196;&#29260;&#38271;&#24230;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#22823;&#39046;&#22495;&#35206;&#30422;&#12290;&#36825;&#26679;&#21487;&#20197;&#21512;&#25104;&#19968;&#20010;&#22266;&#23450;&#30340;&#36890;&#29992;&#25552;&#31034;&#65288;GP&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;NL&#27979;&#35797;&#26597;&#35810;&#20043;&#38388;&#20849;&#29992;&#30340;&#22810;&#26679;&#21270;&#26679;&#26412;&#38598;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#12290;&#25105;&#20204;&#36824;&#23558;GP&#33258;&#36866;&#24212;&#21040;&#30446;&#26631;&#25968;&#25454;&#24211;&#39046;&#22495;&#65288;DA-GP&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#27867;&#21270;&#65307;&#28982;&#21518;&#37319;&#29992;&#20998;&#35299;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#65288;LTMP-DA-GP&#65289;&#26469;&#22788;&#29702;&#36328;&#32452;&#21512;&#27867;&#21270;&#12290;LTMP-DA-GP&#30340;&#21512;&#25104;&#26159;&#31163;&#32447;&#20219;&#21153;&#65292;
&lt;/p&gt;
&lt;p&gt;
Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#22810;&#20010;&#36845;&#20195;&#20013;&#29983;&#25104;&#30340;&#21453;&#39304;&#35780;&#20998;&#20855;&#26377;&#39640;&#19968;&#33268;&#24615;&#65292;&#20869;&#23481;&#21644;&#25991;&#20307;&#35780;&#20998;&#20043;&#38388;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02575</link><description>&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20998;&#22120;&#21527;&#65311;&#35780;&#20272;GPT-4&#25991;&#26412;&#35780;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings. (arXiv:2308.02575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02575
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#22810;&#20010;&#36845;&#20195;&#20013;&#29983;&#25104;&#30340;&#21453;&#39304;&#35780;&#20998;&#20855;&#26377;&#39640;&#19968;&#33268;&#24615;&#65292;&#20869;&#23481;&#21644;&#25991;&#20307;&#35780;&#20998;&#20043;&#38388;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;OpenAI&#30340;GPT-4&#22312;&#22810;&#20010;&#36845;&#20195;&#12289;&#26102;&#38388;&#36328;&#24230;&#21644;&#25991;&#20307;&#21464;&#21270;&#20013;&#29983;&#25104;&#30340;&#21453;&#39304;&#35780;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#35813;&#27169;&#22411;&#26681;&#25454;&#20869;&#23481;&#21644;&#25991;&#20307;&#23545;&#23439;&#35266;&#32463;&#27982;&#23398;&#23398;&#31185;&#39046;&#22495;&#20869;&#30340;&#20219;&#21153;&#22238;&#31572;&#36827;&#34892;&#35780;&#20998;&#12290;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#35780;&#20998;&#30340;&#19968;&#33268;&#24615;&#12289;&#36845;&#20195;&#20043;&#38388;&#30340;&#35780;&#20998;&#30456;&#20851;&#24615;&#20197;&#21450;&#20869;&#23481;&#21644;&#25991;&#20307;&#35780;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19981;&#21516;&#26102;&#38388;&#36328;&#24230;&#30340;ICC&#20998;&#25968;&#22312;0.94&#21040;0.99&#20043;&#38388;&#65292;&#34920;&#26126;GPT-4&#33021;&#22815;&#22312;&#37325;&#22797;&#20219;&#21153;&#20013;&#29983;&#25104;&#19968;&#33268;&#30340;&#35780;&#20998;&#12290;&#20869;&#23481;&#21644;&#25991;&#20307;&#35780;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20026;0.87&#12290;&#24403;&#24212;&#29992;&#19981;&#24688;&#24403;&#30340;&#25991;&#20307;&#26102;&#65292;&#24179;&#22343;&#20869;&#23481;&#35780;&#20998;&#20445;&#25345;&#19981;&#21464;&#65292;&#32780;&#25991;&#20307;&#35780;&#20998;&#19979;&#38477;&#65292;&#36825;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#19968;&#33268;&#35780;&#20998;&#26041;&#38754;&#20855;&#26377;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the consistency of feedback ratings generated by OpenAI's GPT-4, a state-of-the-art artificial intelligence language model, across multiple iterations, time spans and stylistic variations. The model rated responses to tasks within the Higher Education (HE) subject domain of macroeconomics in terms of their content and style. Statistical analysis was conducted in order to learn more about the interrater reliability, consistency of the ratings across iterations and the correlation between ratings in terms of content and style. The results revealed a high interrater reliability with ICC scores ranging between 0.94 and 0.99 for different timespans, suggesting that GPT-4 is capable of generating consistent ratings across repetitions with a clear prompt. Style and content ratings show a high correlation of 0.87. When applying a non-adequate style the average content ratings remained constant, while style ratings decreased, which indicates that the large language model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BGA-MNER&#30340;&#21452;&#21521;&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#35821;&#20041;&#40511;&#27807;&#21644;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#38544;&#24335;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.02570</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#29983;&#25104;&#23545;&#40784;&#23398;&#20064;&#38544;&#24335;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;NER
&lt;/p&gt;
&lt;p&gt;
Learning Implicit Entity-object Relations by Bidirectional Generative Alignment for Multimodal NER. (arXiv:2308.02570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BGA-MNER&#30340;&#21452;&#21521;&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#35821;&#20041;&#40511;&#27807;&#21644;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#38544;&#24335;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(MNER)&#38754;&#20020;&#30340;&#25361;&#25112;&#20027;&#35201;&#26377;&#20004;&#26041;&#38754;: (1) &#24357;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;; (2) &#21305;&#37197;&#23454;&#20307;&#19982;&#22270;&#20687;&#20013;&#20854;&#20851;&#32852;&#30340;&#29289;&#20307;&#12290;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#38544;&#21547;&#30340;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#65292;&#22240;&#20026;&#32570;&#20047;&#30456;&#24212;&#30340;&#27880;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BGA-MNER&#30340;&#21452;&#21521;&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;BGA-MNER&#21253;&#25324;&#38024;&#23545;&#20004;&#31181;&#27169;&#24577;&#20013;&#30340;&#23454;&#20307;&#26174;&#33879;&#20869;&#23481;&#30340;\texttt{&#22270;&#20687;&#21040;&#25991;&#26412;}&#21644;\texttt{&#25991;&#26412;&#21040;&#22270;&#20687;}&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#20849;&#21516;&#20248;&#21270;&#21452;&#21521;&#37325;&#24314;&#30446;&#26631;&#26469;&#23545;&#40784;&#38544;&#21547;&#30340;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#65292;&#22312;&#30452;&#25509;&#32780;&#24378;&#22823;&#30340;&#32422;&#26463;&#19979;&#23454;&#29616;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36890;&#24120;&#21253;&#21547;&#19981;&#21305;&#37197;&#30340;&#32452;&#20214;&#65292;&#23545;&#20110;&#29983;&#25104;&#26469;&#35828;&#26159;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38454;&#27573;&#24615;&#25913;&#36827;&#30340;&#19978;&#19979;&#25991;&#37319;&#26679;&#22120;&#65292;&#29992;&#20110;&#25552;&#21462;&#21305;&#37197;&#30340;&#36328;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#29983;&#25104;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge posed by multimodal named entity recognition (MNER) is mainly two-fold: (1) bridging the semantic gap between text and image and (2) matching the entity with its associated object in image. Existing methods fail to capture the implicit entity-object relations, due to the lack of corresponding annotation. In this paper, we propose a bidirectional generative alignment method named BGA-MNER to tackle these issues. Our BGA-MNER consists of \texttt{image2text} and \texttt{text2image} generation with respect to entity-salient content in two modalities. It jointly optimizes the bidirectional reconstruction objectives, leading to aligning the implicit entity-object relations under such direct and powerful constraints. Furthermore, image-text pairs usually contain unmatched components which are noisy for generation. A stage-refined context sampler is proposed to extract the matched cross-modal content for generation. Extensive experiments on two benchmarks demonstrate that our met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;BioBERT-GRU&#26041;&#27861;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;SNP-&#29305;&#24449;&#20851;&#32852;&#65292;&#32463;&#36807;&#35780;&#20272;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20197;&#24448;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1-score&#12290;</title><link>http://arxiv.org/abs/2308.02569</link><description>&lt;p&gt;
&#22522;&#20110;BioBERT&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;SNP-&#29305;&#24449;&#20851;&#32852;&#30340;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
BioBERT Based SNP-traits Associations Extraction from Biomedical Literature. (arXiv:2308.02569v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;BioBERT-GRU&#26041;&#27861;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;SNP-&#29305;&#24449;&#20851;&#32852;&#65292;&#32463;&#36807;&#35780;&#20272;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20197;&#24448;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1-score&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25991;&#29486;&#20013;&#21253;&#21547;&#22823;&#37327;&#20449;&#24687;&#65292;&#20026;&#24320;&#21457;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#25552;&#20379;&#20102;&#26497;&#22909;&#30340;&#26426;&#20250;&#65292;&#20197;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#20851;&#31995;&#12290;&#37325;&#35201;&#30340;&#19968;&#31867;&#20449;&#24687;&#26159;&#21333;&#26680;&#33527;&#37240;&#22810;&#24577;&#24615;&#65288;SNP&#65289;&#19982;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BioBERT-GRU&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;SNP-&#29305;&#24449;&#20851;&#32852;&#12290;&#36890;&#36807;&#22312;SNPPhenA&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#24471;&#20986;&#32467;&#35770;&#65306;&#35813;&#26032;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;BioBERT-GRU&#23454;&#29616;&#20102;&#31934;&#24230;0.883&#65292;&#21484;&#22238;&#29575;0.882&#21644;F1-score 0.881&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific literature contains a considerable amount of information that provides an excellent opportunity for developing text mining methods to extract biomedical relationships. An important type of information is the relationship between singular nucleotide polymorphisms (SNP) and traits. In this paper, we present a BioBERT-GRU method to identify SNP- traits associations. Based on the evaluation of our method on the SNPPhenA dataset, it is concluded that this new method performs better than previous machine learning and deep learning based methods. BioBERT-GRU achieved the result a precision of 0.883, recall of 0.882 and F1-score of 0.881.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#25991;&#26412;&#22270;&#23398;&#20064;&#26041;&#27861;SimTeG&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#29305;&#24449;&#24037;&#31243;&#21644;&#27169;&#22411;&#35774;&#35745;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.02565</link><description>&lt;p&gt;
SimTeG: &#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#25991;&#26412;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimTeG: A Frustratingly Simple Approach Improves Textual Graph Learning. (arXiv:2308.02565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02565
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#25991;&#26412;&#22270;&#23398;&#20064;&#26041;&#27861;SimTeG&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#29305;&#24449;&#24037;&#31243;&#21644;&#27169;&#22411;&#35774;&#35745;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22270;&#65288;TGs&#65289;&#26159;&#25351;&#33410;&#28857;&#23545;&#24212;&#25991;&#26412;&#65288;&#21477;&#23376;&#25110;&#25991;&#26723;&#65289;&#30340;&#22270;&#24418;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;TGs&#30340;&#34920;&#31034;&#23398;&#20064;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#21644;&#65288;ii&#65289;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#22312;&#21518;&#32773;&#38454;&#27573;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#29616;&#26377;&#22270;&#24418;&#22522;&#20934;&#30340;&#21069;&#32773;&#38454;&#27573;&#20173;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#29305;&#24449;&#24037;&#31243;&#25216;&#26415;&#12290;&#26368;&#36817;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#21033;&#29992;LMs&#26469;&#20419;&#36827;TGs&#30340;&#23398;&#20064;&#65292;&#26041;&#27861;&#35201;&#20040;&#26159;&#22312;&#35745;&#31639;&#23494;&#38598;&#30340;&#26694;&#26550;&#20013;&#32852;&#21512;&#35757;&#32451;&#23427;&#20204;&#65288;&#21512;&#24182;&#20004;&#20010;&#38454;&#27573;&#65289;&#65292;&#35201;&#20040;&#26159;&#35774;&#35745;&#22797;&#26434;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#20219;&#21153;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65288;&#22686;&#24378;&#31532;&#19968;&#38454;&#27573;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SimTeG&#65292;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#25991;&#26412;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#19981;&#21019;&#26032;&#20110;&#26694;&#26550;&#12289;&#27169;&#22411;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual graphs (TGs) are graphs whose nodes correspond to text (sentences or documents), which are widely prevalent. The representation learning of TGs involves two stages: (i) unsupervised feature extraction and (ii) supervised graph representation learning. In recent years, extensive efforts have been devoted to the latter stage, where Graph Neural Networks (GNNs) have dominated. However, the former stage for most existing graph benchmarks still relies on traditional feature engineering techniques. More recently, with the rapid development of language models (LMs), researchers have focused on leveraging LMs to facilitate the learning of TGs, either by jointly training them in a computationally intensive framework (merging the two stages), or designing complex self-supervised training tasks for feature extraction (enhancing the first stage). In this work, we present SimTeG, a frustratingly Simple approach for Textual Graph learning that does not innovate in frameworks, models, and tas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#35789;&#23884;&#20837;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#25366;&#25496;&#31995;&#32479;&#65292;&#29992;&#20110;&#25903;&#25345;&#25506;&#32034;&#25919;&#24220;&#35843;&#26597;&#21457;&#29616;&#30340;&#22823;&#37327;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#24179;&#21488;&#65292;&#25581;&#31034;&#26032;&#30340;&#21382;&#21490;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2308.02556</link><description>&lt;p&gt;
&#24037;&#19994;&#35760;&#24518;&#65306;&#36890;&#36807;&#31070;&#32463;&#35789;&#23884;&#20837;&#21644;&#26426;&#22120;&#23398;&#20064;&#25506;&#32034;&#25919;&#24220;&#35843;&#26597;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Industrial Memories: Exploring the Findings of Government Inquiries with Neural Word Embedding and Machine Learning. (arXiv:2308.02556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02556
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#35789;&#23884;&#20837;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#25366;&#25496;&#31995;&#32479;&#65292;&#29992;&#20110;&#25903;&#25345;&#25506;&#32034;&#25919;&#24220;&#35843;&#26597;&#21457;&#29616;&#30340;&#22823;&#37327;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#24179;&#21488;&#65292;&#25581;&#31034;&#26032;&#30340;&#21382;&#21490;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#25366;&#25496;&#31995;&#32479;&#65292;&#20197;&#25903;&#25345;&#25506;&#32034;&#22823;&#37327;&#20851;&#20110;&#25919;&#24220;&#35843;&#26597;&#21457;&#29616;&#30340;&#25991;&#26412;&#12290;&#23613;&#31649;&#36825;&#20123;&#35843;&#26597;&#20855;&#26377;&#21382;&#21490;&#24847;&#20041;&#21644;&#28508;&#22312;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#20294;&#20851;&#38190;&#21457;&#29616;&#24120;&#24120;&#38544;&#34255;&#22312;&#20887;&#38271;&#30340;&#25991;&#20214;&#20013;&#65292;&#23545;&#19968;&#33324;&#20844;&#20247;&#26469;&#35828;&#26080;&#27861;&#33719;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#35789;&#23884;&#20837;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#23558;&#29233;&#23572;&#20848;&#25919;&#24220;&#23545;&#24037;&#19994;&#23398;&#26657;&#30340;&#35843;&#26597;&#32467;&#26524;&#36827;&#34892;&#36716;&#21270;&#65292;&#21576;&#29616;&#20986;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#25506;&#32034;&#25991;&#26412;&#20197;&#25581;&#31034;&#26032;&#30340;&#21382;&#21490;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a text mining system to support the exploration of large volumes of text detailing the findings of government inquiries. Despite their historical significance and potential societal impact, key findings of inquiries are often hidden within lengthy documents and remain inaccessible to the general public. We transform the findings of the Irish government's inquiry into industrial schools and through the use of word embedding, text classification and visualisation, present an interactive web-based platform that enables the exploration of the text to uncover new historical insights.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#35814;&#32454;&#22320;&#20998;&#26512;&#26053;&#34892;&#32773;&#30340;&#35780;&#35770;&#65292;&#20174;&#32780;&#24110;&#21161;&#26426;&#22330;&#31649;&#29702;&#20102;&#35299;&#26053;&#34892;&#32773;&#30340;&#38656;&#27714;&#65292;&#24182;&#21457;&#29616;&#38656;&#35201;&#25913;&#36827;&#30340;&#26426;&#22330;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.02548</link><description>&lt;p&gt;
&#26053;&#34892;&#32773;&#35780;&#35770;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Aspect based sentimental analysis for travellers' reviews. (arXiv:2308.02548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#35814;&#32454;&#22320;&#20998;&#26512;&#26053;&#34892;&#32773;&#30340;&#35780;&#35770;&#65292;&#20174;&#32780;&#24110;&#21161;&#26426;&#22330;&#31649;&#29702;&#20102;&#35299;&#26053;&#34892;&#32773;&#30340;&#38656;&#27714;&#65292;&#24182;&#21457;&#29616;&#38656;&#35201;&#25913;&#36827;&#30340;&#26426;&#22330;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22330;&#26381;&#21153;&#36136;&#37327;&#35780;&#20272;&#36890;&#24120;&#21487;&#20197;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25214;&#21040;&#65292;&#21253;&#25324;&#35895;&#27468;&#22320;&#22270;&#12290;&#36825;&#23545;&#20110;&#26426;&#22330;&#31649;&#29702;&#26469;&#35828;&#26159;&#24456;&#26377;&#20215;&#20540;&#30340;&#65292;&#21487;&#20197;&#25552;&#39640;&#25152;&#25552;&#20379;&#26381;&#21153;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#25552;&#20379;&#26053;&#34892;&#32773;&#35752;&#35770;&#30340;&#20027;&#39064;&#30340;&#24635;&#20307;&#35780;&#35770;&#65292;&#35201;&#20040;&#25552;&#20379;&#24773;&#24863;&#20540;&#26469;&#26631;&#35760;&#25972;&#20010;&#35780;&#35770;&#65292;&#32780;&#27809;&#26377;&#20855;&#20307;&#25552;&#21450;&#32972;&#21518;&#30340;&#26426;&#22330;&#26381;&#21153;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#20197;&#25552;&#20379;&#23545;&#26053;&#34892;&#32773;&#35780;&#35770;&#30340;&#26356;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#22312;&#35895;&#27468;&#22320;&#22270;&#20851;&#20110;&#36842;&#25308;&#21644;&#22810;&#21704;&#26426;&#22330;&#30340;&#25968;&#25454;&#19978;&#24212;&#29992;&#20102;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#32467;&#26524;&#25552;&#20379;&#20102;&#20351;&#29992;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26469;&#26356;&#22909;&#22320;&#20102;&#35299;&#26053;&#34892;&#32773;&#21644;&#21457;&#29616;&#38656;&#35201;&#25913;&#36827;&#30340;&#26426;&#22330;&#26381;&#21153;&#30340;&#21487;&#35302;&#21450;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Airport service quality evaluation is commonly found on social media, including Google Maps. This valuable for airport management in order to enhance the quality of services provided. However; prior studies either provide general review for topics discussed by travellers or provide sentimental value to tag the entire review without specifically mentioning the airport service that is behind such value. Accordingly, this work proposes using aspect based sentimental analysis in order to provide more detailed analysis for travellers reviews. This works applied aspect based sentimental analysis on data collected from Google Map about Dubai and Doha airports. The results provide tangible reasons to use aspect based sentimental analysis in order to understand more the travellers and spot airport services that are in need for improvement.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#32508;&#21512;&#20102;&#32039;&#24613;&#27807;&#36890;&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#36229;&#36234;&#31616;&#21333;&#20219;&#21153;&#65292;&#24182;&#26377;&#25928;&#27807;&#36890;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.02541</link><description>&lt;p&gt;
&#36808;&#21521;&#26356;&#20855;&#20154;&#31867;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#20132;&#27969;&#65306;&#32039;&#24613;&#27807;&#36890;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards More Human-like AI Communication: A Review of Emergent Communication Research. (arXiv:2308.02541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#32508;&#21512;&#20102;&#32039;&#24613;&#27807;&#36890;&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#36229;&#36234;&#31616;&#21333;&#20219;&#21153;&#65292;&#24182;&#26377;&#25928;&#27807;&#36890;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21521;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#36716;&#21464;&#30340;&#26368;&#36817;&#65292;&#26426;&#22120;&#20934;&#30830;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#23398;&#20064;&#38169;&#20301;&#65292;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#20154;&#31867;&#22312;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26102;&#25152;&#20351;&#29992;&#30340;&#22522;&#26412;&#32467;&#26500;&#21644;&#25512;&#29702;&#26041;&#24335;&#65292;&#21487;&#33021;&#23548;&#33268;&#24847;&#24819;&#19981;&#21040;&#25110;&#19981;&#21487;&#38752;&#30340;&#34892;&#20026;&#12290;&#32039;&#24613;&#27807;&#36890;&#65288;Emecom&#65289;&#26159;&#19968;&#20010;&#36817;&#24180;&#26469;&#21457;&#34920;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#20197;&#36229;&#36234;&#31616;&#21333;&#30340;&#21028;&#21035;&#24615;&#20219;&#21153;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#20132;&#27969;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#33258;&#28982;&#35821;&#35328;&#20351;&#29992;&#33021;&#21147;&#30340;&#20154;&#24037;&#20195;&#29702;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20197;&#20004;&#20010;&#26041;&#38754;&#20171;&#32461;Emecom&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30028;&#23450;&#20102;&#25991;&#29486;&#20013;&#21457;&#29616;&#30340;&#25152;&#26377;&#24120;&#35265;&#29305;&#24615;&#21450;&#20854;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23376;&#31867;&#21035;&#65292;&#24182;&#31361;&#20986;&#23427;&#20204;&#30340;&#29305;&#28857;&#21644;&#24320;&#25918;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the recent shift towards human-centric AI, the need for machines to accurately use natural language has become increasingly important. While a common approach to achieve this is to train large language models, this method presents a form of learning misalignment where the model may not capture the underlying structure and reasoning humans employ in using natural language, potentially leading to unexpected or unreliable behavior. Emergent communication (Emecom) is a field of research that has seen a growing number of publications in recent years, aiming to develop artificial agents capable of using natural language in a way that goes beyond simple discriminative tasks and can effectively communicate and learn new concepts. In this review, we present Emecom under two aspects. Firstly, we delineate all the common proprieties we find across the literature and how they relate to human interactions. Secondly, we identify two subcategories and highlight their characteristics and open chall
&lt;/p&gt;</description></item><item><title>CoSMo&#26159;&#19968;&#20010;&#29992;&#20110;Abstract Wikipedia&#30340;&#20869;&#23481;&#36873;&#25321;&#36807;&#31243;&#30340;&#26500;&#36896;&#35268;&#33539;&#35821;&#35328;&#65292;&#28385;&#36275;&#22810;&#35821;&#35328;&#24314;&#27169;&#12289;&#35206;&#30422;&#22768;&#26126;&#24615;&#20869;&#23481;&#21644;&#20989;&#25968;&#30340;&#20869;&#23481;&#36873;&#25321;&#65292;&#20197;&#21450;&#31867;&#21644;&#23454;&#20363;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.02539</link><description>&lt;p&gt;
CoSMo&#65306;&#19968;&#20010;&#29992;&#20110;Abstract Wikipedia&#20869;&#23481;&#36873;&#25321;&#36807;&#31243;&#30340;&#26500;&#36896;&#35268;&#33539;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
CoSMo: A constructor specification language for Abstract Wikipedia's content selection process. (arXiv:2308.02539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02539
&lt;/p&gt;
&lt;p&gt;
CoSMo&#26159;&#19968;&#20010;&#29992;&#20110;Abstract Wikipedia&#30340;&#20869;&#23481;&#36873;&#25321;&#36807;&#31243;&#30340;&#26500;&#36896;&#35268;&#33539;&#35821;&#35328;&#65292;&#28385;&#36275;&#22810;&#35821;&#35328;&#24314;&#27169;&#12289;&#35206;&#30422;&#22768;&#26126;&#24615;&#20869;&#23481;&#21644;&#20989;&#25968;&#30340;&#20869;&#23481;&#36873;&#25321;&#65292;&#20197;&#21450;&#31867;&#21644;&#23454;&#20363;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#34920;&#31034;&#20449;&#24687;&#25688;&#35201;&#26159;&#19968;&#20010;&#38656;&#35201;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#25191;&#34892;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#25968;&#25454;&#24211;&#35270;&#22270;&#35268;&#33539;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20174;&#32467;&#26500;&#21270;&#36755;&#20837;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27969;&#27700;&#32447;&#30340;&#31532;&#19968;&#38454;&#27573;&#65292;&#21363;&#20869;&#23481;&#36873;&#25321;&#38454;&#27573;&#20197;&#30830;&#23450;&#38656;&#35201;&#36716;&#36848;&#30340;&#20869;&#23481;&#12290;&#23545;&#20110;Abstract Wikipedia&#39033;&#30446;&#32780;&#35328;&#65292;&#38656;&#27714;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#25277;&#35937;&#34920;&#31034;&#38656;&#35201;&#22810;&#35821;&#35328;&#24314;&#27169;&#12289;&#35206;&#30422;&#22768;&#26126;&#24615;&#20869;&#23481;&#21644;&#20989;&#25968;&#30340;&#20869;&#23481;&#36873;&#25321;&#65292;&#20197;&#21450;&#31867;&#21644;&#23454;&#20363;&#12290;&#30446;&#21069;&#27809;&#26377;&#28385;&#36275;&#36825;&#19977;&#20010;&#29305;&#24449;&#20043;&#19968;&#30340;&#24314;&#27169;&#35821;&#35328;&#65292;&#26356;&#19981;&#29992;&#35828;&#19968;&#20010;&#32467;&#21512;&#20102;&#36825;&#19977;&#20010;&#29305;&#24449;&#30340;&#35821;&#35328;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#30456;&#20851;&#26041;&#21672;&#35810;&#65292;&#25105;&#20204;&#36981;&#24490;&#20005;&#35880;&#30340;&#35821;&#35328;&#35774;&#35745;&#36807;&#31243;&#65292;&#21019;&#24314;&#20102;CoSMo&#65292;&#19968;&#20010;&#20840;&#26032;&#30340;&#20869;&#23481;&#36873;&#25321;&#24314;&#27169;&#35821;&#35328;&#65292;&#20197;&#28385;&#36275;&#36825;&#20123;&#20197;&#21450;&#20854;&#20182;&#38656;&#27714;&#65292;&#20351;&#20854;&#22312;Abstract Wikipedia&#20197;&#21450;&#20854;&#20182;&#29615;&#22659;&#20013;&#37117;&#33021;&#26377;&#29992;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#35774;&#35745;&#36807;&#31243;&#12289;&#21407;&#29702;&#21644;&#36873;&#25321;&#65292;
&lt;/p&gt;
&lt;p&gt;
Representing snippets of information abstractly is a task that needs to be performed for various purposes, such as database view specification and the first stage in the natural language generation pipeline for generative AI from structured input, i.e., the content selection stage to determine what needs to be verbalised. For the Abstract Wikipedia project, requirements analysis revealed that such an abstract representation requires multilingual modelling, content selection covering declarative content and functions, and both classes and instances. There is no modelling language that meets either of the three features, let alone a combination. Following a rigorous language design process inclusive of broad stakeholder consultation, we created CoSMo, a novel {\sc Co}ntent {\sc S}election {\sc Mo}deling language that meets these and other requirements so that it may be useful both in Abstract Wikipedia as well as other contexts. We describe the design process, rationale and choices, the 
&lt;/p&gt;</description></item><item><title>ALE&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#27604;NLP&#20013;AL&#31574;&#30053;&#30340;&#20223;&#30495;&#20027;&#21160;&#23398;&#20064;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20379;&#23454;&#35777;&#22522;&#30784;&#21644;&#20844;&#27491;&#30340;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.02537</link><description>&lt;p&gt;
ALE: &#29992;&#20110;NLP&#26597;&#35810;&#31574;&#30053;&#21442;&#25968;&#39537;&#21160;&#27604;&#36739;&#30340;&#22522;&#20110;&#20223;&#30495;&#30340;&#20027;&#21160;&#23398;&#20064;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ALE: A Simulation-Based Active Learning Evaluation Framework for the Parameter-Driven Comparison of Query Strategies for NLP. (arXiv:2308.02537v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02537
&lt;/p&gt;
&lt;p&gt;
ALE&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#27604;NLP&#20013;AL&#31574;&#30053;&#30340;&#20223;&#30495;&#20027;&#21160;&#23398;&#20064;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20379;&#23454;&#35777;&#22522;&#30784;&#21644;&#20844;&#27491;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#25968;&#25454;&#31185;&#23398;&#23478;&#36890;&#36807;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;&#27880;&#37322;&#36807;&#31243;&#33719;&#21462;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#25361;&#25112;&#65292;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25552;&#20986;&#23558;&#26377;&#24076;&#26395;&#30340;&#25968;&#25454;&#28857;&#25512;&#33616;&#32473;&#27880;&#37322;&#21592;&#65292;&#20197;&#20415;&#20182;&#20204;&#27880;&#37322;&#25509;&#19979;&#26469;&#30340;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#25110;&#36830;&#32493;&#30340;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#33410;&#30465;&#27880;&#37322;&#24037;&#20316;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#32773;&#38754;&#20020;&#30528;&#35768;&#22810;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#30340;AL&#31574;&#30053;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#20010;&#23454;&#35777;&#22522;&#30784;&#26469;&#36873;&#25321;&#23427;&#20204;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#35843;&#30740;&#23558;AL&#31574;&#30053;&#20998;&#31867;&#20026;&#27809;&#26377;&#24615;&#33021;&#25351;&#26631;&#30340;&#20998;&#31867;&#27861;&#12290;&#26032;&#22411;AL&#31574;&#30053;&#30340;&#20171;&#32461;&#24615;&#28436;&#31034;&#19982;&#23569;&#37327;&#31574;&#30053;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21487;&#37325;&#29616;&#30340;&#20027;&#21160;&#23398;&#20064;&#35780;&#20272;&#65288;ALE&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29992;&#20110;&#27604;&#36739;&#35780;&#20272;NLP&#20013;&#30340;AL&#31574;&#30053;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#23454;&#29616;AL&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#27604;&#36739;&#36827;&#34892;&#20844;&#27491;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning and deep learning require a large amount of labeled data, which data scientists obtain in a manual, and time-consuming annotation process. To mitigate this challenge, Active Learning (AL) proposes promising data points to annotators they annotate next instead of a subsequent or random sample. This method is supposed to save annotation effort while maintaining model performance. However, practitioners face many AL strategies for different tasks and need an empirical basis to choose between them. Surveys categorize AL strategies into taxonomies without performance indications. Presentations of novel AL strategies compare the performance to a small subset of strategies. Our contribution addresses the empirical basis by introducing a reproducible active learning evaluation (ALE) framework for the comparative evaluation of AL strategies in NLP. The framework allows the implementation of AL strategies with low effort and a fair data-driven comparison through defin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27888;&#22269;&#25903;&#25345;&#26234;&#33021;&#20892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#20316;&#29289;&#31181;&#26893;&#30693;&#35782;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;&#19982;&#26234;&#33021;&#20892;&#19994;&#21644;&#25512;&#33616;&#31995;&#32479;&#37197;&#21512;&#20351;&#29992;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#25968;&#25454;&#30417;&#25511;&#21644;&#28748;&#28297;&#31995;&#32479;&#25511;&#21046;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.02524</link><description>&lt;p&gt;
&#22312;&#27888;&#22269;&#25903;&#25345;&#26234;&#33021;&#20892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Chatbot Application to Support Smart Agriculture in Thailand. (arXiv:2308.02524v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27888;&#22269;&#25903;&#25345;&#26234;&#33021;&#20892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#20316;&#29289;&#31181;&#26893;&#30693;&#35782;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;&#19982;&#26234;&#33021;&#20892;&#19994;&#21644;&#25512;&#33616;&#31995;&#32479;&#37197;&#21512;&#20351;&#29992;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#25968;&#25454;&#30417;&#25511;&#21644;&#28748;&#28297;&#31995;&#32479;&#25511;&#21046;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#24555;&#36895;&#22320;&#22238;&#22797;&#25991;&#26412;&#25110;&#35821;&#38899;&#23545;&#35805;&#30340;&#36719;&#20214;&#12290;&#22312;&#20892;&#19994;&#39046;&#22495;&#20013;&#65292;&#29616;&#26377;&#30340;&#26234;&#33021;&#20892;&#19994;&#31995;&#32479;&#20165;&#20351;&#29992;&#26469;&#33258;&#24863;&#24212;&#21644;&#29289;&#32852;&#32593;&#25216;&#26415;&#30340;&#25968;&#25454;&#65292;&#19981;&#21253;&#25324;&#20316;&#29289;&#31181;&#26893;&#30693;&#35782;&#26469;&#25903;&#25345;&#20892;&#27665;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#22686;&#24378;&#36825;&#19968;&#28857;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#21487;&#20197;&#25104;&#20026;&#20892;&#27665;&#30340;&#21161;&#25163;&#65292;&#25552;&#20379;&#20316;&#29289;&#31181;&#26893;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LINE &#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#20316;&#20026;&#20449;&#24687;&#21644;&#30693;&#35782;&#34920;&#31034;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#20316;&#29289;&#31181;&#26893;&#24314;&#35758;&#12290;&#23427;&#19982;&#26234;&#33021;&#20892;&#19994;&#21644;&#25512;&#33616;&#31995;&#32479;&#37197;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340; LINE &#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#21253;&#25324;&#20116;&#20010;&#20027;&#35201;&#30340;&#21151;&#33021;&#65288;&#24320;&#22987;/&#20572;&#27490;&#33756;&#21333;&#12289;&#20027;&#39029;&#12289;&#28404;&#28748;&#39029;&#38754;&#12289;&#38654;&#28748;&#39029;&#38754;&#21644;&#30417;&#25511;&#39029;&#38754;&#65289;&#12290;&#20892;&#27665;&#20204;&#23558;&#20250;&#33719;&#24471;&#29992;&#20110;&#25968;&#25454;&#30417;&#25511;&#20197;&#25903;&#25345;&#20915;&#31574;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#21487;&#20197;&#36890;&#36807; LINE &#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#25511;&#21046;&#28748;&#28297;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A chatbot is a software developed to help reply to text or voice conversations automatically and quickly in real time. In the agriculture sector, the existing smart agriculture systems just use data from sensing and internet of things (IoT) technologies that exclude crop cultivation knowledge to support decision-making by farmers. To enhance this, the chatbot application can be an assistant to farmers to provide crop cultivation knowledge. Consequently, we propose the LINE chatbot application as an information and knowledge representation providing crop cultivation recommendations to farmers. It works with smart agriculture and recommendation systems. Our proposed LINE chatbot application consists of five main functions (start/stop menu, main page, drip irri gation page, mist irrigation page, and monitor page). Farmers will receive information for data monitoring to support their decision-making. Moreover, they can control the irrigation system via the LINE chatbot. Furthermore, farmer
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21333;&#35843;&#24615;&#32422;&#26463;&#25913;&#36827;&#25991;&#31456;&#36830;&#36143;&#24615;&#35780;&#20272;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#36143;&#24615;&#35780;&#20998;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#23616;&#37096;&#36830;&#36143;&#24615;&#21028;&#21035;&#27169;&#22411;&#21644;&#19968;&#20010;&#26631;&#28857;&#31526;&#21495;&#20462;&#27491;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#26641;&#20316;&#20026;&#22238;&#24402;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;NLPCC 2023&#20849;&#20139;&#20219;&#21153;7&#30340;&#31532;&#19968;&#36187;&#36947;&#20013;&#33719;&#24471;&#20102;&#31532;&#19977;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.02506</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#35843;&#24615;&#32422;&#26463;&#25913;&#36827;&#25991;&#31456;&#36830;&#36143;&#24615;&#35780;&#20272;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving the Generalization Ability in Essay Coherence Evaluation through Monotonic Constraints. (arXiv:2308.02506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02506
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21333;&#35843;&#24615;&#32422;&#26463;&#25913;&#36827;&#25991;&#31456;&#36830;&#36143;&#24615;&#35780;&#20272;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#36143;&#24615;&#35780;&#20998;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#23616;&#37096;&#36830;&#36143;&#24615;&#21028;&#21035;&#27169;&#22411;&#21644;&#19968;&#20010;&#26631;&#28857;&#31526;&#21495;&#20462;&#27491;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#26641;&#20316;&#20026;&#22238;&#24402;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;NLPCC 2023&#20849;&#20139;&#20219;&#21153;7&#30340;&#31532;&#19968;&#36187;&#36947;&#20013;&#33719;&#24471;&#20102;&#31532;&#19977;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#36143;&#24615;&#26159;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#22312;&#35780;&#20272;&#35770;&#25991;&#26102;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#22240;&#32032;&#36827;&#34892;&#35780;&#20272;&#12290;&#31532;&#19968;&#20010;&#22240;&#32032;&#26159;&#36923;&#36753;&#36830;&#36143;&#24615;&#65292;&#34920;&#29616;&#20026;&#36866;&#24403;&#20351;&#29992;&#35805;&#35821;&#36830;&#25509;&#35789;&#21644;&#24314;&#31435;&#21477;&#23376;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#31532;&#20108;&#20010;&#22240;&#32032;&#26159;&#26631;&#28857;&#31526;&#21495;&#30340;&#36866;&#24403;&#24615;&#65292;&#22240;&#20026;&#19981;&#36866;&#24403;&#30340;&#26631;&#28857;&#31526;&#21495;&#21487;&#33021;&#23548;&#33268;&#21477;&#23376;&#32467;&#26500;&#28151;&#20081;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#36143;&#24615;&#35780;&#20998;&#27169;&#22411;&#65292;&#20854;&#30001;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#22238;&#24402;&#27169;&#22411;&#32452;&#25104;&#65306;&#19968;&#20010;&#23616;&#37096;&#36830;&#36143;&#24615;&#21028;&#21035;&#27169;&#22411;&#21644;&#19968;&#20010;&#26631;&#28857;&#31526;&#21495;&#20462;&#27491;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#26641;&#20316;&#20026;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#23545;&#36755;&#20837;&#29305;&#24449;&#26045;&#21152;&#20102;&#21333;&#35843;&#24615;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#22312;NLPCC 2023&#20849;&#20139;&#20219;&#21153;7&#30340;&#31532;&#19968;&#36187;&#36947;&#20013;&#33719;&#24471;&#20102;&#31532;&#19977;&#21517;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31616;&#35201;&#20171;&#32461;&#20102;&#23545;&#20110;&#20854;&#20182;&#36187;&#36947;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coherence is a crucial aspect of evaluating text readability and can be assessed through two primary factors when evaluating an essay in a scoring scenario. The first factor is logical coherence, characterized by the appropriate use of discourse connectives and the establishment of logical relationships between sentences. The second factor is the appropriateness of punctuation, as inappropriate punctuation can lead to confused sentence structure. To address these concerns, we propose a coherence scoring model consisting of a regression model with two feature extractors: a local coherence discriminative model and a punctuation correction model. We employ gradient-boosting regression trees as the regression model and impose monotonicity constraints on the input features. The results show that our proposed model better generalizes unseen data. The model achieved third place in track 1 of NLPCC 2023 shared task 7. Additionally, we briefly introduce our solution for the remaining tracks, wh
&lt;/p&gt;</description></item><item><title>MyVoice&#26159;&#19968;&#20010;&#29992;&#20110;&#25910;&#38598;&#38463;&#25289;&#20271;&#35821;&#26041;&#35328;&#35821;&#38899;&#30340;&#20247;&#21253;&#24179;&#21488;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#22823;&#22411;&#26041;&#35328;&#35821;&#38899;&#25968;&#25454;&#38598;&#24182;&#20351;&#20854;&#20844;&#24320;&#21487;&#29992;&#30340;&#26426;&#20250;&#12290;&#36129;&#29486;&#32773;&#21487;&#20197;&#36873;&#25321;&#32454;&#31890;&#24230;&#26041;&#35328;&#24182;&#35760;&#24405;&#35805;&#35821;&#65292;&#24179;&#21488;&#36824;&#25552;&#20379;&#20102;&#36136;&#37327;&#20445;&#35777;&#31995;&#32479;&#26469;&#36807;&#28388;&#20302;&#36136;&#37327;&#21644;&#34394;&#20551;&#30340;&#24405;&#38899;&#65292;&#24182;&#20801;&#35768;&#36129;&#29486;&#32773;&#35780;&#20272;&#24405;&#38899;&#30340;&#36136;&#37327;&#21644;&#25552;&#20379;&#21453;&#39304;&#12290;&#35813;&#24179;&#21488;&#36824;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#31649;&#29702;&#21592;&#35282;&#33394;&#21487;&#20197;&#28155;&#21152;&#26032;&#30340;&#25968;&#25454;&#25110;&#20219;&#21153;&#65292;&#20419;&#36827;&#22810;&#26679;&#21270;&#21644;&#22823;&#35268;&#27169;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#38899;&#25968;&#25454;&#30340;&#25910;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.02503</link><description>&lt;p&gt;
MyVoice: &#38463;&#25289;&#20271;&#35821;&#35821;&#38899;&#36164;&#28304;&#21327;&#20316;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
MyVoice: Arabic Speech Resource Collaboration Platform. (arXiv:2308.02503v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02503
&lt;/p&gt;
&lt;p&gt;
MyVoice&#26159;&#19968;&#20010;&#29992;&#20110;&#25910;&#38598;&#38463;&#25289;&#20271;&#35821;&#26041;&#35328;&#35821;&#38899;&#30340;&#20247;&#21253;&#24179;&#21488;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#22823;&#22411;&#26041;&#35328;&#35821;&#38899;&#25968;&#25454;&#38598;&#24182;&#20351;&#20854;&#20844;&#24320;&#21487;&#29992;&#30340;&#26426;&#20250;&#12290;&#36129;&#29486;&#32773;&#21487;&#20197;&#36873;&#25321;&#32454;&#31890;&#24230;&#26041;&#35328;&#24182;&#35760;&#24405;&#35805;&#35821;&#65292;&#24179;&#21488;&#36824;&#25552;&#20379;&#20102;&#36136;&#37327;&#20445;&#35777;&#31995;&#32479;&#26469;&#36807;&#28388;&#20302;&#36136;&#37327;&#21644;&#34394;&#20551;&#30340;&#24405;&#38899;&#65292;&#24182;&#20801;&#35768;&#36129;&#29486;&#32773;&#35780;&#20272;&#24405;&#38899;&#30340;&#36136;&#37327;&#21644;&#25552;&#20379;&#21453;&#39304;&#12290;&#35813;&#24179;&#21488;&#36824;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#31649;&#29702;&#21592;&#35282;&#33394;&#21487;&#20197;&#28155;&#21152;&#26032;&#30340;&#25968;&#25454;&#25110;&#20219;&#21153;&#65292;&#20419;&#36827;&#22810;&#26679;&#21270;&#21644;&#22823;&#35268;&#27169;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#38899;&#25968;&#25454;&#30340;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MyVoice&#65292;&#19968;&#20010;&#29992;&#20110;&#25910;&#38598;&#38463;&#25289;&#20271;&#35821;&#35821;&#38899;&#26469;&#22686;&#24378;&#26041;&#35328;&#35821;&#38899;&#25216;&#26415;&#30340;&#20247;&#21253;&#24179;&#21488;&#12290;&#35813;&#24179;&#21488;&#25552;&#20379;&#20102;&#35774;&#35745;&#22823;&#22411;&#26041;&#35328;&#35821;&#38899;&#25968;&#25454;&#38598;&#30340;&#26426;&#20250;&#65292;&#24182;&#20351;&#20854;&#20844;&#24320;&#21487;&#29992;&#12290;MyVoice&#20801;&#35768;&#36129;&#29486;&#32773;&#36873;&#25321;&#22478;&#24066;/&#22269;&#23478;&#32423;&#30340;&#32454;&#31890;&#24230;&#26041;&#35328;&#65292;&#24182;&#35760;&#24405;&#26174;&#31034;&#30340;&#35805;&#35821;&#12290;&#29992;&#25143;&#21487;&#20197;&#22312;&#36129;&#29486;&#32773;&#21644;&#27880;&#37322;&#32773;&#20043;&#38388;&#20999;&#25442;&#35282;&#33394;&#12290;&#35813;&#24179;&#21488;&#37319;&#29992;&#20102;&#36136;&#37327;&#20445;&#35777;&#31995;&#32479;&#65292;&#22312;&#21457;&#36865;&#24405;&#38899;&#36827;&#34892;&#39564;&#35777;&#20043;&#21069;&#65292;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#21644;&#34394;&#20551;&#30340;&#24405;&#38899;&#12290;&#22312;&#39564;&#35777;&#38454;&#27573;&#65292;&#36129;&#29486;&#32773;&#21487;&#20197;&#35780;&#20272;&#24405;&#38899;&#30340;&#36136;&#37327;&#12289;&#27880;&#37322;&#23427;&#20204;&#65292;&#24182;&#25552;&#20379;&#21453;&#39304;&#65292;&#28982;&#21518;&#30001;&#31649;&#29702;&#21592;&#36827;&#34892;&#23457;&#26680;&#12290;&#27492;&#22806;&#65292;&#35813;&#24179;&#21488;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#65292;&#20351;&#31649;&#29702;&#21592;&#35282;&#33394;&#33021;&#22815;&#28155;&#21152;&#38500;&#26041;&#35328;&#35821;&#38899;&#21644;&#21333;&#35789;&#25910;&#38598;&#20043;&#22806;&#30340;&#26032;&#25968;&#25454;&#25110;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#23558;&#26174;&#31034;&#32473;&#36129;&#29486;&#32773;&#12290;&#20174;&#32780;&#20419;&#36827;&#25910;&#38598;&#22810;&#26679;&#21270;&#21644;&#22823;&#35268;&#27169;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#38899;&#25968;&#25454;&#30340;&#21327;&#20316;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MyVoice, a crowdsourcing platform designed to collect Arabic speech to enhance dialectal speech technologies. This platform offers an opportunity to design large dialectal speech datasets; and makes them publicly available. MyVoice allows contributors to select city/country-level fine-grained dialect and record the displayed utterances. Users can switch roles between contributors and annotators. The platform incorporates a quality assurance system that filters out low-quality and spurious recordings before sending them for validation. During the validation phase, contributors can assess the quality of recordings, annotate them, and provide feedback which is then reviewed by administrators. Furthermore, the platform offers flexibility to admin roles to add new data or tasks beyond dialectal speech and word collection, which are displayed to contributors. Thus, enabling collaborative efforts in gathering diverse and large Arabic speech data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22788;&#29702;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#26465;&#20214;&#21644;&#21305;&#37197;&#36923;&#36753;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#36741;&#21161;&#31579;&#36873;&#24739;&#32773;-&#35797;&#39564;&#20505;&#36873;&#20154;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#65306;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. (arXiv:2308.02180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22788;&#29702;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#26465;&#20214;&#21644;&#21305;&#37197;&#36923;&#36753;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#36741;&#21161;&#31579;&#36873;&#24739;&#32773;-&#35797;&#39564;&#20505;&#36873;&#20154;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#26159;&#21307;&#30103;&#20256;&#36882;&#21644;&#21457;&#29616;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#24222;&#22823;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#19981;&#21487;&#25193;&#23637;&#30340;&#25163;&#21160;&#22788;&#29702;&#65292;&#35813;&#36807;&#31243;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#32959;&#30244;&#23398;&#20026;&#37325;&#28857;&#39046;&#22495;&#65292;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20010;&#27491;&#22312;&#32654;&#22269;&#19968;&#20010;&#22823;&#22411;&#21307;&#30103;&#32593;&#32476;&#36827;&#34892;&#27979;&#35797;&#37096;&#32626;&#30340;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#31995;&#32479;&#12290;&#21021;&#27493;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65306;&#20808;&#36827;&#30340;LLM&#65288;&#22914;GPT-4&#65289;&#21487;&#20197;&#31435;&#21363;&#36830;&#25509;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#30340;&#21512;&#26684;&#26465;&#20214;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#30340;&#21305;&#37197;&#36923;&#36753;&#65288;&#20363;&#22914;&#23884;&#22871;&#30340;AND/OR/NOT&#65289;&#12290;&#34429;&#28982;&#20173;&#19981;&#23436;&#32654;&#65292;LLM&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#24378;&#22522;&#20934;&#32447;&#65292;&#24182;&#21487;&#33021;&#20316;&#20026;&#22312;&#20154;&#19982;&#20154;&#20043;&#38388;&#36827;&#34892;&#20505;&#36873;&#24739;&#32773;-&#35797;&#39564;&#21010;&#20998;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#19968;&#20123;&#24212;&#29992;LLM&#36827;&#34892;&#31471;&#21040;&#31471;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#37325;&#35201;&#22686;&#38271;&#39046;&#22495;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#38480;&#21046;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.00121</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#65306;AI&#20316;&#20026;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23433;&#20840;&#27979;&#35797;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28183;&#36879;&#27979;&#35797;&#26159;&#19968;&#39033;&#38656;&#35201;&#39640;&#27700;&#24179;&#19987;&#19994;&#30693;&#35782;&#30340;&#27963;&#21160;&#65292;&#24182;&#28041;&#21450;&#35768;&#22810;&#25163;&#21160;&#27979;&#35797;&#21644;&#20998;&#26512;&#27493;&#39588;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29992;&#20363;&#65306;&#29992;&#20110;&#23433;&#20840;&#27979;&#35797;&#20219;&#21153;&#30340;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#22312;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#20013;&#36827;&#34892;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#38381;&#29615;&#21453;&#39304;&#65292;&#23558;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20302;&#32423;&#25805;&#20316;&#19982;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#65288;&#36890;&#36807;SSH&#36830;&#25509;&#65289;&#30456;&#36830;&#65292;&#24182;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#34394;&#25311;&#26426;&#29366;&#24577;&#20197;&#23547;&#25214;&#28431;&#27934;&#65292;&#24182;&#25552;&#20379;&#20855;&#20307;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25913;&#36827;&#30340;&#36884;&#24452;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14361</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#30340;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#22312;Kaggle&#30340;&#8220;&#20010;&#24615;&#21270;&#21307;&#23398;&#65306;&#37325;&#26032;&#23450;&#20041;&#30284;&#30151;&#27835;&#30103;&#8221;&#25968;&#25454;&#38598;&#20013;&#23545;&#22522;&#22240;&#31361;&#21464;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#19982;BERT&#12289;Electra&#12289;Roberta&#12289;XLNet&#12289;Distilbert&#20197;&#21450;&#23427;&#20204;&#30340;LSTM&#38598;&#25104;&#31561;&#30693;&#21517;&#36716;&#25442;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;&#22343;&#26041;&#35823;&#24046;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#36824;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#23436;&#32654;&#32467;&#21512;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#38598;&#25104;&#27169;&#22411;&#22312;&#22522;&#22240;&#31361;&#21464;&#20998;&#31867;&#31561;&#22256;&#38590;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.12375</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#19978;&#20855;&#26377;&#21019;&#26032;&#65292;&#20294;&#24182;&#38750;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12375
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#22312;&#21253;&#21547;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#36890;&#24120;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26080;&#20849;&#35782;&#65306;&#20363;&#22914;&#65292;&#34429;&#28982;Xie&#31561;&#20154;&#65288;2021&#24180;&#65289;&#23558;ICL&#27604;&#20316;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;Min&#31561;&#20154;&#65288;2022b&#24180;&#65289;&#35748;&#20026;ICL&#29978;&#33267;&#19981;&#33021;&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#65292;&#65288;2&#65289;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#36755;&#20837;-&#26631;&#31614;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#65288;3&#65289;ICL&#22914;&#20309;&#32858;&#21512;&#26469;&#33258;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#36890;&#24120;&#20250;&#25972;&#21512;&#19978;&#19979;&#25991;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#20294;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#31995;&#34987;&#21306;&#21035;&#23545;&#24453;&#65292;&#27169;&#22411;&#19981;&#20250;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#21516;&#23545;&#24453;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#23545;LLMs&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIST&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20165;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22270;&#20687;&#29305;&#23450;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#22270;&#20687;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11315</link><description>&lt;p&gt;
&#29983;&#25104;&#22270;&#20687;&#29305;&#23450;&#25991;&#26412;&#25913;&#21892;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Generating Image-Specific Text Improves Fine-grained Image Classification. (arXiv:2307.11315v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIST&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20165;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22270;&#20687;&#29305;&#23450;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#22270;&#20687;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#20248;&#20110;&#20165;&#35270;&#35273;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#37197;&#23545;&#30340;&#25991;&#26412;/&#22270;&#20687;&#25551;&#36848;&#65292;&#23545;&#20110;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#26469;&#35828;&#65292;&#20173;&#28982;&#24456;&#38590;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIST&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20165;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22270;&#20687;&#29305;&#23450;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#25991;&#26412;&#25551;&#36848;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#20998;&#31867;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#37096;&#20998;&#21253;&#25324;&#65306;1. &#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#20026;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#65292;&#20197;&#21450;2. &#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#27599;&#20010;&#22270;&#20687;&#19982;&#20445;&#30041;&#26631;&#31614;&#30340;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#21305;&#37197;&#65292;&#36825;&#20123;&#25551;&#36848;&#25429;&#25417;&#20102;&#22270;&#20687;&#20013;&#30456;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#21644;&#29983;&#25104;&#30340;&#25991;&#26412;&#23545;&#19978;&#24494;&#35843;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#20010;&#23545;&#40784;&#30340;&#35270;&#35273;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;GIST&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent vision-language models outperform vision-only models on many image classification tasks. However, because of the absence of paired text/image descriptions, it remains difficult to fine-tune these models for fine-grained image classification. In this work, we propose a method, GIST, for generating image-specific fine-grained text descriptions from image-only datasets, and show that these text descriptions can be used to improve classification. Key parts of our method include 1. prompting a pretrained large language model with domain-specific prompts to generate diverse fine-grained text descriptions for each class and 2. using a pretrained vision-language model to match each image to label-preserving text descriptions that capture relevant visual features in the image. We demonstrate the utility of GIST by fine-tuning vision-language models on the image-and-generated-text pairs to learn an aligned vision-language representation space for improved classification. We evaluate our l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21435;&#20559;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#38750;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10511</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#36890;&#29992;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
General Debiasing for Multimodal Sentiment Analysis. (arXiv:2307.10511v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10511
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21435;&#20559;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#38750;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#24037;&#20316;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#65292;&#20294;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#21040;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#24773;&#24863;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#25968;&#25454;&#38598;&#20013;&#22823;&#22810;&#25968;&#24102;&#26377;&#34013;&#33394;&#32972;&#26223;&#30340;&#35270;&#39057;&#37117;&#26377;&#27491;&#38754;&#26631;&#31614;&#65292;&#27169;&#22411;&#23558;&#20381;&#36182;&#20110;&#36825;&#26679;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#8220;&#34013;&#33394;&#32972;&#26223;&#8221;&#24182;&#19981;&#26159;&#19968;&#20010;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21435;&#20559;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#38750;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20498;&#25968;&#27010;&#29575;&#21152;&#26435;&#65288;Inverse Probability Weighting&#65292;IPW&#65289;&#30340;&#36890;&#29992;&#21435;&#20559;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33258;&#36866;&#24212;&#22320;&#20026;&#20855;&#26377;&#26356;&#22823;&#20559;&#24046;&#65288;&#21363;&#26356;&#20005;&#37325;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65289;&#30340;&#26679;&#26412;&#20998;&#37197;&#36739;&#23567;&#30340;&#26435;&#37325;&#12290;&#36825;&#20010;&#21435;&#20559;&#26694;&#26550;&#30340;&#20851;&#38190;&#22312;&#20110;&#20272;&#35745;&#27599;&#20010;&#26679;&#26412;&#30340;&#20559;&#24046;&#65292;&#36825;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#23454;&#29616;&#65306;1&#65289;&#23558;&#40065;&#26834;&#29305;&#24449;&#21644;&#20559;&#35265;&#29305;&#24449;&#20998;&#31163;&#20986;&#26469;
&lt;/p&gt;
&lt;p&gt;
Existing work on Multimodal Sentiment Analysis (MSA) utilizes multimodal information for prediction yet unavoidably suffers from fitting the spurious correlations between multimodal features and sentiment labels. For example, if most videos with a blue background have positive labels in a dataset, the model will rely on such correlations for prediction, while ``blue background'' is not a sentiment-related feature. To address this problem, we define a general debiasing MSA task, which aims to enhance the Out-Of-Distribution (OOD) generalization ability of MSA models by reducing their reliance on spurious correlations. To this end, we propose a general debiasing framework based on Inverse Probability Weighting (IPW), which adaptively assigns small weights to the samples with larger bias i.e., the severer spurious correlations). The key to this debiasing framework is to estimate the bias of each sample, which is achieved by two steps: 1) disentangling the robust features and biased featur
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mask-tuning&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;Masked Language Modeling (MLM)&#35757;&#32451;&#30446;&#26631;&#25972;&#21512;&#21040;&#24494;&#35843;&#36807;&#31243;&#20013;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Mask-tuning&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#39640;&#20102;PLMs&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10457</link><description>&lt;p&gt;
&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Pre-trained Language Models' Generalization. (arXiv:2307.10457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mask-tuning&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;Masked Language Modeling (MLM)&#35757;&#32451;&#30446;&#26631;&#25972;&#21512;&#21040;&#24494;&#35843;&#36807;&#31243;&#20013;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Mask-tuning&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#39640;&#20102;PLMs&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#36890;&#24120;&#21463;&#21040;&#20854;&#27867;&#21270;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#21363;&#24403;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#21516;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#20854;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#36825;&#31181;&#31034;&#20363;&#34987;&#31216;&#20026;&#8220;&#38750;&#20998;&#24067;/&#26410;&#35265;&#31034;&#20363;&#8221;&#12290;&#36825;&#19968;&#38480;&#21046;&#28304;&#20110;PLMs&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#65292;&#34394;&#20551;&#30456;&#20851;&#24615;&#23545;&#20110;&#24120;&#35265;&#31034;&#20363;&#31867;&#22411;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#19968;&#33324;&#31034;&#20363;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Mask-tuning&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#35757;&#32451;&#30446;&#26631;&#25972;&#21512;&#21040;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;PLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Mask-tuning&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#22686;&#24378;&#20102;PLMs&#23545;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;Mask-tuning&#25552;&#39640;&#20102;PLMs&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26356;&#21152;&#23454;&#29992;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reusability of state-of-the-art Pre-trained Language Models (PLMs) is often limited by their generalization problem, where their performance drastically decreases when evaluated on examples that differ from the training dataset, known as Out-of-Distribution (OOD)/unseen examples. This limitation arises from PLMs' reliance on spurious correlations, which work well for frequent example types but not for general examples. To address this issue, we propose a training approach called Mask-tuning, which integrates Masked Language Modeling (MLM) training objectives into the fine-tuning process to enhance PLMs' generalization. Comprehensive experiments demonstrate that Mask-tuning surpasses current state-of-the-art techniques and enhances PLMs' generalization on OOD datasets while improving their performance on in-distribution datasets. The findings suggest that Mask-tuning improves the reusability of PLMs on unseen data, making them more practical and effective for real-world applications
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;MLP&#34701;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08941</link><description>&lt;p&gt;
NTK-&#36817;&#20284;MLP&#34701;&#21512;&#29992;&#20110;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;MLP&#34701;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#24050;&#25104;&#20026;&#20027;&#35201;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#24494;&#35843;PLM&#21644;&#36827;&#34892;&#25512;&#29702;&#20063;&#26159;&#26114;&#36149;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#33021;&#21147;&#36739;&#20302;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#19968;&#20123;&#36890;&#29992;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#37327;&#21270;&#21644;&#33976;&#39311;&#65289;&#26469;&#20943;&#23569;PLM&#24494;&#35843;&#30340;&#35745;&#31639;/&#23384;&#20648;&#24320;&#38144;&#65292;&#20294;&#24456;&#23569;&#26377;&#19968;&#27425;&#24615;&#21387;&#32553;&#25216;&#26415;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#27169;&#22359;&#20013;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;NTK&#36817;&#20284;MLP&#34701;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;PLM&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;MLP&#37325;&#26032;&#35270;&#20026;&#19968;&#26463;&#23376;MLP&#65292;&#24182;&#23558;&#23427;&#20204;&#32858;&#31867;&#20026;&#32473;&#23450;&#25968;&#37327;&#30340;&#36136;&#24515;&#65292;&#28982;&#21518;&#23558;&#20854;&#24674;&#22797;&#20026;&#21387;&#32553;&#30340;MLP&#65292;&#24182;&#24847;&#22806;&#22320;&#26174;&#31034;&#20986;&#23545;&#21407;&#22987;PLM&#30340;NTK&#36827;&#34892;&#33391;&#22909;&#36817;&#20284;&#30340;&#25928;&#26524;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#39564;&#35777;PLM&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural l
&lt;/p&gt;</description></item><item><title>CAME&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#20449;&#25351;&#23548;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#24182;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#20869;&#23384;&#39640;&#25928;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31283;&#23450;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02047</link><description>&lt;p&gt;
CAME: &#38752;&#33258;&#20449;&#25351;&#23548;&#30340;&#33258;&#36866;&#24212;&#20869;&#23384;&#39640;&#25928;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
CAME: Confidence-guided Adaptive Memory Efficient Optimization. (arXiv:2307.02047v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02047
&lt;/p&gt;
&lt;p&gt;
CAME&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#20449;&#25351;&#23548;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#24182;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#20869;&#23384;&#39640;&#25928;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31283;&#23450;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#22914;Adam&#21644;LAMB&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#24615;&#38656;&#35201;&#32500;&#25252;&#27599;&#20010;&#21442;&#25968;&#26799;&#24230;&#30340;&#20108;&#38454;&#30697;&#20272;&#35745;&#65292;&#36825;&#24102;&#26469;&#20102;&#39640;&#39069;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#20869;&#23384;&#39640;&#25928;&#30340;&#20248;&#21270;&#22120;&#65288;&#22914;Adafactor&#65289;&#26469;&#22823;&#24133;&#20943;&#23569;&#36741;&#21161;&#20869;&#23384;&#30340;&#20351;&#29992;&#65292;&#20294;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#20449;&#24230;&#30340;&#31574;&#30053;&#26469;&#20943;&#23569;&#29616;&#26377;&#20869;&#23384;&#39640;&#25928;&#20248;&#21270;&#22120;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAME&#65292;&#20197;&#21516;&#26102;&#23454;&#29616;&#20004;&#20010;&#30446;&#26631;&#65306;&#24555;&#36895;&#25910;&#25947;&#65292;&#19982;&#20256;&#32479;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#30456;&#20284;&#65292;&#20197;&#21450;&#20302;&#20869;&#23384;&#20351;&#29992;&#29575;&#65292;&#19982;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#30456;&#20284;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;CAME&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#20363;&#22914;BERT&#21644;GPT-2&#35757;&#32451;&#65289;&#20013;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#22823;&#25209;&#37327;&#30340;BERT&#39044;&#35757;&#32451;&#20013;&#65292;CAME&#27604;&#20854;&#20182;&#26041;&#27861;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02046</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25512;&#33616;&#31995;&#32479; (LLMs)
&lt;/p&gt;
&lt;p&gt;
Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02046
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#32321;&#33635;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#20197;&#28385;&#36275;&#20854;&#21916;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21644;&#25972;&#21512;&#25991;&#26412;&#20391;&#20449;&#24687;&#22312;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#26159;DNN&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#12289;&#25429;&#25417;&#25991;&#26412;&#20391;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#20013;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65288;&#20363;&#22914;ChatGPT&#21644;GPT4&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22522;&#26412;&#32844;&#36131;&#19978;&#26377;&#30528;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.00925</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#31181;&#19982;&#35745;&#31639;&#26426;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#38598;&#21512;&#31574;&#30053;&#26469;&#30830;&#20445;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#19968;&#32452;&#20505;&#36873;&#24230;&#37327;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#26368;&#22823;&#21270;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#30340;&#38598;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#38598;&#21512;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26082;&#23637;&#31034;&#20102;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#27604;&#36739;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#20063;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The so
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#20026;&#32473;&#23450;&#30340;&#25991;&#26412;&#25512;&#33616;&#26368;&#36866;&#21512;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#19968;&#20010;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#26681;&#25454;&#36755;&#20837;&#20869;&#23481;&#20998;&#26512;&#24182;&#39044;&#27979;&#26368;&#20339;&#25688;&#35201;&#29983;&#25104;&#22120;&#12290;&#22235;&#31181;&#26031;&#27931;&#25991;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#35299;&#20915;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#20013;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#30340;&#19981;&#21516;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26031;&#27931;&#25991;&#25688;&#35201;&#29983;&#25104;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.11518</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#22411;&#65306;&#25490;&#24207;&#26031;&#27931;&#25991;&#25688;&#35201;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
One model to rule them all: ranking Slovene summarizers. (arXiv:2306.11518v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11518
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#20026;&#32473;&#23450;&#30340;&#25991;&#26412;&#25512;&#33616;&#26368;&#36866;&#21512;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#19968;&#20010;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#26681;&#25454;&#36755;&#20837;&#20869;&#23481;&#20998;&#26512;&#24182;&#39044;&#27979;&#26368;&#20339;&#25688;&#35201;&#29983;&#25104;&#22120;&#12290;&#22235;&#31181;&#26031;&#27931;&#25991;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#35299;&#20915;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#20013;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#30340;&#19981;&#21516;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26031;&#27931;&#25991;&#25688;&#35201;&#29983;&#25104;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#22810;&#24180;&#26469;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20174;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#21040;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#24182;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#27169;&#22411;&#25110;&#26041;&#27861;&#21487;&#20197;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#25991;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#20026;&#32473;&#23450;&#30340;&#25991;&#26412;&#25512;&#33616;&#26368;&#36866;&#21512;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#37319;&#29992;&#20102;&#19968;&#20010;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#20998;&#26512;&#36755;&#20837;&#20869;&#23481;&#24182;&#39044;&#27979;&#21738;&#31181;&#25688;&#35201;&#29983;&#25104;&#22120;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;ROUGE&#20998;&#25968;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;&#20803;&#27169;&#22411;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;&#26031;&#27931;&#25991;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#36873;&#25321;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#38024;&#23545;&#26031;&#27931;&#25991;&#35821;&#35328;&#24320;&#21457;&#30340;&#65292;&#20351;&#29992;&#36755;&#20837;&#30340;&#19981;&#21516;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#20854;Doc2Vec&#25991;&#26723;&#34920;&#31034;&#12290;&#36825;&#22235;&#20010;&#26031;&#27931;&#25991;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#35299;&#20915;&#20102;&#19982;&#22312;&#35821;&#35328;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#20013;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#30340;&#19981;&#21516;&#25361;&#25112;&#12290;&#25105;&#20204;&#33258;&#21160;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;SloMetaSum&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#37096;&#20998;&#25163;&#21160;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26031;&#27931;&#25991;&#25688;&#35201;&#29983;&#25104;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization is an essential task in natural language processing, and researchers have developed various approaches over the years, ranging from rule-based systems to neural networks. However, there is no single model or approach that performs well on every type of text. We propose a system that recommends the most suitable summarization model for a given text. The proposed system employs a fully connected neural network that analyzes the input content and predicts which summarizer should score the best in terms of ROUGE score for a given input. The meta-model selects among four different summarization models, developed for the Slovene language, using different properties of the input, in particular its Doc2Vec document representation. The four Slovene summarization models deal with different challenges associated with text summarization in a less-resourced language. We evaluate the proposed SloMetaSum model performance automatically and parts of it manually. The results show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPI&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#24544;&#23454;&#22320;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2306.01153</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#32493;&#21518;&#39564;&#25512;&#29702;&#23454;&#29616;&#22810;&#26679;&#21644;&#24544;&#23454;&#30340;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diverse and Faithful Knowledge-Grounded Dialogue Generation via Sequential Posterior Inference. (arXiv:2306.01153v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPI&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#24544;&#23454;&#22320;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20107;&#23454;&#30693;&#35782;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#24544;&#23454;&#24615;&#30340;&#22238;&#22797;&#23545;&#20110;&#21019;&#24314;&#31867;&#20154;&#12289;&#21487;&#20449;&#20219;&#30340;&#23545;&#35805;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;"&#36830;&#32493;&#21518;&#39564;&#25512;&#29702;&#65288;SPI&#65289;"&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#23545;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#36817;&#20284;&#37319;&#26679;&#26469;&#36873;&#25321;&#30693;&#35782;&#24182;&#29983;&#25104;&#23545;&#35805;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;SPI&#19981;&#38656;&#35201;&#25512;&#29702;&#32593;&#32476;&#25110;&#20551;&#35774;&#21518;&#39564;&#20998;&#24067;&#20855;&#26377;&#19968;&#20010;&#31616;&#21333;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#20854;&#30452;&#25509;&#26597;&#35810;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#30452;&#25509;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#30340;&#30693;&#35782;&#36873;&#25321;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capability to generate responses with diversity and faithfulness using factual knowledge is paramount for creating a human-like, trustworthy dialogue system. Common strategies either adopt a two-step paradigm, which optimizes knowledge selection and response generation separately, and may overlook the inherent correlation between these two tasks, or leverage conditional variational method to jointly optimize knowledge selection and response generation by employing an inference network. In this paper, we present an end-to-end learning framework, termed Sequential Posterior Inference (SPI), capable of selecting knowledge and generating dialogues by approximately sampling from the posterior distribution. Unlike other methods, SPI does not require the inference network or assume a simple geometry of the posterior distribution. This straightforward and intuitive inference procedure of SPI directly queries the response generation model, allowing for accurate knowledge selection and gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#37051;&#22495;&#27604;&#36739;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#26469;&#25552;&#39640;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#36825;&#20123;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18462</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#22495;&#27604;&#36739;&#30340;&#35821;&#35328;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks against Language Models via Neighbourhood Comparison. (arXiv:2305.18462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#37051;&#22495;&#27604;&#36739;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#26469;&#25552;&#39640;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#36825;&#20123;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIAs)&#26088;&#22312;&#39044;&#27979;&#19968;&#20010;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#23384;&#22312;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#25915;&#20987;&#20381;&#36182;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#27169;&#22411;&#20542;&#21521;&#20110;&#23558;&#26356;&#39640;&#30340;&#27010;&#29575;&#20998;&#37197;&#32473;&#35757;&#32451;&#26679;&#26412;&#32780;&#38750;&#38750;&#35757;&#32451;&#28857;&#12290;&#28982;&#32780;&#65292;&#23545;&#27169;&#22411;&#20998;&#25968;&#30340;&#31616;&#21333;&#38408;&#20540;&#35774;&#23450;&#24448;&#24448;&#23548;&#33268;&#39640;&#35823;&#25253;&#29575;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#26679;&#26412;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#21442;&#32771;&#27169;&#22411;&#30340;&#25915;&#20987;&#21487;&#20197;&#23558;&#27169;&#22411;&#20998;&#25968;&#19982;&#22312;&#31867;&#20284;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#21442;&#32771;&#27169;&#22411;&#33719;&#24471;&#30340;&#20998;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;MIAs&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#35757;&#32451;&#21442;&#32771;&#27169;&#22411;&#65292;&#36825;&#31181;&#25915;&#20987;&#30340;&#20570;&#27861;&#26159;&#20551;&#23450;&#25932;&#26041;&#30693;&#36947;&#19982;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#23494;&#20999;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20551;&#23450;&#25915;&#20987;&#32773;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#30340;&#37051;&#22495;&#26679;&#26412;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#22312;&#26356;&#29616;&#23454;&#30340;&#25104;&#21592;&#25512;&#26029;&#22330;&#26223;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;&#20960;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#23545;&#35805;&#29983;&#25104;&#65292;&#24182;&#31361;&#26174;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic sce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JADE&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#36827;&#34892;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.11769</link><description>&lt;p&gt;
&#25552;&#21319;&#32852;&#21512;&#23398;&#20064;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65306;&#22522;&#20110;&#32852;&#21512;&#23398;&#20064;&#30340;&#38382;&#31572;&#19982;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner. (arXiv:2305.11769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JADE&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#36827;&#34892;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#21253;&#25324;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#20174;&#32593;&#32476;&#19978;&#25910;&#38598;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20316;&#20026;&#39044;&#20808;&#35757;&#32451;&#30340;&#25968;&#25454;&#65292;&#24573;&#35270;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#38656;&#35201;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#30340;&#38656;&#27714;&#65292;&#36825;&#38656;&#35201;&#23545;&#22270;&#20687;&#21644;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#35814;&#32454;&#30340;&#29702;&#35299;&#12290;&#23558;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#38598;&#25104;&#21040;&#39044;&#20808;&#35757;&#32451;&#20013;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#33719;&#21462;&#22270;&#20687;-&#38382;&#39064;-&#31572;&#26696;&#20197;&#21450;&#22270;&#20687;-&#20301;&#32622;-&#23383;&#24149;&#19977;&#20803;&#32452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#27492;&#22806;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#25968;&#25454;&#38598;&#36890;&#24120;&#30001;&#20110;&#25163;&#21160;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#32780;&#35268;&#27169;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#32852;&#21512;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#65288;JADE&#65289;&#65292;&#23427;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#26131;&#20110;&#33719;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained multimodal models have demonstrated significant success in a range of downstream tasks, including image captioning, image-text retrieval, visual question answering (VQA), etc. However, many of these methods rely on image-text pairs collected from the web as pre-training data and unfortunately overlook the need for fine-grained feature alignment between vision and language modalities, which requires detailed understanding of images and language expressions. While integrating VQA and dense captioning (DC) into pre-training can address this issue, acquiring image-question-answer as well as image-location-caption triplets is challenging and time-consuming. Additionally, publicly available datasets for VQA and dense captioning are typically limited in scale due to manual data collection and labeling efforts. In this paper, we propose a novel method called Joint QA and DC GEneration (JADE), which utilizes a pre-trained multimodal model and easily-crawled image-text pairs to
&lt;/p&gt;</description></item><item><title>Graphologue&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21709;&#24212;&#36716;&#25442;&#20026;&#22270;&#24418;&#21270;&#22270;&#34920;&#20197;&#22686;&#24378;&#20854;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#21644;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#33410;&#28857;&#21644;&#38142;&#25509;&#26469;&#19982;&#36825;&#20123;&#22270;&#34920;&#36827;&#34892;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2305.11473</link><description>&lt;p&gt;
Graphologue&#65306;&#29992;&#20132;&#20114;&#24335;&#22270;&#34920;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. (arXiv:2305.11473v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11473
&lt;/p&gt;
&lt;p&gt;
Graphologue&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21709;&#24212;&#36716;&#25442;&#20026;&#22270;&#24418;&#21270;&#22270;&#34920;&#20197;&#22686;&#24378;&#20854;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#21644;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#33410;&#28857;&#21644;&#38142;&#25509;&#26469;&#19982;&#36825;&#20123;&#22270;&#34920;&#36827;&#34892;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30001;&#20110;&#26131;&#20110;&#33719;&#21462;&#21644;&#22312;&#22810;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#26234;&#33021;&#32780;&#36817;&#26469;&#39118;&#38753;&#19968;&#26102;&#12290;&#28982;&#32780;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#22312;&#25903;&#25345;&#22797;&#26434;&#20449;&#24687;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#26174;&#30528;&#30340;&#38480;&#21046;&#65292;&#21407;&#22240;&#26159;&#22522;&#20110;&#25991;&#26412;&#30340;&#23186;&#20171;&#21644;&#32447;&#24615;&#23545;&#35805;&#32467;&#26500;&#25552;&#20379;&#30340;&#21151;&#33021;&#19981;&#36275;&#12290;&#36890;&#36807;&#19982;&#21313;&#21517;&#21442;&#19982;&#32773;&#30340;&#24418;&#24335;&#21270;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#30028;&#38754;&#36890;&#24120;&#20250;&#21576;&#29616;&#20887;&#38271;&#30340;&#21709;&#24212;&#65292;&#20351;&#20154;&#20204;&#38590;&#20197;&#24555;&#36895;&#29702;&#35299;&#21644;&#28789;&#27963;&#22320;&#19982;&#21508;&#31181;&#20449;&#24687;&#36827;&#34892;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Graphologue&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23558;LLM&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21709;&#24212;&#36716;&#25442;&#20026;&#22270;&#24418;&#21270;&#22270;&#34920;&#65292;&#20197;&#20415;&#20110;&#20449;&#24687;&#26597;&#25214;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;Graphologue&#37319;&#29992;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#30028;&#38754;&#35774;&#35745;&#65292;&#20174;LLM&#21709;&#24212;&#20013;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#24182;&#23454;&#26102;&#26500;&#24314;&#33410;&#28857;&#38142;&#25509;&#22270;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#21644;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#33410;&#28857;&#21644;&#38142;&#25509;&#26469;&#19982;&#36825;&#20123;&#22270;&#34920;&#36827;&#34892;&#20132;&#20114;&#65292;&#20197;&#25506;&#32034;&#30456;&#20851;&#20449;&#24687;&#21644;&#36319;&#36827;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#30028;&#38754;&#30456;&#27604;&#65292;Graphologue&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#22312;&#22797;&#26434;&#20449;&#24687;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#21644;&#28385;&#24847;&#24230;&#12290;Graphologue&#20026;&#22686;&#24378;LLM&#22312;&#21508;&#31181;&#24212;&#29992;&#21644;&#39046;&#22495;&#20013;&#30340;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented intelligence exhibited on diverse applications. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks. We present Graphologue, an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks. Graphologue employs novel prompting strategies and interface designs to extract entities and relationships from LLM responses and constructs node-link diagrams in real-time. Further, users can int
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#30005;&#35805;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#32452;&#21487;&#20197;&#29992;&#20110;&#30005;&#35805;&#20998;&#31867;&#30340;&#26412;&#22320;&#35889;&#26102;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Haar&#29305;&#24449;&#21644;SVM&#20998;&#31867;&#30340;&#26799;&#24230;&#30452;&#26041;&#22270;&#36827;&#34892;&#30005;&#35805;&#20998;&#31867;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10270</link><description>&lt;p&gt;
&#22686;&#24378;&#26412;&#22320;&#35889;&#26102;&#29305;&#24449;&#29992;&#20110;&#35821;&#38899;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Boosting Local Spectro-Temporal Features for Speech Analysis. (arXiv:2305.10270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#30005;&#35805;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#32452;&#21487;&#20197;&#29992;&#20110;&#30005;&#35805;&#20998;&#31867;&#30340;&#26412;&#22320;&#35889;&#26102;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Haar&#29305;&#24449;&#21644;SVM&#20998;&#31867;&#30340;&#26799;&#24230;&#30452;&#26041;&#22270;&#36827;&#34892;&#30005;&#35805;&#20998;&#31867;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#30005;&#35805;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#32452;&#21487;&#20197;&#29992;&#20110;&#30005;&#35805;&#20998;&#31867;&#30340;&#26412;&#22320;&#35889;&#26102;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20004;&#32452;&#24120;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#30340;&#29305;&#24449;&#65288;Haar&#29305;&#24449;&#21644;SVM&#20998;&#31867;&#30340;&#26799;&#24230;&#30452;&#26041;&#22270;&#65288;HoG&#65289;&#65289;&#36827;&#34892;&#30005;&#35805;&#20998;&#31867;&#30340;&#19968;&#20123;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the problem of phone classification in the context of speech recognition, and explore several sets of local spectro-temporal features that can be used for phone classification. In particular, we present some preliminary results for phone classification using two sets of features that are commonly used for object detection: Haar features and SVM-classified Histograms of Gradients (HoG)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;PET&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32858;&#31867;&#27969;&#31243;&#23454;&#20307;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#25216;&#26415;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#30340;&#32321;&#29712;&#24037;&#20316;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#21516;&#19968;&#27969;&#31243;&#23454;&#20307;&#37325;&#22797;&#25552;&#21450;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03960</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#27969;&#31243;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#35268;&#21017;&#20043;&#22806;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;(arXiv:2305.03960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text. (arXiv:2305.03960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;PET&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32858;&#31867;&#27969;&#31243;&#23454;&#20307;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#25216;&#26415;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#30340;&#32321;&#29712;&#24037;&#20316;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#21516;&#19968;&#27969;&#31243;&#23454;&#20307;&#37325;&#22797;&#25552;&#21450;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#33258;&#21160;&#29983;&#25104;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#20852;&#26041;&#27861;&#65292;&#21487;&#36991;&#20813;&#25163;&#21160;&#21019;&#24314;&#24418;&#24335;&#21270;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#20174;&#25991;&#26412;&#27969;&#31243;&#25551;&#36848;&#20013;&#25552;&#21462;&#20986;&#27969;&#31243;&#23454;&#20307;&#65288;&#22914;&#21442;&#19982;&#32773;&#12289;&#27963;&#21160;&#12289;&#23545;&#35937;&#31561;&#65289;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#24102;&#26377;&#25991;&#26412;&#27969;&#31243;&#25551;&#36848;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;(PET)&#24050;&#32463;&#20986;&#29256;&#65292;&#20854;&#20276;&#38543;&#30528;&#19968;&#31181;&#22522;&#26412;&#30340;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#24403;&#21069;&#29366;&#24577;&#19979;&#65292;PET&#32570;&#20047;&#26377;&#20851;&#20004;&#20010;&#25552;&#21450;&#26159;&#21542;&#25351;&#20195;&#20102;&#30456;&#21516;&#25110;&#19981;&#21516;&#30340;&#27969;&#31243;&#23454;&#20307;&#30340;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#26159;&#21542;&#22312;&#30446;&#26631;&#27169;&#22411;&#20013;&#21019;&#24314;&#19968;&#20010;&#25110;&#20004;&#20010;&#24314;&#27169;&#20803;&#32032;&#30340;&#37325;&#35201;&#20915;&#31574;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#20363;&#22914;&#65292;&#20004;&#20010;&#25968;&#25454;&#22788;&#29702;&#30340;&#25552;&#21450;&#26159;&#21542;&#24847;&#21619;&#30528;&#22788;&#29702;&#19981;&#21516;&#25110;&#30456;&#21516;&#30340;&#25968;&#25454;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#27969;&#31243;&#23454;&#20307;&#30340;&#25552;&#21450;&#26469;&#25193;&#23637;PET&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#25216;&#26415;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Automated generation of business process models from natural language text is an emerging methodology for avoiding the manual creation of formal business process models. For this purpose, process entities like actors, activities, objects etc., and relations among them are extracted from textual process descriptions. A high-quality annotated corpus of textual process descriptions (PET) has been published accompanied with a basic process extraction approach. In its current state, however, PET lacks information about whether two mentions refer to the same or different process entities, which corresponds to the crucial decision of whether to create one or two modeling elements in the target model. Consequently, it is ambiguous whether, for instance, two mentions of data processing mean processing of different, or the same data. In this paper, we extend the PET dataset by clustering mentions of process entities and by proposing a new baseline technique for process extraction equipped with a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#31649;&#36947;AttDef&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#65292;&#35813;&#31649;&#36947;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;56.59%&#33267;79.97%&#21644;15.25%&#33267;48.34%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.02394</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#22240;&#30340;&#38450;&#24481;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending against Insertion-based Textual Backdoor Attacks via Attribution. (arXiv:2305.02394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#31649;&#36947;AttDef&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#65292;&#35813;&#31649;&#36947;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;56.59%&#33267;79.97%&#21644;15.25%&#33267;48.34%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;&#27169;&#24335;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#35757;&#32451;&#26399;&#38388;&#21521;&#27169;&#22411;&#28155;&#21152;&#21518;&#38376;&#26159;&#26377;&#25928;&#30340;&#12290;&#38450;&#24481;&#27492;&#31867;&#21518;&#38376;&#25915;&#20987;&#24050;&#21464;&#24471;&#32039;&#36843;&#21644;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AttDef&#30340;&#39640;&#25928;&#24402;&#22240;&#31649;&#36947;&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20855;&#26377;&#36739;&#22823;&#24402;&#22240;&#20998;&#25968;&#30340;&#20196;&#29260;&#35270;&#20026;&#28508;&#22312;&#35302;&#21457;&#22120;&#65292;&#22240;&#20026;&#36739;&#22823;&#30340;&#24402;&#22240;&#35789;&#23545;&#20110;&#38169;&#35823;&#39044;&#27979;&#32467;&#26524;&#20570;&#20986;&#36739;&#22823;&#36129;&#29486;&#65292;&#22240;&#27492;&#26356;&#26377;&#21487;&#33021;&#26159;&#27745;&#26579;&#35302;&#21457;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#22806;&#37096;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#21306;&#20998;&#36755;&#20837;&#26159;&#21542;&#34987;&#27745;&#26579;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#31181;&#24120;&#35265;&#30340;&#25915;&#20987;&#22330;&#26223;&#65288;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#65289;&#20013;&#20855;&#26377;&#36275;&#22815;&#30340;&#27867;&#21270;&#24615;&#65292;&#36825;&#19968;&#28857;&#25345;&#32493;&#25913;&#21892;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;AttDef&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#20004;&#31181;&#25915;&#20987;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;79.97%&#65288;&#25552;&#39640;&#20102;56.59%&#65289;&#21644;48.34%&#65288;&#25552;&#39640;&#20102;15.25%&#65289;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#38450;&#24481;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the false prediction results and therefore are more likely to be poison triggers. Additionally, we further utilize an external pre-trained language model to distinguish whether input is poisoned or not. We show that our proposed method can generalize sufficiently well in two common attack scenarios (poisoning training data and testing data), which consistently improves previous methods. For instance, AttDef can successfully mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34% 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;NLP&#39046;&#22495;&#36807;&#21435;&#30340;&#20154;&#31867;&#35780;&#20272;&#30340;&#20877;&#29616;&#24615;&#38382;&#39064;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#37096;&#20998;&#20154;&#31867;&#35780;&#20272;&#37117;&#26080;&#27861;&#37325;&#22797;&#25110;&#20877;&#29616;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#32570;&#22833;&#20449;&#24687;&#12289;&#26080;&#22238;&#24212;&#20316;&#32773;&#21644;&#23454;&#39564;&#32570;&#38519;&#31561;&#21407;&#22240;&#23548;&#33268;&#12290;&#36825;&#20010;&#32467;&#26524;&#25552;&#31034;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#32771;&#34385;&#22914;&#20309;&#35774;&#35745;&#21644;&#25253;&#21578;&#20154;&#31867;&#35780;&#20272;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.01633</link><description>&lt;p&gt;
&#32570;&#22833;&#20449;&#24687;&#12289;&#26080;&#22238;&#24212;&#20316;&#32773;&#12289;&#23454;&#39564;&#32570;&#38519;&#65306;NLP&#20013;&#19981;&#21487;&#33021;&#35780;&#20272;&#20197;&#21069;&#30340;&#20154;&#31867;&#35780;&#20272;&#30340;&#20877;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in NLP. (arXiv:2305.01633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01633
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;NLP&#39046;&#22495;&#36807;&#21435;&#30340;&#20154;&#31867;&#35780;&#20272;&#30340;&#20877;&#29616;&#24615;&#38382;&#39064;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#37096;&#20998;&#20154;&#31867;&#35780;&#20272;&#37117;&#26080;&#27861;&#37325;&#22797;&#25110;&#20877;&#29616;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#32570;&#22833;&#20449;&#24687;&#12289;&#26080;&#22238;&#24212;&#20316;&#32773;&#21644;&#23454;&#39564;&#32570;&#38519;&#31561;&#21407;&#22240;&#23548;&#33268;&#12290;&#36825;&#20010;&#32467;&#26524;&#25552;&#31034;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#32771;&#34385;&#22914;&#20309;&#35774;&#35745;&#21644;&#25253;&#21578;&#20154;&#31867;&#35780;&#20272;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#25105;&#20204;&#22312;&#35782;&#21035;&#19968;&#32452;&#20808;&#21069;&#36866;&#21512;&#36827;&#34892;&#21327;&#35843;&#30740;&#31350;&#30340;NLP&#39046;&#22495;&#20154;&#31867;&#35780;&#20272;&#30340;&#21162;&#21147;&#65292;&#20197;&#32771;&#23519;&#26159;&#20160;&#20040;&#20351;&#24471;NLP&#39046;&#22495;&#30340;&#20154;&#31867;&#35780;&#20272;&#26356;/ less&#33021;&#20877;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#21644;&#21457;&#29616;&#65292;&#20854;&#20013;&#21253;&#25324;&#20165;&#26377;13&#65285;&#30340;&#35770;&#25991;&#20855;&#26377;&#65288;i&#65289;&#36275;&#22815;&#20302;&#30340;&#20877;&#29616;&#38556;&#30861;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#36275;&#22815;&#30340;&#21487;&#33719;&#21462;&#20449;&#24687;&#65292;&#25165;&#21487;&#20197;&#34987;&#32771;&#34385;&#36827;&#34892;&#20877;&#29616;&#65292;&#24182;&#19988;&#25105;&#20204;&#36873;&#25321;&#36827;&#34892;&#20877;&#29616;&#30340;&#25152;&#26377;&#23454;&#39564;&#37117;&#34987;&#21457;&#29616;&#23384;&#22312;&#32570;&#38519;&#65292;&#36825;&#20351;&#24471;&#36827;&#34892;&#20877;&#29616;&#30340;&#26377;&#24847;&#20041;&#24615;&#20540;&#24471;&#24576;&#30097;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#24471;&#19981;&#23558;&#25105;&#20204;&#30340;&#21327;&#35843;&#30740;&#31350;&#35774;&#35745;&#20174;&#20877;&#29616;&#26041;&#27861;&#26356;&#25913;&#20026;&#26631;&#20934;&#21270;-&#28982;&#21518;&#20877;&#29616;&#20004;&#27425;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24635;&#20307;&#32780;&#35328;&#65288;&#26159;&#36127;&#38754;&#30340;&#65289;&#21457;&#29616;&#65292;NLP&#39046;&#22495;&#20013;&#30340;&#32477;&#22823;&#22810;&#25968;&#20154;&#31867;&#35780;&#20272;&#37117;&#19981;&#33021;&#37325;&#22797;&#21644;/&#25110;&#19981;&#33021;&#20877;&#29616;&#21644;/&#25110;&#20855;&#26377;&#22826;&#22810;&#32570;&#38519;&#20197;&#35777;&#26126;&#20854;&#21487;&#20877;&#29616;&#24615;&#12290;&#36825;&#25551;&#32472;&#20102;&#19968;&#20010;&#21487;&#24597;&#30340;&#30011;&#38754;&#65292;&#20294;&#20063;&#20026;&#37325;&#26032;&#32771;&#34385;&#22914;&#20309;&#35774;&#35745;&#21644;&#25253;&#21578;NLP&#39046;&#22495;&#30340;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report our efforts in identifying a set of previous human evaluations in NLP that would be suitable for a coordinated study examining what makes human evaluations in NLP more/less reproducible. We present our results and findings, which include that just 13\% of papers had (i) sufficiently low barriers to reproduction, and (ii) enough obtainable information, to be considered for reproduction, and that all but one of the experiments we selected for reproduction was discovered to have flaws that made the meaningfulness of conducting a reproduction questionable. As a result, we had to change our coordinated study design from a reproduce approach to a standardise-then-reproduce-twice approach. Our overall (negative) finding that the great majority of human evaluations in NLP is not repeatable and/or not reproducible and/or too flawed to justify reproduction, paints a dire picture, but presents an opportunity for a rethink about how to design and report human evaluations in NLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14104</link><description>&lt;p&gt;
&#20174;&#24369;&#25991;&#26412;&#30417;&#30563;&#20013;&#23398;&#20064;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#38469;&#20114;&#21160;&#26159;&#22810;&#26679;&#19988;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23558;&#23427;&#20204;&#35270;&#20026;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#21487;&#33021;&#30340;&#20114;&#21160;&#30340;&#37325;&#23614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#20154;&#38469;&#20114;&#21160;&#30340;&#33539;&#24335;&#65292;&#23558;&#20854;&#20316;&#20026;&#33258;&#30001;&#25991;&#26412;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#24773;&#20917;&#21644;&#20154;&#38469;&#20851;&#31995;&#30340;&#26080;&#38480;&#31354;&#38388;&#36827;&#34892;&#28789;&#27963;&#24314;&#27169;&#12290;&#20026;&#20102;&#20811;&#26381;&#32570;&#20047;&#29305;&#23450;&#20110;&#27492;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#23383;&#24149;&#25968;&#25454;&#65292;&#20197;&#27492;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#30340;&#20266;&#26631;&#31614;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#33021;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#36890;&#36807;&#34913;&#37327;&#25105;&#20204;&#39044;&#27979;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#19982;&#20107;&#23454;&#30340;&#22522;&#30784;&#24615;&#30340;&#21508;&#31181;&#25351;&#26631;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#30693;&#35782;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23558;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#21040;&#30340;&#19990;&#30028;&#30693;&#35782;&#34701;&#20837;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#65292;&#23558;&#26126;&#30830;&#30340;&#30693;&#35782;&#19982;&#35270;&#35273;&#35821;&#35328;&#23545;&#34701;&#21512;&#65292;&#36890;&#36807;&#22235;&#20010;&#30693;&#35782;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#20219;&#21153;&#25512;&#21160;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#30693;&#35782;&#30340;&#30456;&#20114;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.13923</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#30340;&#30693;&#35782;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Retrieval-based Knowledge Augmented Vision Language Pre-training. (arXiv:2304.13923v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#30693;&#35782;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23558;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#21040;&#30340;&#19990;&#30028;&#30693;&#35782;&#34701;&#20837;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#65292;&#23558;&#26126;&#30830;&#30340;&#30693;&#35782;&#19982;&#35270;&#35273;&#35821;&#35328;&#23545;&#34701;&#21512;&#65292;&#36890;&#36807;&#22235;&#20010;&#30693;&#35782;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#20219;&#21153;&#25512;&#21160;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#30693;&#35782;&#30340;&#30456;&#20114;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#27169;&#22411;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21487;&#21916;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#20173;&#26410;&#21033;&#29992;&#19990;&#30028;&#30693;&#35782;&#65292;&#19990;&#30028;&#30693;&#35782;&#38544;&#21547;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#65292;&#20294;&#21253;&#21547;&#20016;&#23500;&#21644;&#20114;&#34917;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;REtrieval-based knowledge Augmented Vision Language Pre-training (REAVL)&#27169;&#22411;&#65292;&#23427;&#20174;&#30693;&#35782;&#22270;&#35889;(KGs)&#20013;&#26816;&#32034;&#19990;&#30028;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent progress in large-scale vision and language representation learning, Vision Language Pretraining (VLP) models have achieved promising improvements on various multi-modal downstream tasks. Albeit powerful, these pre-training models still do not take advantage of world knowledge, which is implicit in multi-modal data but comprises abundant and complementary information. In this work, we propose a REtrieval-based knowledge Augmented Vision Language Pre-training model (REAVL), which retrieves world knowledge from knowledge graphs (KGs) and incorporates them in vision-language pre-training. REAVL has two core components: a knowledge retriever that retrieves knowledge given multi-modal data, and a knowledge-augmented model that fuses multi-modal data and knowledge. By novelly unifying four knowledge-aware self-supervised tasks, REAVL promotes the mutual integration of multi-modal data and knowledge by fusing explicit knowledge with vision-language pairs for masked multi-modal dat
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25361;&#25112;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#23398;&#20064;&#12289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#21644;&#19981;&#23545;&#40784;&#25968;&#25454;&#23398;&#20064;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01008</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Multimodal Learning: A Survey. (arXiv:2304.01008v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01008
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25361;&#25112;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#23398;&#20064;&#12289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#21644;&#19981;&#23545;&#40784;&#25968;&#25454;&#23398;&#20064;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#29702;&#35299;&#21644;&#20998;&#26512;&#26469;&#33258;&#22810;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#19979;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#20110;&#37197;&#23545;&#25968;&#25454;&#21644;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#27169;&#22411;&#30340;&#25193;&#23637;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#37492;&#20110;&#37326;&#22806;&#26377;&#22823;&#35268;&#27169;&#26410;&#27880;&#37322;&#30340;&#25968;&#25454;&#21487;&#29992;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#20026;&#32531;&#35299;&#27880;&#37322;&#29942;&#39048;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#31574;&#30053;&#12290;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;SSML&#65289;&#24314;&#31435;&#22312;&#36825;&#20004;&#20010;&#26041;&#21521;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20379;&#20102;&#20174;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;SSML&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#38416;&#36848;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#38754;&#20020;&#30340;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#65292;&#65288;2&#65289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19982;&#19981;&#23545;&#40784;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#36825;&#20123;&#25361;&#25112;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#65288;1&#65289;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning, which aims to understand and analyze information from multiple modalities, has achieved substantial progress in the supervised regime in recent years. However, the heavy dependence on data paired with expensive human annotations impedes scaling up models. Meanwhile, given the availability of large-scale unannotated data in the wild, self-supervised learning has become an attractive strategy to alleviate the annotation bottleneck. Building on these two directions, self-supervised multimodal learning (SSML) provides ways to learn from raw multimodal data. In this survey, we provide a comprehensive review of the state-of-the-art in SSML, in which we elucidate three major challenges intrinsic to self-supervised learning with multimodal data: (1) learning representations from multimodal data without labels, (2) fusion of different modalities, and (3) learning with unaligned data. We then detail existing solutions to these challenges. Specifically, we consider (1) object
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;LLM-PTM&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16756</link><description>&lt;p&gt;
LLM&#29992;&#20110;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;: &#38754;&#21521;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability. (arXiv:2303.16756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;LLM-PTM&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24739;&#32773;&#19982;&#36866;&#21512;&#30340;&#20020;&#24202;&#35797;&#39564;&#36827;&#34892;&#21305;&#37197;&#26159;&#25512;&#36827;&#21307;&#23398;&#30740;&#31350;&#21644;&#25552;&#20379;&#26368;&#20339;&#25252;&#29702;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#25968;&#25454;&#26631;&#20934;&#21270;&#12289;&#20262;&#29702;&#32771;&#34385;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19982;&#20020;&#24202;&#35797;&#39564;&#26631;&#20934;&#20043;&#38388;&#20114;&#25805;&#20316;&#24615;&#32570;&#20047;&#31561;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#26469;&#25913;&#21892;EHRs&#21644;&#20020;&#24202;&#35797;&#39564;&#25551;&#36848;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;LLM&#30340;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#65288;LLM-PTM&#65289;&#30340;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#24179;&#34913;&#20102;LLMs&#30340;&#22909;&#22788;&#65292;&#21516;&#26102;&#30830;&#20445;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#30340;&#23433;&#20840;&#21644;&#20445;&#23494;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;LLM-PTM&#26041;&#27861;&#65292;&#24615;&#33021;&#24179;&#22343;&#25552;&#39640;&#20102;7.32&#65285;&#65292;&#26032;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#20102;12.12&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12%. Additionally, we present case stud
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11403</link><description>&lt;p&gt;
eP-ALM:&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#24863;&#30693;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36804;&#20170;&#20026;&#27490;&#32473;&#19990;&#30028;&#30041;&#19979;&#20102;&#28145;&#21051;&#21360;&#35937;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#27169;&#22411;&#25152;&#20855;&#26377;&#30340;&#38750;&#21516;&#23547;&#24120;&#30340;&#33021;&#21147;&#12290;&#22312;&#35270;&#35273;&#26041;&#38754;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;&#21363;ViT&#65289;&#20063;&#22312;&#36861;&#38543;&#21516;&#19968;&#36235;&#21183;&#65292;&#21462;&#24471;&#20102;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#38543;&#30528;&#36825;&#31181;&#21333;&#27169;&#22411;&#30340;&#20016;&#23500;&#22810;&#26679;&#65292;&#33258;&#28982;&#20250;&#24341;&#21457;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#38656;&#35201;&#36319;&#38543;&#36825;&#20010;&#36235;&#21183;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#21162;&#21147;&#38598;&#20013;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#24182;&#25552;&#20986;&#29992;&#24863;&#30693;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#20182;&#20204;&#20173;&#28982;&#35757;&#32451;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#20381;&#36182;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#22312;&#24040;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;CLIP&#65289;&#65292;&#24182;&#28155;&#21152;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#22823;&#22810;&#25968;&#20851;&#27880;Zero-Shot&#21644;In Context Learning&#65292;&#35266;&#23519;&#21040;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;eP-ALM&#65292;&#19968;&#31181;&#23558;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#20855;&#26377;&#26497;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26032;&#30340;&#39044;&#35757;&#32451;&#65292;&#20173;&#28982;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#33258;&#21160;&#21270;&#25991;&#26412;&#25688;&#35201;&#29983;&#25104;&#31185;&#23398;&#25991;&#29486;&#30340;&#25554;&#22270;&#26631;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#21270;&#25688;&#35201;&#27169;&#22411; PEGASUS &#23545;&#24341;&#29992;&#22270;&#34920;&#30340;&#27573;&#33853;&#36827;&#34892;&#25688;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#35270;&#35273;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#20302;&#36136;&#37327;&#20316;&#32773;&#25776;&#20889;&#30340;&#26631;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#23545;&#22909;&#26631;&#39064;&#32570;&#20047;&#26126;&#30830;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2302.12324</link><description>&lt;p&gt;
&#25688;&#35201;&#21363;&#26631;&#39064;&#65306;&#21033;&#29992;&#33258;&#21160;&#21270;&#25991;&#26412;&#25688;&#35201;&#29983;&#25104;&#31185;&#23398;&#25991;&#29486;&#30340;&#25554;&#22270;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization. (arXiv:2302.12324v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#33258;&#21160;&#21270;&#25991;&#26412;&#25688;&#35201;&#29983;&#25104;&#31185;&#23398;&#25991;&#29486;&#30340;&#25554;&#22270;&#26631;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#21270;&#25688;&#35201;&#27169;&#22411; PEGASUS &#23545;&#24341;&#29992;&#22270;&#34920;&#30340;&#27573;&#33853;&#36827;&#34892;&#25688;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#35270;&#35273;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#20302;&#36136;&#37327;&#20316;&#32773;&#25776;&#20889;&#30340;&#26631;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#23545;&#22909;&#26631;&#39064;&#32570;&#20047;&#26126;&#30830;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#22909;&#30340;&#25554;&#22270;&#26631;&#39064;&#21487;&#20197;&#24110;&#21161;&#35770;&#25991;&#35835;&#32773;&#29702;&#35299;&#22797;&#26434;&#30340;&#31185;&#23398;&#22270;&#34920;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#24050;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#20854;&#26631;&#39064;&#24120;&#24120;&#20889;&#24471;&#24456;&#24046;&#12290;&#33258;&#21160;&#29983;&#25104;&#26631;&#39064;&#21487;&#20197;&#24110;&#21161;&#35770;&#25991;&#20316;&#32773;&#25552;&#20379;&#33391;&#22909;&#30340;&#36215;&#22987;&#26631;&#39064;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#25913;&#36827;&#36136;&#37327;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#24120;&#23558;&#25554;&#22270;&#26631;&#39064;&#29983;&#25104;&#35270;&#20026;&#19968;&#39033;&#35270;&#35273;&#21040;&#35821;&#35328;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#23558;&#20854;&#20316;&#20026;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#26356;&#20026;&#26377;&#25928;&#12290;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#21270;&#25688;&#35201;&#27169;&#22411; PEGASUS &#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#19987;&#38376;&#29992;&#20110;&#23558;&#24341;&#29992;&#22270;&#34920;&#30340;&#27573;&#33853;&#65288;&#20363;&#22914;&#65292;&#8220;&#22270;3&#26174;&#31034;...&#8221;&#65289;&#25688;&#35201;&#20026;&#22270;&#34920;&#26631;&#39064;&#12290;&#22312;&#22823;&#35268;&#27169; arXiv &#22270;&#34920;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#35270;&#35273;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;(i) &#20302;&#36136;&#37327;&#20316;&#32773;&#25776;&#20889;&#30340;&#26631;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450; (ii) &#23545;&#22909;&#26631;&#39064;&#32570;&#20047;&#26126;&#30830;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Good figure captions help paper readers understand complex scientific figures. Unfortunately, even published papers often have poorly written captions. Automatic caption generation could aid paper writers by providing good starting captions that can be refined for better quality. Prior work often treated figure caption generation as a vision-to-language task. In this paper, we show that it can be more effectively tackled as a text summarization task in scientific documents. We fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs (e.g., "Figure 3 shows...") into figure captions. Experiments on large-scale arXiv figures show that our method outperforms prior vision methods in both automatic and human evaluations. We further conducted an in-depth investigation focused on two key challenges: (i) the common presence of low-quality author-written captions and (ii) the lack of clear standards for good captions. Our code and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09656</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#35299;&#37322;&#65306;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#23545;&#40784;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Selective Explanations: Leveraging Human Input to Align Explainable AI. (arXiv:2301.09656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22240;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#29983;&#20135;&#21644;&#28040;&#36153;&#26041;&#24335;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;XAI&#25216;&#26415;&#24448;&#24448;&#38590;&#20197;&#20351;&#29992;&#24182;&#32570;&#20047;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#20351;AI&#35299;&#37322;&#20855;&#26377;&#36873;&#25321;&#24615;&#65288;&#36825;&#26159;&#20154;&#31867;&#35299;&#37322;&#30340;&#22522;&#26412;&#23646;&#24615;&#20043;&#19968;&#65289;&#26469;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#36890;&#36807;&#26681;&#25454;&#25509;&#25910;&#26041;&#30340;&#20559;&#22909;&#26377;&#36873;&#25321;&#24615;&#22320;&#21576;&#29616;&#22823;&#37327;&#27169;&#22411;&#21407;&#22240;&#30340;&#23376;&#38598;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#26679;&#26412;&#19978;&#30340;&#20154;&#31867;&#36755;&#20837;&#26469;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#24320;&#36767;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#36873;&#25321;&#24615;&#30446;&#26631;&#12289;&#36755;&#20837;&#31867;&#22411;&#31561;&#12290;&#20316;&#20026;&#19968;&#20010;&#23637;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#26469;&#25506;&#32034;&#22522;&#20110;&#20915;&#31574;&#32773;&#35748;&#20026;&#30456;&#20851;&#30340;&#36873;&#25321;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#26816;&#26597;&#20174;&#22823;&#19968;&#32452;&#27169;&#22411;&#21407;&#22240;&#20013;&#36873;&#25321;&#30340;&#19977;&#20010;&#23376;&#38598;&#19982;&#26410;&#36873;&#25321;&#30340;&#23376;&#38598;&#30456;&#27604;&#65292;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a vast collection of explainable AI (XAI) algorithms have been developed in recent years, they are often criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective -- a fundamental property of human explanations -- by selectively presenting a subset from a large set of model reasons based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small sample. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three out of a bro
&lt;/p&gt;</description></item><item><title>TikTalk&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#26234;&#33021;&#19988;&#31867;&#20284;&#20154;&#31867;&#30340;&#38386;&#32842;&#26426;&#22120;&#20154;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;&#27969;&#34892;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#25910;&#38598;&#30340;38K&#20010;&#35270;&#39057;&#21644;367K&#20010;&#29992;&#25143;&#23545;&#35805;&#12290;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;TikTalk&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#20174;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#31572;&#30340;&#38590;&#24230;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#26356;&#39057;&#32321;&#22320;&#24341;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#20026;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.05880</link><description>&lt;p&gt;
TikTalk: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#30495;&#23454;&#19990;&#30028;&#38386;&#32842;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World. (arXiv:2301.05880v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05880
&lt;/p&gt;
&lt;p&gt;
TikTalk&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#26234;&#33021;&#19988;&#31867;&#20284;&#20154;&#31867;&#30340;&#38386;&#32842;&#26426;&#22120;&#20154;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;&#27969;&#34892;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#25910;&#38598;&#30340;38K&#20010;&#35270;&#39057;&#21644;367K&#20010;&#29992;&#25143;&#23545;&#35805;&#12290;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;TikTalk&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#20174;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#31572;&#30340;&#38590;&#24230;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#26356;&#39057;&#32321;&#22320;&#24341;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#20026;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20013;&#26234;&#33021;&#21644;&#20154;&#31867;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;TikTalk&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#27969;&#34892;&#30340;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#25910;&#38598;&#20102;38K&#20010;&#35270;&#39057;&#65292;&#20197;&#21450;&#29992;&#25143;&#22312;&#20854;&#19979;&#21457;&#24067;&#30340;367K&#20010;&#23545;&#35805;&#12290;&#29992;&#25143;&#26681;&#25454;&#20182;&#20204;&#35266;&#30475;&#35270;&#39057;&#26102;&#30340;&#22810;&#27169;&#24577;&#32463;&#39564;&#36827;&#34892;&#33258;&#21457;&#24615;&#23545;&#35805;&#65292;&#36825;&#26377;&#21161;&#20110;&#37325;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#38386;&#32842;&#29615;&#22659;&#12290;&#19982;&#20043;&#21069;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;TikTalk&#20013;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#23548;&#33268;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#20174;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#25429;&#25417;&#20154;&#31867;&#20852;&#36259;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#31572;&#30340;&#38590;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20013;&#26356;&#39057;&#32321;&#22320;&#24341;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#12290;&#36825;&#20123;&#20107;&#23454;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#23637;&#31034;&#20102;TikTalk&#30340;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#38386;&#32842;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#23545;&#35805;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To facilitate the research on intelligent and human-like chatbots with multi-modal context, we introduce a new video-based multi-modal dialogue dataset, called TikTalk. We collect 38K videos from a popular video-sharing platform, along with 367K conversations posted by users beneath them. Users engage in spontaneous conversations based on their multi-modal experiences from watching videos, which helps recreate real-world chitchat context. Compared to previous multi-modal dialogue datasets, the richer context types in TikTalk lead to more diverse conversations, but also increase the difficulty in capturing human interests from intricate multi-modal information to generate personalized responses. Moreover, external knowledge is more frequently evoked in our dataset. These facts reveal new challenges for multi-modal dialogue models. We quantitatively demonstrate the characteristics of TikTalk, propose a video-based multi-modal chitchat task, and evaluate several dialogue baselines. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#26500;&#21270;&#30340;&#30693;&#35782;&#21644;&#32479;&#19968;&#30340;&#26816;&#32034;-&#29983;&#25104;&#26041;&#27861;&#65292;&#22686;&#24378;&#22810;&#27169;&#24577;&#22810;&#36339;&#38382;&#31572;&#12290;&#20351;&#29992;&#23454;&#20307;&#20013;&#24515;&#34701;&#21512;&#32534;&#30721;&#22120;&#23545;&#40784;&#19981;&#21516;&#27169;&#24577;&#30340;&#26469;&#28304;&#65292;&#20351;&#29992;&#32479;&#19968;&#30340;&#26816;&#32034;-&#29983;&#25104;&#35299;&#30721;&#22120;&#25972;&#21512;&#20013;&#38388;&#26816;&#32034;&#32467;&#26524;&#36827;&#34892;&#31572;&#26696;&#29983;&#25104;&#65292;&#24182;&#33258;&#36866;&#24212;&#20915;&#23450;&#26816;&#32034;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2212.08632</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#30340;&#30693;&#35782;&#21644;&#32479;&#19968;&#30340;&#26816;&#32034;-&#29983;&#25104;&#65292;&#22686;&#24378;&#22810;&#27169;&#24577;&#22810;&#36339;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-modal and Multi-hop Question Answering via Structured Knowledge and Unified Retrieval-Generation. (arXiv:2212.08632v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08632
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#30340;&#30693;&#35782;&#21644;&#32479;&#19968;&#30340;&#26816;&#32034;-&#29983;&#25104;&#26041;&#27861;&#65292;&#22686;&#24378;&#22810;&#27169;&#24577;&#22810;&#36339;&#38382;&#31572;&#12290;&#20351;&#29992;&#23454;&#20307;&#20013;&#24515;&#34701;&#21512;&#32534;&#30721;&#22120;&#23545;&#40784;&#19981;&#21516;&#27169;&#24577;&#30340;&#26469;&#28304;&#65292;&#20351;&#29992;&#32479;&#19968;&#30340;&#26816;&#32034;-&#29983;&#25104;&#35299;&#30721;&#22120;&#25972;&#21512;&#20013;&#38388;&#26816;&#32034;&#32467;&#26524;&#36827;&#34892;&#31572;&#26696;&#29983;&#25104;&#65292;&#24182;&#33258;&#36866;&#24212;&#20915;&#23450;&#26816;&#32034;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#36339;&#38382;&#31572;&#28041;&#21450;&#36890;&#36807;&#25512;&#29702;&#22810;&#20010;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#36755;&#20837;&#28304;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20998;&#21035;&#26816;&#32034;&#35777;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#29983;&#25104;&#31572;&#26696;&#65292;&#22240;&#27492;&#19981;&#33021;&#20805;&#20998;&#36830;&#25509;&#20505;&#36873;&#39033;&#65292;&#24182;&#19988;&#26080;&#27861;&#24314;&#27169;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#26816;&#32034;&#21644;&#29983;&#25104;&#30340;&#27969;&#27700;&#32447;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26816;&#32034;&#24615;&#33021;&#36739;&#20302;&#26102;&#29983;&#25104;&#24615;&#33021;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#30693;&#35782;&#21644;&#32479;&#19968;&#30340;&#26816;&#32034;-&#29983;&#25104;&#65288;SKURG&#65289;&#26041;&#27861;&#12290;SKURG&#20351;&#29992;&#23454;&#20307;&#20013;&#24515;&#34701;&#21512;&#32534;&#30721;&#22120;&#26469;&#23545;&#40784;&#19981;&#21516;&#27169;&#24577;&#30340;&#26469;&#28304;&#65292;&#20351;&#29992;&#20849;&#20139;&#23454;&#20307;&#12290;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#32479;&#19968;&#30340;&#26816;&#32034;-&#29983;&#25104;&#35299;&#30721;&#22120;&#26469;&#25972;&#21512;&#20013;&#38388;&#26816;&#32034;&#32467;&#26524;&#36827;&#34892;&#31572;&#26696;&#29983;&#25104;&#65292;&#24182;&#33258;&#36866;&#24212;&#20915;&#23450;&#26816;&#32034;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal multi-hop question answering involves answering a question by reasoning over multiple input sources from different modalities. Existing methods often retrieve evidences separately and then use a language model to generate an answer based on the retrieved evidences, and thus do not adequately connect candidates and are unable to model the interdependent relations during retrieval. Moreover, the pipelined approaches of retrieval and generation might result in poor generation performance when retrieval performance is low. To address these issues, we propose a Structured Knowledge and Unified Retrieval-Generation (SKURG) approach. SKURG employs an Entity-centered Fusion Encoder to align sources from different modalities using shared entities. It then uses a unified Retrieval-Generation Decoder to integrate intermediate retrieval results for answer generation and also adaptively determine the number of retrieval steps. Extensive experiments on two representative multi-modal mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#20849;&#21516;&#20851;&#27880;&#32593;&#32476;&#65288;SceneGATE&#65289;&#29992;&#20110;TextVQA&#65292;&#36890;&#36807;&#21457;&#29616;&#22330;&#26223;&#22270;&#30340;&#28508;&#22312;&#35821;&#20041;&#65292;&#25429;&#25417;&#20102;&#22270;&#20687;&#20013;&#29289;&#20307;&#12289;OCR&#26631;&#35760;&#21644;&#38382;&#39064;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22312;Text-VQA&#21644;ST-VQA&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08283</link><description>&lt;p&gt;
SceneGATE&#65306;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#25991;&#26412;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#20849;&#21516;&#20851;&#27880;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering. (arXiv:2212.08283v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#20849;&#21516;&#20851;&#27880;&#32593;&#32476;&#65288;SceneGATE&#65289;&#29992;&#20110;TextVQA&#65292;&#36890;&#36807;&#21457;&#29616;&#22330;&#26223;&#22270;&#30340;&#28508;&#22312;&#35821;&#20041;&#65292;&#25429;&#25417;&#20102;&#22270;&#20687;&#20013;&#29289;&#20307;&#12289;OCR&#26631;&#35760;&#21644;&#38382;&#39064;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22312;Text-VQA&#21644;ST-VQA&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;TextVQA&#26041;&#27861;&#37117;&#27880;&#37325;&#36890;&#36807;&#31616;&#21333;&#30340;transformer&#32534;&#30721;&#22120;&#26469;&#25972;&#21512;&#29289;&#20307;&#12289;&#22330;&#26223;&#25991;&#26412;&#21644;&#38382;&#39064;&#35789;&#65292;&#20294;&#36825;&#26080;&#27861;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#20849;&#21516;&#20851;&#27880;&#32593;&#32476;&#65288;SceneGATE&#65289;&#29992;&#20110;TextVQA&#65292;&#35813;&#32593;&#32476;&#25581;&#31034;&#20102;&#22270;&#20687;&#20013;&#29289;&#20307;&#12289;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#26631;&#35760;&#21644;&#38382;&#39064;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;TextVQA&#30340;&#22330;&#26223;&#22270;&#26469;&#21457;&#29616;&#22270;&#20687;&#30340;&#28508;&#22312;&#35821;&#20041;&#65292;&#25105;&#20204;&#21019;&#36896;&#20102;&#19968;&#20010;&#24341;&#23548;&#20851;&#27880;&#27169;&#22359;&#26469;&#25429;&#33719;&#35821;&#35328;&#21644;&#35270;&#35273;&#20043;&#38388;&#30340;&#20869;&#37096;&#20132;&#20114;&#20316;&#20026;&#36328;&#27169;&#24577;&#20132;&#20114;&#30340;&#21521;&#23548;&#12290;&#20026;&#20102;&#26126;&#30830;&#25945;&#25480;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#38598;&#25104;&#20102;&#20004;&#20010;&#27880;&#24847;&#27169;&#22359;&#65292;&#21363;&#22522;&#20110;&#22330;&#26223;&#22270;&#30340;&#35821;&#20041;&#20851;&#31995;&#24863;&#30693;&#27880;&#24847;&#21644;&#20301;&#32622;&#20851;&#31995;&#24863;&#30693;&#27880;&#24847;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;Text-VQA&#21644;ST-VQA&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;SceneGATE&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#34920;&#29616;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most TextVQA approaches focus on the integration of objects, scene texts and question words by a simple transformer encoder. But this fails to capture the semantic relations between different modalities. The paper proposes a Scene Graph based co-Attention Network (SceneGATE) for TextVQA, which reveals the semantic relations among the objects, Optical Character Recognition (OCR) tokens and the question words. It is achieved by a TextVQA-based scene graph that discovers the underlying semantics of an image. We created a guided-attention module to capture the intra-modal interplay between the language and the vision as a guidance for inter-modal interactions. To make explicit teaching of the relations between the two modalities, we proposed and integrated two attention modules, namely a scene graph-based semantic relation-aware attention and a positional relation-aware attention. We conducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA. It is shown that our SceneG
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#32508;&#36848;&#65292;&#20174;&#32467;&#26500;&#21270;&#21644;&#21151;&#33021;&#23548;&#21521;&#30340;&#23646;&#24615;&#30340;&#35282;&#24230;&#65292;&#28085;&#30422;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#32534;&#31243;&#20013;&#30340;&#24212;&#29992;&#39046;&#22495;&#30340;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#12289;&#25216;&#26415;&#21644;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#31243;&#24207;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2212.05773</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#32534;&#31243;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Natural Language Processing for Programming. (arXiv:2212.05773v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#32508;&#36848;&#65292;&#20174;&#32467;&#26500;&#21270;&#21644;&#21151;&#33021;&#23548;&#21521;&#30340;&#23646;&#24615;&#30340;&#35282;&#24230;&#65292;&#28085;&#30422;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#32534;&#31243;&#20013;&#30340;&#24212;&#29992;&#39046;&#22495;&#30340;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#12289;&#25216;&#26415;&#21644;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#31243;&#24207;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#32534;&#31243;&#20013;&#30340;&#24212;&#29992;&#26088;&#22312;&#21033;&#29992;NLP&#25216;&#26415;&#26469;&#36741;&#21161;&#32534;&#31243;&#12290;&#23427;&#22240;&#25552;&#39640;&#29983;&#20135;&#21147;&#30340;&#26377;&#25928;&#24615;&#32780;&#26085;&#30410;&#26222;&#36941;&#12290;&#32534;&#31243;&#35821;&#35328;&#19982;&#33258;&#28982;&#35821;&#35328;&#19981;&#21516;&#65292;&#23427;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#21644;&#21151;&#33021;&#24615;&#12290;&#26500;&#24314;&#22522;&#20110;&#32467;&#26500;&#30340;&#34920;&#31034;&#21644;&#21151;&#33021;&#23548;&#21521;&#30340;&#31639;&#27861;&#26159;&#31243;&#24207;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#26680;&#24515;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32467;&#26500;&#21270;&#21644;&#21151;&#33021;&#23548;&#21521;&#30340;&#23646;&#24615;&#30340;&#35282;&#24230;&#36827;&#34892;&#31995;&#32479;&#32508;&#36848;&#65292;&#28085;&#30422;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#26041;&#27861;&#12289;&#25216;&#26415;&#21644;&#27169;&#22411;&#65292;&#26088;&#22312;&#20102;&#35299;&#27599;&#20010;&#32452;&#20214;&#20013;&#36825;&#20004;&#20010;&#23646;&#24615;&#30340;&#20316;&#29992;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#26410;&#34987;&#24320;&#21457;&#30340;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing for programming aims to use NLP techniques to assist programming. It is increasingly prevalent for its effectiveness in improving productivity. Distinct from natural language, a programming language is highly structured and functional. Constructing a structure-based representation and a functionality-oriented algorithm is at the heart of program understanding and generation. In this paper, we conduct a systematic review covering tasks, datasets, evaluation methods, techniques, and models from the perspective of the structure-based and functionality-oriented property, aiming to understand the role of the two properties in each component. Based on the analysis, we illustrate unexplored areas and suggest potential directions for future work.
&lt;/p&gt;</description></item><item><title>QAmeleon&#26159;&#19968;&#31181;&#36890;&#36807;&#20165;&#20351;&#29992;5&#20010;&#31034;&#20363;&#36827;&#34892;&#22810;&#35821;&#35328;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#24182;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#24182;&#33021;&#24357;&#21512;&#33521;&#35821;&#21644;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2211.08264</link><description>&lt;p&gt;
QAmeleon: &#20165;&#38656;5&#20010;&#31034;&#20363;&#30340;&#22810;&#35821;&#35328;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
QAmeleon: Multilingual QA with Only 5 Examples. (arXiv:2211.08264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08264
&lt;/p&gt;
&lt;p&gt;
QAmeleon&#26159;&#19968;&#31181;&#36890;&#36807;&#20165;&#20351;&#29992;5&#20010;&#31034;&#20363;&#36827;&#34892;&#22810;&#35821;&#35328;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#24182;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#24182;&#33021;&#24357;&#21512;&#33521;&#35821;&#21644;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#26159;&#36817;&#26399;&#38382;&#31572;&#65288;QA&#65289;&#25216;&#26415;&#36827;&#23637;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#38590;&#20197;&#25910;&#38598;&#19988;&#25104;&#26412;&#39640;&#26114;&#65292;&#19988;&#24456;&#23569;&#23384;&#22312;&#20110;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#65292;&#20351;&#24471;QA&#25216;&#26415;&#23545;&#20110;&#23569;&#25968;&#35821;&#35328;&#19981;&#21487;&#35775;&#38382;&#12290;&#19982;&#26500;&#24314;&#22823;&#22411;&#21333;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#26159;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#29615;&#22659;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;QAmeleon&#20351;&#29992;PLM&#33258;&#21160;&#29983;&#25104;&#36328;&#22810;&#35821;&#35328;&#30340;&#25968;&#25454;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;QA&#27169;&#22411;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;&#36890;&#36807;&#20165;&#22312;&#27599;&#31181;&#35821;&#35328;&#20013;&#20351;&#29992;5&#20010;&#31034;&#20363;&#23545;PLM&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#65292;&#21487;&#20197;&#33719;&#24471;&#20248;&#20110;&#22522;&#20110;&#32763;&#35793;&#30340;&#22522;&#32447;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24357;&#21512;&#20102;&#20165;&#33521;&#35821;&#30340;&#22522;&#32447;&#27169;&#22411;&#19982;&#36817;50,000&#20010;&#25163;&#24037;&#26631;&#35760;&#31034;&#20363;&#35757;&#32451;&#30340;&#23436;&#20840;&#30417;&#30563;&#19978;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#32422;60&#65285;&#65292;&#24182;&#19988;&#22987;&#32456;&#30456;&#27604;&#20110;&#30452;&#25509;&#22312;&#26631;&#35760;&#31034;&#20363;&#19978;&#24494;&#35843;QA&#27169;&#22411;&#65292;&#37117;&#33021;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of large, high-quality datasets has been one of the main drivers of recent progress in question answering (QA). Such annotated datasets however are difficult and costly to collect, and rarely exist in languages other than English, rendering QA technology inaccessible to underrepresented languages. An alternative to building large monolingual training datasets is to leverage pre-trained language models (PLMs) under a few-shot learning setting. Our approach, QAmeleon, uses a PLM to automatically generate multilingual data upon which QA models are trained, thus avoiding costly annotation. Prompt tuning the PLM for data synthesis with only five examples per language delivers accuracy superior to translation-based baselines, bridges nearly 60% of the gap between an English-only baseline and a fully supervised upper bound trained on almost 50,000 hand labeled examples, and always leads to substantial improvements compared to fine-tuning a QA model directly on labeled example
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#23545;&#25277;&#35937;&#23545;&#35805;&#25688;&#35201;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20174;&#22330;&#26223;&#12289;&#26041;&#27861;&#21644;&#35780;&#20272;&#30340;&#35282;&#24230;&#20998;&#31867;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#29616;&#26377;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2210.09894</link><description>&lt;p&gt;
&#25277;&#35937;&#23545;&#35805;&#25688;&#35201;&#30340;&#20998;&#31867;&#65306;&#22330;&#26223;&#12289;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Taxonomy of Abstractive Dialogue Summarization: Scenarios, Approaches and Future Directions. (arXiv:2210.09894v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#23545;&#25277;&#35937;&#23545;&#35805;&#25688;&#35201;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20174;&#22330;&#26223;&#12289;&#26041;&#27861;&#21644;&#35780;&#20272;&#30340;&#35282;&#24230;&#20998;&#31867;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#29616;&#26377;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#23545;&#35805;&#25688;&#35201;&#26159;&#29983;&#25104;&#21253;&#21547;&#23545;&#35805;&#20013;&#35201;&#28857;&#20449;&#24687;&#30340;&#31616;&#26126;&#27969;&#30021;&#25688;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#31038;&#20132;&#36890;&#35759;&#24179;&#21488;&#30340;&#22823;&#35268;&#27169;&#20986;&#29616;&#21644;&#23545;&#35805;&#20449;&#24687;&#29702;&#35299;&#19982;&#28040;&#21270;&#30340;&#32039;&#36843;&#38656;&#27714;&#65292;&#25277;&#35937;&#23545;&#35805;&#25688;&#35201;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#25991;&#26723;&#25688;&#35201;&#20013;&#30340;&#26032;&#38395;&#25110;&#25991;&#31456;&#19981;&#21516;&#65292;&#23545;&#35805;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#21644;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#35821;&#35328;&#39118;&#26684;&#21644;&#26684;&#24335;&#12289;&#20998;&#25955;&#30340;&#20449;&#24687;&#12289;&#28789;&#27963;&#30340;&#35805;&#35821;&#32467;&#26500;&#21644;&#19981;&#26126;&#30830;&#30340;&#20027;&#39064;&#36793;&#30028;&#12290;&#26412;&#35843;&#26597;&#20174;&#22330;&#26223;&#12289;&#26041;&#27861;&#21644;&#35780;&#20272;&#30340;&#35282;&#24230;&#23545;&#25277;&#35937;&#23545;&#35805;&#25688;&#35201;&#30340;&#29616;&#26377;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#26681;&#25454;&#36755;&#20837;&#23545;&#35805;&#30340;&#31867;&#22411;&#65292;&#23558;&#35813;&#20219;&#21153;&#20998;&#20026;&#24320;&#25918;&#22495;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#20004;&#20010;&#24191;&#27867;&#31867;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#26041;&#21521;&#19978;&#29616;&#26377;&#25216;&#26415;&#30340;&#20998;&#31867;&#65292;&#21363;&#27880;&#20837;&#24335;&#23545;&#35805;&#29983;&#25104;&#12289;&#29983;&#25104;&#24335;&#23545;&#35805;&#29983;&#25104;&#21644;&#30693;&#35782;&#39537;&#21160;&#22411;&#23545;&#35805;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstractive dialogue summarization is to generate a concise and fluent summary covering the salient information in a dialogue among two or more interlocutors. It has attracted great attention in recent years based on the massive emergence of social communication platforms and an urgent requirement for efficient dialogue information understanding and digestion. Different from news or articles in traditional document summarization, dialogues bring unique characteristics and additional challenges, including different language styles and formats, scattered information, flexible discourse structures and unclear topic boundaries. This survey provides a comprehensive investigation on existing work for abstractive dialogue summarization from scenarios, approaches to evaluations. It categorizes the task into two broad categories according to the type of input dialogues, i.e., open-domain and task-oriented, and presents a taxonomy of existing techniques in three directions, namely, injecting dia
&lt;/p&gt;</description></item><item><title>Claim-Dissector&#26159;&#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#24182;&#30830;&#23450;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#31995;&#32479;&#30340;&#20010;&#20154;&#36129;&#29486;&#20197;&#21450;&#35777;&#25454;&#25152;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#30340;&#36129;&#29486;&#37117;&#21487;&#20197;&#34987;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2207.14116</link><description>&lt;p&gt;
Claim-Dissector: &#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14116
&lt;/p&gt;
&lt;p&gt;
Claim-Dissector&#26159;&#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#24182;&#30830;&#23450;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#31995;&#32479;&#30340;&#20010;&#20154;&#36129;&#29486;&#20197;&#21450;&#35777;&#25454;&#25152;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#30340;&#36129;&#29486;&#37117;&#21487;&#20197;&#34987;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Claim-Dissector&#65292;&#19968;&#31181;&#38024;&#23545;&#20107;&#23454;&#26680;&#26597;&#21644;&#20998;&#26512;&#30340;&#26032;&#22411;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#32473;&#20986;&#19968;&#20010;&#22768;&#26126;&#21644;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#65292;&#32852;&#21512;&#23398;&#20064;&#35782;&#21035;&#65306;&#65288;i&#65289;&#19982;&#32473;&#23450;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#65288;ii&#65289;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#24314;&#35758;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#24320;&#27599;&#20010;&#35777;&#25454;&#30340;&#30456;&#20851;&#24615;&#27010;&#29575;&#21450;&#20854;&#23545;&#26368;&#32456;&#30495;&#23454;&#24615;&#27010;&#29575;&#30340;&#24433;&#21709;-&#26368;&#32456;&#30495;&#23454;&#24615;&#27010;&#29575;&#19982;&#27599;&#20010;&#35777;&#25454;&#30456;&#20851;&#24615;&#27010;&#29575;&#30340;&#32447;&#24615;&#25972;&#21512;&#25104;&#27604;&#20363;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27599;&#20010;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#27010;&#29575;&#30340;&#20010;&#20154;&#36129;&#29486;&#12290;&#22312;&#27599;&#20010;&#35777;&#25454;&#30340;&#30456;&#20851;&#24615;&#27010;&#29575;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#21306;&#20998;&#27599;&#20010;&#30456;&#20851;&#35777;&#25454;&#26159;&#25903;&#25345;&#65288;S&#65289;&#36824;&#26159;&#21453;&#39539;&#65288;R&#65289;&#22768;&#26126;&#12290;&#36825;&#26679;&#21487;&#20197;&#37327;&#21270;S/R&#27010;&#29575;&#23545;&#26368;&#32456;&#32467;&#35770;&#30340;&#36129;&#29486;&#25110;&#26816;&#27979;&#26377;&#24322;&#35758;&#30340;&#35777;&#25454;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#31995;&#32479;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#22312;FEVER&#31454;&#36187;&#20013;&#65292;&#20854;&#32467;&#26524;&#19982;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidences jointly learns to identify: (i) the relevant evidences to the given claim, (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way -- the final veracity probability is proportional to a linear ensemble of per-evidence relevance probabilities. In this way, the individual contributions of evidences towards the final predicted probability can be identified. In per-evidence relevance probability, our model can further distinguish whether each relevant evidence is supporting (S) or refuting (R) the claim. This allows to quantify how much the S/R probability contributes to the final verdict or to detect disagreeing evidence.  Despite its interpretable nature, our system achieves results competitive with state-of-the-art on the FEVER 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$C^3$&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35270;&#39057;&#23545;&#35805;&#20013;&#30340;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#35270;&#39057;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#23545;&#35937;&#32423;&#25110;&#21160;&#20316;&#32423;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#25552;&#39640;&#22810;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2106.08914</link><description>&lt;p&gt;
$C^3$: &#29992;&#20110;&#35270;&#39057;&#23545;&#35805;&#30340;&#32452;&#21512;&#23545;&#25239;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
$C^3$: Compositional Counterfactual Contrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$C^3$&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35270;&#39057;&#23545;&#35805;&#20013;&#30340;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#35270;&#39057;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#23545;&#35937;&#32423;&#25110;&#21160;&#20316;&#32423;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#25552;&#39640;&#22810;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#23558;&#35270;&#39057;&#29702;&#35299;&#21644;&#23545;&#35805;&#29702;&#35299;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#19982;&#23545;&#35805;&#21644;&#35270;&#39057;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#26465;&#20214;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#37096;&#20998;&#26159;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#35265;&#32780;&#38750;&#21457;&#23637;&#22810;&#27169;&#24577;&#25512;&#29702;&#23454;&#29616;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#23545;&#25239;&#23545;&#27604;&#23398;&#20064;&#65288;$C^3$&#65289;&#26041;&#27861;&#65292;&#20197;&#24320;&#21457;&#35270;&#39057;&#23545;&#35805;&#20013;&#20851;&#20110;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#26679;&#26412;&#30340;&#23545;&#27604;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#27493;&#38271;&#21644;&#23545;&#35805;&#20013;&#30340;&#26631;&#35760;&#30340;&#20107;&#23454;/&#21453;&#20107;&#23454;&#37319;&#26679;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#23545;&#35937;&#32423;&#25110;&#21160;&#20316;&#32423;&#21464;&#21270;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#38598;&#20013;&#20110;&#23545;&#27604;&#38544;&#34255;&#29366;&#24577;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-grounded dialogue systems aim to integrate video understanding and dialogue understanding to generate responses that are relevant to both the dialogue and video context. Most existing approaches employ deep learning models and have achieved remarkable performance, given the relatively small datasets available. However, the results are partly accomplished by exploiting biases in the datasets rather than developing multimodal reasoning, resulting in limited generalization. In this paper, we propose a novel approach of Compositional Counterfactual Contrastive Learning ($C^3$) to develop contrastive training between factual and counterfactual samples in video-grounded dialogues. Specifically, we design factual/counterfactual sampling based on the temporal steps in videos and tokens in dialogues and propose contrastive loss functions that exploit object-level or action-level variance. Different from prior approaches, we focus on contrastive hidden state representations among compositi
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#33521;&#35821;&#21040;&#32422;&#40065;&#24052;&#35821;&#21160;&#35789;&#30701;&#35821;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#22312;&#23478;&#24237;&#39046;&#22495;&#25910;&#38598;&#20102;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#21160;&#35789;&#30701;&#35821;&#32452;&#30340;&#35789;&#27719;&#65292;&#24182;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#39564;&#35777;&#20102;&#37325;&#20889;&#35268;&#21017;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31995;&#32479;&#30340;&#36755;&#20986;&#19982;&#35895;&#27468;&#32763;&#35793;&#30340;&#21709;&#24212;&#21305;&#37197;&#29575;&#36229;&#36807;70%&#12290;</title><link>http://arxiv.org/abs/2104.04125</link><description>&lt;p&gt;
&#35774;&#35745;&#21644;&#23454;&#29616;&#33521;&#35821;&#21040;&#32422;&#40065;&#24052;&#35821;&#21160;&#35789;&#30701;&#35821;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Design and Implementation of English To Yor\`ub\'a Verb Phrase Machine Translation System. (arXiv:2104.04125v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.04125
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#33521;&#35821;&#21040;&#32422;&#40065;&#24052;&#35821;&#21160;&#35789;&#30701;&#35821;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#22312;&#23478;&#24237;&#39046;&#22495;&#25910;&#38598;&#20102;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#21160;&#35789;&#30701;&#35821;&#32452;&#30340;&#35789;&#27719;&#65292;&#24182;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#39564;&#35777;&#20102;&#37325;&#20889;&#35268;&#21017;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31995;&#32479;&#30340;&#36755;&#20986;&#19982;&#35895;&#27468;&#32763;&#35793;&#30340;&#21709;&#24212;&#21305;&#37197;&#29575;&#36229;&#36807;70%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#33521;&#35821;&#21040;&#32422;&#40065;&#24052;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#33521;&#35821;&#21160;&#35789;&#30701;&#35821;&#25991;&#26412;&#32763;&#35793;&#25104;&#32422;&#40065;&#24052;&#35821;&#12290;&#22312;&#23478;&#24237;&#39046;&#22495;&#25910;&#38598;&#20102;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#21160;&#35789;&#30701;&#35821;&#32452;&#30340;&#35789;&#27719;&#65292;&#24182;&#36890;&#36807;&#22312;&#35789;&#20856;&#20013;&#20998;&#37197;&#21305;&#37197;&#35789;&#30340;&#20540;&#26469;&#23436;&#25104;&#35789;&#27719;&#32763;&#35793;&#12290;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#23454;&#29616;&#20102;&#20004;&#31181;&#35821;&#35328;&#30340;&#21477;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#39564;&#35777;&#20102;&#37325;&#20889;&#35268;&#21017;&#12290;&#37319;&#29992;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#24182;&#23545;&#19987;&#23478;&#27969;&#30021;&#24230;&#36827;&#34892;&#35780;&#20998;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#31995;&#32479;&#30340;&#36755;&#20986;&#19982;&#26679;&#26412;&#35895;&#27468;&#32763;&#35793;&#30340;&#21709;&#24212;&#21305;&#37197;&#29575;&#36229;&#36807;70%&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to develop an English-to-Yoruba machine translation system which can translate English verb phrase text to its Yoruba equivalent.Words from both languages Source Language and Target Language were collected for the verb phrase group in the home domain. The lexical translation is done by assigning values of the matching word in the dictionary. The syntax of the two languages was realized using Context-Free Grammar, we validated the rewrite rules with finite state automata. The human evaluation method was used and expert fluency was scored. The evaluation shows the system performed better than that of sampled Google translation with over 70 percent of the response matching that of the system's output.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#23545;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26631;&#35760;&#20462;&#25913;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#65292;&#24182;&#26088;&#22312;&#25351;&#23548;&#26032;&#30340;&#30740;&#31350;&#24182;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#25915;&#20987;&#32452;&#20214;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2103.00676</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26631;&#35760;&#20462;&#25913;&#23545;&#25239;&#25915;&#20987;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Token-Modification Adversarial Attacks for Natural Language Processing: A Survey. (arXiv:2103.00676v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.00676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#23545;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26631;&#35760;&#20462;&#25913;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#65292;&#24182;&#26088;&#22312;&#25351;&#23548;&#26032;&#30340;&#30740;&#31350;&#24182;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#25915;&#20987;&#32452;&#20214;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#24456;&#22810;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#20854;&#20013;&#65292;&#32477;&#22823;&#22810;&#25968;&#25915;&#20987;&#36890;&#36807;&#20462;&#25913;&#21333;&#20010;&#25991;&#26723;&#26631;&#35760;&#26469;&#23454;&#29616;&#25104;&#21151;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#26631;&#35760;&#20462;&#25913;&#25915;&#20987;&#12290;&#27599;&#31181;&#26631;&#35760;&#20462;&#25913;&#25915;&#20987;&#37117;&#30001;&#19968;&#32452;&#29305;&#23450;&#30340;&#22522;&#26412;&#32452;&#20214;&#23450;&#20041;&#65292;&#20363;&#22914;&#23545;&#25915;&#20987;&#32773;&#30340;&#32422;&#26463;&#25110;&#29305;&#23450;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#26631;&#35760;&#20462;&#25913;&#25915;&#20987;&#36827;&#34892;&#35843;&#26597;&#65292;&#24182;&#25552;&#21462;&#27599;&#31181;&#25915;&#20987;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19982;&#25915;&#20987;&#26080;&#20851;&#30340;&#26694;&#26550;&#26469;&#32452;&#32455;&#25105;&#20204;&#30340;&#35843;&#30740;&#65292;&#20174;&#32780;&#23545;&#35813;&#39046;&#22495;&#36827;&#34892;&#26377;&#25928;&#30340;&#20998;&#31867;&#65292;&#24182;&#26041;&#20415;&#36827;&#34892;&#32452;&#20214;&#27604;&#36739;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#25351;&#23548;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#36827;&#20837;&#36825;&#19968;&#39046;&#22495;&#65292;&#24182;&#25512;&#21160;&#23545;&#20110;&#20010;&#20307;&#25915;&#20987;&#32452;&#20214;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are now many adversarial attacks for natural language processing systems. Of these, a vast majority achieve success by modifying individual document tokens, which we call here a token-modification attack. Each token-modification attack is defined by a specific combination of fundamental components, such as a constraint on the adversary or a particular search algorithm. Motivated by this observation, we survey existing token-modification attacks and extract the components of each. We use an attack-independent framework to structure our survey which results in an effective categorisation of the field and an easy comparison of components. This survey aims to guide new researchers to this field and spark further research into individual attack components.
&lt;/p&gt;</description></item></channel></rss>