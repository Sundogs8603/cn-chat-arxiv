<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>MT-Ranker &#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#31995;&#32479;&#38388;&#25490;&#21517;&#26469;&#35299;&#20915;&#20102;&#20256;&#32479;&#29992;&#22238;&#24402;&#26041;&#27861;&#20135;&#29983;&#32477;&#23545;&#32763;&#35793;&#36136;&#37327;&#20998;&#25968;&#30340;&#19981;&#36275;&#65292;&#25552;&#20379;&#20102;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#35780;&#20998;&#32467;&#26524;&#65292;&#24182;&#22312;&#27809;&#26377;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2401.17099</link><description>&lt;p&gt;
MT-Ranker: &#26080;&#21442;&#32771;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#36890;&#36807;&#31995;&#32479;&#38388;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
MT-Ranker: Reference-free machine translation evaluation by inter-system ranking. (arXiv:2401.17099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17099
&lt;/p&gt;
&lt;p&gt;
MT-Ranker &#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#31995;&#32479;&#38388;&#25490;&#21517;&#26469;&#35299;&#20915;&#20102;&#20256;&#32479;&#29992;&#22238;&#24402;&#26041;&#27861;&#20135;&#29983;&#32477;&#23545;&#32763;&#35793;&#36136;&#37327;&#20998;&#25968;&#30340;&#19981;&#36275;&#65292;&#25552;&#20379;&#20102;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#35780;&#20998;&#32467;&#26524;&#65292;&#24182;&#22312;&#27809;&#26377;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#35780;&#20272;&#34987;&#35270;&#20026;&#22238;&#24402;&#38382;&#39064;&#65292;&#21363;&#20135;&#29983;&#19968;&#20010;&#32477;&#23545;&#30340;&#32763;&#35793;&#36136;&#37327;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;&#19968;&#26159;&#20998;&#25968;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#20154;&#31867;&#26631;&#27880;&#21592;&#24456;&#38590;&#32473;&#20986;&#19968;&#33268;&#30340;&#20998;&#25968;&#65307;&#20108;&#26159;&#22823;&#22810;&#25968;&#35780;&#20998;&#26041;&#27861;&#22522;&#20110;&#65288;&#21442;&#32771;&#65292;&#32763;&#35793;&#65289;&#23545;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#27809;&#26377;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#24120;&#24120;&#20851;&#24515;&#19968;&#20010;&#26032;&#30340;MT&#31995;&#32479;&#26159;&#21542;&#27604;&#26576;&#20123;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#25110;&#26356;&#24046;&#12290;&#27492;&#22806;&#65292;&#26080;&#21442;&#32771;MT&#35780;&#20272;&#22312;&#23454;&#36341;&#20013;&#36234;&#26469;&#36234;&#23454;&#29992;&#21644;&#24517;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20004;&#20010;&#23454;&#36341;&#32771;&#34385;&#22240;&#32032;&#23578;&#26410;&#20849;&#21516;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#26080;&#21442;&#32771;MT&#35780;&#20272;&#36716;&#21270;&#20026;&#19968;&#20010;&#25104;&#23545;&#25490;&#21517;&#38382;&#39064;&#12290;&#32473;&#23450;&#28304;&#21477;&#23376;&#21644;&#19968;&#23545;&#32763;&#35793;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#39044;&#27979;&#21738;&#20010;&#32763;&#35793;&#26356;&#22909;&#12290;&#38500;&#20102;&#25552;&#20986;&#36825;&#31181;&#26032;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#30340;&#33539;&#24335;&#21487;&#20197;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, Machine Translation (MT) Evaluation has been treated as a regression problem -- producing an absolute translation-quality score. This approach has two limitations: i) the scores lack interpretability, and human annotators struggle with giving consistent scores; ii) most scoring methods are based on (reference, translation) pairs, limiting their applicability in real-world scenarios where references are absent. In practice, we often care about whether a new MT system is better or worse than some competitors. In addition, reference-free MT evaluation is increasingly practical and necessary. Unfortunately, these two practical considerations have yet to be jointly explored. In this work, we formulate the reference-free MT evaluation into a pairwise ranking problem. Given the source sentence and a pair of translations, our system predicts which translation is better. In addition to proposing this new formulation, we further show that this new paradigm can demonstrate superior
&lt;/p&gt;</description></item><item><title>StrokeNUWA&#26159;&#19968;&#20010;&#25506;&#32034;&#22312;&#30690;&#37327;&#22270;&#24418;&#19978;&#26356;&#22909;&#30340;&#35270;&#35273;&#34920;&#31034;&#30340;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#31508;&#30011;&#26631;&#35760;&#8221;&#34920;&#31034;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#30690;&#37327;&#22270;&#24418;&#29983;&#25104;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.17093</link><description>&lt;p&gt;
StrokeNUWA&#65306;&#29992;&#20110;&#30690;&#37327;&#22270;&#24418;&#21512;&#25104;&#30340;&#31508;&#30011;&#20998;&#35789;
&lt;/p&gt;
&lt;p&gt;
StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis. (arXiv:2401.17093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17093
&lt;/p&gt;
&lt;p&gt;
StrokeNUWA&#26159;&#19968;&#20010;&#25506;&#32034;&#22312;&#30690;&#37327;&#22270;&#24418;&#19978;&#26356;&#22909;&#30340;&#35270;&#35273;&#34920;&#31034;&#30340;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#31508;&#30011;&#26631;&#35760;&#8221;&#34920;&#31034;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#30690;&#37327;&#22270;&#24418;&#29983;&#25104;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21033;&#29992;LLMs&#36827;&#34892;&#35270;&#35273;&#21512;&#25104;&#65292;&#20256;&#32479;&#26041;&#27861;&#36890;&#36807;&#19987;&#38376;&#30340;&#35270;&#35273;&#27169;&#22359;&#23558;&#20809;&#26629;&#22270;&#20687;&#20449;&#24687;&#36716;&#25442;&#20026;&#31163;&#25955;&#30340;&#32593;&#26684;&#26631;&#35760;&#65292;&#20174;&#32780;&#30772;&#22351;&#20102;&#27169;&#22411;&#25429;&#25417;&#35270;&#35273;&#22330;&#26223;&#30340;&#30495;&#23454;&#35821;&#20041;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#19968;&#31181;&#26367;&#20195;&#22270;&#20687;&#34920;&#31034;&#26041;&#27861;&#65292;&#21363;&#30690;&#37327;&#22270;&#24418;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#23545;&#22270;&#20687;&#20449;&#24687;&#26356;&#33258;&#28982;&#12289;&#26356;&#35821;&#20041;&#36830;&#36143;&#30340;&#20998;&#21106;&#26469;&#26377;&#25928;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StrokeNUWA&#65292;&#36825;&#26159;&#19968;&#39033;&#24320;&#21019;&#24615;&#24037;&#20316;&#65292;&#25506;&#32034;&#22312;&#30690;&#37327;&#22270;&#24418;&#19978;&#26356;&#22909;&#30340;&#35270;&#35273;&#34920;&#31034;&#8220;&#31508;&#30011;&#26631;&#35760;&#8221;&#65292;&#23427;&#26412;&#36136;&#19978;&#20855;&#26377;&#20016;&#23500;&#30340;&#35270;&#35273;&#35821;&#20041;&#65292;&#19982;LLMs&#33258;&#28982;&#20860;&#23481;&#19988;&#39640;&#24230;&#21387;&#32553;&#12290;&#37197;&#22791;&#31508;&#30011;&#26631;&#35760;&#65292;StrokeNUWA&#22312;&#30690;&#37327;&#22270;&#24418;&#29983;&#25104;&#20219;&#21153;&#20013;&#21487;&#20197;&#26174;&#33879;&#36229;&#36234;&#20256;&#32479;&#30340;&#22522;&#20110;LLM&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#25104;&#32489;&#12290;&#27492;&#22806;&#65292;StrokeNUWA&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#36895;&#24230;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;94&#20493;&#65292;&#24182;&#20855;&#26377;&#38750;&#24120;&#20986;&#33394;&#30340;SVG&#20195;&#30721;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
To leverage LLMs for visual synthesis, traditional methods convert raster image information into discrete grid tokens through specialized visual modules, while disrupting the model's ability to capture the true semantic representation of visual scenes. This paper posits that an alternative representation of images, vector graphics, can effectively surmount this limitation by enabling a more natural and semantically coherent segmentation of the image information. Thus, we introduce StrokeNUWA, a pioneering work exploring a better visual representation ''stroke tokens'' on vector graphics, which is inherently visual semantics rich, naturally compatible with LLMs, and highly compressed. Equipped with stroke tokens, StrokeNUWA can significantly surpass traditional LLM-based and optimization-based methods across various metrics in the vector graphic generation task. Besides, StrokeNUWA achieves up to a 94x speedup in inference over the speed of prior methods with an exceptional SVG code com
&lt;/p&gt;</description></item><item><title>NNOSE&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#26377;&#25928;&#25552;&#21462;&#32844;&#19994;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#20854;&#20182;&#25968;&#25454;&#38598;&#20013;&#30340;&#37051;&#36817;&#25216;&#33021;&#26469;&#25552;&#39640;&#25216;&#33021;&#25552;&#21462;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2401.17092</link><description>&lt;p&gt;
NNOSE&#65306;&#26368;&#36817;&#37051;&#32844;&#19994;&#25216;&#33021;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
NNOSE: Nearest Neighbor Occupational Skill Extraction. (arXiv:2401.17092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17092
&lt;/p&gt;
&lt;p&gt;
NNOSE&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#26377;&#25928;&#25552;&#21462;&#32844;&#19994;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#20854;&#20182;&#25968;&#25454;&#38598;&#20013;&#30340;&#37051;&#36817;&#25216;&#33021;&#26469;&#25552;&#39640;&#25216;&#33021;&#25552;&#21462;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21171;&#21160;&#21147;&#24066;&#22330;&#27491;&#22312;&#24555;&#36895;&#21464;&#21270;&#65292;&#24341;&#21457;&#20102;&#23545;&#20174;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#32844;&#19994;&#25216;&#33021;&#30340;&#22686;&#21152;&#20852;&#36259;&#12290;&#38543;&#30528;&#33521;&#35821;&#22522;&#20934;&#24037;&#20316;&#25551;&#36848;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#65292;&#38656;&#35201;&#22788;&#29702;&#23427;&#20204;&#30340;&#22810;&#26679;&#24615;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#32844;&#19994;&#25216;&#33021;&#25968;&#25454;&#38598;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#24615;&#65292;&#36890;&#36807;&#21512;&#24182;&#21644;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#26469;&#25552;&#21462;&#25216;&#33021;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#23569;&#35265;&#30340;&#25216;&#33021;&#65292;&#24182;&#20811;&#26381;&#25968;&#25454;&#38598;&#38388;&#25216;&#33021;&#30340;&#31232;&#32570;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#65292;&#37319;&#29992;&#22806;&#37096;&#25968;&#25454;&#23384;&#20648;&#26469;&#20197;&#25968;&#25454;&#38598;&#32479;&#19968;&#30340;&#26041;&#24335;&#26816;&#32034;&#30456;&#20284;&#30340;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;NNOSE&#65288;Nearest Neighbor Occupational Skill Extraction&#65289;&#36890;&#36807;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#26816;&#32034;&#20854;&#20182;&#25968;&#25454;&#38598;&#20013;&#30340;&#37051;&#36817;&#25216;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#12290;&#36825;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25216;&#33021;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The labor market is changing rapidly, prompting increased interest in the automatic extraction of occupational skills from text. With the advent of English benchmark job description datasets, there is a need for systems that handle their diversity well. We tackle the complexity in occupational skill datasets tasks -- combining and leveraging multiple datasets for skill extraction, to identify rarely observed skills within a dataset, and overcoming the scarcity of skills across datasets. In particular, we investigate the retrieval-augmentation of language models, employing an external datastore for retrieving similar skills in a dataset-unifying manner. Our proposed method, \textbf{N}earest \textbf{N}eighbor \textbf{O}ccupational \textbf{S}kill \textbf{E}xtraction (NNOSE) effectively leverages multiple datasets by retrieving neighboring skills from other datasets in the datastore. This improves skill extraction \emph{without} additional fine-tuning. Crucially, we observe a performance g
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SemScore&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#30452;&#25509;&#27604;&#36739;&#27169;&#22411;&#36755;&#20986;&#21644;&#40644;&#37329;&#30446;&#26631;&#22238;&#24212;&#65292;&#29992;&#20110;&#35780;&#20272;&#25351;&#20196;&#35843;&#26657;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SemScore&#25351;&#26631;&#22312;&#19982;&#20154;&#24037;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.17072</link><description>&lt;p&gt;
SemScore: &#22522;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#30340;&#25351;&#20196;&#35843;&#26657;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity. (arXiv:2401.17072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17072
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SemScore&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#30452;&#25509;&#27604;&#36739;&#27169;&#22411;&#36755;&#20986;&#21644;&#40644;&#37329;&#30446;&#26631;&#22238;&#24212;&#65292;&#29992;&#20110;&#35780;&#20272;&#25351;&#20196;&#35843;&#26657;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SemScore&#25351;&#26631;&#22312;&#19982;&#20154;&#24037;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#20196;&#35843;&#26657;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#36866;&#21512;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22238;&#24212;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24403;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#25163;&#21160;&#35780;&#20272;&#26469;&#21028;&#26029;&#29983;&#25104;&#22238;&#24212;&#30340;&#36136;&#37327;&#12290;&#30001;&#20110;&#36825;&#31181;&#25163;&#21160;&#35780;&#20272;&#32791;&#26102;&#65292;&#19981;&#23481;&#26131;&#25193;&#23637;&#21040;&#23545;&#22810;&#20010;&#27169;&#22411;&#21644;&#27169;&#22411;&#21464;&#20307;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#30701;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#35780;&#20272;&#25351;&#26631;SemScore&#65292;&#36890;&#36807;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#30452;&#25509;&#23558;&#27169;&#22411;&#36755;&#20986;&#19982;&#40644;&#37329;&#30446;&#26631;&#22238;&#24212;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;12&#20010;&#30693;&#21517;&#30340;&#25351;&#20196;&#35843;&#26657;LLMs&#30340;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#20102;&#22522;&#20110;8&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#30340;&#27604;&#36739;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;SemScore&#25351;&#26631;&#22312;&#19982;&#20154;&#24037;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#12289;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#26356;&#22797;&#26434;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#23545;&#20110;&#35780;&#20272;&#25351;&#20196;&#35843;&#26657;LLMs&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable advancements in their ability to generate fitting responses to natural language instructions. However, many current works rely on manual evaluation to judge the quality of generated responses. Since such manual evaluation is time-consuming, it does not easily scale to the evaluation of multiple models and model variants. In this short paper, we propose a straightforward but remarkably effective evaluation metric called SemScore, in which we directly compare model outputs to gold target responses using semantic textual similarity (STS). We conduct a comparative evaluation of the model outputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation metrics for text generation. We find that our proposed SemScore metric outperforms all other, in many cases more complex, evaluation metrics in terms of correlation to human evaluation. These findings indicate the utility of our proposed metric for 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26356;&#20840;&#38754;&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#25152;&#26377;&#32452;&#20214;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.17043</link><description>&lt;p&gt;
CRUD-RAG: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20013;&#25991;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models. (arXiv:2401.17043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17043
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26356;&#20840;&#38754;&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#25152;&#26377;&#32452;&#20214;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#28304;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;LLM&#30340;&#24120;&#35265;&#38480;&#21046;&#65292;&#21253;&#25324;&#36807;&#26102;&#30340;&#20449;&#24687;&#21644;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#8220;&#34394;&#26500;&#8221;&#20869;&#23481;&#30340;&#20542;&#21521;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;RAG&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#22312;&#33539;&#22260;&#21644;&#22810;&#26679;&#24615;&#19978;&#23384;&#22312;&#38480;&#21046;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#35780;&#20272;&#38382;&#31572;&#24212;&#29992;&#65292;&#24573;&#35270;&#20102;RAG&#21487;&#33021;&#26377;&#20248;&#21183;&#30340;&#26356;&#24191;&#27867;&#30340;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21482;&#35780;&#20272;RAG&#27969;&#31243;&#20013;LLM&#32452;&#20214;&#30340;&#24615;&#33021;&#65292;&#24182;&#24573;&#35270;&#26816;&#32034;&#32452;&#20214;&#21644;&#22806;&#37096;&#30693;&#35782;&#25968;&#25454;&#24211;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#22312;&#21508;&#31181;RAG&#24212;&#29992;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;RAG&#31995;&#32479;&#30340;&#25152;&#26377;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content. However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous. Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we have categorized the ran
&lt;/p&gt;</description></item><item><title>&#25191;&#34892;&#21160;&#20316;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#23545;&#20110;&#25351;&#31034;&#28548;&#28165;&#35831;&#27714;&#65288;iCR&#65289;&#31574;&#30053;&#30340;&#23398;&#20064;&#36129;&#29486;&#26377;&#38480;&#65292;&#20294;&#21487;&#20197;&#20174;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#20309;&#26102;&#25552;&#38382;&#25351;&#31034;CR&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#20309;&#26102;&#25552;&#38382;&#30340;&#20219;&#21153;&#21487;&#20197;&#26356;&#25104;&#21151;&#22320;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2401.17039</link><description>&lt;p&gt;
&#21521;&#20248;&#38597;&#20114;&#21160;&#36808;&#36827;&#65306;&#25191;&#34892;&#21160;&#20316;&#23545;&#25351;&#31034;&#28548;&#28165;&#35831;&#27714;&#24314;&#27169;&#31574;&#30053;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Taking Action Towards Graceful Interaction: The Effects of Performing Actions on Modelling Policies for Instruction Clarification Requests. (arXiv:2401.17039v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17039
&lt;/p&gt;
&lt;p&gt;
&#25191;&#34892;&#21160;&#20316;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#23545;&#20110;&#25351;&#31034;&#28548;&#28165;&#35831;&#27714;&#65288;iCR&#65289;&#31574;&#30053;&#30340;&#23398;&#20064;&#36129;&#29486;&#26377;&#38480;&#65292;&#20294;&#21487;&#20197;&#20174;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#20309;&#26102;&#25552;&#38382;&#25351;&#31034;CR&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#20309;&#26102;&#25552;&#38382;&#30340;&#20219;&#21153;&#21487;&#20197;&#26356;&#25104;&#21151;&#22320;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28548;&#28165;&#35831;&#27714;&#26159;&#35299;&#20915;&#25351;&#31034;&#36981;&#24490;&#20114;&#21160;&#20013;&#30340;&#27807;&#36890;&#38382;&#39064;&#65288;&#20363;&#22914;&#30001;&#20110;&#27495;&#20041;&#25110;&#27169;&#31946;&#19981;&#28165;&#65289;&#30340;&#19968;&#31181;&#26426;&#21046;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#37325;&#35201;&#65292;&#20294;&#21363;&#20351;&#26159;&#29087;&#32451;&#30340;&#27169;&#22411;&#20063;&#38590;&#20197;&#20135;&#29983;&#25110;&#35299;&#37322;&#36825;&#31181;&#20462;&#22797;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20851;&#20110;&#23558;&#21160;&#20316;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#22312;&#24314;&#27169;iCR&#31574;&#30053;&#20013;&#30340;&#24433;&#21709;&#30340;&#19977;&#20010;&#20551;&#35774;&#12290;&#19982;&#26368;&#21021;&#30340;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#20854;&#23545;&#23398;&#20064;iCR&#31574;&#30053;&#30340;&#36129;&#29486;&#26377;&#38480;&#65292;&#20294;&#20173;&#28982;&#21487;&#20197;&#20174;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20013;&#25552;&#21462;&#19968;&#20123;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#35777;&#25454;&#65292;&#21363;&#21363;&#20351;&#26159;&#21160;&#26426;&#20805;&#20998;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20063;&#26080;&#27861;&#23398;&#20064;&#21040;&#20160;&#20040;&#26102;&#20505;&#25552;&#38382;&#25351;&#31034;CR&#30340;&#33391;&#22909;&#31574;&#30053;&#65292;&#32780;&#20915;&#23450;&#20309;&#26102;&#25552;&#38382;&#21487;&#20197;&#26356;&#25104;&#21151;&#22320;&#36827;&#34892;&#24314;&#27169;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#33539;&#24335;&#23398;&#20064;&#20803;&#36890;&#20449;&#34892;&#20026;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clarification requests are a mechanism to help solve communication problems, e.g. due to ambiguity or underspecification, in instruction-following interactions. Despite their importance, even skilful models struggle with producing or interpreting such repair acts. In this work, we test three hypotheses concerning the effects of action taking as an auxiliary task in modelling iCR policies. Contrary to initial expectations, we conclude that its contribution to learning an iCR policy is limited, but some information can still be extracted from prediction uncertainty. We present further evidence that even well-motivated, Transformer-based models fail to learn good policies for when to ask Instruction CRs (iCRs), while the task of determining what to ask about can be more successfully modelled. Considering the implications of these findings, we further discuss the shortcomings of the data-driven paradigm for learning meta-communication acts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36776;&#21035;&#34394;&#26500;&#22768;&#38899;&#30340;&#20316;&#32773;&#39564;&#35777;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#23567;&#35828;&#30340;&#35821;&#26009;&#24211;&#20013;&#23545;&#35282;&#33394;&#30340;&#24341;&#35821;&#36827;&#34892;&#39118;&#26684;&#34920;&#31034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#27169;&#22411;&#20013;&#65292;&#25429;&#25417;&#21040;&#30340;&#39118;&#26684;&#21644;&#20027;&#39064;&#20449;&#24687;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;&#20294;&#22312;&#24402;&#23646;&#24341;&#35821;&#26041;&#38754;&#24182;&#19981;&#19968;&#23450;&#27604;&#20165;&#26377;&#35821;&#20041;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.16968</link><description>&lt;p&gt;
&#36776;&#21035;&#34394;&#26500;&#22768;&#38899;&#65306;&#24341;&#25991;&#24402;&#23646;&#30340;&#20316;&#32773;&#39564;&#35777;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Distinguishing Fictional Voices: a Study of Authorship Verification Models for Quotation Attribution. (arXiv:2401.16968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36776;&#21035;&#34394;&#26500;&#22768;&#38899;&#30340;&#20316;&#32773;&#39564;&#35777;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#23567;&#35828;&#30340;&#35821;&#26009;&#24211;&#20013;&#23545;&#35282;&#33394;&#30340;&#24341;&#35821;&#36827;&#34892;&#39118;&#26684;&#34920;&#31034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#27169;&#22411;&#20013;&#65292;&#25429;&#25417;&#21040;&#30340;&#39118;&#26684;&#21644;&#20027;&#39064;&#20449;&#24687;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;&#20294;&#22312;&#24402;&#23646;&#24341;&#35821;&#26041;&#38754;&#24182;&#19981;&#19968;&#23450;&#27604;&#20165;&#26377;&#35821;&#20041;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#21160;&#26816;&#27979;&#30452;&#25509;&#24341;&#35821;&#30340;&#35762;&#35805;&#32773;&#30340;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#20851;&#20110;&#35282;&#33394;&#30340;&#19968;&#33324;&#20449;&#24687;&#65292;&#32780;&#20542;&#21521;&#20110;&#19978;&#19979;&#25991;&#20013;&#25214;&#21040;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#20363;&#22914;&#21608;&#22260;&#23454;&#20307;&#30340;&#25552;&#21450;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23545;&#35282;&#33394;&#36827;&#34892;&#39118;&#26684;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#20316;&#32773;&#39564;&#35777;&#27169;&#22411;&#22312;&#33521;&#35821;&#23567;&#35828;&#30340;&#22823;&#35821;&#26009;&#24211;&#65288;&#39033;&#30446;&#23545;&#35805;&#23567;&#35828;&#35821;&#26009;&#24211;&#65289;&#20013;&#32534;&#30721;&#20182;&#20204;&#30340;&#24341;&#35821;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19968;&#20123;&#36825;&#20123;&#27169;&#22411;&#20013;&#25429;&#25417;&#21040;&#30340;&#39118;&#26684;&#21644;&#20027;&#39064;&#20449;&#24687;&#30340;&#32452;&#21512;&#33021;&#22815;&#20934;&#30830;&#22320;&#21306;&#20998;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;&#20294;&#24403;&#24402;&#22240;&#24341;&#35821;&#26102;&#65292;&#19981;&#19968;&#23450;&#20248;&#20110;&#20165;&#26377;&#35821;&#20041;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#22312;&#19981;&#21516;&#30340;&#23567;&#35828;&#20013;&#26377;&#25152;&#21464;&#21270;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#19987;&#38376;&#38024;&#23545;&#25991;&#23398;&#25991;&#26412;&#21644;&#35282;&#33394;&#30740;&#31350;&#35774;&#35745;&#30340;&#25991;&#20307;&#27979;&#37327;&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches to automatically detect the speaker of an utterance of direct speech often disregard general information about characters in favor of local information found in the context, such as surrounding mentions of entities. In this work, we explore stylistic representations of characters built by encoding their quotes with off-the-shelf pretrained Authorship Verification models in a large corpus of English novels (the Project Dialogism Novel Corpus). Results suggest that the combination of stylistic and topical information captured in some of these models accurately distinguish characters among each other, but does not necessarily improve over semantic-only models when attributing quotes. However, these results vary across novels and more investigation of stylometric models particularly tailored for literary texts and the study of characters should be conducted.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65288;LLMEA&#65289;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#21319;&#23454;&#20307;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16960</link><description>&lt;p&gt;
&#20004;&#20010;&#22836;&#32988;&#20110;&#19968;: &#20026;&#23454;&#20307;&#23545;&#40784;&#25972;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Two Heads Are Better Than One: Integrating Knowledge from Knowledge Graphs and Large Language Models for Entity Alignment. (arXiv:2401.16960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65288;LLMEA&#65289;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#21319;&#23454;&#20307;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#26159;&#21019;&#24314;&#26356;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#20808;&#20915;&#26465;&#20214;&#65292;&#28041;&#21450;&#22312;&#19981;&#21516;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#23450;&#20301;&#31561;&#20215;&#23454;&#20307;&#12290;&#30446;&#21069;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#30693;&#35782;&#23884;&#20837;&#27169;&#22411;&#33719;&#21462;&#21253;&#21547;&#21508;&#31181;&#30456;&#20284;&#24615;&#65288;&#32467;&#26500;&#12289;&#20851;&#31995;&#21644;&#23646;&#24615;&#65289;&#30340;&#23454;&#20307;&#23884;&#20837;&#12290;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#34701;&#21512;&#26426;&#21046;&#36827;&#34892;&#38598;&#25104;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22266;&#26377;&#30340;&#24322;&#36136;&#24615;&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#26041;&#38754;&#30340;&#20449;&#24687;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#38544;&#24335;&#25429;&#25417;&#23454;&#20307;&#35821;&#20041;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36825;&#31181;&#38544;&#24335;&#30693;&#35782;&#23578;&#26410;&#34987;&#29992;&#20110;&#23454;&#20307;&#23545;&#40784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#23454;&#20307;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65288;LLMEA&#65289;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#30693;&#35782;&#19982;LLM&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment, which is a prerequisite for creating a more comprehensive Knowledge Graph (KG), involves pinpointing equivalent entities across disparate KGs. Contemporary methods for entity alignment have predominantly utilized knowledge embedding models to procure entity embeddings that encapsulate various similarities-structural, relational, and attributive. These embeddings are then integrated through attention-based information fusion mechanisms. Despite this progress, effectively harnessing multifaceted information remains challenging due to inherent heterogeneity. Moreover, while Large Language Models (LLMs) have exhibited exceptional performance across diverse downstream tasks by implicitly capturing entity semantics, this implicit knowledge has yet to be exploited for entity alignment. In this study, we propose a Large Language Model-enhanced Entity Alignment framework (LLMEA), integrating structural knowledge from KGs with semantic knowledge from LLMs to enhance entity alig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#39532;&#32819;&#20182;&#35821;&#36825;&#31181;&#28151;&#21512;&#35821;&#35328;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#28151;&#21512;&#35821;&#35328;&#25991;&#23383;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16895</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#30740;&#31350;&#65306;&#23558;&#20302;&#36164;&#28304;&#39532;&#32819;&#20182;&#35821;&#35270;&#20026;&#22810;&#35821;&#35328;&#20195;&#30721;&#20999;&#25442;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Transfer from Related Languages: Treating Low-Resource Maltese as Multilingual Code-Switching. (arXiv:2401.16895v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#39532;&#32819;&#20182;&#35821;&#36825;&#31181;&#28151;&#21512;&#35821;&#35328;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#28151;&#21512;&#35821;&#35328;&#25991;&#23383;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#65292;&#20294;&#22312;&#19982;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#35821;&#35328;&#23384;&#22312;&#25991;&#23383;&#24046;&#24322;&#26102;&#65292;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#20351;&#29992;&#38899;&#35793;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#25509;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#30340;&#25991;&#23383;&#19982;&#30446;&#26631;&#35821;&#35328;&#30340;&#25991;&#23383;&#23545;&#40784;&#65292;&#20174;&#32780;&#22686;&#24378;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#28151;&#21512;&#35821;&#35328;&#26469;&#35828;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#22240;&#20026;&#21482;&#26377;&#35821;&#35328;&#30340;&#26576;&#20010;&#23376;&#38598;&#20174;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#21463;&#30410;&#65292;&#32780;&#20854;&#20313;&#37096;&#20998;&#21463;&#21040;&#38459;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#39532;&#32819;&#20182;&#35821;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#38463;&#25289;&#20271;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#33521;&#35821;&#37325;&#22823;&#24433;&#21709;&#65292;&#24182;&#19988;&#37319;&#29992;&#25289;&#19969;&#25991;&#33050;&#26412;&#30340;&#38378;&#31859;&#29305;&#35821;&#35328;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#21333;&#35789;&#32423;&#21035;&#30340;&#35789;&#28304;&#23398;&#27880;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26126;&#26234;&#20915;&#31574;&#22914;&#20309;&#22788;&#29702;&#39532;&#32819;&#20182;&#35821;&#30340;&#27599;&#20010;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although multilingual language models exhibit impressive cross-lingual transfer capabilities on unseen languages, the performance on downstream tasks is impacted when there is a script disparity with the languages used in the multilingual model's pre-training data. Using transliteration offers a straightforward yet effective means to align the script of a resource-rich language with a target language, thereby enhancing cross-lingual transfer capabilities. However, for mixed languages, this approach is suboptimal, since only a subset of the language benefits from the cross-lingual transfer while the remainder is impeded. In this work, we focus on Maltese, a Semitic language, with substantial influences from Arabic, Italian, and English, and notably written in Latin script. We present a novel dataset annotated with word-level etymology. We use this dataset to train a classifier that enables us to make informed decisions regarding the appropriate processing of each token in the Maltese la
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#29366;&#24577;&#20540;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#21644;&#33258;&#35757;&#32451;&#26469;&#25913;&#21892;&#29366;&#24577;&#20540;&#29983;&#25104;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16862</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;&#23398;&#20064;&#21644;&#33258;&#35757;&#32451;&#30340;&#20302;&#36164;&#28304;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20013;&#30340;&#29366;&#24577;&#20540;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
State Value Generation with Prompt Learning and Self-Training for Low-Resource Dialogue State Tracking. (arXiv:2401.16862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16862
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#29366;&#24577;&#20540;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#21644;&#33258;&#35757;&#32451;&#26469;&#25913;&#21892;&#29366;&#24577;&#20540;&#29983;&#25104;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20302;&#36164;&#28304;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#39318;&#20808;&#33719;&#21462;&#29366;&#24577;&#20540;&#65292;&#28982;&#21518;&#22522;&#20110;&#20540;&#26469;&#29983;&#25104;&#27133;&#20301;&#31867;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#29366;&#24577;&#20540;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#25277;&#21462;&#30340;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#38656;&#35201;&#19978;&#19979;&#25991;&#29702;&#35299;&#30340;&#20540;&#65292;&#24182;&#19988;&#20063;&#19981;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#20540;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65288;SVAG&#65289;&#65292;&#23558;DST&#20998;&#35299;&#20026;&#29366;&#24577;&#20540;&#29983;&#25104;&#21644;&#39046;&#22495;&#27133;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#29366;&#24577;&#20540;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#33258;&#35757;&#32451;&#36827;&#19968;&#27493;&#25913;&#21892;&#29366;&#24577;&#20540;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#65292;&#26088;&#22312;&#22312;&#33258;&#35757;&#32451;&#26399;&#38388;&#26816;&#27979;&#19981;&#23436;&#25972;&#29983;&#25104;&#21644;&#38169;&#35823;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#25968;&#25454;&#36873;&#25321;&#12290;&#22312;MultiWOZ 2.1&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#26377;&#19981;&#21040;10&#20159;&#20010;&#21442;&#25968;&#65292;&#23601;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, low-resource dialogue state tracking (DST) has received increasing attention. First obtaining state values then based on values to generate slot types has made great progress in this task. However, obtaining state values is still an under-studied problem. Existing extraction-based approaches cannot capture values that require the understanding of context and are not generalizable either. To address these issues, we propose a novel State VAlue Generation based framework (SVAG), decomposing DST into state value generation and domain slot generation. Specifically, we propose to generate state values and use self-training to further improve state value generation. Moreover, we design an estimator aiming at detecting incomplete generation and incorrect generation for pseudo-labeled data selection during self-training. Experimental results on the MultiWOZ 2.1 dataset show that our method which has only less than 1 billion parameters achieves state-of-the-art performance under the d
&lt;/p&gt;</description></item><item><title>H2O-Danube-1.8B &#26159;&#19968;&#20010;&#22312; 1T &#20010;&#26631;&#35760;&#19978;&#35757;&#32451;&#30340; 18 &#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#25351;&#26631;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#21644;&#20248;&#21270;&#35757;&#32451;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.16818</link><description>&lt;p&gt;
H2O-Danube-1.8B &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
H2O-Danube-1.8B Technical Report. (arXiv:2401.16818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16818
&lt;/p&gt;
&lt;p&gt;
H2O-Danube-1.8B &#26159;&#19968;&#20010;&#22312; 1T &#20010;&#26631;&#35760;&#19978;&#35757;&#32451;&#30340; 18 &#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#25351;&#26631;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#21644;&#20248;&#21270;&#35757;&#32451;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; H2O-Danube-1.8B&#65292;&#36825;&#26159;&#19968;&#20010;&#22312; 1T &#20010;&#26631;&#35760;&#19978;&#35757;&#32451;&#30340; 18 &#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#36981;&#24490; LLama 2 &#21644; Mistral &#30340;&#26680;&#24515;&#21407;&#21017;&#12290;&#25105;&#20204;&#21033;&#29992;&#21644;&#25913;&#36827;&#20102;&#21508;&#31181;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#27169;&#22411;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#24635;&#26631;&#35760;&#25968;&#37327;&#26126;&#26174;&#23569;&#20110;&#30456;&#20284;&#35268;&#27169;&#30340;&#21442;&#32771;&#27169;&#22411;&#65292;&#20294;&#23427;&#22312;&#20247;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#35757;&#32451;&#30340;&#32842;&#22825;&#27169;&#22411;&#12290;&#25105;&#20204;&#20197; Apache 2.0 &#35768;&#21487;&#35777;&#23558; H2O-Danube-1.8B &#24320;&#25918;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160; LLMs &#30340;&#32463;&#27982;&#27665;&#20027;&#21270;&#65292;&#35753;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present H2O-Danube-1.8B, a 1.8B language model trained on 1T tokens following the core principles of LLama 2 and Mistral. We leverage and refine various techniques for pre-training large language models. Although our model is trained on significantly fewer total tokens compared to reference models of similar size, it exhibits highly competitive metrics across a multitude of benchmarks. We additionally release a chat model trained with supervised fine-tuning followed by direct preference optimization. We make H2O-Danube-1.8B openly available under Apache 2.0 license further democratizing LLMs to a wider audience economically.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ScaleEval&#65292;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#36777;&#35770;&#30340;&#20803;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20132;&#27969;&#22411;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#26469;&#26377;&#25928;&#12289;&#21487;&#38752;&#12289;&#39640;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16788</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#32773;&#26159;&#21542;&#21487;&#20449;&#65311;&#36890;&#36807;&#20195;&#29702;&#36777;&#35770;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20803;&#35780;&#20272;&#26469;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate. (arXiv:2401.16788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ScaleEval&#65292;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#36777;&#35770;&#30340;&#20803;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20132;&#27969;&#22411;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#26469;&#26377;&#25928;&#12289;&#21487;&#38752;&#12289;&#39640;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#20855;&#26377;&#23454;&#29992;&#24615;&#65292;&#20294;&#35201;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#21487;&#38752;&#22320;&#35780;&#20272;LLMs&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#20195;&#35780;&#20272;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;LLMs&#26469;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;LLMs&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#20803;&#35780;&#20272;&#36890;&#24120;&#21463;&#29616;&#26377;&#22522;&#20934;&#30340;&#35206;&#30422;&#33539;&#22260;&#38480;&#21046;&#65292;&#25110;&#32773;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#12290;&#36825;&#20984;&#26174;&#20102;&#36843;&#20999;&#38656;&#35201;&#21487;&#25193;&#23637;&#30340;&#20803;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#12289;&#21487;&#38752;&#12289;&#39640;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28508;&#22312;&#30340;&#26032;&#30340;&#12289;&#29992;&#25143;&#23450;&#20041;&#30340;&#22330;&#26223;&#20013;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ScaleEval&#65292;&#36825;&#26159;&#19968;&#20010;&#20195;&#29702;&#36777;&#35770;&#36741;&#21161;&#30340;&#20803;&#35780;&#20272;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#20132;&#27969;&#22411;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#26694;&#26550;&#25903;&#25345;&#22810;&#36718;&#35752;&#35770;&#65292;&#20197;&#24110;&#21161;&#20154;&#24037;&#26631;&#27880;&#32773;&#21028;&#26029;&#26368;&#20855;&#33021;&#21147;&#30340;
&lt;/p&gt;
&lt;p&gt;
Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#20013;&#31181;&#26063;&#20027;&#20041;&#25991;&#26412;&#30340;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;87.94%&#30340;&#20934;&#30830;&#29575;&#12290;&#27169;&#22411;&#30340;&#26368;&#20339;&#34920;&#29616;&#26159;&#20351;&#29992;MCNN-LSTM&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;BERT&#23884;&#20837;&#21644;RNN/LSTM&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#32456;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16748</link><description>&lt;p&gt;
&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#26816;&#27979;&#31181;&#26063;&#20027;&#20041;&#25991;&#26412;&#65306;&#19968;&#31181;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Detecting Racist Text in Bengali: An Ensemble Deep Learning Framework. (arXiv:2401.16748v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#23391;&#21152;&#25289;&#35821;&#20013;&#31181;&#26063;&#20027;&#20041;&#25991;&#26412;&#30340;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;87.94%&#30340;&#20934;&#30830;&#29575;&#12290;&#27169;&#22411;&#30340;&#26368;&#20339;&#34920;&#29616;&#26159;&#20351;&#29992;MCNN-LSTM&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;BERT&#23884;&#20837;&#21644;RNN/LSTM&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#32456;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31181;&#26063;&#20027;&#20041;&#26159;&#25105;&#20204;&#22269;&#23478;&#20197;&#21450;&#20840;&#19990;&#30028;&#30340;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#29616;&#35937;&#12290;&#27599;&#22825;&#25105;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#21644;&#34394;&#25311;&#29983;&#27963;&#20013;&#37117;&#20250;&#36935;&#21040;&#19968;&#20123;&#31181;&#26063;&#20027;&#20041;&#35328;&#35770;&#12290;&#34429;&#28982;&#25105;&#20204;&#21487;&#20197;&#22312;&#34394;&#25311;&#29983;&#27963;&#20013;&#28040;&#38500;&#36825;&#31181;&#31181;&#26063;&#20027;&#20041;&#65288;&#20363;&#22914;&#31038;&#20132;&#23186;&#20307;&#65289;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#26816;&#27979;&#36825;&#20123;&#31181;&#26063;&#20027;&#20041;&#35328;&#35770;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23391;&#21152;&#25289;&#35821;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26631;&#27880;&#24182;&#36827;&#34892;&#20102;&#25968;&#25454;&#26631;&#31614;&#39564;&#35777;&#12290;&#32463;&#36807;&#24191;&#27867;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#25104;&#21151;&#23454;&#29616;&#20102;87.94&#65285;&#30340;&#25991;&#26412;&#26816;&#27979;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;BERT&#23884;&#20837;&#24212;&#29992;&#20102;RNN&#21644;LSTM&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;MCNN-LSTM&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;&#26368;&#21518;&#65292;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#26469;&#32467;&#21512;&#25152;&#26377;&#27169;&#22411;&#32467;&#26524;&#65292;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Racism is an alarming phenomenon in our country as well as all over the world. Every day we have come across some racist comments in our daily life and virtual life. Though we can eradicate this racism from virtual life (such as Social Media). In this paper, we have tried to detect those racist comments with NLP and deep learning techniques. We have built a novel dataset in the Bengali Language. Further, we annotated the dataset and conducted data label validation. After extensive utilization of deep learning methodologies, we have successfully achieved text detection with an impressive accuracy rate of 87.94\% using the Ensemble approach. We have applied RNN and LSTM models using BERT Embeddings. However, the MCNN-LSTM model performed highest among all those models. Lastly, the Ensemble approach has been followed to combine all the model results to increase overall performance.
&lt;/p&gt;</description></item><item><title>MT-Eval&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#30340;&#32508;&#21512;&#24615;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;-LLM&#23545;&#35805;&#65292;&#23558;&#20132;&#20114;&#27169;&#24335;&#20998;&#20026;&#22235;&#31867;&#65292;&#24182;&#21019;&#24314;&#20102;&#22810;&#36718;&#26597;&#35810;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16745</link><description>&lt;p&gt;
MT-Eval&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#36718;&#33021;&#21147;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models. (arXiv:2401.16745v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16745
&lt;/p&gt;
&lt;p&gt;
MT-Eval&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#30340;&#32508;&#21512;&#24615;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;-LLM&#23545;&#35805;&#65292;&#23558;&#20132;&#20114;&#27169;&#24335;&#20998;&#20026;&#22235;&#31867;&#65292;&#24182;&#21019;&#24314;&#20102;&#22810;&#36718;&#26597;&#35810;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#19979;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#22797;&#26434;&#30340;&#22810;&#36718;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#36718;&#35780;&#20272;&#19978;&#65292;&#24573;&#35270;&#20102;&#27169;&#22411;&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MT-Eval&#65292;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;-LLM&#23545;&#35805;&#65292;&#25105;&#20204;&#23558;&#20132;&#20114;&#27169;&#24335;&#20998;&#20026;&#22235;&#31867;&#65306;&#22238;&#24518;&#12289;&#25193;&#23637;&#12289;&#20462;&#25913;&#21644;&#36319;&#36827;&#12290;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#29616;&#26377;&#25968;&#25454;&#38598;&#25110;&#20351;&#29992;GPT-4&#21019;&#24314;&#26032;&#30340;&#31034;&#20363;&#26500;&#24314;&#20102;&#27599;&#20010;&#31867;&#21035;&#30340;&#22810;&#36718;&#26597;&#35810;&#65292;&#20197;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#20026;&#20102;&#30740;&#31350;&#24433;&#21709;&#22810;&#36718;&#33021;&#21147;&#30340;&#22240;&#32032;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;1170&#20010;&#22810;&#36718;&#26597;&#35810;&#30340;&#21333;&#36718;&#29256;&#26412;&#65292;&#24182;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;11&#20010;&#30693;&#21517;LLM&#36827;&#34892;&#35780;&#20272;&#34920;&#26126;&#65292;&#23613;&#31649;&#38381;&#28304;&#27169;&#22411;&#36890;&#24120;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#20294;&#26576;&#20123;&#24320;&#28304;&#27169;&#22411;&#36229;&#36807;&#20102;GPT-3.5-Turbo&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly relied upon for complex multi-turn conversations across diverse real-world applications. However, existing benchmarks predominantly focus on single-turn evaluations, overlooking the models' capabilities in multi-turn interactions. To address this gap, we introduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn conversational abilities. By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up. We construct multi-turn queries for each category either by augmenting existing datasets or by creating new examples with GPT-4 to avoid data leakage. To study the factors impacting multi-turn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance. Our evaluation of 11 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in s
&lt;/p&gt;</description></item><item><title>Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#20174;&#32780;&#20248;&#21270;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16736</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Engineering A Large Language Model From Scratch. (arXiv:2401.16736v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16736
&lt;/p&gt;
&lt;p&gt;
Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#20174;&#32780;&#20248;&#21270;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#21019;&#26032;&#25216;&#26415;&#30340;&#24320;&#21457;&#21644;&#21457;&#24067;&#12290;Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#29420;&#29305;&#30340;&#37197;&#32622;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#20248;&#21270;&#24615;&#33021;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24314;&#31435;&#26377;&#24847;&#20041;&#30340;&#20851;&#32852;&#12290;&#30001;&#20110;&#20854;&#25299;&#25169;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#37197;&#32622;&#65292;&#23427;&#21487;&#20197;&#25552;&#21462;&#29305;&#24449;&#24182;&#23398;&#20064;&#22797;&#26434;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#27169;&#20223;&#20154;&#31867;&#35821;&#35328;&#12290;Atinuke&#26159;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26080;&#32541;&#38598;&#25104;&#12290;softmax&#12289;&#23884;&#20837;&#21644;&#22810;&#22836;&#27880;&#24847;&#21147;&#31561;&#39640;&#32423;&#30697;&#38453;&#25805;&#20316;&#20351;&#24471;&#23545;&#25991;&#26412;&#12289;&#22768;&#38899;&#21644;&#35270;&#35273;&#20449;&#21495;&#30340;&#32454;&#33268;&#22788;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#23558;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#36719;&#20214;&#35774;&#35745;&#21407;&#21017;&#21644;&#25968;&#23398;&#26041;&#27861;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#21487;&#20280;&#32553;&#30340;&#26694;&#26550;&#65292;&#23558;&#25991;&#26412;&#25551;&#36848;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#20803;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#35299;&#37322;&#27169;&#22411;&#20013;&#29702;&#35299;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21457;&#29616;&#20154;&#21487;&#35299;&#37322;&#30340;&#25551;&#36848;&#31526;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#35299;&#37322;&#31070;&#32463;&#20803;&#65292;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16731</link><description>&lt;p&gt;
&#20026;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#20803;&#29983;&#25104;&#20449;&#24687;&#24615;&#25991;&#26412;&#25551;&#36848;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Generating Informative Textual Description for Neurons in Language Models. (arXiv:2401.16731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#21487;&#20280;&#32553;&#30340;&#26694;&#26550;&#65292;&#23558;&#25991;&#26412;&#25551;&#36848;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#20803;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#35299;&#37322;&#27169;&#22411;&#20013;&#29702;&#35299;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21457;&#29616;&#20154;&#21487;&#35299;&#37322;&#30340;&#25551;&#36848;&#31526;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#35299;&#37322;&#31070;&#32463;&#20803;&#65292;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#21040;&#21508;&#31181;&#19990;&#30028;&#30693;&#35782;&#24182;&#36866;&#24212;&#20855;&#26377;&#26377;&#38480;&#36164;&#28304;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29702;&#35299;&#21738;&#20123;&#20449;&#24687;&#23578;&#19981;&#28165;&#26970;&#65292;&#32780;&#22312;&#35782;&#21035;&#23427;&#20204;&#26041;&#38754;&#30340;&#31070;&#32463;&#20803;&#32423;&#36129;&#29486;&#22522;&#26412;&#19978;&#26159;&#26410;&#30693;&#30340;&#12290;&#31070;&#32463;&#20803;&#35299;&#37322;&#30340;&#20256;&#32479;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#26377;&#38480;&#30340;&#39044;&#23450;&#20041;&#25551;&#36848;&#31526;&#65292;&#35201;&#20040;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#20197;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#35299;&#37322;&#20027;&#27169;&#22411;&#31070;&#32463;&#20803;&#30340;&#27425;&#35201;&#27169;&#22411;&#12290;&#26412;&#25991;&#20197;BERT&#20026;&#20363;&#65292;&#23581;&#35797;&#25670;&#33073;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#21487;&#20280;&#32553;&#30340;&#26694;&#26550;&#65292;&#23558;&#25991;&#26412;&#25551;&#36848;&#19982;&#31070;&#32463;&#20803;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#21033;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#22312;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#20154;&#21487;&#35299;&#37322;&#30340;&#25551;&#36848;&#31526;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#35299;&#37322;&#24102;&#26377;&#36825;&#20123;&#25551;&#36848;&#31526;&#30340;&#31070;&#32463;&#20803;&#12290;&#36890;&#36807;&#21508;&#31181;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in transformer-based language models have allowed them to capture a wide variety of world knowledge that can be adapted to downstream tasks with limited resources. However, what pieces of information are understood in these models is unclear, and neuron-level contributions in identifying them are largely unknown. Conventional approaches in neuron explainability either depend on a finite set of pre-defined descriptors or require manual annotations for training a secondary model that can then explain the neurons of the primary model. In this paper, we take BERT as an example and we try to remove these constraints and propose a novel and scalable framework that ties textual descriptions to neurons. We leverage the potential of generative language models to discover human-interpretable descriptors present in a dataset and use an unsupervised approach to explain neurons with these descriptors. Through various qualitative and quantitative analyses, we demonstrate the effe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#24635;&#32467;&#20102;&#26368;&#36817;&#22312;&#20167;&#24680;&#35328;&#35770;&#23457;&#26680;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#21548;&#35273;&#20803;&#32032;&#22312;&#20256;&#25773;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#22823;&#22411;&#27169;&#22411;&#23545;&#23457;&#26680;&#33021;&#21147;&#30340;&#37325;&#26032;&#23450;&#20041;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#20102;&#22312;&#23569;&#25968;&#35821;&#35328;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#30740;&#31350;&#24046;&#36317;&#21644;&#22788;&#29702;&#20302;&#36164;&#28304;&#29615;&#22659;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.16727</link><description>&lt;p&gt;
&#26368;&#36817;&#22312;&#20167;&#24680;&#35328;&#35770;&#23457;&#26680;&#26041;&#38754;&#30340;&#36827;&#23637;&#65306;&#22810;&#27169;&#24577;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large Models. (arXiv:2401.16727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16727
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#24635;&#32467;&#20102;&#26368;&#36817;&#22312;&#20167;&#24680;&#35328;&#35770;&#23457;&#26680;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#20102;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#21548;&#35273;&#20803;&#32032;&#22312;&#20256;&#25773;&#20167;&#24680;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#22823;&#22411;&#27169;&#22411;&#23545;&#23457;&#26680;&#33021;&#21147;&#30340;&#37325;&#26032;&#23450;&#20041;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#20102;&#22312;&#23569;&#25968;&#35821;&#35328;&#21644;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#30740;&#31350;&#24046;&#36317;&#21644;&#22788;&#29702;&#20302;&#36164;&#28304;&#29615;&#22659;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#20132;&#27969;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#23457;&#26680;&#20167;&#24680;&#35328;&#35770;&#65288;HS&#65289;&#38754;&#20020;&#30528;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#30001;&#25968;&#23383;&#20869;&#23481;&#30340;&#22810;&#27169;&#24577;&#29305;&#24615;&#25152;&#24102;&#26469;&#30340;&#12290;&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#28145;&#20837;&#30740;&#31350;&#20102;HS&#23457;&#26680;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#23835;&#36215;&#35282;&#33394;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#23545;&#24403;&#21069;&#25991;&#29486;&#30340;&#20840;&#38754;&#20998;&#26512;&#24320;&#22987;&#65292;&#25581;&#31034;&#20102;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#21548;&#35273;&#20803;&#32032;&#22312;&#20256;&#25773;HS&#20013;&#30340;&#24494;&#22937;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#36235;&#21183;&#65292;&#21363;&#23558;&#36825;&#20123;&#27169;&#24577;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;HS&#30340;&#20256;&#25773;&#20855;&#26377;&#22797;&#26434;&#24615;&#21644;&#24494;&#22937;&#24615;&#12290;&#23545;&#20110;&#30001;LLMs&#21644;LMMs&#24102;&#26469;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20102;&#20854;&#23545;&#26816;&#27979;&#21644;&#23457;&#26680;&#33021;&#21147;&#36793;&#30028;&#30340;&#37325;&#26032;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#29616;&#26377;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#25968;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#20197;&#21450;&#22312;&#22788;&#29702;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#38656;&#35201;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving landscape of online communication, moderating hate speech (HS) presents an intricate challenge, compounded by the multimodal nature of digital content. This comprehensive survey delves into the recent strides in HS moderation, spotlighting the burgeoning role of large language models (LLMs) and large multimodal models (LMMs). Our exploration begins with a thorough analysis of current literature, revealing the nuanced interplay between textual, visual, and auditory elements in propagating HS. We uncover a notable trend towards integrating these modalities, primarily due to the complexity and subtlety with which HS is disseminated. A significant emphasis is placed on the advances facilitated by LLMs and LMMs, which have begun to redefine the boundaries of detection and moderation capabilities. We identify existing gaps in research, particularly in the context of underrepresented languages and cultures, and the need for solutions to handle low-resource settings. The survey
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#19968;&#33268;&#24615;&#35780;&#20272;&#65292;&#24182;&#37319;&#29992;&#23618;&#32676;&#29702;&#35770;&#26041;&#27861;&#23558;&#20854;&#25193;&#23637;&#21040;&#27861;&#24459;&#12289;&#21496;&#27861;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#36229;&#25991;&#26412;&#20013;&#36827;&#34892;&#20840;&#23616;&#35780;&#20272;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#25919;&#24220;&#30340;&#19968;&#33268;&#24615;&#24182;&#24212;&#23545;&#35823;&#23548;&#20449;&#24687;&#21644;&#30456;&#20851;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16713</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#23618;&#32676;&#29702;&#35770;&#26816;&#27979;&#19981;&#19968;&#33268;&#24615;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Prospects for inconsistency detection using large language models and sheaves. (arXiv:2401.16713v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16713
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#19968;&#33268;&#24615;&#35780;&#20272;&#65292;&#24182;&#37319;&#29992;&#23618;&#32676;&#29702;&#35770;&#26041;&#27861;&#23558;&#20854;&#25193;&#23637;&#21040;&#27861;&#24459;&#12289;&#21496;&#27861;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#36229;&#25991;&#26412;&#20013;&#36827;&#34892;&#20840;&#23616;&#35780;&#20272;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#25919;&#24220;&#30340;&#19968;&#33268;&#24615;&#24182;&#24212;&#23545;&#35823;&#23548;&#20449;&#24687;&#21644;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23545;&#36923;&#36753;&#19968;&#33268;&#24615;&#36827;&#34892;&#21512;&#29702;&#30340;&#25968;&#20540;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#32676;&#29702;&#35770;&#30340;&#25968;&#23398;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#35780;&#20272;&#25552;&#21319;&#21040;&#27861;&#24459;&#12289;&#21496;&#27861;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#36229;&#25991;&#26412;&#20013;&#65292;&#24182;&#20840;&#23616;&#35780;&#20272;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#26395;&#22686;&#24378;&#25919;&#24220;&#30340;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#25171;&#20987;&#35823;&#23548;&#20449;&#24687;&#21644;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate that large language models can produce reasonable numerical ratings of the logical consistency of claims. We also outline a mathematical approach based on sheaf theory for lifting such ratings to hypertexts such as laws, jurisprudence, and social media and evaluating their consistency globally. This approach is a promising avenue to increasing consistency in and of government, as well as to combating mis- and disinformation and related ills.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#34394;&#26500;&#35805;&#35821;&#26816;&#27979;&#30340;&#20998;&#31867;&#23454;&#39564;&#65292;&#21033;&#29992;&#20102;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#21644;&#26032;&#30340;&#29305;&#24449;&#38598;&#12290;&#34394;&#26500;&#35805;&#35821;&#30340;&#26816;&#27979;&#26377;&#21161;&#20110;&#20016;&#23500;&#22823;&#22411;&#25991;&#21270;&#36951;&#20135;&#26723;&#26696;&#65292;&#24182;&#24110;&#21161;&#26356;&#24191;&#27867;&#22320;&#29702;&#35299;&#34394;&#26500;&#21465;&#20107;&#30340;&#29305;&#36136;&#12290;</title><link>http://arxiv.org/abs/2401.16678</link><description>&lt;p&gt;
&#34394;&#26500;&#35805;&#35821;&#30340;&#26816;&#27979;&#19982;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
The Detection and Understanding of Fictional Discourse. (arXiv:2401.16678v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#34394;&#26500;&#35805;&#35821;&#26816;&#27979;&#30340;&#20998;&#31867;&#23454;&#39564;&#65292;&#21033;&#29992;&#20102;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#21644;&#26032;&#30340;&#29305;&#24449;&#38598;&#12290;&#34394;&#26500;&#35805;&#35821;&#30340;&#26816;&#27979;&#26377;&#21161;&#20110;&#20016;&#23500;&#22823;&#22411;&#25991;&#21270;&#36951;&#20135;&#26723;&#26696;&#65292;&#24182;&#24110;&#21161;&#26356;&#24191;&#27867;&#22320;&#29702;&#35299;&#34394;&#26500;&#21465;&#20107;&#30340;&#29305;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19982;&#34394;&#26500;&#35805;&#35821;&#26816;&#27979;&#20219;&#21153;&#30456;&#20851;&#30340;&#20998;&#31867;&#23454;&#39564;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#24403;&#20195;&#19987;&#19994;&#20986;&#29256;&#30340;&#23567;&#35828;&#12289;&#26469;&#33258;Hathi Trust&#30340;&#21382;&#21490;&#23567;&#35828;&#12289;&#31881;&#19997;&#25152;&#20889;&#25991;&#12289;&#26469;&#33258;Reddit&#30340;&#25925;&#20107;&#12289;&#27665;&#38388;&#20256;&#35828;&#12289;GPT&#29983;&#25104;&#30340;&#25925;&#20107;&#20197;&#21450;&#33521;&#35821;&#19990;&#30028;&#25991;&#23398;&#20316;&#21697;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#8220;&#36229;&#35789;&#20041;&#8221;&#29305;&#24449;&#38598;&#65292;&#20197;&#20419;&#36827;&#35821;&#20041;&#27010;&#25324;&#30340;&#30446;&#26631;&#12290;&#34394;&#26500;&#35805;&#35821;&#30340;&#26816;&#27979;&#21487;&#20197;&#24110;&#21161;&#20016;&#23500;&#25105;&#20204;&#23545;&#22823;&#22411;&#25991;&#21270;&#36951;&#20135;&#26723;&#26696;&#30340;&#20102;&#35299;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#24191;&#27867;&#22320;&#29702;&#35299;&#34394;&#26500;&#21465;&#20107;&#30340;&#29420;&#29305;&#29305;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a variety of classification experiments related to the task of fictional discourse detection. We utilize a diverse array of datasets, including contemporary professionally published fiction, historical fiction from the Hathi Trust, fanfiction, stories from Reddit, folk tales, GPT-generated stories, and anglophone world literature. Additionally, we introduce a new feature set of word "supersenses" that facilitate the goal of semantic generalization. The detection of fictional discourse can help enrich our knowledge of large cultural heritage archives and assist with the process of understanding the distinctive qualities of fictional storytelling more broadly.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#20197;&#21450;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.16659</link><description>&lt;p&gt;
&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
History-Aware Conversational Dense Retrieval. (arXiv:2401.16659v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16659
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#31995;&#32479;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#20197;&#21450;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25628;&#32034;&#36890;&#36807;&#23454;&#29616;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#22810;&#36718;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#20449;&#24687;&#26816;&#32034;&#30340;&#20415;&#21033;&#12290;&#25903;&#25345;&#36825;&#31181;&#20132;&#20114;&#38656;&#35201;&#23545;&#23545;&#35805;&#36755;&#20837;&#26377;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#20197;&#20415;&#26681;&#25454;&#21382;&#21490;&#20449;&#24687;&#21046;&#23450;&#33391;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#29305;&#21035;&#26159;&#65292;&#25628;&#32034;&#26597;&#35810;&#24212;&#21253;&#25324;&#26469;&#33258;&#20808;&#21069;&#23545;&#35805;&#22238;&#21512;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#32463;&#36807;&#31934;&#35843;&#30340;&#39044;&#35757;&#32451;&#19987;&#38376;&#26816;&#32034;&#22120;&#36827;&#34892;&#25972;&#20010;&#23545;&#35805;&#24335;&#25628;&#32034;&#20250;&#35805;&#30340;&#20248;&#21270;&#65292;&#36825;&#21487;&#33021;&#20250;&#21464;&#24471;&#20887;&#38271;&#21644;&#22024;&#26434;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#25163;&#21160;&#30417;&#30563;&#20449;&#21495;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#24863;&#30693;&#30340;&#23545;&#35805;&#24335;&#31264;&#23494;&#26816;&#32034;(HAConvDR)&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#20010;&#24605;&#24819;&#65306;&#19978;&#19979;&#25991;&#21435;&#22122;&#30340;&#26597;&#35810;&#37325;&#26500;&#21644;&#26681;&#25454;&#21382;&#21490;&#36718;&#27425;&#30340;&#23454;&#38469;&#24433;&#21709;&#36827;&#34892;&#33258;&#21160;&#25366;&#25496;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search facilitates complex information retrieval by enabling multi-turn interactions between users and the system. Supporting such interactions requires a comprehensive understanding of the conversational inputs to formulate a good search query based on historical information. In particular, the search query should include the relevant information from the previous conversation turns. However, current approaches for conversational dense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever using the whole conversational search session, which can be lengthy and noisy. Moreover, existing approaches are limited by the amount of manual supervision signals in the existing datasets. To address the aforementioned issues, we propose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which incorporates two ideas: context-denoised query reformulation and automatic mining of supervision signals based on the actual impact of historical turns. Experime
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;OWSM v3.1&#22522;&#20110;E-Branchformer&#30340;&#26356;&#22909;&#21644;&#26356;&#24555;&#30340;&#24320;&#25918;&#24335;Whisper&#39118;&#26684;&#35821;&#38899;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#29256;&#26412;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#35813;&#35770;&#25991;&#36824;&#20844;&#24320;&#21457;&#24067;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.16658</link><description>&lt;p&gt;
OWSM v3.1: &#22522;&#20110;E-Branchformer&#30340;&#26356;&#22909;&#21644;&#26356;&#24555;&#30340;&#24320;&#25918;&#24335;Whisper&#39118;&#26684;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer. (arXiv:2401.16658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16658
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;OWSM v3.1&#22522;&#20110;E-Branchformer&#30340;&#26356;&#22909;&#21644;&#26356;&#24555;&#30340;&#24320;&#25918;&#24335;Whisper&#39118;&#26684;&#35821;&#38899;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#29256;&#26412;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#35813;&#35770;&#25991;&#36824;&#20844;&#24320;&#21457;&#24067;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#20513;&#23548;&#37319;&#29992;&#23436;&#20840;&#24320;&#25918;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25512;&#21160;&#36879;&#26126;&#24230;&#21644;&#24320;&#25918;&#31185;&#23398;&#12290;&#20316;&#20026;&#19968;&#20010;&#21021;&#27493;&#30340;&#27493;&#39588;&#65292;&#24320;&#25918;&#24335;Whisper&#39118;&#26684;&#35821;&#38899;&#27169;&#22411;(OWSM)&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#21644;&#24320;&#28304;&#24037;&#20855;&#37325;&#26032;&#22797;&#21046;&#20102;OpenAI&#30340;Whisper&#12290;&#20026;&#20102;&#22797;&#21046;Whisper&#65292;&#20043;&#21069;&#30340;OWSM v1&#21040;v3&#27169;&#22411;&#20173;&#28982;&#22522;&#20110;Transformer&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19981;&#22914;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#39640;OWSM&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;E-Branchformer&#30340;OWSM v3.1&#27169;&#22411;&#65292;&#26377;&#20004;&#20010;&#35268;&#27169;&#65292;&#21363;100M&#21644;1B&#12290;1B&#27169;&#22411;&#26159;&#30446;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#26368;&#22823;&#30340;&#22522;&#20110;E-Branchformer&#30340;&#35821;&#38899;&#27169;&#22411;&#12290;&#23427;&#22312;&#22823;&#37096;&#20998;&#35780;&#20272;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#27604;&#20043;&#21069;&#30340;OWSM v3&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#28436;&#31034;&#20102;&#39640;&#36798;25%&#30340;&#26356;&#24555;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#25968;&#25454;&#20934;&#22791;&#33050;&#26412;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35757;&#32451;&#26085;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have advocated for fully open foundation models to promote transparency and open science. As an initial step, the Open Whisper-style Speech Model (OWSM) reproduced OpenAI's Whisper using publicly available data and open-source toolkits. With the aim of reproducing Whisper, the previous OWSM v1 through v3 models were still based on Transformer, which might lead to inferior performance compared to other state-of-the-art speech encoders. In this work, we aim to improve the performance and efficiency of OWSM without extra training data. We present E-Branchformer based OWSM v3.1 models at two scales, i.e., 100M and 1B. The 1B model is the largest E-Branchformer based speech model that has been made publicly available. It outperforms the previous OWSM v3 in a vast majority of evaluation benchmarks, while demonstrating up to 25% faster inference speed. We publicly release the data preparation scripts, pre-trained models and training logs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24674;&#22797;&#24515;&#26234;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21457;&#29616;&#20351;&#29992;&#22522;&#20110;MCMC&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.16657</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#24674;&#22797;&#24515;&#26234;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Recovering Mental Representations from Large Language Models with Markov Chain Monte Carlo. (arXiv:2401.16657v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24674;&#22797;&#24515;&#26234;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21457;&#29616;&#20351;&#29992;&#22522;&#20110;MCMC&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#37319;&#26679;&#31639;&#27861;&#36827;&#34892;&#20154;&#31867;&#30740;&#31350;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#29702;&#35299;&#20854;&#24515;&#26234;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#20154;&#31867;&#25110;LLMs&#37117;&#21487;&#20197;&#36890;&#36807;&#20869;&#30465;&#30340;&#26041;&#24335;&#30452;&#25509;&#25581;&#31034;&#20854;&#24515;&#26234;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#19968;&#31181;&#37319;&#26679;&#31639;&#27861;&#30340;&#20803;&#32032;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#20351;&#29992;&#30452;&#25509;&#37319;&#26679;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#36827;&#34892;LLMs&#25554;&#38382;&#26102;&#24674;&#22797;&#20154;&#31867;&#21270;&#34920;&#31034;&#31243;&#24230;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22522;&#20110;MCMC&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#28508;&#21147;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#20351;&#29992;LLMs&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating sampling algorithms with people has proven a useful method for efficiently probing and understanding their mental representations. We propose that the same methods can be used to study the representations of Large Language Models (LLMs). While one can always directly prompt either humans or LLMs to disclose their mental representations introspectively, we show that increased efficiency can be achieved by using LLMs as elements of a sampling algorithm. We explore the extent to which we recover human-like representations when LLMs are interrogated with Direct Sampling and Markov chain Monte Carlo (MCMC). We found a significant increase in efficiency and performance using adaptive sampling algorithms based on MCMC. We also highlight the potential of our method to yield a more general method of conducting Bayesian inference \textit{with} LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#32418;&#38431;&#27979;&#35797;&#65288;GBRT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#26679;&#21270;&#25552;&#31034;&#26469;&#35302;&#21457;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22238;&#24212;&#12290;&#36890;&#36807;&#20351;&#29992;&#23433;&#20840;&#20998;&#31867;&#22120;&#35780;&#20998;&#21644;&#21453;&#21521;&#20256;&#25773;&#26469;&#26356;&#26032;&#25552;&#31034;&#65292;GBRT&#22312;&#21457;&#29616;&#35302;&#21457;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22238;&#24212;&#30340;&#25552;&#31034;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.16656</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Gradient-Based Language Model Red Teaming. (arXiv:2401.16656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#32418;&#38431;&#27979;&#35797;&#65288;GBRT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#26679;&#21270;&#25552;&#31034;&#26469;&#35302;&#21457;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22238;&#24212;&#12290;&#36890;&#36807;&#20351;&#29992;&#23433;&#20840;&#20998;&#31867;&#22120;&#35780;&#20998;&#21644;&#21453;&#21521;&#20256;&#25773;&#26469;&#26356;&#26032;&#25552;&#31034;&#65292;GBRT&#22312;&#21457;&#29616;&#35302;&#21457;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22238;&#24212;&#30340;&#25552;&#31034;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#38431;&#27979;&#35797;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24369;&#28857;&#65292;&#36890;&#36807;&#20135;&#29983;&#23545;&#25239;&#24615;&#25552;&#31034;&#26469;&#35302;&#21457;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#22238;&#24212;&#12290;&#32418;&#38431;&#27979;&#35797;&#23545;&#20110;&#27169;&#22411;&#23545;&#40784;&#21644;&#35780;&#20272;&#37117;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#20154;&#24037;&#25805;&#20316;&#65292;&#21171;&#21160;&#24378;&#24230;&#22823;&#19988;&#38590;&#20197;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#32418;&#38431;&#27979;&#35797;&#65288;GBRT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#65292;&#33021;&#22815;&#24341;&#36215;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#19981;&#23433;&#20840;&#30340;&#22238;&#24212;&#12290;GBRT&#26159;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23433;&#20840;&#20998;&#31867;&#22120;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#20923;&#32467;&#30340;&#23433;&#20840;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#26469;&#26356;&#26032;&#25552;&#31034;&#12290;&#20026;&#20102;&#25552;&#39640;&#36755;&#20837;&#25552;&#31034;&#30340;&#36830;&#36143;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21464;&#31181;&#65292;&#36890;&#36807;&#28155;&#21152;&#30495;&#23454;&#24615;&#25439;&#22833;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#29983;&#25104;&#25552;&#31034;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#23398;&#20064;&#25552;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GBRT&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#33021;&#22815;&#35302;&#21457;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22238;&#24212;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Red teaming is a common strategy for identifying weaknesses in generative language models (LMs), where adversarial prompts are produced that trigger an LM to generate unsafe responses. Red teaming is instrumental for both model alignment and evaluation, but is labor-intensive and difficult to scale when done by humans. In this paper, we present Gradient-Based Red Teaming (GBRT), a red teaming method for automatically generating diverse prompts that are likely to cause an LM to output unsafe responses. GBRT is a form of prompt learning, trained by scoring an LM response with a safety classifier and then backpropagating through the frozen safety classifier and LM to update the prompt. To improve the coherence of input prompts, we introduce two variants that add a realism loss and fine-tune a pretrained model to generate the prompts instead of learning the prompts directly. Our experiments show that GBRT is more effective at finding prompts that trigger an LM to generate unsafe responses 
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#27010;&#29575;&#21028;&#26029;&#32463;&#24120;&#26159;&#19981;&#36830;&#36143;&#30340;&#65292;&#26174;&#31034;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#19968;&#26679;&#30340;&#38750;&#29702;&#24615;&#20559;&#24046;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#23558;&#33258;&#22238;&#24402;LLMs&#19982;&#38544;&#24615;&#36125;&#21494;&#26031;&#25512;&#26029;&#32852;&#31995;&#36215;&#26469;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.16646</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#36830;&#36143;&#27010;&#29575;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Incoherent Probability Judgments in Large Language Models. (arXiv:2401.16646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16646
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#27010;&#29575;&#21028;&#26029;&#32463;&#24120;&#26159;&#19981;&#36830;&#36143;&#30340;&#65292;&#26174;&#31034;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#19968;&#26679;&#30340;&#38750;&#29702;&#24615;&#20559;&#24046;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#23558;&#33258;&#22238;&#24402;LLMs&#19982;&#38544;&#24615;&#36125;&#21494;&#26031;&#25512;&#26029;&#32852;&#31995;&#36215;&#26469;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#36830;&#36143;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#20294;&#23427;&#20204;&#26159;&#21542;&#21516;&#26679;&#25797;&#38271;&#24418;&#25104;&#36830;&#36143;&#30340;&#27010;&#29575;&#21028;&#26029;&#65311;&#25105;&#20204;&#20351;&#29992;&#27010;&#29575;&#36523;&#20221;&#21644;&#37325;&#22797;&#21028;&#26029;&#26469;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#27010;&#29575;&#21028;&#26029;&#30340;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#21028;&#26029;&#32463;&#24120;&#26159;&#19981;&#36830;&#36143;&#30340;&#65292;&#26174;&#31034;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#27010;&#29575;&#29702;&#35770;&#35268;&#21017;&#20559;&#31163;&#12290;&#27492;&#22806;&#65292;&#24403;&#35201;&#27714;&#23545;&#21516;&#19968;&#20107;&#20214;&#36827;&#34892;&#21028;&#26029;&#26102;&#65292;LLMs&#20135;&#29983;&#30340;&#27010;&#29575;&#21028;&#26029;&#30340;&#22343;&#20540;-&#26041;&#24046;&#20851;&#31995;&#21576;&#29616;&#20986;&#20154;&#31867;&#25152;&#35265;&#21040;&#30340;&#20498;U&#24418;&#29366;&#12290;&#25105;&#20204;&#25552;&#20986;&#36825;&#20123;&#38750;&#29702;&#24615;&#30340;&#20559;&#31163;&#21487;&#20197;&#36890;&#36807;&#23558;&#33258;&#22238;&#24402;LLMs&#19982;&#38544;&#24615;&#36125;&#21494;&#26031;&#25512;&#26029;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#19982;&#20154;&#31867;&#27010;&#29575;&#21028;&#26029;&#30340;&#36125;&#21494;&#26031;&#25277;&#26679;&#22120;&#27169;&#22411;&#36827;&#34892;&#31867;&#27604;&#26469;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Large Language Models (LLMs) trained for next-word prediction have demonstrated remarkable proficiency at producing coherent text. But are they equally adept at forming coherent probability judgments? We use probabilistic identities and repeated judgments to assess the coherence of probability judgments made by LLMs. Our results show that the judgments produced by these models are often incoherent, displaying human-like systematic deviations from the rules of probability theory. Moreover, when prompted to judge the same event, the mean-variance relationship of probability judgments produced by LLMs shows an inverted-U-shaped like that seen in humans. We propose that these deviations from rationality can be explained by linking autoregressive LLMs to implicit Bayesian inference and drawing parallels with the Bayesian Sampler model of human probability judgments.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20026;&#20363;&#65292;&#21457;&#24067;&#22312;GitHub&#21644;Hugging Face&#19978;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.16640</link><description>&lt;p&gt;
TeenyTinyLlama&#65306;&#22522;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35757;&#32451;&#30340;&#24320;&#28304;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese. (arXiv:2401.16640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16640
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20026;&#20363;&#65292;&#21457;&#24067;&#22312;GitHub&#21644;Hugging Face&#19978;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#30340;&#36827;&#23637;&#36824;&#19981;&#24179;&#34913;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;LLMs&#26159;&#22312;&#20687;&#33521;&#35821;&#36825;&#26679;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#65292;&#20294;&#22810;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#31245;&#24046;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#22810;&#35821;&#35328;&#22522;&#30784;&#26377;&#26102;&#20250;&#38480;&#21046;&#23427;&#20204;&#20135;&#29983;&#30340;&#21103;&#20135;&#21697;&#65292;&#22914;&#35745;&#31639;&#38656;&#27714;&#21644;&#35768;&#21487;&#21046;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#20026;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20351;&#29992;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#36807;&#31243;&#12289;&#20854;&#23616;&#38480;&#24615;&#21644;&#20248;&#21183;&#12290;&#36825;&#23601;&#26159;TeenyTinyLlama&#65306;&#20004;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#29983;&#25104;&#30340;&#32039;&#20945;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;GitHub&#21644;Hugging Face&#19978;&#20197;&#23485;&#26494;&#30340;Apache 2.0&#35768;&#21487;&#35777;&#21457;&#24067;&#23427;&#20204;&#65292;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;&#35814;&#35265;https://github.com/Nkluge-correa/TeenyTinyLlama
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development. See https://github.com/Nkluge-correa/TeenyTinyLlama
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#24402;&#22240;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16638</link><description>&lt;p&gt;
&#25171;&#30772;Transformer&#27169;&#22411;&#30340;&#26463;&#32538;&#65306;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#24402;&#22240;&#25552;&#20379;&#20102;&#22312;&#19981;&#24494;&#35843;&#39044;&#35757;&#32451;LLMs&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#30340;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs. (arXiv:2401.16638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#24402;&#22240;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24494;&#35843;&#26159;&#19968;&#31181;&#24120;&#29992;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#24402;&#22240;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#24615;&#33021;&#30340;&#20445;&#25345;&#65292;&#24182;&#25552;&#21319;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#20309;Transformer&#27169;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#27010;&#24565;&#36816;&#31639;&#31526;&#65292;&#20250;&#24471;&#21040;&#19968;&#20010;&#25237;&#24433;&#21040;&#28508;&#22312;&#27010;&#24565;&#31354;&#38388;&#19978;&#30340;&#32467;&#26524;&#65292;&#26412;&#25991;&#20013;&#31216;&#20043;&#20026;&#19978;&#19979;&#25991;&#24402;&#22240;&#12290;&#29305;&#23450;&#30340;&#27010;&#24565;&#36816;&#31639;&#31526;&#22312;&#30417;&#30563;&#23398;&#20064;&#38454;&#27573;&#36890;&#36807;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#34920;&#26126;&#65292;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#24402;&#22240;&#21487;&#20197;&#25913;&#21892;&#37492;&#21035;&#22120;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large pre-trained language models (LLMs) on particular datasets is a commonly employed strategy in Natural Language Processing (NLP) classification tasks. However, this approach usually results in a loss of models generalizability. In this paper, we present a framework that allows for maintaining generalizability, and enhances the performance on the downstream task by utilizing task-specific context attribution. We show that a linear transformation of the text representation from any transformer model using the task-specific concept operator results in a projection onto the latent concept space, referred to as context attribution in this paper. The specific concept operator is optimized during the supervised learning stage via novel loss functions. The proposed framework demonstrates that context attribution of the text representation for each task objective can improve the capacity of the discriminator function and thus achieve better performance for the classification tas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16635</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble. (arXiv:2401.16635v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;RLHF&#20381;&#36182;&#20110;&#36890;&#36807;&#26377;&#38480;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;RLHF&#21487;&#33021;&#20135;&#29983;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22870;&#21169;&#27169;&#22411;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#32771;&#34385;&#21040;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#21644;&#36164;&#28304;&#26114;&#36149;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21253;&#25324;&#32447;&#24615;&#23618;&#38598;&#25104;&#21644;&#22522;&#20110;LoRA&#30340;&#38598;&#25104;&#22312;&#20869;&#30340;&#39640;&#25928;&#38598;&#25104;&#26041;&#27861;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#38598;&#25104;&#22870;&#21169;&#27169;&#22411;&#36816;&#34892;Best-of-$n$&#21644;Proximal Policy Optimization&#65292;&#24182;&#39564;&#35777;&#25105;&#20204;&#30340;&#38598;&#25104;&#26041;&#27861;&#26377;&#21161;&#20110;&#25913;&#21892;RLHF&#36755;&#20986;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is a widely adopted approach for aligning large language models with human values. However, RLHF relies on a reward model that is trained with a limited amount of human preference data, which could lead to inaccurate predictions. As a result, RLHF may produce outputs that are misaligned with human values. To mitigate this issue, we contribute a reward ensemble method that allows the reward model to make more accurate predictions. As using an ensemble of large language model-based reward models can be computationally and resource-expensive, we explore efficient ensemble methods including linear-layer ensemble and LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy Optimization with our ensembled reward models, and verify that our ensemble methods help improve the alignment performance of RLHF outputs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToPro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#36755;&#20837;&#21477;&#23376;&#20998;&#35299;&#20026;&#21333;&#20010;&#35789;&#27719;&#24182;&#24212;&#29992;&#25552;&#31034;&#27169;&#26495;&#65292;&#22312;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32467;&#26500;&#19981;&#21516;&#30340;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16589</link><description>&lt;p&gt;
ToPro: &#36328;&#35821;&#35328;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#30340;&#22522;&#20110;Token&#32423;&#21035;&#30340;&#25552;&#31034;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
ToPro: Token-Level Prompt Decomposition for Cross-Lingual Sequence Labeling Tasks. (arXiv:2401.16589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16589
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToPro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#36755;&#20837;&#21477;&#23376;&#20998;&#35299;&#20026;&#21333;&#20010;&#35789;&#27719;&#24182;&#24212;&#29992;&#25552;&#31034;&#27169;&#26495;&#65292;&#22312;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32467;&#26500;&#19981;&#21516;&#30340;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21477;&#23376;&#32423;&#21035;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#21482;&#26377;&#23569;&#25968;&#32771;&#34385;&#21040;&#20102;&#35789;&#27719;&#32423;&#21035;&#30340;&#26631;&#27880;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#35789;&#24615;&#26631;&#27880;&#65288;POS&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Token&#32423;&#21035;&#30340;&#25552;&#31034;&#20998;&#35299;&#65288;ToPro&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#35789;&#27719;&#32423;&#21035;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12290;ToPro&#26041;&#27861;&#23558;&#36755;&#20837;&#21477;&#23376;&#20998;&#35299;&#20026;&#21333;&#20010;&#35789;&#27719;&#65292;&#24182;&#23545;&#27599;&#20010;&#35789;&#27719;&#24212;&#29992;&#19968;&#20010;&#25552;&#31034;&#27169;&#26495;&#12290;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;NER&#21644;POS&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;ToPro&#30340;&#24494;&#35843;&#22312;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#20248;&#20110;Vanilla&#24494;&#35843;&#21644;Prompt-Tuning&#65292;&#23588;&#20854;&#23545;&#20110;&#32467;&#26500;&#19982;&#28304;&#35821;&#35328;&#33521;&#35821;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;mT5&#27169;&#22411;&#26102;&#20063;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based methods have been successfully applied to multilingual pretrained language models for zero-shot cross-lingual understanding. However, most previous studies primarily focused on sentence-level classification tasks, and only a few considered token-level labeling tasks such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging. In this paper, we propose Token-Level Prompt Decomposition (ToPro), which facilitates the prompt-based method for token-level sequence labeling tasks. The ToPro method decomposes an input sentence into single tokens and applies one prompt template to each token. Our experiments on multilingual NER and POS tagging datasets demonstrate that ToPro-based fine-tuning outperforms Vanilla fine-tuning and Prompt-Tuning in zero-shot cross-lingual transfer, especially for languages that are typologically different from the source language English. Our method also attains state-of-the-art performance when employed with the mT5 model. Besides, our exp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#21457;&#29616;ChatGPT&#22312;&#31038;&#20132;&#12289;&#20998;&#26512;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#31867;&#23545;&#35805;&#26356;&#20855;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#23613;&#31649;&#22312;&#24773;&#32490;&#26041;&#38754;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.16587</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;ChatGPT&#29983;&#25104;&#23545;&#35805;&#20043;&#38388;&#30340;&#35821;&#35328;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
A Linguistic Comparison between Human and ChatGPT-Generated Conversations. (arXiv:2401.16587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#21457;&#29616;ChatGPT&#22312;&#31038;&#20132;&#12289;&#20998;&#26512;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#31867;&#23545;&#35805;&#26356;&#20855;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#23613;&#31649;&#22312;&#24773;&#32490;&#26041;&#38754;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#21644;LLM&#29983;&#25104;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#20351;&#29992;&#20102;&#30001;ChatGPT-3.5&#29983;&#25104;&#30340;19.5K&#20010;&#23545;&#35805;&#20316;&#20026;EmpathicDialogues&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#12290;&#30740;&#31350;&#37319;&#29992;Linguistic Inquiry and Word Count (LIWC) &#20998;&#26512;&#65292;&#27604;&#36739;&#20102;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#21644;&#20154;&#31867;&#23545;&#35805;&#22312;118&#20010;&#35821;&#35328;&#31867;&#21035;&#19978;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#26174;&#31034;&#20154;&#31867;&#23545;&#35805;&#20855;&#26377;&#26356;&#22823;&#30340;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#20294;ChatGPT&#22312;&#31038;&#20132;&#36807;&#31243;&#12289;&#20998;&#26512;&#39118;&#26684;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#33394;&#24425;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;LLMs&#8220;&#27604;&#30495;&#20154;&#26356;&#20687;&#30495;&#20154;&#8221;&#30340;&#26368;&#26032;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;ChatGPT&#21644;&#20154;&#31867;&#23545;&#35805;&#20043;&#38388;&#27809;&#26377;&#25214;&#21040;&#31215;&#26497;&#25110;&#28040;&#26497;&#24773;&#32490;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#23545;&#35805;&#23884;&#20837;&#30340;&#20998;&#31867;&#22120;&#20998;&#26512;&#34920;&#26126;&#65292;&#23613;&#31649;&#23545;&#35805;&#20013;&#27809;&#26377;&#26126;&#30830;&#25552;&#21450;&#24773;&#32490;&#65292;&#20294;&#23545;&#24773;&#24863;&#20215;&#20540;&#30340;&#38544;&#24615;&#32534;&#30721;&#23384;&#22312;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;&#20004;&#20010;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores linguistic differences between human and LLM-generated dialogues, using 19.5K dialogues generated by ChatGPT-3.5 as a companion to the EmpathicDialogues dataset. The research employs Linguistic Inquiry and Word Count (LIWC) analysis, comparing ChatGPT-generated conversations with human conversations across 118 linguistic categories. Results show greater variability and authenticity in human dialogues, but ChatGPT excels in categories such as social processes, analytical style, cognition, attentional focus, and positive emotional tone, reinforcing recent findings of LLMs being "more human than human." However, no significant difference was found in positive or negative affect between ChatGPT and human dialogues. Classifier analysis of dialogue embeddings indicates implicit coding of the valence of affect despite no explicit mention of affect in the conversations. The research also contributes a novel, companion ChatGPT-generated dataset of conversations between two i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32763;&#35793;&#36164;&#28304;&#20174;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#21033;&#29992;&#21040;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#30340;&#26041;&#27861;&#65292;&#20197;&#28385;&#36275;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.16582</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25991;&#26412;&#32763;&#35793;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Massively Multilingual Text Translation For Low-Resource Languages. (arXiv:2401.16582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32763;&#35793;&#36164;&#28304;&#20174;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#21033;&#29992;&#21040;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#30340;&#26041;&#27861;&#65292;&#20197;&#28385;&#36275;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#32763;&#35793;&#24212;&#29992;&#20110;&#36164;&#28304;&#26497;&#20026;&#26377;&#38480;&#30340;&#35821;&#35328;&#65292;&#26082;&#26377;&#25991;&#21270;&#30446;&#30340;&#65292;&#20063;&#26377;&#20154;&#36947;&#20027;&#20041;&#30446;&#30340;&#65292;&#26088;&#22312;&#25327;&#25937;&#21644;&#22797;&#20852;&#36825;&#20123;&#35821;&#35328;&#65292;&#24182;&#28385;&#36275;&#30001;COVID-19&#22823;&#27969;&#34892;&#21152;&#36895;&#30340;&#24403;&#22320;&#31038;&#21306;&#30340;&#26085;&#24120;&#38656;&#27714;&#12290;&#22312;&#35768;&#22810;&#20154;&#36947;&#20027;&#20041;&#24037;&#20316;&#20013;&#65292;&#23545;&#36164;&#28304;&#26497;&#20854;&#26377;&#38480;&#30340;&#35821;&#35328;&#36827;&#34892;&#32763;&#35793;&#36890;&#24120;&#19981;&#38656;&#35201;&#19968;&#20010;&#36890;&#29992;&#30340;&#32763;&#35793;&#24341;&#25806;&#65292;&#32780;&#26159;&#38656;&#35201;&#19968;&#20010;&#19987;&#38376;&#30340;&#38024;&#23545;&#29305;&#23450;&#25991;&#26412;&#30340;&#32763;&#35793;&#24341;&#25806;&#12290;&#20363;&#22914;&#65292;&#21307;&#30103;&#35760;&#24405;&#12289;&#21355;&#29983;&#31243;&#24207;&#12289;&#25919;&#24220;&#36890;&#20449;&#12289;&#32039;&#24613;&#31243;&#24207;&#21644;&#23447;&#25945;&#25991;&#26412;&#37117;&#23646;&#20110;&#26377;&#38480;&#30340;&#25991;&#26412;&#12290;&#23613;&#31649;&#36824;&#19981;&#23384;&#22312;&#36866;&#29992;&#20110;&#25152;&#26377;&#35821;&#35328;&#30340;&#36890;&#29992;&#32763;&#35793;&#24341;&#25806;&#65292;&#20294;&#26159;&#21487;&#33021;&#21487;&#20197;&#23558;&#22810;&#35821;&#35328;&#24050;&#30693;&#30340;&#26377;&#38480;&#25991;&#26412;&#32763;&#35793;&#25104;&#26032;&#30340;&#12289;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#65292;&#20174;&#32780;&#20943;&#23569;&#20154;&#24037;&#32763;&#35793;&#30340;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#35797;&#22270;&#21033;&#29992;&#26469;&#33258;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#30340;&#32763;&#35793;&#36164;&#28304;&#65292;&#39640;&#25928;&#22320;&#20026;&#22810;&#35821;&#35328;&#24050;&#30693;&#30340;&#24120;&#35268;&#25991;&#26412;&#25552;&#20379;&#26368;&#20339;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#26032;&#30340;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#29615;&#22659;&#20013;&#23454;&#29616;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation into severely low-resource languages has both the cultural goal of saving and reviving those languages and the humanitarian goal of assisting the everyday needs of local communities that are accelerated by the recent COVID-19 pandemic. In many humanitarian efforts, translation into severely low-resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, low-resource languages may be possible and reduce human translation effort. We attempt to leverage translation resources from rich-resource languages to efficiently produce best possible translation quality for well known texts, which are available in multiple languages, in a new, low-resource lan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#21319;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.16578</link><description>&lt;p&gt;
&#21457;&#25381;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#38271;&#65292;&#25552;&#21319;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;LLM&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports. (arXiv:2401.16578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#21319;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25918;&#23556;&#23398;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#36827;&#20102;&#25253;&#21578;&#29983;&#25104;&#65292;&#20294;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#21644;&#20020;&#24202;&#25928;&#33021;&#65288;CE&#65289;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#20020;&#24202;&#32972;&#26223;&#30340;&#35821;&#20041;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#36807;&#20998;&#24378;&#35843;&#20020;&#24202;&#32454;&#33410;&#65292;&#38477;&#20302;&#20102;&#25253;&#21578;&#30340;&#28165;&#26224;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4 1&#65292;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#19978;&#19979;&#25991;&#25351;&#23548;&#23398;&#20064;&#65288;ICIL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLM&#30340;&#35780;&#20272;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#26631;&#20934;&#20445;&#25345;&#19968;&#33268;&#65292;&#23454;&#29616;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25253;&#21578;&#19982;&#20154;&#31867;&#29983;&#25104;&#25253;&#21578;&#20043;&#38388;&#30340;&#35814;&#32454;&#27604;&#36739;&#12290;&#36825;&#36827;&#19968;&#27493;&#36890;&#36807;&#22238;&#24402;&#27169;&#22411;&#26469;&#32508;&#21512;&#21477;&#23376;&#35780;&#20272;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#8220;&#35814;&#32454;GPT-4&#65288;5&#27425;&#35757;&#32451;&#65289;&#8221;&#27169;&#22411;&#33719;&#24471;&#20102;0.48&#30340;&#20998;&#25968;&#65292;&#20248;&#20110;METEOR&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our ''Detailed GPT-4 (5-shot)'' model achieves a 0.48 score, outperforming the METEOR metric 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#12289;&#20998;&#24067;&#24335;&#30340;LLM&#26550;&#26500;&#27010;&#24565;&#65292;&#36890;&#36807;&#22312;&#36890;&#29992;&#35745;&#31639;&#26426;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#25552;&#20379;&#25353;&#38656;&#35775;&#38382;&#30340;&#21487;&#23450;&#21046;&#26381;&#21153;&#65292;&#35299;&#20915;&#20102;LLMs&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#35775;&#38382;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#36164;&#28304;&#21644;&#24212;&#29992;&#38656;&#27714;&#30340;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.16577</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#25353;&#38656;&#21487;&#23450;&#21046;&#30340;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
LLMs as On-demand Customizable Service. (arXiv:2401.16577v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16577
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#12289;&#20998;&#24067;&#24335;&#30340;LLM&#26550;&#26500;&#27010;&#24565;&#65292;&#36890;&#36807;&#22312;&#36890;&#29992;&#35745;&#31639;&#26426;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#25552;&#20379;&#25353;&#38656;&#35775;&#38382;&#30340;&#21487;&#23450;&#21046;&#26381;&#21153;&#65292;&#35299;&#20915;&#20102;LLMs&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#35775;&#38382;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#36164;&#28304;&#21644;&#24212;&#29992;&#38656;&#27714;&#30340;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#35775;&#38382;&#36825;&#20123;&#27169;&#22411;&#37117;&#23384;&#22312;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#36164;&#28304;&#23494;&#38598;&#38656;&#27714;&#12289;&#38271;&#26102;&#38388;&#35757;&#32451;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#12289;&#20998;&#24067;&#24335;&#30340;LLM&#26550;&#26500;&#27010;&#24565;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#22312;&#24322;&#26500;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#37096;&#32626;&#24615;&#65292;&#21253;&#25324;&#36890;&#29992;&#35745;&#31639;&#26426;&#65288;&#22914;&#31508;&#35760;&#26412;&#30005;&#33041;&#65289;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#65288;&#22914;&#23884;&#20837;&#24335;&#31995;&#32479;&#65289;&#12290;&#36890;&#36807;&#24341;&#20837;"&#20998;&#23618;"&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20351;LLMs&#21487;&#20197;&#25353;&#38656;&#35775;&#38382;&#65292;&#20316;&#20026;&#21487;&#23450;&#21046;&#30340;&#26381;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#30830;&#20445;&#21487;&#29992;&#35745;&#31639;&#36164;&#28304;&#21644;&#29992;&#25143;&#24212;&#29992;&#38656;&#27714;&#20043;&#38388;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;&#25105;&#20204;&#39044;&#35265;&#21040;&#65292;&#20998;&#23618;LLM&#30340;&#27010;&#24565;&#23558;&#36171;&#20104;&#24191;&#27867;&#30340;&#20247;&#21253;&#29992;&#25143;&#22522;&#30784;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#25216;&#26415;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable language understanding and generation capabilities. However, training, deploying, and accessing these models pose notable challenges, including resource-intensive demands, extended training durations, and scalability issues. To address these issues, we introduce a concept of hierarchical, distributed LLM architecture that aims at enhancing the accessibility and deployability of LLMs across heterogeneous computing platforms, including general-purpose computers (e.g., laptops) and IoT-style devices (e.g., embedded systems). By introducing a "layered" approach, the proposed architecture enables on-demand accessibility to LLMs as a customizable service. This approach also ensures optimal trade-offs between the available computational resources and the user's application needs. We envision that the concept of hierarchical LLM will empower extensive, crowd-sourced user bases to harness the capabilities of LLMs, thereby fostering advan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#27979;&#31574;&#30053;&#65292;&#21517;&#20026;&#24341;&#23548;&#36974;&#32617;&#65292;&#22312;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#20013;&#36890;&#36807;&#28040;&#38500;&#19981;&#21516;&#30340;&#27169;&#24577;&#26469;&#35780;&#20272;&#20854;&#23545;&#21160;&#35789;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27491;&#30830;&#30340;&#21160;&#35789;&#65292;&#19982;&#20808;&#21069;&#20174;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#25506;&#27979;&#25216;&#26415;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#24418;&#25104;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2401.16575</link><description>&lt;p&gt;
&#36229;&#36234;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65306;&#20351;&#29992;&#24341;&#23548;&#36974;&#32617;&#30340;&#22810;&#27169;&#24577;Transformer&#20013;&#30340;&#21160;&#35789;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking. (arXiv:2401.16575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#27979;&#31574;&#30053;&#65292;&#21517;&#20026;&#24341;&#23548;&#36974;&#32617;&#65292;&#22312;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#20013;&#36890;&#36807;&#28040;&#38500;&#19981;&#21516;&#30340;&#27169;&#24577;&#26469;&#35780;&#20272;&#20854;&#23545;&#21160;&#35789;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27491;&#30830;&#30340;&#21160;&#35789;&#65292;&#19982;&#20808;&#21069;&#20174;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#25506;&#27979;&#25216;&#26415;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#24418;&#25104;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#35201;&#30340;&#25506;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#20102;&#35299;&#36817;&#26399;&#22810;&#27169;&#24577;&#22270;&#20687;-&#35821;&#35328;Transformer&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#35780;&#20272;&#26159;&#22312;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#37325;&#28857;&#20851;&#27880;&#35745;&#25968;&#12289;&#20851;&#31995;&#12289;&#23646;&#24615;&#31561;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24341;&#23548;&#36974;&#32617;&#30340;&#26367;&#20195;&#25506;&#27979;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36974;&#32617;&#26469;&#28040;&#38500;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#34987;&#36974;&#32617;&#30340;&#21333;&#35789;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#32771;&#34385;ROI&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36825;&#20123;&#29305;&#24449;&#26159;&#36890;&#36807;&#29289;&#20307;&#26816;&#27979;&#22120;&#33719;&#24471;&#30340;&#36755;&#20837;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#24341;&#23548;&#36974;&#32617;&#22312;ViLBERT&#12289;LXMERT&#12289;UNITER&#21644;VisualBERT&#19978;&#30740;&#31350;&#21160;&#35789;&#30340;&#29702;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27491;&#30830;&#30340;&#21160;&#35789;&#12290;&#36825;&#19982;&#20808;&#21069;&#20174;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#25506;&#27979;&#25216;&#26415;&#20013;&#24471;&#20986;&#30340;&#32463;&#24120;&#26080;&#27861;&#25104;&#21151;&#30340;&#24773;&#20917;&#24418;&#25104;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant probing approaches rely on the zero-shot performance of image-text matching tasks to gain a finer-grained understanding of the representations learned by recent multimodal image-language transformer models. The evaluation is carried out on carefully curated datasets focusing on counting, relations, attributes, and others. This work introduces an alternative probing strategy called guided masking. The proposed approach ablates different modalities using masking and assesses the model's ability to predict the masked word with high accuracy. We focus on studying multimodal models that consider regions of interest (ROI) features obtained by object detectors as input tokens. We probe the understanding of verbs using guided masking on ViLBERT, LXMERT, UNITER, and VisualBERT and show that these models can predict the correct verb with high accuracy. This contrasts with previous conclusions drawn from image-text matching probing techniques that frequently fail in situations requir
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#24565;&#31354;&#38388;&#30340;&#35821;&#20041;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#26080;&#27861;&#25429;&#25417;&#21644;&#37327;&#21270;&#8220;&#24847;&#20041;&#8221;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16569</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#27010;&#24565;&#31354;&#38388;&#35821;&#20041;&#36890;&#20449;&#30340;&#39046;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-Based Domain Learning for Semantic Communication with Conceptual Spaces. (arXiv:2401.16569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16569
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#24565;&#31354;&#38388;&#30340;&#35821;&#20041;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#26080;&#27861;&#25429;&#25417;&#21644;&#37327;&#21270;&#8220;&#24847;&#20041;&#8221;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20934;&#30830;&#20256;&#36882;&#31526;&#21495;&#30456;&#27604;&#65292;&#20197;&#20934;&#30830;&#20256;&#36882;&#24847;&#20041;&#20026;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#24050;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;&#36825;&#31181;&#33539;&#24335;&#36890;&#24120;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#29616;&#20195;&#21457;&#23637;&#65292;&#20197;&#25552;&#39640;&#36890;&#20449;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25429;&#25417;&#21644;&#37327;&#21270;&#8220;&#24847;&#20041;&#8221;&#30340;&#32454;&#33410;&#32570;&#20047;&#19968;&#20010;&#26631;&#20934;&#27169;&#22411;&#65292;&#35768;&#22810;&#39046;&#20808;&#30340;&#35821;&#20041;&#36890;&#20449;&#26041;&#27861;&#37319;&#29992;&#40657;&#30418;&#26694;&#26550;&#65292;&#23545;&#27169;&#22411;&#30340;&#20855;&#20307;&#23398;&#20064;&#20869;&#23481;&#30693;&#20043;&#29978;&#23569;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#27010;&#24565;&#31354;&#38388;&#26694;&#26550;&#65292;&#20197;&#20960;&#20309;&#26041;&#24335;&#26126;&#30830;&#24314;&#27169;&#24847;&#20041;&#12290;&#34429;&#28982;&#20197;&#21069;&#20351;&#29992;&#27010;&#24565;&#31354;&#38388;&#30740;&#31350;&#35821;&#20041;&#36890;&#20449;&#30340;&#24037;&#20316;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#20808;&#21069;&#30340;&#23581;&#35797;&#28041;&#21450;&#25163;&#24037;&#21046;&#20316;&#27010;&#24565;&#31354;&#38388;&#27169;&#22411;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#27010;&#24565;&#31354;&#38388;&#35821;&#20041;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication with the goal of accurately conveying meaning, rather than accurately transmitting symbols, has become an area of growing interest. This paradigm, termed semantic communication, typically leverages modern developments in artificial intelligence and machine learning to improve the efficiency and robustness of communication systems. However, a standard model for capturing and quantifying the details of "meaning" is lacking, with many leading approaches to semantic communication adopting a black-box framework with little understanding of what exactly the model is learning. One solution is to utilize the conceptual spaces framework, which models meaning explicitly in a geometric manner. Though prior work studying semantic communication with conceptual spaces has shown promising results, these previous attempts involve hand-crafting a conceptual space model, severely limiting the scalability and practicality of the approach. In this work, we develop a framework for learning a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#21360;&#22320;&#35821;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21518;&#24724;&#34920;&#36798;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#21518;&#24724;&#35821;&#35328;&#34920;&#36798;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#39046;&#22495;&#65292;&#25581;&#31034;&#20102;&#21518;&#24724;&#30340;&#26469;&#28304;&#21644;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.16561</link><description>&lt;p&gt;
&#21360;&#22320;&#35821;&#22825;&#22478;&#23383;&#27597;&#33050;&#26412;&#20013;&#30340;&#22810;&#31867;&#21518;&#24724;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-class Regret Detection in Hindi Devanagari Script. (arXiv:2401.16561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#21360;&#22320;&#35821;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21518;&#24724;&#34920;&#36798;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#21518;&#24724;&#35821;&#35328;&#34920;&#36798;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#39046;&#22495;&#65292;&#25581;&#31034;&#20102;&#21518;&#24724;&#30340;&#26469;&#28304;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20351;&#29992;&#21360;&#22320;&#35821;&#30340;&#20154;&#25968;&#22823;&#24133;&#22686;&#21152;&#12290;&#21518;&#24724;&#26159;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#24120;&#35265;&#30340;&#24773;&#24863;&#20307;&#39564;&#12290;&#35768;&#22810;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20351;&#29992;&#32773;&#32463;&#24120;&#20998;&#20139;&#20182;&#20204;&#30340;&#21518;&#24724;&#32463;&#21382;&#21644;&#24847;&#35265;&#12290;&#22914;&#26524;&#26377;&#26426;&#20250;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#33258;&#24049;&#30340;&#36873;&#25321;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#21644;&#23545;&#19981;&#21516;&#36873;&#25321;&#30340;&#28212;&#26395;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#21518;&#24724;&#30340;&#26469;&#28304;&#23545;&#20110;&#30740;&#31350;&#20854;&#23545;&#34892;&#20026;&#21644;&#20915;&#31574;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#21518;&#24724;&#20197;&#21450;&#23427;&#22914;&#20309;&#22312;&#21360;&#22320;&#35821;&#20013;&#34920;&#36798;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#30340;&#26469;&#28304;&#65292;&#27599;&#20010;&#21477;&#23376;&#37117;&#34987;&#25163;&#21160;&#20998;&#31867;&#20026;&#8220;&#34892;&#21160;&#21518;&#24724;&#8221;&#12289;&#8220;&#19981;&#20316;&#20026;&#21518;&#24724;&#8221;&#21644;&#8220;&#26080;&#21518;&#24724;&#8221;&#20013;&#30340;&#19968;&#31867;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#30740;&#31350;&#21360;&#22320;&#35821;&#25991;&#26412;&#20013;&#30340;&#21518;&#24724;&#35821;&#35328;&#34920;&#36798;&#65292;&#24182;&#30830;&#23450;&#19982;&#21518;&#24724;&#26368;&#39057;&#32321;&#30456;&#20851;&#30340;&#25991;&#26412;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20010;&#20307;&#20204;&#26368;&#24120;&#19982;&#21518;&#24724;&#30456;&#20851;&#30340;&#39046;&#22495;&#26159;...
&lt;/p&gt;
&lt;p&gt;
The number of Hindi speakers on social media has increased dramatically in recent years. Regret is a common emotional experience in our everyday life. Many speakers on social media, share their regretful experiences and opinions regularly. It might cause a re-evaluation of one's choices and a desire to make a different option if given the chance. As a result, knowing the source of regret is critical for investigating its impact on behavior and decision-making. This study focuses on regret and how it is expressed, specifically in Hindi, on various social media platforms. In our study, we present a novel dataset from three different sources, where each sentence has been manually classified into one of three classes "Regret by action", "Regret by inaction", and "No regret". Next, we use this dataset to investigate the linguistic expressions of regret in Hindi text and also identify the textual domains that are most frequently associated with regret. Our findings indicate that individuals 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29992;&#20110;&#20248;&#20808;&#25490;&#24207;&#35823;&#23548;&#24615;&#20449;&#24687;&#20260;&#23475;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#23558;&#24615;&#21035;&#20316;&#20026;&#20027;&#35201;&#21464;&#37327;&#65292;&#30740;&#31350;LLM&#22312;&#35780;&#20272;&#38169;&#35823;&#20449;&#24687;&#30340;&#21361;&#23475;&#24615;&#26102;&#26159;&#21542;&#33021;&#22815;&#21453;&#26144;&#19981;&#21516;&#24615;&#21035;&#32676;&#20307;&#30340;&#35266;&#28857;&#65292;&#32467;&#26524;&#21457;&#29616;LLM&#21487;&#33021;&#22840;&#22823;&#20102;&#24615;&#21035;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.16558</link><description>&lt;p&gt;
&#22810;&#26679;&#20294;&#26377;&#20998;&#27495;&#65306;LLM&#21487;&#33021;&#22840;&#22823;&#19982;&#38169;&#35823;&#20449;&#24687;&#26377;&#20851;&#30340;&#20260;&#23475;&#38382;&#39064;&#19978;&#30340;&#24615;&#21035;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Diverse, but Divisive: LLMs Can Exaggerate Gender Differences in Opinion Related to Harms of Misinformation. (arXiv:2401.16558v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29992;&#20110;&#20248;&#20808;&#25490;&#24207;&#35823;&#23548;&#24615;&#20449;&#24687;&#20260;&#23475;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#23558;&#24615;&#21035;&#20316;&#20026;&#20027;&#35201;&#21464;&#37327;&#65292;&#30740;&#31350;LLM&#22312;&#35780;&#20272;&#38169;&#35823;&#20449;&#24687;&#30340;&#21361;&#23475;&#24615;&#26102;&#26159;&#21542;&#33021;&#22815;&#21453;&#26144;&#19981;&#21516;&#24615;&#21035;&#32676;&#20307;&#30340;&#35266;&#28857;&#65292;&#32467;&#26524;&#21457;&#29616;LLM&#21487;&#33021;&#22840;&#22823;&#20102;&#24615;&#21035;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#23548;&#24615;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#26222;&#36941;&#20256;&#25773;&#23545;&#31038;&#20250;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#19987;&#19994;&#30340;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#22312;&#24212;&#23545;&#27492;&#23041;&#32961;&#26102;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#38382;&#39064;&#35268;&#27169;&#24040;&#22823;&#36843;&#20351;&#20182;&#20204;&#24517;&#39035;&#23545;&#26377;&#38480;&#30340;&#36164;&#28304;&#36827;&#34892;&#20248;&#20808;&#25490;&#24207;&#12290;&#36825;&#31181;&#20248;&#20808;&#25490;&#24207;&#21487;&#33021;&#32771;&#34385;&#21040;&#19968;&#31995;&#21015;&#22240;&#32032;&#65292;&#22914;&#23545;&#29305;&#23450;&#20154;&#32676;&#36896;&#25104;&#30340;&#20260;&#23475;&#39118;&#38505;&#19981;&#21516;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20419;&#36827;&#27492;&#31867;&#20248;&#20808;&#25490;&#24207;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#22240;&#20026;&#20107;&#23454;&#26680;&#26597;&#24433;&#21709;&#30528;&#31038;&#20250;&#21508;&#20010;&#22810;&#26679;&#21270;&#37096;&#20998;&#65292;&#25152;&#20197;&#37325;&#35201;&#30340;&#26159;&#22312;&#23545;&#32034;&#36180;&#30340;&#20248;&#20808;&#25490;&#24207;&#36807;&#31243;&#20013;&#21453;&#26144;&#22810;&#26679;&#21270;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;LLM&#22312;&#35780;&#20272;&#38169;&#35823;&#20449;&#24687;&#30340;&#21361;&#23475;&#24615;&#26102;&#65292;&#33021;&#21542;&#21453;&#26144;&#19981;&#21516;&#32676;&#20307;&#30340;&#35266;&#28857;&#65292;&#37325;&#28857;&#20851;&#27880;&#24615;&#21035;&#20316;&#20026;&#20027;&#35201;&#21464;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#65288;1&#65289;&#24102;&#26377;&#26126;&#30830;&#24615;&#21035;&#25351;&#31216;&#30340;&#25552;&#31034;&#22312;&#32654;&#22269;&#31038;&#20250;&#30456;&#20851;&#20027;&#39064;&#19978;&#26159;&#21542;&#21453;&#26144;&#20102;&#24615;&#21035;&#24046;&#24322;&#30340;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
The pervasive spread of misinformation and disinformation poses a significant threat to society. Professional fact-checkers play a key role in addressing this threat, but the vast scale of the problem forces them to prioritize their limited resources. This prioritization may consider a range of factors, such as varying risks of harm posed to specific groups of people. In this work, we investigate potential implications of using a large language model (LLM) to facilitate such prioritization. Because fact-checking impacts a wide range of diverse segments of society, it is important that diverse views are represented in the claim prioritization process. This paper examines whether a LLM can reflect the views of various groups when assessing the harms of misinformation, focusing on gender as a primary variable. We pose two central questions: (1) To what extent do prompts with explicit gender references reflect gender differences in opinion in the United States on topics of social relevance
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.16553</link><description>&lt;p&gt;
SelectLLM&#65306;LLMs&#33021;&#21542;&#36873;&#25321;&#37325;&#35201;&#30340;&#25351;&#20196;&#36827;&#34892;&#27880;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#20351;&#27169;&#22411;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#19968;&#23567;&#32452;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#22823;&#37327;&#26356;&#22024;&#26434;&#30340;&#25351;&#20196;&#12290;&#30001;&#20110;&#25351;&#20196;&#26159;&#26080;&#26631;&#31614;&#30340;&#65292;&#19988;&#21709;&#24212;&#26159;&#33258;&#28982;&#25991;&#26412;&#65292;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36873;&#25321;&#26080;&#26631;&#31614;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#65292;&#31216;&#20026;SelectLLM&#65292;&#23427;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#39640;&#32423;&#24605;&#24819;&#26159;&#21033;&#29992;LLMs&#36890;&#36807;&#25552;&#31034;&#26469;&#20272;&#35745;&#27599;&#20010;&#25351;&#20196;&#22312;&#27809;&#26377;&#30456;&#24212;&#26631;&#31614;&#65288;&#21363;&#21709;&#24212;&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#12290;SelectLLM&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#65288;&#20363;&#22914;CoreSet&#65289;&#23558;&#26080;&#26631;&#31614;&#25351;&#20196;&#21010;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#65292;&#28982;&#21518;&#25552;&#31034;LLMs&#22312;&#20854;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GuReT&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20869;&#30106;&#21644;&#21518;&#24724;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25506;&#32034;&#20854;&#22312;&#25991;&#26412;&#20013;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#22312;&#20869;&#30106;&#21644;&#21518;&#24724;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16541</link><description>&lt;p&gt;
GuReT&#65306;&#21306;&#20998;&#19982;&#20869;&#30106;&#30456;&#20851;&#21644;&#21518;&#24724;&#30456;&#20851;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
GuReT: Distinguishing Guilt and Regret related Text. (arXiv:2401.16541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GuReT&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20869;&#30106;&#21644;&#21518;&#24724;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25506;&#32034;&#20854;&#22312;&#25991;&#26412;&#20013;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#22312;&#20869;&#30106;&#21644;&#21518;&#24724;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20915;&#31574;&#21644;&#24773;&#32490;&#65288;&#29305;&#21035;&#26159;&#20869;&#30106;&#21644;&#21518;&#24724;&#65289;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#23545;&#34892;&#20026;&#21644;&#24184;&#31119;&#24863;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#27169;&#22411;&#20013;&#24448;&#24448;&#24573;&#35270;&#20102;&#36825;&#20123;&#24773;&#32490;&#30340;&#24494;&#22937;&#21306;&#21035;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21078;&#26512;&#20869;&#30106;&#21644;&#21518;&#24724;&#20043;&#38388;&#30340;&#20851;&#31995;&#21450;&#20854;&#29420;&#29305;&#30340;&#25991;&#26412;&#26631;&#24535;&#65292;&#24357;&#34917;&#20102;&#24773;&#24863;&#35745;&#31639;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26174;&#33879;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20869;&#30106;&#21644;&#21518;&#24724;&#35782;&#21035;&#35270;&#20026;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#20845;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#30740;&#31350;&#36824;&#37319;&#29992;&#20102;&#38142;&#29366;&#24605;&#32500;&#21644;&#26641;&#29366;&#24605;&#32500;&#31561;&#21019;&#26032;&#25512;&#29702;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#35299;&#37322;&#36923;&#36753;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#26368;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;85.3%&#30340;&#23439;F1&#20998;&#25968;&#65292;&#21487;&#20197;&#36798;&#21040;90.4%&#30340;&#23439;F1&#20998;&#25968;&#65292;&#23637;&#31034;&#20102;&#20854;&#36739;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intricate relationship between human decision-making and emotions, particularly guilt and regret, has significant implications on behavior and well-being. Yet, these emotions subtle distinctions and interplay are often overlooked in computational models. This paper introduces a dataset tailored to dissect the relationship between guilt and regret and their unique textual markers, filling a notable gap in affective computing research. Our approach treats guilt and regret recognition as a binary classification task and employs three machine learning and six transformer-based deep learning techniques to benchmark the newly created dataset. The study further implements innovative reasoning methods like chain-of-thought and tree-of-thought to assess the models interpretive logic. The results indicate a clear performance edge for transformer-based models, achieving a 90.4% macro F1 score compared to the 85.3% scored by the best machine learning classifier, demonstrating their superior ca
&lt;/p&gt;</description></item><item><title>InfoLossQA&#26159;&#19968;&#20010;&#38024;&#23545;&#25991;&#26412;&#31616;&#21270;&#20013;&#20449;&#24687;&#25439;&#22833;&#30340;&#29305;&#24449;&#21270;&#19982;&#24674;&#22797;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#38382;&#31572;&#23545;&#30340;&#24418;&#24335;&#65292;&#24110;&#21161;&#35835;&#32773;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#25991;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20449;&#24687;&#25439;&#22833;&#39057;&#32321;&#21457;&#29983;&#65292;&#32780;QA&#23545;&#21017;&#33021;&#25552;&#20379;&#21738;&#20123;&#20449;&#24687;&#34987;&#20002;&#22833;&#30340;&#24635;&#32467;&#12290;</title><link>http://arxiv.org/abs/2401.16475</link><description>&lt;p&gt;
InfoLossQA: &#25991;&#26412;&#31616;&#21270;&#20013;&#20449;&#24687;&#25439;&#22833;&#30340;&#29305;&#24449;&#21270;&#19982;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification. (arXiv:2401.16475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16475
&lt;/p&gt;
&lt;p&gt;
InfoLossQA&#26159;&#19968;&#20010;&#38024;&#23545;&#25991;&#26412;&#31616;&#21270;&#20013;&#20449;&#24687;&#25439;&#22833;&#30340;&#29305;&#24449;&#21270;&#19982;&#24674;&#22797;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#38382;&#31572;&#23545;&#30340;&#24418;&#24335;&#65292;&#24110;&#21161;&#35835;&#32773;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#25991;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20449;&#24687;&#25439;&#22833;&#39057;&#32321;&#21457;&#29983;&#65292;&#32780;QA&#23545;&#21017;&#33021;&#25552;&#20379;&#21738;&#20123;&#20449;&#24687;&#34987;&#20002;&#22833;&#30340;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#26088;&#22312;&#20351;&#19987;&#19994;&#25991;&#26412;&#23545;&#26222;&#36890;&#35835;&#32773;&#26356;&#26131;&#29702;&#35299;&#65292;&#20294;&#24120;&#24120;&#23548;&#33268;&#20449;&#24687;&#21024;&#38500;&#21644;&#27169;&#31946;&#19981;&#28165;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;InfoLossQA&#26694;&#26550;&#65292;&#29992;&#20197;&#29305;&#24449;&#21270;&#21644;&#24674;&#22797;&#30001;&#31616;&#21270;&#24341;&#36215;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#20197;&#38382;&#31572;&#65288;QA&#65289;&#23545;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#22522;&#20110;&#8220;&#38382;&#39064;&#35752;&#35770;&#8221;&#29702;&#35770;&#65292;&#36825;&#20123;QA&#23545;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#28145;&#20837;&#20102;&#35299;&#25991;&#26412;&#12290;&#25105;&#20204;&#23545;&#35813;&#26694;&#26550;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#30001;104&#20010;&#21307;&#23398;&#30740;&#31350;&#31185;&#23398;&#25688;&#35201;&#30340;104&#20010;LLM&#31616;&#21270;&#20013;&#25152;&#34893;&#29983;&#30340;1000&#20010;&#35821;&#35328;&#23398;&#23478;&#31574;&#21010;&#30340;QA&#23545;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#25968;&#25454;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20449;&#24687;&#25439;&#22833;&#32463;&#24120;&#21457;&#29983;&#65292;&#24182;&#19988;QA&#23545;&#21487;&#20197;&#39640;&#23618;&#27425;&#22320;&#24635;&#32467;&#20986;&#21738;&#20123;&#20449;&#24687;&#34987;&#20002;&#22833;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#65306;&#31471;&#21040;&#31471;&#20419;&#20351;&#24320;&#28304;&#21644;&#21830;&#19994;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27969;&#27700;&#32447;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;QA&#23545;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text simplification aims to make technical texts more accessible to laypeople but often results in deletion of information and vagueness. This work proposes InfoLossQA, a framework to characterize and recover simplification-induced information loss in form of question-and-answer (QA) pairs. Building on the theory of Question Under Discussion, the QA pairs are designed to help readers deepen their knowledge of a text. We conduct a range of experiments with this framework. First, we collect a dataset of 1,000 linguist-curated QA pairs derived from 104 LLM simplifications of scientific abstracts of medical studies. Our analyses of this data reveal that information loss occurs frequently, and that the QA pairs give a high-level overview of what information was lost. Second, we devise two methods for this task: end-to-end prompting of open-source and commercial language models, and a natural language inference pipeline. With a novel evaluation framework considering the correctness of QA pai
&lt;/p&gt;</description></item><item><title>ReGAL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#20195;&#30721;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#20989;&#25968;&#24211;&#65292;&#21033;&#29992;&#36825;&#20123;&#20849;&#20139;&#20989;&#25968;&#24211;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2401.16467</link><description>&lt;p&gt;
ReGAL: &#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReGAL: Refactoring Programs to Discover Generalizable Abstractions. (arXiv:2401.16467v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16467
&lt;/p&gt;
&lt;p&gt;
ReGAL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#20195;&#30721;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#20989;&#25968;&#24211;&#65292;&#21033;&#29992;&#36825;&#20123;&#20849;&#20139;&#20989;&#25968;&#24211;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#31243;&#24207;&#21512;&#25104;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#24320;&#21457;&#26377;&#29992;&#25277;&#35937;&#25152;&#38656;&#30340;&#20840;&#23616;&#35270;&#35282;&#65307;&#23427;&#20204;&#36890;&#24120;&#19968;&#27425;&#39044;&#27979;&#19968;&#20010;&#31243;&#24207;&#65292;&#32463;&#24120;&#37325;&#22797;&#30456;&#21516;&#30340;&#21151;&#33021;&#12290;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#20887;&#20313;&#20195;&#30721;&#26082;&#20302;&#25928;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36890;&#29992;&#25277;&#35937;&#23398;&#20064;&#30340;&#37325;&#26500;&#26041;&#27861;&#65288;ReGAL&#65289;&#65292;&#36890;&#36807;&#20195;&#30721;&#37325;&#26500;&#26469;&#23398;&#20064;&#21487;&#37325;&#29992;&#20989;&#25968;&#24211;&#65292;&#21363;&#22312;&#19981;&#25913;&#21464;&#20195;&#30721;&#25191;&#34892;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#37325;&#32452;&#20195;&#30721;&#12290;ReGAL&#20174;&#19968;&#23567;&#32452;&#29616;&#26377;&#31243;&#24207;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#25191;&#34892;&#39564;&#35777;&#21644;&#32454;&#21270;&#25277;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ReGAL&#21457;&#29616;&#30340;&#20849;&#20139;&#20989;&#25968;&#24211;&#20351;&#24471;&#22312;&#19981;&#21516;&#39046;&#22495;&#39044;&#27979;&#31243;&#24207;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;LOGO&#22270;&#24418;&#29983;&#25104;&#12289;&#26085;&#26399;&#25512;&#29702;&#21644;&#22522;&#20110;Minecraft&#30340;&#25991;&#23383;&#28216;&#25103;TextCraft&#65289;&#19978;&#65292;&#24320;&#28304;&#21644;&#19987;&#26377;&#30340;LLMs&#22312;&#20351;&#29992;ReGAL&#20989;&#25968;&#24211;&#39044;&#27979;&#31243;&#24207;&#26102;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e. restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On three datasets (LOGO graphics generation, Date reasoning, and TextCraft, a Minecraft-based text game), both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL fun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16458</link><description>&lt;p&gt;
&#20449;&#29992;&#39118;&#38505;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65306;&#20174;P2P&#20511;&#36151;&#30340;&#36151;&#27454;&#25551;&#36848;&#20013;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending. (arXiv:2401.16458v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
P2P&#20511;&#36151;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#30340;&#34701;&#36164;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#32447;&#24179;&#21488;&#23558;&#20511;&#27454;&#20154;&#19982;&#25918;&#27454;&#20154;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;P2P&#20511;&#36151;&#38754;&#20020;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25918;&#27454;&#20154;&#24448;&#24448;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#20511;&#27454;&#20154;&#30340;&#20449;&#29992;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#20511;&#27454;&#20154;&#22312;&#36151;&#27454;&#30003;&#35831;&#36807;&#31243;&#20013;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22788;&#29702;&#36825;&#20123;&#25991;&#26412;&#25551;&#36848;&#65292;LLM&#26159;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#27169;&#24335;&#21644;&#35821;&#20041;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#23558;LLM&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;Lending Club&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;BERT&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#26174;&#33879;&#25552;&#39640;&#20102;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#22266;&#26377;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#20197;&#21450;&#28508;&#22312;&#20559;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism, linking borrowers with lenders through online platforms. However, P2P lending faces the challenge of information asymmetry, as lenders often lack sufficient data to assess the creditworthiness of borrowers. This paper proposes a novel approach to address this issue by leveraging the textual descriptions provided by borrowers during the loan application process. Our methodology involves processing these textual descriptions using a Large Language Model (LLM), a powerful tool capable of discerning patterns and semantics within the text. Transfer learning is applied to adapt the LLM to the specific task at hand.  Our results derived from the analysis of the Lending Club dataset show that the risk score generated by BERT, a widely used LLM, significantly improves the performance of credit risk classifiers. However, the inherent opacity of LLM-based systems, coupled with uncertainties about potential biases, unders
&lt;/p&gt;</description></item><item><title>KAUCUS&#24341;&#20837;&#20102;&#30693;&#35782;&#22686;&#24378;&#29992;&#25143;&#27169;&#25311;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#25311;&#22120;&#21161;&#25163;&#20132;&#20114;&#65292;&#24182;&#33021;&#22815;&#24555;&#36895;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#21161;&#25163;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16454</link><description>&lt;p&gt;
KAUCUS: &#30693;&#35782;&#22686;&#24378;&#29992;&#25143;&#27169;&#25311;&#22120;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
KAUCUS: Knowledge Augmented User Simulators for Training Language Model Assistants. (arXiv:2401.16454v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16454
&lt;/p&gt;
&lt;p&gt;
KAUCUS&#24341;&#20837;&#20102;&#30693;&#35782;&#22686;&#24378;&#29992;&#25143;&#27169;&#25311;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#25311;&#22120;&#21161;&#25163;&#20132;&#20114;&#65292;&#24182;&#33021;&#22815;&#24555;&#36895;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#21161;&#25163;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#26377;&#29992;&#20132;&#20114;&#25968;&#25454;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#24320;&#21457;&#20986;&#19968;&#20010;&#26377;&#25928;&#30340;&#22810;&#36718;&#25351;&#20196;&#36319;&#38543;&#21161;&#25163;&#12290;&#29702;&#24819;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#38500;&#20102;&#20381;&#38752;&#20854;&#20869;&#22312;&#26435;&#37325;&#22806;&#65292;&#36824;&#24212;&#33021;&#22815;&#24555;&#36895;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#27169;&#25311;&#20114;&#32852;&#32593;&#19978;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#12290;&#20197;&#24448;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#36890;&#24120;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#20027;&#35201;&#26159;&#23553;&#38381;&#39046;&#22495;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#20005;&#26684;&#30340;&#27169;&#24335;&#65292;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#24555;&#36895;&#25193;&#23637;&#20197;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Kaucus&#30340;&#30693;&#35782;&#22686;&#24378;&#29992;&#25143;&#27169;&#25311;&#22120;&#26694;&#26550;&#65292;&#20197;&#27010;&#36848;&#21019;&#24314;&#22810;&#26679;&#21270;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#26080;&#32541;&#22320;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#19979;&#28216;&#21161;&#25163;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;GPT-J&#30340;&#27169;&#25311;&#22120;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#27169;&#25311;&#22120;&#21644;&#25688;&#35201;&#25511;&#21046;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#25311;&#22120;&#21161;&#25163;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
An effective multi-turn instruction-following assistant can be developed by creating a simulator that can generate useful interaction data. Apart from relying on its intrinsic weights, an ideal user simulator should also be able to bootstrap external knowledge rapidly in its raw form to simulate the multifarious diversity of text available over the internet. Previous user simulators generally lacked diversity, were mostly closed domain, and necessitated rigid schema making them inefficient to rapidly scale to incorporate external knowledge. In this regard, we introduce, Kaucus, a Knowledge-Augmented User Simulator framework, to outline a process of creating diverse user simulators, that can seamlessly exploit external knowledge as well as benefit downstream assistant model training. Through two GPT-J based simulators viz., a Retrieval Augmented Simulator and a Summary Controlled Simulator we generate diverse simulator-assistant interactions. Through reward and preference model-based ev
&lt;/p&gt;</description></item><item><title>FaKnow&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31639;&#27861;&#24211;&#65292;&#21253;&#21547;&#22810;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#21644;&#24037;&#20855;&#65292;&#24182;&#35299;&#20915;&#20102;&#19981;&#21516;&#26694;&#26550;&#19979;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16441</link><description>&lt;p&gt;
FaKnow: &#19968;&#20010;&#29992;&#20110;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
FaKnow: A Unified Library for Fake News Detection. (arXiv:2401.16441v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16441
&lt;/p&gt;
&lt;p&gt;
FaKnow&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31639;&#27861;&#24211;&#65292;&#21253;&#21547;&#22810;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#21644;&#24037;&#20855;&#65292;&#24182;&#35299;&#20915;&#20102;&#19981;&#21516;&#26694;&#26550;&#19979;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22823;&#37327;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31639;&#27861;&#24212;&#36816;&#32780;&#29983;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#22312;&#19981;&#21516;&#30340;&#26694;&#26550;&#19979;&#24320;&#21457;&#65292;&#27599;&#20010;&#26694;&#26550;&#21448;&#35201;&#27714;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#22240;&#27492;&#38459;&#30861;&#20102;&#21487;&#37325;&#22797;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#30340;&#20195;&#30721;&#24320;&#21457;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#20887;&#20313;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FaKnow&#65292;&#19968;&#20010;&#32479;&#19968;&#19988;&#20840;&#38754;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31639;&#27861;&#24211;&#12290;&#23427;&#28085;&#30422;&#20102;&#22810;&#31181;&#24120;&#29992;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#31038;&#20250;&#29615;&#22659;&#30340;&#26041;&#27861;&#12290;&#35813;&#24211;&#28085;&#30422;&#20102;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#30340;&#23436;&#25972;&#33539;&#22260;&#65292;&#22312;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20869;&#26377;&#25928;&#32452;&#32455;&#20102;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#35757;&#32451;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#36741;&#21161;&#21151;&#33021;&#21644;&#24037;&#20855;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#21644;&#26085;&#24535;&#35760;&#24405;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21270;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past years, a large number of fake news detection algorithms based on deep learning have emerged. However, they are often developed under different frameworks, each mandating distinct utilization methodologies, consequently hindering reproducibility. Additionally, a substantial amount of redundancy characterizes the code development of such fake news detection models. To address these concerns, we propose FaKnow, a unified and comprehensive fake news detection algorithm library. It encompasses a variety of widely used fake news detection models, categorized as content-based and social context-based approaches. This library covers the full spectrum of the model training and evaluation process, effectively organizing the data, models, and training procedures within a unified framework. Furthermore, it furnishes a series of auxiliary functionalities and tools, including visualization, and logging. Our work contributes to the standardization and unification of fake news detection 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#38598;&#20449;&#24687;&#26816;&#32034;&#21644;&#25552;&#21462;&#20110;&#19968;&#20307;&#30340;&#24037;&#20855;&#65292;&#24212;&#29992;&#20110;COVID-19 Open Research Dataset (CORD-19)&#12290;&#20027;&#35201;&#30446;&#30340;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#26356;&#22909;&#30340;COVID-19&#30456;&#20851;&#35770;&#25991;&#30340;&#25628;&#32034;&#24037;&#20855;&#65292;&#24110;&#21161;&#20182;&#20204;&#25214;&#21040;&#21442;&#32771;&#35770;&#25991;&#24182;&#31361;&#20986;&#26174;&#31034;&#25991;&#26412;&#20013;&#30340;&#30456;&#20851;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.16430</link><description>&lt;p&gt;
&#19968;&#20010;&#38024;&#23545;Covid-19&#30456;&#20851;&#35770;&#25991;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#25552;&#21462;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
An Information Retrieval and Extraction Tool for Covid-19 Related Papers. (arXiv:2401.16430v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#38598;&#20449;&#24687;&#26816;&#32034;&#21644;&#25552;&#21462;&#20110;&#19968;&#20307;&#30340;&#24037;&#20855;&#65292;&#24212;&#29992;&#20110;COVID-19 Open Research Dataset (CORD-19)&#12290;&#20027;&#35201;&#30446;&#30340;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#26356;&#22909;&#30340;COVID-19&#30456;&#20851;&#35770;&#25991;&#30340;&#25628;&#32034;&#24037;&#20855;&#65292;&#24110;&#21161;&#20182;&#20204;&#25214;&#21040;&#21442;&#32771;&#35770;&#25991;&#24182;&#31361;&#20986;&#26174;&#31034;&#25991;&#26412;&#20013;&#30340;&#30456;&#20851;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;COVID-19&#22823;&#27969;&#34892;&#23545;&#20840;&#29699;&#30340;&#21355;&#29983;&#31995;&#32479;&#36896;&#25104;&#20102;&#20005;&#37325;&#24433;&#21709;&#12290;&#20854;&#20005;&#37325;&#24615;&#20197;&#21450;&#20010;&#20154;&#21644;&#32452;&#32455;&#24320;&#23637;&#23545;&#31574;&#30740;&#31350;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#23548;&#33268;&#31185;&#23398;&#26399;&#21002;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#26032;&#30340;&#30740;&#31350;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#23558;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#21644;&#25552;&#21462;&#65288;IE&#65289;&#30340;&#26041;&#38754;&#24212;&#29992;&#20110;COVID-19 Open Research Dataset&#65288;CORD-19&#65289;&#30340;&#26032;&#39062;&#24037;&#20855;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#26356;&#22909;&#30340;COVID-19&#30456;&#20851;&#35770;&#25991;&#30340;&#25628;&#32034;&#24037;&#20855;&#65292;&#24110;&#21161;&#20182;&#20204;&#25214;&#21040;&#21442;&#32771;&#35770;&#25991;&#24182;&#31361;&#20986;&#26174;&#31034;&#25991;&#26412;&#20013;&#30340;&#30456;&#20851;&#23454;&#20307;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24212;&#29992;&#38544;&#21547;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#26469;&#26681;&#25454;&#30740;&#31350;&#26041;&#38754;&#23545;CORD-19&#20013;&#25152;&#26377;&#33521;&#25991;&#25688;&#35201;&#30340;&#20027;&#39064;&#36827;&#34892;&#24314;&#27169;&#12290;&#25552;&#21462;&#27599;&#20010;&#25688;&#35201;&#30340;&#30456;&#20851;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#23558;&#20854;&#38142;&#25509;&#21040;&#30456;&#24212;&#30340;UMLS&#27010;&#24565;&#12290;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;K&#26368;&#36817;&#37051;&#31639;&#27861;&#26469;&#23545;&#30456;&#20851;&#35770;&#25991;&#36827;&#34892;&#25490;&#21517;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#24037;&#20855;&#24050;&#32463;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Background: The COVID-19 pandemic has caused severe impacts on health systems worldwide. Its critical nature and the increased interest of individuals and organizations to develop countermeasures to the problem has led to a surge of new studies in scientific journals. Objetive: We sought to develop a tool that incorporates, in a novel way, aspects of Information Retrieval (IR) and Extraction (IE) applied to the COVID-19 Open Research Dataset (CORD-19). The main focus of this paper is to provide researchers with a better search tool for COVID-19 related papers, helping them find reference papers and hightlight relevant entities in text. Method: We applied Latent Dirichlet Allocation (LDA) to model, based on research aspects, the topics of all English abstracts in CORD-19. Relevant named entities of each abstract were extracted and linked to the corresponding UMLS concept. Regular expressions and the K-Nearest Neighbors algorithm were used to rank relevant papers. Results: Our tool has s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32467;&#21512;&#20027;&#39064;&#24314;&#27169;&#21644;&#24341;&#29992;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#26469;&#30740;&#31350;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20851;&#20110;&#23562;&#37325;&#31169;&#23494;&#21644;&#23478;&#24237;&#29983;&#27963;&#30340;&#26696;&#20363;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#21644;&#32452;&#32455;&#20855;&#26377;&#30456;&#20284;&#20027;&#39064;&#21644;&#24341;&#29992;&#27169;&#24335;&#30340;&#26696;&#20363;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#33021;&#22815;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16429</link><description>&lt;p&gt;
&#32467;&#21512;&#20027;&#39064;&#24314;&#27169;&#21644;&#24341;&#29992;&#32593;&#32476;&#20998;&#26512;&#30740;&#31350;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20851;&#20110;&#23562;&#37325;&#31169;&#20154;&#21644;&#23478;&#24237;&#29983;&#27963;&#26435;&#21033;&#30340;&#26696;&#20363;&#27861;
&lt;/p&gt;
&lt;p&gt;
Combining topic modelling and citation network analysis to study case law from the European Court on Human Rights on the right to respect for private and family life. (arXiv:2401.16429v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32467;&#21512;&#20027;&#39064;&#24314;&#27169;&#21644;&#24341;&#29992;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#26469;&#30740;&#31350;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20851;&#20110;&#23562;&#37325;&#31169;&#23494;&#21644;&#23478;&#24237;&#29983;&#27963;&#30340;&#26696;&#20363;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#21644;&#32452;&#32455;&#20855;&#26377;&#30456;&#20284;&#20027;&#39064;&#21644;&#24341;&#29992;&#27169;&#24335;&#30340;&#26696;&#20363;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#33021;&#22815;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;HUDOC&#31561;&#27861;&#24459;&#26696;&#20363;&#27861;&#25968;&#25454;&#24211;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#20026;&#20102;&#22788;&#29702;&#22914;&#27492;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#27861;&#24459;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#39640;&#25928;&#30340;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#26696;&#20363;&#27861;&#25968;&#25454;&#24211;&#36890;&#24120;&#21253;&#21547;&#26696;&#20214;&#30340;&#25991;&#26412;&#20869;&#23481;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#24341;&#29992;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#26469;&#33258;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20851;&#20110;&#27431;&#27954;&#20154;&#26435;&#20844;&#32422;&#31532;8&#26465;&#20851;&#20110;&#23562;&#37325;&#31169;&#20154;&#21644;&#23478;&#24237;&#29983;&#27963;&#12289;&#23478;&#24237;&#21644;&#36890;&#20449;&#26435;&#21033;&#30340;&#26696;&#20363;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#24182;&#27604;&#36739;&#20102;&#20027;&#39064;&#24314;&#27169;&#21644;&#24341;&#29992;&#32593;&#32476;&#22312;&#26681;&#25454;&#19968;&#33324;&#20027;&#39064;&#21644;&#24341;&#29992;&#27169;&#24335;&#25214;&#21040;&#21644;&#32452;&#32455;&#31532;8&#26465;&#26696;&#20363;&#27861;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#32467;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#26159;&#21542;&#27604;&#20165;&#24212;&#29992;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#25163;&#24037;&#25910;&#38598;&#21644;&#27880;&#37322;&#30340;&#20851;&#20110;&#39537;&#36880;&#30340;&#31532;8&#26465;&#26696;&#20363;&#27861;&#29420;&#29305;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#32452;&#21512;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;&#20351;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As legal case law databases such as HUDOC continue to grow rapidly, it has become essential for legal researchers to find efficient methods to handle such large-scale data sets. Such case law databases usually consist of the textual content of cases together with the citations between them. This paper focuses on case law from the European Court of Human Rights on Article 8 of the European Convention of Human Rights, the right to respect private and family life, home and correspondence. In this study, we demonstrate and compare the potential of topic modelling and citation network to find and organize case law on Article 8 based on their general themes and citation patterns, respectively. Additionally, we explore whether combining these two techniques leads to better results compared to the application of only one of the methods. We evaluate the effectiveness of the combined method on a unique manually collected and annotated dataset of Aricle 8 case law on evictions. The results of our
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15378</link><description>&lt;p&gt;
&#22522;&#20110;RAG&#30340;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#25552;&#26696;&#65306;MufassirQAS LLM
&lt;/p&gt;
&lt;p&gt;
A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15378
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#29702;&#35299;&#23447;&#25945;&#23384;&#22312;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#30340;&#25361;&#25112;&#12290;&#38382;&#31572;&#26426;&#22120;&#20154;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#12290;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24314;&#31435;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#29992;&#20110;&#23447;&#25945;&#21551;&#33945;&#30340;&#38382;&#39064;&#22238;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;LLM&#20063;&#26377;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#20542;&#21521;&#65292;&#31216;&#20026;&#24187;&#35273;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#21487;&#33021;&#21253;&#21547;&#20398;&#36785;&#20010;&#20154;&#23447;&#25945;&#20449;&#20208;&#12289;&#36328;&#23447;&#27966;&#20914;&#31361;&#21644;&#26377;&#20105;&#35758;&#25110;&#25935;&#24863;&#30340;&#35805;&#39064;&#30340;&#20869;&#23481;&#12290;&#23427;&#38656;&#35201;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#32780;&#19981;&#20250;&#23459;&#25196;&#20167;&#24680;&#35328;&#35770;&#25110;&#20882;&#29359;&#26576;&#20123;&#32676;&#20307;&#30340;&#20154;&#25110;&#20182;&#20204;&#30340;&#20449;&#20208;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#25968;&#25454;&#24211;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#26469;&#25552;&#39640;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#30340;&#38382;&#31572;&#31995;&#32479;&#31216;&#20026;"MufassirQAS"&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#35813;&#31995;&#32479;&#24182;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#23447;&#25945;&#34892;&#19994;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
&lt;/p&gt;</description></item><item><title>&#22312;&#37329;&#34701;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#65292;&#36890;&#36807;&#20026;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#65292;&#25105;&#20204;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#24322;&#26500;&#37329;&#34701;&#25968;&#25454;&#21644;&#20445;&#35777;&#31934;&#24230;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.15328</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance. (arXiv:2401.15328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15328
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#65292;&#36890;&#36807;&#20026;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#65292;&#25105;&#20204;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#24322;&#26500;&#37329;&#34701;&#25968;&#25454;&#21644;&#20445;&#35777;&#31934;&#24230;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#37329;&#34701;&#31561;&#19987;&#19994;&#39046;&#22495;&#20013;&#36935;&#21040;&#20102;&#38169;&#35823;&#20256;&#25773;&#21644;&#20135;&#29983;&#24187;&#35273;&#31561;&#25361;&#25112;&#65292;&#20854;&#20013;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#31934;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22806;&#37096;&#24037;&#20855;&#22686;&#24378;&#33021;&#21147;&#65292;&#23558;&#26576;&#20123;&#25512;&#29702;&#27493;&#39588;&#36716;&#31227;&#21040;&#26356;&#36866;&#21512;&#35813;&#20219;&#21153;&#30340;&#22806;&#37096;&#24037;&#20855;&#19978;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;LLM&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#37329;&#34701;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#22312;LLaMA-2 13B Chat&#27169;&#22411;&#19978;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#20351;&#20854;&#26082;&#20805;&#24403;&#8220;&#20219;&#21153;&#36335;&#30001;&#22120;&#8221;&#21448;&#20805;&#24403;&#8220;&#20219;&#21153;&#35299;&#20915;&#22120;&#8221;&#12290;&#35813;&#8220;&#20219;&#21153;&#36335;&#30001;&#22120;&#8221;&#21160;&#24577;&#23558;&#38382;&#39064;&#23450;&#21521;&#21040;LLM&#20869;&#37096;&#22238;&#31572;&#25110;&#36890;&#36807;&#24037;&#20855;&#38598;&#20013;&#30340;&#27491;&#30830;&#24037;&#20855;&#22806;&#37096;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;SFT&#27169;&#22411;Raven&#30456;&#27604;&#22522;&#30784;&#27169;&#22411;&#21644;&#20165;&#26377;SFT&#30340;&#22522;&#20934;&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;&#20102;35.2%&#21644;5.06%&#65292;&#22312;&#31454;&#20105;&#21147;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount. We explore the potential of language model augmentation with external tools to mitigate these limitations and offload certain reasoning steps to external tools that are more suited for the task, instead of solely depending on the LLM's inherent abilities. More concretely, using financial domain question-answering datasets, we apply supervised fine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and 'task solver'. The 'task router' dynamically directs a question to either be answered internally by the LLM or externally via the right tool from the tool set. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2% and 5.06% over the base model and SFT-only baselines, respectively, and is highly competitive with st
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#36861;&#36394;&#30740;&#31350;&#65292;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20102;&#27719;&#24635;&#21644;&#27979;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20154;&#24037;&#25968;&#25454;&#36136;&#37327;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2401.14698</link><description>&lt;p&gt;
&#21453;&#24605;LLM&#29983;&#25104;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#65306;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#30340;&#36861;&#36394;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Under the Surface: Tracking the Artifactuality of LLM-Generated Data. (arXiv:2401.14698v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#36861;&#36394;&#30740;&#31350;&#65292;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20102;&#27719;&#24635;&#21644;&#27979;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20154;&#24037;&#25968;&#25454;&#36136;&#37327;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#20154;&#24037;&#25968;&#25454;&#26041;&#38754;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#20316;&#29992;&#12290;LLM&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#29983;&#25104;&#22810;&#31181;&#36755;&#20986;&#65292;&#21253;&#25324;&#27880;&#37322;&#12289;&#20559;&#22909;&#12289;&#25351;&#20196;&#25552;&#31034;&#12289;&#27169;&#25311;&#23545;&#35805;&#21644;&#33258;&#30001;&#25991;&#26412;&#12290;&#30001;&#20110;&#36825;&#20123;LLM&#29983;&#25104;&#25968;&#25454;&#24418;&#24335;&#22312;&#24212;&#29992;&#20013;&#32463;&#24120;&#20132;&#21449;&#65292;&#23427;&#20204;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#35757;&#32451;&#24490;&#29615;&#20013;&#21512;&#24182;&#30340;&#20154;&#24037;&#25968;&#25454;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#37325;&#22823;&#20851;&#27880;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#20154;&#24037;&#25968;&#25454;&#29983;&#24577;&#31995;&#32479;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#27719;&#24635;&#36215;&#26469;&#65292;&#20174;&#26356;&#20005;&#26684;&#21463;&#38480;&#30340;&#25968;&#25454;&#22914;&#8220;&#20219;&#21153;&#26631;&#31614;&#8221;&#21040;&#26356;&#33258;&#30001;&#30340;&#8220;&#33258;&#30001;&#25991;&#26412;&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#21387;&#21147;&#27979;&#35797;&#65292;&#24182;&#19982;&#20154;&#24037;&#25968;&#25454;&#22312;&#21508;&#31181;&#29616;&#26377;&#22522;&#20934;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20154;&#24037;&#25968;&#25454;&#33021;&#22815;&#21305;&#37197;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#26412;&#25991;&#25581;&#31034;&#20102;&#38544;&#34255;&#30340;&#24040;&#22823;&#38544;&#24739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work delves into the expanding role of large language models (LLMs) in generating artificial data. LLMs are increasingly employed to create a variety of outputs, including annotations, preferences, instruction prompts, simulated dialogues, and free text. As these forms of LLM-generated data often intersect in their application, they exert mutual influence on each other and raise significant concerns about the quality and diversity of the artificial data incorporated into training cycles, leading to an artificial data ecosystem. To the best of our knowledge, this is the first study to aggregate various types of LLM-generated text data, from more tightly constrained data like "task labels" to more lightly constrained "free-form text". We then stress test the quality and implications of LLM-generated artificial data, comparing it with human data across various existing benchmarks. Despite artificial data's capability to match human performance, this paper reveals significant hidden d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#22312;&#22122;&#22768;&#23391;&#21152;&#25289;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#36866;&#29992;&#30340;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.14360</link><description>&lt;p&gt;
&#22122;&#22768;&#23391;&#21152;&#25289;&#35821;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#20013;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis on Noisy Bengali Texts. (arXiv:2401.14360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#22312;&#22122;&#22768;&#23391;&#21152;&#25289;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#36866;&#29992;&#30340;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23391;&#21152;&#25289;&#35821;&#34987;&#35748;&#20026;&#26159;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#65292;&#20294;&#24773;&#24863;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#25991;&#29486;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#22122;&#22768;&#23391;&#21152;&#25289;&#25991;&#26412;&#39046;&#22495;&#65292;&#23545;&#24773;&#24863;&#20998;&#26512;&#30340;&#25506;&#32034;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65288;NC-SentNoB&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#39044;&#23384;&#22312;&#30340;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#20013;&#22823;&#32422;15K&#20010;&#22122;&#22768;&#23391;&#21152;&#25289;&#25991;&#26412;&#20013;&#30340;&#21313;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#36755;&#20837;&#22122;&#22768;&#25991;&#26412;&#21010;&#20998;&#20026;&#22810;&#20010;&#26631;&#31614;&#26469;&#35782;&#21035;&#22122;&#22768;&#31867;&#22411;&#65292;&#28982;&#21518;&#24341;&#20837;&#22522;&#32447;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#26469;&#20943;&#23569;&#22122;&#22768;&#65292;&#20197;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#38024;&#23545;&#22122;&#22768;&#21644;&#20943;&#23569;&#22122;&#22768;&#25991;&#26412;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#20351;&#29992;&#30340;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#26356;&#36866;&#29992;&#30340;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Bengali is considered a language with limited resources, sentiment analysis has been a subject of extensive research in the literature. Nevertheless, there is a scarcity of exploration into sentiment analysis specifically in the realm of noisy Bengali texts. In this paper, we introduce a dataset (NC-SentNoB) that we annotated manually to identify ten different types of noise found in a pre-existing sentiment analysis dataset comprising of around 15K noisy Bengali texts. At first, given an input noisy text, we identify the noise type, addressing this as a multi-label classification task. Then, we introduce baseline noise reduction methods to alleviate noise prior to conducting sentiment analysis. Finally, we assess the performance of fine-tuned sentiment analysis models with both noisy and noise-reduced texts to make comparisons. The experimental findings indicate that the noise reduction methods utilized are not satisfactory, highlighting the need for more suitable noise reductio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.13802</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#20195;&#30721;&#29983;&#25104;&#12290;LLMs&#20027;&#35201;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;/&#23569;&#26679;&#26412;&#33539;&#24335;&#20013;&#34987;&#29992;&#20110;&#25351;&#23548;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#65288;CCD&#65289;&#36825;&#19968;&#38750;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#23478;&#26063;&#8212;&#8212;&#19978;&#19979;&#25991;&#35821;&#35328;&#23398;&#20064;&#65288;ICLL&#65289;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;&#12290;&#22312;ICLL&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#19982;&#32473;&#23450;&#24418;&#24335;&#35821;&#35328;&#30456;&#21516;&#30340;&#23383;&#31526;&#20018;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29702;&#35299;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20197;&#21450;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.12973</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#35821;&#35328;&#23398;&#20064;&#65306;&#26550;&#26500;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Language Learning: Architectures and Algorithms. (arXiv:2401.12973v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#23478;&#26063;&#8212;&#8212;&#19978;&#19979;&#25991;&#35821;&#35328;&#23398;&#20064;&#65288;ICLL&#65289;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;&#12290;&#22312;ICLL&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#19982;&#32473;&#23450;&#24418;&#24335;&#35821;&#35328;&#30456;&#21516;&#30340;&#23383;&#31526;&#20018;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29702;&#35299;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20197;&#21450;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20196;&#20154;&#24778;&#21497;&#30340;&#33021;&#21147;&#65306;&#23427;&#20204;&#33021;&#22815;&#20174;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#26032;&#30340;&#20989;&#25968;&#12290;&#30446;&#21069;&#65292;&#25105;&#20204;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#21457;&#29983;&#30340;&#20102;&#35299;&#20027;&#35201;&#26469;&#33258;&#20110;&#22312;&#26497;&#20854;&#31616;&#21333;&#30340;&#23398;&#20064;&#38382;&#39064;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;&#32447;&#24615;&#22238;&#24402;&#21644;&#20851;&#32852;&#35760;&#24518;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#38382;&#39064;&#19982;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#30340;&#8220;&#30495;&#27491;&#8221;&#19978;&#19979;&#25991;&#23398;&#20064;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#21518;&#32773;&#19981;&#20165;&#28041;&#21450;&#26816;&#32034;&#21644;&#20989;&#25968;&#36817;&#20284;&#65292;&#36824;&#21253;&#25324;&#20102;&#33258;&#30001;&#29983;&#25104;&#35821;&#35328;&#21644;&#20854;&#20182;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#19968;&#20010;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#35821;&#35328;&#23398;&#20064;&#65288;ICLL&#65289;&#30340;&#26032;&#22411;&#38382;&#39064;&#23478;&#26063;&#65292;&#26469;&#25506;&#35752;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#22312;ICLL&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#34987;&#21576;&#29616;&#19968;&#32452;&#26469;&#33258;&#24418;&#24335;&#35821;&#35328;&#30340;&#23383;&#31526;&#20018;&#65292;&#24182;&#38656;&#35201;&#29983;&#25104;&#19982;&#35813;&#35821;&#35328;&#30456;&#21516;&#30340;&#20854;&#20182;&#23383;&#31526;&#20018;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#36890;&#36807;&#38543;&#26426;&#26377;&#38480;&#33258;&#21160;&#26426;&#29983;&#25104;&#30340;&#27491;&#21017;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#31181;&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#65288;&#21253;&#25324;&#20960;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale neural language models exhibit a remarkable capacity for in-context learning (ICL): they can infer novel functions from datasets provided as input. Most of our current understanding of when and how ICL arises comes from LMs trained on extremely simple learning problems like linear regression and associative recall. There remains a significant gap between these model problems and the "real" ICL exhibited by LMs trained on large text corpora, which involves not just retrieval and function approximation but free-form generation of language and other structured outputs. In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL). In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language. We focus on in-context learning of regular languages generated by random finite automata. We evaluate a diverse set of neural sequence models (including seve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SLANG&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#20114;&#32852;&#32593;&#19978;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#20934;&#26041;&#27861;FOCUS&#65292;&#33021;&#24110;&#21161;LLMs&#26356;&#22909;&#22320;&#29702;&#35299;&#26032;&#30340;&#30701;&#35821;&#21644;&#29992;&#27861;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.12585</link><description>&lt;p&gt;
SLANG: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
SLANG: New Concept Comprehension of Large Language Models. (arXiv:2401.12585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SLANG&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#20114;&#32852;&#32593;&#19978;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#20934;&#26041;&#27861;FOCUS&#65292;&#33021;&#24110;&#21161;LLMs&#26356;&#22909;&#22320;&#29702;&#35299;&#26032;&#30340;&#30701;&#35821;&#21644;&#29992;&#27861;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#30340;&#21160;&#24577;&#24615;&#65292;&#23588;&#20854;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#20442;&#35821;&#21644;&#34920;&#24773;&#21253;&#31561;&#26041;&#38754;&#30340;&#20307;&#29616;&#65292;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36866;&#24212;&#24615;&#24102;&#26469;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20165;&#32465;&#23450;&#22312;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#65292;&#24456;&#38590;&#36319;&#19978;&#22312;&#32447;&#31038;&#21306;&#20013;&#24555;&#36895;&#35821;&#35328;&#36827;&#21270;&#30340;&#27493;&#20240;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#23545;&#20114;&#32852;&#32593;&#19978;&#26032;&#27010;&#24565;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;&#36991;&#20813;&#39640;&#25104;&#26412;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#25345;&#32493;&#37325;&#35757;&#32451;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;LLMs&#22312;&#29702;&#35299;&#26032;&#20852;&#35821;&#35328;&#36235;&#21183;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934; - SLANG&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#20934;&#26041;&#27861; FOCUS&#65292;&#23427;&#33021;&#22686;&#24378;LLMs&#23545;&#26032;&#30340;&#30701;&#35821;&#21644;&#29992;&#27861;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#35821;&#35328;&#36716;&#21464;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#20363;&#36827;&#34892;&#35814;&#32454;&#30740;&#31350;&#65292;&#20316;&#20026;&#32972;&#26223;&#20381;&#25454;&#65292;&#20197;&#24418;&#25104;&#26356;&#31934;&#30830;&#21644;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#26032;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research addresses the critical need to bridge this gap, aiming to enhance LLMs' comprehension of evolving new concepts on the internet, without the high cost and impracticality of continual retraining. To address this issue, we propose a new benchmark $\textbf{SLANG}$ to assess LLMs' proficiency in comprehending emerging linguistic trends and a baseline approach $\textbf{FOCUS}$, which uses causal inference to enhance LLMs to understand new phrases and usage patterns. This approach involves scrutinizing real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly em
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#26469;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#12290;LSE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#39046;&#22495;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;LSEd&#12290;&#23454;&#39564;&#35777;&#26126;LSEd&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10893</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Location Sensitive Embedding for Knowledge Graph Embedding. (arXiv:2401.10893v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#26469;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#12290;LSE&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#39046;&#22495;&#20855;&#26377;&#29702;&#35770;&#22522;&#30784;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;&#21464;&#20307;LSEd&#12290;&#23454;&#39564;&#35777;&#26126;LSEd&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#23558;&#30693;&#35782;&#22270;&#35889;&#36716;&#21270;&#20026;&#36830;&#32493;&#30340;&#12289;&#20302;&#32500;&#24230;&#30340;&#31354;&#38388;&#65292;&#26377;&#21161;&#20110;&#25512;&#29702;&#21644;&#34917;&#20840;&#20219;&#21153;&#12290;&#35813;&#39046;&#22495;&#20027;&#35201;&#20998;&#20026;&#20256;&#32479;&#30340;&#36317;&#31163;&#27169;&#22411;&#21644;&#35821;&#20041;&#21305;&#37197;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#36317;&#31163;&#27169;&#22411;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#26080;&#27861;&#26377;&#25928;&#21306;&#20998;&#22270;&#35889;&#20013;&#30340;&#8220;&#22836;&#23454;&#20307;&#8221;&#21644;&#8220;&#23614;&#23454;&#20307;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20301;&#32622;&#25935;&#24863;&#23884;&#20837;&#65288;LSE&#65289;&#26041;&#27861;&#12290;LSE&#36890;&#36807;&#20851;&#31995;&#29305;&#23450;&#30340;&#26144;&#23556;&#20462;&#25913;&#22836;&#23454;&#20307;&#65292;&#23558;&#20851;&#31995;&#27010;&#24565;&#21270;&#20026;&#32447;&#24615;&#21464;&#25442;&#32780;&#19981;&#20165;&#20165;&#26159;&#24179;&#31227;&#12290;LSE&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#21253;&#25324;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;&#19982;&#29616;&#26377;&#27169;&#22411;&#30340;&#32852;&#31995;&#65292;&#37117;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#19968;&#31181;&#26356;&#31616;&#21270;&#30340;&#21464;&#20307;LSEd&#21033;&#29992;&#23545;&#35282;&#30697;&#38453;&#36827;&#34892;&#21464;&#25442;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#33021;&#12290;&#22312;&#23545;&#22235;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#27979;&#35797;&#20013;&#65292;LSEd&#35201;&#20040;&#34920;&#29616;&#26356;&#22909;&#65292;&#35201;&#20040;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding transforms knowledge graphs into a continuous, low-dimensional space, facilitating inference and completion tasks. This field is mainly divided into translational distance models and semantic matching models. A key challenge in translational distance models is their inability to effectively differentiate between 'head' and 'tail' entities in graphs. To address this, the novel location-sensitive embedding (LSE) method has been developed. LSE innovatively modifies the head entity using relation-specific mappings, conceptualizing relations as linear transformations rather than mere translations. The theoretical foundations of LSE, including its representational capabilities and its connections to existing models, have been thoroughly examined. A more streamlined variant, LSEd, employs a diagonal matrix for transformations to enhance practical efficiency. In tests conducted on four large-scale datasets for link prediction, LSEd either outperforms or is competitive
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10337</link><description>&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#26159;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#22312;&#25991;&#26412;&#30693;&#35782;&#24211;&#20013;&#26377;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20889;&#20316;&#20013;&#35782;&#21035;TTPs&#65292;&#36890;&#24120;&#31216;&#20026;TTP&#26144;&#23556;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20197;&#32463;&#20856;&#30340;&#22810;&#31867;&#25110;&#22810;&#26631;&#31614;&#20998;&#31867;&#35774;&#32622;&#20026;&#30446;&#26631;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#31867;&#21035;&#65288;&#21363;TTPs&#65289;&#65292;&#26631;&#31614;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#22797;&#26434;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#31181;&#35774;&#32622;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#23558;&#25991;&#26412;&#19982;TTP&#26631;&#31614;&#20043;&#38388;&#30340;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#20026;&#25991;&#26412;&#20998;&#37197;&#32473;TTP&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20165;&#20165;&#22312;&#22823;&#22411;&#26631;&#31614;&#31354;&#38388;&#19978;&#31454;&#20105;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#25928;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#27604;&#36739;&#26426;&#21046;&#30340;&#31070;&#32463;&#21305;&#37197;&#26550;&#26500;&#65292;&#20419;&#36827;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20302;&#24310;&#36831;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26041;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#30005;&#23376;&#21830;&#21153;&#39038;&#23458;&#30340;&#28040;&#24687;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30005;&#23376;&#21830;&#21153;&#20080;&#21334;&#21452;&#26041;&#22312;&#32447;&#28040;&#24687;&#20013;&#30340;&#21363;&#26102;&#22238;&#31572;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38382;&#39064;&#29702;&#35299;&#21644;&#22238;&#31572;&#29575;&#26041;&#38754;&#30456;&#23545;&#22686;&#21152;&#20102;&#24456;&#22810;&#65292;&#23545;&#20110;&#25552;&#39640;&#39038;&#23458;&#30340;&#36141;&#29289;&#20307;&#39564;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.09785</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20080;&#21334;&#21452;&#26041;&#22312;&#32447;&#28040;&#24687;&#20013;&#30340;&#21363;&#26102;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Instant Answering in E-Commerce Buyer-Seller Messaging. (arXiv:2401.09785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09785
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20302;&#24310;&#36831;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26041;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#30005;&#23376;&#21830;&#21153;&#39038;&#23458;&#30340;&#28040;&#24687;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30005;&#23376;&#21830;&#21153;&#20080;&#21334;&#21452;&#26041;&#22312;&#32447;&#28040;&#24687;&#20013;&#30340;&#21363;&#26102;&#22238;&#31572;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38382;&#39064;&#29702;&#35299;&#21644;&#22238;&#31572;&#29575;&#26041;&#38754;&#30456;&#23545;&#22686;&#21152;&#20102;&#24456;&#22810;&#65292;&#23545;&#20110;&#25552;&#39640;&#39038;&#23458;&#30340;&#36141;&#29289;&#20307;&#39564;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#39038;&#23458;&#32463;&#24120;&#23547;&#27714;&#35814;&#32454;&#30340;&#20135;&#21697;&#20449;&#24687;&#20197;&#20570;&#20986;&#36141;&#20080;&#20915;&#31574;&#65292;&#36890;&#24120;&#36890;&#36807;&#30452;&#25509;&#21521;&#21334;&#23478;&#21457;&#36865;&#25193;&#23637;&#26597;&#35810;&#26469;&#32852;&#31995;&#12290;&#36825;&#31181;&#25163;&#21160;&#22238;&#22797;&#35201;&#27714;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#24182;&#19988;&#22312;&#21709;&#24212;&#26102;&#38388;&#27874;&#21160;&#33539;&#22260;&#20174;&#20960;&#23567;&#26102;&#21040;&#20960;&#22825;&#26102;&#24178;&#25200;&#20102;&#39038;&#23458;&#30340;&#36141;&#29289;&#20307;&#39564;&#12290;&#25105;&#20204;&#26088;&#22312;&#20351;&#29992;&#39046;&#20808;&#30340;&#30005;&#23376;&#21830;&#21153;&#21830;&#24215;&#20013;&#30340;&#29305;&#23450;&#39046;&#22495;&#32852;&#21512;&#38382;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#33258;&#21160;&#22788;&#29702;&#39038;&#23458;&#23545;&#21334;&#23478;&#30340;&#35810;&#38382;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#23558;&#24403;&#21069;&#20026;&#21333;&#20010;&#38382;&#39064;&#35774;&#35745;&#30340;QA&#31995;&#32479;&#35843;&#25972;&#20026;&#35299;&#20915;&#35814;&#32454;&#30340;&#39038;&#23458;&#26597;&#35810;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#20302;&#24310;&#36831;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26041;&#27861;&#8212;&#8212;MESSAGE-TO-QUESTION&#65288;M2Q&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#20174;&#28040;&#24687;&#20013;&#35782;&#21035;&#21644;&#25552;&#21462;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#26469;&#23558;&#20080;&#23478;&#28040;&#24687;&#37325;&#26032;&#26500;&#24314;&#25104;&#31616;&#27905;&#30340;&#38382;&#39064;&#12290;&#19982;&#22522;&#32447;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;M2Q&#22312;&#38382;&#39064;&#29702;&#35299;&#26041;&#38754;&#30456;&#23545;&#22686;&#21152;&#20102;757%&#65292;&#22312;&#32852;&#21512;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#22238;&#31572;&#29575;&#22686;&#21152;&#20102;1746%&#12290;&#23454;&#38469;&#37096;&#32626;&#34920;&#26126;&#65292;&#33258;&#21160;&#21270;&#22238;&#31572;&#31995;&#32479;&#21487;&#20197;&#20197;&#19968;&#20010;&#24179;&#22343;&#30340;&#22238;&#31572;&#36895;&#29575;&#24555;4.67&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce customers frequently seek detailed product information for purchase decisions, commonly contacting sellers directly with extended queries. This manual response requirement imposes additional costs and disrupts buyer's shopping experience with response time fluctuations ranging from hours to days. We seek to automate buyer inquiries to sellers in a leading e-commerce store using a domain-specific federated Question Answering (QA) system. The main challenge is adapting current QA systems, designed for single questions, to address detailed customer queries. We address this with a low-latency, sequence-to-sequence approach, MESSAGE-TO-QUESTION ( M2Q ). It reformulates buyer messages into succinct questions by identifying and extracting the most salient information from a message. Evaluation against baselines shows that M2Q yields relative increases of 757% in question understanding, and 1,746% in answering rate from the federated QA system. Live deployment shows that automatic a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#24494;&#35843;&#20004;&#31181;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#31649;&#36947;&#21644;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.08406</link><description>&lt;p&gt;
RAG vs Fine-tuning: &#31649;&#36947;&#65292;&#26435;&#34913;&#20197;&#21450;&#22312;&#20892;&#19994;&#19978;&#30340;&#20010;&#26696;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. (arXiv:2401.08406v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#24494;&#35843;&#20004;&#31181;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#31649;&#36947;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#31243;&#24207;&#26102;&#65292;&#24320;&#21457;&#32773;&#36890;&#24120;&#26377;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#26469;&#25972;&#21512;&#19987;&#26377;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#65306;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#24494;&#35843;&#12290;RAG&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#22686;&#24378;&#25552;&#31034;&#20449;&#24687;&#65292;&#32780;&#24494;&#35843;&#21017;&#23558;&#38468;&#21152;&#30693;&#35782;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#24182;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24494;&#35843;&#21644;RAG&#30340;&#31649;&#36947;&#65292;&#24182;&#23545;&#22810;&#31181;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;Llama2-13B&#65292;GPT-3.5&#21644;GPT-4&#65289;&#36827;&#34892;&#20102;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#30001;&#22810;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#21253;&#25324;&#20174;PDF&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#29983;&#25104;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#65292;&#24182;&#21033;&#29992;GPT-4&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;RAG&#21644;&#24494;&#35843;&#31649;&#36947;&#19981;&#21516;&#38454;&#27573;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#23545;&#20892;&#19994;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#20316;&#20026;&#19968;&#20010;&#20135;&#19994;&#65292;&#20892;&#19994;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#26041;&#38754;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22823;&#30340;&#28183;&#36879;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, an
&lt;/p&gt;</description></item><item><title>LEGO&#26159;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#29702;&#35299;&#21644;&#31934;&#30830;&#30340;&#26631;&#35782;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06071</link><description>&lt;p&gt;
LEGO: &#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LEGO:Language Enhanced Multi-modal Grounding Model. (arXiv:2401.06071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06071
&lt;/p&gt;
&lt;p&gt;
LEGO&#26159;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#29702;&#35299;&#21644;&#31934;&#30830;&#30340;&#26631;&#35782;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#27169;&#24577;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20027;&#35201;&#24378;&#35843;&#25429;&#25417;&#27599;&#31181;&#27169;&#24577;&#20869;&#30340;&#20840;&#23616;&#20449;&#24687;&#65292;&#32780;&#24573;&#35270;&#20102;&#36328;&#27169;&#24577;&#24863;&#30693;&#23616;&#37096;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#29702;&#35299;&#36755;&#20837;&#25968;&#25454;&#32454;&#31890;&#24230;&#32454;&#33410;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#26356;&#32454;&#33268;&#29702;&#35299;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#22312;&#22810;&#20010;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32454;&#31890;&#24230;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEGO&#65292;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;&#12290;&#38500;&#20102;&#20687;&#20854;&#20182;&#22810;&#27169;&#24577;&#27169;&#22411;&#19968;&#26679;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38656;&#35201;&#35814;&#32454;&#29702;&#35299;&#36755;&#20837;&#20869;&#30340;&#23616;&#37096;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#23637;&#31034;&#20102;&#31934;&#30830;&#30340;&#26631;&#35782;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose LEGO, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification 
&lt;/p&gt;</description></item><item><title>TwinBooster&#32467;&#21512;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04478</link><description>&lt;p&gt;
TwinBooster: &#32467;&#21512;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#22686;&#24378;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction. (arXiv:2401.04478v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04478
&lt;/p&gt;
&lt;p&gt;
TwinBooster&#32467;&#21512;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#21644;&#24320;&#21457;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#23545;&#20998;&#23376;&#27963;&#24615;&#21644;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#35745;&#31639;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#20351;&#29992;&#36804;&#20170;&#20026;&#27490;&#20165;&#38480;&#20110;&#22823;&#37327;&#25968;&#25454;&#21487;&#29992;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;Siamese&#31070;&#32463;&#32593;&#32476;Barlow Twins&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#26816;&#27979;&#26041;&#27861;&#20449;&#24687;&#21644;&#20998;&#23376;&#25351;&#32441;&#25552;&#21462;&#30495;&#23454;&#30340;&#20998;&#23376;&#20449;&#24687;&#12290;TwinBooster&#36890;&#36807;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#30340;&#23646;&#24615;&#39044;&#27979;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#27969;&#27700;&#32447;&#22312;FS-Mol&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#31361;&#30772;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36890;&#24120;&#25968;&#25454;&#31232;&#32570;&#30340;&#20851;&#38190;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of drug discovery and development relies on the precise prediction of molecular activities and properties. While in silico molecular property prediction has shown remarkable potential, its use has been limited so far to assays for which large amounts of data are available. In this study, we use a fine-tuned large language model to integrate biological assays based on their textual information, coupled with Barlow Twins, a Siamese neural network using a novel self-supervised learning approach. This architecture uses both assay information and molecular fingerprints to extract the true molecular information. TwinBooster enables the prediction of properties of unseen bioassays and molecules by providing state-of-the-art zero-shot learning tasks. Remarkably, our artificial intelligence pipeline shows excellent performance on the FS-Mol benchmark. This breakthrough demonstrates the application of deep learning to critical property prediction tasks where data is typically scarce.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RAISE&#30340;&#26550;&#26500;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#25972;&#21512;&#21040;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#32452;&#20214;&#35760;&#24518;&#31995;&#32479;&#26469;&#22686;&#24378;&#20195;&#29702;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21487;&#25511;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#39044;liminary evaluations&#34920;&#26126;&#65292;RAISE&#22312;&#25151;&#22320;&#20135;&#38144;&#21806;&#39046;&#22495;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.02777</link><description>&lt;p&gt;
&#20174;LLM&#21040;&#23545;&#35805;&#20195;&#29702;&#65306;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#35760;&#24518;&#22686;&#24378;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. (arXiv:2401.02777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RAISE&#30340;&#26550;&#26500;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#25972;&#21512;&#21040;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#32452;&#20214;&#35760;&#24518;&#31995;&#32479;&#26469;&#22686;&#24378;&#20195;&#29702;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21487;&#25511;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#39044;liminary evaluations&#34920;&#26126;&#65292;RAISE&#22312;&#25151;&#22320;&#20135;&#38144;&#21806;&#39046;&#22495;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RAISE&#65288;Scratchpad&#21644;Examples&#36741;&#21161;&#25512;&#29702;&#21644;&#34892;&#20026;&#65289;,&#19968;&#31181;&#20808;&#36827;&#30340;&#26550;&#26500;&#65292;&#22686;&#24378;&#20102;&#23558;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#23545;&#35805;&#20195;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;RAISE&#26159;ReAct&#26694;&#26550;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#21253;&#25324;&#19968;&#20010;&#21452;&#32452;&#20214;&#35760;&#24518;&#31995;&#32479;&#65292;&#27169;&#20223;&#20154;&#31867;&#30340;&#30701;&#26399;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#20445;&#25345;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#21644;&#36830;&#32493;&#24615;&#12290;&#23427;&#21253;&#25324;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20195;&#29702;&#26500;&#24314;&#24773;&#26223;&#65292;&#21253;&#25324;&#23545;&#35805;&#36873;&#25321;&#65292;&#22330;&#26223;&#25552;&#21462;&#65292;CoT&#23436;&#25104;&#21644;&#22330;&#26223;&#22686;&#24378;&#31561;&#38454;&#27573;&#65292;&#26368;&#32456;&#23548;&#33268;LLMs&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;&#36825;&#31181;&#26041;&#27861;&#20284;&#20046;&#25552;&#39640;&#20102;&#20195;&#29702;&#22312;&#22797;&#26434;&#30340;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21487;&#25511;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#22312;&#25151;&#22320;&#20135;&#38144;&#21806;&#29615;&#22659;&#20013;&#30340;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#65292;RAISE&#30456;&#23545;&#20110;&#20256;&#32479;&#20195;&#29702;&#26377;&#19968;&#20123;&#20248;&#21183;&#65292;&#34920;&#26126;&#23427;&#22312;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#26469;&#24320;&#21457;&#26356;&#20855;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#22810;&#21151;&#33021;&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;AI&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile convers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21628;&#21505;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#26469;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20197;&#35299;&#20915;&#21333;&#25552;&#31034;&#35780;&#20272;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#24403;&#21069;LLMs&#30495;&#27491;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.00595</link><description>&lt;p&gt;
&#29366;&#24577;&#26159;&#20160;&#20040;&#33402;&#26415;&#65311;&#22810;&#25552;&#31034;LLM&#35780;&#20272;&#30340;&#21628;&#21505;&#12290;
&lt;/p&gt;
&lt;p&gt;
State of What Art? A Call for Multi-Prompt LLM Evaluation. (arXiv:2401.00595v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21628;&#21505;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#26469;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20197;&#35299;&#20915;&#21333;&#25552;&#31034;&#35780;&#20272;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#24403;&#21069;LLMs&#30495;&#27491;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#20381;&#36182;&#20110;&#21333;&#20010;&#25351;&#20196;&#27169;&#26495;&#26469;&#35780;&#20272;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25152;&#26377;LLMs&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#36890;&#36807;&#21333;&#25552;&#31034;&#35780;&#20272;&#33719;&#24471;&#30340;&#32467;&#26524;&#30340;&#33030;&#24369;&#24615;&#65292;&#32435;&#20837;&#20102;6.5M&#20010;&#23454;&#20363;&#65292;&#28041;&#21450;20&#31181;&#19981;&#21516;&#30340;LLMs&#21644;&#26469;&#33258;3&#20010;&#22522;&#20934;&#30340;39&#20010;&#20219;&#21153;&#12290;&#20026;&#20102;&#25913;&#36827;&#20998;&#26512;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#29305;&#23450;&#29992;&#20363;&#65288;&#20363;&#22914;LLM&#24320;&#21457;&#20154;&#21592;&#19982;&#23545;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#24863;&#20852;&#36259;&#30340;&#24320;&#21457;&#20154;&#21592;&#65289;&#30340;&#23450;&#21046;&#35780;&#20272;&#25351;&#26631;&#65292;&#30830;&#20445;&#26356;&#21487;&#38752;&#21644;&#26377;&#24847;&#20041;&#30340;LLM&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23454;&#26045;&#36825;&#20123;&#26631;&#20934;&#65292;&#24182;&#23545;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#24403;&#21069;LLMs&#30495;&#27491;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have led to the development of various evaluation benchmarks. These benchmarks typically rely on a single instruction template for evaluating all LLMs on a specific task. In this paper, we comprehensively analyze the brittleness of results obtained via single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. To improve robustness of the analysis, we propose to evaluate LLMs with a set of diverse prompts instead. We discuss tailored evaluation metrics for specific use cases (e.g., LLM developers vs. developers interested in a specific downstream task), ensuring a more reliable and meaningful assessment of LLM capabilities. We then implement these criteria and conduct evaluations of multiple models, providing insights into the true strengths and limitations of current LLMs.
&lt;/p&gt;</description></item><item><title>Auto311&#26159;&#31532;&#19968;&#20010;&#22788;&#29702;&#38750;&#32039;&#24613;&#30005;&#35805;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#20943;&#36731;&#38750;&#32039;&#24613;&#30005;&#35805;&#36127;&#25285;&#65292;&#25552;&#20379;&#24555;&#36895;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#39044;&#27979;&#20107;&#20214;&#31867;&#22411;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#26696;&#20214;&#25253;&#21578;&#65292;&#24182;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#26469;&#23436;&#21892;&#25253;&#21578;&#65292;&#31995;&#32479;&#19982;&#20027;&#21483;&#20154;&#20043;&#38388;&#30340;&#23545;&#35805;&#32467;&#26500;&#24471;&#21040;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2312.14185</link><description>&lt;p&gt;
Auto311: &#19968;&#31181;&#22522;&#20110;&#20449;&#24515;&#25351;&#23548;&#30340;&#33258;&#21160;&#38750;&#32039;&#24613;&#36890;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Auto311: A Confidence-guided Automated System for Non-emergency Calls. (arXiv:2312.14185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14185
&lt;/p&gt;
&lt;p&gt;
Auto311&#26159;&#31532;&#19968;&#20010;&#22788;&#29702;&#38750;&#32039;&#24613;&#30005;&#35805;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#20943;&#36731;&#38750;&#32039;&#24613;&#30005;&#35805;&#36127;&#25285;&#65292;&#25552;&#20379;&#24555;&#36895;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#39044;&#27979;&#20107;&#20214;&#31867;&#22411;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#26696;&#20214;&#25253;&#21578;&#65292;&#24182;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#26469;&#23436;&#21892;&#25253;&#21578;&#65292;&#31995;&#32479;&#19982;&#20027;&#21483;&#20154;&#20043;&#38388;&#30340;&#23545;&#35805;&#32467;&#26500;&#24471;&#21040;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#24613;&#21644;&#38750;&#32039;&#24613;&#21709;&#24212;&#31995;&#32479;&#26159;&#22320;&#26041;&#25919;&#24220;&#25552;&#20379;&#30340;&#22522;&#26412;&#26381;&#21153;&#65292;&#23545;&#20110;&#20445;&#25252;&#29983;&#21629;&#12289;&#29615;&#22659;&#21644;&#36130;&#20135;&#33267;&#20851;&#37325;&#35201;&#12290;&#26377;&#25928;&#22788;&#29702;&#65288;&#38750;&#65289;&#32039;&#24613;&#30005;&#35805;&#23545;&#20844;&#20849;&#23433;&#20840;&#21644;&#31119;&#31049;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20943;&#36731;&#38750;&#32039;&#24613;&#30005;&#35805;&#30340;&#36127;&#25285;&#65292;&#20127;&#38656;911&#27714;&#21161;&#30340;&#23621;&#27665;&#23558;&#33719;&#24471;&#24555;&#36895;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#19982;&#32435;&#20160;&#32500;&#23572;&#32039;&#24613;&#36890;&#20449;&#37096;&#38376;&#21512;&#20316;&#65292;&#20998;&#26512;&#20102;11,796&#20010;&#38750;&#32039;&#24613;&#21628;&#21483;&#24405;&#38899;&#65292;&#24182;&#24320;&#21457;&#20102;Auto311&#65292;&#31532;&#19968;&#20010;&#22788;&#29702;311&#38750;&#32039;&#24613;&#21628;&#21483;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#65288;1&#65289;&#26377;&#25928;&#21160;&#24577;&#22320;&#39044;&#27979;&#27491;&#22312;&#36827;&#34892;&#30340;&#38750;&#32039;&#24613;&#20107;&#20214;&#31867;&#22411;&#65292;&#20197;&#22312;&#36890;&#35805;&#36807;&#31243;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#26696;&#20214;&#25253;&#21578;&#65307;&#65288;2&#65289;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#23436;&#25104;&#29983;&#25104;&#30340;&#25253;&#21578;&#65307;&#65288;3&#65289;&#20197;&#20248;&#21270;&#30340;&#20449;&#24515;&#27700;&#24179;&#23433;&#25490;&#31995;&#32479;&#21644;&#20027;&#21483;&#20154;&#20043;&#38388;&#30340;&#23545;&#35805;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#35780;&#20272;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emergency and non-emergency response systems are essential services provided by local governments and critical to protecting lives, the environment, and property. The effective handling of (non-)emergency calls is critical for public safety and well-being. By reducing the burden through non-emergency callers, residents in critical need of assistance through 911 will receive a fast and effective response. Collaborating with the Department of Emergency Communications (DEC) in Nashville, we analyzed 11,796 non-emergency call recordings and developed Auto311, the first automated system to handle 311 non-emergency calls, which (1) effectively and dynamically predicts ongoing non-emergency incident types to generate tailored case reports during the call; (2) itemizes essential information from dialogue contexts to complete the generated reports; and (3) strategically structures system-caller dialogues with optimized confidence. We used real-world data to evaluate the system's effectiveness a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#23450;&#20301;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#25214;&#21040;&#20102;&#23384;&#20648;&#20102;&#26377;&#20851;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#30340;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2312.12141</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23450;&#20301;&#20107;&#23454;&#30693;&#35782;&#65306;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space. (arXiv:2312.12141v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#23450;&#20301;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#25214;&#21040;&#20102;&#23384;&#20648;&#20102;&#26377;&#20851;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#30340;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#26102;&#65292;&#23376;&#20540;&#20855;&#26377;&#21487;&#20154;&#31867;&#35299;&#37322;&#30340;&#27010;&#24565;&#30340;&#21407;&#22240;&#12290;&#23376;&#20540;&#30340;softmax&#20043;&#21069;&#30340;&#20540;&#36890;&#36807;&#19968;&#20010;&#21152;&#27861;&#20989;&#25968;&#30456;&#21152;&#65292;&#22240;&#27492;&#35789;&#27719;&#31354;&#38388;&#20013;&#21069;&#20960;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#20250;&#22686;&#21152;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#23545;&#25968;&#27010;&#29575;&#22686;&#21152;&#26469;&#35745;&#31639;&#23618;&#21644;&#23376;&#20540;&#30340;&#37325;&#35201;&#24615;&#27604;&#27010;&#29575;&#22686;&#21152;&#26356;&#22909;&#65292;&#22240;&#20026;&#23545;&#25968;&#27010;&#29575;&#22686;&#21152;&#30340;&#26354;&#32447;&#21576;&#32447;&#24615;&#21333;&#35843;&#22686;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35745;&#31639;&#20869;&#31215;&#26469;&#35780;&#20272;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#30340;&#23376;&#20540;&#34987;&#21069;&#38754;&#30340;&#23618;&#28608;&#27963;&#30340;&#31243;&#24230;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20107;&#23454;&#30693;&#35782;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#23384;&#20648;&#30340;&#20301;&#32622;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27880;&#24847;&#21147;&#23618;&#23384;&#20648;&#8220;&#24052;&#40654;&#19982;&#27861;&#22269;&#30456;&#20851;&#8221;&#12290;FFN&#23618;&#23384;&#20648;&#8220;&#24052;&#40654;&#26159;&#19968;&#20010;&#39318;&#37117;/&#22478;&#24066;&#8221;&#65292;&#30001;&#27880;&#24847;&#21147;&#23376;&#20540;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
We find the location of factual knowledge in large language models by exploring the residual stream and analyzing subvalues in vocabulary space. We find the reason why subvalues have human-interpretable concepts when projecting into vocabulary space. The before-softmax values of subvalues are added by an addition function, thus the probability of top tokens in vocabulary space will increase. Based on this, we find using log probability increase to compute the significance of layers and subvalues is better than probability increase, since the curve of log probability increase has a linear monotonically increasing shape. Moreover, we calculate the inner products to evaluate how much a feed-forward network (FFN) subvalue is activated by previous layers. Base on our methods, we find where factual knowledge &lt;France, capital, Paris&gt; is stored. Specifically, attention layers store "Paris is related to France". FFN layers store "Paris is a capital/city", activated by attention subvalues relate
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#35328;&#35821;&#19981;&#27969;&#30021;&#31243;&#24230;&#33258;&#21160;&#35843;&#25972;&#33647;&#29289;&#65292;&#36890;&#36807;&#23545;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.11509</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33647;&#29289;&#35843;&#25972;&#31995;&#32479;&#20197;&#20943;&#23569;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11509
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#35328;&#35821;&#19981;&#27969;&#30021;&#31243;&#24230;&#33258;&#21160;&#35843;&#25972;&#33647;&#29289;&#65292;&#36890;&#36807;&#23545;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#20026;&#24739;&#26377;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#34394;&#25311;&#24739;&#32773;&#24320;&#20855;&#33647;&#29289;&#22788;&#26041;&#65292;&#24182;&#26681;&#25454;&#38646;&#25104;&#26412;&#39057;&#32321;&#27979;&#37327;&#32467;&#26524;&#65292;&#35843;&#25972;&#33647;&#29289;&#21644;&#21058;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31995;&#32479;&#30340;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#22312;&#25105;&#20204;&#26500;&#24314;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#21644;&#35780;&#20272;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#33391;&#22909;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20004;&#20010;&#27169;&#22359;&#65292;&#25105;&#20204;&#20174;&#25991;&#29486;&#20013;&#25910;&#38598;&#20102;&#20851;&#20110;&#33647;&#29289;&#27835;&#30103;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#25928;&#26524;&#30340;&#25968;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#20449;&#30340;&#24739;&#32773;&#27169;&#25311;&#31995;&#32479;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#23545;&#21487;&#33021;&#23384;&#22312;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#20154;&#32676;&#36827;&#34892;&#20102;&#25968;&#25454;&#26631;&#27880;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;:
&lt;/p&gt;
&lt;p&gt;
We propose a Reinforcement-Learning-based system that would automatically prescribe a hypothetical patient medication that may help the patient with their mental-health-related speech disfluency, and adjust the medication and the dosages in response to zero-cost frequent measurement of the fluency of the patient. We demonstrate the components of the system: a module that detects and evaluates speech disfluency on a large dataset we built, and a Reinforcement Learning algorithm that automatically finds good combinations of medications. To support the two modules, we collect data on the effect of psychiatric medications for speech disfluency from the literature, and build a plausible patient simulation system. We demonstrate that the Reinforcement Learning system is, under some circumstances, able to converge to a good medication regime. We collect and label a dataset of people with possible speech disfluency and demonstrate our methods using that dataset. Our work is a proof of concept:
&lt;/p&gt;</description></item><item><title>SEF-VC&#26159;&#19968;&#31181;&#26080;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#21442;&#32771;&#35821;&#38899;&#20013;&#23398;&#20064;&#24182;&#34701;&#20837;&#35828;&#35805;&#20154;&#38899;&#33394;&#65292;&#20855;&#26377;&#31283;&#23450;&#30340;&#35757;&#32451;&#21644;&#20248;&#36234;&#30340;&#35821;&#38899;&#36716;&#25442;&#24615;&#33021;&#12290;&#19982;&#24378;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35821;&#38899;&#30340;&#21516;&#26102;&#33021;&#22815;&#26356;&#22909;&#22320;&#20445;&#25345;&#19982;&#30446;&#26631;&#21442;&#32771;&#30340;&#30456;&#20284;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#24456;&#30701;&#30340;&#21442;&#32771;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2312.08676</link><description>&lt;p&gt;
SEF-VC: &#26080;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#19982;&#20132;&#21449;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention. (arXiv:2312.08676v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08676
&lt;/p&gt;
&lt;p&gt;
SEF-VC&#26159;&#19968;&#31181;&#26080;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#21442;&#32771;&#35821;&#38899;&#20013;&#23398;&#20064;&#24182;&#34701;&#20837;&#35828;&#35805;&#20154;&#38899;&#33394;&#65292;&#20855;&#26377;&#31283;&#23450;&#30340;&#35757;&#32451;&#21644;&#20248;&#36234;&#30340;&#35821;&#38899;&#36716;&#25442;&#24615;&#33021;&#12290;&#19982;&#24378;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35821;&#38899;&#30340;&#21516;&#26102;&#33021;&#22815;&#26356;&#22909;&#22320;&#20445;&#25345;&#19982;&#30446;&#26631;&#21442;&#32771;&#30340;&#30456;&#20284;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#24456;&#30701;&#30340;&#21442;&#32771;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#26088;&#22312;&#23558;&#28304;&#35828;&#35805;&#20154;&#30340;&#38899;&#33394;&#36716;&#25442;&#20026;&#20219;&#24847;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#38899;&#33394;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#35328;&#20869;&#23481;&#19981;&#21464;&#12290;&#23613;&#31649;&#36890;&#36807;&#25552;&#20379;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#35821;&#38899;&#30340;&#35828;&#35805;&#20154;&#30456;&#20284;&#24615;&#65292;&#20294;&#35828;&#35805;&#20154;&#30456;&#20284;&#24615;&#20173;&#28982;&#33853;&#21518;&#20110;&#30495;&#23454;&#24405;&#38899;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SEF-VC&#65292;&#19968;&#31181;&#26080;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#22823;&#30340;&#20301;&#32622;&#26080;&#20851;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#21442;&#32771;&#35821;&#38899;&#20013;&#23398;&#20064;&#24182;&#34701;&#20837;&#35828;&#35805;&#20154;&#38899;&#33394;&#65292;&#28982;&#21518;&#20197;&#38750;&#33258;&#22238;&#24402;&#26041;&#24335;&#20174;HuBERT&#35821;&#20041;&#26631;&#35760;&#20013;&#37325;&#26500;&#27874;&#24418;&#12290;SEF-VC&#30340;&#31616;&#27905;&#35774;&#35745;&#22686;&#24378;&#20102;&#20854;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#35821;&#38899;&#36716;&#25442;&#24615;&#33021;&#12290;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#35777;&#26126;&#65292;SEF-VC&#20248;&#20110;&#24378;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#22522;&#32447;&#65292;&#33021;&#22815;&#29983;&#25104;&#19982;&#30446;&#26631;&#21442;&#32771;&#30340;&#26356;&#39640;&#30456;&#20284;&#24615;&#30340;&#39640;&#36136;&#37327;&#35821;&#38899;&#65292;&#21363;&#20351;&#23545;&#20110;&#24456;&#30701;&#30340;&#21442;&#32771;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#38463;&#25289;&#20271;&#35821;&#22312;&#32447;&#27602;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#35843;&#30740;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#24046;&#36317;&#21644;&#38382;&#39064;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2312.07228</link><description>&lt;p&gt;
&#27602;&#24615;&#35821;&#35328;&#26816;&#27979;&#65306;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Toxic language detection: a systematic review of Arabic datasets. (arXiv:2312.07228v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07228
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#38463;&#25289;&#20271;&#35821;&#22312;&#32447;&#27602;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#35843;&#30740;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#24046;&#36317;&#21644;&#38382;&#39064;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38463;&#25289;&#20271;&#35821;&#30340;&#27602;&#24615;&#35821;&#35328;&#26816;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#24182;&#19988;&#23545;&#20110;&#22238;&#39038;&#29992;&#20110;&#35757;&#32451;&#24320;&#21457;&#35299;&#20915;&#26041;&#26696;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#24050;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#23545;&#38463;&#25289;&#20271;&#35821;&#22312;&#32447;&#27602;&#24615;&#35821;&#35328;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#30740;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25910;&#38598;&#20102;&#24635;&#20849;54&#20010;&#21487;&#29992;&#25968;&#25454;&#38598;&#21450;&#20854;&#23545;&#24212;&#30340;&#35770;&#25991;&#65292;&#24182;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#20998;&#26512;&#65292;&#32771;&#34385;&#20102;18&#20010;&#26631;&#20934;&#65292;&#28085;&#30422;&#20102;&#21487;&#29992;&#24615;&#32454;&#33410;&#12289;&#20869;&#23481;&#12289;&#27880;&#37322;&#36807;&#31243;&#21644;&#37325;&#22797;&#20351;&#29992;&#24615;&#22235;&#20010;&#20027;&#35201;&#32500;&#24230;&#12290;&#36825;&#27425;&#20998;&#26512;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#29616;&#26377;&#30340;&#24046;&#36317;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#24037;&#20316;&#25552;&#20986;&#24314;&#35758;&#12290;&#20026;&#26041;&#20415;&#30740;&#31350;&#31038;&#21306;&#65292;&#20998;&#26512;&#30340;&#25968;&#25454;&#38598;&#21015;&#34920;&#24050;&#32463;&#32500;&#25252;&#22312;GitHub&#20179;&#24211;&#20013;&#65288;https://github.com/Imene1/Arabic-toxic-language&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of toxic language in the Arabic language has emerged as an active area of research in recent years, and reviewing the existing datasets employed for training the developed solutions has become a pressing need. This paper offers a comprehensive survey of Arabic datasets focused on online toxic language. We systematically gathered a total of 54 available datasets and their corresponding papers and conducted a thorough analysis, considering 18 criteria across four primary dimensions: availability details, content, annotation process, and reusability. This analysis enabled us to identify existing gaps and make recommendations for future research works. For the convenience of the research community, the list of the analysed datasets is maintained in a GitHub repository (https://github.com/Imene1/Arabic-toxic-language).
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36825;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;LLMs&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;LLMs&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#36739;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2312.05934</link><description>&lt;p&gt;
Fine-Tuning&#36824;&#26159;&#26816;&#32034;&#65311;&#27604;&#36739;&#22312;LLMs&#20013;&#30340;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. (arXiv:2312.05934v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36825;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;LLMs&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;LLMs&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#36739;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20013;&#23553;&#35013;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#27491;&#22914;&#23427;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22806;&#37096;&#25968;&#25454;&#38598;&#26469;&#25972;&#21512;&#26032;&#30340;&#20449;&#24687;&#25110;&#25913;&#36827;LLMs&#22312;&#24050;&#35265;&#20449;&#24687;&#19978;&#30340;&#33021;&#21147;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65306;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20027;&#39064;&#30340;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#33021;&#22815;&#25552;&#20379;&#19968;&#23450;&#30340;&#25913;&#36827;&#65292;&#20294;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#23436;&#20840;&#26032;&#30693;&#35782;&#19978;&#22987;&#32456;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#24456;&#38590;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#24182;&#19988;&#26292;&#38706;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiGPrompt&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2312.03731</link><description>&lt;p&gt;
&#22810;&#20010;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#22270;&#24418;&#25552;&#31034;&#30340;MultiGPrompt
&lt;/p&gt;
&lt;p&gt;
MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiGPrompt&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#21487;&#20197;&#22266;&#26377;&#22320;&#23545;Web&#19978;&#30456;&#20114;&#36830;&#25509;&#30340;&#23545;&#35937;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25903;&#25345;&#19968;&#31995;&#21015;Web&#24212;&#29992;&#65292;&#27604;&#22914;&#32593;&#32476;&#20998;&#26512;&#21644;&#20869;&#23481;&#25512;&#33616;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#27969;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#31471;&#21040;&#31471;&#30417;&#30563;&#26694;&#26550;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#19982;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#30340;&#21487;&#29992;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#20026;&#20102;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#24182;&#22686;&#24378;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#22522;&#20110;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#32780;&#25552;&#31034;&#21017;&#34987;&#25552;&#20986;&#26469;&#36827;&#19968;&#27493;&#32553;&#23567;&#39044;&#35757;&#32451;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#30446;&#26631;&#24046;&#36317;&#12290;&#34429;&#28982;&#24050;&#32463;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#24418;&#23398;&#20064;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25506;&#32034;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#21033;&#29992;&#21333;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23548;&#33268;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#33021;&#23398;&#20064;&#30340;&#36890;&#29992;&#30693;&#35782;&#30340;&#23376;&#38598;&#21463;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;MultiGPrompt&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#22270;&#24418;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs can inherently model interconnected objects on the Web, thereby facilitating a series of Web applications, such as web analyzing and content recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a mainstream technique for graph representation learning. However, their efficacy within an end-to-end supervised framework is significantly tied to the availabilityof task-specific labels. To mitigate labeling costs and enhance robustness in few-shot settings, pre-training on self-supervised tasks has emerged as a promising method, while prompting has been proposed to further narrow the objective gap between pretext and downstream tasks. Although there has been some initial exploration of prompt-based learning on graphs, they primarily leverage a single pretext task, resulting in a limited subset of general knowledge that could be learned from the pre-training data. Hence, in this paper, we propose MultiGPrompt, a novel multi-task pre-training and prompting framework to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#34917;&#26426;&#21046;&#23454;&#29616;&#20102;&#20013;&#26029;&#21644;&#24490;&#29615;&#26426;&#21046;&#65292;&#20351;&#20256;&#32479;&#35299;&#30721;&#36827;&#31243;&#21464;&#24471;&#38750;&#21333;&#35843;&#12290;&#21033;&#29992;&#20013;&#26029;&#26426;&#21046;&#21487;&#20197;&#25512;&#36831;&#29983;&#25104;&#20195;&#30721;&#65292;&#22686;&#24378;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#65307;&#21033;&#29992;&#24490;&#29615;&#26426;&#21046;&#21487;&#20197;&#24490;&#29615;&#26356;&#26032;&#21644;&#21516;&#27493;&#29983;&#25104;&#30340;&#27599;&#20010;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2311.17972</link><description>&lt;p&gt;
&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Infilling Code Generation. (arXiv:2311.17972v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#34917;&#26426;&#21046;&#23454;&#29616;&#20102;&#20013;&#26029;&#21644;&#24490;&#29615;&#26426;&#21046;&#65292;&#20351;&#20256;&#32479;&#35299;&#30721;&#36827;&#31243;&#21464;&#24471;&#38750;&#21333;&#35843;&#12290;&#21033;&#29992;&#20013;&#26029;&#26426;&#21046;&#21487;&#20197;&#25512;&#36831;&#29983;&#25104;&#20195;&#30721;&#65292;&#22686;&#24378;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#65307;&#21033;&#29992;&#24490;&#29615;&#26426;&#21046;&#21487;&#20197;&#24490;&#29615;&#26356;&#26032;&#21644;&#21516;&#27493;&#29983;&#25104;&#30340;&#27599;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#23558;&#34917;&#20805;&#25805;&#20316;&#34701;&#20837;&#33258;&#22238;&#24402;&#35299;&#30721;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#33021;&#22815;&#36827;&#34892;&#22635;&#20805;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#22635;&#20805;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;&#34917;&#20805;&#25805;&#20316;&#26088;&#22312;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#21069;&#32512;&#21644;&#21518;&#32512;&#22635;&#20805;&#20013;&#38388;&#20869;&#23481;&#65292;&#32780;&#33258;&#34917;&#26426;&#21046;&#39034;&#24207;&#29983;&#25104;&#36825;&#20123;&#21608;&#22260;&#19978;&#19979;&#25991;&#21644;&#34987;&#22635;&#20805;&#20869;&#23481;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#22312;&#20256;&#32479;&#35299;&#30721;&#20013;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#20013;&#26029;&#21644;&#24490;&#29615;&#26426;&#21046;&#65292;&#20351;&#20854;&#36827;&#21270;&#20026;&#38750;&#21333;&#35843;&#36807;&#31243;&#12290;&#20013;&#26029;&#26426;&#21046;&#20801;&#35768;&#25512;&#36831;&#29983;&#25104;&#29305;&#23450;&#30340;&#20195;&#30721;&#65292;&#30452;&#21040;&#30830;&#23450;&#30340;&#21518;&#32512;&#24314;&#31435;&#65292;&#22686;&#24378;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#12290;&#21516;&#26102;&#65292;&#24490;&#29615;&#26426;&#21046;&#21033;&#29992;&#33258;&#34917;&#21644;&#20174;&#24038;&#21040;&#21491;&#35299;&#30721;&#30340;&#20114;&#34917;&#24615;&#65292;&#21487;&#20197;&#24490;&#29615;&#26356;&#26032;&#21644;&#21516;&#27493;&#27599;&#20010;&#29983;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces self-infilling code generation, a general framework that incorporates infilling operations into auto-regressive decoding. Our approach capitalizes on the observation that recent infilling-capable code language models can self-infill: whereas infilling operations aim to fill in the middle based on a predefined prefix and suffix, self-infilling sequentially generates both such surrounding context and the infilled content. We utilize this capability to introduce novel interruption and looping mechanisms in conventional decoding, evolving it into a non-monotonic process. Interruptions allow for postponing the generation of specific code until a definitive suffix is established, enhancing control over the output. Meanwhile, the looping mechanism, which leverages the complementary nature of self-infilling and left-to-right decoding, can iteratively update and synchronize each piece of generation cyclically. Extensive experiments are conducted to demonstrate that our prop
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;WarAgent&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#27169;&#25311;&#21382;&#21490;&#22269;&#38469;&#20914;&#31361;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#20854;&#25928;&#26524;&#21644;&#30740;&#31350;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#25112;&#20105;&#30340;&#24341;&#21457;&#22240;&#32032;&#21644;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2311.17227</link><description>&lt;p&gt;
&#25112;&#20105;&#19982;&#21644;&#24179;&#65288;WarAgent&#65289;&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#19990;&#30028;&#22823;&#25112;&#22810;&#26234;&#33021;&#20307;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars. (arXiv:2311.17227v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;WarAgent&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#27169;&#25311;&#21382;&#21490;&#22269;&#38469;&#20914;&#31361;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#20854;&#25928;&#26524;&#21644;&#30740;&#31350;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#25112;&#20105;&#30340;&#24341;&#21457;&#22240;&#32032;&#21644;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#22312;&#21382;&#21490;&#30340;&#21313;&#23383;&#36335;&#21475;&#36991;&#20813;&#25112;&#20105;&#65311;&#36825;&#20010;&#38382;&#39064;&#22312;&#20154;&#31867;&#21382;&#21490;&#19978;&#19968;&#30452;&#34987;&#20010;&#20154;&#12289;&#23398;&#32773;&#12289;&#20915;&#31574;&#32773;&#21644;&#32452;&#32455;&#36861;&#27714;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#26681;&#25454;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WarAgent&#30340;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;AI&#31995;&#32479;&#65292;&#20197;&#27169;&#25311;&#21442;&#19982;&#22269;&#23478;&#65292;&#22312;&#21382;&#21490;&#19978;&#30340;&#22269;&#38469;&#20914;&#31361;&#65292;&#21253;&#25324;&#31532;&#19968;&#27425;&#19990;&#30028;&#22823;&#25112;&#65288;WWI&#65289;&#12289;&#31532;&#20108;&#27425;&#19990;&#30028;&#22823;&#25112;&#65288;WWII&#65289;&#21644;&#20013;&#22269;&#21476;&#20195;&#30340;&#25112;&#22269;&#26102;&#26399;&#65288;WSP&#65289;&#20013;&#30340;&#20915;&#31574;&#21644;&#21518;&#26524;&#12290;&#36890;&#36807;&#35780;&#20272;&#27169;&#25311;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23574;&#31471;AI&#31995;&#32479;&#22312;&#30740;&#31350;&#22797;&#26434;&#30340;&#38598;&#20307;&#20154;&#31867;&#34892;&#20026;&#65292;&#22914;&#22269;&#38469;&#20914;&#31361;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#33021;&#21147;&#30340;&#36827;&#23637;&#21644;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#20123;&#27169;&#25311;&#20013;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20063;&#20026;&#32771;&#23519;&#24341;&#21457;&#25112;&#20105;&#30340;&#35302;&#21457;&#22240;&#32032;&#21644;&#26465;&#20214;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#20102;&#35299;&#25112;&#20105;&#30340;&#35302;&#21457;&#22240;&#32032;&#21644;&#26465;&#20214;&#25552;&#20379;&#20102;&#26032;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems' abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#24037;&#31243;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#25216;&#26415;&#21253;&#25324;&#21033;&#29992;LLM&#30340;&#25968;&#25454;&#25286;&#20998;&#21644;&#25968;&#25454;&#32763;&#26032;&#25216;&#26415;&#25552;&#39640;&#23884;&#20837;&#31354;&#38388;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#24341;&#20837;&#22522;&#20110;LLM&#30340;&#23494;&#24230;&#38142;&#26465;&#21644;&#33258;&#36866;&#24212;&#25991;&#26412;&#32763;&#26032;&#31639;&#27861;&#35780;&#20272;&#25968;&#25454;&#32763;&#26032;&#21487;&#20449;&#24230;&#65292;&#24320;&#21457;&#38544;&#24335;&#30693;&#35782;&#25193;&#23637;&#21644;&#24605;&#32771;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#21450;&#36890;&#36807;&#37325;&#26500;&#29616;&#26377;&#33050;&#26412;&#29983;&#25104;&#26032;&#30340;&#39640;&#36136;&#37327;&#33050;&#26412;&#12290;&#22312;&#20351;&#29992;&#24037;&#31243;&#27169;&#25311;&#36719;&#20214;RedHawk-SC&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#26102;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#33021;&#22815;&#25193;&#23637;&#21644;&#20998;&#31867;&#33050;&#26412;&#65292;&#24182;&#22312;&#19982;IKEC&#32467;&#21512;&#20351;&#29992;&#26102;&#25552;&#39640;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.16267</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#31243;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#25968;&#25454;&#23884;&#20837;&#30340;&#26032;&#22411;&#39044;&#22788;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Novel Preprocessing Technique for Data Embedding in Engineering Code Generation Using Large Language Model. (arXiv:2311.16267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#24037;&#31243;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#25216;&#26415;&#21253;&#25324;&#21033;&#29992;LLM&#30340;&#25968;&#25454;&#25286;&#20998;&#21644;&#25968;&#25454;&#32763;&#26032;&#25216;&#26415;&#25552;&#39640;&#23884;&#20837;&#31354;&#38388;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#24341;&#20837;&#22522;&#20110;LLM&#30340;&#23494;&#24230;&#38142;&#26465;&#21644;&#33258;&#36866;&#24212;&#25991;&#26412;&#32763;&#26032;&#31639;&#27861;&#35780;&#20272;&#25968;&#25454;&#32763;&#26032;&#21487;&#20449;&#24230;&#65292;&#24320;&#21457;&#38544;&#24335;&#30693;&#35782;&#25193;&#23637;&#21644;&#24605;&#32771;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#21450;&#36890;&#36807;&#37325;&#26500;&#29616;&#26377;&#33050;&#26412;&#29983;&#25104;&#26032;&#30340;&#39640;&#36136;&#37327;&#33050;&#26412;&#12290;&#22312;&#20351;&#29992;&#24037;&#31243;&#27169;&#25311;&#36719;&#20214;RedHawk-SC&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#26102;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#33021;&#22815;&#25193;&#23637;&#21644;&#20998;&#31867;&#33050;&#26412;&#65292;&#24182;&#22312;&#19982;IKEC&#32467;&#21512;&#20351;&#29992;&#26102;&#25552;&#39640;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#20027;&#35201;&#36129;&#29486;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#29305;&#23450;&#39046;&#22495;&#20195;&#30721;&#26102;&#30340;&#24615;&#33021;&#65306;&#65288;i&#65289;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#25286;&#20998;&#21644;&#25968;&#25454;&#32763;&#26032;&#25216;&#26415;&#26469;&#25552;&#39640;&#23884;&#20837;&#31354;&#38388;&#30340;&#35821;&#20041;&#34920;&#31034;&#65307;&#65288;ii&#65289;&#24341;&#20837;&#30001;LLM&#39537;&#21160;&#30340;&#23494;&#24230;&#38142;&#26465;&#20197;&#29992;&#20110;&#32763;&#26032;&#21487;&#20449;&#24230;&#65288;CoDRC&#65289;&#65292;&#20197;&#21450;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#32763;&#26032;&#21487;&#38752;&#24615;&#30340;&#33258;&#36866;&#24212;&#25991;&#26412;&#32763;&#26032;&#65288;ATR&#65289;&#31639;&#27861;&#65307;&#65288;iii&#65289;&#24320;&#21457;&#38544;&#24335;&#30693;&#35782;&#25193;&#23637;&#21644;&#24605;&#32771;&#65288;IKEC&#65289;&#25552;&#31034;&#25216;&#26415;&#65307;&#65288;iv&#65289;&#36890;&#36807;&#26377;&#25928;&#37325;&#26500;&#29616;&#26377;&#33050;&#26412;&#26469;&#20351;&#29992;LLM&#29983;&#25104;&#26032;&#30340;&#39640;&#36136;&#37327;&#33050;&#26412;&#12290;&#25105;&#20204;&#20197;&#24037;&#31243;&#27169;&#25311;&#36719;&#20214;RedHawk-SC&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#22312;&#25193;&#23637;&#21644;&#20998;&#31867;&#33050;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#24403;&#19982;IKEC&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#22686;&#24378;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#20197;&#26816;&#32034;&#26356;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#26368;&#32456;&#23454;&#29616;73.33&#65285;&#30340;&#8220;&#20849;&#29616;&#30334;&#20998;&#27604;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present four main contributions to enhance the performance of Large Language Models (LLMs) in generating domain-specific code: (i) utilizing LLM-based data splitting and data renovation techniques to improve the semantic representation of embeddings' space; (ii) introducing the Chain of Density for Renovation Credibility (CoDRC), driven by LLMs, and the Adaptive Text Renovation (ATR) algorithm for assessing data renovation reliability; (iii) developing the Implicit Knowledge Expansion and Contemplation (IKEC) Prompt technique; and (iv) effectively refactoring existing scripts to generate new and high-quality scripts with LLMs. By using engineering simulation software RedHawk-SC as a case study, we demonstrate the effectiveness of our data pre-processing method for expanding and categorizing scripts. When combined with IKEC, these techniques enhance the Retrieval-Augmented Generation (RAG) method in retrieving more relevant information, ultimately achieving a 73.33% "Percentage of Co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.11482</link><description>&lt;p&gt;
AGI&#31995;&#32479;&#30340;&#20803;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#25552;&#31034;(meta prompting)&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#25216;&#26415;&#65292;&#37325;&#26032;&#22609;&#36896;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12289;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#21033;&#29992;&#12290;&#22522;&#20110;&#31867;&#22411;&#29702;&#35770;&#21644;&#33539;&#30068;&#35770;&#65292;&#20803;&#25552;&#31034;&#27880;&#37325;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#20197;&#20869;&#23481;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20803;&#25552;&#31034;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#23558;&#20854;&#19982;&#23569;&#26679;&#26412;&#25552;&#31034;(few-shot prompting)&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#37325;&#28857;&#20851;&#27880;&#23558;&#20803;&#25552;&#31034;&#25193;&#23637;&#21040;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#23637;&#31034;&#22914;&#20309;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#20998;&#25104;&#36739;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20196;&#29260;&#25928;&#29575;&#65292;&#24182;&#20351;&#38382;&#39064;&#27714;&#35299;&#30340;&#27604;&#36739;&#26356;&#21152;&#20844;&#24179;&#65292;&#23588;&#20854;&#26159;&#19982;&#23569;&#26679;&#26412;&#31034;&#20363;&#26041;&#27861;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#25552;&#31034;&#20219;&#21153;&#65292;&#20801;&#35768;LLMs&#20197;&#36845;&#20195;&#30340;&#20803;&#32534;&#31243;&#24418;&#24335;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive study of Meta Prompting, an innovative technique reshaping the utilization of large language models (LLMs), multi-modal foundation models, and AI systems in problem-solving and data interpretation. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications. A key focus is on extending Meta Prompting to complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency and enabling more equitable problem-solving comparisons, especially against few-shot example methods. Additionally, the paper introduces Meta Prompting for Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative, metaprogramming-like manner. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CORECT&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#20851;&#31995;&#26102;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36741;&#21161;&#36328;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#25429;&#25417;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2311.04507</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#36741;&#21161;&#36328;&#27169;&#24577;&#20132;&#20114;&#30340;&#20851;&#31995;&#26102;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23545;&#35805;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction. (arXiv:2311.04507v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CORECT&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#20851;&#31995;&#26102;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36741;&#21161;&#36328;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#25429;&#25417;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35782;&#21035;&#26159;&#20154;&#31867;&#23545;&#35805;&#29702;&#35299;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#38543;&#30528;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24341;&#20837;&#65292;&#22914;&#35821;&#35328;&#12289;&#22768;&#38899;&#21644;&#38754;&#37096;&#34920;&#24773;&#65292;&#36825;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20316;&#20026;&#19968;&#31181;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#39044;&#27979;&#23545;&#35805;&#20013;&#27599;&#20010;&#21477;&#23376;&#65288;&#21363;&#35805;&#35821;&#65289;&#30340;&#24773;&#32490;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20840;&#23616;&#34920;&#31034;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#23545;&#35805;&#32423;&#21035;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#26469;&#25429;&#25417;&#12290;&#23616;&#37096;&#34920;&#31034;&#36890;&#24120;&#26159;&#36890;&#36807;&#35828;&#35805;&#32773;&#25110;&#24773;&#32490;&#21464;&#21270;&#30340;&#26102;&#38388;&#20449;&#24687;&#26469;&#25512;&#26029;&#30340;&#65292;&#24573;&#35270;&#20102;&#35805;&#35821;&#32423;&#21035;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#32479;&#19968;&#36755;&#20837;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#29305;&#24449;&#65292;&#32780;&#19981;&#21033;&#29992;&#27169;&#24577;&#29305;&#23450;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#31995;&#26102;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#36741;&#21161;&#36328;&#27169;&#24577;&#20132;&#20114;&#65288;CORECT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#25928;&#25429;&#25417;&#23545;&#35805;&#20013;&#24773;&#24863;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition is a crucial task for human conversation understanding. It becomes more challenging with the notion of multimodal data, e.g., language, voice, and facial expressions. As a typical solution, the global- and the local context information are exploited to predict the emotional label for every single sentence, i.e., utterance, in the dialogue. Specifically, the global representation could be captured via modeling of cross-modal interactions at the conversation level. The local one is often inferred using the temporal information of speakers or emotional shifts, which neglects vital factors at the utterance level. Additionally, most existing approaches take fused features of multiple modalities in an unified input without leveraging modality-specific representations. Motivating from these problems, we propose the Relational Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction (CORECT), an novel neural network framework that effectively captures convers
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.09499</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27425;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. (arXiv:2310.09499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09499
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#20013;&#30340;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#37327;&#21270;&#12289;&#21098;&#26525;&#21644;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;LLMs&#30340;&#25928;&#29575;&#25104;&#20026;LLM&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;LLMs&#21098;&#26525;&#33267;&#33267;&#23569;50%&#30340;&#31232;&#30095;&#24615;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#23427;&#26681;&#25454;&#25935;&#24863;&#24230;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#31232;&#30095;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#38477;&#20302;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#24403;&#31232;&#30095;&#24230;&#38750;&#24120;&#39640;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Various Large Language Models(LLMs) from the Generative Pretrained Transformer~(GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50\% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs.
&lt;/p&gt;</description></item><item><title>EMO&#25552;&#20986;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#29616;&#35937;&#12290;EMO&#21033;&#29992;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#19978;&#30028;&#26469;&#31616;&#21270;&#35757;&#32451;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;EMO&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04691</link><description>&lt;p&gt;
EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04691
&lt;/p&gt;
&lt;p&gt;
EMO&#25552;&#20986;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#29616;&#35937;&#12290;EMO&#21033;&#29992;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#19978;&#30028;&#26469;&#31616;&#21270;&#35757;&#32451;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;EMO&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26159;&#20154;&#25991;&#26412;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#23427;&#20204;&#20027;&#35201;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#31561;&#21516;&#20110;&#26368;&#23567;&#21270;&#32463;&#39564;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#20998;&#24067;&#20043;&#38388;&#30340;&#21069;&#21521;&#20132;&#21449;&#29109;&#12290;&#28982;&#32780;&#65292;&#24403;&#20174;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#30340;&#20998;&#24067;&#35299;&#30721;&#26102;&#65292;&#20173;&#28982;&#32463;&#24120;&#35266;&#23519;&#21040;&#21508;&#31181;&#36864;&#21270;&#29616;&#35937;&#12290;&#25105;&#20204;&#30830;&#23450;&#21069;&#21521;&#20132;&#21449;&#29109;&#20316;&#20026;&#20154;&#19982;&#27169;&#22411;&#20998;&#24067;&#23545;&#40784;&#30340;&#36317;&#31163;&#24230;&#37327;&#26159;&#27425;&#20248;&#30340;&#65292;&#21407;&#22240;&#26377;&#65306;&#65288;1&#65289;&#21484;&#22238;&#20248;&#21270;&#65292;&#65288;2&#65289;&#36127;&#26679;&#26412;&#22810;&#26679;&#24615;&#24573;&#35270;&#21644;&#65288;3&#65289;&#35757;&#32451;&#27979;&#35797;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#12290;EMO&#21033;&#29992;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#20869;&#22312;&#29305;&#24615;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#30001;&#20110;&#30452;&#25509;&#35745;&#31639;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;EMO&#19978;&#30028;&#26469;&#31616;&#21270;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#32463;&#36807;&#24191;&#27867;&#35780;&#20272;&#20043;&#21518;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural language models are probabilistic models of human text. They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution. However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models. We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch. In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges. Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training. Upon extensive evaluation of language model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#20174;&#26059;&#24459;&#20013;&#29983;&#25104;&#38899;&#33410;&#32423;&#27468;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#21644;&#29983;&#25104;&#22120;&#32593;&#32476;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#25506;&#32034;ChatGPT&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#29983;&#25104;&#27468;&#35789;&#30340;&#36830;&#36143;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00863</link><description>&lt;p&gt;
&#20174;&#26059;&#24459;&#20013;&#21033;&#29992;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#38899;&#33410;&#32423;&#27468;&#35789;
&lt;/p&gt;
&lt;p&gt;
Syllable-level lyrics generation from melody exploiting character-level language model. (arXiv:2310.00863v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#20174;&#26059;&#24459;&#20013;&#29983;&#25104;&#38899;&#33410;&#32423;&#27468;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#21644;&#29983;&#25104;&#22120;&#32593;&#32476;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#25506;&#32034;ChatGPT&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#29983;&#25104;&#27468;&#35789;&#30340;&#36830;&#36143;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#19982;&#20276;&#22863;&#26059;&#24459;&#32039;&#23494;&#30456;&#20851;&#30340;&#27468;&#35789;&#28041;&#21450;&#24314;&#31435;&#38899;&#20048;&#38899;&#31526;&#19982;&#27468;&#35789;&#38899;&#33410;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#20010;&#36807;&#31243;&#38656;&#35201;&#23545;&#38899;&#33410;&#32423;&#12289;&#35789;&#32423;&#21644;&#21477;&#32423;&#35821;&#20041;&#24847;&#20041;&#19978;&#30340;&#38899;&#20048;&#32422;&#26463;&#21644;&#35821;&#20041;&#27169;&#24335;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#20844;&#24320;&#30340;&#38899;&#33410;&#32423;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#20197;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38899;&#33410;&#32423;&#27468;&#35789;&#29983;&#25104;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#30693;&#35782;&#34701;&#20837;&#38899;&#33410;&#32423;Transformer&#29983;&#25104;&#22120;&#32593;&#32476;&#30340;&#26463;&#25628;&#32034;&#36807;&#31243;&#20013;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25506;&#32034;&#22522;&#20110;ChatGPT&#30340;&#29983;&#25104;&#27468;&#35789;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#20154;&#24037;&#20027;&#35266;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29983;&#25104;&#27468;&#35789;&#30340;&#36830;&#36143;&#24615;&#21644;&#27491;&#30830;&#24615;&#65292;&#28040;&#38500;&#20102;&#35757;&#32451;&#26114;&#36149;&#30340;&#26032;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of lyrics tightly connected to accompanying melodies involves establishing a mapping between musical notes and syllables of lyrics. This process requires a deep understanding of music constraints and semantic patterns at syllable-level, word-level, and sentence-level semantic meanings. However, pre-trained language models specifically designed at the syllable level are publicly unavailable. To solve these challenging issues, we propose to exploit fine-tuning character-level language models for syllable-level lyrics generation from symbolic melody. In particular, our method endeavors to incorporate linguistic knowledge of the language model into the beam search process of a syllable-level Transformer generator network. Additionally, by exploring ChatGPT-based evaluation for generated lyrics, along with human subjective evaluation, we demonstrate that our approach enhances the coherence and correctness of the generated lyrics, eliminating the need to train expensive new la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#24182;&#21457;&#29616;&#20316;&#23478;&#20204;&#26356;&#20542;&#21521;&#20110;&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#38454;&#27573;&#20013;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2309.12570</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#21019;&#36896;&#21147;&#25903;&#25345;: &#19968;&#39033;&#28041;&#21450;&#26032;&#20852;&#20316;&#23478;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers. (arXiv:2309.12570v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#24182;&#21457;&#29616;&#20316;&#23478;&#20204;&#26356;&#20542;&#21521;&#20110;&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#38454;&#27573;&#20013;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#20854;&#33021;&#22815;&#36981;&#24490;&#25351;&#20196;&#24182;&#21442;&#19982;&#23545;&#35805;&#20114;&#21160;&#65292;&#24341;&#21457;&#20102;&#22312;&#21508;&#31181;&#25903;&#25345;&#24037;&#20855;&#20013;&#21033;&#29992;&#23427;&#20204;&#30340;&#20852;&#36259;&#22686;&#21152;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#23454;&#35777;&#29992;&#25143;&#30740;&#31350;&#65288;n=30&#65289;&#25506;&#35752;&#20102;&#29616;&#20195;LLM&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#21512;&#20316;&#20889;&#20316;&#30028;&#38754;&#35774;&#35745;&#22522;&#20110;&#23558;&#20889;&#20316;&#35270;&#20026;&#19968;&#20010;&#30446;&#26631;&#23548;&#21521;&#30340;&#24605;&#32500;&#36807;&#31243;&#30340;&#35748;&#30693;&#36807;&#31243;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#38750;&#32447;&#24615;&#30340;&#35748;&#30693;&#27963;&#21160;&#65306;&#35268;&#21010;&#12289;&#32763;&#35793;&#21644;&#23457;&#26597;&#12290;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#25552;&#20132;&#19968;&#20221;&#21518;&#23436;&#25104;&#35843;&#26597;&#65292;&#20197;&#25552;&#20379;&#20851;&#20110;LLM&#20316;&#20026;&#20889;&#20316;&#21512;&#20316;&#32773;&#28508;&#21147;&#21644;&#38382;&#39064;&#30340;&#21453;&#39304;&#12290;&#36890;&#36807;&#20998;&#26512;&#20316;&#23478;-LLM&#20114;&#21160;,&#25105;&#20204;&#21457;&#29616;&#20316;&#23478;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#35748;&#30693;&#27963;&#21160;&#20013;&#37117;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#65292;&#20294;&#20182;&#20204;&#21457;&#29616;LLM&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#26041;&#38754;&#26356;&#26377;&#24110;&#21161;&#12290;&#36890;&#36807;&#20998;&#26512;&#20114;&#21160;&#21644;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) capable of following instructions and engaging in conversational interactions sparked increased interest in their utilization across various support tools. We investigate the utility of modern LLMs in assisting professional writers via an empirical user study (n=30). The design of our collaborative writing interface is grounded in the cognitive process model of writing that views writing as a goal-oriented thinking process encompassing non-linear cognitive activities: planning, translating, and reviewing. Participants are asked to submit a post-completion survey to provide feedback on the potential and pitfalls of LLMs as writing collaborators. Upon analyzing the writer-LLM interactions, we find that while writers seek LLM's help across all three types of cognitive activities, they find LLMs more helpful in translation and reviewing. Our findings from analyzing both the interactions and the survey responses highlight future research direc
&lt;/p&gt;</description></item><item><title>MAPLE&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08648</link><description>&lt;p&gt;
MAPLE: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings. (arXiv:2309.08648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08648
&lt;/p&gt;
&lt;p&gt;
MAPLE&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31227;&#21160;&#24212;&#29992;&#30340;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#30001;&#20110;&#22797;&#26434;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#65292;&#39044;&#27979;&#24212;&#29992;&#30340;&#20351;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE)&#27169;&#22411;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#20934;&#30830;&#39044;&#27979;&#24212;&#29992;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;MAPLE&#30340;&#33021;&#21147;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#36825;&#20123;&#24378;&#22823;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;MAPLE&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#24377;&#24615;&#12290;&#23613;&#31649;&#20854;&#20027;&#35201;&#35774;&#35745;&#38754;&#21521;&#24212;&#29992;&#39044;&#27979;&#65292;&#20294;&#32467;&#26524;&#20063;&#24378;&#35843;&#20102;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;LLM&#22312;&#24212;&#29992;&#20351;&#29992;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24314;&#35758;&#22312;&#24314;&#27169;&#21508;&#31181;&#39046;&#22495;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#65292;&#23427;&#20204;&#20855;&#26377;&#21464;&#38761;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid advancement of mobile applications, predicting app usage remains a formidable challenge due to intricate user behaviours and ever-evolving contexts. To address these issues, this paper introduces the Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE) model. This innovative approach utilizes Large Language Models (LLMs) to predict app usage accurately. Rigorous testing on two public datasets highlights MAPLE's capability to decipher intricate patterns and comprehend user contexts. These robust results confirm MAPLE's versatility and resilience across various scenarios. While its primary design caters to app prediction, the outcomes also emphasize the broader applicability of LLMs in different domains. Through this research, we emphasize the potential of LLMs in app usage prediction and suggest their transformative capacity in modelling human behaviours across diverse fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;&#65292;&#21253;&#25324;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#31561;&#26041;&#38754;&#12290;&#21363;&#20351;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08345</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases. (arXiv:2309.08345v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;&#65292;&#21253;&#25324;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#31561;&#26041;&#38754;&#12290;&#21363;&#20351;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#19982;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#31561;&#29616;&#23454;&#29615;&#22659;&#30340;&#25972;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#27424;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24433;&#21709;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#24212;&#29992;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#8220;&#20135;&#29983;&#34394;&#20551;&#20449;&#24687;&#8221;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;LM&#22312;&#22788;&#29702;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#20219;&#21153;&#26102;&#25152;&#36935;&#21040;&#30340;&#20581;&#22766;&#24615;&#25361;&#25112;&#12290;&#30740;&#31350;&#35206;&#30422;&#20102;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#21508;&#31181;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#25581;&#31034;&#20102;&#21363;&#20351;&#22312;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#20808;&#36827;&#30340;&#23567;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have already demonstrated remarkable abilities in understanding and generating both natural and formal language. Despite these advances, their integration with real-world environments such as large-scale knowledge bases (KBs) remains an underdeveloped area, affecting applications such as semantic parsing and indulging in "hallucinated" information. This paper is an experimental investigation aimed at uncovering the robustness challenges that LMs encounter when tasked with knowledge base question answering (KBQA). The investigation covers scenarios with inconsistent data distribution between training and inference, such as generalization to unseen domains, adaptation to various language variations, and transferability across different datasets. Our comprehensive experiments reveal that even when employed with our proposed data augmentation techniques, advanced small and large language models exhibit poor performance in various dimensions. While the LM is a promisin
&lt;/p&gt;</description></item><item><title>&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#36873;&#25321;&#19982;&#27979;&#35797;&#36755;&#20837;&#35821;&#20041;&#30456;&#20284;&#30340;&#28436;&#31034;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#65292;&#20294;&#26159;&#32771;&#34385;&#21040;&#35821;&#35328;&#27169;&#22411;&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#33021;&#22815;&#26356;&#22909;&#22320;&#25351;&#23548;&#28436;&#31034;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.07900</link><description>&lt;p&gt;
&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#27495;&#20041;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Ambiguity-Aware In-Context Learning with Large Language Models. (arXiv:2309.07900v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07900
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#36873;&#25321;&#19982;&#27979;&#35797;&#36755;&#20837;&#35821;&#20041;&#30456;&#20284;&#30340;&#28436;&#31034;&#26377;&#21161;&#20110;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#65292;&#20294;&#26159;&#32771;&#34385;&#21040;&#35821;&#35328;&#27169;&#22411;&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#33021;&#22815;&#26356;&#22909;&#22320;&#25351;&#23548;&#28436;&#31034;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;In-context learning, ICL&#65289;&#20013;&#65292;&#20165;&#21521;LLMs&#23637;&#31034;&#23569;&#37327;&#20219;&#21153;&#29305;&#23450;&#28436;&#31034;&#24050;&#32463;&#23548;&#33268;&#20102;&#19979;&#28216;&#22686;&#30410;&#65292;&#26080;&#38656;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#20110;&#25552;&#31034;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#22914;&#20309;&#20026;ICL&#36873;&#25321;&#22909;&#30340;&#28436;&#31034;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#26159;&#21033;&#29992;ICL&#28436;&#31034;&#21644;&#27979;&#35797;&#36755;&#20837;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#26816;&#32034;&#22120;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#32771;&#34385;LLM&#20851;&#20110;&#35813;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#22240;&#27492;&#24182;&#19981;&#26368;&#20248;&#12290;&#26681;&#25454;&#20043;&#21069;&#30340;&#24037;&#20316;&#65288;Min&#31561;&#65292;2022&#65289;&#65292;&#25105;&#20204;&#24050;&#32463;&#30693;&#36947;&#19982;&#28436;&#31034;&#37197;&#23545;&#30340;&#26631;&#31614;&#20250;&#23545;&#27169;&#22411;&#39044;&#27979;&#36896;&#25104;&#20559;&#35265;&#12290;&#36825;&#24341;&#23548;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65306;&#32771;&#34385;&#21040;LLM&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#19982;&#36755;&#20986;&#26631;&#31614;&#31354;&#38388;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#26159;&#21542;&#26377;&#21161;&#20110;&#26356;&#22909;&#30340;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#20165;&#36873;&#25321;&#35821;&#20041;&#30456;&#20284;&#30340;ICL&#28436;&#31034;&#26159;&#26377;&#30410;&#30340;&#65292;&#21516;&#26102;&#20063;&#35201;&#32771;&#34385;LLM&#20851;&#20110;&#20219;&#21153;&#30340;&#29616;&#26377;&#30693;&#35782;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) i.e. showing LLMs only a few task-specific demonstrations has led to downstream gains with no task-specific fine-tuning required. However, LLMs are sensitive to the choice of prompts, and therefore a crucial research question is how to select good demonstrations for ICL. One effective strategy is leveraging semantic similarity between the ICL demonstrations and test inputs by using a text retriever, which however is sub-optimal as that does not consider the LLM's existing knowledge about that task. From prior work (Min et al., 2022), we already know that labels paired with the demonstrations bias the model predictions. This leads us to our hypothesis whether considering LLM's existing knowledge about the task, especially with respect to the output label space can help in a better demonstration selection strategy. Through extensive experimentation on three text classification tasks, we find that it is beneficial to not only choose semantically similar ICL demon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#28040;&#34701;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#36335;&#24452;&#26469;&#21435;&#38500;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#33391;&#34892;&#20026;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20943;&#23569;GPT-2&#27602;&#24615;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#65292;&#20165;&#28040;&#34701;12&#26465;&#22240;&#26524;&#36793;&#20013;&#30340;11.6K&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#27602;&#24615;&#29983;&#25104;&#65292;&#24182;&#22312;&#20854;&#20182;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.05973</link><description>&lt;p&gt;
&#20999;&#26029;&#30005;&#36335;: &#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#28040;&#34701;&#21435;&#38500;&#27169;&#22411;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Circuit Breaking: Removing Model Behaviors with Targeted Ablation. (arXiv:2309.05973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#28040;&#34701;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#36335;&#24452;&#26469;&#21435;&#38500;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#33391;&#34892;&#20026;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20943;&#23569;GPT-2&#27602;&#24615;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#65292;&#20165;&#28040;&#34701;12&#26465;&#22240;&#26524;&#36793;&#20013;&#30340;11.6K&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#27602;&#24615;&#29983;&#25104;&#65292;&#24182;&#22312;&#20854;&#20182;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#34920;&#29616;&#20986;&#22312;&#39044;&#35757;&#32451;&#30446;&#26631;&#19978;&#25552;&#39640;&#24615;&#33021;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#38477;&#20302;&#24615;&#33021;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#34701;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#22240;&#26524;&#36335;&#24452;&#65292;&#20197;&#31105;&#29992;&#19982;&#19981;&#33391;&#34892;&#20026;&#26377;&#20851;&#30340;&#35745;&#31639;&#30005;&#36335;&#65292;&#20174;&#32780;&#21435;&#38500;&#19981;&#33391;&#34892;&#20026;&#12290;&#22312;&#25317;&#26377;&#27169;&#22411;&#34920;&#29616;&#24046;&#30340;&#23567;&#22411;&#36755;&#20837;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23398;&#20250;&#20102;&#28040;&#34701;&#19968;&#23567;&#37096;&#20998;&#37325;&#35201;&#30340;&#22240;&#26524;&#36335;&#24452;&#12290;&#22312;&#20943;&#23569;GPT-2&#27602;&#24615;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#28040;&#34701;&#20165;&#20165;12&#26465;&#22240;&#26524;&#36793;&#20013;&#30340;11.6K&#65292;&#21487;&#20197;&#20943;&#36731;&#27602;&#24615;&#29983;&#25104;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. We propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. Given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. In the setting of reducing GPT-2 toxic language generation, we find ablating just 12 of the 11.6K causal edges mitigates toxic generation with minimal degradation of performance on other inputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;Gscore&#20316;&#20026;&#34913;&#37327;&#29983;&#25104;&#32467;&#26524;&#36136;&#37327;&#30340;&#32508;&#21512;&#25351;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.04823</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Generation Capabilities of Large Chinese Language Models. (arXiv:2308.04823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;Gscore&#20316;&#20026;&#34913;&#37327;&#29983;&#25104;&#32467;&#26524;&#36136;&#37327;&#30340;&#32508;&#21512;&#25351;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CG-Eval&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23545;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#31185;&#23398;&#24037;&#31243;&#12289;&#20154;&#25991;&#31038;&#31185;&#12289;&#25968;&#23398;&#35745;&#31639;&#12289;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#12289;&#21496;&#27861;&#32771;&#35797;&#21644;&#27880;&#20876;&#20250;&#35745;&#24072;&#32771;&#35797;&#20845;&#20010;&#23398;&#31185;&#20013;&#29983;&#25104;&#20934;&#30830;&#21644;&#30456;&#20851;&#30340;&#22238;&#31572;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;Gscore&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#21152;&#26435;&#27714;&#21644;&#24471;&#21040;&#30340;&#32508;&#21512;&#25351;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#19982;&#21442;&#32771;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#27979;&#35797;&#25968;&#25454;&#21644;&#27979;&#35797;&#32467;&#26524;&#21487;&#22312;&#27492;http URL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents CG-Eval, the first comprehensive evaluation of the generation capabilities of large Chinese language models across a wide range of academic disciplines. The models' performance was assessed based on their ability to generate accurate and relevant responses to different types of questions in six disciplines, namely, Science and Engineering, Humanities and Social Sciences, Mathematical Calculations, Medical Practitioner Qualification Examination, Judicial Examination, and Certified Public Accountant Examination. This paper also presents Gscore, a composite index derived from the weighted sum of multiple metrics to measure the quality of model's generation against a reference. The test data and test results can be found at this http URL
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#20854;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#23545;&#29992;&#25143;&#26597;&#35810;&#30340;&#22238;&#22797;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11489</link><description>&lt;p&gt;
&#20026;&#20107;&#23454;&#24863;&#30693;&#35821;&#35328;&#24314;&#27169;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling. (arXiv:2306.11489v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11489
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#20854;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#23545;&#29992;&#25143;&#26597;&#35810;&#30340;&#22238;&#22797;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ChatGPT&#20316;&#20026;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22240;&#20854;&#24378;&#22823;&#30340;&#26032;&#20852;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#65292;LLMs&#26377;&#21487;&#33021;&#21462;&#20195;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#65292;&#25104;&#20026;&#21442;&#25968;&#21270;&#30693;&#35782;&#24211;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;LLMs&#25797;&#38271;&#22522;&#20110;&#22823;&#35821;&#26009;&#24211;&#23398;&#20064;&#27010;&#29575;&#35821;&#35328;&#27169;&#24335;&#65292;&#24182;&#19982;&#20154;&#31867;&#36827;&#34892;&#23545;&#35805;&#65292;&#20294;&#23427;&#20204;&#19982;&#20043;&#21069;&#36739;&#23567;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#19968;&#26679;&#65292;&#22312;&#29983;&#25104;&#22522;&#20110;&#30693;&#35782;&#30340;&#20869;&#23481;&#26102;&#20173;&#28982;&#38590;&#20197;&#22238;&#24518;&#20107;&#23454;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#25968;&#25454;&#39537;&#21160;&#30340;PLMs&#65292;&#23558;&#26126;&#30830;&#30340;&#20107;&#23454;&#30693;&#35782;&#34701;&#20837;PLMs&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#29983;&#25104;&#38656;&#35201;&#20107;&#23454;&#30693;&#35782;&#30340;&#25991;&#26412;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#29992;&#25143;&#26597;&#35810;&#25552;&#20379;&#26356;&#22810;&#35265;&#35299;&#30340;&#22238;&#22797;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#26377;&#20851;&#20351;&#29992;KG&#22686;&#24378;PLMs&#30340;&#30740;&#31350;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;PLM&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention due to its powerful emergent abilities. Some researchers suggest that LLMs could potentially replace structured knowledge bases like knowledge graphs (KGs) and function as parameterized knowledge bases. However, while LLMs are proficient at learning probabilistic language patterns based on large corpus and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance to generate texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-t
&lt;/p&gt;</description></item><item><title>ReactGenie&#26159;&#19968;&#20010;&#25903;&#25345;&#26500;&#24314;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#30340;&#38754;&#21521;&#23545;&#35937;&#29366;&#24577;&#25277;&#35937;&#65292;&#20351;&#24471;&#19981;&#21516;&#27169;&#24577;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.09649</link><description>&lt;p&gt;
ReactGenie&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38754;&#21521;&#23545;&#35937;&#29366;&#24577;&#25277;&#35937;&#26469;&#25903;&#25345;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
ReactGenie: An Object-Oriented State Abstraction for Complex Multimodal Interactions Using Large Language Models. (arXiv:2306.09649v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09649
&lt;/p&gt;
&lt;p&gt;
ReactGenie&#26159;&#19968;&#20010;&#25903;&#25345;&#26500;&#24314;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#30340;&#38754;&#21521;&#23545;&#35937;&#29366;&#24577;&#25277;&#35937;&#65292;&#20351;&#24471;&#19981;&#21516;&#27169;&#24577;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20132;&#20114;&#24050;&#34987;&#35777;&#26126;&#27604;&#20256;&#32479;&#30340;&#22270;&#24418;&#30028;&#38754;&#26356;&#21152;&#28789;&#27963;&#12289;&#39640;&#25928;&#21644;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#21644;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#24320;&#21457;&#26694;&#26550;&#35201;&#20040;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#22810;&#27169;&#24577;&#21629;&#20196;&#30340;&#22797;&#26434;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#35201;&#20040;&#38656;&#35201;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#22823;&#37327;&#20195;&#30721;&#26469;&#25903;&#25345;&#36825;&#20123;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ReactGenie&#65292;&#36825;&#26159;&#19968;&#20010;&#32534;&#31243;&#26694;&#26550;&#65292;&#20351;&#29992;&#20849;&#20139;&#30340;&#38754;&#21521;&#23545;&#35937;&#29366;&#24577;&#25277;&#35937;&#26469;&#25903;&#25345;&#26500;&#24314;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#12290;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#20849;&#20139;&#29366;&#24577;&#25277;&#35937;&#20351;&#24471;&#20351;&#29992;ReactGenie&#30340;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#21644;&#32452;&#21512;&#36825;&#20123;&#27169;&#24577;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;ReactGenie&#26159;&#26500;&#24314;&#22270;&#24418;&#24212;&#29992;&#31243;&#24207;&#30340;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#30340;&#33258;&#28982;&#25193;&#23637;&#65292;&#23601;&#20687;&#20351;&#29992;React-Redux&#19968;&#26679;&#12290;&#24320;&#21457;&#20154;&#21592;&#21482;&#38656;&#35201;&#28155;&#21152;&#19968;&#20123;&#27880;&#37322;&#21644;&#31034;&#20363;&#26469;&#25351;&#31034;&#33258;&#28982;&#35821;&#35328;&#22914;&#20309;&#26144;&#23556;&#21040;&#29992;&#25143;&#21487;&#35775;&#38382;&#30340;&#29366;&#24577;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal interactions have been shown to be more flexible, efficient, and adaptable for diverse users and tasks than traditional graphical interfaces. However, existing multimodal development frameworks either do not handle the complexity and compositionality of multimodal commands well or require developers to write a substantial amount of code to support these multimodal interactions. In this paper, we present ReactGenie, a programming framework that uses a shared object-oriented state abstraction to support building complex multimodal mobile applications. Having different modalities share the same state abstraction allows developers using ReactGenie to seamlessly integrate and compose these modalities to deliver multimodal interaction.  ReactGenie is a natural extension to the existing workflow of building a graphical app, like the workflow with React-Redux. Developers only have to add a few annotations and examples to indicate how natural language is mapped to the user-accessible
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#32463;&#39564;&#24615;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#26368;&#22810;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;Spoken&#29256;&#26412;&#30340;&#25991;&#26412;&#22522;&#20934;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#27169;&#22411;&#35780;&#20272;&#21644;&#25512;&#21160;&#26410;&#26469;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.13009</link><description>&lt;p&gt;
&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Textually Pretrained Speech Language Models. (arXiv:2305.13009v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#32463;&#39564;&#24615;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#26368;&#22810;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;Spoken&#29256;&#26412;&#30340;&#25991;&#26412;&#22522;&#20934;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#27169;&#22411;&#35780;&#20272;&#21644;&#25512;&#21160;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SpeechLMs&#65289;&#20165;&#22788;&#29702;&#21644;&#29983;&#25104;&#38899;&#39057;&#25968;&#25454;&#65292;&#27809;&#26377;&#25991;&#23383;&#30417;&#30563;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TWIST&#65292;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;SpeechLMs&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;TWIST&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#20248;&#20110;&#20919;&#21551;&#21160;&#30340;SpeechLM&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#65288;&#22914;&#35821;&#38899;&#20998;&#35789;&#22120;&#12289;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#22312;&#26500;&#24314;&#24615;&#33021;&#26356;&#22909;&#30340;SpeechLMs&#26041;&#38754;&#37117;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36804;&#20170;&#20026;&#27490;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#26368;&#22810;&#30340;SpeechLM&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;Spoken&#29256;&#26412;&#30340;StoryCloze&#25991;&#26412;&#22522;&#20934;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#27169;&#22411;&#35780;&#20272;&#24182;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;&#25105;&#20204;&#20844;&#24320;&#25552;&#20379;&#35821;&#38899;&#26679;&#26412;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#65306;https://pages.cs.huji.ac.il/
&lt;/p&gt;
&lt;p&gt;
Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.11863</link><description>&lt;p&gt;
&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#30340;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#21333;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#22823;&#33041;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#19982;&#22823;&#33041;&#30340;&#30740;&#31350;&#37117;&#20351;&#29992;&#20102;&#31867;&#20284;GPT-2&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#26159;&#21542;&#26356;&#22823;&#30340;&#24320;&#28304;&#27169;&#22411;&#65288;&#22914;OPT&#21644;LLaMA&#31995;&#21015;&#65289;&#26356;&#36866;&#29992;&#20110;&#39044;&#27979;&#20351;&#29992;fMRI&#35760;&#24405;&#30340;&#22823;&#33041;&#21453;&#24212;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20174;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#22823;&#33041;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#36328;3&#20010;&#21463;&#35797;&#32773;&#30340;&#20445;&#30041;&#27979;&#35797;&#38598;&#30456;&#20851;&#24615;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;&#24403;&#25193;&#23637;fMRI&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#26102;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#30340;&#23545;&#25968;&#32447;&#24615;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#23545;&#20351;&#29992;HuBERT&#65292;WavLM&#21644;Whisper&#30340;&#22768;&#23398;&#32534;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#24102;&#26469;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22122;&#38899;&#22825;&#33457;&#26495;&#20998;&#26512;&#20102;&#36825;&#20123;&#22823;&#35268;&#27169;&#19988;&#39640;&#24615;&#33021;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations from transformer-based unidirectional language models are known to be effective at predicting brain responses to natural language. However, most studies comparing language models to brains have used GPT-2 or similarly sized language models. Here we tested whether larger open-source models such as those from the OPT and LLaMA families are better at predicting brain responses recorded using fMRI. Mirroring scaling results from other contexts, we found that brain prediction performance scales log-linearly with model size from 125M to 30B parameter models, with ~15% increased encoding performance as measured by correlation with a held-out test set across 3 subjects. Similar log-linear behavior was observed when scaling the size of the fMRI training set. We also characterized scaling for acoustic encoding models that use HuBERT, WavLM, and Whisper, and we found comparable improvements with model size. A noise ceiling analysis of these large, high-performance encoding models 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35752;&#35770;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38590;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#36827;&#34892;&#23545;&#35805;&#24182;&#20462;&#27491;&#39044;&#27979;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#35752;&#35770;&#25552;&#39640;&#20934;&#30830;&#24615;&#39640;&#36798;25%&#12290;</title><link>http://arxiv.org/abs/2305.11789</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38590;&#39064;&#65306;&#19968;&#31181;&#22522;&#20110;&#35752;&#35770;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach. (arXiv:2305.11789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35752;&#35770;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38590;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#36827;&#34892;&#23545;&#35805;&#24182;&#20462;&#27491;&#39044;&#27979;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#35752;&#35770;&#25552;&#39640;&#20934;&#30830;&#24615;&#39640;&#36798;25%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#35752;&#35770;&#12289;&#35299;&#37322;&#24182;&#30456;&#20114;&#36190;&#21516;&#25110;&#21453;&#23545;&#31561;&#26041;&#24335;&#20849;&#21516;&#35299;&#20915;&#20849;&#21516;&#38382;&#39064;&#12290;&#21516;&#26679;&#65292;&#22914;&#26524;&#31995;&#32479;&#22312;&#35299;&#20915;&#20219;&#21153;&#26102;&#33021;&#19982;&#20154;&#31867;&#36827;&#34892;&#35752;&#35770;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#20043;&#21069;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20013;&#65292;&#31995;&#32479;&#21482;&#33021;&#20570;&#20986;&#39044;&#27979;&#65292;&#20154;&#31867;&#21482;&#33021;&#23601;&#36825;&#20123;&#39044;&#27979;&#25552;&#38382;&#65292;&#32780;&#27809;&#26377;&#24444;&#27492;&#38388;&#30340;&#24847;&#35265;&#20132;&#25442;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#20351;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#23545;&#35805;&#36827;&#34892;&#35752;&#35770;&#21644;&#20462;&#27491;&#39044;&#27979;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#19982;&#20154;&#31867;&#36827;&#34892;&#26377;&#30410;&#30340;&#35752;&#35770;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#39640;&#36798;25&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21024;&#38500;&#26631;&#20934;&#26469;&#35780;&#20272;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#65292;&#35813;&#26041;&#27861;&#38543;&#26426;&#36974;&#30422;&#26631;&#35760;&#30340;&#37096;&#20998;&#21521;&#37327;&#34920;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#30828;&#21024;&#38500;&#26631;&#20934;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.10496</link><description>&lt;p&gt;
&#34701;&#21512;&#24402;&#22240;&#37325;&#35201;&#24615;&#20197;&#25552;&#39640;&#24544;&#23454;&#24230;&#35780;&#20272;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incorporating Attribution Importance for Improving Faithfulness Metrics. (arXiv:2305.10496v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21024;&#38500;&#26631;&#20934;&#26469;&#35780;&#20272;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#65292;&#35813;&#26041;&#27861;&#38543;&#26426;&#36974;&#30422;&#26631;&#35760;&#30340;&#37096;&#20998;&#21521;&#37327;&#34920;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#30828;&#21024;&#38500;&#26631;&#20934;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#26159;&#25552;&#20379;&#23545;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#36827;&#34892;&#39044;&#27979;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#19968;&#20010;&#26356;&#21152;&#20934;&#30830;&#30340;&#24402;&#22240;&#26041;&#27861;&#26631;&#24535;&#30528;&#23427;&#26356;&#21152;&#24544;&#23454;&#65292;&#23427;&#21487;&#20197;&#26356;&#21152;&#20934;&#30830;&#22320;&#21453;&#26144;&#21738;&#20123;&#37096;&#20998;&#30340;&#36755;&#20837;&#23545;&#39044;&#27979;&#26356;&#21152;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24544;&#23454;&#24230;&#35780;&#20272;&#26041;&#27861;&#65292;&#22914;&#20805;&#20998;&#24615;&#21644;&#20840;&#38754;&#24615;&#65292;&#21482;&#20351;&#29992;&#19968;&#31181;&#30828;&#21024;&#38500;&#26631;&#20934;&#65292;&#21363;&#23436;&#20840;&#21024;&#38500;&#25110;&#20445;&#30041;&#30001;&#32473;&#23450;&#24402;&#22240;&#26041;&#27861;&#25490;&#21517;&#26368;&#39640;&#30340;&#39030;&#37096;&#26631;&#35760;&#65292;&#24182;&#35266;&#23519;&#39044;&#27979;&#21487;&#33021;&#24615;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#30828;&#21024;&#38500;&#26631;&#20934;&#24573;&#30053;&#20102;&#27599;&#20010;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#65292;&#25226;&#23427;&#20204;&#20840;&#37096;&#31561;&#21516;&#22320;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36719;&#21024;&#38500;&#26631;&#20934;&#12290;&#25105;&#20204;&#19981;&#20250;&#23436;&#20840;&#21024;&#38500;&#25110;&#20445;&#30041;&#36755;&#20837;&#20013;&#30340;&#26631;&#35760;&#65292;&#32780;&#26159;&#38543;&#26426;&#22320;&#36974;&#30422;&#20195;&#34920;&#24402;&#22240;&#26041;&#27861;&#37325;&#35201;&#24615;&#30340;&#37096;&#20998;&#26631;&#35760;&#21521;&#37327;&#34920;&#31034;&#12290;&#22522;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;&#19981;&#21516;&#30340;&#24402;&#22240;&#26041;&#27861;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods (FAs) are popular approaches for providing insights into the model reasoning process of making predictions. The more faithful a FA is, the more accurately it reflects which parts of the input are more important for the prediction. Widely used faithfulness metrics, such as sufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely removing or retaining the top most important tokens ranked by a given FA and observing the changes in predictive likelihood. However, this hard criterion ignores the importance of each individual token, treating them all equally for computing sufficiency and comprehensiveness. In this paper, we propose a simple yet effective soft erasure criterion. Instead of entirely removing or retaining tokens from the input, we randomly mask parts of the token vector representations proportionately to their FA importance. Extensive experiments across various natural language processing tasks and different FAs show that our sof
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10163</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20013;&#22269;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#19978;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;GPT&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;ChatGPT&#24050;&#34987;&#25972;&#21512;&#21040;&#21508;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#27969;&#20013;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#20854;&#24494;&#35843;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#65292;&#38459;&#30861;&#20102;&#20854;&#22312;&#38656;&#35201;&#24191;&#27867;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#35821;&#20041;&#30693;&#35782;&#30340;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#65288;CNMLE&#65289;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#65292;&#21363;&#20174;&#20004;&#20010;&#26041;&#38754;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#23558;&#21307;&#23398;&#32972;&#26223;&#30693;&#35782;&#25552;&#21462;&#20026;&#35821;&#20041;&#25351;&#20196;&#26469;&#25351;&#23548;ChatGPT&#30340;&#25512;&#26029;&#12290;&#31867;&#20284;&#22320;&#65292;&#30456;&#20851;&#30340;&#21307;&#30103;&#38382;&#39064;&#34987;&#35782;&#21035;&#24182;&#20316;&#20026;&#28436;&#31034;&#36755;&#20837;&#32473;ChatGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;ChatGPT&#26080;&#27861;&#22312;CNMLE&#19978;&#33719;&#24471;&#21512;&#26684;&#20998;&#25968;&#65288;51&#20998;&#65289;&#65292;&#21482;&#26377;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#35757;&#32451;&#30340;&#27169;&#22411;&#25104;&#21151;&#36890;&#36807;&#32771;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., onl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#30340;&#20027;&#21160;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#22810;&#31181;&#20027;&#21160;&#21644;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36951;&#24536;-&#23398;&#20064;&#26354;&#32447;&#26041;&#27861;&#26469;&#24179;&#34913;&#19981;&#24536;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#23398;&#20064;&#30340;&#20004;&#20010;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.03923</link><description>&lt;p&gt;
&#20027;&#21160;&#30340;&#36830;&#32493;&#23398;&#20064;&#65306;&#22312;&#20219;&#21153;&#24207;&#21015;&#20013;&#26631;&#35760;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Continual Learning: Labelling Queries in a Sequence of Tasks. (arXiv:2305.03923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#30340;&#20027;&#21160;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#22810;&#31181;&#20027;&#21160;&#21644;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36951;&#24536;-&#23398;&#20064;&#26354;&#32447;&#26041;&#27861;&#26469;&#24179;&#34913;&#19981;&#24536;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#23398;&#20064;&#30340;&#20004;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20013;&#65292;&#33719;&#21462;&#26032;&#30693;&#35782;&#24182;&#19981;&#24536;&#35760;&#24050;&#23398;&#20869;&#23481;&#26159;&#20854;&#26680;&#24515;&#12290;&#32780;&#20219;&#21153;&#26159;&#25353;&#39034;&#24207;&#20986;&#29616;&#30340;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#20934;&#22791;&#21644;&#27880;&#37322;&#21017;&#36890;&#24120;&#26159;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#36830;&#32493;&#23398;&#20064;&#26469;&#36866;&#24212;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#20219;&#21153;&#30340;&#20027;&#21160;&#36830;&#32493;&#23398;&#20064;&#65288;ACL&#65289;&#20013;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#20219;&#21153;&#21253;&#25324;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#27744;&#21644;&#19968;&#20010;&#27880;&#37322;&#39044;&#31639;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;AL&#21644;CL&#31639;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#65292;&#31867;&#21035;&#21644;&#20219;&#21153;&#22686;&#37327;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#25581;&#31034;&#20102;&#19981;&#24536;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#23398;&#20064;&#22312;CL&#21644;AL&#20013;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23613;&#31649;&#22312;&#20197;&#21069;&#20219;&#21153;&#30340;&#27880;&#37322;&#25910;&#38598;&#19978;&#26465;&#20214;&#26597;&#35810;&#31574;&#30053;&#20250;&#25552;&#39640;&#39046;&#22495;&#21644;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#36951;&#24536;-&#23398;&#20064;&#26354;&#32447;&#21017;&#26356;&#22909;&#22320;&#24179;&#34913;&#20102;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring new knowledge without forgetting what has been learned in a sequence of tasks is the central focus of continual learning (CL). While tasks arrive sequentially, the training data are often prepared and annotated independently, leading to CL of incoming supervised learning tasks. This paper considers the under-explored problem of active continual learning (ACL) for a sequence of active learning (AL) tasks, where each incoming task includes a pool of unlabelled data and an annotation budget. We investigate the effectiveness and interplay between several AL and CL algorithms in the domain, class and task-incremental scenarios. Our experiments reveal the trade-off between two contrasting goals of not forgetting the old knowledge and the ability to quickly learn in CL and AL. While conditioning the query strategy on the annotations collected for the previous tasks leads to improved task performance on the domain and task incremental learning, our proposed forgetting-learning profil
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#20316;&#20026;&#36328;&#35821;&#35328;&#26041;&#26696;&#21487;&#20197;&#20943;&#23569;&#32763;&#35793;&#35821;&#35328;&#23398;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.11501</link><description>&lt;p&gt;
&#20351;&#29992;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#20943;&#23569;&#32763;&#35793;&#35821;&#35328;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Translationese Reduction using Abstract Meaning Representation. (arXiv:2304.11501v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11501
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#20316;&#20026;&#36328;&#35821;&#35328;&#26041;&#26696;&#21487;&#20197;&#20943;&#23569;&#32763;&#35793;&#35821;&#35328;&#23398;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#25991;&#26412;&#25110;&#35805;&#35821;&#20855;&#26377;&#19982;&#26412;&#22320;&#35821;&#35328;&#19981;&#21516;&#30340;&#20960;&#20010;&#26126;&#26174;&#29305;&#24449;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#32763;&#35793;&#35821;&#35328;&#23398;&#65292;&#24050;&#32463;&#26377;&#20102;&#24456;&#22909;&#30340;&#25991;&#29486;&#35760;&#24405;&#65292;&#24403;&#20986;&#29616;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#38598;&#20013;&#26102;&#65292;&#20250;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#20943;&#23569;&#20154;&#24037;&#32763;&#35793;&#25991;&#26412;&#20013;&#32763;&#35793;&#35821;&#35328;&#23398;&#30340;&#30740;&#31350;&#24037;&#20316;&#36824;&#24456;&#23569;&#12290;&#25105;&#20204;&#20551;&#35774;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#20316;&#20026;&#19968;&#31181;&#20174;&#34920;&#38754;&#24418;&#24335;&#25277;&#35937;&#20986;&#26469;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#29992;&#20316;&#20943;&#23569;&#32763;&#35793;&#35821;&#35328;&#23398;&#25968;&#37327;&#30340;&#19968;&#31181;&#36328;&#35821;&#35328;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558;&#33521;&#25991;&#32763;&#35793;&#35299;&#26512;&#25104;AMR&#22270;&#65292;&#28982;&#21518;&#20174;&#35813;AMR&#29983;&#25104;&#25991;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26356;&#25509;&#36817;&#38750;&#32763;&#35793;&#35821;&#35328;&#25991;&#26412;&#30340;&#25991;&#26412;&#65292;&#36890;&#36807;&#23439;&#35266;&#32423;&#21035;&#30340;&#24230;&#37327;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;AMR&#20316;&#20026;&#36328;&#35821;&#35328;&#26041;&#26696;&#21487;&#20197;&#23454;&#29616;&#20943;&#23569;&#32763;&#35793;&#35821;&#35328;&#23398;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#21478;&#22806;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#22522;&#20110;&#24448;&#36820;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#21477;&#27861;&#25511;&#21046;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translated texts or utterances bear several hallmarks distinct from texts originating in the language. This phenomenon, known as translationese, is well-documented, and when found in training or test sets can affect model performance. Still, work to mitigate the effect of translationese in human translated text is understudied. We hypothesize that Abstract Meaning Representation (AMR), a semantic representation which abstracts away from the surface form, can be used as an interlingua to reduce the amount of translationese in translated texts. By parsing English translations into an AMR graph and then generating text from that AMR, we obtain texts that more closely resemble non-translationese by macro-level measures. We show that across four metrics, and qualitatively, using AMR as an interlingua enables the reduction of translationese and we compare our results to two additional approaches: one based on round-trip machine translation and one based on syntactically controlled generation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30740;&#31350;&#20102;&#30005;&#21147;&#38656;&#27714;&#21644;&#31038;&#20250;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#35789;&#39057;&#12289;&#20844;&#20247;&#24773;&#24863;&#12289;&#20027;&#39064;&#20998;&#24067;&#21644;&#35789;&#23884;&#20837;&#31561;&#25991;&#26412;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#27425;&#26085;&#30340;&#30005;&#21147;&#38656;&#27714;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#35777;&#23454;&#20102;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25913;&#36827;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.07535</link><description>&lt;p&gt;
&#26032;&#38395;&#21644;&#36127;&#33655;&#65306;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29992;&#20110;&#39044;&#27979;&#27425;&#26085;&#30005;&#21147;&#31995;&#32479;&#38656;&#27714;&#30340;&#37327;&#21270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
News and Load: A Quantitative Exploration of Natural Language Processing Applications for Forecasting Day-ahead Electricity System Demand. (arXiv:2301.07535v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30740;&#31350;&#20102;&#30005;&#21147;&#38656;&#27714;&#21644;&#31038;&#20250;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#35789;&#39057;&#12289;&#20844;&#20247;&#24773;&#24863;&#12289;&#20027;&#39064;&#20998;&#24067;&#21644;&#35789;&#23884;&#20837;&#31561;&#25991;&#26412;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#27425;&#26085;&#30340;&#30005;&#21147;&#38656;&#27714;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#35777;&#23454;&#20102;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25913;&#36827;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#38656;&#27714;&#19982;&#22825;&#27668;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#30830;&#35748;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#20551;&#26085;&#21644;&#37325;&#22823;&#20107;&#20214;&#31561;&#34892;&#20026;&#21644;&#31038;&#20250;&#22240;&#32032;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25104;&#29087;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#38656;&#27714;&#39044;&#27979;&#25216;&#26415;&#65292;&#25506;&#32034;&#20102;&#30005;&#21147;&#38656;&#27714;&#19982;&#26356;&#32454;&#33268;&#30340;&#31038;&#20250;&#20107;&#20214;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35789;&#39057;&#12289;&#20844;&#20247;&#24773;&#24863;&#12289;&#20027;&#39064;&#20998;&#24067;&#21644;&#35789;&#23884;&#20837;&#31561;&#25991;&#26412;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#27425;&#26085;&#39044;&#27979;&#12290;&#36825;&#20123;&#29305;&#24449;&#20013;&#21253;&#21547;&#20102;&#20840;&#29699;&#22823;&#27969;&#34892;&#12289;&#25919;&#27835;&#12289;&#22269;&#38469;&#20914;&#31361;&#12289;&#20132;&#36890;&#31561;&#31038;&#20250;&#20107;&#20214;&#12290;&#36890;&#36807;&#35752;&#35770;&#22240;&#26524;&#25928;&#24212;&#21644;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#20851;&#32852;&#26426;&#21046;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#21487;&#20197;&#20026;&#20256;&#32479;&#30340;&#30005;&#21147;&#38656;&#27714;&#20998;&#26512;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#65292;&#35777;&#23454;&#20102;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25913;&#36827;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between electricity demand and weather is well established in power systems, along with the importance of behavioral and social aspects such as holidays and significant events. This study explores the link between electricity demand and more nuanced information about social events. This is done using mature Natural Language Processing (NLP) and demand forecasting techniques. The results indicate that day-ahead forecasts are improved by textual features such as word frequencies, public sentiments, topic distributions, and word embeddings. The social events contained in these features include global pandemics, politics, international conflicts, transportation, etc. Causality effects and correlations are discussed to propose explanations for the mechanisms behind the links highlighted. This study is believed to bring a new perspective to traditional electricity demand analysis. It confirms the feasibility of improving forecasts from unstructured text, with potential conse
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MoleculeSTM&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#32467;&#26500;-&#25991;&#26412;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21270;&#23398;&#32467;&#26500;&#21644;&#25991;&#26412;&#25551;&#36848;&#65292;&#21487;&#20197;&#23454;&#29616;&#22522;&#20110;&#25991;&#26412;&#30340;&#26816;&#32034;&#21644;&#32534;&#36753;&#12290;&#36890;&#36807;&#26500;&#24314;&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#36827;&#34892;&#39564;&#35777;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#24320;&#25918;&#35789;&#27719;&#21644;&#32452;&#21512;&#24615;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10789</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#20998;&#23376;&#32467;&#26500;-&#25991;&#26412;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#26816;&#32034;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing. (arXiv:2212.10789v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MoleculeSTM&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#32467;&#26500;-&#25991;&#26412;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21270;&#23398;&#32467;&#26500;&#21644;&#25991;&#26412;&#25551;&#36848;&#65292;&#21487;&#20197;&#23454;&#29616;&#22522;&#20110;&#25991;&#26412;&#30340;&#26816;&#32034;&#21644;&#32534;&#36753;&#12290;&#36890;&#36807;&#26500;&#24314;&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#36827;&#34892;&#39564;&#35777;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#24320;&#25918;&#35789;&#27719;&#21644;&#32452;&#21512;&#24615;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#20013;&#27491;&#22312;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#21033;&#29992;&#20998;&#23376;&#30340;&#21270;&#23398;&#32467;&#26500;&#65292;&#24573;&#35270;&#20102;&#21270;&#23398;&#39046;&#22495;&#20013;&#21487;&#29992;&#30340;&#20016;&#23500;&#25991;&#26412;&#30693;&#35782;&#12290;&#23558;&#25991;&#26412;&#30693;&#35782;&#32435;&#20837;&#32771;&#34385;&#21487;&#20197;&#23454;&#29616;&#26032;&#30340;&#33647;&#29289;&#35774;&#35745;&#30446;&#26631;&#65292;&#36866;&#24212;&#22522;&#20110;&#25991;&#26412;&#30340;&#25351;&#23548;&#21644;&#39044;&#27979;&#22797;&#26434;&#30340;&#29983;&#29289;&#27963;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#30340;&#20998;&#23376;&#32467;&#26500;-&#25991;&#26412;&#27169;&#22411;MoleculeSTM&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#20998;&#23376;&#30340;&#21270;&#23398;&#32467;&#26500;&#21644;&#25991;&#26412;&#25551;&#36848;&#26469;&#23454;&#29616;&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#12290;&#20026;&#20102;&#35757;&#32451;MoleculeSTM&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;PubChemSTM&#65292;&#21253;&#21547;&#36229;&#36807;28&#19975;&#20010;&#21270;&#23398;&#32467;&#26500;-&#25991;&#26412;&#23545;&#12290;&#20026;&#20102;&#23637;&#31034;MoleculeSTM&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#22522;&#20110;&#25991;&#26412;&#25351;&#20196;&#30340;&#25361;&#25112;&#24615;&#38646;&#26679;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#32467;&#26500;-&#25991;&#26412;&#26816;&#32034;&#21644;&#20998;&#23376;&#32534;&#36753;&#12290;MoleculeSTM&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#29305;&#24615;&#65306;&#24320;&#25918;&#35789;&#27719;&#21644;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23454;&#29616;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing adoption of artificial intelligence in drug discovery. However, existing studies use machine learning to mainly utilize the chemical structures of molecules but ignore the vast textual knowledge available in chemistry. Incorporating textual knowledge enables us to realize new drug design objectives, adapt to text-based instructions and predict complex biological activities. Here we present a multi-modal molecule structure-text model, MoleculeSTM, by jointly learning molecules' chemical structures and textual descriptions via a contrastive learning strategy. To train MoleculeSTM, we construct a large multi-modal dataset, namely, PubChemSTM, with over 280,000 chemical structure-text pairs. To demonstrate the effectiveness and utility of MoleculeSTM, we design two challenging zero-shot tasks based on text instructions, including structure-text retrieval and molecule editing. MoleculeSTM has two main properties: open vocabulary and compositionality via natural language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20998;&#26512;&#20102;&#29616;&#26377;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#21644;&#20165;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#21457;&#29616;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#22312;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#20165;&#35270;&#35273;&#27169;&#22411;&#22312;&#38656;&#35201;&#26356;&#23616;&#37096;&#20449;&#24687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#24378;&#12290;</title><link>http://arxiv.org/abs/2212.00281</link><description>&lt;p&gt;
&#23450;&#20301; vs. &#35821;&#20041;&#65306;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Localization vs. Semantics: Visual Representations in Unimodal and Multimodal Models. (arXiv:2212.00281v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20998;&#26512;&#20102;&#29616;&#26377;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#21644;&#20165;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#21457;&#29616;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#22312;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#20165;&#35270;&#35273;&#27169;&#22411;&#22312;&#38656;&#35201;&#26356;&#23616;&#37096;&#20449;&#24687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36890;&#36807;&#35270;&#35273;&#19982;&#35821;&#35328;&#39044;&#35757;&#32451;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#32852;&#21512;&#23398;&#20064;&#33539;&#24335;&#26159;&#21542;&#26377;&#21161;&#20110;&#29702;&#35299;&#27599;&#20010;&#20010;&#20307;&#27169;&#24577;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#31350;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#23545;&#27604;&#20998;&#26512;&#29616;&#26377;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#21644;&#20165;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#26088;&#22312;&#20197;&#32454;&#33268;&#30340;&#26041;&#24335;&#35780;&#20272;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#35266;&#23519;&#34920;&#26126;&#65292;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#22312;&#26631;&#31614;&#39044;&#27979;&#20219;&#21153;&#65288;&#22914;&#23545;&#35937;&#21644;&#23646;&#24615;&#39044;&#27979;&#65289;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#20165;&#35270;&#35273;&#27169;&#22411;&#22312;&#38656;&#35201;&#26356;&#23616;&#37096;&#20449;&#24687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#24378;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#33021;&#22815;&#38416;&#26126;&#35821;&#35328;&#22312;&#35270;&#35273;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25104;&#20026;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32463;&#39564;&#25351;&#21335;&#12290;&#20195;&#30721;&#23558;&#22312;https://github.com/Lizw14/visual_probing&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive advancements achieved through vision-and-language pretraining, it remains unclear whether this joint learning paradigm can help understand each individual modality. In this work, we conduct a comparative analysis of the visual representations in existing vision-and-language models and vision-only models by probing a broad range of tasks, aiming to assess the quality of the learned representations in a nuanced manner. Interestingly, our empirical observations suggest that vision-and-language models are better at label prediction tasks like object and attribute prediction, while vision-only models are stronger at dense prediction tasks that require more localized information. We hope our study sheds light on the role of language in visual learning, and serves as an empirical guide for various pretrained models. Code will be released at https://github.com/Lizw14/visual_probing
&lt;/p&gt;</description></item></channel></rss>