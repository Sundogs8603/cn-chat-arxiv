<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#19968;&#32452;&#39640;&#24230;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#22810;&#20041;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08600</link><description>&lt;p&gt;
&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Sparse Autoencoders Find Highly Interpretable Features in Language Models. (arXiv:2309.08600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#19968;&#32452;&#39640;&#24230;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#22810;&#20041;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#29702;&#35299;&#30340;&#19968;&#20010;&#38556;&#30861;&#26159;&#22810;&#20041;&#24615;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#22312;&#22810;&#20010;&#35821;&#20041;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#28608;&#27963;&#12290;&#22810;&#20041;&#24615;&#20351;&#25105;&#20204;&#26080;&#27861;&#25214;&#21040;&#31616;&#27905;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#24037;&#20316;&#12290;&#22810;&#20041;&#24615;&#30340;&#19968;&#20010;&#29468;&#27979;&#21407;&#22240;&#26159;&#21472;&#21152;&#25928;&#24212;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#37197;&#32473;&#28608;&#27963;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#36807;&#23436;&#22791;&#26041;&#21521;&#38598;&#21512;&#65292;&#32780;&#19981;&#26159;&#20010;&#21035;&#31070;&#32463;&#20803;&#65292;&#34920;&#31034;&#26356;&#22810;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#30830;&#23450;&#36825;&#20123;&#26041;&#21521;&#65292;&#20197;&#37325;&#26500;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#12290;&#36825;&#20123;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#30340;&#19968;&#32452;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#27604;&#20854;&#20182;&#26041;&#27861;&#37492;&#23450;&#20986;&#30340;&#26041;&#21521;&#26356;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#65292;&#35299;&#37322;&#24615;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#34913;&#37327;&#30340;&#12290;&#21024;&#38500;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#20363;&#22914;&#36890;&#36807;&#21024;&#38500;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the roadblocks to a better understanding of neural networks' internals is \textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Ablating these features enables precise model editing, for example, by remo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22806;&#37096;&#24178;&#25200;&#23545;&#21442;&#25968;&#21270;&#30693;&#35782;&#22270;&#35889;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#31243;&#24230;&#12289;&#26041;&#27861;&#12289;&#20301;&#32622;&#21644;&#26684;&#24335;&#30340;&#24178;&#25200;&#22240;&#32032;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#20135;&#29983;&#19982;&#20854;&#21442;&#25968;&#21270;&#30693;&#35782;&#19981;&#19968;&#33268;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.08594</link><description>&lt;p&gt;
"&#21512;&#24182;&#20914;&#31361;&#65281;&#25506;&#32034;&#22806;&#37096;&#24178;&#25200;&#23545;&#21442;&#25968;&#21270;&#30693;&#35782;&#22270;&#35889;&#30340;&#24433;&#21709;"
&lt;/p&gt;
&lt;p&gt;
"Merge Conflicts!" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs. (arXiv:2309.08594v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22806;&#37096;&#24178;&#25200;&#23545;&#21442;&#25968;&#21270;&#30693;&#35782;&#22270;&#35889;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#31243;&#24230;&#12289;&#26041;&#27861;&#12289;&#20301;&#32622;&#21644;&#26684;&#24335;&#30340;&#24178;&#25200;&#22240;&#32032;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#20135;&#29983;&#19982;&#20854;&#21442;&#25968;&#21270;&#30693;&#35782;&#19981;&#19968;&#33268;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#21462;&#20102;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#31216;&#20026;&#23427;&#20204;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#25345;&#19982;&#20154;&#31867;&#25351;&#20196;&#30340;&#19968;&#33268;&#24182;&#19982;&#26102;&#20465;&#36827;&#65292;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#36807;&#31243;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#30340;&#25903;&#25345;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#24403;&#22806;&#37096;&#30693;&#35782;&#24178;&#25200;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#65292;LLMs&#23558;&#22914;&#20309;&#20570;&#20986;&#21453;&#24212;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#31995;&#32479;&#22320;&#25366;&#25496;LLMs&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#24182;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#21442;&#25968;&#21270;&#30693;&#35782;&#22270;&#35889;&#26469;&#25581;&#31034;LLMs&#30340;&#19981;&#21516;&#30693;&#35782;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#31243;&#24230;&#12289;&#26041;&#27861;&#12289;&#20301;&#32622;&#21644;&#26684;&#24335;&#30340;&#24178;&#25200;&#22240;&#32032;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#40657;&#30418;&#21644;&#24320;&#28304;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLMs&#20542;&#21521;&#20110;&#20135;&#29983;&#19982;&#20854;&#21442;&#25968;&#21270;&#30693;&#35782;&#19981;&#19968;&#33268;&#30340;&#22238;&#22797;&#65292;&#29305;&#21035;&#26159;&#22312;&#36935;&#21040;&#30452;&#25509;&#20914;&#31361;&#25110;&#28151;&#28102;&#21464;&#21270;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) acquire extensive knowledge during pre-training, known as their parametric knowledge. However, in order to remain up-to-date and align with human instructions, LLMs inevitably require external knowledge during their interactions with users. This raises a crucial question: How will LLMs respond when external knowledge interferes with their parametric knowledge? To investigate this question, we propose a framework that systematically elicits LLM parametric knowledge and introduces external knowledge. Specifically, we uncover the impacts by constructing a parametric knowledge graph to reveal the different knowledge structures of LLMs, and introduce external knowledge through distractors of varying degrees, methods, positions, and formats. Our experiments on both black-box and open-source models demonstrate that LLMs tend to produce responses that deviate from their parametric knowledge, particularly when they encounter direct conflicts or confounding changes o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLMs&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#36816;&#29992;&#35866;&#35821;&#21644;&#20439;&#35821;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;mLLMs&#22312;&#29702;&#35299;&#27604;&#21947;&#24615;&#35866;&#35821;&#12289;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#21644;&#25512;&#29702;&#20854;&#20182;&#35821;&#35328;&#30340;&#35866;&#35821;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.08591</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;LLMs&#26159;&#21542;&#20855;&#26377;&#25991;&#21270;&#22810;&#26679;&#24615;&#30340;&#25512;&#29702;&#33021;&#21147;&#65311;&#23545;&#22810;&#20803;&#25991;&#21270;&#35866;&#35821;&#21644;&#20439;&#35821;&#30340;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings. (arXiv:2309.08591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLMs&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#36816;&#29992;&#35866;&#35821;&#21644;&#20439;&#35821;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;mLLMs&#22312;&#29702;&#35299;&#27604;&#21947;&#24615;&#35866;&#35821;&#12289;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#21644;&#25512;&#29702;&#20854;&#20182;&#35821;&#35328;&#30340;&#35866;&#35821;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#31572;&#21644;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#25797;&#38271;&#65292;&#20294;&#22312;&#24773;&#22659;&#32972;&#26223;&#19979;&#36827;&#34892;&#25512;&#29702;&#26102;&#65292;&#20154;&#31867;&#30340;&#26399;&#26395;&#22240;&#30456;&#20851;&#25991;&#21270;&#20849;&#21516;&#28857;&#32780;&#24322;&#12290;&#30001;&#20110;&#20154;&#31867;&#35821;&#35328;&#19982;&#22810;&#31181;&#25991;&#21270;&#30456;&#20851;&#32852;&#65292;LLMs&#20063;&#24212;&#35813;&#26159;&#20855;&#26377;&#25991;&#21270;&#22810;&#26679;&#24615;&#30340;&#25512;&#29702;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;LLMs&#65288;mLLMs&#65289;&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#36816;&#29992;&#35866;&#35821;&#21644;&#20439;&#35821;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;1&#65289;mLLMs&#21482;&#8220;&#30693;&#36947;&#8221;&#26377;&#38480;&#30340;&#35866;&#35821;&#65292;&#24182;&#19988;&#20165;&#20165;&#35760;&#20303;&#35866;&#35821;&#24182;&#19981;&#33021;&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#29702;&#35299;&#23427;&#20204;&#65307;&#65288;2&#65289;mLLMs&#22312;&#25512;&#29702;&#27604;&#21947;&#24615;&#30340;&#35866;&#35821;&#21644;&#20439;&#35821;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#24403;&#34987;&#35201;&#27714;&#36873;&#25321;&#38169;&#35823;&#31572;&#26696;&#26102;&#65292;&#32780;&#19981;&#26159;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#65307;&#65288;3&#65289;&#22312;&#25512;&#29702;&#26469;&#33258;&#20854;&#20182;&#35821;&#35328;&#30340;&#35866;&#35821;&#21644;&#20439;&#35821;&#26102;&#65292;mLLMs&#23384;&#22312;&#8220;&#25991;&#21270;&#24046;&#36317;&#8221;&#12290;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;MAPS&#65288;&#22810;&#20803;&#25991;&#21270;&#35866;&#35821;&#21644;&#20439;&#35821;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are highly adept at question answering and reasoning tasks, but when reasoning in situational context, human expectations vary depending on the relevant cultural common ground. As human languages are associated with diverse cultures, LLMs should also be culturally-diverse reasoners. In this paper, we study the ability of a wide range of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and sayings in a conversational context. Our experiments reveal that: (1) mLLMs 'knows' limited proverbs and memorizing proverbs does not mean understanding them within a conversational context; (2) mLLMs struggle to reason with figurative proverbs and sayings, and when asked to select the wrong answer (instead of asking it to select the correct answer); and (3) there is a "culture gap" in mLLMs when reasoning about proverbs and sayings translated from other languages. We construct and release our evaluation dataset MAPS (MulticultrAl Proverbs and Sayings) fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21482;&#38656;&#36827;&#34892;&#24494;&#35843;&#23601;&#21487;&#20197;&#35757;&#32451;&#19968;&#31181;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#22791;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20219;&#21153;&#30340;&#25928;&#26524;&#65292;&#24182;&#36229;&#36807;&#20102;&#20256;&#32479;&#30417;&#30563;&#25216;&#26415;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.08590</link><description>&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#23398;&#20250;&#25104;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation Models Can Learn to be Few-shot Learners. (arXiv:2309.08590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21482;&#38656;&#36827;&#34892;&#24494;&#35843;&#23601;&#21487;&#20197;&#35757;&#32451;&#19968;&#31181;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#22791;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#36827;&#34892;&#33258;&#36866;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20219;&#21153;&#30340;&#25928;&#26524;&#65292;&#24182;&#36229;&#36807;&#20102;&#20256;&#32479;&#30417;&#30563;&#25216;&#26415;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#26159;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#23398;&#20064;&#22312;&#26032;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20063;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#26356;&#23567;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21521;&#19987;&#38376;&#30340;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;ICL&#30340;&#35757;&#32451;&#65292;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20219;&#21153;&#19978;&#36827;&#34892;&#28436;&#31034;&#12290;&#36890;&#36807;ICL&#30340;&#33021;&#21147;&#65292;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#30456;&#20851;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#35843;&#25972;&#20854;&#36755;&#20986;&#20197;&#36866;&#24212;&#35813;&#39046;&#22495;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#30417;&#30563;&#25216;&#26415;&#20197;&#21450;&#20855;&#26377;40B&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;ICL&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#25209;&#37327;&#25512;&#29702;&#65292;&#24182;&#22312;&#32763;&#35793;&#36136;&#37327;&#21644;&#21363;&#26102;&#36866;&#24212;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#21363;&#22312;&#23637;&#31034;&#21333;&#20010;&#31034;&#20363;&#21518;&#33021;&#22815;&#37325;&#29616;&#29305;&#23450;&#26415;&#35821;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent ability of Large Language Models to use a small number of examples to learn to perform in novel domains and tasks, also called in-context learning (ICL). In this work, we show that a much smaller model can be trained to perform ICL by fine-tuning towards a specialized training objective, exemplified on the task of domain adaptation for neural machine translation. With this capacity for ICL, the model can take advantage of relevant few-shot examples to adapt its output towards the domain. We compare the quality of this domain adaptation to traditional supervised techniques and ICL with a 40B-parameter Large Language Model. Our approach allows efficient batch inference on a mix of domains and outperforms state-of-the-art baselines in terms of both translation quality and immediate adaptation rate, i.e. the ability to reproduce a specific term after being shown a single example.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SECToR&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#65292;</title><link>http://arxiv.org/abs/2309.08589</link><description>&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#26159;&#19968;&#31181;&#31574;&#30053;&#25913;&#36827;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Reasoning is a Policy Improvement Operator. (arXiv:2309.08589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08589
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SECToR&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20854;&#20196;&#20154;&#36190;&#21497;&#30340;&#26032;&#33021;&#21147;&#20196;&#19990;&#30028;&#20026;&#20043;&#24778;&#21497;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30446;&#21069;&#32570;&#20047;&#33258;&#25105;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#25509;&#21463;&#22823;&#37327;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SECToR&#65288;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#23454;&#29616;&#33258;&#25105;&#25945;&#32946;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#12290;&#21463;&#21040;&#20197;&#21069;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;Silver&#31561;&#20154;&#65292;2017&#65289;&#21644;&#20154;&#31867;&#35748;&#30693;&#65288;Kahneman&#65292;2011&#65289;&#20013;&#30340;&#30456;&#20851;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;SECToR&#39318;&#20808;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#36880;&#28176;&#24605;&#32771;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;SECToR&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#29983;&#25104;&#30456;&#21516;&#30340;&#31572;&#26696;&#65292;&#36825;&#27425;&#19981;&#20877;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#12290;&#36890;&#36807;SECToR&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#23398;&#20250;&#20102;&#36827;&#34892;&#22810;&#36798;29&#20301;&#25968;&#23383;&#30340;&#21152;&#27861;&#36816;&#31639;&#65292;&#32780;&#27809;&#26377;&#20219;&#20309;&#36229;&#36807;6&#20301;&#25968;&#23383;&#30340;&#22522;&#20934;&#30495;&#23454;&#31034;&#20363;&#65292;&#20165;&#36890;&#36807;&#21021;&#22987;&#30340;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Large language models have astounded the world with fascinating new capabilities. However, they currently lack the ability to teach themselves new skills, relying instead on being trained on large amounts of human-generated data. We introduce SECToR (Self-Education via Chain-of-Thought Reasoning), a proof-of-concept demonstration that language models can successfully teach themselves new skills using chain-of-thought reasoning. Inspired by previous work in both reinforcement learning (Silver et al., 2017) and human cognition (Kahneman, 2011), SECToR first uses chain-of-thought reasoning to slowly think its way through problems. SECToR then fine-tunes the model to generate those same answers, this time without using chain-of-thought reasoning. Language models trained via SECToR autonomously learn to add up to 29-digit numbers without any access to any ground truth examples beyond an initial supervised fine-tuning phase consisting only of numbers with 6 or fewer digits. Our central hypot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#27169;&#22411;&#33976;&#39311;&#22686;&#24378;&#21644;&#25913;&#36827;&#27491;&#24335;&#39118;&#26684;&#36716;&#31227;&#25968;&#25454;&#38598;&#30340;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#19987;&#23478;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.08583</link><description>&lt;p&gt;
ICLEF: &#22522;&#20110;&#19987;&#23478;&#21453;&#39304;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#21487;&#35299;&#37322;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer. (arXiv:2309.08583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#27169;&#22411;&#33976;&#39311;&#22686;&#24378;&#21644;&#25913;&#36827;&#27491;&#24335;&#39118;&#26684;&#36716;&#31227;&#25968;&#25454;&#38598;&#30340;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#19987;&#23478;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#21069;&#30340;&#24037;&#20316;&#27809;&#26377;&#35299;&#20915;&#39118;&#26684;&#36716;&#31227;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#35299;&#37322;&#65292;&#20294;&#26159;&#24403;&#23384;&#22312;&#26356;&#23567;&#12289;&#24191;&#27867;&#20998;&#24067;&#19988;&#36879;&#26126;&#30340;&#26367;&#20195;&#21697;&#26102;&#65292;&#20351;&#29992;&#36825;&#26679;&#22797;&#26434;&#30340;&#31995;&#32479;&#26159;&#20302;&#25928;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;ChatGPT&#20013;&#36827;&#34892;&#27169;&#22411;&#33976;&#39311;&#26469;&#22686;&#24378;&#21644;&#25913;&#36827;&#19968;&#20010;&#27491;&#24335;&#39118;&#26684;&#36716;&#31227;&#25968;&#25454;&#38598;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#21892;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#65292;&#21363;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICLEF:&#22522;&#20110;&#19987;&#23478;&#21453;&#39304;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#65292;&#20351;ChatGPT&#20805;&#24403;&#20854;&#33258;&#36523;&#36755;&#20986;&#30340;&#35780;&#35770;&#32773;&#12290;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;9960&#20010;&#21487;&#35299;&#37322;&#30340;&#27491;&#24335;&#39118;&#26684;&#36716;&#31227;&#23454;&#20363;&#65288;e-GYAFC&#65289;&#30340;&#25968;&#25454;&#38598;&#26469;&#23637;&#31034;&#24403;&#21069;&#20844;&#24320;&#20998;&#21457;&#30340;&#32463;&#36807;&#25351;&#23548;&#30340;&#27169;&#22411;&#65288;&#22312;&#26576;&#20123;&#35774;&#32622;&#20013;&#21253;&#25324;ChatGPT&#65289;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#25105;&#20204;&#30340;&#39640;-
&lt;/p&gt;
&lt;p&gt;
While state-of-the-art language models excel at the style transfer task, current work does not address explainability of style transfer systems. Explanations could be generated using large language models such as GPT-3.5 and GPT-4, but the use of such complex systems is inefficient when smaller, widely distributed, and transparent alternatives are available. We propose a framework to augment and improve a formality style transfer dataset with explanations via model distillation from ChatGPT. To further refine the generated explanations, we propose a novel way to incorporate scarce expert human feedback using in-context learning (ICLEF: In-Context Learning from Expert Feedback) by prompting ChatGPT to act as a critic to its own outputs. We use the resulting dataset of 9,960 explainable formality style transfer instances (e-GYAFC) to show that current openly distributed instruction-tuned models (and, in some settings, ChatGPT) perform poorly on the task, and that fine-tuning on our high-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37327;&#21270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21360;&#24230;&#21644;&#35199;&#26041;&#19978;&#30340;&#38472;&#35268;&#20559;&#35265;&#24046;&#24322;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#31181;&#22995;&#21644;&#23447;&#25945;&#19978;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#30740;&#31350;&#21457;&#29616;&#22823;&#22810;&#25968;&#27979;&#35797;&#30340;&#27169;&#22411;&#22312;&#21360;&#24230;&#32972;&#26223;&#19979;&#23545;&#21051;&#26495;&#21360;&#35937;&#26377;&#26174;&#33879;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#19982;&#35199;&#26041;&#32972;&#26223;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#31616;&#21333;&#24178;&#39044;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08573</link><description>&lt;p&gt;
&#21360;&#24230;&#20063;&#23384;&#22312;&#31181;&#22995;&#20027;&#20041;&#20294;&#19981;&#23384;&#22312;&#31181;&#26063;&#20027;&#20041;&#21527;&#65311;&#37327;&#21270;&#21360;&#24230;&#21644;&#35199;&#26041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Casteist but Not Racist? Quantifying Disparities in Large Language Model Bias between India and the West. (arXiv:2309.08573v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37327;&#21270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21360;&#24230;&#21644;&#35199;&#26041;&#19978;&#30340;&#38472;&#35268;&#20559;&#35265;&#24046;&#24322;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#31181;&#22995;&#21644;&#23447;&#25945;&#19978;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#30740;&#31350;&#21457;&#29616;&#22823;&#22810;&#25968;&#27979;&#35797;&#30340;&#27169;&#22411;&#22312;&#21360;&#24230;&#32972;&#26223;&#19979;&#23545;&#21051;&#26495;&#21360;&#35937;&#26377;&#26174;&#33879;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#19982;&#35199;&#26041;&#32972;&#26223;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#31616;&#21333;&#24178;&#39044;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29616;&#22312;&#27599;&#22825;&#34987;&#25968;&#30334;&#19975;&#29992;&#25143;&#20351;&#29992;&#65292;&#20182;&#20204;&#33021;&#22815;&#20256;&#36798;&#31038;&#20250;&#20559;&#35265;&#65292;&#20351;&#29992;&#25143;&#36973;&#21463;&#20877;&#29616;&#20260;&#23475;&#12290;&#24050;&#26377;&#22823;&#37327;&#30340;&#20851;&#20110;LLM&#20559;&#35265;&#30340;&#23398;&#26415;&#30740;&#31350;&#23384;&#22312;&#65292;&#20294;&#20027;&#35201;&#37319;&#29992;&#35199;&#26041;&#20013;&#24515;&#35270;&#35282;&#65292;&#30456;&#23545;&#36739;&#23569;&#20851;&#27880;&#20840;&#29699;&#21335;&#26041;&#22320;&#21306;&#30340;&#20559;&#35265;&#27700;&#24179;&#21644;&#28508;&#22312;&#20260;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#27969;&#34892;LLMs&#20013;&#30340;&#38472;&#35268;&#20559;&#35265;&#65292;&#37319;&#29992;&#20197;&#21360;&#24230;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#24182;&#27604;&#36739;&#21360;&#24230;&#21644;&#35199;&#26041;&#32972;&#26223;&#19979;&#30340;&#20559;&#35265;&#27700;&#24179;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;Indian-BhED&#65288;&#21360;&#24230;&#20559;&#35265;&#35780;&#20272;&#25968;&#25454;&#38598;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#31181;&#22995;&#21644;&#23447;&#25945;&#19978;&#30340;&#21051;&#26495;&#21644;&#21453;&#21051;&#26495;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21360;&#24230;&#32972;&#26223;&#19979;&#65292;&#22823;&#22810;&#25968;&#27979;&#35797;&#30340;LLMs&#23545;&#21051;&#26495;&#21360;&#35937;&#26377;&#24378;&#28872;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#19982;&#35199;&#26041;&#32972;&#26223;&#30456;&#27604;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Instruction Prompting&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#24178;&#39044;&#25163;&#27573;&#26469;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#21051;&#26495;&#21360;&#35937;&#21644;&#21453;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), now used daily by millions of users, can encode societal biases, exposing their users to representational harms. A large body of scholarship on LLM bias exists but it predominantly adopts a Western-centric frame and attends comparatively less to bias levels and potential harms in the Global South. In this paper, we quantify stereotypical bias in popular LLMs according to an Indian-centric frame and compare bias levels between the Indian and Western contexts. To do this, we develop a novel dataset which we call Indian-BhED (Indian Bias Evaluation Dataset), containing stereotypical and anti-stereotypical examples for caste and religion contexts. We find that the majority of LLMs tested are strongly biased towards stereotypes in the Indian context, especially as compared to the Western context. We finally investigate Instruction Prompting as a simple intervention to mitigate such bias and find that it significantly reduces both stereotypical and anti-stereoty
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#36801;&#31227;&#21040;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#19981;&#21516;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08565</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#33021;&#21542;&#36801;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Transferable are Attribute Controllers on Pretrained Multilingual Translation Models?. (arXiv:2309.08565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#25511;&#21046;&#22120;&#36801;&#31227;&#21040;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#19981;&#21516;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23450;&#21046;&#20026;&#31526;&#21512;&#32454;&#31890;&#24230;&#23646;&#24615;&#65288;&#22914;&#24418;&#24335;&#65289;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#33267;&#23569;&#19968;&#20123;&#24102;&#26377;&#23646;&#24615;&#27880;&#37322;&#30340;&#30417;&#30563;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#31232;&#32570;&#20173;&#28982;&#26159;&#23558;&#27492;&#23450;&#21046;&#33021;&#21147;&#26222;&#21450;&#21040;&#26356;&#24191;&#27867;&#35821;&#35328;&#33539;&#22260;&#65292;&#23588;&#20854;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#19968;&#20010;&#29942;&#39048;&#12290;&#37492;&#20110;&#26368;&#36817;&#22312;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20316;&#20026;&#23545;&#27809;&#26377;&#30417;&#30563;&#25968;&#25454;&#30340;&#35821;&#35328;&#36827;&#34892;&#23646;&#24615;&#25511;&#21046;&#33021;&#21147;&#36801;&#31227;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;NLLB-200&#27169;&#22411;&#23545;&#23646;&#24615;&#25511;&#21046;&#22120;&#30340;&#36801;&#31227;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#26102;&#25511;&#21046;&#25216;&#26415;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#39046;&#22495;&#40065;&#26834;&#24615;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#20004;&#31181;&#33539;&#24335;&#26159;&#20114;&#34917;&#30340;&#65292;&#36890;&#36807;&#19968;&#33268;&#30340;&#25913;&#36827;&#26469;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customizing machine translation models to comply with fine-grained attributes such as formality has seen tremendous progress recently. However, current approaches mostly rely on at least some supervised data with attribute annotation. Data scarcity therefore remains a bottleneck to democratizing such customization possibilities to a wider range of languages, lower-resource ones in particular. Given recent progress in pretrained massively multilingual translation models, we use them as a foundation to transfer the attribute controlling capabilities to languages without supervised data. In this work, we present a comprehensive analysis of transferring attribute controllers based on a pretrained NLLB-200 model. We investigate both training- and inference-time control techniques under various data scenarios, and uncover their relative strengths and weaknesses in zero-shot performance and domain robustness. We show that both paradigms are complementary, as shown by consistent improvements o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;S4&#65289;&#19982;&#21367;&#31215;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#22312;&#32447;&#35821;&#38899;&#35782;&#21035;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;Librispeech&#30340;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#20302;&#30340;&#35782;&#21035;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.08551</link><description>&lt;p&gt;
&#22312;&#32447;&#35821;&#38899;&#35782;&#21035;&#20013;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22686;&#24378;&#26500;&#22411;
&lt;/p&gt;
&lt;p&gt;
Augmenting conformers with structured state space models for online speech recognition. (arXiv:2309.08551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;S4&#65289;&#19982;&#21367;&#31215;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#22312;&#32447;&#35821;&#38899;&#35782;&#21035;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;Librispeech&#30340;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#20302;&#30340;&#35782;&#21035;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35821;&#38899;&#35782;&#21035;&#26159;&#19968;&#31181;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;ASR&#31995;&#32479;&#24212;&#29992;&#22330;&#26223;&#65292;&#20854;&#20013;&#27169;&#22411;&#21482;&#33021;&#35775;&#38382;&#24038;&#20391;&#19978;&#19979;&#25991;&#12290;&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;S4&#65289;&#22686;&#24378;&#22312;&#32447;ASR&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;S4&#26159;&#19968;&#31867;&#25552;&#20379;&#20102;&#35775;&#38382;&#20219;&#24847;&#38271;&#24038;&#20391;&#19978;&#19979;&#25991;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#24335;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;S4&#27169;&#22411;&#30340;&#21508;&#20010;&#21464;&#31181;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#23558;&#20854;&#19982;&#21367;&#31215;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#26377;&#25928;&#30340;&#35774;&#35745;&#26159;&#20351;&#29992;&#20855;&#26377;&#23454;&#20540;&#24490;&#29615;&#26435;&#37325;&#30340;&#23567;&#22411;S4&#27169;&#22411;&#19982;&#26412;&#22320;&#21367;&#31215;&#30456;&#21472;&#21152;&#65292;&#20351;&#23427;&#20204;&#21487;&#20197;&#20114;&#34917;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#26469;&#33258;Librispeech&#30340;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;4.01%/8.53%&#30340;WER&#65292;&#20248;&#20110;&#32463;&#36807;&#35814;&#32454;&#35843;&#20248;&#30340;Conformers&#12290;
&lt;/p&gt;
&lt;p&gt;
Online speech recognition, where the model only accesses context to the left, is an important and challenging use case for ASR systems. In this work, we investigate augmenting neural encoders for online ASR by incorporating structured state-space sequence models (S4), which are a family of models that provide a parameter-efficient way of accessing arbitrarily long left context. We perform systematic ablation studies to compare variants of S4 models and propose two novel approaches that combine them with convolutions. We find that the most effective design is to stack a small S4 using real-valued recurrent weights with a local convolution, allowing them to work complementarily. Our best model achieves WERs of 4.01%/8.53% on test sets from Librispeech, outperforming Conformers with extensively tuned convolution.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#30340;&#25928;&#26524;&#19982;&#26816;&#32034;&#22120;&#24615;&#33021;&#30456;&#20851;&#65292;&#23545;&#20110;&#24369;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#25552;&#39640;&#20102;&#20998;&#25968;&#65292;&#20294;&#23545;&#20110;&#24378;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#36890;&#24120;&#20250;&#25439;&#23475;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.08541</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26597;&#35810;&#21644;&#25991;&#26723;&#25193;&#23637;&#20309;&#26102;&#22833;&#36133;&#65311;&#26041;&#27861;&#12289;&#26816;&#32034;&#22120;&#21644;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets. (arXiv:2309.08541v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08541
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#30340;&#25928;&#26524;&#19982;&#26816;&#32034;&#22120;&#24615;&#33021;&#30456;&#20851;&#65292;&#23545;&#20110;&#24369;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#25552;&#39640;&#20102;&#20998;&#25968;&#65292;&#20294;&#23545;&#20110;&#24378;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#36890;&#24120;&#20250;&#25439;&#23475;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#21487;&#20197;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#25216;&#26415;&#26159;&#21542;&#26222;&#36941;&#26377;&#30410;&#65292;&#36824;&#26159;&#20165;&#22312;&#29305;&#23450;&#35774;&#32622;&#19979;&#26377;&#25928;&#65292;&#20363;&#22914;&#23545;&#20110;&#29305;&#23450;&#30340;&#26816;&#32034;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#39046;&#22495;&#25110;&#26597;&#35810;&#31867;&#22411;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;&#22522;&#20110;LM&#30340;&#25193;&#23637;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26816;&#32034;&#22120;&#24615;&#33021;&#19982;&#25193;&#23637;&#30340;&#22686;&#30410;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#65306;&#25193;&#23637;&#25913;&#21892;&#20102;&#36739;&#24369;&#27169;&#22411;&#30340;&#20998;&#25968;&#65292;&#20294;&#36890;&#24120;&#20250;&#25439;&#23475;&#36739;&#24378;&#27169;&#22411;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#36235;&#21183;&#22312;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#19968;&#32452;&#23454;&#39564;&#20013;&#25104;&#31435;&#12290;&#36890;&#36807;&#23450;&#24615;&#38169;&#35823;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#23613;&#31649;&#25193;&#23637;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65288;&#21487;&#33021;&#25913;&#21892;&#20102;&#21484;&#22238;&#29575;&#65289;&#65292;&#20294;&#23427;&#20204;&#20063;&#22686;&#21152;&#20102;&#22122;&#22768;&#65292;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20986;&#39030;&#32423;&#30456;&#20851;&#25991;&#26723;&#65288;&#20174;&#32780;&#24341;&#20837;&#20102;&#38169;&#35823;&#30340;&#27491;&#20363;&#65289;
&lt;/p&gt;
&lt;p&gt;
Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;EvoPrompt&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;EvoPrompt&#21487;&#20197;&#33258;&#21160;&#21270;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#21644;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08532</link><description>&lt;p&gt;
&#36890;&#36807;&#36827;&#21270;&#31639;&#27861;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24378;&#22823;&#30340;&#25552;&#31034;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers. (arXiv:2309.08532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;EvoPrompt&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;EvoPrompt&#21487;&#20197;&#33258;&#21160;&#21270;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#21644;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21162;&#21147;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;EvoPrompt&#65292;&#23427;&#20511;&#37492;&#20102;&#36827;&#21270;&#31639;&#27861;&#30340;&#24605;&#24819;&#65292;&#22240;&#20026;&#23427;&#20204;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#25910;&#25947;&#24615;&#12290;&#20026;&#20102;&#20351;&#36827;&#21270;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#24182;&#19988;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#31163;&#25955;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#36830;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#39640;&#25928;&#20248;&#21270;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EvoPrompt&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#26799;&#24230;&#25110;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19968;&#32452;&#25552;&#31034;&#20013;&#24320;&#22987;&#65292;&#24182;&#22522;&#20110;&#36827;&#21270;&#31639;&#23376;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#65292;&#26681;&#25454;&#24320;&#21457;&#38598;&#25913;&#36827;&#25552;&#31034;&#30340;&#31181;&#32676;&#12290;&#25105;&#20204;&#23545;&#38381;&#28304;&#21644;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;GPT-3&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#23454;&#29992;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#35821;&#38899;&#23383;&#24149;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#26469;&#33258;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;Im2Sp&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;COOC&#21644;Flickr8k&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#24211;&#19978;&#21047;&#26032;&#20102;Im2Sp&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#25913;&#36827;&#20102;Im2Sp&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.08531</link><description>&lt;p&gt;
&#23454;&#29616;&#23454;&#29992;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#35821;&#38899;&#23383;&#24149;&#29983;&#25104;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#22810;&#27169;&#24577;&#20196;&#29260;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens. (arXiv:2309.08531v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#23454;&#29992;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#35821;&#38899;&#23383;&#24149;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#26469;&#33258;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#21040;Im2Sp&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;COOC&#21644;Flickr8k&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#24211;&#19978;&#21047;&#26032;&#20102;Im2Sp&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#25913;&#36827;&#20102;Im2Sp&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26500;&#24314;&#24378;&#22823;&#39640;&#25928;&#30340;&#22270;&#20687;&#21040;&#35821;&#38899;&#23383;&#24149;&#29983;&#25104;&#65288;Im2Sp&#65289;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#19982;&#22270;&#20687;&#29702;&#35299;&#21644;&#35821;&#35328;&#24314;&#27169;&#26377;&#20851;&#30340;&#20016;&#23500;&#30693;&#35782;&#24182;&#34701;&#20837;&#21040;Im2Sp&#20013;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;Im2Sp&#30340;&#36755;&#20986;&#35774;&#32622;&#20026;&#31163;&#25955;&#21270;&#30340;&#35821;&#38899;&#21333;&#20301;&#65292;&#21363;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#37327;&#21270;&#35821;&#38899;&#29305;&#24449;&#12290;&#36825;&#20123;&#35821;&#38899;&#21333;&#20301;&#20027;&#35201;&#21253;&#21547;&#35821;&#35328;&#20449;&#24687;&#65292;&#32780;&#25233;&#21046;&#20102;&#35821;&#38899;&#30340;&#20854;&#20182;&#29305;&#24449;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24314;&#27169;&#33021;&#21147;&#34701;&#20837;&#21040;Im2Sp&#30340;&#21475;&#35821;&#24314;&#27169;&#20013;&#12290;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#24211;COCO&#21644;Flickr8k&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;Im2Sp&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;Im2Sp&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#31867;&#20284;&#20110;&#35821;&#38899;&#21333;&#20301;&#26696;&#20363;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#20687;&#21333;&#20301;&#65292;&#36825;&#20123;&#22270;&#20687;&#21333;&#20301;&#26159;&#36890;&#36807;&#35270;&#35273;&#29305;&#24449;&#32534;&#30721;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#20581;&#24247;&#22768;&#26126;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;750&#20010;&#30001;&#21307;&#23398;&#19987;&#23478;&#26631;&#27880;&#30340;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#20020;&#24202;&#30740;&#31350;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#35777;&#25454;&#26816;&#32034;&#12289;&#30495;&#23454;&#24615;&#39044;&#27979;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.08503</link><description>&lt;p&gt;
HealthFC&#65306;&#19968;&#20221;&#29992;&#20110;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#20107;&#23454;&#26816;&#39564;&#30340;&#20581;&#24247;&#22768;&#26126;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HealthFC: A Dataset of Health Claims for Evidence-Based Medical Fact-Checking. (arXiv:2309.08503v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#20581;&#24247;&#22768;&#26126;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;750&#20010;&#30001;&#21307;&#23398;&#19987;&#23478;&#26631;&#27880;&#30340;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#20020;&#24202;&#30740;&#31350;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#35777;&#25454;&#26816;&#32034;&#12289;&#30495;&#23454;&#24615;&#39044;&#27979;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#36890;&#36807;&#20114;&#32852;&#32593;&#26597;&#35810;&#20581;&#24247;&#30456;&#20851;&#24314;&#35758;&#24050;&#25104;&#20026;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#21028;&#26029;&#22312;&#32447;&#25214;&#21040;&#30340;&#21307;&#23398;&#22768;&#26126;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25214;&#21040;&#30456;&#24212;&#30340;&#35777;&#25454;&#65292;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20107;&#23454;&#26816;&#39564;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36890;&#36807;&#21487;&#38752;&#30693;&#35782;&#26469;&#28304;&#30340;&#35777;&#25454;&#35780;&#20272;&#20107;&#23454;&#22768;&#26126;&#30495;&#23454;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#25512;&#21160;&#27492;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;750&#20010;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#65292;&#22312;&#21487;&#20449;&#24230;&#26041;&#38754;&#30001;&#21307;&#23398;&#19987;&#23478;&#36827;&#34892;&#20102;&#26631;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#36866;&#24403;&#30340;&#20020;&#24202;&#30740;&#31350;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#31361;&#20986;&#20854;&#29305;&#28857;&#21644;&#25361;&#25112;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#19982;&#33258;&#21160;&#20107;&#23454;&#26816;&#39564;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#35777;&#25454;&#26816;&#32034;&#12289;&#30495;&#23454;&#24615;&#39044;&#27979;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#19981;&#21516;&#26041;&#27861;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seeking health-related advice on the internet has become a common practice in the digital era. Determining the trustworthiness of medical claims found online and finding appropriate evidence for this information is increasingly challenging. Fact-checking has emerged as an approach to assess the veracity of factual claims using evidence from credible knowledge sources. To help advance the automation of this task, in this paper, we introduce a novel dataset of 750 health-related claims, labeled for veracity by medical experts and backed with evidence from appropriate clinical studies. We provide an analysis of the dataset, highlighting its characteristics and challenges. The dataset can be used for Machine Learning tasks related to automated fact-checking such as evidence retrieval, veracity prediction, and explanation generation. For this purpose, we provide baseline models based on different approaches, examine their performance, and discuss the findings.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30693;&#35782;&#24037;&#31243;&#20219;&#21153;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23558;&#20027;&#39064;&#21644;&#20851;&#31995;&#23545;&#36716;&#21270;&#20026;&#23383;&#31526;&#20018;&#26684;&#24335;&#65292;&#24182;&#23558;&#23427;&#20204;&#38142;&#25509;&#21040;&#30456;&#24212;&#30340;Wikidata QID&#19978;&#65292;&#24320;&#21457;&#20102;LLMKE&#27969;&#27700;&#32447;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#30340;&#30693;&#35782;&#22240;&#39046;&#22495;&#32780;&#24322;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#23454;&#39564;&#20197;&#30830;&#23450;&#20854;&#22312;&#33258;&#21160;&#30693;&#35782;&#24211;&#34917;&#20840;&#21644;&#20462;&#27491;&#26041;&#38754;&#30340;&#24212;&#29992;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#36824;&#26174;&#31034;&#20102;LLMs&#22312;&#21327;&#20316;&#30693;&#35782;&#24037;&#31243;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.08491</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24037;&#31243;&#65288;LLMKE&#65289;&#65306;&#20197;Wikidata&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata. (arXiv:2309.08491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30693;&#35782;&#24037;&#31243;&#20219;&#21153;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23558;&#20027;&#39064;&#21644;&#20851;&#31995;&#23545;&#36716;&#21270;&#20026;&#23383;&#31526;&#20018;&#26684;&#24335;&#65292;&#24182;&#23558;&#23427;&#20204;&#38142;&#25509;&#21040;&#30456;&#24212;&#30340;Wikidata QID&#19978;&#65292;&#24320;&#21457;&#20102;LLMKE&#27969;&#27700;&#32447;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#30340;&#30693;&#35782;&#22240;&#39046;&#22495;&#32780;&#24322;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#23454;&#39564;&#20197;&#30830;&#23450;&#20854;&#22312;&#33258;&#21160;&#30693;&#35782;&#24211;&#34917;&#20840;&#21644;&#20462;&#27491;&#26041;&#38754;&#30340;&#24212;&#29992;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#36824;&#26174;&#31034;&#20102;LLMs&#22312;&#21327;&#20316;&#30693;&#35782;&#24037;&#31243;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;ISWC 2023 LM-KBC&#25361;&#25112;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30693;&#35782;&#24037;&#31243;&#20219;&#21153;&#30340;&#24212;&#29992;&#12290;&#38024;&#23545;&#35813;&#20219;&#21153;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#23558;&#26469;&#33258;Wikidata&#30340;&#20027;&#39064;&#21644;&#20851;&#31995;&#23545;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;&#23383;&#31526;&#20018;&#26684;&#24335;&#65292;&#24182;&#23558;&#23427;&#20204;&#38142;&#25509;&#21040;&#30456;&#24212;&#30340;Wikidata QID&#19978;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#36827;&#34892;&#30693;&#35782;&#24037;&#31243;&#30340;&#27969;&#27700;&#32447;&#65288;LLMKE&#65289;&#65292;&#32467;&#21512;&#20102;&#30693;&#35782;&#25506;&#27979;&#21644;Wikidata&#23454;&#20307;&#26144;&#23556;&#12290;&#35813;&#26041;&#27861;&#22312;&#23646;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;0.701&#30340;&#23439;&#24179;&#22343;F1&#20998;&#25968;&#65292;&#24471;&#20998;&#22312;1.00&#21040;0.328&#20043;&#38388;&#21464;&#21270;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#30693;&#35782;&#22240;&#39046;&#22495;&#32780;&#24322;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#23454;&#39564;&#26469;&#30830;&#23450;LLMs&#22312;&#33258;&#21160;&#30693;&#35782;&#24211;&#65288;&#22914;Wikidata&#65289;&#34917;&#20840;&#21644;&#20462;&#27491;&#26041;&#38754;&#30340;&#24212;&#29992;&#26465;&#20214;&#12290;&#32467;&#26524;&#30340;&#35843;&#26597;&#36824;&#26174;&#31034;&#20102;LLMs&#22312;&#21327;&#20316;&#30693;&#35782;&#24037;&#31243;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#30340;&#36129;&#29486;&#12290;LLMKE&#33719;&#32988;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge. For this task, given subject and relation pairs sourced from Wikidata, we utilize pre-trained LLMs to produce the relevant objects in string format and link them to their respective Wikidata QIDs. We developed a pipeline using LLMs for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata entity mapping. The method achieved a macro-averaged F1-score of 0.701 across the properties, with the scores varying from 1.00 to 0.328. These results demonstrate that the knowledge of LLMs varies significantly depending on the domain and that further experimentation is required to determine the circumstances under which LLMs can be used for automatic Knowledge Base (e.g., Wikidata) completion and correction. The investigation of the results also suggests the promising contribution of LLMs in collaborative knowledge engineering. LLMKE won
&lt;/p&gt;</description></item><item><title>SilverRetriever&#26159;&#19968;&#20010;&#29305;&#20026;&#27874;&#20848;&#35821;&#38382;&#31572;&#31995;&#32479;&#24320;&#21457;&#30340;&#31070;&#32463;&#26816;&#32034;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#19982;&#26356;&#22823;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08469</link><description>&lt;p&gt;
SilverRetriever&#65306;&#25552;&#21319;&#27874;&#20848;&#38382;&#31572;&#31995;&#32479;&#30340;&#31070;&#32463;&#36890;&#36947;&#26816;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SilverRetriever: Advancing Neural Passage Retrieval for Polish Question Answering. (arXiv:2309.08469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08469
&lt;/p&gt;
&lt;p&gt;
SilverRetriever&#26159;&#19968;&#20010;&#29305;&#20026;&#27874;&#20848;&#35821;&#38382;&#31572;&#31995;&#32479;&#24320;&#21457;&#30340;&#31070;&#32463;&#26816;&#32034;&#22120;&#65292;&#36890;&#36807;&#35757;&#32451;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#19982;&#26356;&#22823;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#31572;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#20110;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#32452;&#20214;&#26469;&#25214;&#21040;&#21253;&#21547;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#31070;&#32463;&#26816;&#32034;&#22120;&#27604;&#35789;&#27719;&#26367;&#20195;&#26041;&#24335;&#26356;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#27969;&#34892;&#35821;&#35328;&#22914;&#33521;&#35821;&#25110;&#20013;&#25991;&#19978;&#65292;&#23545;&#20110;&#20854;&#20182;&#35821;&#35328;&#22914;&#27874;&#20848;&#35821;&#65292;&#21487;&#29992;&#30340;&#27169;&#22411;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SilverRetriever&#65292;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#25163;&#21160;&#26631;&#35760;&#25110;&#24369;&#26631;&#35760;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27874;&#20848;&#35821;&#31070;&#32463;&#26816;&#32034;&#22120;&#12290;SilverRetriever&#22312;&#27874;&#20848;&#35821;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19982;&#26356;&#22823;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#19982;&#35813;&#27169;&#22411;&#19968;&#36215;&#65292;&#25105;&#20204;&#36824;&#24320;&#28304;&#20102;&#20116;&#20010;&#26032;&#30340;&#27573;&#33853;&#26816;&#32034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern open-domain question answering systems often rely on accurate and efficient retrieval components to find passages containing the facts necessary to answer the question. Recently, neural retrievers have gained popularity over lexical alternatives due to their superior performance. However, most of the work concerns popular languages such as English or Chinese. For others, such as Polish, few models are available. In this work, we present SilverRetriever, a neural retriever for Polish trained on a diverse collection of manually or weakly labeled datasets. SilverRetriever achieves much better results than other Polish models and is competitive with larger multilingual models. Together with the model, we open-source five new passage retrieval datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#28151;&#21512;&#32534;&#30721;&#22120;&#26041;&#27861;&#20174;&#20004;&#20010;&#35828;&#35805;&#20154;&#24773;&#20917;&#25193;&#23637;&#21040;&#20102;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08454</link><description>&lt;p&gt;
&#28151;&#21512;&#32534;&#30721;&#22120;&#25903;&#25345;&#36830;&#32493;&#35821;&#38899;&#20998;&#31163;&#29992;&#20110;&#20250;&#35758;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Mixture Encoder Supporting Continuous Speech Separation for Meeting Recognition. (arXiv:2309.08454v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#28151;&#21512;&#32534;&#30721;&#22120;&#26041;&#27861;&#20174;&#20004;&#20010;&#35828;&#35805;&#20154;&#24773;&#20917;&#25193;&#23637;&#21040;&#20102;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#22788;&#29702;&#37325;&#21472;&#30340;&#35821;&#38899;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#39318;&#20808;&#23558;&#35821;&#38899;&#20998;&#31163;&#25104;&#26080;&#37325;&#21472;&#30340;&#27969;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#20449;&#21495;&#36827;&#34892;ASR&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22312;ASR&#27169;&#22411;&#20013;&#21253;&#21547;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#12290;&#35813;&#28151;&#21512;&#32534;&#30721;&#22120;&#21033;&#29992;&#21407;&#22987;&#37325;&#21472;&#30340;&#35821;&#38899;&#26469;&#20943;&#36731;&#35821;&#38899;&#20998;&#31163;&#24341;&#20837;&#30340;&#20266;&#24433;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20165;&#38024;&#23545;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#38899;&#20998;&#31163;&#22120;&#65288;&#21253;&#25324;&#24378;&#22823;&#30340;TF-GridNet&#27169;&#22411;&#65289;&#35780;&#20272;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#36824;&#23637;&#31034;&#20102;TF-GridNet&#30340;&#24378;&#22823;&#20998;&#31163;&#33021;&#21147;&#65292;&#22823;&#22823;&#32553;&#23567;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-life applications of automatic speech recognition (ASR) require processing of overlapped speech. A commonmethod involves first separating the speech into overlap-free streams and then performing ASR on the resulting signals. Recently, the inclusion of a mixture encoder in the ASR model has been proposed. This mixture encoder leverages the original overlapped speech to mitigate the effect of artifacts introduced by the speech separation. Previously, however, the method only addressed two-speaker scenarios. In this work, we extend this approach to more natural meeting contexts featuring an arbitrary number of speakers and dynamic overlaps. We evaluate the performance using different speech separators, including the powerful TF-GridNet model. Our experiments show state-of-the-art performance on the LibriCSS dataset and highlight the advantages of the mixture encoder. Furthermore, they demonstrate the strong separation of TF-GridNet which largely closes the gap between previous m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#33521;&#25991;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#20256;&#32479;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#36825;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#19978;&#19979;&#25991;&#38382;&#31572;&#12289;&#25688;&#35201;&#12289;&#20998;&#31867;&#21644;&#34920;&#26684;&#29702;&#35299;&#31561;&#22810;&#20010;&#20219;&#21153;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19979;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.08448</link><description>&lt;p&gt;
&#25512;&#36827;&#20256;&#32479;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65306;&#36808;&#21521;&#20840;&#38754;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Advancing the Evaluation of Traditional Chinese Language Models: Towards a Comprehensive Benchmark Suite. (arXiv:2309.08448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#33521;&#25991;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#20256;&#32479;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#36825;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#19978;&#19979;&#25991;&#38382;&#31572;&#12289;&#25688;&#35201;&#12289;&#20998;&#31867;&#21644;&#34920;&#26684;&#29702;&#35299;&#31561;&#22810;&#20010;&#20219;&#21153;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19979;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#39046;&#22495;&#20013;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#35780;&#20272;&#20854;&#24615;&#33021;&#30340;&#26377;&#25928;&#22522;&#20934;&#30340;&#38656;&#27714;&#21464;&#24471;&#36843;&#20999;&#12290;&#22312;&#20013;&#25991;&#35821;&#22659;&#19979;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#22914;DRCD&#12289;TTQA&#12289;CMDQA&#21644;FGC&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#22810;&#26679;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#33521;&#25991;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#20256;&#32479;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#38382;&#31572;&#12289;&#25688;&#35201;&#12289;&#20998;&#31867;&#21644;&#34920;&#26684;&#29702;&#35299;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19979;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3.5&#21644;Taiwa&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of large language models is an essential task in the field of language understanding and generation. As language models continue to advance, the need for effective benchmarks to assess their performance has become imperative. In the context of Traditional Chinese, there is a scarcity of comprehensive and diverse benchmarks to evaluate the capabilities of language models, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA, and FGC dataset. To address this gap, we propose a novel set of benchmarks that leverage existing English datasets and are tailored to evaluate language models in Traditional Chinese. These benchmarks encompass a wide range of tasks, including contextual question-answering, summarization, classification, and table understanding. The proposed benchmarks offer a comprehensive evaluation framework, enabling the assessment of language models' capabilities across different tasks. In this paper, we evaluate the performance of GPT-3.5, Taiwa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#22320;&#23558;&#35777;&#25454;&#32435;&#20837;&#30693;&#35782;&#23494;&#38598;&#22411;&#23545;&#35805;&#29983;&#25104;&#30340;&#26694;&#26550; (u-EIDG)&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#21160;&#35777;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#25366;&#25496;&#21487;&#38752;&#30340;&#35777;&#25454;&#30495;&#23454;&#24615;&#26631;&#31614;&#65292;&#20197;&#25552;&#39640;&#23545;&#35805;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08380</link><description>&lt;p&gt;
&#21457;&#25496;&#30693;&#35782;&#23494;&#38598;&#22411;&#23545;&#35805;&#29983;&#25104;&#20013;&#35777;&#25454;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing Potential of Evidence in Knowledge-Intensive Dialogue Generation. (arXiv:2309.08380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#22320;&#23558;&#35777;&#25454;&#32435;&#20837;&#30693;&#35782;&#23494;&#38598;&#22411;&#23545;&#35805;&#29983;&#25104;&#30340;&#26694;&#26550; (u-EIDG)&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#21160;&#35777;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#25366;&#25496;&#21487;&#38752;&#30340;&#35777;&#25454;&#30495;&#23454;&#24615;&#26631;&#31614;&#65292;&#20197;&#25552;&#39640;&#23545;&#35805;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#23545;&#35805;&#29983;&#25104;&#30340;&#36807;&#31243;&#23545;&#20110;&#25552;&#39640;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#35777;&#25454;&#29255;&#27573;&#20316;&#20026;&#30693;&#35782;&#24615;&#30340;&#25903;&#25745;&#25903;&#25345;&#23545;&#35805;&#22238;&#22797;&#30340;&#20107;&#23454;&#12290;&#28982;&#32780;&#65292;&#24341;&#20837;&#26080;&#20851;&#20869;&#23481;&#24448;&#24448;&#20250;&#23545;&#22238;&#22797;&#36136;&#37327;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#23481;&#26131;&#23548;&#33268;&#34394;&#26500;&#30340;&#22238;&#24212;&#12290;&#20808;&#21069;&#20851;&#20110;&#35777;&#25454;&#26816;&#32034;&#19982;&#25972;&#21512;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#24037;&#20316;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;&#35777;&#25454;&#65292;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#22320;&#23450;&#20301;&#26377;&#29992;&#30340;&#29255;&#27573;&#65292;&#24182;&#24573;&#35270;&#20102;KIDG&#25968;&#25454;&#38598;&#20013;&#30340;&#38544;&#34255;&#35777;&#25454;&#26631;&#31614;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25496;&#35777;&#25454;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#22320;&#23558;&#35777;&#25454;&#32435;&#20837;&#30693;&#35782;&#23494;&#38598;&#22411;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#26694;&#26550;&#65288;u-EIDG&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#21160;&#35777;&#25454;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#33021;&#21147;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#25366;&#25496;&#21487;&#38752;&#30340;&#35777;&#25454;&#30495;&#23454;&#24615;&#26631;&#31614;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#35777;&#25454;&#26631;&#31614;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35777;&#25454;&#25351;&#31034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating external knowledge into dialogue generation (KIDG) is crucial for improving the correctness of response, where evidence fragments serve as knowledgeable snippets supporting the factual dialogue replies. However, introducing irrelevant content often adversely impacts reply quality and easily leads to hallucinated responses. Prior work on evidence retrieval and integration in dialogue systems falls short of fully leveraging existing evidence since the model fails to locate useful fragments accurately and overlooks hidden evidence labels within the KIDG dataset. To fully Unleash the potential of evidence, we propose a framework to effectively incorporate Evidence in knowledge-Intensive Dialogue Generation (u-EIDG). Specifically, we introduce an automatic evidence generation framework that harnesses the power of Large Language Models (LLMs) to mine reliable evidence veracity labels from unlabeled data. By utilizing these evidence labels, we train a reliable evidence indicator
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Qatent PatFig&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;30,000&#22810;&#20010;&#19987;&#21033;&#22270;&#30340;&#30701;&#25991;&#21644;&#38271;&#25991;&#26631;&#39064;&#12289;&#21442;&#32771;&#32534;&#21495;&#12289;&#26415;&#35821;&#21644;&#25551;&#36848;&#22270;&#20687;&#32452;&#20214;&#20043;&#38388;&#20114;&#21160;&#30340;&#26368;&#23567;&#32034;&#36180;&#38598;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#23545;LVLM&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#21457;&#29616;&#24341;&#20837;&#19981;&#21516;&#22522;&#20110;&#25991;&#26412;&#30340;&#32447;&#32034;&#21487;&#20197;&#25913;&#21892;&#19987;&#21033;&#22270;&#26631;&#39064;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08379</link><description>&lt;p&gt;
PatFig&#65306;&#20026;&#19987;&#21033;&#22270;&#29983;&#25104;&#30701;&#25991;&#21644;&#38271;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
PatFig: Generating Short and Long Captions for Patent Figures. (arXiv:2309.08379v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Qatent PatFig&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;30,000&#22810;&#20010;&#19987;&#21033;&#22270;&#30340;&#30701;&#25991;&#21644;&#38271;&#25991;&#26631;&#39064;&#12289;&#21442;&#32771;&#32534;&#21495;&#12289;&#26415;&#35821;&#21644;&#25551;&#36848;&#22270;&#20687;&#32452;&#20214;&#20043;&#38388;&#20114;&#21160;&#30340;&#26368;&#23567;&#32034;&#36180;&#38598;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#23545;LVLM&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#21457;&#29616;&#24341;&#20837;&#19981;&#21516;&#22522;&#20110;&#25991;&#26412;&#30340;&#32447;&#32034;&#21487;&#20197;&#25913;&#21892;&#19987;&#21033;&#22270;&#26631;&#39064;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Qatent PatFig&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;&#19987;&#21033;&#22270;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#36229;&#36807;11,000&#20010;&#27431;&#27954;&#19987;&#21033;&#30003;&#35831;&#30340;30,000&#22810;&#20010;&#19987;&#21033;&#22270;&#12290;&#23545;&#20110;&#27599;&#20010;&#22270;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#30701;&#25991;&#21644;&#38271;&#25991;&#26631;&#39064;&#12289;&#21442;&#32771;&#32534;&#21495;&#21450;&#20854;&#30456;&#24212;&#26415;&#35821;&#65292;&#20197;&#21450;&#25551;&#36848;&#22270;&#20687;&#32452;&#20214;&#20043;&#38388;&#20114;&#21160;&#30340;&#26368;&#23567;&#32034;&#36180;&#38598;&#12290;&#20026;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;Qatent PatFig&#19978;&#23545;LVLM&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20197;&#29983;&#25104;&#30701;&#25991;&#21644;&#38271;&#25991;&#25551;&#36848;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#19987;&#21033;&#22270;&#26631;&#39064;&#29983;&#25104;&#36807;&#31243;&#30340;&#39044;&#27979;&#38454;&#27573;&#24341;&#20837;&#19981;&#21516;&#22522;&#20110;&#25991;&#26412;&#30340;&#32447;&#32034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Qatent PatFig, a novel large-scale patent figure dataset comprising 30,000+ patent figures from over 11,000 European patent applications. For each figure, this dataset provides short and long captions, reference numerals, their corresponding terms, and the minimal claim set that describes the interactions between the components of the image. To assess the usability of the dataset, we finetune an LVLM model on Qatent PatFig to generate short and long descriptions, and we investigate the effects of incorporating various text-based cues at the prediction stage of the patent figure captioning process.
&lt;/p&gt;</description></item><item><title>DiaCorrect&#26159;&#19968;&#20010;&#38169;&#35823;&#26657;&#27491;&#21518;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#25196;&#22768;&#22120;&#20998;&#26512;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36755;&#20837;&#24405;&#38899;&#21644;&#21021;&#22987;&#31995;&#32479;&#36755;&#20986;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#33258;&#21160;&#32416;&#27491;&#21021;&#22987;&#30340;&#35828;&#35805;&#32773;&#27963;&#21160;&#65292;&#20197;&#20943;&#23569;&#20998;&#26512;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2309.08377</link><description>&lt;p&gt;
DiaCorrect&#65306;&#25196;&#22768;&#22120;&#20998;&#26512;&#30340;&#38169;&#35823;&#26657;&#27491;&#21518;&#31471;
&lt;/p&gt;
&lt;p&gt;
DiaCorrect: Error Correction Back-end For Speaker Diarization. (arXiv:2309.08377v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08377
&lt;/p&gt;
&lt;p&gt;
DiaCorrect&#26159;&#19968;&#20010;&#38169;&#35823;&#26657;&#27491;&#21518;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#25196;&#22768;&#22120;&#20998;&#26512;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36755;&#20837;&#24405;&#38899;&#21644;&#21021;&#22987;&#31995;&#32479;&#36755;&#20986;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#33258;&#21160;&#32416;&#27491;&#21021;&#22987;&#30340;&#35828;&#35805;&#32773;&#27963;&#21160;&#65292;&#20197;&#20943;&#23569;&#20998;&#26512;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiaCorrect&#30340;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;&#65292;&#20197;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#25913;&#36827;&#25196;&#22768;&#22120;&#20998;&#26512;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#21463;&#21040;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#38169;&#35823;&#26657;&#27491;&#25216;&#26415;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#20004;&#20010;&#24182;&#34892;&#30340;&#21367;&#31215;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#30340;&#35299;&#30721;&#22120;&#32452;&#25104;&#12290;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#24405;&#38899;&#21644;&#21021;&#22987;&#31995;&#32479;&#36755;&#20986;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;DiaCorrect&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;&#21021;&#22987;&#30340;&#35828;&#35805;&#32773;&#27963;&#21160;&#65292;&#20197;&#26368;&#23567;&#21270;&#20998;&#26512;&#38169;&#35823;&#12290;&#22312;2&#20010;&#25196;&#22768;&#22120;&#30005;&#35805;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;DiaCorrect&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#36827;&#21021;&#22987;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110; https://github.com/BUTSpeechFIT/diacorrect.
&lt;/p&gt;
&lt;p&gt;
In this work, we propose an error correction framework, named DiaCorrect, to refine the output of a diarization system in a simple yet effective way. This method is inspired by error correction techniques in automatic speech recognition. Our model consists of two parallel convolutional encoders and a transform-based decoder. By exploiting the interactions between the input recording and the initial system's outputs, DiaCorrect can automatically correct the initial speaker activities to minimize the diarization errors. Experiments on 2-speaker telephony data show that the proposed DiaCorrect can effectively improve the initial model's results. Our source code is publicly available at https://github.com/BUTSpeechFIT/diacorrect.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22836;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#26435;&#37325;&#32465;&#23450;&#30340;&#26041;&#24335;&#23545;&#36755;&#20837;&#23884;&#20837;&#36827;&#34892;&#37325;&#26500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19979;&#28216;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#39044;&#31639;&#30456;&#20284;&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;GLUE&#20998;&#25968;&#22686;&#21152;&#21644;LAMBADA&#20934;&#30830;&#24615;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2309.08351</link><description>&lt;p&gt;
&#26080;&#22836;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#23545;&#27604;&#26435;&#37325;&#32465;&#23450;&#23398;&#20064;&#32780;&#38750;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Headless Language Models: Learning without Predicting with Contrastive Weight Tying. (arXiv:2309.08351v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08351
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22836;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#26435;&#37325;&#32465;&#23450;&#30340;&#26041;&#24335;&#23545;&#36755;&#20837;&#23884;&#20837;&#36827;&#34892;&#37325;&#26500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19979;&#28216;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#39044;&#31639;&#30456;&#20284;&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;GLUE&#20998;&#25968;&#22686;&#21152;&#21644;LAMBADA&#20934;&#30830;&#24615;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#36890;&#24120;&#28041;&#21450;&#23545;&#22823;&#37327;&#21333;&#35789;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#27880;&#24847;&#21147;&#20174;&#27010;&#29575;&#39044;&#27979;&#36716;&#31227;&#21040;&#36890;&#36807;&#23545;&#27604;&#26435;&#37325;&#32465;&#23450;&#30340;&#26041;&#24335;&#23545;&#36755;&#20837;&#23884;&#20837;&#36827;&#34892;&#37325;&#26500;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;&#21333;&#35821;&#21644;&#22810;&#35821;&#22659;&#19979;&#39044;&#35757;&#32451;&#26080;&#22836;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#23454;&#38469;&#20248;&#21183;&#65292;&#21487;&#20197;&#23558;&#35757;&#32451;&#35745;&#31639;&#35201;&#27714;&#20943;&#23569;&#39640;&#36798;20&#20493;&#65292;&#21516;&#26102;&#22686;&#24378;&#19979;&#28216;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;&#19982;&#22312;&#30456;&#20284;&#35745;&#31639;&#39044;&#31639;&#19979;&#30340;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26174;&#33879;&#30340;+1.6 GLUE&#20998;&#25968;&#22686;&#21152;&#21644;&#26174;&#33879;&#30340;+2.7 LAMBADA&#20934;&#30830;&#24615;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pre-training of language models usually consists in predicting probability distributions over extensive token vocabularies. In this study, we propose an innovative method that shifts away from probability prediction and instead focuses on reconstructing input embeddings in a contrastive fashion via Constrastive Weight Tying (CWT). We apply this approach to pretrain Headless Language Models in both monolingual and multilingual contexts. Our method offers practical advantages, substantially reducing training computational requirements by up to 20 times, while simultaneously enhancing downstream performance and data efficiency. We observe a significant +1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement compared to classical LMs within similar compute budgets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#24037;&#31243;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#21322;&#32467;&#26500;&#21270;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#22686;&#24378;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#39564;&#35777;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08347</link><description>&lt;p&gt;
&#29983;&#25104;&#21322;&#32467;&#26500;&#21270;&#35299;&#37322;&#30340;&#22870;&#21169;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reward Engineering for Generating Semi-structured Explanation. (arXiv:2309.08347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#24037;&#31243;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#21322;&#32467;&#26500;&#21270;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#22686;&#24378;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#39564;&#35777;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#32467;&#26500;&#21270;&#35299;&#37322;&#25551;&#32472;&#20102;&#19968;&#20010;&#25512;&#29702;&#32773;&#30340;&#38544;&#24335;&#36807;&#31243;&#21644;&#26174;&#24335;&#34920;&#31034;&#12290;&#36825;&#31181;&#35299;&#37322;&#31361;&#20986;&#20102;&#22312;&#29305;&#23450;&#26597;&#35810;&#20013;&#21487;&#29992;&#20449;&#24687;&#22914;&#20309;&#19982;&#25512;&#29702;&#32773;&#20174;&#20869;&#37096;&#26435;&#37325;&#20135;&#29983;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#26368;&#36817;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#29983;&#25104;&#32467;&#26500;&#21270;&#35299;&#37322;&#20197;&#39564;&#35777;&#27169;&#22411;&#30495;&#27491;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#23545;&#20110;&#35268;&#27169;&#19981;&#26159;&#24456;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#26126;&#26174;&#65292;&#22240;&#20026;&#25512;&#29702;&#32773;&#34987;&#26399;&#26395;&#23558;&#39034;&#24207;&#30340;&#31572;&#26696;&#19982;&#20307;&#29616;&#27491;&#30830;&#23637;&#31034;&#21644;&#27491;&#30830;&#25512;&#29702;&#36807;&#31243;&#30340;&#32467;&#26500;&#21270;&#35299;&#37322;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#30417;&#30563;&#24494;&#35843;(SFT)&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#28982;&#21518;&#22312;&#24378;&#21270;&#23398;&#20064;(RL)&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#24037;&#31243;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#22870;&#21169;&#32858;&#21512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
Semi-structured explanation depicts the implicit process of a reasoner with an explicit representation. This explanation highlights how available information in a specific query is supplemented with information a reasoner produces from its internal weights towards generating an answer. Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify model's true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs, as the reasoner is expected to couple a sequential answer with a structured explanation which embodies both the correct presentation and the correct reasoning process. In this work, we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge, and then introduce a carefully crafted reward engineering method in reinforcement learning (RL) to better address this problem. We investigate multiple reward aggregation methods and provide a de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;&#65292;&#21253;&#25324;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#31561;&#26041;&#38754;&#12290;&#21363;&#20351;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08345</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases. (arXiv:2309.08345v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;&#65292;&#21253;&#25324;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#31561;&#26041;&#38754;&#12290;&#21363;&#20351;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#19982;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#31561;&#29616;&#23454;&#29615;&#22659;&#30340;&#25972;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#27424;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24433;&#21709;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#24212;&#29992;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#8220;&#20135;&#29983;&#34394;&#20551;&#20449;&#24687;&#8221;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;LM&#22312;&#22788;&#29702;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#20219;&#21153;&#26102;&#25152;&#36935;&#21040;&#30340;&#20581;&#22766;&#24615;&#25361;&#25112;&#12290;&#30740;&#31350;&#35206;&#30422;&#20102;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#21508;&#31181;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#25581;&#31034;&#20102;&#21363;&#20351;&#22312;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#20808;&#36827;&#30340;&#23567;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have already demonstrated remarkable abilities in understanding and generating both natural and formal language. Despite these advances, their integration with real-world environments such as large-scale knowledge bases (KBs) remains an underdeveloped area, affecting applications such as semantic parsing and indulging in "hallucinated" information. This paper is an experimental investigation aimed at uncovering the robustness challenges that LMs encounter when tasked with knowledge base question answering (KBQA). The investigation covers scenarios with inconsistent data distribution between training and inference, such as generalization to unseen domains, adaptation to various language variations, and transferability across different datasets. Our comprehensive experiments reveal that even when employed with our proposed data augmentation techniques, advanced small and large language models exhibit poor performance in various dimensions. While the LM is a promisin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21151;&#33021;&#20998;&#24067;&#35821;&#20041;&#20013;&#65292;&#24403;&#35821;&#26009;&#24211;&#20005;&#26684;&#36981;&#24490;&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#26102;&#65292;&#21151;&#33021;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#19978;&#20301;&#35789;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#19968;&#31181;&#35757;&#32451;&#30446;&#26631;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#26222;&#36941;&#37327;&#21270;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#30340;&#21453;&#21521;&#19979;&#23454;&#29616;&#19978;&#20301;&#35789;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#20123;&#20551;&#35774;&#21644;&#30446;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08325</link><description>&lt;p&gt;
&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#19982;&#37327;&#21270;&#65306;&#22312;&#21151;&#33021;&#20998;&#24067;&#35821;&#20041;&#20013;&#25506;&#31350;&#19978;&#20301;&#35789;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Distributional Inclusion Hypothesis and Quantifications: Probing Hypernymy in Functional Distributional Semantics. (arXiv:2309.08325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21151;&#33021;&#20998;&#24067;&#35821;&#20041;&#20013;&#65292;&#24403;&#35821;&#26009;&#24211;&#20005;&#26684;&#36981;&#24490;&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#26102;&#65292;&#21151;&#33021;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#19978;&#20301;&#35789;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#19968;&#31181;&#35757;&#32451;&#30446;&#26631;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#26222;&#36941;&#37327;&#21270;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#30340;&#21453;&#21521;&#19979;&#23454;&#29616;&#19978;&#20301;&#35789;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#20123;&#20551;&#35774;&#21644;&#30446;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#20998;&#24067;&#35821;&#20041;&#65288;FDS&#65289;&#36890;&#36807;&#30495;&#26465;&#20214;&#20989;&#25968;&#23545;&#21333;&#35789;&#30340;&#21547;&#20041;&#36827;&#34892;&#24314;&#27169;&#12290;&#24403;&#35821;&#26009;&#24211;&#20005;&#26684;&#36981;&#24490;&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#26102;&#65292;FDS&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#19978;&#20301;&#35789;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;&#24471;FDS&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#31616;&#21333;&#30340;&#26222;&#36941;&#37327;&#21270;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#21253;&#21547;&#20551;&#35774;&#30340;&#21453;&#21521;&#19979;&#23454;&#29616;&#19978;&#20301;&#35789;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#30446;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional Distributional Semantics (FDS) models the meaning of words by truth-conditional functions. This provides a natural representation for hypernymy, but no guarantee that it is learnt when FDS models are trained on a corpus. We demonstrate that FDS models learn hypernymy when a corpus strictly follows the Distributional Inclusion Hypothesis. We further introduce a training objective that allows FDS to handle simple universal quantifications, thus enabling hypernymy learning under the reverse of DIH. Experimental results on both synthetic and real data sets confirm our hypotheses and the effectiveness of our proposed objective.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#36234;&#20027;&#39064;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#21270;&#30340;&#20840;&#38754;&#38750;&#20998;&#24067;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#31574;&#30053;&#65292;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#30340;&#31934;&#32454;&#35843;&#33410;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.08316</link><description>&lt;p&gt;
&#36328;&#36234;&#20027;&#39064;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#21270;&#65306;&#23545;&#20840;&#38754;&#30340;&#38750;&#20998;&#24067;&#22330;&#26223;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bridging Topic, Domain, and Language Shifts: An Evaluation of Comprehensive Out-of-Distribution Scenarios. (arXiv:2309.08316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#36234;&#20027;&#39064;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#21270;&#30340;&#20840;&#38754;&#38750;&#20998;&#24067;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#31574;&#30053;&#65292;&#21253;&#25324;&#22522;&#20110;&#25552;&#31034;&#30340;&#31934;&#32454;&#35843;&#33410;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65288;&#22914;&#20105;&#35770;&#25366;&#25496;&#65289;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#32463;&#24120;&#19979;&#38477;&#12290;&#36825;&#31181;&#38477;&#32423;&#21457;&#29983;&#22312;&#26032;&#35805;&#39064;&#20986;&#29616;&#65292;&#25110;&#20854;&#20182;&#25991;&#26412;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#24471;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#38750;&#20998;&#24067;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#24847;&#22320;&#20445;&#30041;&#29305;&#23450;&#23454;&#20363;&#36827;&#34892;&#27979;&#35797;&#26469;&#27169;&#25311;&#36825;&#31181;&#20998;&#24067;&#21464;&#21270;&#65292;&#20363;&#22914;&#31038;&#20132;&#23186;&#20307;&#39046;&#22495;&#25110;&#22826;&#38451;&#33021;&#20027;&#39064;&#12290;&#19982;&#20808;&#21069;&#20851;&#27880;&#29305;&#23450;&#21464;&#21270;&#21644;&#24230;&#37327;&#26631;&#20934;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#30830;&#23450;&#27867;&#21270;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#28085;&#30422;&#20027;&#39064;&#12289;&#39046;&#22495;&#21644;&#35821;&#35328;&#21464;&#21270;&#30340;&#21313;&#19968;&#20010;&#20998;&#31867;&#20219;&#21153;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#25552;&#31034;&#30340;&#31934;&#32454;&#35843;&#33410;&#20855;&#26377;&#26356;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#22312;&#35821;&#20041;&#19978;&#20027;&#35201;&#26377;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20063;&#26377;&#31867;&#20284;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) excel in in-distribution (ID) scenarios where train and test data are independent and identically distributed. However, their performance often degrades in real-world applications like argument mining. Such degradation happens when new topics emerge, or other text domains and languages become relevant. To assess LMs' generalization abilities in such out-of-distribution (OOD) scenarios, we simulate such distribution shifts by deliberately withholding specific instances for testing, as from the social media domain or the topic Solar Energy.  Unlike prior studies focusing on specific shifts and metrics in isolation, we comprehensively analyze OOD generalization. We define three metrics to pinpoint generalization flaws and propose eleven classification tasks covering topic, domain, and language shifts. Overall, we find superior performance of prompt-based fine-tuning, notably when train and test splits primarily differ semantically. Simultaneously, in-context learning
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#33258;&#27965;&#24615;&#21644;&#21477;&#38388;&#36830;&#36143;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08303</link><description>&lt;p&gt;
&#33258;&#27965;&#21465;&#36848;&#24335;&#21551;&#31034;&#22312;&#28436;&#32462;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Narrative Prompts on Abductive Natural Language Inference. (arXiv:2309.08303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08303
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#33258;&#27965;&#24615;&#21644;&#21477;&#38388;&#36830;&#36143;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25925;&#20107;&#29702;&#35299;&#21644;&#26085;&#24120;&#24773;&#22659;&#25512;&#29702;&#65292;&#28436;&#32462;&#19968;&#30452;&#34987;&#35270;&#20026;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#28436;&#32462;&#20219;&#21153; - &#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;$\alpha$NLI&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#20004;&#20010;&#35266;&#23519;&#20013;&#25512;&#26029;&#20986;&#26368;&#21512;&#29702;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#21477;&#38388;&#36830;&#36143;&#24615;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#21033;&#29992;&#24182;&#19981;&#20805;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#33258;&#27965;&#24615;&#21644;&#21477;&#38388;&#36830;&#36143;&#24615;&#30340;&#25552;&#31034;&#35843;&#20248;&#27169;&#22411; $\alpha$-PACE&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#27965;&#26694;&#26550;&#65292;&#29992;&#20110;&#24341;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#36755;&#20837;&#30340;&#21465;&#36848;&#32972;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;&#32447;&#24615;&#21465;&#36848;&#21644;&#36870;&#26102;&#38388;&#39034;&#24207;&#31561;&#21508;&#31181;&#21465;&#36848;&#24207;&#21015;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102; $\alpha$-PACE &#30340;&#24517;&#35201;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abduction has long been seen as crucial for narrative comprehension and reasoning about everyday situations. The abductive natural language inference ($\alpha$NLI) task has been proposed, and this narrative text-based task aims to infer the most plausible hypothesis from the candidates given two observations. However, the inter-sentential coherence and the model consistency have not been well exploited in the previous works on this task. In this work, we propose a prompt tuning model $\alpha$-PACE, which takes self-consistency and inter-sentential coherence into consideration. Besides, we propose a general self-consistent framework that considers various narrative sequences (e.g., linear narrative and reverse chronology) for guiding the pre-trained language model in understanding the narrative context of input. We conduct extensive experiments and thorough ablation studies to illustrate the necessity and effectiveness of $\alpha$-PACE. The performance of our method shows significant im
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26367;&#20195;BERT&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21253;&#25324;&#38543;&#26426;&#26631;&#35760;&#32622;&#25442;&#65288;RTS&#65289;&#12289;&#22522;&#20110;&#31751;&#30340;&#38543;&#26426;&#26631;&#35760;&#32622;&#25442;&#65288;C-RTS&#65289;&#21644;&#20132;&#25442;&#35821;&#35328;&#24314;&#27169;&#65288;SLM&#65289;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#30446;&#26631;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#19982;&#19979;&#28216;&#24212;&#29992;&#21305;&#37197;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.08272</link><description>&lt;p&gt;
Transformer&#32467;&#26500;&#33258;&#30417;&#30563;&#30446;&#26631;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Structural Self-Supervised Objectives for Transformers. (arXiv:2309.08272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26367;&#20195;BERT&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21253;&#25324;&#38543;&#26426;&#26631;&#35760;&#32622;&#25442;&#65288;RTS&#65289;&#12289;&#22522;&#20110;&#31751;&#30340;&#38543;&#26426;&#26631;&#35760;&#32622;&#25442;&#65288;C-RTS&#65289;&#21644;&#20132;&#25442;&#35821;&#35328;&#24314;&#27169;&#65288;SLM&#65289;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#30446;&#26631;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#19982;&#19979;&#28216;&#24212;&#29992;&#21305;&#37197;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26080;&#30417;&#30563;&#21407;&#22987;&#25968;&#25454;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#20854;&#26356;&#39640;&#25928;&#19988;&#19982;&#19979;&#28216;&#24212;&#29992;&#26356;&#21152;&#19968;&#33268;&#12290;&#22312;&#31532;&#19968;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#26367;&#20195;BERT&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20998;&#21035;&#26159;&#38543;&#26426;&#26631;&#35760;&#32622;&#25442;&#65288;RTS&#65289;&#12289;&#22522;&#20110;&#31751;&#30340;&#38543;&#26426;&#26631;&#35760;&#32622;&#25442;&#65288;C-RTS&#65289;&#21644;&#20132;&#25442;&#35821;&#35328;&#24314;&#27169;&#65288;SLM&#65289;&#12290;&#36825;&#20123;&#30446;&#26631;&#28041;&#21450;&#21040;&#26631;&#35760;&#30340;&#20132;&#25442;&#32780;&#19981;&#26159;&#23631;&#34109;&#65292;&#20854;&#20013;RTS&#21644;C-RTS&#26088;&#22312;&#39044;&#27979;&#26631;&#35760;&#30340;&#21407;&#22987;&#24615;&#65292;&#32780;SLM&#21017;&#39044;&#27979;&#21407;&#22987;&#26631;&#35760;&#30340;&#20540;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;RTS&#21644;C-RTS&#38656;&#35201;&#26356;&#23569;&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;MLM&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;&#20351;&#29992;&#20102;&#30456;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;SLM&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;MLM&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#19982;&#19979;&#28216;&#24212;&#29992;&#21305;&#37197;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#21644;CC-News&#31561;&#22823;&#22411;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This thesis focuses on improving the pre-training of natural language models using unsupervised raw data to make them more efficient and aligned with downstream applications.  In the first part, we introduce three alternative pre-training objectives to BERT's Masked Language Modeling (MLM), namely Random Token Substitution (RTS), Cluster-based Random Token Substitution (C-RTS), and Swapped Language Modeling (SLM). These objectives involve token swapping instead of masking, with RTS and C-RTS aiming to predict token originality and SLM predicting the original token values. Results show that RTS and C-RTS require less pre-training time while maintaining performance comparable to MLM. Surprisingly, SLM outperforms MLM on certain tasks despite using the same computational budget.  In the second part, we proposes self-supervised pre-training tasks that align structurally with downstream applications, reducing the need for labeled data. We use large corpora like Wikipedia and CC-News to trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#35821;&#38899;&#21512;&#25104;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#35821;&#38899;&#36716;&#25442;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#65292;&#20248;&#20110;&#22522;&#20110;&#22810;&#35821;&#31181;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.08255</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27969;&#30340;&#35821;&#38899;&#36716;&#25442;&#23454;&#29616;&#36328;&#35821;&#35328;&#30693;&#35782;&#33976;&#39311;&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Knowledge Distillation via Flow-based Voice Conversion for Robust Polyglot Text-To-Speech. (arXiv:2309.08255v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#35821;&#38899;&#21512;&#25104;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#35821;&#38899;&#36716;&#25442;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#65292;&#20248;&#20110;&#22522;&#20110;&#22810;&#35821;&#31181;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#35821;&#38899;&#21512;&#25104;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#19978;&#28216;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#27169;&#22411;&#21644;&#19968;&#20010;&#19979;&#28216;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;4&#20010;&#38454;&#27573;&#12290;&#22312;&#21069;&#20004;&#20010;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;VC&#27169;&#22411;&#23558;&#30446;&#26631;&#21306;&#22495;&#30340;&#35805;&#35821;&#36716;&#25442;&#20026;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#12290;&#22312;&#31532;&#19977;&#20010;&#38454;&#27573;&#65292;&#23558;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#19982;&#30446;&#26631;&#35821;&#35328;&#24405;&#38899;&#20013;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#25345;&#32493;&#26102;&#38388;&#32467;&#21512;&#36215;&#26469;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#21333;&#35828;&#35805;&#20154;&#22768;&#23398;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#26368;&#21518;&#19968;&#20010;&#38454;&#27573;&#23558;&#35757;&#32451;&#19968;&#20010;&#19982;&#35821;&#35328;&#26080;&#20851;&#30340;&#22768;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#25552;&#20986;&#30340;&#33539;&#20363;&#20248;&#20110;&#22522;&#20110;&#35757;&#32451;&#22823;&#22411;&#22810;&#35821;&#31181;TTS&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#35821;&#35328;&#12289;&#35828;&#35805;&#32773;&#21644;&#25968;&#25454;&#37327;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a framework for cross-lingual speech synthesis, which involves an upstream Voice Conversion (VC) model and a downstream Text-To-Speech (TTS) model. The proposed framework consists of 4 stages. In the first two stages, we use a VC model to convert utterances in the target locale to the voice of the target speaker. In the third stage, the converted data is combined with the linguistic features and durations from recordings in the target language, which are then used to train a single-speaker acoustic model. Finally, the last stage entails the training of a locale-independent vocoder. Our evaluations show that the proposed paradigm outperforms state-of-the-art approaches which are based on training a large multilingual TTS model. In addition, our experiments demonstrate the robustness of our approach with different model architectures, languages, speakers and amounts of data. Moreover, our solution is especially beneficial in low-resource settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20027;&#35201;&#25506;&#31350;LLMs&#22312;&#38271;&#31687;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#30340;&#21487;&#22238;&#31572;&#24615;&#65292;&#36890;&#36807;&#25552;&#20986;&#20174;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20174;&#38271;&#25991;&#26723;&#25688;&#35201;&#20013;&#29983;&#25104;&#21518;&#32493;&#38382;&#39064;&#23545;LLMs&#36827;&#34892;&#25512;&#29702;&#21644;&#25512;&#26029;&#30340;&#25361;&#25112;&#65292;&#24182;&#30830;&#35748;&#20102;LLMs&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.08210</link><description>&lt;p&gt;
&#25506;&#31350;LLMs&#22312;&#38271;&#31687;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#30340;&#21487;&#22238;&#31572;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating Answerability of LLMs for Long-Form Question Answering. (arXiv:2309.08210v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#25506;&#31350;LLMs&#22312;&#38271;&#31687;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#30340;&#21487;&#22238;&#31572;&#24615;&#65292;&#36890;&#36807;&#25552;&#20986;&#20174;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20174;&#38271;&#25991;&#26723;&#25688;&#35201;&#20013;&#29983;&#25104;&#21518;&#32493;&#38382;&#39064;&#23545;LLMs&#36827;&#34892;&#25512;&#29702;&#21644;&#25512;&#26029;&#30340;&#25361;&#25112;&#65292;&#24182;&#30830;&#35748;&#20102;LLMs&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25105;&#20204;&#36827;&#20837;LLMs&#30340;&#26032;&#26102;&#20195;&#65292;&#29702;&#35299;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#38480;&#21046;&#21644;&#24046;&#24322;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#21462;&#24471;&#36827;&#23637;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22823;&#22411;LLMs&#65288;&#20363;&#22914;ChatGPT&#65289;&#19982;&#26356;&#23567;&#20294;&#26377;&#25928;&#30340;&#24320;&#28304;LLMs&#21450;&#20854;&#31934;&#31616;&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#38271;&#31687;&#38382;&#39064;&#22238;&#31572;&#65288;LFQA&#65289;&#65292;&#22240;&#20026;&#23427;&#22312;&#23454;&#36341;&#20013;&#26377;&#22810;&#20010;&#23454;&#38469;&#19988;&#26377;&#24433;&#21709;&#21147;&#30340;&#24212;&#29992;&#65288;&#20363;&#22914;&#25925;&#38556;&#25490;&#38500;&#12289;&#23458;&#25143;&#26381;&#21153;&#31561;&#65289;&#65292;&#20294;&#23545;LLMs&#26469;&#35828;&#20173;&#28982;&#26159;&#19981;&#22815;&#30740;&#31350;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20174;&#38271;&#25991;&#26723;&#25688;&#35201;&#20013;&#29983;&#25104;&#21518;&#32493;&#38382;&#39064;&#21487;&#20197;&#20026;LLMs&#25552;&#20379;&#20174;&#38271;&#19978;&#19979;&#25991;&#36827;&#34892;&#25512;&#29702;&#21644;&#25512;&#26029;&#30340;&#25361;&#25112;&#24615;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20986;&#30340;&#20174;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#20026;LLMs&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#24182;&#26174;&#31034;&#20102;LLMs&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
As we embark on a new era of LLMs, it becomes increasingly crucial to understand their capabilities, limitations, and differences. Toward making further progress in this direction, we strive to build a deeper understanding of the gaps between massive LLMs (e.g., ChatGPT) and smaller yet effective open-source LLMs and their distilled counterparts. To this end, we specifically focus on long-form question answering (LFQA) because it has several practical and impactful applications (e.g., troubleshooting, customer service, etc.) yet is still understudied and challenging for LLMs. We propose a question-generation method from abstractive summaries and show that generating follow-up questions from summaries of long documents can create a challenging setting for LLMs to reason and infer from long contexts. Our experimental results confirm that: (1) our proposed method of generating questions from abstractive summaries pose a challenging setup for LLMs and shows performance gaps between LLMs li
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25991;&#20214;&#32534;&#30721;&#20026;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#35789;&#27719;&#29305;&#24449;&#21644;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#28508;&#22312;&#29305;&#24449;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#26816;&#32034;&#31995;&#32479;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.08187</link><description>&lt;p&gt;
&#32534;&#30721;&#27010;&#25324;&#65306;&#23558;&#25991;&#20214;&#27010;&#25324;&#20026;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#65292;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Encoded Summarization: Summarizing Documents into Continuous Vector Space for Legal Case Retrieval. (arXiv:2309.08187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25991;&#20214;&#32534;&#30721;&#20026;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#35789;&#27719;&#29305;&#24449;&#21644;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#28508;&#22312;&#29305;&#24449;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#26816;&#32034;&#31995;&#32479;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25105;&#20204;&#30340;&#30701;&#35821;&#35780;&#20998;&#26694;&#26550;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25991;&#20214;&#32534;&#30721;&#20026;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24212;&#23545;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#35789;&#27719;&#29305;&#24449;&#21644;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#28508;&#22312;&#29305;&#24449;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35789;&#27719;&#29305;&#24449;&#21644;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#28508;&#22312;&#29305;&#24449;&#30456;&#20114;&#34917;&#20805;&#65292;&#21487;&#20197;&#25552;&#39640;&#26816;&#32034;&#31995;&#32479;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26696;&#20363;&#27010;&#25324;&#22312;&#19981;&#21516;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65306;&#20351;&#29992;&#25552;&#20379;&#30340;&#27010;&#25324;&#21644;&#36827;&#34892;&#32534;&#30721;&#27010;&#25324;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#20219;&#21153;&#30340;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;65.6%&#21644;57.6%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our method for tackling a legal case retrieval task by introducing our method of encoding documents by summarizing them into continuous vector space via our phrase scoring framework utilizing deep neural networks. On the other hand, we explore the benefits from combining lexical features and latent features generated with neural networks. Our experiments show that lexical features and latent features generated with neural networks complement each other to improve the retrieval system performance. Furthermore, our experimental results suggest the importance of case summarization in different aspects: using provided summaries and performing encoded summarization. Our approach achieved F1 of 65.6% and 57.6% on the experimental datasets of legal case retrieval tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#33976;&#39311;&#23398;&#20064;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#21477;&#32423;&#35821;&#20041;&#25628;&#32034;&#20219;&#21153;&#20013;&#30340;&#20302;&#36164;&#28304;&#24773;&#20917;&#12290;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#21333;&#35821;&#21040;&#21452;&#35821;&#35821;&#20041;&#25628;&#32034;&#36827;&#34892;&#20256;&#36882;&#65292;&#24182;&#20174;&#21452;&#35821;&#21040;&#22810;&#35821;&#35821;&#20041;&#25628;&#32034;&#36827;&#34892;&#20803;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#35821;&#22659;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.08185</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#33976;&#39311;&#23398;&#20064;&#36827;&#34892;&#22810;&#35821;&#21477;&#32423;&#35821;&#20041;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multilingual Sentence-Level Semantic Search using Meta-Distillation Learning. (arXiv:2309.08185v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#33976;&#39311;&#23398;&#20064;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#21477;&#32423;&#35821;&#20041;&#25628;&#32034;&#20219;&#21153;&#20013;&#30340;&#20302;&#36164;&#28304;&#24773;&#20917;&#12290;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#21333;&#35821;&#21040;&#21452;&#35821;&#35821;&#20041;&#25628;&#32034;&#36827;&#34892;&#20256;&#36882;&#65292;&#24182;&#20174;&#21452;&#35821;&#21040;&#22810;&#35821;&#35821;&#20041;&#25628;&#32034;&#36827;&#34892;&#20803;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#35821;&#22659;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#21477;&#32423;&#35821;&#20041;&#25628;&#32034;&#26159;&#22312;&#19981;&#21516;&#35821;&#35328;&#32452;&#21512;&#20013;&#26816;&#32034;&#19982;&#26597;&#35810;&#30456;&#20851;&#20869;&#23481;&#30340;&#20219;&#21153;&#12290;&#36825;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#21547;&#20041;&#12290;&#19982;&#21333;&#35821;&#25110;&#21452;&#35821;&#25628;&#32034;&#30456;&#27604;&#65292;&#22810;&#35821;&#21477;&#32423;&#35821;&#20041;&#25628;&#32034;&#30456;&#23545;&#36739;&#23569;&#34987;&#25506;&#32034;&#19988;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#35813;&#20219;&#21153;&#32570;&#20047;&#22810;&#35821;&#35328;&#24182;&#34892;&#36164;&#28304;&#21644;&#38656;&#35201;&#35268;&#36991;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#22330;&#26223;&#30340;&#23545;&#40784;&#26041;&#27861;&#65306;MAML-Align&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#20803;&#33976;&#39311;&#23398;&#20064;&#30340;MAML&#65292;&#21363;&#22522;&#20110;&#20248;&#21270;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#22120;&#65292;&#23558;&#30693;&#35782;&#20174;&#19987;&#38376;&#20174;&#21333;&#35821;&#21040;&#21452;&#35821;&#35821;&#20041;&#25628;&#32034;&#36827;&#34892;&#20256;&#36882;&#30340;&#25945;&#24072;&#20803;&#20256;&#36882;&#27169;&#22411;T-MAML&#33976;&#39311;&#21040;&#23398;&#29983;&#27169;&#22411;S-MAML&#65292;&#21518;&#32773;&#20174;&#21452;&#35821;&#21040;&#22810;&#35821;&#35821;&#20041;&#25628;&#32034;&#36827;&#34892;&#20803;&#20256;&#36882;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#20803;&#33976;&#39311;&#24212;&#29992;&#20110;&#22810;&#35821;&#22659;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual semantic search is the task of retrieving relevant contents to a query expressed in different language combinations. This requires a better semantic understanding of the user's intent and its contextual meaning. Multilingual semantic search is less explored and more challenging than its monolingual or bilingual counterparts, due to the lack of multilingual parallel resources for this task and the need to circumvent "language bias". In this work, we propose an alignment approach: MAML-Align, specifically for low-resource scenarios. Our approach leverages meta-distillation learning based on MAML, an optimization-based Model-Agnostic Meta-Learner. MAML-Align distills knowledge from a Teacher meta-transfer model T-MAML, specialized in transferring from monolingual to bilingual semantic search, to a Student model S-MAML, which meta-transfers from bilingual to multilingual semantic search. To the best of our knowledge, we are the first to extend meta-distillation to a multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(&#22914;GPT3.5)&#21487;&#20197;&#35299;&#20915;&#21644;&#35299;&#37322;&#29289;&#29702;&#35789;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#35745;&#31639;&#21644;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#35299;&#20915;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#33021;&#22815;&#24635;&#32467;&#28041;&#21450;&#30340;&#30693;&#35782;&#12289;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#21019;&#36896;&#26032;&#30340;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08182</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#21644;&#35299;&#37322;&#29289;&#29702;&#35789;&#38382;&#39064;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Model to Solve and Explain Physics Word Problems Approaching Human Level. (arXiv:2309.08182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(&#22914;GPT3.5)&#21487;&#20197;&#35299;&#20915;&#21644;&#35299;&#37322;&#29289;&#29702;&#35789;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#35745;&#31639;&#21644;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#35299;&#20915;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#33021;&#22815;&#24635;&#32467;&#28041;&#21450;&#30340;&#30693;&#35782;&#12289;&#29983;&#25104;&#35299;&#37322;&#65292;&#24182;&#21019;&#36896;&#26032;&#30340;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22522;&#20110;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#19981;&#20165;&#21487;&#20197;&#35299;&#20915;&#32431;&#25968;&#23398;&#39064;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#29289;&#29702;&#35789;&#38382;&#39064;-&#21363;&#22522;&#20110;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#36827;&#34892;&#35745;&#31639;&#21644;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#27880;&#37322;&#20102;&#31532;&#19968;&#20010;&#29289;&#29702;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;-PhysQA&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;1000&#20010;&#21021;&#20013;&#29289;&#29702;&#35789;&#38382;&#39064;&#65288;&#21253;&#25324;&#36816;&#21160;&#23398;&#12289;&#36136;&#37327;&#21644;&#23494;&#24230;&#12289;&#21147;&#23398;&#12289;&#28909;&#23398;&#21644;&#30005;&#23398;&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;GPT3.5&#26469;&#29983;&#25104;&#36825;&#20123;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#21457;&#29616;GPT3.5&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#19978;&#33258;&#21160;&#35299;&#20915;49.3%&#30340;&#38382;&#39064;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#19978;&#21017;&#20026;73.2%&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#31867;&#20284;&#38382;&#39064;&#21450;&#20854;&#31572;&#26696;&#20316;&#20026;&#25552;&#31034;&#65292;LLM&#21487;&#20197;&#35299;&#20915;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#22522;&#30784;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;&#38500;&#20102;&#33258;&#21160;&#35299;&#20915;&#38382;&#39064;&#65292;GPT3.5&#36824;&#21487;&#20197;&#24635;&#32467;&#38382;&#39064;&#28041;&#21450;&#30340;&#30693;&#35782;&#25110;&#20027;&#39064;&#65292;&#29983;&#25104;&#30456;&#20851;&#35299;&#37322;&#65292;&#24182;&#26681;&#25454;&#36755;&#20837;&#38382;&#39064;&#32508;&#21512;&#20986;&#26032;&#30340;&#29289;&#29702;&#35789;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work demonstrates that large language model (LLM) pre-trained on texts can not only solve pure math word problems, but also physics word problems-problems to be solved by calculation and inference based on some prior physical knowledge. We collect and annotate the first physics word problem dataset-PhysQA, which contains over 1000 junior high school physics word problems (on Kinematics, Mass&amp;Density, Mechanics, Heat, Electricity). Then we use OpenAI' s GPT3.5 to generate the answer of these problems and found that GPT3.5 could automatically solve 49.3% of the problems on zero-shot learning and 73.2% on few-shot learning. This result show that by using similar problem and its answer as prompt, LLM could solve elementary physics word problems approaching human level. Besides automatically solving problems, GPT3.5 could also summarize the knowledge or topic examined by the problem, generate the relevant explanation, and synthesis new physics word problems according tothe input problem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25925;&#38556;&#27169;&#24335;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#32454;&#35843;GPT-3.5&#27169;&#22411;&#22312;&#27880;&#37322;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#24403;&#21069;&#21487;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#21644;&#24320;&#31665;&#21363;&#29992;&#30340;GPT-3.5&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.08181</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25925;&#38556;&#27169;&#24335;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Failure Mode Classification: An Investigation. (arXiv:2309.08181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25925;&#38556;&#27169;&#24335;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#32454;&#35843;GPT-3.5&#27169;&#22411;&#22312;&#27880;&#37322;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#24403;&#21069;&#21487;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#21644;&#24320;&#31665;&#21363;&#29992;&#30340;GPT-3.5&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#25925;&#38556;&#27169;&#24335;&#20998;&#31867; (FMC) &#20013;&#30340;&#26377;&#25928;&#24615;&#12290;FMC &#26159;&#22312;&#32500;&#25252;&#39046;&#22495;&#20013;&#33258;&#21160;&#20026;&#35266;&#27979;&#32467;&#26524;&#26631;&#35760;&#30456;&#24212;&#25925;&#38556;&#27169;&#24335;&#20195;&#30721;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#20943;&#23569;&#21487;&#38752;&#24615;&#24037;&#31243;&#24072;&#25163;&#21160;&#20998;&#26512;&#24037;&#20316;&#25351;&#20196;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#26469;&#20351; LLM &#33021;&#22815;&#20351;&#29992;&#21463;&#38480;&#21046;&#30340;&#20195;&#30721;&#21015;&#34920;&#26469;&#39044;&#27979;&#32473;&#23450;&#35266;&#27979;&#32467;&#26524;&#30340;&#25925;&#38556;&#27169;&#24335;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27880;&#37322;&#25968;&#25454;&#19978;&#32454;&#35843;&#30340; GPT-3.5 &#27169;&#22411; (F1=0.80) &#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#21487;&#29992;&#30340;&#22312;&#30456;&#21516;&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411; (F1=0.60)&#12290;&#32454;&#35843;&#27169;&#22411;&#36824;&#20248;&#20110;&#24320;&#31665;&#21363;&#29992;&#30340; GPT-3.5 (F1=0.46)&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#23545;&#20351;&#29992;LLMs&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the first investigation into the effectiveness of Large Language Models (LLMs) for Failure Mode Classification (FMC). FMC, the task of automatically labelling an observation with a corresponding failure mode code, is a critical task in the maintenance domain as it reduces the need for reliability engineers to spend their time manually analysing work orders. We detail our approach to prompt engineering to enable an LLM to predict the failure mode of a given observation using a restricted code list. We demonstrate that the performance of a GPT-3.5 model (F1=0.80) fine-tuned on annotated data is a significant improvement over a currently available text classification model (F1=0.60) trained on the same annotated data set. The fine-tuned model also outperforms the out-of-the box GPT-3.5 (F1=0.46). This investigation reinforces the need for high quality fine-tuning data sets for domain-specific tasks using LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#24067;&#24335;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;FedJudge&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#35774;&#22791;&#25110;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#65292;&#24182;&#23558;&#21442;&#25968;&#32858;&#21512;&#21644;&#20998;&#24067;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;&#36825;&#35299;&#20915;&#20102;&#38598;&#20013;&#24335;&#35757;&#32451;&#27861;&#24459;LLMs&#24341;&#21457;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#21644;&#20998;&#24067;&#20559;&#31227;&#23548;&#33268;&#30340;FL&#26041;&#27861;&#25928;&#26524;&#38477;&#20302;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.08173</link><description>&lt;p&gt;
FedJudge: &#20998;&#24067;&#24335;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FedJudge: Federated Legal Large Language Model. (arXiv:2309.08173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#24067;&#24335;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;FedJudge&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#35774;&#22791;&#25110;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#65292;&#24182;&#23558;&#21442;&#25968;&#32858;&#21512;&#21644;&#20998;&#24067;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;&#36825;&#35299;&#20915;&#20102;&#38598;&#20013;&#24335;&#35757;&#32451;&#27861;&#24459;LLMs&#24341;&#21457;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#21644;&#20998;&#24067;&#20559;&#31227;&#23548;&#33268;&#30340;FL&#26041;&#27861;&#25928;&#26524;&#38477;&#20302;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27861;&#24459;&#26234;&#33021;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#36741;&#21161;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#21644;&#26222;&#36890;&#20154;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27861;&#24459;LLMs&#30340;&#38598;&#20013;&#24335;&#35757;&#32451;&#24341;&#21457;&#20102;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#27861;&#24459;&#25968;&#25454;&#20998;&#25955;&#22312;&#21253;&#21547;&#25935;&#24863;&#20010;&#20154;&#20449;&#24687;&#30340;&#21508;&#20010;&#26426;&#26500;&#20043;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#23558;&#27861;&#24459;LLMs&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#65288;FL&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;FL&#65292;&#27861;&#24459;LLMs&#21487;&#20197;&#22312;&#35774;&#22791;&#25110;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#26412;&#22320;&#24494;&#35843;&#65292;&#20854;&#21442;&#25968;&#34987;&#32858;&#21512;&#24182;&#20998;&#24067;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#65292;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#32780;&#26080;&#38656;&#30452;&#25509;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#38459;&#30861;&#20102;LLMs&#22312;FL&#29615;&#22659;&#20013;&#30340;&#20840;&#38754;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#27861;&#24459;&#25968;&#25454;&#30340;&#20998;&#24067;&#20559;&#31227;&#20943;&#23569;&#20102;FL&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20998;&#24067;&#24335;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;FedJudge&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have gained prominence in the field of Legal Intelligence, offering potential applications in assisting legal professionals and laymen. However, the centralized training of these Legal LLMs raises data privacy concerns, as legal data is distributed among various institutions containing sensitive individual information. This paper addresses this challenge by exploring the integration of Legal LLMs with Federated Learning (FL) methodologies. By employing FL, Legal LLMs can be fine-tuned locally on devices or clients, and their parameters are aggregated and distributed on a central server, ensuring data privacy without directly sharing raw data. However, computation and communication overheads hinder the full fine-tuning of LLMs under the FL setting. Moreover, the distribution shift of legal data reduces the effectiveness of FL methods. To this end, in this paper, we propose the first Federated Legal Large Language Model (FedJudge) framework, which fine-tunes 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#25506;&#32034;&#30340;LLM&#20195;&#29702;&#65288;LASER&#65289;&#29992;&#20110;Web&#23548;&#33322;&#20219;&#21153;&#12290;&#35813;&#20195;&#29702;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#36716;&#25442;&#29366;&#24577;&#65292;&#36890;&#36807;&#25191;&#34892;&#21160;&#20316;&#23436;&#25104;&#20219;&#21153;&#65292;&#33021;&#22815;&#36731;&#26494;&#20174;&#38169;&#35823;&#20013;&#24674;&#22797;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.08172</link><description>&lt;p&gt;
LASER&#65306;&#20855;&#26377;&#29366;&#24577;&#31354;&#38388;&#25506;&#32034;&#30340;LLM&#20195;&#29702;&#29992;&#20110;Web&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
LASER: LLM Agent with State-Space Exploration for Web Navigation. (arXiv:2309.08172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#25506;&#32034;&#30340;LLM&#20195;&#29702;&#65288;LASER&#65289;&#29992;&#20110;Web&#23548;&#33322;&#20219;&#21153;&#12290;&#35813;&#20195;&#29702;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#36716;&#25442;&#29366;&#24577;&#65292;&#36890;&#36807;&#25191;&#34892;&#21160;&#20316;&#23436;&#25104;&#20219;&#21153;&#65292;&#33021;&#22815;&#36731;&#26494;&#20174;&#38169;&#35823;&#20013;&#24674;&#22797;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35832;&#22914;Web&#23548;&#33322;&#20043;&#31867;&#30340;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#65292;&#20294;&#20808;&#21069;&#30340;&#26041;&#27861;&#38544;&#21547;&#22320;&#20551;&#35774;&#27169;&#22411;&#21482;&#33021;&#20197;&#27491;&#21521;&#26041;&#24335;&#25191;&#34892;&#65292;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#20165;&#25552;&#20379;&#27491;&#20363;&#36712;&#36857;&#20316;&#20026;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25945;&#25480;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#25512;&#29702;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#38169;&#35823;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20132;&#20114;&#20219;&#21153;&#24314;&#27169;&#20026;&#29366;&#24577;&#31354;&#38388;&#25506;&#32034;&#65292;&#20854;&#20013;LLM&#20195;&#29702;&#36890;&#36807;&#25191;&#34892;&#21160;&#20316;&#22312;&#39044;&#23450;&#20041;&#30340;&#19968;&#32452;&#29366;&#24577;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#36825;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#28789;&#27963;&#22320;&#36827;&#34892;&#22238;&#28335;&#65292;&#20174;&#32780;&#33021;&#22815;&#36731;&#26494;&#20174;&#38169;&#35823;&#20013;&#24674;&#22797;&#12290;&#25105;&#20204;&#22312;WebShop&#20219;&#21153;&#19978;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;LASER&#20195;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;LASER&#20195;&#29702;&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been successfully adapted for interactive decision-making tasks like web navigation. While achieving decent performance, previous methods implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to teach the model how to reason in the interactive environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance. To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of states by performing actions to complete the task. This formulation enables flexible back-tracking, allowing the model to easily recover from errors. We evaluate our proposed LLM Agent with State-Space ExploRation (LASER) on the WebShop task. Experimental results show that our LASER agent significantly outperforms previo
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#25105;&#25512;&#27979;&#35299;&#30721;&#30340;Draft &amp; Verify&#26041;&#27861;&#33021;&#22815;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#25345;&#36755;&#20986;&#36136;&#37327;&#65292;&#24182;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.08168</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#25105;&#25512;&#27979;&#35299;&#30721;&#23454;&#29616;&#26080;&#25439;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#30340;Draft &amp; Verify&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Draft &amp; Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding. (arXiv:2309.08168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08168
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#25105;&#25512;&#27979;&#35299;&#30721;&#30340;Draft &amp; Verify&#26041;&#27861;&#33021;&#22815;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#25345;&#36755;&#20986;&#36136;&#37327;&#65292;&#24182;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#31216;&#20026;&#33258;&#25105;&#25512;&#27979;&#35299;&#30721;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#26469;&#23454;&#29616;&#65306;&#36215;&#33609;&#21644;&#39564;&#35777;&#12290;&#36215;&#33609;&#38454;&#27573;&#20197;&#31245;&#20302;&#36136;&#37327;&#20294;&#26356;&#24555;&#30340;&#36895;&#24230;&#29983;&#25104;&#33609;&#26696;&#26631;&#35760;&#65292;&#36825;&#26159;&#36890;&#36807;&#22312;&#36215;&#33609;&#36807;&#31243;&#20013;&#26377;&#36873;&#25321;&#22320;&#36339;&#36807;&#26576;&#20123;&#20013;&#38388;&#23618;&#26469;&#23454;&#29616;&#30340;&#12290;&#38543;&#21518;&#65292;&#39564;&#35777;&#38454;&#27573;&#20351;&#29992;&#21407;&#22987;LLM&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#39564;&#35777;&#37027;&#20123;&#36215;&#33609;&#20135;&#29983;&#30340;&#36755;&#20986;&#26631;&#35760;&#12290;&#36825;&#20010;&#36807;&#31243;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#19982;&#26410;&#20462;&#25913;&#30340;LLM&#20135;&#29983;&#30340;&#36755;&#20986;&#23436;&#20840;&#30456;&#21516;&#65292;&#20174;&#32780;&#30830;&#20445;&#36755;&#20986;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#39069;&#22806;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#19988;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#25512;&#29702;&#21152;&#36895;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel inference scheme, self-speculative decoding, for accelerating Large Language Models (LLMs) without the need for an auxiliary model. This approach is characterized by a two-stage process: drafting and verification. The drafting stage generates draft tokens at a slightly lower quality but more quickly, which is achieved by selectively skipping certain intermediate layers during drafting Subsequently, the verification stage employs the original LLM to validate those draft output tokens in one forward pass. This process ensures the final output remains identical to that produced by the unaltered LLM, thereby maintaining output quality. The proposed method requires no additional neural network training and no extra memory footprint, making it a plug-and-play and cost-effective solution for inference acceleration. Benchmarks with LLaMA-2 and its fine-tuned models demonstrated a speedup up to 1.73$\times$.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#36827;&#34892;&#27979;&#37327;&#26102;&#65292;&#19981;&#21516;&#30340;&#25552;&#31034;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#20154;&#26684;&#24471;&#20998;&#65292;&#22240;&#27492;&#32570;&#20047;&#23458;&#35266;&#26631;&#20934;&#26469;&#21028;&#26029;&#21738;&#20010;&#25552;&#31034;&#26356;&#27491;&#30830;&#12290;</title><link>http://arxiv.org/abs/2309.08163</link><description>&lt;p&gt;
&#30740;&#31350;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#27979;&#37327;&#20013;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Applicability of Self-Assessment Tests for Personality Measurement of Large Language Models. (arXiv:2309.08163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08163
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#36827;&#34892;&#27979;&#37327;&#26102;&#65292;&#19981;&#21516;&#30340;&#25552;&#31034;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#20154;&#26684;&#24471;&#20998;&#65292;&#22240;&#27492;&#32570;&#20047;&#23458;&#35266;&#26631;&#20934;&#26469;&#21028;&#26029;&#21738;&#20010;&#25552;&#31034;&#26356;&#27491;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#21457;&#23637;&#65292;&#21508;&#31181;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#20351;&#29992;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#30340;&#24515;&#29702;&#24037;&#20855;&#26469;&#37327;&#21270;&#23427;&#20204;&#30340;&#34892;&#20026;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#20351;&#29992;&#20154;&#26684;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#20154;&#26684;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#20851;&#20110;&#20351;&#29992;&#20154;&#26684;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#19977;&#20010;&#19981;&#21516;&#35770;&#25991;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#26469;&#35780;&#20272;&#21516;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#19977;&#20010;&#25552;&#31034;&#23548;&#33268;&#20102;&#38750;&#24120;&#19981;&#21516;&#30340;&#20154;&#26684;&#24471;&#20998;&#12290;&#36825;&#19968;&#31616;&#21333;&#27979;&#35797;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#26684;&#33258;&#25105;&#35780;&#20272;&#24471;&#20998;&#21462;&#20915;&#20110;&#25552;&#31034;&#32773;&#30340;&#20027;&#35266;&#36873;&#25321;&#12290;&#30001;&#20110;&#25105;&#20204;&#19981;&#30693;&#36947;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#24471;&#20998;&#30340;&#30495;&#23454;&#20540;&#65292;&#22240;&#20026;&#27492;&#31867;&#38382;&#39064;&#27809;&#26377;&#27491;&#30830;&#31572;&#26696;&#65292;&#25152;&#20197;&#26080;&#27861;&#22768;&#26126;&#26576;&#20010;&#25552;&#31034;&#27604;&#20854;&#20182;&#25552;&#31034;&#26356;&#27491;&#30830;&#25110;&#26356;&#19981;&#27491;&#30830;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20154;&#26684;&#36873;&#39033;&#39034;&#24207;&#23545;&#31216;&#24615;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLM) evolve in their capabilities, various recent studies have tried to quantify their behavior using psychological tools created to study human behavior. One such example is the measurement of "personality" of LLMs using personality self-assessment tests. In this paper, we take three such studies on personality measurement of LLMs that use personality self-assessment tests created to study human behavior. We use the prompts used in these three different papers to measure the personality of the same LLM. We find that all three prompts lead very different personality scores. This simple test reveals that personality self-assessment scores in LLMs depend on the subjective choice of the prompter. Since we don't know the ground truth value of personality scores for LLMs as there is no correct answer to such questions, there's no way of claiming if one prompt is more or less correct than the other. We then introduce the property of option order symmetry for persona
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21442;&#32771;&#30340;&#23545;&#35805;&#35780;&#20272;&#65288;RADE&#65289;&#26041;&#27861;&#21033;&#29992;&#39044;&#21019;&#24314;&#30340;&#35821;&#21477;&#20316;&#20026;&#21442;&#32771;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#19968;&#23545;&#22810;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#32534;&#30721;&#22120;&#22686;&#24378;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.08156</link><description>&lt;p&gt;
RADE: &#22522;&#20110;&#21442;&#32771;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue. (arXiv:2309.08156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08156
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#30340;&#23545;&#35805;&#35780;&#20272;&#65288;RADE&#65289;&#26041;&#27861;&#21033;&#29992;&#39044;&#21019;&#24314;&#30340;&#35821;&#21477;&#20316;&#20026;&#21442;&#32771;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#19968;&#23545;&#22810;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#32534;&#30721;&#22120;&#22686;&#24378;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#22312;&#20110;&#19968;&#23545;&#22810;&#38382;&#39064;&#65292;&#21363;&#38500;&#20102;&#40644;&#37329;&#22238;&#24212;&#20197;&#22806;&#36824;&#26377;&#35768;&#22810;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#30446;&#21069;&#65292;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#38656;&#35201;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#21487;&#38752;&#30340;&#20154;&#24037;&#35780;&#20272;&#21487;&#33021;&#32791;&#26102;&#21644;&#32791;&#36164;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21442;&#32771;&#30340;&#23545;&#35805;&#35780;&#20272;&#65288;RADE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#21019;&#24314;&#30340;&#35821;&#21477;&#20316;&#20026;&#21442;&#32771;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#40644;&#37329;&#22238;&#24212;&#65292;&#20197;&#32531;&#35299;&#19968;&#23545;&#22810;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;
&lt;/p&gt;
&lt;p&gt;
Evaluating open-domain dialogue systems is challenging for reasons such as the one-to-many problem, i.e., many appropriate responses other than just the golden response. As of now, automatic evaluation methods need better consistency with humans, while reliable human evaluation can be time- and cost-intensive. To this end, we propose the Reference-Assisted Dialogue Evaluation (RADE) approach under the multi-task learning framework, which leverages the pre-created utterance as reference other than the gold response to relief the one-to-many problem. Specifically, RADE explicitly compares reference and the candidate response to predict their overall scores. Moreover, an auxiliary response generation task enhances prediction via a shared encoder. To support RADE, we extend three datasets with additional rated responses other than just a golden response by human annotation. Experiments on our three datasets and two existing benchmarks demonstrate the effectiveness of our method, where Pear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CTC-based&#35821;&#38899;&#35782;&#21035;&#20013;&#29992;&#20110;&#23398;&#20064;&#26356;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#21644;&#32553;&#30701;&#24207;&#21015;&#38271;&#24230;&#30340;&#21333;&#27169;&#32858;&#21512;&#26041;&#27861;(UMA)&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#38598;&#25104;&#21516;&#19968;&#25991;&#26412;&#26631;&#35760;&#30340;&#29305;&#24449;&#24103;&#65292;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#35782;&#21035;&#38169;&#35823;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UMA&#22312;&#26222;&#36890;&#35805;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#38598;&#25104;&#33258;&#26465;&#20214;CTC&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08150</link><description>&lt;p&gt;
CTC-based&#35821;&#38899;&#35782;&#21035;&#30340;&#21333;&#27169;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unimodal Aggregation for CTC-based Speech Recognition. (arXiv:2309.08150v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CTC-based&#35821;&#38899;&#35782;&#21035;&#20013;&#29992;&#20110;&#23398;&#20064;&#26356;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#21644;&#32553;&#30701;&#24207;&#21015;&#38271;&#24230;&#30340;&#21333;&#27169;&#32858;&#21512;&#26041;&#27861;(UMA)&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#38598;&#25104;&#21516;&#19968;&#25991;&#26412;&#26631;&#35760;&#30340;&#29305;&#24449;&#24103;&#65292;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#35782;&#21035;&#38169;&#35823;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UMA&#22312;&#26222;&#36890;&#35805;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#38598;&#25104;&#33258;&#26465;&#20214;CTC&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38750;&#33258;&#22238;&#24402;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#36827;&#34892;&#30740;&#31350;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#27169;&#32858;&#21512;&#65288;UMA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#23646;&#20110;&#21516;&#19968;&#25991;&#26412;&#26631;&#35760;&#30340;&#29305;&#24449;&#24103;&#36827;&#34892;&#20998;&#21106;&#21644;&#38598;&#25104;&#65292;&#20174;&#32780;&#23398;&#20064;&#26356;&#22909;&#30340;&#25991;&#26412;&#26631;&#35760;&#29305;&#24449;&#34920;&#31034;&#12290;&#29305;&#24449;&#24103;&#21644;&#26435;&#37325;&#37117;&#26469;&#33258;&#20110;&#32534;&#30721;&#22120;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#21333;&#27169;&#26435;&#37325;&#38598;&#25104;&#29305;&#24449;&#24103;&#65292;&#24182;&#32463;&#36807;&#35299;&#30721;&#22120;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#35757;&#32451;&#26102;&#37319;&#29992;&#20102;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#25439;&#22833;&#12290;&#19982;&#24120;&#35268;CTC&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#21040;&#20102;&#26356;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#32553;&#30701;&#20102;&#24207;&#21015;&#38271;&#24230;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35782;&#21035;&#38169;&#35823;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;&#19977;&#20010;&#26222;&#36890;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UMA&#30456;&#27604;&#20854;&#20182;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;&#26041;&#27861;&#65288;&#22914;&#33258;&#26465;&#20214;CTC&#65289;&#34920;&#29616;&#20986;&#26356;&#22909;&#25110;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#33258;&#26465;&#20214;CTC&#38598;&#25104;&#21040;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper works on non-autoregressive automatic speech recognition. A unimodal aggregation (UMA) is proposed to segment and integrate the feature frames that belong to the same text token, and thus to learn better feature representations for text tokens. The frame-wise features and weights are both derived from an encoder. Then, the feature frames with unimodal weights are integrated and further processed by a decoder. Connectionist temporal classification (CTC) loss is applied for training. Compared to the regular CTC, the proposed method learns better feature representations and shortens the sequence length, resulting in lower recognition error and computational complexity. Experiments on three Mandarin datasets show that UMA demonstrates superior or comparable performance to other advanced non-autoregressive methods, such as self-conditioned CTC. Moreover, by integrating self-conditioned CTC into the proposed framework, the performance can be further noticeably improved.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#38899;&#39057;&#24046;&#24322;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#29305;&#24449;&#34920;&#31034;&#31354;&#38388;&#26469;&#25913;&#36827;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#21442;&#32771;&#38899;&#39057;&#21644;&#36755;&#20837;&#38899;&#39057;&#65292;&#29983;&#25104;&#25551;&#36848;&#23427;&#20204;&#24046;&#24322;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#28151;&#21512;&#25216;&#26415;&#26469;&#28040;&#38500;&#24046;&#24322;&#21644;&#21407;&#22987;&#36755;&#20837;&#20043;&#38388;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.08141</link><description>&lt;p&gt;
&#38899;&#39057;&#24046;&#24322;&#23398;&#20064;&#29992;&#20110;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio Difference Learning for Audio Captioning. (arXiv:2309.08141v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#38899;&#39057;&#24046;&#24322;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#29305;&#24449;&#34920;&#31034;&#31354;&#38388;&#26469;&#25913;&#36827;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#21442;&#32771;&#38899;&#39057;&#21644;&#36755;&#20837;&#38899;&#39057;&#65292;&#29983;&#25104;&#25551;&#36848;&#23427;&#20204;&#24046;&#24322;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#28151;&#21512;&#25216;&#26415;&#26469;&#28040;&#38500;&#24046;&#24322;&#21644;&#21407;&#22987;&#36755;&#20837;&#20043;&#38388;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21363;&#38899;&#39057;&#24046;&#24322;&#23398;&#20064;&#65292;&#29992;&#20110;&#25913;&#36827;&#38899;&#39057;&#23383;&#24149;&#29983;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#26412;&#27010;&#24565;&#26159;&#21019;&#24314;&#19968;&#20010;&#20445;&#30041;&#38899;&#39057;&#20043;&#38388;&#20851;&#31995;&#30340;&#29305;&#24449;&#34920;&#31034;&#31354;&#38388;&#65292;&#20174;&#32780;&#33021;&#22815;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#22797;&#26434;&#38899;&#39057;&#20449;&#24687;&#30340;&#23383;&#24149;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#21442;&#32771;&#38899;&#39057;&#21644;&#36755;&#20837;&#38899;&#39057;&#65292;&#36890;&#36807;&#20849;&#20139;&#32534;&#30721;&#22120;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#20174;&#36825;&#20123;&#24046;&#24322;&#29305;&#24449;&#29983;&#25104;&#23383;&#24149;&#25551;&#36848;&#23427;&#20204;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#25216;&#26415;&#65292;&#28041;&#21450;&#23558;&#36755;&#20837;&#38899;&#39057;&#19982;&#39069;&#22806;&#38899;&#39057;&#28151;&#21512;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#38899;&#39057;&#20316;&#20026;&#21442;&#32771;&#12290;&#36825;&#26679;&#65292;&#28151;&#21512;&#38899;&#39057;&#19982;&#21442;&#32771;&#38899;&#39057;&#20043;&#38388;&#30340;&#24046;&#24322;&#22238;&#21040;&#21407;&#22987;&#36755;&#20837;&#38899;&#39057;&#12290;&#36825;&#20801;&#35768;&#23558;&#21407;&#22987;&#36755;&#20837;&#30340;&#23383;&#24149;&#20316;&#20026;&#20854;&#24046;&#24322;&#30340;&#23383;&#24149;&#20351;&#29992;&#65292;&#28040;&#38500;&#20102;&#20026;&#24046;&#24322;&#28155;&#21152;&#39069;&#22806;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel training paradigm, audio difference learning, for improving audio captioning. The fundamental concept of the proposed learning method is to create a feature representation space that preserves the relationship between audio, enabling the generation of captions that detail intricate audio information. This method employs a reference audio along with the input audio, both of which are transformed into feature representations via a shared encoder. Captions are then generated from these differential features to describe their differences. Furthermore, a unique technique is proposed that involves mixing the input audio with additional audio, and using the additional audio as a reference. This results in the difference between the mixed audio and the reference audio reverting back to the original input audio. This allows the original input's caption to be used as the caption for their difference, eliminating the need for additional annotations for the difference
&lt;/p&gt;</description></item><item><title>PromptTTS++&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35828;&#35805;&#32773;&#25552;&#31034;&#26469;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#22768;&#23398;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2309.08140</link><description>&lt;p&gt;
PromptTTS++&#65306;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25511;&#21046;&#25552;&#31034;&#24335;&#25991;&#26412;&#36716;&#35821;&#38899;&#20013;&#30340;&#35828;&#35805;&#32773;&#36523;&#20221;
&lt;/p&gt;
&lt;p&gt;
PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions. (arXiv:2309.08140v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08140
&lt;/p&gt;
&lt;p&gt;
PromptTTS++&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35828;&#35805;&#32773;&#25552;&#31034;&#26469;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#22768;&#23398;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PromptTTS++&#65292;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#21512;&#25104;&#31995;&#32479;&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#20026;&#20102;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;TTS&#26694;&#26550;&#20013;&#25511;&#21046;&#35828;&#35805;&#32773;&#36523;&#20221;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35828;&#35805;&#32773;&#25552;&#31034;&#30340;&#27010;&#24565;&#65292;&#35813;&#25552;&#31034;&#25551;&#36848;&#20102;&#35821;&#38899;&#29305;&#24449;&#65288;&#22914;&#20013;&#24615;&#12289;&#24180;&#36731;&#12289;&#32769;&#24180;&#21644;&#27785;&#38391;&#65289;&#65292;&#26088;&#22312;&#19982;&#35828;&#35805;&#39118;&#26684;&#22823;&#33268;&#29420;&#31435;&#12290;&#30001;&#20110;&#30446;&#21069;&#27809;&#26377;&#21253;&#21547;&#35828;&#35805;&#32773;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;LibriTTS-R&#35821;&#26009;&#24211;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#35828;&#35805;&#32773;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#22768;&#23398;&#27169;&#22411;&#19982;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#26469;&#24314;&#27169;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22810;&#26679;&#21270;&#35828;&#35805;&#32773;&#22240;&#32032;&#12290;&#19982;&#20043;&#21069;&#20165;&#20381;&#36182;&#26679;&#24335;&#25552;&#31034;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26679;&#24335;&#25552;&#31034;&#20165;&#25551;&#36848;&#20102;&#35828;&#35805;&#32773;&#20010;&#24615;&#21270;&#30340;&#26377;&#38480;&#26041;&#38754;&#65292;&#22914;&#38899;&#35843;&#12289;&#35828;&#35805;&#36895;&#24230;&#21644;&#33021;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39069;&#22806;&#30340;&#35828;&#35805;&#32773;&#25552;&#31034;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21040;&#22768;&#23398;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system that allows control over speaker identity using natural language descriptions. To control speaker identity within the prompt-based TTS framework, we introduce the concept of speaker prompt, which describes voice characteristics (e.g., gender-neutral, young, old, and muffled) designed to be approximately independent of speaking style. Since there is no large-scale dataset containing speaker prompts, we first construct a dataset based on the LibriTTS-R corpus with manually annotated speaker prompts. We then employ a diffusion-based acoustic model with mixture density networks to model diverse speaker factors in the training data. Unlike previous studies that rely on style prompts describing only a limited aspect of speaker individuality, such as pitch, speaking speed, and energy, our method utilizes an additional speaker prompt to effectively learn the mapping from natural language descriptions to the acoustic f
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#23454;&#20307;&#37051;&#22495;&#20449;&#24687;&#21644;&#25551;&#36848;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#32534;&#31243;&#35774;&#35745;&#35838;&#31243;&#30693;&#35782;&#22270;&#35889;&#20013;&#23884;&#20837;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.08100</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#23454;&#20307;&#37051;&#22495;&#20449;&#24687;&#21644;&#25551;&#36848;&#20449;&#24687;&#30340;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#12299;
&lt;/p&gt;
&lt;p&gt;
Research on Joint Representation Learning Methods for Entity Neighborhood Information and Description Information. (arXiv:2309.08100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08100
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#23454;&#20307;&#37051;&#22495;&#20449;&#24687;&#21644;&#25551;&#36848;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#32534;&#31243;&#35774;&#35745;&#35838;&#31243;&#30693;&#35782;&#22270;&#35889;&#20013;&#23884;&#20837;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;&#32534;&#31243;&#35774;&#35745;&#35838;&#31243;&#30693;&#35782;&#22270;&#35889;&#20013;&#23884;&#20837;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23454;&#20307;&#37051;&#22495;&#20449;&#24687;&#21644;&#25551;&#36848;&#20449;&#24687;&#30340;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#37319;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#33719;&#21462;&#23454;&#20307;&#37051;&#22495;&#33410;&#28857;&#30340;&#29305;&#24449;&#65292;&#32467;&#21512;&#20851;&#31995;&#29305;&#24449;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;BERT-WWM&#27169;&#22411;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#33719;&#21462;&#23454;&#20307;&#25551;&#36848;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23558;&#23454;&#20307;&#37051;&#22495;&#20449;&#24687;&#21644;&#25551;&#36848;&#20449;&#24687;&#30340;&#21521;&#37327;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#26368;&#32456;&#30340;&#23454;&#20307;&#21521;&#37327;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#32534;&#31243;&#35774;&#35745;&#35838;&#31243;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the issue of poor embedding performance in the knowledge graph of a programming design course, a joint represen-tation learning model that combines entity neighborhood infor-mation and description information is proposed. Firstly, a graph at-tention network is employed to obtain the features of entity neigh-boring nodes, incorporating relationship features to enrich the structural information. Next, the BERT-WWM model is utilized in conjunction with attention mechanisms to obtain the representation of entity description information. Finally, the final entity vector representation is obtained by combining the vector representations of entity neighborhood information and description information. Experimental results demonstrate that the proposed model achieves favorable performance on the knowledge graph dataset of the pro-gramming design course, outperforming other baseline models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21051;&#30011;&#20102;&#36890;&#29992;&#35821;&#38899;&#34920;&#31034;&#30340;&#26102;&#24207;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#34920;&#31034;&#21160;&#24577;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#21487;&#25512;&#24191;&#24615;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.08099</link><description>&lt;p&gt;
&#21051;&#30011;&#36890;&#29992;&#35821;&#38899;&#34920;&#31034;&#30340;&#26102;&#24207;&#21160;&#24577;&#65292;&#23454;&#29616;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Characterizing the temporal dynamics of universal speech representations for generalizable deepfake detection. (arXiv:2309.08099v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21051;&#30011;&#20102;&#36890;&#29992;&#35821;&#38899;&#34920;&#31034;&#30340;&#26102;&#24207;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#34920;&#31034;&#21160;&#24577;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#21487;&#25512;&#24191;&#24615;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#28145;&#24230;&#20266;&#36896;&#35821;&#38899;&#26816;&#27979;&#31995;&#32479;&#22312;&#38754;&#23545;&#26410;&#35265;&#36807;&#30340;&#25915;&#20987;&#26679;&#26412;&#65288;&#21363;&#30001;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#29983;&#25104;&#31639;&#27861;&#29983;&#25104;&#30340;&#26679;&#26412;&#65289;&#26102;&#32570;&#20047;&#21487;&#25512;&#24191;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#36890;&#29992;&#35821;&#38899;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21019;&#26032;&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#32780;&#23545;&#34920;&#31034;&#26412;&#36523;&#20960;&#20046;&#27809;&#26377;&#25913;&#36827;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#25581;&#31034;&#36825;&#20123;&#34920;&#31034;&#30340;&#38271;&#26399;&#26102;&#24207;&#21160;&#24577;&#23545;&#20110;&#25552;&#39640;&#21487;&#25512;&#24191;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#34920;&#31034;&#21160;&#24577;&#30340;&#26032;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19979;&#29983;&#25104;&#20102;&#31867;&#20284;&#30340;&#34920;&#31034;&#21160;&#24577;&#27169;&#24335;&#12290;&#22312;ASVspoof 2019&#21644;2021&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#26816;&#27979;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#28145;&#24230;&#20266;&#36896;&#26041;&#27861;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#26174;&#33879;&#25913;&#36827;&#20102;&#22810;&#20010;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing deepfake speech detection systems lack generalizability to unseen attacks (i.e., samples generated by generative algorithms not seen during training). Recent studies have explored the use of universal speech representations to tackle this issue and have obtained inspiring results. These works, however, have focused on innovating downstream classifiers while leaving the representation itself untouched. In this study, we argue that characterizing the long-term temporal dynamics of these representations is crucial for generalizability and propose a new method to assess representation dynamics. Indeed, we show that different generative models generate similar representation dynamics patterns with our proposed method. Experiments on the ASVspoof 2019 and 2021 datasets validate the benefits of the proposed method to detect deepfakes from methods unseen during training, significantly improving on several benchmark methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;&#31038;&#20250;&#31185;&#23398;&#26041;&#27861;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#22312;&#20998;&#26512;&#23186;&#20307;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#35770;&#40511;&#27807;&#30340;&#21487;&#33021;&#26041;&#21521;&#65292;&#21253;&#25324;&#27169;&#22411;&#36879;&#26126;&#24230;&#12289;&#32771;&#34385;&#25991;&#26723;&#22806;&#37096;&#20449;&#24687;&#21644;&#36328;&#25991;&#26723;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.08069</link><description>&lt;p&gt;
&#22312;&#26032;&#38395;&#20998;&#26512;&#20013;&#36830;&#25509;&#21508;&#20010;&#28857;&#65306;&#20851;&#20110;&#23186;&#20307;&#20559;&#35265;&#21644;&#26694;&#26550;&#30340;&#36328;&#23398;&#31185;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots in News Analysis: A Cross-Disciplinary Survey of Media Bias and Framing. (arXiv:2309.08069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;&#31038;&#20250;&#31185;&#23398;&#26041;&#27861;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#22312;&#20998;&#26512;&#23186;&#20307;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#35770;&#40511;&#27807;&#30340;&#21487;&#33021;&#26041;&#21521;&#65292;&#21253;&#25324;&#27169;&#22411;&#36879;&#26126;&#24230;&#12289;&#32771;&#34385;&#25991;&#26723;&#22806;&#37096;&#20449;&#24687;&#21644;&#36328;&#25991;&#26723;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#35265;&#22312;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#34920;&#29616;&#21644;&#24433;&#21709;&#26159;&#31038;&#20250;&#31185;&#23398;&#20960;&#21313;&#24180;&#26469;&#30340;&#26680;&#24515;&#35758;&#39064;&#65292;&#24182;&#19988;&#36817;&#24180;&#26469;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#24110;&#21161;&#25193;&#22823;&#20998;&#26512;&#35268;&#27169;&#25110;&#25552;&#20379;&#33258;&#21160;&#21270;&#31243;&#24207;&#26469;&#35843;&#26597;&#20559;&#35265;&#26032;&#38395;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#20027;&#23548;&#22320;&#20301;&#30340;&#26041;&#27861;&#35770;&#26410;&#33021;&#35299;&#20915;&#29702;&#35770;&#23186;&#20307;&#30740;&#31350;&#20013;&#28041;&#21450;&#30340;&#22797;&#26434;&#38382;&#39064;&#21644;&#24433;&#21709;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#31038;&#20250;&#31185;&#23398;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#29992;&#20110;&#20998;&#26512;&#23186;&#20307;&#20559;&#35265;&#30340;&#20856;&#22411;&#20219;&#21153;&#24418;&#24335;&#12289;&#26041;&#27861;&#21644;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#29702;&#35770;&#27169;&#22411;&#21644;&#39044;&#27979;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#35780;&#20272;&#20043;&#38388;&#40511;&#27807;&#30340;&#21487;&#33021;&#26041;&#21521;&#12290;&#36825;&#20123;&#21253;&#25324;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12289;&#32771;&#34385;&#25991;&#26723;&#22806;&#37096;&#20449;&#24687;&#12289;&#20197;&#21450;&#36328;&#25991;&#26723;&#25512;&#29702;&#32780;&#38750;&#21333;&#26631;&#31614;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
The manifestation and effect of bias in news reporting have been central topics in the social sciences for decades, and have received increasing attention in the NLP community recently. While NLP can help to scale up analyses or contribute automatic procedures to investigate the impact of biased news in society, we argue that methodologies that are currently dominant fall short of addressing the complex questions and effects addressed in theoretical media studies. In this survey paper, we review social science approaches and draw a comparison with typical task formulations, methods, and evaluation metrics used in the analysis of media bias in NLP. We discuss open questions and suggest possible directions to close identified gaps between theory and predictive models, and their evaluation. These include model transparency, considering document-external information, and cross-document reasoning rather than single-label assignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26032;&#38395;&#27010;&#36848;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20250;&#37325;&#22797;&#21644;&#24378;&#21270;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#22411;&#20013;&#30340;&#26377;&#20559;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#20855;&#26377;&#25511;&#21046;&#20154;&#21475;&#23646;&#24615;&#30340;&#36755;&#20837;&#25991;&#26723;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08047</link><description>&lt;p&gt;
&#35843;&#26597;&#26032;&#38395;&#27010;&#36848;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Investigating Gender Bias in News Summarization. (arXiv:2309.08047v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26032;&#38395;&#27010;&#36848;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20250;&#37325;&#22797;&#21644;&#24378;&#21270;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#37327;&#21270;&#27169;&#22411;&#20013;&#30340;&#26377;&#20559;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#20855;&#26377;&#25511;&#21046;&#20154;&#21475;&#23646;&#24615;&#30340;&#36755;&#20837;&#25991;&#26723;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#36848;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#12290;&#20197;&#24448;&#23545;&#27010;&#36848;&#27169;&#22411;&#30340;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#23427;&#20204;&#22312;&#20869;&#23481;&#36873;&#25321;&#12289;&#35821;&#27861;&#27491;&#30830;&#24615;&#21644;&#36830;&#36143;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;LLMs&#20250;&#37325;&#22797;&#21644;&#24378;&#21270;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#19968;&#20010;&#30456;&#23545;&#21463;&#38480;&#21046;&#30340;&#29615;&#22659;&#65292;&#27604;&#22914;&#27010;&#36848;&#65292;&#36825;&#20123;&#20559;&#35265;&#20250;&#23545;&#27169;&#22411;&#30340;&#36755;&#20986;&#20135;&#29983;&#24433;&#21709;&#21527;&#65311;&#20026;&#20102;&#35299;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20123;&#20851;&#20110;&#27010;&#36848;&#27169;&#22411;&#20013;&#30340;&#26377;&#20559;&#34892;&#20026;&#30340;&#23450;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20123;&#23454;&#38469;&#26041;&#27861;&#26469;&#37327;&#21270;&#23427;&#20204;&#12290;&#30001;&#20110;&#25105;&#20204;&#21457;&#29616;&#36755;&#20837;&#25991;&#26723;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#24178;&#25200;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;&#20180;&#32454;&#25511;&#21046;&#20154;&#21475;&#23646;&#24615;&#30340;&#36755;&#20837;&#25991;&#26723;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35268;&#36991;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20173;&#28982;&#20351;&#29992;&#19968;&#20123;&#29616;&#23454;&#30340;&#36755;&#20837;&#25991;&#26723;&#36827;&#34892;&#24037;&#20316;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19987;&#38376;&#26500;&#24314;&#30340;&#27010;&#36848;&#27169;&#22411;&#21644;&#36890;&#29992;&#29992;&#36884;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarization is an important application of large language models (LLMs). Most previous evaluation of summarization models has focused on their performance in content selection, grammaticality and coherence. However, it is well known that LLMs reproduce and reinforce harmful social biases. This raises the question: Do these biases affect model outputs in a relatively constrained setting like summarization?  To help answer this question, we first motivate and introduce a number of definitions for biased behaviours in summarization models, along with practical measures to quantify them. Since we find biases inherent to the input document can confound our analysis, we additionally propose a method to generate input documents with carefully controlled demographic attributes. This allows us to sidestep this issue, while still working with somewhat realistic input documents.  Finally, we apply our measures to summaries generated by both purpose-built summarization models and general purpose
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AV2Wav&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#65292;&#20811;&#26381;&#20102;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22768;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08030</link><description>&lt;p&gt;
AV2Wav&#65306;&#22522;&#20110;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#30340;&#25193;&#25955;&#37325;&#21512;&#25104;&#25216;&#26415;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement. (arXiv:2309.08030v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AV2Wav&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#65292;&#20811;&#26381;&#20102;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22768;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#24178;&#20928;&#21644;&#22122;&#22768;&#35821;&#38899;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#20013;&#65292;&#24178;&#20928;&#30340;&#25968;&#25454;&#19981;&#22815;&#22810;&#65307;&#22823;&#22810;&#25968;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#38598;&#37117;&#26159;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#65292;&#21253;&#21547;&#32972;&#26223;&#22122;&#22768;&#21644;&#28151;&#21709;&#65292;&#36825;&#38459;&#30861;&#20102;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AV2Wav&#65292;&#19968;&#31181;&#22522;&#20110;&#37325;&#21512;&#25104;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#19979;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#36136;&#37327;&#20272;&#35745;&#22120;&#20174;&#38899;&#39057;-&#35270;&#35273;&#35821;&#26009;&#24211;&#20013;&#33719;&#21462;&#20960;&#20046;&#24178;&#20928;&#30340;&#35821;&#38899;&#23376;&#38598;&#65292;&#24182;&#22312;&#27492;&#23376;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#26469;&#33258;AV-HuBERT&#30340;&#36830;&#32493;&#35821;&#38899;&#34920;&#31034;&#29983;&#25104;&#22768;&#27874;&#24418;&#65292;&#20855;&#26377;&#22122;&#22768;&#40065;&#26834;&#35757;&#32451;&#12290;&#25105;&#20204;&#20351;&#29992;&#36830;&#32493;&#32780;&#19981;&#26159;&#31163;&#25955;&#34920;&#31034;&#26469;&#20445;&#30041;&#38901;&#24459;&#21644;&#35828;&#35805;&#32773;&#20449;&#24687;&#12290;&#20165;&#20165;&#36890;&#36807;&#22768;&#30721;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#23601;&#27604;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26356;&#22909;&#22320;&#25191;&#34892;&#35821;&#38899;&#22686;&#24378;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;fine-tune&#27169;&#22411;&#65292;&#20197;&#36716;&#21270;&#20026;&#22312;&#22810;&#20219;&#21153;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#32852;&#21512;&#22810;&#24103;&#22768;&#23398;&#21040;&#35821;&#38899;&#36716;&#21270;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech enhancement systems are typically trained using pairs of clean and noisy speech. In audio-visual speech enhancement (AVSE), there is not as much ground-truth clean data available; most audio-visual datasets are collected in real-world environments with background noise and reverberation, hampering the development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based audio-visual speech enhancement approach that can generate clean speech despite the challenges of real-world training data. We obtain a subset of nearly clean speech from an audio-visual corpus using a neural quality estimator, and then train a diffusion model on this subset to generate waveforms conditioned on continuous speech representations from AV-HuBERT with noise-robust training. We use continuous rather than discrete representations to retain prosody and speaker information. With this vocoding task alone, the model can perform speech enhancement better than a masking-based baseline. We further fine-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#20116;&#20010;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25928;&#26524;&#65292;&#20026;&#35299;&#38145;&#20020;&#24202;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.08008</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#25552;&#31034;&#31574;&#30053;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing. (arXiv:2309.08008v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08008
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#20116;&#20010;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#25928;&#26524;&#65292;&#20026;&#35299;&#38145;&#20020;&#24202;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#25110;&#26114;&#36149;&#30340;&#39046;&#22495;&#65292;&#22914;&#20020;&#24202;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#35201;&#35299;&#38145;&#36825;&#20123;LLMs&#20013;&#38544;&#34255;&#30340;&#20020;&#24202;&#30693;&#35782;&#65292;&#25105;&#20204;&#38656;&#35201;&#35774;&#35745;&#26377;&#25928;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#24341;&#23548;&#23427;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#29305;&#23450;&#30340;&#20020;&#24202;NLP&#20219;&#21153;&#12290;&#36825;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#38376;&#38656;&#35201;&#20102;&#35299;&#19981;&#21516;LLMs&#21644;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#30340;&#33402;&#26415;&#21644;&#31185;&#23398;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#32780;&#31995;&#32479;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#38024;&#23545;&#20116;&#20010;&#20020;&#24202;NLP&#20219;&#21153;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#30340;&#35780;&#20272;&#65306;&#20020;&#24202;&#24847;&#20041;&#28040;&#27495;&#12289;&#29983;&#29289;&#21307;&#23398;&#35777;&#25454;&#25552;&#21462;&#12289;&#20849;&#25351;&#28040;&#35299;&#12289;&#33647;&#29289;&#29366;&#24577;&#25552;&#21462;&#21644;&#33647;&#29289;&#23646;&#24615;&#25552;&#21462;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#36817;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21253;&#25324;&#31616;&#21333;&#21069;&#32512;&#12289;&#31616;&#21333;&#22635;&#31354;&#12289;&#24605;&#32500;&#38142;&#21644;&#39044;&#26399;&#25552;&#31034;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduce
&lt;/p&gt;</description></item><item><title>DiariST&#26159;&#31532;&#19968;&#20010;&#27969;&#24335;&#35821;&#38899;&#32763;&#35793;&#21644;&#35828;&#35805;&#32773;&#20998;&#31163;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#38598;&#25104;&#26631;&#35760;&#32423;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#21644;t&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;ST&#21644;SD&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08007</link><description>&lt;p&gt;
DiariST: &#24102;&#26377;&#35828;&#35805;&#32773;&#20998;&#31163;&#30340;&#27969;&#24335;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
DiariST: Streaming Speech Translation with Speaker Diarization. (arXiv:2309.08007v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08007
&lt;/p&gt;
&lt;p&gt;
DiariST&#26159;&#31532;&#19968;&#20010;&#27969;&#24335;&#35821;&#38899;&#32763;&#35793;&#21644;&#35828;&#35805;&#32773;&#20998;&#31163;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#38598;&#25104;&#26631;&#35760;&#32423;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#21644;t&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;ST&#21644;SD&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23545;&#35805;&#24405;&#38899;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#28041;&#21450;&#19968;&#20123;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#30340;&#25361;&#25112;&#65292;&#22914;&#27809;&#26377;&#20934;&#30830;&#30340;&#35789;&#26102;&#38388;&#25139;&#30340;&#35828;&#35805;&#32773;&#20998;&#31163;&#65288;SD&#65289;&#21644;&#22788;&#29702;&#37325;&#21472;&#35821;&#38899;&#30340;&#27969;&#24335;&#26041;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiariST&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#27969;&#24335;&#30340;ST&#21644;SD&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#22522;&#20110;&#22522;&#20110;&#31070;&#32463;&#20256;&#36882;&#22120;&#30340;&#27969;&#24335;ST&#31995;&#32479;&#26500;&#24314;&#65292;&#24182;&#38598;&#25104;&#20102;&#26368;&#21021;&#29992;&#20110;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#30340;&#26631;&#35760;&#32423;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#21644;t&#21521;&#37327;&#12290;&#30001;&#20110;&#35813;&#39046;&#22495;&#32570;&#20047;&#35780;&#20272;&#22522;&#20934;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;AliMeeting&#35821;&#26009;&#24211;&#30340;&#21442;&#32771;&#20013;&#25991;&#36716;&#24405;&#25104;&#33521;&#25991;&#26469;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;DiariST-AliMeeting&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#38750;&#29305;&#23450;&#35828;&#35805;&#32773;BLEU&#21644;&#29305;&#23450;&#35828;&#35805;&#32773;BLEU&#65292;&#20197;&#34913;&#37327;ST&#30340;&#36136;&#37327;&#65292;&#24182;&#32771;&#34385;SD&#30340;&#20934;&#30830;&#24615;&#12290;&#19982;&#22522;&#20110;Whisper&#30340;&#31163;&#32447;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#36827;&#34892;&#27969;&#24335;&#25512;&#29702;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;ST&#21644;SD&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end speech translation (ST) for conversation recordings involves several under-explored challenges such as speaker diarization (SD) without accurate word time stamps and handling of overlapping speech in a streaming fashion. In this work, we propose DiariST, the first streaming ST and SD solution. It is built upon a neural transducer-based streaming ST system and integrates token-level serialized output training and t-vector, which were originally developed for multi-talker speech recognition. Due to the absence of evaluation benchmarks in this area, we develop a new evaluation dataset, DiariST-AliMeeting, by translating the reference Chinese transcriptions of the AliMeeting corpus into English. We also propose new metrics, called speaker-agnostic BLEU and speaker-attributed BLEU, to measure the ST quality while taking SD accuracy into account. Our system achieves a strong ST and SD capability compared to offline systems based on Whisper, while performing streaming inference for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;4&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#21592;&#32676;&#20307;&#23545;4&#20010;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#27979;&#35797;&#65292;&#20998;&#26512;&#20102;&#35780;&#20272;&#21592;&#32676;&#20307;&#23545;&#23545;&#35805;&#31995;&#32479;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20110;Likert&#35780;&#20272;&#65292;&#35780;&#20272;&#21592;&#32676;&#20307;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#32780;Pairwise&#35780;&#20272;&#27809;&#26377;&#12290;&#27492;&#22806;&#65292;&#36824;&#21457;&#29616;&#20102;&#19981;&#21516;&#35780;&#20272;&#21592;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#35780;&#20272;&#21592;&#30340;&#23458;&#35266;&#24615;&#26159;&#26377;&#30410;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.07998</link><description>&lt;p&gt;
&#25506;&#32034;&#20154;&#31867;&#35780;&#20272;&#21592;&#32676;&#20307;&#23545;&#38754;&#21521;&#23545;&#35805;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Human Evaluator Group on Chat-Oriented Dialogue Evaluation. (arXiv:2309.07998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;4&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#21592;&#32676;&#20307;&#23545;4&#20010;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#27979;&#35797;&#65292;&#20998;&#26512;&#20102;&#35780;&#20272;&#21592;&#32676;&#20307;&#23545;&#23545;&#35805;&#31995;&#32479;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#20110;Likert&#35780;&#20272;&#65292;&#35780;&#20272;&#21592;&#32676;&#20307;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#32780;Pairwise&#35780;&#20272;&#27809;&#26377;&#12290;&#27492;&#22806;&#65292;&#36824;&#21457;&#29616;&#20102;&#19981;&#21516;&#35780;&#20272;&#21592;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#35780;&#20272;&#21592;&#30340;&#23458;&#35266;&#24615;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#35780;&#20272;&#34987;&#24191;&#27867;&#25509;&#21463;&#20316;&#20026;&#35780;&#20272;&#38754;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#22312;&#25307;&#21215;&#35780;&#20272;&#21592;&#30340;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#35780;&#20272;&#21592;&#32676;&#20307;&#65292;&#22914;&#39046;&#22495;&#19987;&#23478;&#12289;&#22823;&#23398;&#29983;&#21644;&#19987;&#19994;&#26631;&#27880;&#21592;&#65292;&#24050;&#34987;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#23545;&#35805;&#31995;&#32479;&#65292;&#23613;&#31649;&#19981;&#28165;&#26970;&#35780;&#20272;&#21592;&#32676;&#20307;&#30340;&#36873;&#25321;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;4&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#21592;&#32676;&#20307;&#23545;4&#20010;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#27979;&#35797;&#65292;&#20998;&#26512;&#20102;&#35780;&#20272;&#21592;&#32676;&#20307;&#23545;&#23545;&#35805;&#31995;&#32479;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;Likert&#35780;&#20272;&#22312;&#35780;&#20272;&#21592;&#32676;&#20307;&#26041;&#38754;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;Pairwise&#35780;&#20272;&#27809;&#26377;&#36825;&#31181;&#31283;&#20581;&#24615;&#65292;&#22312;&#26356;&#25913;&#35780;&#20272;&#21592;&#32676;&#20307;&#26102;&#21482;&#35266;&#23519;&#21040;&#20102;&#19968;&#20123;&#32454;&#24494;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#20004;&#20010;&#26174;&#33879;&#30340;&#23616;&#38480;&#24615;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#27700;&#24179;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#19987;&#38271;&#30340;&#35780;&#20272;&#21592;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#34920;&#26126;&#35780;&#20272;&#21592;&#30340;&#23458;&#35266;&#24615;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human evaluation has been widely accepted as the standard for evaluating chat-oriented dialogue systems. However, there is a significant variation in previous work regarding who gets recruited as evaluators. Evaluator groups such as domain experts, university students, and professional annotators have been used to assess and compare dialogue systems, although it is unclear to what extent the choice of an evaluator group can affect results. This paper analyzes the evaluator group impact on dialogue system evaluation by testing 4 state-of-the-art dialogue systems using 4 distinct evaluator groups. Our analysis reveals a robustness towards evaluator groups for Likert evaluations that is not seen for Pairwise, with only minor differences observed when changing evaluator groups. Furthermore, two notable limitations to this robustness are observed, which reveal discrepancies between evaluators with different levels of chatbot expertise and indicate that evaluator objectivity is beneficial fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#23454;&#20307;&#37325;&#35201;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#27604;&#20256;&#32479;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20351;&#29992;&#25351;&#20196;&#35843;&#26657;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#25552;&#31034;&#25928;&#26524;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.07990</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#22659;&#20449;&#24687;&#23454;&#29616;&#26377;&#25928;&#30340;&#23454;&#20307;&#37325;&#35201;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Leveraging Contextual Information for Effective Entity Salience Detection. (arXiv:2309.07990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#23454;&#20307;&#37325;&#35201;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#27604;&#20256;&#32479;&#30340;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20351;&#29992;&#25351;&#20196;&#35843;&#26657;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#25552;&#31034;&#25928;&#26524;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#38395;&#25991;&#31456;&#31561;&#25991;&#26412;&#25991;&#26723;&#20013;&#65292;&#20869;&#23481;&#21644;&#20851;&#38190;&#20107;&#20214;&#36890;&#24120;&#22260;&#32469;&#30528;&#25991;&#26723;&#20013;&#25552;&#21040;&#30340;&#19968;&#37096;&#20998;&#23454;&#20307;&#23637;&#24320;&#12290;&#36825;&#20123;&#23454;&#20307;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#37325;&#35201;&#30340;&#23454;&#20307;&#65292;&#23545;&#20110;&#35835;&#32773;&#26469;&#35828;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25991;&#26723;&#20869;&#23481;&#30340;&#26377;&#29992;&#32447;&#32034;&#12290;&#35782;&#21035;&#23454;&#20307;&#30340;&#37325;&#35201;&#24615;&#22312;&#25628;&#32034;&#12289;&#25490;&#21517;&#21644;&#22522;&#20110;&#23454;&#20307;&#30340;&#25688;&#35201;&#31561;&#22810;&#20010;&#19979;&#28216;&#24212;&#29992;&#20013;&#37117;&#26377;&#24110;&#21161;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#29305;&#24449;&#24037;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;&#20132;&#21449;&#32534;&#30721;&#22120;&#39118;&#26684;&#26550;&#26500;&#23545;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#27604;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20351;&#29992;&#20195;&#34920;&#20013;&#31561;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20351;&#29992;&#25351;&#20196;&#35843;&#26657;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#25552;&#31034;&#20250;&#20135;&#29983;&#36739;&#24046;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#25351;&#20196;&#35843;&#26657;&#30340;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#30340;&#38382;&#39064;&#25968;&#37327;&#36807;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
In text documents such as news articles, the content and key events usually revolve around a subset of all the entities mentioned in a document. These entities, often deemed as salient entities, provide useful cues of the aboutness of a document to a reader. Identifying the salience of entities was found helpful in several downstream applications such as search, ranking, and entity-centric summarization, among others. Prior work on salient entity detection mainly focused on machine learning models that require heavy feature engineering. We show that fine-tuning medium-sized language models with a cross-encoder style architecture yields substantial performance gains over feature engineering approaches. To this end, we conduct a comprehensive benchmarking of four publicly available datasets using models representative of the medium-sized pre-trained language model family. Additionally, we show that zero-shot prompting of instruction-tuned language models yields inferior results, indicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#21033;&#29992;My Science Tutor&#65288;MyST&#65289;&#20799;&#31461;&#35821;&#38899;&#35821;&#26009;&#24211;&#21644;&#26356;&#26377;&#25928;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26469;&#25913;&#36827;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#23545;&#20799;&#31461;&#35821;&#38899;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#23558;Whisper&#31995;&#32479;&#25972;&#21512;&#21040;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#26174;&#31034;&#20102;&#34920;&#29616;&#21487;&#34892;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.07927</link><description>&lt;p&gt;
Kid-Whisper: &#21161;&#21147;&#22635;&#34917;&#20799;&#31461;&#19982;&#25104;&#20154;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#24046;&#36317;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults. (arXiv:2309.07927v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#21033;&#29992;My Science Tutor&#65288;MyST&#65289;&#20799;&#31461;&#35821;&#38899;&#35821;&#26009;&#24211;&#21644;&#26356;&#26377;&#25928;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26469;&#25913;&#36827;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#23545;&#20799;&#31461;&#35821;&#38899;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#23558;Whisper&#31995;&#32479;&#25972;&#21512;&#21040;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#26174;&#31034;&#20102;&#34920;&#29616;&#21487;&#34892;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#36827;&#23637;&#65292;&#20363;&#22914;Whisper&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#31995;&#32479;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#24182;&#19981;&#36866;&#29992;&#20110;&#20799;&#31461;ASR&#65292;&#21407;&#22240;&#26159;&#36866;&#29992;&#20110;&#20799;&#31461;&#30340;&#19987;&#29992;&#25968;&#25454;&#24211;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#19988;&#20799;&#31461;&#35821;&#38899;&#20855;&#26377;&#19982;&#25104;&#20154;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;My Science Tutor&#65288;MyST&#65289;&#20799;&#31461;&#35821;&#38899;&#35821;&#26009;&#24211;&#25552;&#39640;Whisper&#35782;&#21035;&#20799;&#31461;&#35821;&#38899;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#22312;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#26356;&#26377;&#25928;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#22686;&#24378;&#20102;MyST&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#25913;&#36827;&#20799;&#31461;ASR&#24615;&#33021;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#23558;Whisper&#26377;&#25928;&#25972;&#21512;&#21040;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Automatic Speech Recognition (ASR) systems, exemplified by Whisper, have demonstrated the potential of these systems to approach human-level performance given sufficient data. However, this progress doesn't readily extend to ASR for children due to the limited availability of suitable child-specific databases and the distinct characteristics of children's speech. A recent study investigated leveraging the My Science Tutor (MyST) children's speech corpus to enhance Whisper's performance in recognizing children's speech. This paper builds on these findings by enhancing the utility of the MyST dataset through more efficient data preprocessing. We also highlight important challenges towards improving children's ASR performance. The results showcase the viable and efficient integration of Whisper for effective children's speech recognition.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.07864</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Rise and Potential of Large Language Model Based Agents: A Survey. (arXiv:2309.07864v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07864
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#30340;&#23835;&#36215;&#21644;&#28508;&#21147;&#65306;&#19968;&#39033;&#35843;&#26597;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#31867;&#19968;&#30452;&#36861;&#27714;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36798;&#21040;&#25110;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#30446;&#26631;&#65292;&#32780;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26377;&#26395;&#26041;&#24335;&#30340;AI&#20195;&#29702;&#12290;AI&#20195;&#29702;&#26159;&#33021;&#24863;&#30693;&#29615;&#22659;&#12289;&#20570;&#20986;&#20915;&#31574;&#21644;&#37319;&#21462;&#34892;&#21160;&#30340;&#20154;&#24037;&#23454;&#20307;&#12290;&#33258;20&#19990;&#32426;&#20013;&#21494;&#20197;&#26469;&#65292;&#20154;&#20204;&#20026;&#24320;&#21457;&#26234;&#33021;AI&#20195;&#29702;&#36827;&#34892;&#20102;&#35768;&#22810;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#31639;&#27861;&#25110;&#35757;&#32451;&#31574;&#30053;&#30340;&#36827;&#27493;&#19978;&#65292;&#20197;&#22686;&#24378;&#29305;&#23450;&#33021;&#21147;&#25110;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23454;&#38469;&#19978;&#65292;&#31038;&#21306;&#25152;&#32570;&#20047;&#30340;&#26159;&#19968;&#20010;&#36275;&#22815;&#36890;&#29992;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;&#35774;&#35745;&#33021;&#36866;&#24212;&#21508;&#31181;&#22330;&#26223;&#30340;AI&#20195;&#29702;&#30340;&#36215;&#28857;&#12290;&#30001;&#20110;&#23637;&#31034;&#20986;&#30340;&#22810;&#21151;&#33021;&#21644;&#26174;&#33879;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#28508;&#22312;&#20652;&#21270;&#21058;&#65292;&#20026;&#26500;&#24314;&#36890;&#29992;AI&#20195;&#29702;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#21033;&#29992;LLMs&#20316;&#20026;&#26500;&#24314;AI&#20195;&#29702;&#30340;&#22522;&#30784;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent AI agents since the mid-20th century. However, these efforts have mainly focused on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a sufficiently general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile and remarkable capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many research efforts have leveraged LLMs as the foundation to build AI agents and ha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#22686;&#21152;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#38382;&#31572;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#21644;&#27169;&#22411;&#26657;&#20934;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24615;&#33021;&#25913;&#36827;&#19982;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.07822</link><description>&lt;p&gt;
CATfOOD&#65306;&#21453;&#20107;&#23454;&#22686;&#24378;&#35757;&#32451;&#20197;&#25552;&#39640;&#39046;&#22495;&#22806;&#24615;&#33021;&#21644;&#26657;&#20934;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration. (arXiv:2309.07822v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#22686;&#21152;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#38382;&#31572;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#24615;&#33021;&#21644;&#27169;&#22411;&#26657;&#20934;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24615;&#33021;&#25913;&#36827;&#19982;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35268;&#27169;&#26041;&#38754;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#25552;&#31034;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#25991;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;LLM&#26469;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#65288;CF&#65289;&#23454;&#20363;&#65288;&#21363;&#26368;&#23567;&#31243;&#24230;&#30340;&#25913;&#21464;&#36755;&#20837;&#65289;&#65292;&#20197;&#25552;&#39640;SLM&#22312;&#25688;&#35201;&#38382;&#31572;&#65288;QA&#65289;&#35774;&#32622;&#19979;&#30340;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;LLM&#29983;&#25104;&#22120;&#20013;&#65292;&#36825;&#31181;&#25968;&#25454;&#22686;&#24378;&#22987;&#32456;&#33021;&#22815;&#25552;&#39640;OOD&#24615;&#33021;&#65292;&#24182;&#25913;&#36827;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#21644;&#22522;&#20110;&#29702;&#24615;&#22686;&#24378;&#30340;&#26657;&#20934;&#27169;&#22411;&#30340;&#27169;&#22411;&#26657;&#20934;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#24615;&#33021;&#25552;&#21319;&#19982;CF&#23454;&#20363;&#22312;&#22806;&#35266;&#24418;&#24335;&#21644;&#35821;&#20041;&#20869;&#23481;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21576;&#27491;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26657;&#20934;&#26356;&#23481;&#26131;&#30340;CF&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#37197;&#37325;&#35201;&#24615;&#26102;&#30340;&#29109;&#20063;&#36739;&#20302;&#65292;&#36825;&#34920;&#26126;&#29702;&#24615;&#22686;&#24378;&#30340;&#26657;&#20934;&#22120;&#26356;&#20559;&#22909;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have shown remarkable capabilities at scale, particularly at generating text conditioned on a prompt. In our work, we investigate the use of LLMs to augment training data of small language models~(SLMs) with automatically generated counterfactual~(CF) instances -- i.e. minimally altered inputs -- in order to improve out-of-domain~(OOD) performance of SLMs in the extractive question answering~(QA) setup. We show that, across various LLM generators, such data augmentation consistently enhances OOD performance and improves model calibration for both confidence-based and rationale-augmented calibrator models. Furthermore, these performance improvements correlate with higher diversity of CF instances in terms of their surface form and semantic content. Finally, we show that CF augmented models which are easier to calibrate also exhibit much lower entropy when assigning importance, indicating that rationale-augmented calibrators prefer concise ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#35780;&#20272;&#20102;&#31227;&#21160;&#20005;&#32899;&#28216;&#25103;&#24212;&#29992;&#20013;&#21475;&#35821;&#21270;&#20154;&#24418;&#26426;&#22120;&#20154;&#23545;&#21487;&#29992;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#29992;&#25143;&#26356;&#21916;&#27426;&#19982;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#30340;&#26426;&#22120;&#20154;&#36827;&#34892;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2309.07773</link><description>&lt;p&gt;
&#31227;&#21160;&#20005;&#32899;&#28216;&#25103;&#20013;&#21475;&#35821;&#21270;&#20154;&#24418;&#26426;&#22120;&#20154;&#23545;&#21487;&#29992;&#24615;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Usability Evaluation of Spoken Humanoid Embodied Conversational Agents in Mobile Serious Games. (arXiv:2309.07773v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#35843;&#26597;&#35780;&#20272;&#20102;&#31227;&#21160;&#20005;&#32899;&#28216;&#25103;&#24212;&#29992;&#20013;&#21475;&#35821;&#21270;&#20154;&#24418;&#26426;&#22120;&#20154;&#23545;&#21487;&#29992;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#29992;&#25143;&#26356;&#21916;&#27426;&#19982;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#30340;&#26426;&#22120;&#20154;&#36827;&#34892;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31227;&#21160;&#20005;&#32899;&#28216;&#25103;&#24212;&#29992;&#20013;&#21475;&#35821;&#21270;&#20154;&#24418;&#26426;&#22120;&#20154;&#65288;HECA&#65289;&#23545;&#21487;&#29992;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#23454;&#35777;&#35843;&#26597;&#12290;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22810;&#20010;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20132;&#20114;&#24187;&#35273;&#23545;&#20132;&#20114;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#30740;&#31350;&#20102;&#20004;&#31181;&#26426;&#22120;&#20154;&#21576;&#29616;&#26041;&#24335;&#65306;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#30340;&#26426;&#22120;&#20154;&#65288;HECA&#65289;&#21644;&#20302;&#20154;&#31867;&#30456;&#20284;&#24230;&#30340;&#26426;&#22120;&#20154;&#65288;&#25991;&#26412;&#65289;&#12290;&#23454;&#39564;&#30340;&#30446;&#30340;&#26159;&#35780;&#20272;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#26426;&#22120;&#20154;&#26159;&#21542;&#33021;&#22815;&#24341;&#21457;&#20154;&#31867;&#24187;&#35273;&#24182;&#24433;&#21709;&#21487;&#29992;&#24615;&#12290;&#39640;&#20154;&#31867;&#30456;&#20284;&#24230;&#26426;&#22120;&#20154;&#26159;&#26681;&#25454;ECA&#35774;&#35745;&#27169;&#22411;&#36827;&#34892;&#35774;&#35745;&#30340;&#65292;&#35813;&#27169;&#22411;&#26159;&#19968;&#31181;ECA&#24320;&#21457;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;90&#20301;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#19982;HECA&#36827;&#34892;&#20132;&#20114;&#12290;&#20004;&#20010;&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#22312;&#32479;&#35745;&#23398;&#19978;&#20855;&#26377;&#26174;&#33879;&#24615;&#65292;&#25928;&#24212;&#22823;&#23567;&#36739;&#22823;&#65288;d=1.01&#65289;&#65292;&#35768;&#22810;&#21442;&#19982;&#32773;&#36890;&#36807;&#35299;&#37322;&#36873;&#25321;&#26469;&#35777;&#26126;&#20182;&#20204;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an empirical investigation of the extent to which spoken Humanoid Embodied Conversational Agents (HECAs) can foster usability in mobile serious game (MSG) applications. The aim of the research is to assess the impact of multiple agents and illusion of humanness on the quality of the interaction. The experiment investigates two styles of agent presentation: an agent of high human-likeness (HECA) and an agent of low human-likeness (text). The purpose of the experiment is to assess whether and how agents of high humanlikeness can evoke the illusion of humanness and affect usability. Agents of high human-likeness were designed by following the ECA design model that is a proposed guide for ECA development. The results of the experiment with 90 participants show that users prefer to interact with the HECAs. The difference between the two versions is statistically significant with a large effect size (d=1.01), with many of the participants justifying their choice by saying
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23450;&#21046;&#21270;&#30340;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#39046;&#22495;&#29305;&#23450;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#23450;&#21046;&#21270;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#26041;&#26696;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#24573;&#35270;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03126</link><description>&lt;p&gt;
&#27599;&#20010;&#20154;&#37117;&#24212;&#35813;&#24471;&#21040;&#22870;&#21169;&#65306;&#23398;&#20064;&#23450;&#21046;&#30340;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Everyone Deserves A Reward: Learning Customized Human Preferences. (arXiv:2309.03126v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23450;&#21046;&#21270;&#30340;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#39046;&#22495;&#29305;&#23450;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#23450;&#21046;&#21270;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#26041;&#26696;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#24573;&#35270;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#27169;&#22411;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#20132;&#20114;&#36136;&#37327;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#26159;&#22810;&#20803;&#30340;&#65292;&#36825;&#23548;&#33268;&#20102;&#22522;&#20110;&#19981;&#21516;&#23447;&#25945;&#12289;&#25919;&#27835;&#12289;&#25991;&#21270;&#31561;&#30340;&#22810;&#26679;&#21270;&#20154;&#31867;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#20154;&#23545;&#21508;&#31181;&#20027;&#39064;&#37117;&#21487;&#20197;&#26377;&#33258;&#24049;&#29420;&#29305;&#30340;&#20559;&#22909;&#12290;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#24573;&#35270;&#20102;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#26679;&#24615;&#65292;&#21482;&#20351;&#29992;&#19968;&#20010;&#36890;&#29992;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#23450;&#21046;&#25110;&#20010;&#24615;&#21270;&#24212;&#29992;&#22330;&#26223;&#26469;&#35828;&#26159;&#19981;&#22815;&#28385;&#24847;&#30340;&#12290;&#20026;&#20102;&#25506;&#32034;&#23450;&#21046;&#21270;&#30340;&#20559;&#22909;&#23398;&#20064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#20102;&#26469;&#33258;&#22235;&#20010;&#23454;&#38469;&#39046;&#22495;&#20013;&#23545;&#27599;&#20010;&#32473;&#23450;&#26597;&#35810;&#30340;&#39318;&#36873;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#20174;&#25968;&#25454;&#25928;&#29575;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#23450;&#21046;&#21270;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#26041;&#26696;&#65292;&#24182;&#22312;&#36890;&#29992;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#30340;&#39046;&#22495;&#29305;&#23450;&#20559;&#22909;&#25968;&#25454;&#38598;&#19978;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward models (RMs) are crucial in aligning large language models (LLMs) with human preferences for improving interaction quality. However, the real world is pluralistic, which leads to diversified human preferences based on different religions, politics, cultures, etc. Moreover, each individual can have their own unique preferences on various topics. Neglecting the diversity of human preferences, current LLM training processes only use a general reward model, which is below satisfaction for customized or personalized application scenarios. To explore customized preference learning, we collect a domain-specific preference (DSP) dataset, which collects preferred responses to each given query from four practical domains. Besides, from the perspective of data efficiency, we proposed a three-stage customized RM learning scheme, whose effectiveness is empirically verified on both general preference datasets and our DSP set. Furthermore, we test multiple training and data strategies on the t
&lt;/p&gt;</description></item><item><title>HAE-RAE Bench&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24378;&#35843;&#20102;&#21516;&#36136;&#35821;&#26009;&#24211;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02706</link><description>&lt;p&gt;
HAE-RAE Bench: &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models. (arXiv:2309.02706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02706
&lt;/p&gt;
&lt;p&gt;
HAE-RAE Bench&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24378;&#35843;&#20102;&#21516;&#36136;&#35821;&#26009;&#24211;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20851;&#27880;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#24182;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#38889;&#35821;&#35821;&#35328;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HAE-RAE Bench&#65292;&#22312;&#35789;&#27719;&#12289;&#21382;&#21490;&#21644;&#19968;&#33324;&#30693;&#35782;&#31561;6&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#35780;&#20272;&#31361;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;(LLSMs)&#19982;&#20687;GPT-3.5&#36825;&#26679;&#30340;&#20840;&#38754;&#36890;&#29992;&#27169;&#22411;&#30456;&#27604;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#27604;GPT-3.5&#32422;&#23567;13&#20493;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#36825;&#19968;&#35266;&#23519;&#24378;&#35843;&#20102;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#26102;&#21516;&#36136;&#35821;&#26009;&#24211;&#30340;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#24403;&#36825;&#20123;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;......
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) pretrained on massive corpora exhibit remarkable capabilities across a wide range of tasks, however, the attention given to non-English languages has been limited in this field of research. To address this gap and assess the proficiency of language models in the Korean language and culture, we present HAE-RAE Bench, covering 6 tasks including vocabulary, history, and general knowledge. Our evaluation of language models on this benchmark highlights the potential advantages of employing Large Language-Specific Models(LLSMs) over a comprehensive, universal model like GPT-3.5. Remarkably, our study reveals that models approximately 13 times smaller than GPT-3.5 can exhibit similar performance levels in terms of language-specific knowledge retrieval. This observation underscores the importance of homogeneous corpora for training professional-level language-specific models. On the contrary, we also observe a perplexing performance dip in these smaller LMs when th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"Wan Juan"&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20013;&#33521;&#25991;&#25968;&#25454;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#21508;&#31181;&#32593;&#32476;&#26469;&#28304;&#37319;&#38598;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#25991;&#26412;&#21644;&#35270;&#39057;&#27169;&#24577;&#65292;&#24635;&#37327;&#36229;&#36807;2TB&#12290;&#23427;&#34987;&#29992;&#20110;&#35757;&#32451;InternLM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#32500;&#24230;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.10755</link><description>&lt;p&gt;
WanJuan: &#29992;&#20110;&#25512;&#36827;&#33521;&#25991;&#21644;&#20013;&#25991;&#22823;&#27169;&#22411;&#30340;&#32508;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models. (arXiv:2308.10755v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"Wan Juan"&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20013;&#33521;&#25991;&#25968;&#25454;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#21508;&#31181;&#32593;&#32476;&#26469;&#28304;&#37319;&#38598;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#25991;&#26412;&#21644;&#35270;&#39057;&#27169;&#24577;&#65292;&#24635;&#37327;&#36229;&#36807;2TB&#12290;&#23427;&#34987;&#29992;&#20110;&#35757;&#32451;InternLM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#32500;&#24230;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;GPT-4&#30340;&#26222;&#21450;&#26174;&#33879;&#21152;&#36895;&#20102;&#22823;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#23548;&#33268;&#20102;&#35768;&#22810;&#24341;&#20154;&#27880;&#30446;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#30340;&#21019;&#24314;&#12290;&#36825;&#20123;&#23574;&#31471;&#27169;&#22411;&#30340;&#20986;&#33394;&#34920;&#29616;&#24402;&#21151;&#20110;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#39046;&#20808;&#33539;&#24335;&#20013;&#20351;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#32454;&#33410;&#36890;&#24120;&#34987;&#20445;&#23494;&#12290;&#36825;&#31181;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#21152;&#19978;&#24320;&#28304;&#25968;&#25454;&#30340;&#31232;&#32570;&#65292;&#38459;&#30861;&#20102;&#31038;&#21306;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;"Wan Juan"&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20013;&#25991;&#21644;&#33521;&#25991;&#25968;&#25454;&#65292;&#37319;&#38598;&#33258;&#24191;&#27867;&#30340;&#32593;&#32476;&#26469;&#28304;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#25991;&#26412;&#21644;&#35270;&#39057;&#27169;&#24577;&#65292;&#24635;&#37327;&#36229;&#36807;2TB&#12290;&#23427;&#34987;&#29992;&#20110;&#35757;&#32451;InternLM&#27169;&#22411;&#65292;&#22312;&#22810;&#32500;&#24230;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#19982;&#30456;&#20284;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#25152;&#26377;&#25968;&#25454;&#21487;&#22312;htt
&lt;/p&gt;
&lt;p&gt;
The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents "Wan Juan", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at htt
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#26816;&#32034;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#20043;&#38388;&#30340;&#19981;&#21487;&#24494;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09308</link><description>&lt;p&gt;
&#21487;&#24494;&#26816;&#32034;&#22686;&#24378;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#30340;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentiable Retrieval Augmentation via Generative Language Modeling for E-commerce Query Intent Classification. (arXiv:2308.09308v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#26816;&#32034;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#20043;&#38388;&#30340;&#19981;&#21487;&#24494;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#26816;&#32034;&#22120;&#21644;&#22806;&#37096;&#35821;&#26009;&#24211;&#26469;&#22686;&#24378;&#19979;&#28216;&#27169;&#22411;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#65292;&#22914;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#37096;&#20998;&#20043;&#38388;&#30340;&#19981;&#21487;&#24494;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#20998;&#21035;&#25110;&#24322;&#27493;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#19979;&#28216;&#27169;&#22411;&#26469;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#19982;&#31471;&#21040;&#31471;&#32852;&#21512;&#35757;&#32451;&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Differentiable Retrieval Augmentation via Generative lANguage modeling&#65288;Dragan&#65289;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#37325;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#30340;&#19968;&#20010;&#26377;&#25361;&#25112;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#28040;&#34701;&#30740;&#31350;&#22343;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#19988;&#21512;&#29702;&#22320;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmentation, which enhances downstream models by a knowledge retriever and an external corpus instead of by merely increasing the number of model parameters, has been successfully applied to many natural language processing (NLP) tasks such as text classification, question answering and so on. However, existing methods that separately or asynchronously train the retriever and downstream model mainly due to the non-differentiability between the two parts, usually lead to degraded performance compared to end-to-end joint training. In this paper, we propose Differentiable Retrieval Augmentation via Generative lANguage modeling(Dragan), to address this problem by a novel differentiable reformulation. We demonstrate the effectiveness of our proposed method on a challenging NLP task in e-commerce search, namely query intent classification. Both the experimental results and ablation study show that the proposed method significantly and reasonably improves the state-of-the-art basel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.14096</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#23454;&#20307;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#37325;&#35201;&#23376;&#20219;&#21153;&#65292;&#30446;&#21069;&#38754;&#20020;&#30528;&#20247;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26469;&#33258;&#20110;&#32570;&#20047;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37329;&#34701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#30340;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#24320;&#21457;&#26377;&#25928;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#25152;&#38656;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#27169;&#24335;&#21305;&#37197;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;&#29616;&#26377;&#24320;&#28304;LLMs&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#21160;&#30495;&#23454;&#19990;&#30028;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#24230;&#25513;&#30721;&#29575;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;MLM&#39044;&#35757;&#32451;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#25513;&#30721;&#29575;&#65292;&#36798;&#21040;&#20102;&#23545;BERT-base&#21644;BERT-large&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;0.46%&#21644;0.25%&#30340;&#24179;&#22343;GLUE&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21152;&#24555;&#20102;BERT-base&#30340;&#39044;&#35757;&#32451;&#36895;&#24230;&#65292;&#36824;&#23454;&#29616;&#20102;&#23545;BERT-large&#30340;&#24085;&#32047;&#25176;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.15096</link><description>&lt;p&gt;
MLM&#39044;&#35757;&#32451;&#30340;&#21160;&#24577;&#25513;&#30721;&#29575;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Dynamic Masking Rate Schedules for MLM Pretraining. (arXiv:2305.15096v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#24230;&#25513;&#30721;&#29575;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;MLM&#39044;&#35757;&#32451;&#30340;&#36136;&#37327;&#65292;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#25513;&#30721;&#29575;&#65292;&#36798;&#21040;&#20102;&#23545;BERT-base&#21644;BERT-large&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;0.46%&#21644;0.25%&#30340;&#24179;&#22343;GLUE&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21152;&#24555;&#20102;BERT-base&#30340;&#39044;&#35757;&#32451;&#36895;&#24230;&#65292;&#36824;&#23454;&#29616;&#20102;&#23545;BERT-large&#30340;&#24085;&#32047;&#25176;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#20351;&#29992;&#20102;&#21407;&#22987;BERT&#27169;&#22411;&#30340;&#22266;&#23450;&#25513;&#30721;&#29575;15%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25513;&#30721;&#29575;&#26469;&#26367;&#20195;&#22266;&#23450;&#29575;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#32447;&#24615;&#38477;&#20302;&#25513;&#30721;&#29575;&#21487;&#20197;&#27604;&#22266;&#23450;&#29575;&#22522;&#20934;&#20998;&#21035;&#25552;&#39640;BERT-base&#21644;BERT-large&#30340;&#24179;&#22343;GLUE&#20934;&#30830;&#29575;0.46%&#21644;0.25%&#12290;&#36825;&#20123;&#25552;&#21319;&#26469;&#33258;&#20110;&#25509;&#35302;&#39640;&#21644;&#20302;&#25513;&#30721;&#29575;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#22312;&#20004;&#31181;&#35774;&#32622;&#20013;&#37117;&#24102;&#26469;&#20102;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25513;&#30721;&#29575;&#35843;&#24230;&#26159;&#25552;&#39640;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;BERT-base&#30340;&#39044;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;1.89&#20493;&#65292;&#24182;&#23545;BERT-large&#23454;&#29616;&#20102;&#24085;&#32047;&#25176;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most works on transformers trained with the Masked Language Modeling (MLM) objective use the original BERT model's fixed masking rate of 15%. We propose to instead dynamically schedule the masking rate throughout training. We find that linearly decreasing the masking rate over the course of pretraining improves average GLUE accuracy by up to 0.46% and 0.25% in BERT-base and BERT-large, respectively, compared to fixed rate baselines. These gains come from exposure to both high and low masking rate regimes, providing benefits from both settings. Our results demonstrate that masking rate scheduling is a simple way to improve the quality of masked language models, achieving up to a 1.89x speedup in pretraining for BERT-base as well as a Pareto improvement for BERT-large.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;</title><link>http://arxiv.org/abs/2305.08339</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#65306;&#26412;&#22320;&#35821;&#27861;&#20998;&#26512;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis. (arXiv:2305.08339v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;LLMs&#22312;&#21327;&#21161;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#33258;&#21160;&#26631;&#27880;&#20026;&#29305;&#23450;&#35821;&#35328;&#20449;&#24687;&#31867;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#30340;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#12289;&#22522;&#20110;GPT-4&#30340;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#32534;&#30721;&#22120;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#30528;&#20248;&#20110;ChatGPT&#12290;&#19982;&#20154;&#31867;&#26631;&#27880;&#21592;&#30456;&#27604;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25972;&#20307;&#34920;&#29616;&#30053;&#20302;&#20110;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#34920;&#29616;&#65292;&#20294;&#24050;&#32463;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#24471;&#20998;:&#36947;&#27465;&#26631;&#35760;99.95&#65285;&#65292;&#21407;&#22240;&#26631;&#35760;91.91&#65285;&#65292;&#36947;&#27465;&#32773;&#26631;&#35760;95.35&#65285;&#65292;&#34987;&#36947;&#27465;&#32773;&#26631;&#35760;89.74&#65285;&#21644;&#21152;&#24378;&#26631;&#35760;96.47&#65285;&#12290;&#36825;&#34920;&#26126;&#65292;&#22312;&#35821;&#35328;&#31867;&#21035;&#28165;&#26224;&#19988;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots based on Large Language Models (LLMs) have shown strong capabilities in language understanding. In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information. Specifically, we examined to what extent LLMs understand the functional elements constituting the speech act of apology from a local grammar perspective, by comparing the performance of ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and a human coder in the annotation task. The results demonstrate that the Bing chatbot significantly outperformed ChatGPT in the task. Compared to human annotator, the overall performance of the Bing chatbot was slightly less satisfactory. However, it already achieved high F1 scores: 99.95% for the tag of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible to use LLM-
&lt;/p&gt;</description></item><item><title>&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#38754;&#20020;&#30528;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25361;&#25112;&#65292;&#20294;&#20854;&#22312;&#23458;&#25143;&#26381;&#21153;&#12289;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#36328;&#35821;&#35328;&#20132;&#27969;&#31561;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.06623</link><description>&lt;p&gt;
&#25506;&#32034;&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
Exploring the State of the Art in Legal QA Systems. (arXiv:2304.06623v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06623
&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#38754;&#20020;&#30528;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25361;&#25112;&#65292;&#20294;&#20854;&#22312;&#23458;&#25143;&#26381;&#21153;&#12289;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#36328;&#35821;&#35328;&#20132;&#27969;&#31561;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#19982;&#27861;&#24459;&#39046;&#22495;&#30456;&#20851;&#30340;&#38382;&#39064;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22797;&#26434;&#30340;&#27861;&#24459;&#25991;&#26723;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#20026;&#27861;&#24459;&#38382;&#39064;&#25552;&#20379;&#20934;&#30830;&#30340;&#31572;&#26696;&#36890;&#24120;&#38656;&#35201;&#30456;&#20851;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#36825;&#20351;&#24471;&#21363;&#20351;&#23545;&#20110;&#20154;&#31867;&#19987;&#23478;&#26469;&#35828;&#65292;&#36825;&#39033;&#20219;&#21153;&#20063;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#38382;&#31572;&#31995;&#32479;&#65288;QA&#65289;&#26088;&#22312;&#29983;&#25104;&#23545;&#20197;&#20154;&#31867;&#35821;&#35328;&#25552;&#20986;&#30340;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#23427;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#29702;&#35299;&#38382;&#39064;&#24182;&#25628;&#32034;&#20449;&#24687;&#20197;&#25214;&#21040;&#30456;&#20851;&#31572;&#26696;&#12290;QA&#20855;&#26377;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#23458;&#25143;&#26381;&#21153;&#12289;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#36328;&#35821;&#35328;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#35832;&#22914;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#22788;&#29702;&#22797;&#26434;&#21644;&#27169;&#31946;&#38382;&#39064;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query typically necessitates specialized knowledge in the relevant domain, which makes this task all the more challenging, even for human experts. QA (Question answering systems) are designed to generate answers to questions asked in human languages. They use natural language processing to understand questions and search through information to find relevant answers. QA has various practical applications, including customer service, education, research, and cross-lingual communication. However, they face challenges such as improving natural language understanding and handling complex and ambiguous questions. Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22870;&#21169;&#20195;&#29702;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#24314;&#35758;&#30340;&#30446;&#26631;&#26469;&#22609;&#36896;&#25506;&#32034;&#31574;&#30053;&#65292;&#20351;&#20195;&#29702;&#26397;&#30528;&#20154;&#31867;&#26377;&#24847;&#20041;&#19988;&#21487;&#33021;&#26377;&#29992;&#30340;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#65292;&#26080;&#38656;&#20154;&#31867;&#30340;&#20171;&#20837;&#12290;</title><link>http://arxiv.org/abs/2302.06692</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Guiding Pretraining in Reinforcement Learning with Large Language Models. (arXiv:2302.06692v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06692
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22870;&#21169;&#20195;&#29702;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#24314;&#35758;&#30340;&#30446;&#26631;&#26469;&#22609;&#36896;&#25506;&#32034;&#31574;&#30053;&#65292;&#20351;&#20195;&#29702;&#26397;&#30528;&#20154;&#31867;&#26377;&#24847;&#20041;&#19988;&#21487;&#33021;&#26377;&#29992;&#30340;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#65292;&#26080;&#38656;&#20154;&#31867;&#30340;&#20171;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#27809;&#26377;&#23494;&#38598;&#19988;&#24418;&#29366;&#33391;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#36890;&#36807;&#22870;&#21169;&#20195;&#29702;&#35775;&#38382;&#26032;&#39062;&#29366;&#24577;&#25110;&#36716;&#25442;&#30340;&#20869;&#22312;&#21160;&#26426;&#25506;&#32034;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#20294;&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#26377;&#38480;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#32972;&#26223;&#30693;&#35782;&#26469;&#22609;&#36896;&#25506;&#32034;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;ELLM&#65288;&#20351;&#29992;LLMs&#36827;&#34892;&#25506;&#32034;&#65289;&#65292;&#36890;&#36807;&#32473;&#20195;&#29702;&#22870;&#21169;&#20854;&#36798;&#25104;&#30001;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#20195;&#29702;&#24403;&#21069;&#29366;&#24577;&#25551;&#36848;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#65292;&#24341;&#23548;&#20195;&#29702;&#26397;&#30528;&#20154;&#31867;&#26377;&#24847;&#20041;&#19988;&#21487;&#33021;&#26377;&#29992;&#30340;&#34892;&#20026;&#26041;&#21521;&#21457;&#23637;&#65292;&#26080;&#38656;&#20154;&#31867;&#30340;&#20171;&#20837;&#12290;&#25105;&#20204;&#22312;Crafter&#28216;&#25103;&#29615;&#22659;&#21644;Housekeep&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#20013;&#35780;&#20272;&#20102;ELLM&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;ELLM&#35757;&#32451;&#30340;&#20195;&#29702;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26377;&#26356;&#22909;&#30340;&#24120;&#35782;&#34892;&#20026;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#36890;&#24120;&#19982;&#20154;&#31867;&#34892;&#20026;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning algorithms typically struggle in the absence of a dense, well-shaped reward function. Intrinsically motivated exploration methods address this limitation by rewarding agents for visiting novel states or transitions, but these methods offer limited benefits in large environments where most discovered novelty is irrelevant for downstream tasks. We describe a method that uses background knowledge from text corpora to shape exploration. This method, called ELLM (Exploring with LLMs) rewards an agent for achieving goals suggested by a language model prompted with a description of the agent's current state. By leveraging large-scale language model pretraining, ELLM guides agents toward human-meaningful and plausibly useful behaviors without requiring a human in the loop. We evaluate ELLM in the Crafter game environment and the Housekeep robotic simulator, showing that ELLM-trained agents have better coverage of common-sense behaviors during pretraining and usually matc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#25903;&#25345;&#21644;&#23436;&#21892;&#27010;&#24565;&#38544;&#21947;&#29702;&#35770;&#65292;&#21516;&#26102;&#20063;&#23558;&#38544;&#21947;&#29702;&#35770;&#20316;&#20026;&#24847;&#20041;&#20986;&#29616;&#30340;&#22522;&#30784;&#65292;&#21487;&#20197;&#23450;&#37327;&#22320;&#25506;&#32034;&#24182;&#25972;&#21512;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26694;&#26550;&#20013;&#12290;</title><link>http://arxiv.org/abs/2209.12234</link><description>&lt;p&gt;
&#36328;&#26102;&#24577;&#25968;&#25454;&#20998;&#26512;&#25903;&#25345;&#21644;&#23436;&#21892;&#27010;&#24565;&#38544;&#21947;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Diachronic Data Analysis Supports and Refines Conceptual Metaphor Theory. (arXiv:2209.12234v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12234
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#25903;&#25345;&#21644;&#23436;&#21892;&#27010;&#24565;&#38544;&#21947;&#29702;&#35770;&#65292;&#21516;&#26102;&#20063;&#23558;&#38544;&#21947;&#29702;&#35770;&#20316;&#20026;&#24847;&#20041;&#20986;&#29616;&#30340;&#22522;&#30784;&#65292;&#21487;&#20197;&#23450;&#37327;&#22320;&#25506;&#32034;&#24182;&#25972;&#21512;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23545;&#38544;&#21947;&#20998;&#26512;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#25968;&#25454;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#39564;&#20998;&#26512;&#26469;&#25506;&#32034;&#38271;&#26399;&#20197;&#26469;&#30340;&#29468;&#24819;&#65292;&#24182;&#39318;&#27425;&#23545;&#38544;&#21947;&#30340;&#31995;&#32479;&#29305;&#24449;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#12290;&#21453;&#36807;&#26469;&#65292;&#36825;&#20063;&#20351;&#24471;&#38544;&#21947;&#29702;&#35770;&#21487;&#20197;&#20316;&#20026;&#24847;&#20041;&#20986;&#29616;&#30340;&#22522;&#30784;&#65292;&#21487;&#20197;&#36890;&#36807;&#23450;&#37327;&#30340;&#26041;&#27861;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#25972;&#21512;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a contribution to metaphor analysis, we introduce a statistical, data-based investigation with empirical analysis of long-standing conjectures and a first-ever empirical exploration of the systematic features of metaphors. Conversely, this also makes metaphor theory available as a basis of meaning emergence that can be quantitatively explored and integrated into the framework of NLP.
&lt;/p&gt;</description></item><item><title>VQA-GNN&#26159;&#19968;&#31181;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#22810;&#27169;&#24577;&#30693;&#35782;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#34701;&#21512;&#30340;&#26032;&#30340;VQA&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.11501</link><description>&lt;p&gt;
VQA-GNN: &#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#22810;&#27169;&#24577;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering. (arXiv:2205.11501v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11501
&lt;/p&gt;
&lt;p&gt;
VQA-GNN&#26159;&#19968;&#31181;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#22810;&#27169;&#24577;&#30693;&#35782;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#34701;&#21512;&#30340;&#26032;&#30340;VQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572; (VQA) &#38656;&#35201;&#31995;&#32479;&#36890;&#36807;&#32479;&#19968;&#38750;&#32467;&#26500;&#21270;&#65288;&#20363;&#22914;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#19978;&#19979;&#25991; "QA&#19978;&#19979;&#25991;"&#65289;&#21644;&#32467;&#26500;&#21270;&#65288;&#20363;&#22914;QA&#19978;&#19979;&#25991;&#21644;&#22330;&#26223;&#30340;&#30693;&#35782;&#22270; "&#27010;&#24565;&#22270;"&#65289;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#36827;&#34892;&#27010;&#24565;&#32423;&#21035;&#30340;&#25512;&#29702;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#36830;&#25509;&#30456;&#24212;&#30340;&#35270;&#35273;&#33410;&#28857;&#21644;&#27010;&#24565;&#33410;&#28857;&#26469;&#21512;&#24182;&#22330;&#26223;&#22270;&#21644;&#27010;&#24565;&#22270;&#65292;&#28982;&#21518;&#23558;QA&#19978;&#19979;&#25991;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#33021;&#20174;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#21040;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#21333;&#21521;&#34701;&#21512;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#22810;&#27169;&#24577;&#30693;&#35782;&#30340;&#24322;&#26500;&#32852;&#21512;&#25512;&#29702;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VQA-GNN&#65292;&#19968;&#31181;&#26032;&#30340;VQA&#27169;&#22411;&#65292;&#23427;&#22312;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#22810;&#27169;&#24577;&#30693;&#35782;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#34701;&#21512;&#65292;&#20197;&#33719;&#24471;&#32479;&#19968;&#30340;&#30693;&#35782;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36229;&#38142;&#25509;&#36830;&#25509;&#22330;&#26223;&#22270;&#21644;&#27010;&#24565;&#22270;&#65292;&#23454;&#29616;&#20102;&#20114;&#36830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) requires systems to perform concept-level reasoning by unifying unstructured (e.g., the context in question and answer; "QA context") and structured (e.g., knowledge graph for the QA context and scene; "concept graph") multimodal knowledge. Existing works typically combine a scene graph and a concept graph of the scene by connecting corresponding visual nodes and concept nodes, then incorporate the QA context representation to perform question answering. However, these methods only perform a unidirectional fusion from unstructured knowledge to structured knowledge, limiting their potential to capture joint reasoning over the heterogeneous modalities of knowledge. To perform more expressive reasoning, we propose VQA-GNN, a new VQA model that performs bidirectional fusion between unstructured and structured multimodal knowledge to obtain unified knowledge representations. Specifically, we inter-connect the scene graph and the concept graph through a super 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.11155</link><description>&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#22312;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11155
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#34920;&#31034;&#25972;&#20010;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#23558;&#23494;&#24230;&#30697;&#38453;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#20219;&#21153;&#21487;&#20197;&#26356;&#21152;&#26377;&#25928;&#22320;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#12290;&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#26032;&#26426;&#21046;&#65292;&#20197;&#24212;&#23545;&#36755;&#20837;&#20026;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#24182;&#23558;&#35813;&#26426;&#21046;&#24212;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;QA&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#20197;&#22686;&#24378;&#32463;&#20856;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#20449;&#24687;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26032;&#26694;&#26550;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum density matrix represents all the information of the entire quantum system, and novel models of meaning employing density matrices naturally model linguistic phenomena such as hyponymy and linguistic ambiguity, among others in quantum question answering tasks. Naturally, we argue that applying the quantum density matrix into classical Question Answering (QA) tasks can show more effective performance. Specifically, we (i) design a new mechanism based on Long Short-Term Memory (LSTM) to accommodate the case when the inputs are matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural Network (CNN) and gain the LSTM-based QA model with the quantum density matrix. Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging results. Similarly, we argue that the quantum density matrix can also enhance the image feature information and the relationship between the features for the classical image classification. Thus, we (i) combine density mat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#29992;&#20110;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21547;&#31946;&#24615;&#22768;&#26126;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#35777;&#25454;&#27880;&#37322;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#36719;&#26631;&#31614;&#35777;&#25454;&#26680;&#26597;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#20998;&#26512;&#20013;&#21457;&#29616;&#20102;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2104.00640</link><description>&lt;p&gt;
AmbiFC: &#29992;&#35777;&#25454;&#26816;&#39564;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
AmbiFC: Fact-Checking Ambiguous Claims with Evidence. (arXiv:2104.00640v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.00640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#29992;&#20110;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21547;&#31946;&#24615;&#22768;&#26126;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#35777;&#25454;&#27880;&#37322;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#36719;&#26631;&#31614;&#35777;&#25454;&#26680;&#26597;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#20998;&#26512;&#20013;&#21457;&#29616;&#20102;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#24517;&#39035;&#23558;&#22768;&#26126;&#19982;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#36827;&#34892;&#27604;&#36739;&#20197;&#39044;&#27979;&#30495;&#23454;&#24615;&#12290;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#21487;&#33021;&#26080;&#27861;&#26126;&#30830;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#65292;&#24182;&#20135;&#29983;&#21508;&#31181;&#26377;&#25928;&#35299;&#37322;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;&#38656;&#35201;&#27169;&#22411;&#20026;&#27599;&#20010;&#22768;&#26126;&#39044;&#27979;&#21333;&#20010;&#30495;&#23454;&#24615;&#26631;&#31614;&#65292;&#24182;&#19988;&#32570;&#20047;&#31649;&#29702;&#27492;&#31867;&#27169;&#31946;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#23436;&#25972;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#20013;&#33719;&#21462;&#30340;&#32463;&#36807;&#32454;&#31890;&#24230;&#35777;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#38656;&#27714;&#30340;&#29616;&#23454;&#22768;&#26126;&#12290;&#25105;&#20204;&#24443;&#24213;&#20998;&#26512;&#20102;AmbiFC&#20013;&#28041;&#21450;&#21547;&#31946;&#22768;&#26126;&#24341;&#36215;&#30340;&#20105;&#35758;&#65292;&#35266;&#23519;&#21040;&#19982;&#27880;&#37322;&#20154;&#21592;&#30340;&#33258;&#25105;&#35780;&#20272;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#35821;&#35328;&#29616;&#35937;&#24378;&#28872;&#30456;&#20851;&#30340;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#12290;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#35777;&#25454;&#30340;&#21547;&#31946;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#26680;&#26597;&#20219;&#21153;&#65292;&#27604;&#36739;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#27880;&#37322;&#20449;&#21495;&#21644;&#21333;&#26631;&#31614;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated fact-checking systems in real-world scenarios must compare claims with retrieved evidence to predict the veracity. The retrieved evidence may not unambiguously support or refute the claim and yield diverse valid interpretations. Existing fact-checking datasets necessitate that models predict a single veracity label for each claim and lack the ability to manage such ambiguity. We present AmbiFC, a large-scale fact-checking dataset with realistic claims derived from real-world information needs. Our dataset contains fine-grained evidence annotations of passages from complete Wikipedia pages. We thoroughly analyze disagreements arising from ambiguous claims in AmbiFC, observing a strong correlation of annotator disagreement with their self-assessment and expert-annotated linguistic phenomena. We introduce the task of evidence-based fact-checking for ambiguous claims with soft labels, and compare three methodologies incorporating annotation signals with a single-label classificat
&lt;/p&gt;</description></item></channel></rss>