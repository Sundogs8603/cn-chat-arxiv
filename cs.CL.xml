<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>SSLCL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#20559;&#21521;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;&#23427;&#35299;&#20915;&#20102;&#22522;&#20110;SCL&#30340;&#26041;&#27861;&#20013;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#38480;&#21046;&#21644;&#19982;&#29616;&#26377;ERC&#27169;&#22411;&#19981;&#20860;&#23481;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25237;&#24433;&#31163;&#25955;&#26631;&#31614;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#26469;&#25913;&#21892;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16676</link><description>&lt;p&gt;
SSLCL: &#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#20559;&#21521;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning Framework for Emotion Recognition in Conversations. (arXiv:2310.16676v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16676
&lt;/p&gt;
&lt;p&gt;
SSLCL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#20559;&#21521;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;&#23427;&#35299;&#20915;&#20102;&#22522;&#20110;SCL&#30340;&#26041;&#27861;&#20013;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#38480;&#21046;&#21644;&#19982;&#29616;&#26377;ERC&#27169;&#22411;&#19981;&#20860;&#23481;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25237;&#24433;&#31163;&#25955;&#26631;&#31614;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#26469;&#25913;&#21892;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035; (ERC) &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#23545;&#35805;&#20013;&#21457;&#35328;&#32773;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;ERC&#26041;&#27861;&#19987;&#27880;&#20110;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064; (SCL) &#26469;&#22686;&#24378;&#23398;&#21040;&#30340;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;ERC&#20013;&#22522;&#20110;SCL&#30340;&#26041;&#27861;&#21463;&#21040;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;ERC&#27169;&#22411;&#19981;&#20860;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26080;&#27169;&#22411;&#20559;&#21521;&#30340;SCL&#26694;&#26550;&#65292;&#21517;&#20026;Supervised Sample-Label Contrastive Learning with Soft-HGR Maximal Correlation (SSLCL)&#65292;&#23427;&#28040;&#38500;&#20102;&#23545;&#22823;&#25209;&#37327;&#22823;&#23567;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;ERC&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#32780;&#19981;&#24341;&#20837;&#20219;&#20309;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#20551;&#35774;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26631;&#31614;&#34920;&#31034;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#23558;&#31163;&#25955;&#26631;&#31614;&#25237;&#24433;&#21040;&#23494;&#38598;&#34920;&#31034;&#20013;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in conversations (ERC) is a rapidly evolving task within the natural language processing community, which aims to detect the emotions expressed by speakers during a conversation. Recently, a growing number of ERC methods have focused on leveraging supervised contrastive learning (SCL) to enhance the robustness and generalizability of learned features. However, current SCL-based approaches in ERC are impeded by the constraint of large batch sizes and the lack of compatibility with most existing ERC models. To address these challenges, we propose an efficient and model-agnostic SCL framework named Supervised Sample-Label Contrastive Learning with Soft-HGR Maximal Correlation (SSLCL), which eliminates the need for a large batch size and can be seamlessly integrated with existing ERC models without introducing any model-specific assumptions. Specifically, we introduce a novel perspective on utilizing label representations by projecting discrete labels into dense embeddi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#23384;&#22312;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#23545;&#31572;&#26696;&#20915;&#31574;&#21644;&#26684;&#24335;&#20559;&#22909;&#36127;&#36131;&#12290;&#23545;&#40784;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#28151;&#28102;&#12290;</title><link>http://arxiv.org/abs/2310.11732</link><description>&lt;p&gt;
&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#30740;&#31350;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting. (arXiv:2310.11732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#23384;&#22312;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#23545;&#31572;&#26696;&#20915;&#31574;&#21644;&#26684;&#24335;&#20559;&#22909;&#36127;&#36131;&#12290;&#23545;&#40784;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20542;&#21521;&#20110;&#19982;&#39044;&#35757;&#32451;&#30340;LM&#30456;&#27604;&#65292;&#22312;&#36755;&#20986;&#31572;&#26696;&#26102;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#23545;&#40784;&#36807;&#31243;&#23545;&#22810;&#36873;&#35774;&#32622;&#19979;LM&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#23545;&#40784;LM&#22312;&#26657;&#20934;&#26041;&#38754;&#19982;&#20854;&#39044;&#35757;&#32451;&#23545;&#24212;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#36827;&#34892;&#20102;&#35748;&#30495;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#65292;LM&#23384;&#22312;&#20004;&#31181;&#26126;&#26174;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#36127;&#36131;&#31572;&#26696;&#20915;&#31574;&#21644;LM&#30340;&#26684;&#24335;&#20559;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#31616;&#21333;&#30340;&#21512;&#25104;&#23545;&#40784;&#26041;&#26696;&#20013;&#36827;&#34892;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#22312;&#23545;&#40784;LM&#30340;&#26657;&#20934;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#23545;&#40784;LM&#36807;&#24230;&#33258;&#20449;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#28151;&#28102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;&#24120;&#35265;&#30340;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant progress made in practical applications of aligned language models (LMs), they tend to be overconfident in output answers compared to the corresponding pre-trained LMs. In this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting. We first conduct a thoughtful empirical study on how aligned LMs differ in calibration from their pre-trained counterparts. Experimental results reveal that there are two distinct uncertainties in LMs under the multiple-choice setting, which are responsible for the answer decision and the format preference of the LMs, respectively. Then, we investigate the role of these two uncertainties on aligned LM's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty. Furthermore, we examine the utility of common post-hoc cal
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#20915;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#24110;&#21161;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#23545;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20005;&#37325;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2310.09886</link><description>&lt;p&gt;
&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation. (arXiv:2310.09886v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09886
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#20915;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#24110;&#21161;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#23545;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20005;&#37325;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#65288;LSG&#65289;&#26159;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#35753;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451;&#65292;&#20197;&#19981;&#26029;&#23398;&#20064;&#26032;&#30340;&#29983;&#25104;&#27169;&#24335;&#24182;&#36991;&#20813;&#36951;&#24536;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;LSG&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#32500;&#25345;&#26087;&#30693;&#35782;&#65292;&#32780;&#23545;&#36328;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#20851;&#27880;&#36739;&#23569;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#33719;&#21462;&#30340;&#31867;&#20284;&#20219;&#21153;&#30340;&#30693;&#35782;&#26356;&#22909;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#21463;&#20154;&#31867;&#23398;&#20064;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#65288;DMEA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#30830;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#26368;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#20419;&#36827;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23398;&#20064;&#36807;&#31243;&#24456;&#23481;&#26131;&#20559;&#21521;&#20110;&#24403;&#21069;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old knowledge while paying little attention to knowledge transfer across tasks. In contrast, humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic Module Expansion and Adaptation (DMEA), which enables the model to dynamically determine the architecture for acquiring new knowledge based on task correlation and select the most similar previous tasks to facilitate adaptation to new tasks. In addition, as the learning process can easily be biased towards the current task which might cause more severe forgetting of previously learned knowledge, we propose dynamic gradient scaling to balance the lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.10003</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19987;&#21033;&#26435;&#35201;&#27714;&#30340;&#33539;&#22260;&#27979;&#37327;&#20026;&#35813;&#35201;&#27714;&#25152;&#21253;&#21547;&#30340;&#33258;&#20449;&#24687;&#30340;&#20498;&#25968;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#35770;&#65292;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#32597;&#35265;&#30340;&#27010;&#24565;&#27604;&#24179;&#24120;&#30340;&#27010;&#24565;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#22240;&#20026;&#23427;&#26356;&#20196;&#20154;&#24778;&#35766;&#12290;&#33258;&#20449;&#24687;&#26159;&#20174;&#35813;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#35745;&#31639;&#24471;&#20986;&#30340;&#65292;&#20854;&#20013;&#27010;&#29575;&#26159;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20116;&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#65288;&#27599;&#20010;&#21333;&#35789;&#25110;&#23383;&#31526;&#22343;&#20174;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#21462;&#65289;&#21040;&#20013;&#31561;&#27169;&#22411;&#65288;&#20351;&#29992;&#24179;&#22343;&#35789;&#25110;&#23383;&#31526;&#39057;&#29575;&#65289;&#65292;&#20877;&#21040;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT2&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33539;&#22260;&#24230;&#37327;&#20943;&#23569;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#65292;&#36825;&#26159;&#20808;&#21069;&#20316;&#21697;&#20013;&#24050;&#32463;&#20351;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20061;&#20010;&#31995;&#21015;&#30340;&#38024;&#23545;&#19981;&#21516;&#21457;&#26126;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#65292;&#20854;&#20013;&#27599;&#20010;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.15363</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#25991;&#26412;&#21040;SQL&#30340;&#30740;&#31350;&#65306;&#19968;&#20010;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#19968;&#31181;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#22522;&#20934;&#38459;&#30861;&#20102;&#35774;&#35745;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#21644;&#24191;&#27867;&#30340;&#27604;&#36739;&#65292;&#21253;&#25324;&#38382;&#39064;&#34920;&#31034;&#12289;&#31034;&#20363;&#36873;&#25321;&#21644;&#31034;&#20363;&#32452;&#32455;&#65292;&#24182;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#35814;&#32454;&#38416;&#36848;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#65292;&#36798;&#21040;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#26438;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24378;&#35843;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#65292;&#24182;&#22312;&#27492;&#24230;&#37327;&#19979;&#27604;&#36739;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24320;&#28304;LLMs&#65292;&#24182;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#30417;&#30563;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborates their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. Additionally, we investigate open-source LLMs in in-context learning, and further enhance their performance with task-specific superv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;Gscore&#20316;&#20026;&#34913;&#37327;&#29983;&#25104;&#32467;&#26524;&#36136;&#37327;&#30340;&#32508;&#21512;&#25351;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.04823</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Generation Capabilities of Large Chinese Language Models. (arXiv:2308.04823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;Gscore&#20316;&#20026;&#34913;&#37327;&#29983;&#25104;&#32467;&#26524;&#36136;&#37327;&#30340;&#32508;&#21512;&#25351;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CG-Eval&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23545;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#23398;&#31185;&#39046;&#22495;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#31185;&#23398;&#24037;&#31243;&#12289;&#20154;&#25991;&#31038;&#31185;&#12289;&#25968;&#23398;&#35745;&#31639;&#12289;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#12289;&#21496;&#27861;&#32771;&#35797;&#21644;&#27880;&#20876;&#20250;&#35745;&#24072;&#32771;&#35797;&#20845;&#20010;&#23398;&#31185;&#20013;&#29983;&#25104;&#20934;&#30830;&#21644;&#30456;&#20851;&#30340;&#22238;&#31572;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;Gscore&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#21152;&#26435;&#27714;&#21644;&#24471;&#21040;&#30340;&#32508;&#21512;&#25351;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#19982;&#21442;&#32771;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#27979;&#35797;&#25968;&#25454;&#21644;&#27979;&#35797;&#32467;&#26524;&#21487;&#22312;&#27492;http URL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents CG-Eval, the first comprehensive evaluation of the generation capabilities of large Chinese language models across a wide range of academic disciplines. The models' performance was assessed based on their ability to generate accurate and relevant responses to different types of questions in six disciplines, namely, Science and Engineering, Humanities and Social Sciences, Mathematical Calculations, Medical Practitioner Qualification Examination, Judicial Examination, and Certified Public Accountant Examination. This paper also presents Gscore, a composite index derived from the weighted sum of multiple metrics to measure the quality of model's generation against a reference. The test data and test results can be found at this http URL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleTTS 2&#30340;TTS&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#19982;&#20197;&#24448;&#27169;&#22411;&#19981;&#21516;&#65292;StyleTTS 2&#23558;&#26679;&#24335;&#35270;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#22823;&#22411;SLMs&#21644;&#26032;&#30340;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2306.07691</link><description>&lt;p&gt;
StyleTTS 2&#65306;&#36890;&#36807;&#39118;&#26684;&#25193;&#25955;&#21644;&#19982;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models. (arXiv:2306.07691v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleTTS 2&#30340;TTS&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#19982;&#20197;&#24448;&#27169;&#22411;&#19981;&#21516;&#65292;StyleTTS 2&#23558;&#26679;&#24335;&#35270;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#22823;&#22411;SLMs&#21644;&#26032;&#30340;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StyleTTS2&#65292;&#19968;&#31181;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#39118;&#26684;&#25193;&#25955;&#21644;&#19982;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;TTS&#21512;&#25104;&#12290; StyleTTS 2&#36890;&#36807;&#23558;&#26679;&#24335;&#24314;&#27169;&#20026;&#28508;&#22312;&#30340;&#38543;&#26426;&#21464;&#37327;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#26080;&#38656;&#21442;&#32771;&#35821;&#38899;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#28508;&#22312;&#25193;&#25955;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#30340;&#22810;&#26679;&#21270;&#35821;&#38899;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;SLMs&#65288;&#20363;&#22914;WavLM&#65289;&#20316;&#20026;&#37492;&#21035;&#22120;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#22411;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#38899;&#30340;&#33258;&#28982;&#24230;&#12290; StyleTTS 2&#22312;&#21333;&#25196;&#22768;&#22120;LJSpeech&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#20154;&#31867;&#24405;&#38899;&#65292;&#24182;&#22312;&#22810;&#25196;&#22768;&#22120;VCTK&#25968;&#25454;&#38598;&#19978;&#19982;&#20043;&#21305;&#37197;&#65292;&#32463;&#36807;&#27597;&#35821;&#20026;&#33521;&#35821;&#30340;&#20154;&#21592;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#24403;&#22312;LibriTTS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#20197;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#38646;&#26679;&#26412;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#38024;&#23545;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;11&#31181;&#36866;&#24212;&#26041;&#27861;&#22312;&#19981;&#21516;&#27745;&#26579;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#36866;&#24212;&#26041;&#27861;&#23545;&#25991;&#26412;&#27745;&#26579;&#26356;&#25935;&#24863;&#65292;&#21333;&#29420;&#20351;&#29992;&#23567;&#22411;&#25991;&#26412;&#36866;&#37197;&#22120;&#27604;&#20849;&#20139;&#36866;&#37197;&#22120;&#26356;&#40065;&#26834;&#65292;&#21487;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#24178;&#20928;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02080</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models. (arXiv:2306.02080v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02080
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#38024;&#23545;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;11&#31181;&#36866;&#24212;&#26041;&#27861;&#22312;&#19981;&#21516;&#27745;&#26579;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#36866;&#24212;&#26041;&#27861;&#23545;&#25991;&#26412;&#27745;&#26579;&#26356;&#25935;&#24863;&#65292;&#21333;&#29420;&#20351;&#29992;&#23567;&#22411;&#25991;&#26412;&#36866;&#37197;&#22120;&#27604;&#20849;&#20139;&#36866;&#37197;&#22120;&#26356;&#40065;&#26834;&#65292;&#21487;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#24178;&#20928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#34920;&#29616;&#30340;&#21508;&#31181;&#36866;&#24212;&#26041;&#27861;&#65292;&#22914; LoRA&#12289;prompts &#21644; adapters &#31561;&#24050;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36866;&#24212;&#26041;&#27861;&#23545;&#20110;&#20998;&#24067;&#20301;&#31227;&#30340;&#40065;&#26834;&#24615;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;11&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36866;&#24212;&#26041;&#27861;&#22312;4&#20010;&#35270;&#35273;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#32771;&#23519;&#20102;&#21487;&#29992;&#36866;&#24212;&#31034;&#20363;&#21644;&#36866;&#24212;&#36807;&#31243;&#20013;&#21487;&#35757;&#32451;&#21442;&#25968;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#22320;&#65292;&#24341;&#20837;&#20102;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;96&#31181;&#35270;&#35273;&#21644;87&#31181;&#25991;&#26412;&#27745;&#25439;&#65292;&#20197;&#30740;&#31350;&#19981;&#21516;&#36866;&#24212;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#65306;1&#65289;&#36866;&#24212;&#26041;&#27861;&#23545;&#25991;&#26412;&#27745;&#26579;&#27604;&#35270;&#35273;&#27745;&#26579;&#26356;&#25935;&#24863;&#12290;2) &#20840;&#37327;&#24494;&#35843;&#24182;&#19981;&#24635;&#33021;&#25552;&#20379;&#26368;&#39640;&#30340;&#40065;&#26834;&#24615;&#65307;&#30456;&#21453;&#65292;&#36866;&#37197;&#22120;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24178;&#20928;&#24615;&#33021;&#12290;3&#65289;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21333;&#29420;&#20351;&#29992;&#23567;&#22411;&#25991;&#26412;&#36866;&#37197;&#22120;&#36890;&#24120;&#27604;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#31354;&#38388;&#20013;&#20849;&#20139;&#36866;&#37197;&#22120;&#26356;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. The robustness of these adaptation methods against distribution shifts have not been studied. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parameter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#20250;&#19979;&#38477;&#65292;&#21457;&#29616;Pythia&#27169;&#22411;&#22312;&#26356;&#22810;&#30340;&#21442;&#25968;&#19979;&#65292;&#23613;&#31649;&#25972;&#20307;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#24341;&#36848;&#37325;&#22797;&#21644;&#37325;&#26032;&#23450;&#20041;&#25968;&#23398;&#20004;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;&#38656;&#35201;&#22312;&#20219;&#20309;&#26102;&#20505;&#27979;&#35797;&#27169;&#22411;&#22312;&#30456;&#24212;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14681</link><description>&lt;p&gt;
&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36870;&#27604;&#20363;&#32553;&#25918;&#29616;&#35937;&#65306;&#32047;&#36184;&#36824;&#26159;&#36129;&#29486;&#65311;
&lt;/p&gt;
&lt;p&gt;
Emergent inabilities? Inverse scaling over the course of pretraining. (arXiv:2305.14681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14681
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#20250;&#19979;&#38477;&#65292;&#21457;&#29616;Pythia&#27169;&#22411;&#22312;&#26356;&#22810;&#30340;&#21442;&#25968;&#19979;&#65292;&#23613;&#31649;&#25972;&#20307;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#24341;&#36848;&#37325;&#22797;&#21644;&#37325;&#26032;&#23450;&#20041;&#25968;&#23398;&#20004;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;&#38656;&#35201;&#22312;&#20219;&#20309;&#26102;&#20505;&#27979;&#35797;&#27169;&#22411;&#22312;&#30456;&#24212;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#26159;&#21542;&#20250;&#23548;&#33268;&#36870;&#27604;&#20363;&#32553;&#25918;&#29616;&#35937;&#21602;&#65311;&#36825;&#20010;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#20250;&#19979;&#38477;&#65292;&#23613;&#31649;&#25972;&#20307;&#34920;&#29616;&#20173;&#28982;&#19981;&#38169;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#36870;&#27604;&#20363;&#32553;&#25918;&#25361;&#25112;&#20013;&#30340;&#20004;&#20010;&#20219;&#21153; - &#24341;&#36848;&#37325;&#22797;&#21644;&#37325;&#26032;&#23450;&#20041;&#25968;&#23398;&#65292;Pythia&#27169;&#22411;&#36816;&#29992;&#26356;&#22810;&#30340;&#21442;&#25968;&#26102;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#30830;&#23454;&#20250;&#19979;&#38477;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#25972;&#20307;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#31361;&#26174;&#20102;&#22312;&#20219;&#20309;&#26102;&#20505;&#22914;&#26524;&#27169;&#22411;&#34987;&#35757;&#32451;&#20102;&#39069;&#22806;&#30340;&#25968;&#25454;&#65292;&#37027;&#20040;&#38656;&#35201;&#27979;&#35797;&#27169;&#22411;&#22312;&#30456;&#24212;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#25972;&#20307;&#34920;&#29616;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does inverse scaling only occur as a function of model parameter size, or can it also occur over the course of training? We carry out an exploratory study investigating whether, over the course of training on the language modeling task, the performance of language models at specific tasks can decrease while general performance remains high. We find that for two tasks from the Inverse Scaling Challenge - quote-repetition and redefine-math - this is indeed the case. Specifically, we find that for Pythia (Biderman et al., 2023) models with a higher number of parameters, performance decreases over the course of training at these two tasks, despite these models showing standard (positive) scaling overall. This highlights the importance of testing model performance at all relevant benchmarks any time they are trained on additional data, even if their overall performance improves.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;Prompt Tuning&#65288;MPT&#65289;&#22914;&#20309;&#24110;&#21161;&#25913;&#21892;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#20351;&#29992;&#20803;&#23398;&#20064;&#21487;&#20197;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#20013;&#23398;&#20064;&#21021;&#22987;&#21270;Prompt&#23884;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#39564;&#20102;&#20195;&#34920;&#24615;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#22823;&#37327;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;MPT&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2302.08143</link><description>&lt;p&gt;
&#23398;&#20064;&#21021;&#22987;&#21270;&#65306;&#20803;&#23398;&#20064;&#33021;&#21542;&#25552;&#39640;Prompt Tuning&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?. (arXiv:2302.08143v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;Prompt Tuning&#65288;MPT&#65289;&#22914;&#20309;&#24110;&#21161;&#25913;&#21892;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#20351;&#29992;&#20803;&#23398;&#20064;&#21487;&#20197;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#20013;&#23398;&#20064;&#21021;&#22987;&#21270;Prompt&#23884;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#39564;&#20102;&#20195;&#34920;&#24615;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#22823;&#37327;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;MPT&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning (PT)&#26159;&#19968;&#31181;&#21482;&#35843;&#25972;&#27599;&#20010;&#20219;&#21153;&#30340;&#19968;&#20010;&#39069;&#22806;&#26631;&#35760;&#24207;&#21015;&#30340;&#23884;&#20837;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#23436;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#19981;&#21464;&#65292;&#24050;&#32463;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;PT&#24050;&#32463;&#34987;&#35777;&#26126;&#26497;&#22823;&#22320;&#20381;&#36182;&#20110;&#24456;&#22909;&#30340;Prompt&#23884;&#20837;&#30340;&#21021;&#22987;&#21270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;Prompt Tuning (MPT) &#26469;&#31995;&#32479;&#22320;&#25506;&#32034;&#20803;&#23398;&#20064;&#22914;&#20309;&#24110;&#21161;&#36890;&#36807;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#23398;&#20064;&#21021;&#22987;&#21270;Prompt&#23884;&#20837;&#26469;&#25913;&#21892;&#65288;&#22914;&#26524;&#21487;&#20197;&#65289;PT&#20013;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#19978;&#20351;&#29992;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#26469;&#32463;&#39564;&#20998;&#26512;&#20102;&#19968;&#31995;&#21015;&#20195;&#34920;&#24615;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#20998;&#26512;&#19981;&#21516;&#28304;/&#30446;&#26631;&#20219;&#21153;&#37197;&#32622;&#19979;&#30340;&#21508;&#31181;&#35843;&#25972;&#35774;&#32622;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MPT&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#23427;&#29305;&#21035;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#25552;&#21319;&#26159;&#26174;&#33879;&#30340;&#12290;&#23545;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#38382;&#39064;&#22238;&#31572;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;MPT&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;PT&#65292;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most case
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#36873;&#25321;&#26694;&#26550;&#65288;DSIR&#65289;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;KL&#20943;&#23569;&#20316;&#20026;&#25968;&#25454;&#24230;&#37327;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#22312;&#38477;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#37325;&#35201;&#24615;&#26435;&#37325;&#20197;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2302.03169</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Data Selection for Language Models via Importance Resampling. (arXiv:2302.03169v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03169
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#36873;&#25321;&#26694;&#26550;&#65288;DSIR&#65289;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;KL&#20943;&#23569;&#20316;&#20026;&#25968;&#25454;&#24230;&#37327;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#22312;&#38477;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#37325;&#35201;&#24615;&#26435;&#37325;&#20197;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#20110;&#36890;&#29992;&#39046;&#22495;&#65288;&#22914;GPT-3&#65289;&#21644;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;Codex&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20174;&#22823;&#22411;&#21407;&#22987;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#19968;&#20010;&#23376;&#38598;&#65292;&#20197;&#21305;&#37197;&#32473;&#23450;&#19968;&#20123;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#30340;&#25152;&#38656;&#30446;&#26631;&#20998;&#24067;&#12290;&#37492;&#20110;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#24230;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#19987;&#23478;&#25163;&#21160;&#31574;&#21010;&#25968;&#25454;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22312;LM&#25968;&#25454;&#36873;&#25321;&#20013;&#20351;&#29992;&#30340;&#32463;&#20856;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#20302;&#32500;&#31354;&#38388;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#25968;&#25454;&#36873;&#25321;&#19982;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#65288;DSIR&#65289;&#65292;&#23427;&#22312;&#19968;&#20010;&#38477;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20197;&#20415;&#26681;&#25454;&#36825;&#20123;&#26435;&#37325;&#36827;&#34892;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#25968;&#25454;&#36873;&#25321;&#12290;&#20026;&#20102;&#30830;&#23450;&#19968;&#20010;&#21512;&#36866;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;KL&#20943;&#23569;&#65292;&#19968;&#31181;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#34913;&#37327;&#25152;&#36873;&#39044;&#35757;&#32451;&#25968;&#25454;&#19982;&#30446;&#26631;&#20043;&#38388;&#30456;&#20284;&#24230;&#30340;&#25968;&#25454;&#24230;&#37327;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given some unlabeled target samples. Due to the large scale and dimensionality of the raw text data, existing methods use simple heuristics or use experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. To determine an appropriate feature space, we show that KL reduction, a data metric that measures the proximity between selected pretraining data and the target in a feature space, has high correlat
&lt;/p&gt;</description></item><item><title>TalkTheWalk &#26159;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#31934;&#24515;&#31574;&#21010;&#30340;&#39033;&#30446;&#25910;&#34255;&#20013;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#21512;&#25104;&#36924;&#30495;&#39640;&#36136;&#37327;&#30340;&#20250;&#35805;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#26500;&#24314;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2301.11489</link><description>&lt;p&gt;
Talk the Walk: &#38024;&#23545;&#20250;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Talk the Walk: Synthetic Data Generation for Conversational Music Recommendation. (arXiv:2301.11489v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11489
&lt;/p&gt;
&lt;p&gt;
TalkTheWalk &#26159;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#31934;&#24515;&#31574;&#21010;&#30340;&#39033;&#30446;&#25910;&#34255;&#20013;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#21512;&#25104;&#36924;&#30495;&#39640;&#36136;&#37327;&#30340;&#20250;&#35805;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#26500;&#24314;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25910;&#38598;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24191;&#27867;&#23384;&#22312;&#65292;&#20294;&#29992;&#25143;&#24448;&#24448;&#24456;&#38590;&#22312;&#25512;&#33616;&#36136;&#37327;&#36739;&#24046;&#26102;&#36827;&#34892;&#25511;&#21046;&#21644;&#35843;&#25972;&#12290;&#36825;&#20419;&#20351;&#20102;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;(CRSs)&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25552;&#20379;&#23545;&#25512;&#33616;&#30340;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#38656;&#35201;&#21253;&#21547;&#29992;&#25143;&#35805;&#35821;&#21644;&#28085;&#30422;&#22810;&#26679;&#21270;&#20559;&#22909;&#33539;&#22260;&#30340;&#39033;&#30446;&#30340;&#20250;&#35805;&#35757;&#32451;&#25968;&#25454;&#12290;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#22914;&#20247;&#21253;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#25910;&#38598;&#36215;&#26469;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#22312;&#39033;&#30446;&#38598;&#25512;&#33616;&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#27880;&#24847;&#21040;&#36825;&#20010;&#20219;&#21153;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#65292;&#21160;&#26426;&#22312;&#20110;&#38899;&#20048;&#12289;&#26032;&#38395;&#21644;&#39135;&#35889;&#25512;&#33616;&#31561;&#20351;&#29992;&#26696;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;TalkTheWalk&#65292;&#36890;&#36807;&#21033;&#29992;&#24191;&#27867;&#21487;&#33719;&#24471;&#30340;&#31934;&#24515;&#31574;&#21010;&#30340;&#39033;&#30446;&#25910;&#34255;&#20013;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#21512;&#25104;&#36924;&#30495;&#39640;&#36136;&#37327;&#30340;&#20250;&#35805;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;&#39033;&#30446;&#38598;&#31574;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems are ubiquitous yet often difficult for users to control and adjust when recommendation quality is poor. This has motivated the development of conversational recommendation systems (CRSs), with control over recommendations provided through natural language feedback. However, building conversational recommendation systems requires conversational training data involving user utterances paired with items that cover a diverse range of preferences. Such data has proved challenging to collect scalably using conventional methods like crowdsourcing. We address it in the context of item-set recommendation, noting the increasing attention to this task motivated by use cases like music, news and recipe recommendation. We present a new technique, TalkTheWalk, that synthesizes realistic high-quality conversational data by leveraging domain expertise encoded in widely available curated item collections, showing how these can be transformed into corresponding item set curation c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MEAL&#26041;&#27861;&#65292;&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#36827;&#34892;&#20998;&#31867;&#65292;&#31283;&#23450;&#21644;&#27963;&#36291;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#36129;&#29486;&#65292;&#19968;&#20010;&#26159;&#25552;&#20986;&#26032;&#39062;&#30340;&#20943;&#23569;&#36816;&#34892;&#21464;&#24322;&#24615;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#21478;&#19968;&#20010;&#26159;&#24341;&#20837;AL&#20934;&#21017;&#29992;&#20110;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2211.08358</link><description>&lt;p&gt;
MEAL&#65306;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#31283;&#23450;&#21644;&#27963;&#36291;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MEAL: Stable and Active Learning for Few-Shot Prompting. (arXiv:2211.08358v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MEAL&#26041;&#27861;&#65292;&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#36827;&#34892;&#20998;&#31867;&#65292;&#31283;&#23450;&#21644;&#27963;&#36291;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#36129;&#29486;&#65292;&#19968;&#20010;&#26159;&#25552;&#20986;&#26032;&#39062;&#30340;&#20943;&#23569;&#36816;&#34892;&#21464;&#24322;&#24615;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#21478;&#19968;&#20010;&#26159;&#24341;&#20837;AL&#20934;&#21017;&#29992;&#20110;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21551;&#21160;&#21644;&#25552;&#31034;&#65292;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#38598;&#21512;&#65288;&#25968;&#25454;&#36873;&#25321;&#65289;&#21644;&#19981;&#21516;&#30340;&#24494;&#35843;&#36816;&#34892;&#65288;&#36816;&#34892;&#21464;&#24322;&#24615;&#65289;&#20043;&#38388;&#20855;&#26377;&#39640;&#21464;&#21270;&#29575;&#65292;&#36825;&#19981;&#20165;&#38459;&#30861;&#20102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#20844;&#24179;&#27604;&#36739;&#65292;&#32780;&#19988;&#20351;&#24471;&#23569;&#26679;&#26412;&#23398;&#20064;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#36807;&#20110;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#65292;&#29992;&#20110;&#26356;&#31283;&#23450;&#21644;&#26377;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#36816;&#34892;&#21464;&#24322;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#20934;&#21017;&#29992;&#20110;&#25968;&#25454;&#36873;&#25321;&#65292;&#24182;&#21576;&#29616;&#20102;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;AL&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32852;&#21512;&#26041;&#27861;MEAL&#65288;&#22810;&#25552;&#31034;&#24494;&#35843;&#19982;&#39044;&#27979;&#38598;&#25104;&#19982;&#20027;&#21160;&#23398;&#20064;&#65289;&#21487;&#20197;&#31283;&#23450;&#22320;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot classification has made great strides due to foundation models that, through priming and prompting, are highly effective few-shot learners. However, this approach has high variance both across different sets of few shots (data selection) and across different finetuning runs (run variability). This is problematic not only because it impedes the fair comparison of different approaches, but especially because it makes few-shot learning too unreliable for many real-world applications. To alleviate these issues, we make two contributions for more stable and effective few-shot learning: First, we propose novel ensembling methods and show that they substantially reduce run variability. Second, we introduce a new active learning (AL) criterion for data selection and present the first AL-based approach specifically tailored towards prompt-based learning. In our experiments, we show that our combined method, MEAL (Multiprompt finetuning and prediction Ensembling with Active Learning), i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoSumm&#30340;&#26032;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#25191;&#34892;&#26080;&#30417;&#30563;&#30340;&#25277;&#21462;&#24335;&#24847;&#35265;&#25688;&#35201;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#21644;&#36817;&#20284;&#27979;&#22320;&#32447;&#36317;&#31163;&#30340;&#24471;&#20998;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#30340;&#21516;&#26102;&#40065;&#26834;&#24615;&#20063;&#24471;&#21040;&#20102;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2209.07496</link><description>&lt;p&gt;
&#20351;&#29992;&#36817;&#20284;&#27979;&#22320;&#32447;&#30340;&#26080;&#30417;&#30563;&#24847;&#35265;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Opinion Summarization Using Approximate Geodesics. (arXiv:2209.07496v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoSumm&#30340;&#26032;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#25191;&#34892;&#26080;&#30417;&#30563;&#30340;&#25277;&#21462;&#24335;&#24847;&#35265;&#25688;&#35201;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#21644;&#36817;&#20284;&#27979;&#22320;&#32447;&#36317;&#31163;&#30340;&#24471;&#20998;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#30340;&#21516;&#26102;&#40065;&#26834;&#24615;&#20063;&#24471;&#21040;&#20102;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#35265;&#25688;&#35201;&#26159;&#20174;&#29992;&#25143;&#35780;&#35770;&#20013;&#25429;&#33719;&#27969;&#34892;&#35266;&#28857;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Geodesic Summarizer (GeoSumm)&#30340;&#26032;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#25191;&#34892;&#26080;&#30417;&#30563;&#30340;&#25277;&#21462;&#24335;&#24847;&#35265;&#25688;&#35201;&#12290;GeoSumm&#28041;&#21450;&#22522;&#20110;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#19978;&#23545;&#39044;&#35757;&#32451;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#35789;&#20856;&#23398;&#20064;&#26469;&#29983;&#25104;&#25991;&#26412;&#30340;&#34920;&#31034;&#24418;&#24335;&#20026;&#28508;&#22312;&#35821;&#20041;&#21333;&#20803;&#30340;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#26469;&#37327;&#21270;&#35780;&#35770;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#27979;&#22320;&#32447;&#36317;&#31163;&#30340;&#24471;&#20998;&#26426;&#21046;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#20851;&#24615;&#35780;&#20998;&#26469;&#35782;&#21035;&#27969;&#34892;&#35266;&#28857;&#65292;&#20174;&#32780;&#32452;&#25104;&#26222;&#36941;&#24615;&#21644;&#26041;&#38754;&#29305;&#23450;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;GeoSumm&#22312;&#19977;&#20010;&#24847;&#35265;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#39069;&#22806;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#21151;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#29983;&#25104;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinion summarization is the task of creating summaries capturing popular opinions from user reviews. In this paper, we introduce Geodesic Summarizer (GeoSumm), a novel system to perform unsupervised extractive opinion summarization. GeoSumm involves an encoder-decoder based representation learning model, that generates representations of text as a distribution over latent semantic units. GeoSumm generates these representations by performing dictionary learning over pre-trained text representations at multiple decoder layers. We then use these representations to quantify the relevance of review sentences using a novel approximate geodesic distance based scoring mechanism. We use the relevance scores to identify popular opinions in order to compose general and aspect-specific summaries. Our proposed model, GeoSumm, achieves state-of-the-art performance on three opinion summarization datasets. We perform additional experiments to analyze the functioning of our model and showcase the gene
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#65292;&#20026;&#26089;&#26399;&#21644;&#24555;&#36895;&#25511;&#21046;&#30149;&#27602;&#20256;&#25773;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2201.01140</link><description>&lt;p&gt;
&#21033;&#29992;PSSM&#21644;&#35789;&#23884;&#20837;&#39044;&#27979;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#23487;&#20027;
&lt;/p&gt;
&lt;p&gt;
Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01140
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#65292;&#20026;&#26089;&#26399;&#21644;&#24555;&#36895;&#25511;&#21046;&#30149;&#27602;&#20256;&#25773;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24863;&#30149;&#27602;&#30340;&#24555;&#36895;&#31361;&#21464;&#23041;&#32961;&#20844;&#20849;&#20581;&#24247;&#65292;&#21487;&#33021;&#24341;&#21457;&#33268;&#21629;&#30340;&#22823;&#27969;&#34892;&#30149;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#22312;&#29190;&#21457;&#26399;&#38388;&#25110;&#29190;&#21457;&#21518;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#27969;&#24863;&#30149;&#27602;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#29289;&#31181;&#20043;&#38388;&#24490;&#29615;&#20256;&#25773;&#12290;&#22240;&#27492;&#65292;&#26089;&#26399;&#21644;&#24555;&#36895;&#26816;&#27979;&#30149;&#27602;&#23487;&#20027;&#23558;&#26377;&#21161;&#20110;&#20943;&#23569;&#30149;&#27602;&#30340;&#36827;&#19968;&#27493;&#20256;&#25773;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#20301;&#32622;&#29305;&#24322;&#24615;&#24471;&#20998;&#30697;&#38453;&#65288;PSSM&#65289;&#21644;&#23398;&#20064;&#33258;&#35789;&#23884;&#20837;&#21644;&#35789;&#32534;&#30721;&#30340;&#29305;&#24449;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25512;&#26029;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;PSSM&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;95%&#24038;&#21491;&#30340;MCC&#21644;96%&#24038;&#21491;&#30340;F1&#12290;&#20351;&#29992;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#24471;&#21040;&#30340;MCC&#32422;&#20026;96&#65285;&#65292;F1&#32422;&#20026;97&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid mutation of the influenza virus threatens public health. Reassortment among viruses with different hosts can lead to a fatal pandemic. However, it is difficult to detect the original host of the virus during or after an outbreak as influenza viruses can circulate between different species. Therefore, early and rapid detection of the viral host would help reduce the further spread of the virus. We use various machine learning models with features derived from the position-specific scoring matrix (PSSM) and features learned from word embedding and word encoding to infer the origin host of viruses. The results show that the performance of the PSSM-based model reaches the MCC around 95%, and the F1 around 96%. The MCC obtained using the model with word embedding is around 96%, and the F1 is around 97%.
&lt;/p&gt;</description></item><item><title>CoPaSul&#24037;&#20855;&#21253;&#25552;&#20379;&#20102;&#33258;&#21160;&#30340;&#38901;&#24459;&#26631;&#27880;&#21644;&#29305;&#24449;&#25552;&#21462;&#21151;&#33021;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#36718;&#24275;&#30340;&#21442;&#25968;&#21270;&#21644;&#21472;&#21152;&#38901;&#24459;&#39118;&#26684;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35813;&#24037;&#20855;&#21253;&#21487;&#20197;&#24471;&#21040;&#19982;&#38901;&#24459;&#36793;&#30028;&#21644;&#31361;&#20986;&#24615;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#31995;&#25968;&#32858;&#31867;&#24471;&#21040;&#38901;&#24459;&#36718;&#24275;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/1612.04765</link><description>&lt;p&gt;
CoPaSul&#25163;&#20876;--&#22522;&#20110;&#36718;&#24275;&#30340;&#21442;&#25968;&#21270;&#21644;&#21472;&#21152;&#38901;&#24459;&#39118;&#26684;&#21270;
&lt;/p&gt;
&lt;p&gt;
CoPaSul Manual -- Contour-based parametric and superpositional intonation stylization. (arXiv:1612.04765v11 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1612.04765
&lt;/p&gt;
&lt;p&gt;
CoPaSul&#24037;&#20855;&#21253;&#25552;&#20379;&#20102;&#33258;&#21160;&#30340;&#38901;&#24459;&#26631;&#27880;&#21644;&#29305;&#24449;&#25552;&#21462;&#21151;&#33021;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#36718;&#24275;&#30340;&#21442;&#25968;&#21270;&#21644;&#21472;&#21152;&#38901;&#24459;&#39118;&#26684;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35813;&#24037;&#20855;&#21253;&#21487;&#20197;&#24471;&#21040;&#19982;&#38901;&#24459;&#36793;&#30028;&#21644;&#31361;&#20986;&#24615;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#31995;&#25968;&#32858;&#31867;&#24471;&#21040;&#38901;&#24459;&#36718;&#24275;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CoPaSul&#24037;&#20855;&#21253;&#30340;&#30446;&#30340;&#26159;&#33258;&#21160;&#30340;&#38901;&#24459;&#26631;&#27880;&#21644;&#20174;&#38899;&#33410;&#21040;&#35821;&#21477;&#32423;&#21035;&#30340;&#38901;&#24459;&#29305;&#24449;&#25552;&#21462;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#38901;&#24459;&#34987;&#34920;&#31034;&#20026;&#20840;&#23616;&#21644;&#23616;&#37096;&#36718;&#24275;&#30340;&#21472;&#21152;&#65292;&#36825;&#20123;&#36718;&#24275;&#22312;&#22810;&#39033;&#24335;&#31995;&#25968;&#30340;&#21442;&#25968;&#21270;&#25551;&#36848;&#19979;&#12290;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#65288;&#36890;&#24120;&#19982;&#20294;&#19981;&#19968;&#23450;&#38480;&#20110;&#35821;&#35843;&#30701;&#35821;&#30456;&#20851;&#65289;&#65292;&#39118;&#26684;&#21270;&#29992;&#20110;&#20197;&#26102;&#38388;&#21464;&#21270;&#30340;F0&#27700;&#24179;&#21644;&#33539;&#22260;&#26469;&#34920;&#31034;&#38899;&#35843;&#12290;&#22312;&#23616;&#37096;&#23618;&#38754;&#19978;&#65288;&#20363;&#22914;&#65292;&#37325;&#38899;&#32452;&#65289;&#65292;&#25551;&#36848;&#23616;&#37096;&#36718;&#24275;&#24418;&#29366;&#12290;&#36890;&#36807;&#36825;&#31181;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#20960;&#20010;&#19982;&#38901;&#24459;&#36793;&#30028;&#21644;&#31361;&#20986;&#24615;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31995;&#25968;&#32858;&#31867;&#65292;&#21487;&#20197;&#20197;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#24335;&#33719;&#24471;&#38901;&#24459;&#36718;&#24275;&#31867;&#21035;&#12290;&#38500;&#20102;&#22522;&#20110;&#39118;&#26684;&#21270;&#30340;&#29305;&#24449;&#25552;&#21462;&#22806;&#65292;&#36824;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#30340;F0&#21644;&#33021;&#37327;&#27979;&#37327;&#65288;&#20363;&#22914;&#65292;&#24179;&#22343;&#20540;&#21644;&#26041;&#24046;&#65289;&#20197;&#21450;&#38901;&#24459;&#26041;&#38754;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purposes of the CoPaSul toolkit are (1) automatic prosodic annotation and (2) prosodic feature extraction from syllable to utterance level. CoPaSul stands for contour-based, parametric, superpositional intonation stylization. In this framework intonation is represented as a superposition of global and local contours that are described parametrically in terms of polynomial coefficients. On the global level (usually associated but not necessarily restricted to intonation phrases) the stylization serves to represent register in terms of time-varying F0 level and range. On the local level (e.g. accent groups), local contour shapes are described. From this parameterization several features related to prosodic boundaries and prominence can be derived. Furthermore, by coefficient clustering prosodic contour classes can be obtained in a bottom-up way. Next to the stylization-based feature extraction also standard F0 and energy measures (e.g. mean and variance) as well as rhythmic aspects c
&lt;/p&gt;</description></item></channel></rss>