<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#21033;&#29992;&#25171;&#21360;&#35843;&#35797;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22797;&#26434;&#32534;&#31243;&#38382;&#39064;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#27233;&#30382;&#40493;&#35843;&#35797;&#22312;Leetcode&#30340;&#31616;&#21333;&#21644;&#20013;&#31561;&#32423;&#21035;&#38382;&#39064;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;1.5%&#21644;17.9%&#12290;</title><link>http://arxiv.org/abs/2401.05319</link><description>&lt;p&gt;
&#21033;&#29992;&#25171;&#21360;&#35843;&#35797;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Leveraging Print Debugging to Improve Code Generation in Large Language Models. (arXiv:2401.05319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05319
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25171;&#21360;&#35843;&#35797;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22797;&#26434;&#32534;&#31243;&#38382;&#39064;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#27233;&#30382;&#40493;&#35843;&#35797;&#22312;Leetcode&#30340;&#31616;&#21333;&#21644;&#20013;&#31561;&#32423;&#21035;&#38382;&#39064;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;1.5%&#21644;17.9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#30340;&#32534;&#31243;&#38382;&#39064;&#26102;&#24615;&#33021;&#20173;&#19981;&#29702;&#24819;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;"&#25171;&#21360;&#35843;&#35797;"&#26041;&#27861;&#26469;&#24341;&#23548;LLMs&#36827;&#34892;&#35843;&#35797;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#25554;&#20837;&#25171;&#21360;&#35821;&#21477;&#20197;&#36319;&#36394;&#21644;&#20998;&#26512;&#26085;&#24535;&#26469;&#20462;&#22797;&#38169;&#35823;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;Leetcode&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;Leetcode&#22312;&#32447;&#21028;&#39064;&#31995;&#32479;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;GPT-4&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;Leetcode&#30340;&#31616;&#21333;&#21644;&#20013;&#31561;&#32423;&#21035;&#38382;&#39064;&#19978;&#20248;&#20110;&#27233;&#30382;&#40493;&#35843;&#35797;&#20998;&#21035;&#36798;&#21040;1.5%&#21644;17.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant progress in code generation tasks, but their performance in tackling programming problems with complex data structures and algorithms remains suboptimal. To address this issue, we propose an in-context learning approach that guides LLMs to debug by using a "print debugging" method, which involves inserting print statements to trace and analysing logs for fixing the bug. We collect a Leetcode problem dataset and evaluate our method using the Leetcode online judging system. Experiments with GPT-4 demonstrate the effectiveness of our approach, outperforming rubber duck debugging in easy and medium-level Leetcode problems by 1.5% and 17.9%.
&lt;/p&gt;</description></item><item><title>ANIM-400K&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#33258;&#21160;&#21270;&#35270;&#39057;&#37197;&#38899;&#21644;&#20854;&#20182;&#19982;&#35270;&#39057;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#23427;&#35299;&#20915;&#20102;&#35821;&#35328;&#24046;&#24322;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.05314</link><description>&lt;p&gt;
ANIM-400K: &#29992;&#20110;&#33258;&#21160;&#21270;&#35270;&#39057;&#37197;&#38899;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of Video. (arXiv:2401.05314v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05314
&lt;/p&gt;
&lt;p&gt;
ANIM-400K&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#33258;&#21160;&#21270;&#35270;&#39057;&#37197;&#38899;&#21644;&#20854;&#20182;&#19982;&#35270;&#39057;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#23427;&#35299;&#20915;&#20102;&#35821;&#35328;&#24046;&#24322;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#20016;&#23500;&#30340;&#20869;&#23481;&#20013;&#65292;&#39640;&#36798;60&#65285;&#26159;&#29992;&#33521;&#35821;&#21457;&#24067;&#30340;&#65292;&#36825;&#19982;&#20840;&#29699;&#20154;&#21475;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#20840;&#29699;&#21482;&#26377;18.8&#65285;&#26159;&#33521;&#35821;&#20351;&#29992;&#32773;&#65292;&#19988;&#21482;&#26377;5.1&#65285;&#23558;&#20854;&#35270;&#20026;&#27597;&#35821;&#65292;&#23548;&#33268;&#22312;&#32447;&#20449;&#24687;&#33719;&#21462;&#23384;&#22312;&#24046;&#24322;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20351;&#29992;&#26367;&#20195;&#32763;&#35793;&#23383;&#24149;&#26367;&#25442;&#35270;&#39057;&#30340;&#38899;&#36712;&#20173;&#28982;&#26159;&#19968;&#39033;&#22797;&#26434;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#31934;&#30830;&#30340;&#26102;&#24207;&#12289;&#38754;&#37096;&#36816;&#21160;&#21516;&#27493;&#21644;&#38901;&#24459;&#21305;&#37197;&#12290;&#34429;&#28982;&#31471;&#23545;&#31471;&#37197;&#38899;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#25968;&#25454;&#31232;&#32570;&#32487;&#32493;&#38459;&#30861;&#30528;&#31471;&#23545;&#31471;&#21644;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Anim-400K&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;425K&#20010;&#26085;&#35821;&#21644;&#33521;&#35821;&#23545;&#40784;&#30340;&#21160;&#30011;&#35270;&#39057;&#29255;&#27573;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#21508;&#31181;&#19982;&#35270;&#39057;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#37197;&#38899;&#12289;&#21516;&#26102;&#32763;&#35793;&#12289;&#23548;&#21521;&#35270;&#39057;&#25688;&#35201;&#21644;&#31867;&#22411;/&#20027;&#39064;/&#39118;&#26684;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#24050;&#20844;&#24320;&#25552;&#20379;&#32473;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet's wealth of content, with up to 60% published in English, starkly contrasts the global population, where only 18.8% are English speakers, and just 5.1% consider it their native language, leading to disparities in online information access. Unfortunately, automated processes for dubbing of video - replacing the audio track of a video with a translated alternative remains a complex and challenging task due to pipelines, necessitating precise timing, facial movement synchronization, and prosody matching. While end-to-end dubbing offers a solution, data scarcity continues to impede the progress of both end-to-end and pipeline-based methods. In this work, we introduce Anim-400K, a comprehensive dataset of over 425K aligned animated video segments in Japanese and English supporting various video-related tasks, including automated dubbing, simultaneous translation, guided video summarization, and genre/theme/style classification. Our dataset is made publicly available for resea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#8220;&#25105;&#26159;&#19968;&#20010;&#22855;&#24618;&#30340;&#25968;&#25454;&#38598;&#8221;&#65292;&#29992;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#22788;&#29702;&#20803;&#35821;&#35328;&#33258;&#25351;&#30340;&#38472;&#36848;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#22312;&#29983;&#25104;&#21644;&#39564;&#35777;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#37117;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.05300</link><description>&lt;p&gt;
&#25105;&#26159;&#19968;&#20010;&#22855;&#24618;&#30340;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#20803;&#35821;&#35328;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
I am a Strange Dataset: Metalinguistic Tests for Language Models. (arXiv:2401.05300v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;&#8220;&#25105;&#26159;&#19968;&#20010;&#22855;&#24618;&#30340;&#25968;&#25454;&#38598;&#8221;&#65292;&#29992;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#22788;&#29702;&#20803;&#35821;&#35328;&#33258;&#25351;&#30340;&#38472;&#36848;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#22312;&#29983;&#25104;&#21644;&#39564;&#35777;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#37117;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#28041;&#21450;&#20803;&#35821;&#35328;&#33258;&#25351;&#30340;&#38472;&#36848;&#65288;&#8220;&#26412;&#35770;&#25991;&#26377;&#20845;&#20010;&#37096;&#20998;&#12290;&#8221;&#65289;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#22788;&#29702;&#36825;&#26679;&#30340;&#35821;&#35328;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#8220;&#25105;&#26159;&#19968;&#20010;&#22855;&#24618;&#30340;&#25968;&#25454;&#38598;&#8221;&#65292;&#29992;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#21253;&#21547;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#20250;&#32487;&#32493;&#31867;&#20284;&#20110;&#8220;&#36825;&#20010;&#21477;&#23376;&#20013;&#20498;&#25968;&#31532;&#20108;&#20010;&#35789;&#26159;&#8221;&#30340;&#38472;&#36848;&#65288;&#27491;&#30830;&#30340;&#32487;&#32493;&#24212;&#35813;&#26159;&#8220;&#26159;&#8221;&#65289;&#12290;&#22312;&#39564;&#35777;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#20250;&#21028;&#26029;&#31867;&#20284;&#20110;&#8220;&#36825;&#20010;&#21477;&#23376;&#20013;&#20498;&#25968;&#31532;&#20108;&#20010;&#35789;&#26159;&#21477;&#23376;&#12290;&#8221;&#30340;&#38472;&#36848;&#30340;&#30495;&#23454;&#24615;&#65288;&#26159;&#20551;&#30340;&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26368;&#23567;&#24046;&#24322;&#30340;&#38750;&#33258;&#25351;&#20803;&#35821;&#35328;&#31034;&#20363;&#65292;&#26469;&#34917;&#20805;&#20027;&#25968;&#25454;&#38598;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22788;&#29702;&#20803;&#35821;&#35328;&#35821;&#35328;&#12290;&#25968;&#25454;&#38598;&#30001;&#19987;&#23478;&#25163;&#24037;&#21046;&#20316;&#65292;&#38750;&#19987;&#23478;&#26631;&#27880;&#21592;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#24320;&#28304;LLMs&#65288;&#20174;7B&#21040;70B&#30340;&#21442;&#25968;&#65289;&#20197;&#21450;&#36890;&#36807;API&#36827;&#34892;&#27979;&#35797;&#30340;&#38381;&#28304;LLMs&#12290;&#25152;&#26377;&#27169;&#22411;&#22312;&#20004;&#20010;&#23376;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#37117;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statements involving metalinguistic self-reference ("This paper has six sections.") are prevalent in many domains. Can large language models (LLMs) handle such language? In this paper, we present "I am a Strange Dataset", a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like "The penultimate word in this sentence is" (where a correct continuation is "is"). In verification, models judge the truth of statements like "The penultimate word in this sentence is sentence." (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and eve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#20449;&#24687;&#12289;&#35780;&#20272;&#21512;&#27861;&#24615;&#21644;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05273</link><description>&lt;p&gt;
INACIA&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges. (arXiv:2401.05273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#24052;&#35199;&#23457;&#35745;&#27861;&#38498;&#20013;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#20449;&#24687;&#12289;&#35780;&#20272;&#21512;&#27861;&#24615;&#21644;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INACIA&#65288;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36741;&#21161;&#25351;&#20196;&#31995;&#32479;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#24052;&#35199;&#32852;&#37030;&#23457;&#35745;&#27861;&#38498;&#65288;TCU&#65289;&#30340;&#36816;&#33829;&#26694;&#26550;&#20013;&#12290;&#35813;&#31995;&#32479;&#33258;&#21160;&#21270;&#20102;&#26696;&#20214;&#20998;&#26512;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#22522;&#26412;&#20449;&#24687;&#25552;&#21462;&#12289;&#21487;&#21463;&#29702;&#24615;&#23457;&#26597;&#12289;Periculum in mora&#21644;Fumus boni iuris&#20998;&#26512;&#20197;&#21450;&#24314;&#35758;&#29983;&#25104;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;INACIA&#20174;&#26696;&#20214;&#25991;&#20214;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12289;&#35780;&#20272;&#20854;&#21512;&#27861;&#24615;&#24182;&#29983;&#25104;&#21496;&#27861;&#24314;&#35758;&#30340;&#28508;&#21147;&#12290;&#21033;&#29992;&#39564;&#35777;&#25968;&#25454;&#38598;&#21644;LLMs&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#31995;&#32479;&#24615;&#33021;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;INACIA&#22788;&#29702;&#22797;&#26434;&#27861;&#24459;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#34920;&#26126;&#20854;&#36866;&#29992;&#20110;&#22686;&#21152;&#27861;&#24459;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21496;&#27861;&#20844;&#27491;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces INACIA (Instru\c{c}\~ao Assistida com Intelig\^encia Artificial), a groundbreaking system designed to integrate Large Language Models (LLMs) into the operational framework of Brazilian Federal Court of Accounts (TCU). The system automates various stages of case analysis, including basic information extraction, admissibility examination, Periculum in mora and Fumus boni iuris analyses, and recommendations generation. Through a series of experiments, we demonstrate INACIA's potential in extracting relevant information from case documents, evaluating its legal plausibility, and generating judicial recommendations. Utilizing a validation dataset alongside LLMs, our evaluation methodology presents an innovative approach to assessing system performance, correlating highly with human judgment. The results highlight INACIA's proficiency in handling complex legal tasks, indicating its suitability for augmenting efficiency and judicial fairness within legal systems. The pap
&lt;/p&gt;</description></item><item><title>AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05268</link><description>&lt;p&gt;
AUTOACT&#65306;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#23454;&#29616;&#30340;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05268
&lt;/p&gt;
&lt;p&gt;
AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#25506;&#32034;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#26114;&#36149;&#12289;&#19981;&#21487;&#37325;&#22797;&#30340;&#25968;&#25454;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#19988;&#38754;&#20020;&#23558;&#21333;&#19968;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21151;&#33021;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoAct&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#21512;&#25104;&#36712;&#36857;&#12290;&#32473;&#23450;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#24211;&#65292;AutoAct&#39318;&#20808;&#33258;&#21160;&#21512;&#25104;&#35268;&#21010;&#36712;&#36857;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#25110;&#24378;&#38381;&#28304;&#27169;&#22411;&#30340;&#20219;&#20309;&#36741;&#21161;&#12290;&#28982;&#21518;&#65292;AutoAct&#21033;&#29992;&#20998;&#24037;&#31574;&#30053;&#65292;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#21644;&#21512;&#25104;&#36712;&#36857;&#33258;&#21160;&#21306;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#23376;&#20195;&#29702;&#32452;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181;LLMs&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;AutoAct&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;&#20854;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36328;&#25991;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#32654;&#22269;Twitter&#29992;&#25143;&#30456;&#27604;&#65292;&#20013;&#22269;&#26032;&#28010;&#24494;&#21338;&#29992;&#25143;&#22312;&#24773;&#24863;&#24378;&#24230;&#30340;&#21464;&#21270;&#21644;&#28608;&#21160;&#31243;&#24230;&#19978;&#26377;&#26356;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.05254</link><description>&lt;p&gt;
&#20013;&#32654;&#20004;&#22269;&#20043;&#38388;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#34920;&#36798;&#30340;&#20215;&#20540;&#21644;&#28608;&#21160;&#23545;&#27604;&#65306;&#19968;&#20010;&#36328;&#25991;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination. (arXiv:2401.05254v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36328;&#25991;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24773;&#24863;&#34920;&#36798;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#32654;&#22269;Twitter&#29992;&#25143;&#30456;&#27604;&#65292;&#20013;&#22269;&#26032;&#28010;&#24494;&#21338;&#29992;&#25143;&#22312;&#24773;&#24863;&#24378;&#24230;&#30340;&#21464;&#21270;&#21644;&#28608;&#21160;&#31243;&#24230;&#19978;&#26377;&#26356;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#19978;&#20010;&#20307;&#30340;&#24773;&#24863;&#34920;&#36798;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35199;&#26041;&#29615;&#22659;&#20013;&#12290;&#19981;&#21516;&#25991;&#21270;&#20043;&#38388;&#23384;&#22312;&#30528;&#24341;&#21457;&#24773;&#24863;&#34920;&#36798;&#30340;&#37325;&#35201;&#24046;&#24322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32654;&#22269;Twitter&#21644;&#20013;&#22269;&#26032;&#28010;&#24494;&#21338;&#19978;&#30340;&#20004;&#20010;&#20027;&#35201;&#24773;&#24863;&#32500;&#24230;&#65288;&#20215;&#20540;&#21644;&#28608;&#21160;&#65289;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#32654;&#22269;&#21644;&#20013;&#22269;&#20010;&#20307;&#20043;&#38388;&#30340;&#28608;&#21160;&#21644;&#20215;&#20540;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#24046;&#24322;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20851;&#20869;&#23481;&#19978;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#20004;&#20010;&#24179;&#21488;&#19978;&#30340;&#35789;&#35821;&#20351;&#29992;&#21644;&#35805;&#39064;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#65292;&#20197;&#35299;&#35835;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;Twitter&#29992;&#25143;&#26469;&#35828;&#65292;&#36127;&#38754;&#24773;&#32490;&#21644;&#27491;&#38754;&#24773;&#32490;&#20043;&#38388;&#30340;&#24773;&#24863;&#24378;&#24230;&#21464;&#21270;&#19981;&#22826;&#26126;&#26174;&#65292;&#32780;&#23545;&#20110;&#26032;&#28010;&#24494;&#21338;&#29992;&#25143;&#26469;&#35828;&#65292;&#20276;&#38543;&#30528;&#24773;&#24863;&#30340;&#19978;&#21319;&#65292;&#28608;&#21160;&#31243;&#24230;&#26377;&#26356;&#26126;&#26174;&#30340;&#21319;&#32423;&#12290;&#20174;&#35821;&#35328;&#29305;&#24449;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although affective expressions of individuals have been extensively studied using social media, research has primarily focused on the Western context. There are substantial differences among cultures that contribute to their affective expressions. This paper examines the differences between Twitter (X) in the United States and Sina Weibo posts in China on two primary dimensions of affect - valence and arousal. We study the difference in the functional relationship between arousal and valence (so-called V-shaped) among individuals in the US and China and explore the associated content differences. Furthermore, we correlate word usage and topics in both platforms to interpret their differences. We observe that for Twitter users, the variation in emotional intensity is less distinct between negative and positive emotions compared to Weibo users, and there is a sharper escalation in arousal corresponding with heightened emotions. From language features, we discover that affective expressio
&lt;/p&gt;</description></item><item><title>CASA&#26159;&#19968;&#20010;&#22240;&#26524;&#39537;&#21160;&#30340;&#35770;&#35777;&#20805;&#20998;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#21069;&#25552;&#21644;&#32467;&#35770;&#19981;&#19968;&#33268;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#27880;&#20837;&#21069;&#25552;&#20107;&#20214;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#19981;&#36275;&#30340;&#35770;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.05249</link><description>&lt;p&gt;
CASA: &#22240;&#26524;&#39537;&#21160;&#30340;&#35770;&#35777;&#20805;&#20998;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CASA: Causality-driven Argument Sufficiency Assessment. (arXiv:2401.05249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05249
&lt;/p&gt;
&lt;p&gt;
CASA&#26159;&#19968;&#20010;&#22240;&#26524;&#39537;&#21160;&#30340;&#35770;&#35777;&#20805;&#20998;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#21069;&#25552;&#21644;&#32467;&#35770;&#19981;&#19968;&#33268;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#27880;&#20837;&#21069;&#25552;&#20107;&#20214;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#19981;&#36275;&#30340;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#20805;&#20998;&#24615;&#35780;&#20272;&#20219;&#21153;&#26088;&#22312;&#30830;&#23450;&#19968;&#20010;&#32473;&#23450;&#35770;&#35777;&#30340;&#21069;&#25552;&#26159;&#21542;&#25903;&#25345;&#20854;&#32467;&#35770;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23545;&#20154;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#22120;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#25968;&#25454;&#26159;&#36153;&#21147;&#30340;&#65292;&#32780;&#19988;&#30001;&#20110;&#20027;&#35266;&#26631;&#20934;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#26631;&#27880;&#24448;&#24448;&#20063;&#19981;&#19968;&#33268;&#12290;&#21463;&#22240;&#26524;&#25991;&#29486;&#20013;&#30340;&#20805;&#20998;&#27010;&#29575;&#65288;PS&#65289;&#23450;&#20041;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CASA&#65292;&#19968;&#20010;&#38646;&#23556;&#22240;&#26524;&#39537;&#21160;&#30340;&#35770;&#35777;&#20805;&#20998;&#24615;&#35780;&#20272;&#26694;&#26550;&#12290;PS&#34913;&#37327;&#30340;&#26159;&#24403;&#21069;&#25552;&#20107;&#20214;&#21644;&#32467;&#35770;&#20107;&#20214;&#37117;&#19981;&#23384;&#22312;&#26102;&#65292;&#24341;&#20837;&#21069;&#25552;&#20107;&#20214;&#26159;&#21542;&#20250;&#23548;&#33268;&#32467;&#35770;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#20272;&#35745;&#36825;&#20010;&#27010;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19982;&#21069;&#25552;&#21644;&#32467;&#35770;&#19981;&#19968;&#33268;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#27880;&#20837;&#21069;&#25552;&#20107;&#20214;&#23545;&#23427;&#20204;&#36827;&#34892;&#20462;&#25913;&#12290;&#22312;&#20004;&#20010;&#36923;&#36753;&#35884;&#35823;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CASA&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#19981;&#36275;&#30340;&#35770;&#35777;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;CASA&#37096;&#32626;&#22312;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion. To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the probability of sufficiency (PS) definition in the causal literature, we propose CASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion, when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion, and revise them by injecting the premise event. Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21305;&#37197;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2401.05224</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#26159;&#21542;&#20197;&#30456;&#20284;&#26041;&#24335;&#34920;&#31034;&#19990;&#30028;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Vision and Language Encoders Represent the World Similarly?. (arXiv:2401.05224v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05224
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21305;&#37197;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25104;&#20026;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20107;&#23454;&#19978;&#30340;&#27169;&#22411;&#30340;&#23545;&#40784;&#30340;&#25991;&#26412;-&#22270;&#20687;&#32534;&#30721;&#22120;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#27169;&#24577;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;&#22312;&#21508;&#33258;&#39046;&#22495;&#20013;&#20063;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#30001;&#20110;&#23427;&#20204;&#22522;&#26412;&#19978;&#34920;&#31034;&#21516;&#19968;&#20010;&#29289;&#29702;&#19990;&#30028;&#65292;&#21333;&#27169;&#24577;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#23545;&#40784;&#65311;&#36890;&#36807;&#20351;&#29992;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#20998;&#26512;&#22270;&#20687;-&#26631;&#39064;&#22522;&#20934;&#19978;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#22312;&#20687;CLIP&#36825;&#26679;&#30340;&#23545;&#40784;&#32534;&#30721;&#22120;&#20013;&#32570;&#20047;&#32479;&#35745;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26174;&#31034;&#20102;&#21487;&#33021;&#23384;&#22312;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#30340;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#21033;&#29992;&#22270;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26377;&#31181;&#23376;&#22270;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861; - &#24555;&#36895;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#20248;&#21270;&#21644;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#23616;&#37096;CKA&#24230;&#37327;&#30340;&#21305;&#37197;/&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#36866;&#24212;&#21644;&#30417;&#30563;&#24494;&#35843;&#25216;&#26415;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#19979;&#65292;&#20063;&#33021;&#26174;&#33879;&#25552;&#21319;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05215</link><description>&lt;p&gt;
&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#30340;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Large Language Models for Financial Sentiment Analysis. (arXiv:2401.05215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#36866;&#24212;&#21644;&#30417;&#30563;&#24494;&#35843;&#25216;&#26415;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#19979;&#65292;&#20063;&#33021;&#26174;&#33879;&#25552;&#21319;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#26159;&#23558;&#37329;&#34701;&#25991;&#26412;&#20869;&#23481;&#20998;&#31867;&#20026;&#24773;&#32490;&#31867;&#21035;&#65288;&#22914;&#31215;&#26497;&#12289;&#28040;&#26497;&#21644;&#20013;&#24615;&#65289;&#12290;&#26412;&#25991;&#20851;&#27880;&#37329;&#34701;&#26032;&#38395;&#26631;&#39064;&#30340;&#20998;&#31867;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#32570;&#20047;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;[1, 2, 3] &#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;LLMs&#26159;&#20174;&#22823;&#37327;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#35757;&#32451;&#24471;&#21040;&#30340;&#65292;&#20855;&#26377;&#25991;&#26412;&#29702;&#35299;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38656;&#35201;&#24456;&#23569;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26377;&#25928;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#24320;&#28304;&#30340;Llama2-7B&#27169;&#22411;&#65288;2023&#24180;&#65289;&#21644;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#25216;&#26415;[4]&#12290;&#23454;&#39564;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;LLMs&#26469;&#35828;&#36739;&#23567;&#30340;7B&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial sentiment analysis refers to classifying financial text contents into sentiment categories (e.g. positive, negative, and neutral). In this paper, we focus on the classification of financial news title, which is a challenging task due to a lack of large amount of training samples. To overcome this difficulty, we propose to adapt the pretrained large language models (LLMs) [1, 2, 3] to solve this problem. The LLMs, which are trained from huge amount of text corpora,have an advantage in text understanding and can be effectively adapted to domain-specific task while requiring very few amount of training samples. In particular, we adapt the open-source Llama2-7B model (2023) with the supervised fine-tuning (SFT) technique [4]. Experimental evaluation shows that even with the 7B model (which is relatively small for LLMs), our approach significantly outperforms the previous state-of-the-art algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#26223;&#29305;&#23450;&#27010;&#24565;&#32435;&#20837;&#21040;&#35805;&#35821;&#29983;&#25104;&#22120;&#20013;&#65292;&#25552;&#39640;&#20102;&#26631;&#31614;&#35789;&#31354;&#38388;&#30340;&#35206;&#30422;&#24230;&#21644;&#20943;&#23567;&#20102;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.05204</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65306;&#23558;&#24773;&#26223;&#29305;&#23450;&#27010;&#24565;&#32435;&#20837;&#21040;&#35805;&#35821;&#29983;&#25104;&#22120;&#20013;
&lt;/p&gt;
&lt;p&gt;
A Novel Prompt-tuning Method: Incorporating Scenario-specific Concepts into a Verbalizer. (arXiv:2401.05204v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#26223;&#29305;&#23450;&#27010;&#24565;&#32435;&#20837;&#21040;&#35805;&#35821;&#29983;&#25104;&#22120;&#20013;&#65292;&#25552;&#39640;&#20102;&#26631;&#31614;&#35789;&#31354;&#38388;&#30340;&#35206;&#30422;&#24230;&#21644;&#20943;&#23567;&#20102;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35805;&#35821;&#29983;&#25104;&#22120;&#26159;&#25552;&#31034;&#35843;&#25972;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#29992;&#20110;&#23558;&#26631;&#31614;&#35789;&#26144;&#23556;&#21040;&#31867;&#21035;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#35805;&#35821;&#29983;&#25104;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#31867;&#21035;&#21517;&#31216;&#30340;&#21516;&#20041;&#35789;&#25110;&#30456;&#20851;&#35789;&#38598;&#36827;&#34892;&#22686;&#24378;&#21644;&#31934;&#28860;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#29305;&#23450;&#20219;&#21153;&#22330;&#26223;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#27010;&#24565;&#20316;&#20026;&#26631;&#31614;&#35789;&#20505;&#36873;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32423;&#32852;&#26657;&#20934;&#27169;&#22359;&#26469;&#23558;&#20505;&#36873;&#35789;&#31934;&#28860;&#20026;&#27599;&#20010;&#31867;&#21035;&#30340;&#19968;&#32452;&#26631;&#31614;&#35789;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#26631;&#31614;&#35789;&#31354;&#38388;&#20013;&#35206;&#30422;&#24230;&#26377;&#38480;&#21644;&#20559;&#35265;&#36739;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The verbalizer, which serves to map label words to class labels, is an essential component of prompt-tuning. In this paper, we present a novel approach to constructing verbalizers. While existing methods for verbalizer construction mainly rely on augmenting and refining sets of synonyms or related words based on class names, this paradigm suffers from a narrow perspective and lack of abstraction, resulting in limited coverage and high bias in the label-word space. To address this issue, we propose a label-word construction process that incorporates scenario-specific concepts. Specifically, we extract rich concepts from task-specific scenarios as label-word candidates and then develop a novel cascade calibration module to refine the candidates into a set of label words for each class. We evaluate the effectiveness of our proposed approach through extensive experiments on {five} widely used datasets for zero-shot text classification. The results demonstrate that our method outperforms ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-2&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#39135;&#35889;&#65292;&#36890;&#36807;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#23545;&#25991;&#26412;&#29983;&#25104;&#36827;&#34892;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#39135;&#35889;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.05199</link><description>&lt;p&gt;
&#20351;&#29992;GPT-2&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#36827;&#34892;&#39135;&#35889;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Tree Search for Recipe Generation using GPT-2. (arXiv:2401.05199v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-2&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#39135;&#35889;&#65292;&#36890;&#36807;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#23545;&#25991;&#26412;&#29983;&#25104;&#36827;&#34892;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#39135;&#35889;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39135;&#35889;&#29983;&#25104;&#26041;&#27861;&#20026;&#21416;&#24072;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#36896;&#24615;&#24037;&#20855;&#65292;&#21487;&#20197;&#25506;&#32034;&#21644;&#21019;&#36896;&#26032;&#30340;&#26377;&#36259;&#30340;&#28921;&#39274;&#32654;&#39135;&#12290;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#23427;&#20204;&#26377;&#28508;&#21147;&#21019;&#36896;&#20986;&#21487;&#20197;&#28385;&#36275;&#20010;&#20154;&#20559;&#22909;&#12289;&#33203;&#39135;&#38480;&#21046;&#20197;&#21450;&#36866;&#24212;&#24744;&#20912;&#31665;&#20869;&#39135;&#26448;&#30340;&#26032;&#39135;&#35889;&#12290;&#29616;&#26377;&#30340;&#36890;&#36807;LLMs&#29983;&#25104;&#39135;&#35889;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#26469;&#29983;&#25104;&#21548;&#36215;&#26469;&#30495;&#23454;&#30340;&#39135;&#35889;&#12290;&#28982;&#32780;&#65292;&#20180;&#32454;&#26816;&#26597;&#21518;&#21457;&#29616;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#39135;&#35889;&#24448;&#24448;&#26080;&#27861;&#28385;&#36275;&#22522;&#26412;&#35201;&#27714;&#65292;&#27604;&#22914;&#22312;&#40481;&#32905;&#33756;&#32948;&#20013;&#21253;&#21547;&#40481;&#32905;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RecipeMC&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;GPT-2&#24182;&#20381;&#36182;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#12290;RecipeMC&#20801;&#35768;&#25105;&#20204;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#20197;&#23545;&#25991;&#26412;&#29983;&#25104;&#36827;&#34892;&#36719;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#39135;&#35889;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#26356;&#21916;&#27426;&#20351;&#29992;RecipeMC&#29983;&#25104;&#30340;&#39135;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic food recipe generation methods provide a creative tool for chefs to explore and to create new, and interesting culinary delights. Given the recent success of large language models (LLMs), they have the potential to create new recipes that can meet individual preferences, dietary constraints, and adapt to what is in your refrigerator. Existing research on using LLMs to generate recipes has shown that LLMs can be finetuned to generate realistic-sounding recipes. However, on close examination, these generated recipes often fail to meet basic requirements like including chicken as an ingredient in chicken dishes. In this paper, we propose RecipeMC, a text generation method using GPT-2 that relies on Monte Carlo Tree Search (MCTS). RecipeMC allows us to define reward functions to put soft constraints on text generation and thus improve the credibility of the generated recipes. Our results show that human evaluators prefer recipes generated with RecipeMC more often than recipes gen
&lt;/p&gt;</description></item><item><title>&#20998;&#27835;&#27714;&#35299;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#20013;&#65292;&#36890;&#36807;&#26681;&#25454;&#32479;&#35745;&#32622;&#20449;&#24230;&#20998;&#25968;&#23558;&#38382;&#39064;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#38598;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#21644;&#31579;&#36873;&#36873;&#39033;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05190</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#27835;&#27714;&#35299;&#26041;&#27861;&#22312;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Divide and Conquer for Large Language Models Reasoning. (arXiv:2401.05190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05190
&lt;/p&gt;
&lt;p&gt;
&#20998;&#27835;&#27714;&#35299;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#20013;&#65292;&#36890;&#36807;&#26681;&#25454;&#32479;&#35745;&#32622;&#20449;&#24230;&#20998;&#25968;&#23558;&#38382;&#39064;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#38598;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#21644;&#31579;&#36873;&#36873;&#39033;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Chain-of-Thought&#65288;CoT&#65289;&#21450;&#20854;&#34893;&#29983;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;MCQs&#65289;&#30340;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#24037;&#20316;&#37117;&#26159;&#32479;&#19968;&#22788;&#29702;&#25968;&#25454;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#38382;&#39064;&#35299;&#20915;&#30340;&#38590;&#24230;&#65292;&#36825;&#24847;&#21619;&#30528;&#36807;&#20998;&#20851;&#27880;&#31616;&#21333;&#38382;&#39064;&#65292;&#32780;&#23545;&#22797;&#26434;&#38382;&#39064;&#19981;&#22815;&#37325;&#35270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#21463;&#21040;&#20154;&#31867;&#20351;&#29992;&#21551;&#21457;&#24335;&#31574;&#30053;&#23545;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#24182;&#21333;&#29420;&#22788;&#29702;&#30340;&#21551;&#21457;&#65292;&#25552;&#35758;&#23558;&#20998;&#27835;&#26041;&#27861;&#24212;&#29992;&#20110;LLMs&#25512;&#29702;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26681;&#25454;&#32479;&#35745;&#32622;&#20449;&#24230;&#20998;&#25968;&#65288;$\mathcal{CS}$&#65289;&#23558;&#38382;&#39064;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#38598;&#65292;&#28982;&#21518;&#22266;&#23450;&#35299;&#20915;&#30340;&#23376;&#38598;&#65292;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#30340;&#32439;&#32321;&#38382;&#39064;&#65292;&#21253;&#25324;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#25512;&#29702;&#65288;PKR&#65289;&#21644;&#22522;&#20110;&#31579;&#36873;&#36873;&#39033;&#30340;&#25512;&#29702;&#65288;FCR&#65289;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#38598;&#25104;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#25552;&#20986;&#30340;&#20998;&#27835;&#27714;&#35299;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#38590;&#24230;&#30340;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive performance in various reasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its derivative methods, particularly in tasks involving multi-choice questions (MCQs). However, current works all process data uniformly without considering the problem-solving difficulty, which means an excessive focus on simple questions while insufficient to intricate ones. To address this challenge, we inspired by humans using heuristic strategies to categorize tasks and handle them individually, propose to apply the Divide and Conquer to LLMs reasoning. First, we divide questions into different subsets based on the statistical confidence score ($\mathcal{CS}$), then fix nearly resolved sets and conquer demanding nuanced process ones with elaborately designed methods, including Prior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR), as well as their integration variants. Our experiments demonstrate that this proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.05176</link><description>&lt;p&gt;
&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#32763;&#35793;&#20013;&#30340;&#31454;&#20105;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Rival Neural Machine Translation? A Comparative Study. (arXiv:2401.05176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20027;&#27969;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#22522;&#20110;&#38169;&#35823;&#31867;&#22411;&#21644;&#20845;&#20010;&#20998;&#26512;&#32454;&#21017;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#32771;&#23519;&#20102;ChatGPT&#21644;NMT&#24341;&#25806;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#23545;&#20110;ChatGPT&#22312;&#19981;&#21516;&#25552;&#31034;&#21644;NMT&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#24471;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#32780;&#24403;ChatGPT&#25552;&#20379;&#31034;&#20363;&#25110;&#32763;&#35793;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26102;&#65292;&#20154;&#24037;&#35780;&#20272;&#32773;&#24448;&#24448;&#20250;&#32473;&#20104;&#26126;&#26174;&#36739;&#39640;&#30340;&#35780;&#20998;&#12290;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#32500;&#24230;&#20043;&#38388;&#30340;&#20004;&#20004;&#30456;&#20851;&#24615;&#32467;&#26524;&#36739;&#24369;&#19988;&#19981;&#26174;&#33879;&#65292;&#36825;&#34920;&#26126;&#20102;&#20004;&#31181;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the increasing interest in leveraging large language models for translation, this paper evaluates the capabilities of large language models (LLMs) represented by ChatGPT in comparison to the mainstream neural machine translation (NMT) engines in translating Chinese diplomatic texts into English. Specifically, we examine the translation quality of ChatGPT and NMT engines as measured by four automated metrics and human evaluation based on an error-typology and six analytic rubrics. Our findings show that automated metrics yield similar results for ChatGPT under different prompts and NMT systems, while human annotators tend to assign noticeably higher scores to ChatGPT when it is provided an example or contextual information about the translation task. Pairwise correlation between automated metrics and dimensions of human evaluation produces weak and non-significant results, suggesting the divergence between the two methods of translation quality assessment. These findings pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65306;&#22810;&#27169;&#24577;&#21307;&#30103;&#20851;&#27880;&#25688;&#35201;&#29983;&#25104;&#65292;&#36890;&#36807;&#32467;&#21512;&#24739;&#32773;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#21644;&#20010;&#20154;&#20449;&#24687;&#65292;&#29983;&#25104;&#31616;&#30701;&#31934;&#30830;&#30340;&#21672;&#35810;&#20851;&#27880;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.05134</link><description>&lt;p&gt;
&#26159;&#30340;&#65292;&#36825;&#23601;&#26159;&#25105;&#24819;&#35201;&#30340;&#65281;&#21521;&#22810;&#27169;&#24577;&#21307;&#30103;&#21672;&#35810;&#20851;&#27880;&#25688;&#35201;&#29983;&#25104;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Yes, this is what I was looking for! Towards Multi-modal Medical Consultation Concern Summary Generation. (arXiv:2401.05134v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65306;&#22810;&#27169;&#24577;&#21307;&#30103;&#20851;&#27880;&#25688;&#35201;&#29983;&#25104;&#65292;&#36890;&#36807;&#32467;&#21512;&#24739;&#32773;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#21644;&#20010;&#20154;&#20449;&#24687;&#65292;&#29983;&#25104;&#31616;&#30701;&#31934;&#30830;&#30340;&#21672;&#35810;&#20851;&#27880;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20114;&#32852;&#32593;&#22312;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#22686;&#38271;&#36805;&#29467;&#65292;&#26377;&#25928;&#31649;&#29702;&#21644;&#22788;&#29702;&#20449;&#24687;&#20197;&#30830;&#20445;&#20854;&#39640;&#25928;&#21033;&#29992;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#24773;&#32490;&#22256;&#25200;&#21644;&#24515;&#29702;&#25361;&#25112;&#26102;&#21051;&#65292;&#25105;&#20204;&#32463;&#24120;&#36716;&#21521;&#20114;&#32852;&#32593;&#20316;&#20026;&#25105;&#20204;&#26368;&#21021;&#30340;&#25903;&#25345;&#28304;&#65292;&#36873;&#25321;&#23427;&#32780;&#19981;&#26159;&#19982;&#20182;&#20154;&#35752;&#35770;&#25105;&#20204;&#30340;&#24863;&#21463;&#65292;&#22240;&#20026;&#36825;&#28041;&#21450;&#31038;&#20250;&#30340;&#27745;&#21517;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#20851;&#27880;&#25688;&#35201;&#29983;&#25104;&#65288;MMCS&#65289;&#20219;&#21153;&#65292;&#23427;&#25552;&#20379;&#20102;&#20851;&#20110;&#24739;&#32773;&#22312;&#21672;&#35810;&#36807;&#31243;&#20013;&#25552;&#20986;&#30340;&#20027;&#35201;&#20851;&#27880;&#30340;&#31616;&#30701;&#21644;&#31934;&#30830;&#25688;&#35201;&#12290;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#20363;&#22914;&#24739;&#32773;&#30340;&#25163;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#65292;&#26377;&#21161;&#20110;&#20934;&#30830;&#35782;&#21035;&#24739;&#32773;&#30340;&#20851;&#27880;&#28857;&#12290;&#21307;&#29983;&#36824;&#32771;&#34385;&#24739;&#32773;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#20363;&#22914;&#24180;&#40836;&#21644;&#24615;&#21035;&#65292;&#20197;&#20415;&#36866;&#24403;&#22320;&#25551;&#36848;&#21307;&#30103;&#29366;&#20917;&#12290;&#21463;&#24739;&#32773;&#20010;&#20154;&#19978;&#19979;&#25991;&#21644;&#35270;&#35273;&#25163;&#21183;&#30340;&#28508;&#22312;&#30103;&#25928;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, the use of the Internet for healthcare-related tasks has grown by leaps and bounds, posing a challenge in effectively managing and processing information to ensure its efficient utilization. During moments of emotional turmoil and psychological challenges, we frequently turn to the internet as our initial source of support, choosing this over discussing our feelings with others due to the associated social stigma. In this paper, we propose a new task of multi-modal medical concern summary (MMCS) generation, which provides a short and precise summary of patients' major concerns brought up during the consultation. Nonverbal cues, such as patients' gestures and facial expressions, aid in accurately identifying patients' concerns. Doctors also consider patients' personal information, such as age and gender, in order to describe the medical condition appropriately. Motivated by the potential efficacy of patients' personal context and visual gestures, we propose a tr
&lt;/p&gt;</description></item><item><title>BELHD&#26159;&#19968;&#31181;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21516;&#38899;&#24322;&#20041;&#35789;&#28040;&#27495;&#26469;&#22788;&#29702;&#21516;&#38899;&#24322;&#20041;&#35789;&#23545;&#30693;&#35782;&#24211;&#20013;&#23454;&#20307;&#38142;&#25509;&#30340;&#24433;&#21709;&#65292;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05125</link><description>&lt;p&gt;
BELHD: &#20351;&#29992;&#21516;&#38899;&#24322;&#20041;&#35789;&#28040;&#27495;&#26469;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
BELHD: Improving Biomedical Entity Linking with Homonoym Disambiguation. (arXiv:2401.05125v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05125
&lt;/p&gt;
&lt;p&gt;
BELHD&#26159;&#19968;&#31181;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21516;&#38899;&#24322;&#20041;&#35789;&#28040;&#27495;&#26469;&#22788;&#29702;&#21516;&#38899;&#24322;&#20041;&#35789;&#23545;&#30693;&#35782;&#24211;&#20013;&#23454;&#20307;&#38142;&#25509;&#30340;&#24433;&#21709;&#65292;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#26159;&#23558;&#23454;&#20307;&#25552;&#21450;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#20851;&#32852;&#30340;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#22522;&#20110;&#21517;&#31216;&#30340;&#26041;&#27861;&#65292;&#21363;&#20026;&#32473;&#23450;&#30340;&#25552;&#21450;&#35782;&#21035;&#20986;&#30693;&#35782;&#24211;&#20013;&#26368;&#21512;&#36866;&#30340;&#21517;&#31216;&#65292;&#21487;&#20197;&#36890;&#36807;&#23494;&#38598;&#26816;&#32034;&#25110;&#33258;&#22238;&#24402;&#24314;&#27169;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26041;&#27861;&#30452;&#25509;&#36820;&#22238;&#30693;&#35782;&#24211;&#20013;&#30340;&#21517;&#31216;&#65292;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#21516;&#38899;&#24322;&#20041;&#35789;&#65292;&#21363;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#23454;&#20307;&#20849;&#20139;&#30456;&#21516;&#30340;&#21517;&#31216;&#12290;&#36825;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24403;&#21516;&#38899;&#24322;&#20041;&#35789;&#21344;&#23454;&#20307;&#25552;&#21450;&#30340;&#24456;&#22823;&#27604;&#20363;&#26102;&#65288;&#22914;UMLS&#21644;NCBI Gene&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BELHD&#65288;&#20855;&#26377;&#21516;&#38899;&#24322;&#20041;&#35789;&#28040;&#27495;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#65289;&#65292;&#19968;&#31181;&#33021;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#26032;&#22411;&#22522;&#20110;&#21517;&#31216;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BELHD&#22312;BioSyn&#65288;Sung&#31561;&#20154;&#65292;2020&#65289;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#25193;&#23637;&#12290;&#39318;&#20808;&#65292;&#23427;&#23545;&#30693;&#35782;&#24211;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#22312;&#20854;&#20013;&#20351;&#29992;&#33258;&#21160;&#36873;&#25321;&#30340;&#28040;&#27495;&#23383;&#31526;&#20018;&#26469;&#25193;&#23637;&#21516;&#38899;&#24322;&#20041;&#35789;&#65292;&#20174;&#32780;&#24378;&#21046;&#36827;&#34892;&#21807;&#19968;&#30340;&#38142;&#25509;&#20915;&#31574;&#12290;&#20854;&#27425;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Biomedical entity linking (BEL) is the task of grounding entity mentions to a knowledge base (KB). A popular approach to the task are name-based methods, i.e. those identifying the most appropriate name in the KB for a given mention, either via dense retrieval or autoregressive modeling. However, as these methods directly return KB names, they cannot cope with homonyms, i.e. different KB entities sharing the exact same name. This significantly affects their performance, especially for KBs where homonyms account for a large amount of entity mentions (e.g. UMLS and NCBI Gene). We therefore present BELHD (Biomedical Entity Linking with Homonym Disambiguation), a new name-based method that copes with this challenge. Specifically, BELHD builds upon the BioSyn (Sung et al.,2020) model introducing two crucial extensions. First, it performs a preprocessing of the KB in which it expands homonyms with an automatically chosen disambiguating string, thus enforcing unique linking decisions. Second,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25239;&#22122;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#24341;&#20837;&#36866;&#37197;&#22120;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#21442;&#32771;&#35821;&#38899;&#23545;TTS&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21450;&#37319;&#29992;&#35821;&#38899;&#22686;&#24378;&#21069;&#31471;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#65292;&#23545;&#21442;&#32771;&#35821;&#38899;&#20013;&#30340;&#22122;&#22768;&#20855;&#26377;&#39640;&#24230;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05111</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#24449;&#27169;&#22411;&#30340;&#25239;&#22122;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Noise-robust zero-shot text-to-speech synthesis conditioned on self-supervised speech-representation model with adapters. (arXiv:2401.05111v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25239;&#22122;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#24341;&#20837;&#36866;&#37197;&#22120;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#21442;&#32771;&#35821;&#38899;&#23545;TTS&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21450;&#37319;&#29992;&#35821;&#38899;&#22686;&#24378;&#21069;&#31471;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#65292;&#23545;&#21442;&#32771;&#35821;&#38899;&#20013;&#30340;&#22122;&#22768;&#20855;&#26377;&#39640;&#24230;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35821;&#38899;&#34920;&#24449;&#25552;&#21462;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#21521;&#37327;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#27861;&#21487;&#20197;&#38750;&#24120;&#20934;&#30830;&#22320;&#22797;&#21046;&#35828;&#35805;&#20154;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#24403;&#21442;&#32771;&#35821;&#38899;&#20013;&#21547;&#26377;&#22122;&#22768;&#26102;&#65292;&#35813;&#26041;&#27861;&#30340;&#35821;&#38899;&#21512;&#25104;&#36136;&#37327;&#20250;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25239;&#22122;&#38646;&#26679;&#26412;TTS&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36866;&#37197;&#22120;&#24341;&#20837;&#21040;SSL&#27169;&#22411;&#20013;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#21442;&#32771;&#35821;&#38899;&#30340;TTS&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#21069;&#31471;&#12290;&#36890;&#36807;&#36825;&#20123;&#25913;&#36827;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;SSL&#30340;&#38646;&#26679;&#26412;TTS&#22312;&#22122;&#22768;&#21442;&#32771;&#35821;&#38899;&#19979;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#35813;&#26041;&#27861;&#23545;&#21442;&#32771;&#35821;&#38899;&#20013;&#30340;&#22122;&#22768;&#20855;&#26377;&#39640;&#24230;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#19982;SE&#32467;&#21512;&#26377;&#25928;&#22320;&#36816;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The zero-shot text-to-speech (TTS) method, based on speaker embeddings extracted from reference speech using self-supervised learning (SSL) speech representations, can reproduce speaker characteristics very accurately. However, this approach suffers from degradation in speech synthesis quality when the reference speech contains noise. In this paper, we propose a noise-robust zero-shot TTS method. We incorporated adapters into the SSL model, which we fine-tuned with the TTS model using noisy reference speech. In addition, to further improve performance, we adopted a speech enhancement (SE) front-end. With these improvements, our proposed SSL-based zero-shot TTS achieved high-quality speech synthesis with noisy reference speech. Through the objective and subjective evaluations, we confirmed that the proposed method is highly robust to noise in reference speech, and effectively works in combination with SE.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#32844;&#20301;&#24191;&#21578;&#35201;&#27714;&#21644;&#27178;&#21521;&#25216;&#33021;&#38598;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#39044;&#27979;&#20010;&#21035;&#24037;&#20316;&#25551;&#36848;&#25152;&#38656;&#30340;&#25216;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#23618;&#27425;&#20998;&#31867;&#21644;&#22810;&#26631;&#31614;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#22686;&#24378;&#25216;&#26415;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#27431;&#27954;&#23601;&#19994;&#24066;&#22330;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05073</link><description>&lt;p&gt;
&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#32844;&#20301;&#24191;&#21578;&#20013;&#27178;&#21521;&#25216;&#33021;&#30340;&#23618;&#27425;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Classification of Transversal Skills in Job Ads Based on Sentence Embeddings. (arXiv:2401.05073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#32844;&#20301;&#24191;&#21578;&#35201;&#27714;&#21644;&#27178;&#21521;&#25216;&#33021;&#38598;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#39044;&#27979;&#20010;&#21035;&#24037;&#20316;&#25551;&#36848;&#25152;&#38656;&#30340;&#25216;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#23618;&#27425;&#20998;&#31867;&#21644;&#22810;&#26631;&#31614;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#22686;&#24378;&#25216;&#26415;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#27431;&#27954;&#23601;&#19994;&#24066;&#22330;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#32844;&#20301;&#24191;&#21578;&#35201;&#27714;&#21644;&#27178;&#21521;&#25216;&#33021;&#38598;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#37325;&#28857;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20010;&#21035;&#24037;&#20316;&#25551;&#36848;&#25152;&#38656;&#30340;&#25216;&#33021;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#20351;&#29992;ESCO&#65288;&#27431;&#27954;&#25216;&#33021;&#12289;&#33021;&#21147;&#21644;&#32844;&#19994;&#65289;&#20998;&#31867;&#31995;&#32479;&#36827;&#34892;&#26631;&#27880;&#12290;&#22312;&#25216;&#33021;&#35782;&#21035;&#26041;&#38754;&#65292;&#37319;&#29992;&#20102;&#23618;&#27425;&#20998;&#31867;&#21644;&#22810;&#26631;&#31614;&#31574;&#30053;&#65292;&#32780;&#22686;&#24378;&#25216;&#26415;&#21017;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#20351;&#29992;&#33521;&#35821;&#29305;&#23450;&#21644;&#22810;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#24471;&#21040;&#30340;&#32467;&#26524;&#65292;&#21457;&#29616;&#20854;&#20934;&#30830;&#24230;&#30456;&#36817;&#12290;&#23454;&#39564;&#26696;&#20363;&#30740;&#31350;&#35814;&#32454;&#35828;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#12289;&#36229;&#21442;&#25968;&#21644;&#20132;&#21449;&#39564;&#35777;&#32467;&#26524;&#65292;&#31361;&#26174;&#20102;&#23618;&#27425;&#21270;&#26041;&#27861;&#30340;&#21151;&#25928;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#36866;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#27431;&#27954;&#23601;&#19994;&#24066;&#22330;&#12290;&#22240;&#27492;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a classification framework aimed at identifying correlations between job ad requirements and transversal skill sets, with a focus on predicting the necessary skills for individual job descriptions using a deep learning model. The approach involves data collection, preprocessing, and labeling using ESCO (European Skills, Competences, and Occupations) taxonomy. Hierarchical classification and multi-label strategies are used for skill identification, while augmentation techniques address data imbalance, enhancing model robustness. A comparison between results obtained with English-specific and multi-language sentence embedding models reveals close accuracy. The experimental case studies detail neural network configurations, hyperparameters, and cross-validation results, highlighting the efficacy of the hierarchical approach and the suitability of the multi-language model for the diverse European job market. Thus, a new approach is proposed for the hierarchical classifi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#33021;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36328;&#35821;&#35328;&#35299;&#37322;&#22256;&#38590;&#35789;&#30340;&#26032;&#26041;&#27861;&#26469;&#23545;&#40784;&#32763;&#35793;&#29305;&#23450;&#29702;&#35299;&#21644;&#19968;&#33324;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.05072</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32763;&#35793;&#29305;&#23450;&#29702;&#35299;&#19982;&#19968;&#33324;&#29702;&#35299;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Translation-Specific Understanding to General Understanding in Large Language Models. (arXiv:2401.05072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05072
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#33021;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36328;&#35821;&#35328;&#35299;&#37322;&#22256;&#38590;&#35789;&#30340;&#26032;&#26041;&#27861;&#26469;&#23545;&#40784;&#32763;&#35793;&#29305;&#23450;&#29702;&#35299;&#21644;&#19968;&#33324;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#22312;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#23578;&#26410;&#21462;&#24471;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#36896;&#25104;&#24615;&#33021;&#26377;&#38480;&#30340;&#19968;&#20010;&#28508;&#22312;&#21407;&#22240;&#26159;LLMs&#20013;&#32763;&#35793;&#29305;&#23450;&#29702;&#35299;&#19982;&#19968;&#33324;&#29702;&#35299;&#30340;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#23558;&#32763;&#35793;&#29305;&#23450;&#29702;&#35299;&#19982;&#19968;&#33324;&#29702;&#35299;&#23545;&#40784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32763;&#35793;&#36807;&#31243;xIoD&#65288;&#36328;&#35821;&#35328;&#35299;&#37322;&#22256;&#38590;&#35789;&#65289;&#65292;&#26126;&#30830;&#22320;&#34701;&#20837;&#19968;&#33324;&#29702;&#35299;&#23545;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#29702;&#35299;&#20197;&#25351;&#23548;&#32763;&#35793;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;xIoD&#23545;&#38590;&#20197;&#32763;&#35793;&#30340;&#21333;&#35789;&#36827;&#34892;&#36328;&#35821;&#35328;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#30340;&#35299;&#37322;&#22686;&#24378;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37325;&#26032;&#26500;&#24314;&#20102;&#22806;&#37096;&#24037;&#20855;QE&#65292;&#20197;&#35299;&#20915;xIoD&#22312;&#26816;&#27979;&#22256;&#38590;&#35789;&#21644;&#29983;&#25104;&#26377;&#24110;&#21161;&#30340;&#35299;&#37322;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) have shown surprising language understanding and generation capabilities, they have yet to gain a revolutionary advancement in the field of machine translation. One potential cause of the limited performance is the misalignment between the translation-specific understanding and general understanding inside LLMs. To align the translation-specific understanding to the general one, we propose a novel translation process xIoD (Cross-Lingual Interpretation of Difficult words), explicitly incorporating the general understanding on the content incurring inconsistent understanding to guide the translation. Specifically, xIoD performs the cross-lingual interpretation for the difficult-to-translate words and enhances the translation with the generated interpretations. Furthermore, we reframe the external tools of QE to tackle the challenges of xIoD in the detection of difficult words and the generation of helpful interpretations. We conduct experiments on th
&lt;/p&gt;</description></item><item><title>MuTox&#26159;&#31532;&#19968;&#20010;&#39640;&#24230;&#22810;&#35821;&#35328;&#30340;&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35757;&#32451;&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#36328;&#22810;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#27602;&#24615;&#26816;&#27979;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#35206;&#30422;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#35789;&#27719;&#21015;&#34920;&#30340;&#20998;&#31867;&#22120;&#65292;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;&#32422;2.5&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.05060</link><description>&lt;p&gt;
MuTox: &#36890;&#29992;&#22810;&#35821;&#35328;&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#25968;&#25454;&#38598;&#21644;&#38646;&#26679;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot Detector. (arXiv:2401.05060v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05060
&lt;/p&gt;
&lt;p&gt;
MuTox&#26159;&#31532;&#19968;&#20010;&#39640;&#24230;&#22810;&#35821;&#35328;&#30340;&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35757;&#32451;&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#36328;&#22810;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#27602;&#24615;&#26816;&#27979;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#35206;&#30422;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#35789;&#27719;&#21015;&#34920;&#30340;&#20998;&#31867;&#22120;&#65292;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;&#32422;2.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#27169;&#24577;&#65288;&#22522;&#20110;&#38899;&#39057;&#65289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#27602;&#24615;&#26816;&#27979;&#30740;&#31350;&#30456;&#23545;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#33521;&#35821;&#35821;&#35328;&#32780;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#20026;&#30495;&#27491;&#22810;&#35821;&#35328;&#30340;&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#26816;&#27979;&#22880;&#23450;&#22522;&#30784;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MuTox&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#27602;&#24615;&#26631;&#31614;&#30340;&#39640;&#24230;&#22810;&#35821;&#35328;&#30340;&#22522;&#20110;&#38899;&#39057;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;20,000&#20010;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#38899;&#39057;&#29255;&#27573;&#65292;&#20197;&#21450;&#20854;&#20182;19&#31181;&#35821;&#35328;&#30340;4,000&#20010;&#29255;&#27573;&#12290;&#20026;&#20102;&#35777;&#26126;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;MuTox&#22522;&#20110;&#38899;&#39057;&#30340;&#27602;&#24615;&#20998;&#31867;&#22120;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#27602;&#24615;&#26816;&#27979;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25991;&#26412;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30456;&#27604;&#65292;&#35813;&#20998;&#31867;&#22120;&#30340;AUC&#24615;&#33021;&#25552;&#39640;&#20102;&#36229;&#36807;1%&#65292;&#21516;&#26102;&#25193;&#22823;&#20102;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#21313;&#20493;&#20197;&#19978;&#12290;&#19982;&#22522;&#20110;&#35789;&#27719;&#21015;&#34920;&#30340;&#20855;&#26377;&#30456;&#20284;&#35821;&#35328;&#35206;&#30422;&#25968;&#37327;&#30340;&#20998;&#31867;&#22120;&#30456;&#27604;&#65292;MuTox&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25552;&#39640;&#20102;&#32422;2.5&#20493;&#12290;&#36825;&#20010;&#26174;&#33879;&#30340;&#25913;&#36827;&#31361;&#26174;&#20102;&#20854;&#28508;&#22312;&#30340;&#21019;&#26032;&#24615;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in toxicity detection in natural language processing for the speech modality (audio-based) is quite limited, particularly for languages other than English. To address these limitations and lay the groundwork for truly multilingual audio-based toxicity detection, we introduce MuTox, the first highly multilingual audio-based dataset with toxicity labels. The dataset comprises 20,000 audio utterances for English and Spanish, and 4,000 for the other 19 languages. To demonstrate the quality of this dataset, we trained the MuTox audio-based toxicity classifier, which enables zero-shot toxicity detection across a wide range of languages. This classifier outperforms existing text-based trainable classifiers by more than 1% AUC, while expanding the language coverage more than tenfold. When compared to a wordlist-based classifier that covers a similar number of languages, MuTox improves precision and recall by approximately 2.5 times. This significant improvement underscores the potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#22810;&#26679;&#24615;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#21152;&#20837;&#22810;&#26679;&#24615;&#30446;&#26631;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2401.05054</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding. (arXiv:2401.05054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#22810;&#26679;&#24615;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#21152;&#20837;&#22810;&#26679;&#24615;&#30446;&#26631;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#20135;&#29983;&#19981;&#20165;&#27491;&#30830;&#32780;&#19988;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#12290;&#26368;&#36817;&#65292;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#22312;&#29983;&#25104;&#31639;&#27861;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#20135;&#29983;&#26368;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20026;&#29983;&#25104;&#22810;&#26679;&#21270;&#36755;&#20986;&#32780;&#25552;&#20986;&#30340;&#29616;&#26377;&#31639;&#27861;&#20027;&#35201;&#22522;&#20110;&#27874;&#26463;&#25628;&#32034;&#25110;&#38543;&#26426;&#25277;&#26679;&#65292;&#22240;&#27492;&#20854;&#36755;&#20986;&#36136;&#37327;&#21463;&#38480;&#20110;&#36825;&#20123;&#22522;&#26412;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;--&#36890;&#36807;&#23558;&#22810;&#26679;&#24615;&#30446;&#26631;&#24378;&#21152;&#21040;MBR&#35299;&#30721;&#20013;&#26469;&#24320;&#21457;&#20419;&#36827;&#22810;&#26679;&#24615;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;MBR&#30340;&#21464;&#20307;&#65292;&#21363;&#22810;&#26679;&#24615;MBR&#65288;DMBR&#65289;&#21644;k-medoids MBR&#65288;KMBR&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#19968;&#32452;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#21508;&#31181;&#23450;&#21521;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;DMBR&#21644;KMBR&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20256;&#32479;
&lt;/p&gt;
&lt;p&gt;
One of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse. Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed for generating diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying methods. In this paper, we investigate an alternative approach -- we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and $k$-medoids MBR (KMBR), methods to generate a set of sentences with high quality and diversity. We evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a large language model with prompting. The experimental results show that the proposed method achieves a better trad
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM&#30340;&#33258;&#25105;&#23545;&#35805;&#25910;&#38598;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#25105;&#23545;&#35805;&#24230;&#37327;&#26469;&#34913;&#37327;&#23545;&#35805;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#21487;&#20197;&#36873;&#25321;&#36136;&#37327;&#36739;&#39640;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.05033</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#23545;&#35805;&#24341;&#23548;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#30340;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk. (arXiv:2401.05033v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM&#30340;&#33258;&#25105;&#23545;&#35805;&#25910;&#38598;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#25105;&#23545;&#35805;&#24230;&#37327;&#26469;&#34913;&#37327;&#23545;&#35805;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#21487;&#20197;&#36873;&#25321;&#36136;&#37327;&#36739;&#39640;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#24378;&#22823;&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#20294;&#29305;&#21270;&#23427;&#20204;&#20197;&#23454;&#29616;&#29305;&#23450;&#21151;&#33021;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25351;&#31034;&#35843;&#35856;&#65292;&#21363;&#22312;&#20154;&#31867;&#29983;&#25104;&#30340;&#25351;&#20196;&#21644;&#31034;&#20363;&#21709;&#24212;&#19978;&#35843;&#35856;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#38656;&#35201;&#19968;&#23450;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#19981;&#21487;&#29992;&#25110;&#29983;&#25104;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#24403;&#30446;&#26631;&#26159;&#20351;LLM&#36981;&#24490;&#23545;&#35805;&#20013;&#30340;&#29305;&#23450;&#24037;&#20316;&#27969;&#31243;&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#20010;&#25351;&#20196;&#26102;&#65292;&#36825;&#31181;&#25104;&#26412;&#20250;&#22686;&#21152;&#12290;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#25105;&#21338;&#24328;&#25216;&#26415;&#21644;&#20351;&#29992;LLM&#27169;&#25311;&#20154;&#31867;&#20195;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#36890;&#36807;LLM&#25198;&#28436;&#19981;&#21516;&#35282;&#33394;&#36827;&#34892;&#23545;&#35805;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;LLM&#30340;&#8220;&#33258;&#25105;&#23545;&#35805;&#8221;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#36827;&#34892;&#31934;&#32454;&#35843;&#35856;&#21644;&#21033;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#23545;&#35805;&#30340;&#65288;&#37096;&#20998;&#65289;&#25104;&#21151;&#12290;&#35813;&#24230;&#37327;&#29992;&#20110;&#36807;&#28388;&#22522;&#20110;LLM&#30340;&#33258;&#25105;&#23545;&#35805;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#36873;&#25321;&#36136;&#37327;&#36739;&#39640;&#30340;&#26679;&#26412;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions. Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles. This approach generates a training data via "self-talk" of LLMs that can be refined and utilized for supervised fine-tuning. We introduce an automated way to measure the (partial) success of a dialogue. This metric is used to filter the ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23545;&#21516;&#24615;&#20851;&#31995;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#21457;&#29616;&#19977;&#20010;&#21463;&#27426;&#36814;&#30340;MT&#26381;&#21153;&#22312;&#20934;&#30830;&#32763;&#35793;&#28041;&#21450;&#21516;&#24615;&#21035;&#21517;&#35789;&#20043;&#38388;&#20851;&#31995;&#30340;&#21477;&#23376;&#26102;&#23384;&#22312;&#36739;&#22823;&#30340;&#38169;&#35823;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22899;&#24615;&#32844;&#19994;&#30340;&#19978;&#19979;&#25991;&#20013;&#34920;&#29616;&#26356;&#24046;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#35780;&#20272;NLP&#31995;&#32479;&#20013;&#22266;&#26377;&#20559;&#35265;&#25552;&#20379;&#20102;&#19968;&#20010;&#31038;&#20250;&#20851;&#31995;&#26041;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.04972</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#21516;&#24615;&#20851;&#31995;&#20559;&#35265;&#35780;&#20272;&#65306;&#23427;&#31350;&#31455;&#26159;&#35841;&#30340;&#22971;&#23376;&#65311;
&lt;/p&gt;
&lt;p&gt;
Whose wife is it anyway? Assessing bias against same-gender relationships in machine translation. (arXiv:2401.04972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23545;&#21516;&#24615;&#20851;&#31995;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#21457;&#29616;&#19977;&#20010;&#21463;&#27426;&#36814;&#30340;MT&#26381;&#21153;&#22312;&#20934;&#30830;&#32763;&#35793;&#28041;&#21450;&#21516;&#24615;&#21035;&#21517;&#35789;&#20043;&#38388;&#20851;&#31995;&#30340;&#21477;&#23376;&#26102;&#23384;&#22312;&#36739;&#22823;&#30340;&#38169;&#35823;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22899;&#24615;&#32844;&#19994;&#30340;&#19978;&#19979;&#25991;&#20013;&#34920;&#29616;&#26356;&#24046;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#35780;&#20272;NLP&#31995;&#32479;&#20013;&#22266;&#26377;&#20559;&#35265;&#25552;&#20379;&#20102;&#19968;&#20010;&#31038;&#20250;&#20851;&#31995;&#26041;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#32463;&#24120;&#21463;&#21040;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#21644;&#31639;&#27861;&#30340;&#22256;&#25200;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#36755;&#20986;&#20013;&#30340;&#19981;&#21487;&#25509;&#21463;&#30340;&#38169;&#35823;&#12290;&#34429;&#28982;&#23545;&#24615;&#21035;&#35268;&#33539;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#35843;&#26597;&#30740;&#31350;&#65292;&#20294;&#23545;MT&#31995;&#32479;&#26159;&#21542;&#23545;&#31038;&#20250;&#20851;&#31995;&#32534;&#30721;&#20559;&#35265;&#30340;&#24773;&#20917;&#20102;&#35299;&#36739;&#23569;&#65292;&#20363;&#22914;&#8220;&#24459;&#24072;&#21563;&#20102;&#22905;&#30340;&#22971;&#23376;&#8221;&#36825;&#26679;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#20960;&#31181;&#21517;&#35789;&#24615;&#21035;&#35821;&#35328;&#65288;&#20363;&#22914;&#35199;&#29677;&#29273;&#35821;&#65289;&#20013;&#25277;&#21462;&#30340;&#29983;&#25104;&#27169;&#26495;&#21477;&#23376;&#65292;&#35843;&#26597;MT&#31995;&#32479;&#38024;&#23545;&#21516;&#24615;&#20851;&#31995;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#19977;&#20010;&#21463;&#27426;&#36814;&#30340;MT&#26381;&#21153;&#22312;&#20934;&#30830;&#32763;&#35793;&#28041;&#21450;&#21516;&#24615;&#21035;&#21517;&#35789;&#20043;&#38388;&#20851;&#31995;&#30340;&#21477;&#23376;&#26102;&#19968;&#30452;&#23384;&#22312;&#38382;&#39064;&#12290;&#38169;&#35823;&#29575;&#26681;&#25454;&#19978;&#19979;&#25991;&#32780;&#21464;&#21270;&#24456;&#22823;&#65292;&#20363;&#22914;&#24341;&#29992;&#22899;&#24615;&#21344;&#27604;&#36739;&#39640;&#32844;&#19994;&#30340;&#21516;&#24615;&#21477;&#23376;&#30340;&#32763;&#35793;&#20934;&#30830;&#24230;&#36739;&#20302;&#12290;&#25105;&#20204;&#25552;&#20379;&#36825;&#39033;&#24037;&#20316;&#20316;&#20026;&#30740;&#31350;NLP&#31995;&#32479;&#20013;&#22266;&#26377;&#20559;&#35265;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#28041;&#21450;&#31038;&#20250;&#20851;&#31995;&#26041;&#38754;&#30340;&#20559;&#35265;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine translation often suffers from biased data and algorithms that can lead to unacceptable errors in system output. While bias in gender norms has been investigated, less is known about whether MT systems encode bias about social relationships, e.g. sentences such as "the lawyer kissed her wife." We investigate the degree of bias against same-gender relationships in MT systems, using generated template sentences drawn from several noun-gender languages (e.g. Spanish). We find that three popular MT services consistently fail to accurately translate sentences concerning relationships between nouns of the same gender. The error rate varies considerably based on the context, e.g. same-gender sentences referencing high female-representation occupations are translated with lower accuracy. We provide this work as a case study in the evaluation of intrinsic bias in NLP systems, with respect to social relationships.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#21463;&#22270;&#28789;&#27979;&#35797;&#21551;&#21457;&#30340;&#23454;&#35777;&#30740;&#31350;&#36136;&#30097;&#20102;&#20154;&#31867;&#21019;&#36896;&#21147;&#26080;&#27861;&#34987;&#26426;&#22120;&#27169;&#20223;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#20197;&#20960;&#20046;&#19982;&#20154;&#31867;&#26080;&#27861;&#21306;&#20998;&#30340;&#26041;&#24335;&#20889;&#20316;&#21476;&#20856;&#20013;&#25991;&#35799;&#27468;&#65292;&#24182;&#25581;&#31034;&#20102;&#24320;&#28304;&#30340;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;GPT-4&#12290;</title><link>http://arxiv.org/abs/2401.04952</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#20889;&#21476;&#20856;&#35799;&#21527;&#65311;&#19968;&#39033;&#21463;&#22270;&#28789;&#27979;&#35797;&#21551;&#21457;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can AI Write Classical Chinese Poetry like Humans? An Empirical Study Inspired by Turing Test. (arXiv:2401.04952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#21463;&#22270;&#28789;&#27979;&#35797;&#21551;&#21457;&#30340;&#23454;&#35777;&#30740;&#31350;&#36136;&#30097;&#20102;&#20154;&#31867;&#21019;&#36896;&#21147;&#26080;&#27861;&#34987;&#26426;&#22120;&#27169;&#20223;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#20197;&#20960;&#20046;&#19982;&#20154;&#31867;&#26080;&#27861;&#21306;&#20998;&#30340;&#26041;&#24335;&#20889;&#20316;&#21476;&#20856;&#20013;&#25991;&#35799;&#27468;&#65292;&#24182;&#25581;&#31034;&#20102;&#24320;&#28304;&#30340;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20154;&#35748;&#20026;&#65292;&#21019;&#36896;&#21147;&#21644;&#24773;&#24863;&#31561;&#20154;&#31867;&#30340;&#26412;&#36136;&#29305;&#36136;&#27704;&#36828;&#26080;&#27861;&#34987;&#26426;&#22120;&#27169;&#20223;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#23545;&#36825;&#31181;&#20449;&#24565;&#25552;&#20986;&#20102;&#36136;&#30097;&#65306;&#20154;&#24037;&#26234;&#33021;&#33021;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#21019;&#20316;&#35799;&#27468;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;ProFTAP&#65292;&#35813;&#26694;&#26550;&#21463;&#21040;&#22270;&#28789;&#27979;&#35797;&#30340;&#21551;&#21457;&#65292;&#29992;&#20110;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#35799;&#27468;&#21019;&#20316;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21457;&#29616;&#26368;&#36817;&#30340;LLMs&#30830;&#23454;&#20855;&#22791;&#20102;&#20960;&#20046;&#26080;&#27861;&#21306;&#20998;&#30340;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#21476;&#20856;&#20013;&#25991;&#35799;&#27468;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#21508;&#31181;&#24320;&#28304;&#30340;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#21487;&#20197;&#36229;&#36234;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
Some argue that the essence of humanity, such as creativity and sentiment, can never be mimicked by machines. This paper casts doubt on this belief by studying a vital question: Can AI compose poetry as well as humans? To answer the question, we propose ProFTAP, a novel evaluation framework inspired by Turing test to assess AI's poetry writing capability. We apply it on current large language models (LLMs) and find that recent LLMs do indeed possess the ability to write classical Chinese poems nearly indistinguishable from those of humans. We also reveal that various open-source LLMs can outperform GPT-4 on this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#25512;&#29702;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#33258;&#30001;&#25991;&#26412;&#20013;&#23398;&#20064;&#38899;&#39057;&#27010;&#24565;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#24182;&#32508;&#21512;&#32771;&#34385;&#22768;&#23398;&#29305;&#24449;&#21644;&#22768;&#38899;&#26469;&#28304;&#20449;&#24687;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#22768;&#38899;&#20107;&#20214;&#21644;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.04935</link><description>&lt;p&gt;
&#20174;&#23545;&#25239;&#24615;&#33258;&#28982;&#35821;&#35328;&#20013;&#23398;&#20064;&#38899;&#39057;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Learning Audio Concepts from Counterfactual Natural Language. (arXiv:2401.04935v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#25512;&#29702;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#33258;&#30001;&#25991;&#26412;&#20013;&#23398;&#20064;&#38899;&#39057;&#27010;&#24565;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#24182;&#32508;&#21512;&#32771;&#34385;&#22768;&#23398;&#29305;&#24449;&#21644;&#22768;&#38899;&#26469;&#28304;&#20449;&#24687;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#22768;&#38899;&#20107;&#20214;&#21644;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#38899;&#39057;&#20998;&#31867;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#65292;&#32570;&#20047;&#20174;&#33258;&#30001;&#25991;&#26412;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#25551;&#36848;&#38899;&#39057;&#30340;&#21407;&#22987;&#38899;&#39057;&#25991;&#26412;&#23545;&#20013;&#35299;&#38145;&#23398;&#20064;&#32852;&#21512;&#38899;&#39057;-&#25991;&#26412;&#23884;&#20837;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#35757;&#32451;&#27169;&#22411;&#20197;&#35782;&#21035;&#22768;&#38899;&#20107;&#20214;&#21644;&#26469;&#28304;&#30340;&#31995;&#32479;&#26041;&#27861;&#30340;&#25506;&#32034;&#24456;&#23569;&#65292;&#20363;&#22914;&#22312;&#31867;&#20284;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#23460;&#22806;&#27963;&#21160;&#20013;&#30340;&#28895;&#28779;&#21644;&#26538;&#22768;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#38899;&#39057;&#39046;&#22495;&#30340;&#22240;&#26524;&#25512;&#29702;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#24182;&#23558;&#23427;&#20204;&#21253;&#21547;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#26469;&#33258;&#20154;&#24037;&#27880;&#37322;&#21442;&#32771;&#25991;&#26412;&#30340;&#22768;&#23398;&#29305;&#24449;&#21644;&#22768;&#38899;&#26469;&#28304;&#20449;&#24687;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20351;&#29992;&#22810;&#20010;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#39044;&#35757;&#32451;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#20960;&#20010;&#24120;&#35265;&#30340;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional audio classification relied on predefined classes, lacking the ability to learn from free-form text. Recent methods unlock learning joint audio-text embeddings from raw audio-text pairs describing audio in natural language. Despite recent advancements, there is little exploration of systematic methods to train models for recognizing sound events and sources in alternative scenarios, such as distinguishing fireworks from gunshots at outdoor events in similar situations. This study introduces causal reasoning and counterfactual analysis in the audio domain. We use counterfactual instances and include them in our model across different aspects. Our model considers acoustic characteristics and sound source information from human-annotated reference texts. To validate the effectiveness of our model, we conducted pre-training utilizing multiple audio captioning datasets. We then evaluate with several common downstream tasks, demonstrating the merits of the proposed method as one
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04925</link><description>&lt;p&gt;
&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Reasoning Step Length on Large Language Models. (arXiv:2401.04925v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#23545;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;CoT&#30340;&#26377;&#25928;&#24615;&#19982;&#25552;&#31034;&#20013;&#25512;&#29702;&#27493;&#39588;&#30340;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#20010;&#23454;&#35777;&#23454;&#39564;&#26469;&#25506;&#32034;&#36825;&#20123;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#23454;&#39564;&#65292;&#25193;&#23637;&#21644;&#21387;&#32553;CoT&#28436;&#31034;&#20013;&#30340;&#21512;&#29702;&#25512;&#29702;&#27493;&#39588;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#22240;&#32032;&#19981;&#21464;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#31034;&#20013;&#24310;&#38271;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#27809;&#26377;&#21521;&#25552;&#31034;&#20013;&#28155;&#21152;&#26032;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#25552;&#39640;LLM&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#32553;&#30701;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#31361;&#26174;&#20102;CoT&#25552;&#31034;&#20013;&#27493;&#39588;&#25968;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make 
&lt;/p&gt;</description></item><item><title>ANGO&#26159;&#19968;&#20010;&#20013;&#25991;&#39046;&#22495;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#24341;&#20837;&#20102;&#20851;&#38190;&#28857;&#20998;&#31867;&#26631;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#21487;&#37327;&#21270;&#30340;&#38382;&#39064;&#38590;&#24230;&#26631;&#20934;&#65292;&#23545;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.04898</link><description>&lt;p&gt;
ANGO: &#19968;&#20010;&#38754;&#21521;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#39046;&#22495;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain. (arXiv:2401.04898v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04898
&lt;/p&gt;
&lt;p&gt;
ANGO&#26159;&#19968;&#20010;&#20013;&#25991;&#39046;&#22495;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#24341;&#20837;&#20102;&#20851;&#38190;&#28857;&#20998;&#31867;&#26631;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#21487;&#37327;&#21270;&#30340;&#38382;&#39064;&#38590;&#24230;&#26631;&#20934;&#65292;&#23545;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#23384;&#22312;&#25490;&#21517;&#22833;&#30495;&#21644;&#27169;&#22411;&#33021;&#21147;&#20998;&#26512;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;ANGO&#65292;&#19968;&#20010;&#20013;&#25991;&#22810;&#39033;&#36873;&#25321;&#39064;&#35780;&#20272;&#22522;&#20934;&#12290;ANGO&#39318;&#27425;&#25552;&#20986;&#20102;&#8220;&#20851;&#38190;&#28857;&#8221;&#20998;&#31867;&#26631;&#20934;&#65292;ANGO&#20013;&#30340;&#27599;&#20010;&#38382;&#39064;&#21487;&#20197;&#23545;&#24212;&#22810;&#20010;&#20851;&#38190;&#28857;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35780;&#20272;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22522;&#20110;&#30495;&#20154;&#34920;&#29616;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21487;&#37327;&#21270;&#30340;&#38382;&#39064;&#38590;&#24230;&#26631;&#20934;&#65292;&#24182;&#23558;ANGO&#38382;&#39064;&#20998;&#20026;9&#20010;&#38590;&#24230;&#32423;&#21035;&#65292;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#25968;&#25454;&#27844;&#28431;&#30340;&#24433;&#21709;&#24182;&#20805;&#20998;&#21033;&#29992;ANGO&#30340;&#21019;&#26032;&#29305;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29420;&#23478;&#25277;&#26679;&#31574;&#30053;&#21644;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25903;&#25345;&#24555;&#36895;&#27979;&#35797;&#38598;&#36845;&#20195;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ANGO&#23545;&#27169;&#22411;&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#35780;&#20272;&#32467;&#26524;&#20013;&#25581;&#31034;&#20986;&#26356;&#22810;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, various Large Language Models (LLMs) evaluation datasets have emerged, but most of them have issues with distorted rankings and difficulty in model capabilities analysis. Addressing these concerns, this paper introduces ANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes \textit{Keypoint} categorization standard for the first time, each question in ANGO can correspond to multiple keypoints, effectively enhancing interpretability of evaluation results. Base on performance of real humans, we build a quantifiable question difficulty standard and divide ANGO questions into 9 difficulty levels, which provide more precise guidance for model training. To minimize data leakage impact and fully leverage ANGO's innovative features, we have engineered exclusive sampling strategies and a new evaluation framework that support swift testset iteration. Our experiments demonstrate that ANGO poses a stronger challenge to models and reveals more details in evaluation resu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#29992;&#25143;&#32842;&#22825;&#26426;&#22120;&#20154;&#26694;&#26550;&#65288;MUCA&#65289;&#65292;&#35813;&#26694;&#26550;&#25903;&#25345;&#32676;&#32452;&#35752;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#26469;&#30830;&#23450;&#22238;&#24212;&#20869;&#23481;&#12289;&#26102;&#26426;&#21644;&#36866;&#24403;&#30340;&#25509;&#25910;&#32773;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#29992;&#25143;&#27169;&#25311;&#22120;&#65288;MUS&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#34892;&#20026;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#27979;&#35797;&#21644;&#20248;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;</title><link>http://arxiv.org/abs/2401.04883</link><description>&lt;p&gt;
&#22810;&#29992;&#25143;&#32842;&#22825;&#21161;&#25163;&#65288;MUCA&#65289;&#65306;&#19968;&#31181;&#20351;&#29992;LLMs&#26694;&#26550;&#20419;&#36827;&#32676;&#20307;&#23545;&#35805;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate Group Conversations. (arXiv:2401.04883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04883
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#29992;&#25143;&#32842;&#22825;&#26426;&#22120;&#20154;&#26694;&#26550;&#65288;MUCA&#65289;&#65292;&#35813;&#26694;&#26550;&#25903;&#25345;&#32676;&#32452;&#35752;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#26469;&#30830;&#23450;&#22238;&#24212;&#20869;&#23481;&#12289;&#26102;&#26426;&#21644;&#36866;&#24403;&#30340;&#25509;&#25910;&#32773;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#29992;&#25143;&#27169;&#25311;&#22120;&#65288;MUS&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#34892;&#20026;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#27979;&#35797;&#21644;&#20248;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#32780;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#29992;&#25143;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#19978;&#65292;&#37325;&#28857;&#25918;&#22312;&#29992;&#25143;&#36755;&#20837;&#21518;&#20915;&#23450;&#8220;&#22238;&#31572;&#20160;&#20040;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#29992;&#25143;&#32842;&#22825;&#26426;&#22120;&#20154;&#26377;&#26356;&#22797;&#26434;&#30340;3W&#35774;&#35745;&#32500;&#24230;&#8212;&#8212;&#22914;&#20309;&#22238;&#31572;&#65292;&#8220;&#20309;&#26102;&#8221;&#22238;&#24212;&#65292;&#8220;&#22238;&#31572;&#35841;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Multi-User Chat Assistant (MUCA)&#30340;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;&#32676;&#32452;&#35752;&#35770;&#12290;MUCA&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#23376;&#20027;&#39064;&#29983;&#25104;&#22120;&#65292;&#23545;&#35805;&#20998;&#26512;&#22120;&#21644;&#35805;&#35821;&#31574;&#30053;&#20210;&#35009;&#22120;&#12290;&#36825;&#20123;&#27169;&#22359;&#20849;&#21516;&#30830;&#23450;&#21512;&#36866;&#30340;&#22238;&#24212;&#20869;&#23481;&#12289;&#26102;&#26426;&#21644;&#36866;&#24403;&#30340;&#25509;&#25910;&#32773;&#12290;&#20026;&#20102;&#20351;MUCA&#30340;&#20248;&#21270;&#36807;&#31243;&#26356;&#23481;&#26131;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#22810;&#29992;&#25143;&#27169;&#25311;&#22120;&#65288;MUS&#65289;&#65292;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#34892;&#20026;&#12290;&#36825;&#20351;&#24471;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#27169;&#25311;&#29992;&#25143;&#20043;&#38388;&#30340;&#23545;&#35805;&#36827;&#34892;&#26356;&#24555;&#36895;&#30340;&#27169;&#25311;&#65292;&#20174;&#32780;&#20351;&#24471;&#26089;&#26399;&#27979;&#35797;&#21644;&#20248;&#21270;&#36807;&#31243;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have provided a new avenue for chatbot development, while most existing research has primarily centered on single-user chatbots that focus on deciding "What" to answer after user inputs. In this paper, we identified that multi-user chatbots have more complex 3W design dimensions -- "What" to say, "When" to respond, and "Who" to answer. Additionally, we proposed Multi-User Chat Assistant (MUCA), which is an LLM-based framework for chatbots specifically designed for group discussions. MUCA consists of three main modules: Sub-topic Generator, Dialog Analyzer, and Utterance Strategies Arbitrator. These modules jointly determine suitable response contents, timings, and the appropriate recipients. To make the optimizing process for MUCA easier, we further propose an LLM-based Multi-User Simulator (MUS) that can mimic real user behavior. This enables faster simulation of a conversation between the chatbot and simulated users, making the earl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31561;&#24453;&#21442;&#19982;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26597;&#35810;&#20869;&#23384;&#20013;&#26816;&#32034;&#24102;&#26377;&#39537;&#36880;&#26597;&#35810;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#65288;K/V&#23384;&#20648;&#22120;&#65289;&#65292;&#20351;&#29992;&#36880;&#20986;&#31574;&#30053;&#26469;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#24182;&#36866;&#24212;&#21508;&#31181;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.04881</link><description>&lt;p&gt;
Attendre: &#29992;&#21453;&#39537;&#36880;&#26597;&#35810;&#22312;&#22522;&#20110;&#35760;&#24518;&#30340;&#21464;&#21387;&#22120;&#20013;&#31561;&#24453;&#21442;&#19982;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing. (arXiv:2401.04881v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31561;&#24453;&#21442;&#19982;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26597;&#35810;&#20869;&#23384;&#20013;&#26816;&#32034;&#24102;&#26377;&#39537;&#36880;&#26597;&#35810;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#65288;K/V&#23384;&#20648;&#22120;&#65289;&#65292;&#20351;&#29992;&#36880;&#20986;&#31574;&#30053;&#26469;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#24182;&#36866;&#24212;&#21508;&#31181;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;LLMs&#33021;&#22815;&#22788;&#29702;&#26356;&#22797;&#26434;&#31867;&#22411;&#30340;&#36755;&#20837;&#65292;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#19988;&#32463;&#27982;&#22320;&#22788;&#29702;&#21487;&#33021;&#20219;&#24847;&#38271;&#30340;&#24207;&#21015;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;FIFO&#20869;&#23384;&#26469;&#23384;&#20648;&#36807;&#21435;&#22359;&#30340;&#27880;&#24847;&#23376;&#23618;&#30340;&#38190;&#21644;&#20540;&#65292;&#20197;&#20801;&#35768;&#21518;&#32493;&#26597;&#35810;&#21442;&#19982;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#21644;/&#25110;&#32771;&#34385;&#29305;&#23450;&#30340;LM&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20808;&#21069;&#19978;&#19979;&#25991;&#20013;&#30340;&#38190;-&#20540;&#19982;&#24403;&#21069;&#26597;&#35810;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#21040;&#20855;&#26377;&#21452;&#21521;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#20363;&#22914;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25110;PrefixLM&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36880;&#20986;&#31574;&#30053;&#65292;&#20363;&#22914;LRA&#21644;LFA&#65292;&#26469;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#24182;&#36866;&#24212;&#21508;&#31181;&#26550;&#26500;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Attendre&#23618;&#65292;&#19968;&#31181;&#36890;&#36807;&#22312;&#26597;&#35810;&#20869;&#23384;&#20013;&#26816;&#32034;&#24102;&#26377;&#39537;&#36880;&#26597;&#35810;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#65288;K/V&#23384;&#20648;&#22120;&#65289;&#30340;&#31561;&#24453;&#21442;&#19982;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
As LLMs have become capable of processing more complex types of inputs, researchers have recently studied how to efficiently and affordably process possibly arbitrarily long sequences. One effective approach is to use a FIFO memory to store keys and values of an attention sublayer from past chunks to allow subsequent queries to attend. However, this approach requires a large memory and/or takes into the consideration the specific LM architecture. Moreover, due to the causal nature between the key-values in prior context and the queries at present, this approach cannot be extended to bidirectional attention such as in an encoder-decoder or PrefixLM decoder-only architecture. In this paper, we propose to use eviction policies, such as LRA and LFA, to reduce the memory size and adapt to various architectures, and we also propose the Attendre layer, a wait-to-attend mechanism by retrieving the key-value memory (K/V memory) with evicted queries in the query memory (Q memory). As a first ste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23454;&#26102;&#19988;&#36830;&#32493;&#30340;&#35821;&#38899;&#27963;&#21160;&#39044;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#27169;&#22411;&#23558;&#23545;&#35805;&#30340;&#38899;&#39057;&#26144;&#23556;&#21040;&#26410;&#26469;&#30340;&#35821;&#38899;&#27963;&#21160;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2401.04868</link><description>&lt;p&gt;
&#23454;&#26102;&#19988;&#36830;&#32493;&#30340;&#35821;&#38899;&#27963;&#21160;&#39044;&#27979;&#31995;&#32479;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Real-time and Continuous Turn-taking Prediction Using Voice Activity Projection. (arXiv:2401.04868v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23454;&#26102;&#19988;&#36830;&#32493;&#30340;&#35821;&#38899;&#27963;&#21160;&#39044;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#27169;&#22411;&#23558;&#23545;&#35805;&#30340;&#38899;&#39057;&#26144;&#23556;&#21040;&#26410;&#26469;&#30340;&#35821;&#38899;&#27963;&#21160;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23454;&#26102;&#19988;&#36830;&#32493;&#30340;&#35821;&#38899;&#27963;&#21160;&#39044;&#27979;&#31995;&#32479;&#30340;&#28436;&#31034;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;(VAP)&#27169;&#22411;&#65292;&#30452;&#25509;&#23558;&#23545;&#35805;&#30340;&#31435;&#20307;&#22768;&#38899;&#39057;&#26144;&#23556;&#21040;&#26410;&#26469;&#30340;&#35821;&#38899;&#27963;&#21160;&#12290;VAP&#27169;&#22411;&#21253;&#25324;&#23545;&#27604;&#24615;&#39044;&#27979;&#32534;&#30721;(CPC)&#21644;&#33258;&#27880;&#24847;&#21147;&#36716;&#25442;&#22120;&#65292;&#25509;&#30528;&#26159;&#20132;&#21449;&#27880;&#24847;&#21147;&#36716;&#25442;&#22120;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36755;&#20837;&#19978;&#19979;&#25991;&#38899;&#39057;&#38271;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;CPU&#35774;&#32622;&#19979;&#23454;&#26102;&#36816;&#34892;&#65292;&#24615;&#33021;&#25439;&#22833;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
A demonstration of a real-time and continuous turn-taking prediction system is presented. The system is based on a voice activity projection (VAP) model, which directly maps dialogue stereo audio to future voice activities. The VAP model includes contrastive predictive coding (CPC) and self-attention transformers, followed by a cross-attention transformer. We examine the effect of the input context audio length and demonstrate that the proposed system can operate in real-time with CPU settings, with minimal performance degradation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#34892;&#20026;&#19982;&#20027;&#35266;&#35780;&#20272;&#22312;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38388;&#25509;&#20294;&#23458;&#35266;&#35780;&#20272;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#29992;&#25143;&#34892;&#20026;&#25351;&#26631;&#23545;&#35780;&#20272;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.04867</link><description>&lt;p&gt;
&#23545;&#29992;&#25143;&#34892;&#20026;&#36827;&#34892;&#20998;&#26512;&#20197;&#23458;&#35266;&#35780;&#20272;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An Analysis of User Behaviours for Objectively Evaluating Spoken Dialogue Systems. (arXiv:2401.04867v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#34892;&#20026;&#19982;&#20027;&#35266;&#35780;&#20272;&#22312;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38388;&#25509;&#20294;&#23458;&#35266;&#35780;&#20272;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#29992;&#25143;&#34892;&#20026;&#25351;&#26631;&#23545;&#35780;&#20272;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#26696;&#24456;&#37325;&#35201;&#65292;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#20027;&#35266;&#35780;&#20272;&#22312;&#29992;&#25143;&#23454;&#39564;&#20013;&#24120;&#29992;&#65292;&#20294;&#23458;&#35266;&#35780;&#20272;&#23545;&#20110;&#30740;&#31350;&#27604;&#36739;&#21644;&#21487;&#22797;&#21046;&#24615;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#38388;&#25509;&#20294;&#23458;&#35266;&#22320;&#35780;&#20272;&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#31038;&#20132;&#23545;&#35805;&#20219;&#21153;&#20013;&#29992;&#25143;&#34892;&#20026;&#19982;&#20027;&#35266;&#35780;&#20272;&#20998;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#19987;&#27880;&#20542;&#21548;&#12289;&#38754;&#35797;&#21644;&#39318;&#27425;&#20250;&#35758;&#23545;&#35805;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29992;&#25143;&#35805;&#35821;&#26159;&#20027;&#35201;&#22240;&#32032;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#22914;&#19987;&#27880;&#20542;&#21548;&#21644;&#38754;&#35797;&#65292;&#35805;&#35821;&#25968;&#37327;&#21644;&#21333;&#35789;&#25968;&#37327;&#31561;&#25351;&#26631;&#22312;&#35780;&#20272;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#35266;&#23519;&#35821;&#35843;&#19981;&#27969;&#30021;&#31561;&#20063;&#21487;&#20197;&#25351;&#31034;&#27491;&#24335;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#38754;&#35797;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#39640;&#20114;&#21160;&#24615;&#30340;&#23545;&#35805;&#20219;&#21153;&#20013;&#65292;&#22914;&#39318;&#27425;&#20250;&#35758;&#23545;&#35805;&#65292;&#29992;&#25143;&#24773;&#32490;&#21644;&#21442;&#19982;&#31243;&#24230;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Establishing evaluation schemes for spoken dialogue systems is important, but it can also be challenging. While subjective evaluations are commonly used in user experiments, objective evaluations are necessary for research comparison and reproducibility. To address this issue, we propose a framework for indirectly but objectively evaluating systems based on users' behaviours. In this paper, to this end, we investigate the relationship between user behaviours and subjective evaluation scores in social dialogue tasks: attentive listening, job interview, and first-meeting conversation. The results reveal that in dialogue tasks where user utterances are primary, such as attentive listening and job interview, indicators like the number of utterances and words play a significant role in evaluation. Observing disfluency also can indicate the effectiveness of formal tasks, such as job interview. On the other hand, in dialogue tasks with high interactivity, such as first-meeting conversation, b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22359;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.04858</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35821;&#35328;&#25552;&#31034;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
User Embedding Model for Personalized Language Prompting. (arXiv:2401.04858v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22359;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#65292;&#24314;&#27169;&#38271;&#26102;&#38388;&#30340;&#21382;&#21490;&#35760;&#24405;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#33021;&#22815;&#25429;&#25417;&#29992;&#25143;&#19981;&#26029;&#28436;&#21464;&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#20934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#20559;&#22909;&#29702;&#35299;&#20013;&#24314;&#27169;&#38271;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22359;(UEM)&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#20197;&#23884;&#20837;&#24418;&#24335;&#21387;&#32553;&#21644;&#34920;&#31034;&#65292;&#23558;&#20854;&#20316;&#20026;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#36719;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#26174;&#33879;&#26356;&#38271;&#30340;&#21382;&#21490;&#35760;&#24405;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#20351;&#29992;&#34920;&#31034;&#20026;&#23884;&#20837;&#30340;&#29992;&#25143;&#20449;&#21495;&#26469;&#20559;&#32622;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long histories plays a pivotal role in enhancing recommendation systems, allowing to capture user's evolving preferences, resulting in more precise and personalized recommendations. In this study we tackle the challenges of modeling long user histories for preference understanding in natural language. Specifically, we introduce a new User Embedding Module (UEM) that efficiently processes user history in free-form text by compressing and representing them as embeddings, to use them as soft prompts to a LM. Our experiments demonstrate the superior capability of this approach in handling significantly longer histories compared to conventional text based prompting methods, yielding substantial improvements in predictive performance. The main contribution of this research is to demonstrate the ability to bias language models with user signals represented as embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#26356;&#20687;&#22270;&#20070;&#39302;&#36824;&#26159;&#22270;&#20070;&#31649;&#29702;&#21592;&#30340;&#38382;&#39064;&#12290;&#35770;&#25991;&#39318;&#20808;&#38416;&#36848;&#20102; "&#25991;&#29486;&#20027;&#20041; "&#36825;&#19968;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20854;&#30340;&#25361;&#25112;&#65292;&#25351;&#20986;LLMs&#29983;&#25104;&#30340;&#20840;&#26032;&#25991;&#26412;&#22312;&#20869;&#23481;&#19978;&#20381;&#36182;&#20110;&#21407;&#22987;&#20154;&#31867;&#25991;&#26412;&#30340;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545; "&#25991;&#29486;&#20027;&#20041;"&#30340;&#26032;&#39062;&#25361;&#25112;&#65292;&#35752;&#35770;&#20102;LLMs&#29983;&#25104;&#30340; "&#26032;&#24341;&#29992;"&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#24515;&#28789;&#21746;&#23398;&#20013;&#30340;&#35299;&#37322;&#20027;&#20041;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#26377;&#38480;&#20195;&#29702;&#33021;&#21147;&#30340;LLMs&#21487;&#33021;&#23384;&#22312;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04854</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26356;&#20687;&#22270;&#20070;&#39302;&#36824;&#26159;&#22270;&#20070;&#31649;&#29702;&#21592;&#65311;Bibliotechnism&#65292;&#23567;&#35828;&#24341;&#29992;&#38382;&#39064;&#21644;LLM&#30340;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs. (arXiv:2401.04854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#26356;&#20687;&#22270;&#20070;&#39302;&#36824;&#26159;&#22270;&#20070;&#31649;&#29702;&#21592;&#30340;&#38382;&#39064;&#12290;&#35770;&#25991;&#39318;&#20808;&#38416;&#36848;&#20102; "&#25991;&#29486;&#20027;&#20041; "&#36825;&#19968;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20854;&#30340;&#25361;&#25112;&#65292;&#25351;&#20986;LLMs&#29983;&#25104;&#30340;&#20840;&#26032;&#25991;&#26412;&#22312;&#20869;&#23481;&#19978;&#20381;&#36182;&#20110;&#21407;&#22987;&#20154;&#31867;&#25991;&#26412;&#30340;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545; "&#25991;&#29486;&#20027;&#20041;"&#30340;&#26032;&#39062;&#25361;&#25112;&#65292;&#35752;&#35770;&#20102;LLMs&#29983;&#25104;&#30340; "&#26032;&#24341;&#29992;"&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#24515;&#28789;&#21746;&#23398;&#20013;&#30340;&#35299;&#37322;&#20027;&#20041;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#26377;&#38480;&#20195;&#29702;&#33021;&#21147;&#30340;LLMs&#21487;&#33021;&#23384;&#22312;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#65288;&#35821;&#35328;&#27169;&#22411;&#65289;&#26159;&#21542;&#20687;&#22797;&#21360;&#26426;&#25110;&#21360;&#21047;&#26426;&#31561;&#25991;&#21270;&#25216;&#26415;&#19968;&#26679;&#65292;&#20256;&#36755;&#20449;&#24687;&#20294;&#26080;&#27861;&#21019;&#24314;&#26032;&#20869;&#23481;&#65311;&#25105;&#20204;&#23558;&#36825;&#20010;&#27010;&#24565;&#31216;&#20026;"&#25991;&#29486;&#20027;&#20041;"&#65292;&#23427;&#38754;&#20020;&#19968;&#20010;&#25361;&#25112;&#65292;&#21363;LLMs&#32463;&#24120;&#29983;&#25104;&#20840;&#26032;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;"&#25991;&#29486;&#20027;&#20041;"&#23545;&#25239;&#36825;&#20010;&#25361;&#25112;&#36827;&#34892;&#36777;&#25252;&#65292;&#23637;&#31034;&#20102;&#26032;&#30340;&#25991;&#26412;&#20165;&#22312;&#27966;&#29983;&#24847;&#20041;&#19978;&#20855;&#26377;&#24847;&#20041;&#65292;&#22240;&#27492;&#36825;&#20123;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20869;&#23481;&#22312;&#37325;&#35201;&#24847;&#20041;&#19978;&#20381;&#36182;&#20110;&#21407;&#22987;&#20154;&#31867;&#25991;&#26412;&#30340;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#12289;&#26032;&#39062;&#30340;&#25361;&#25112;&#65292;&#21363;LLMs&#29983;&#25104;"&#26032;&#24341;&#29992;"&#30340;&#20363;&#23376;&#65292;&#20351;&#29992;&#26032;&#30340;&#21517;&#31216;&#26469;&#24341;&#29992;&#26032;&#23454;&#20307;&#12290;&#22914;&#26524;LLMs&#19981;&#26159;&#25991;&#21270;&#25216;&#26415;&#32780;&#26159;&#20855;&#26377;&#26377;&#38480;&#24418;&#24335;&#30340;&#20195;&#29702;&#33021;&#21147;&#65288;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#65289;&#65292;&#36825;&#26679;&#30340;&#20363;&#23376;&#21487;&#20197;&#24456;&#22909;&#22320;&#35299;&#37322;&#12290;&#26681;&#25454;&#24515;&#28789;&#21746;&#23398;&#20013;&#30340;&#35299;&#37322;&#20027;&#20041;&#65292;&#20165;&#24403;&#19968;&#20010;&#31995;&#32479;&#30340;&#34892;&#20026;&#21487;&#20197;&#36890;&#36807;&#20551;&#35774;&#23427;&#20855;&#26377;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#26469;&#24456;&#22909;&#22320;&#35299;&#37322;&#26102;&#65292;&#23427;&#25165;&#20855;&#26377;&#36825;&#26679;&#30340;&#20449;&#24565;&#12289;&#27442;&#26395;&#21644;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are LLMs cultural technologies like photocopiers or printing presses, which transmit information but cannot create new content? A challenge for this idea, which we call bibliotechnism, is that LLMs often do generate entirely novel text. We begin by defending bibliotechnism against this challenge, showing how novel text may be meaningful only in a derivative sense, so that the content of this generated text depends in an important sense on the content of original human text. We go on to present a different, novel challenge for bibliotechnism, stemming from examples in which LLMs generate "novel reference", using novel names to refer to novel entities. Such examples could be smoothly explained if LLMs were not cultural technologies but possessed a limited form of agency (beliefs, desires, and intentions). According to interpretationism in the philosophy of mind, a system has beliefs, desires and intentions if and only if its behavior is well-explained by the hypothesis that it has such s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#38750;&#27491;&#24335;&#25991;&#26412;&#20013;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#20174;&#21475;&#35821;&#25991;&#26412;&#20013;&#35782;&#21035;&#30151;&#29366;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35780;&#20272;&#22810;&#20010;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#29992;BERT-based&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#26412;&#30740;&#31350;&#25214;&#21040;&#20102;&#22312;&#38750;&#27491;&#24335;&#25968;&#25454;&#19978;&#34920;&#29616;&#36739;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.04853</link><description>&lt;p&gt;
&#20174;&#21475;&#35821;&#25991;&#26412;&#20013;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Entity Recognition from Colloquial Text. (arXiv:2401.04853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#38750;&#27491;&#24335;&#25991;&#26412;&#20013;&#36827;&#34892;&#23454;&#20307;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#20174;&#21475;&#35821;&#25991;&#26412;&#20013;&#35782;&#21035;&#30151;&#29366;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35780;&#20272;&#22810;&#20010;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#29992;BERT-based&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#26412;&#30740;&#31350;&#25214;&#21040;&#20102;&#22312;&#38750;&#27491;&#24335;&#25968;&#25454;&#19978;&#34920;&#29616;&#36739;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38750;&#27491;&#24335;&#25991;&#26412;&#65288;&#22914;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#38750;&#27491;&#24335;&#20132;&#27969;&#65289;&#20013;&#25552;&#21462;&#27010;&#24565;&#21644;&#24863;&#20852;&#36259;&#30340;&#23454;&#20307;&#26159;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#22312;&#35768;&#22810;&#39046;&#22495;&#65288;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#12289;&#23458;&#25143;&#20851;&#31995;&#31649;&#29702;&#31561;&#65289;&#20013;&#30340;&#37325;&#35201;&#33021;&#21147;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#21644;&#25216;&#26415;&#20027;&#35201;&#38598;&#20013;&#22312;&#27491;&#24335;&#25991;&#26412;&#19978;&#65292;&#22312;&#38750;&#27491;&#24335;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#32780;&#38750;&#27491;&#24335;&#25968;&#25454;&#20855;&#26377;&#19968;&#20123;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#21644;&#35780;&#20272;BERT-based&#27169;&#22411;&#24494;&#35843;&#30340;&#20960;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#30740;&#31350;&#20102;&#20174;&#21475;&#35821;&#25991;&#26412;&#20013;&#35782;&#21035;&#30151;&#29366;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#31574;&#30053;&#36890;&#36807;&#36873;&#25321;&#22522;&#30784;&#27169;&#22411;&#12289;&#35757;&#32451;&#35821;&#26009;&#24211;&#20197;&#21450;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#24212;&#29992;&#26415;&#35821;&#25200;&#21160;&#26469;&#21306;&#20998;&#12290;&#20351;&#29992;&#36825;&#20123;&#31574;&#30053;&#35757;&#32451;&#30340;&#26368;&#20339;&#27169;&#22411;&#32988;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extraction of concepts and entities of interest from non-formal texts such as social media posts and informal communication is an important capability for decision support systems in many domains, including healthcare, customer relationship management, and others. Despite the recent advances in training large language models for a variety of natural language processing tasks, the developed models and techniques have mainly focused on formal texts and do not perform as well on colloquial data, which is characterized by a number of distinct challenges. In our research, we focus on the healthcare domain and investigate the problem of symptom recognition from colloquial texts by designing and evaluating several training strategies for BERT-based model fine-tuning. These strategies are distinguished by the choice of the base model, the training corpora, and application of term perturbations in the training data. The best-performing models trained using these strategies outperform the state-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;PTCAD&#65292;&#29992;&#20110;&#38463;&#25289;&#20271;&#25991;&#26412;&#38899;&#26631;&#21270;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#38463;&#25289;&#20271;&#25991;&#26412;&#38899;&#26631;&#21270;&#35270;&#20026;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;GPT-4&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.04848</link><description>&lt;p&gt;
&#22312;&#36801;&#31227;&#23398;&#20064;&#26102;&#20195;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#38899;&#26631;&#21270;&#65306;&#20165;&#38656;&#26631;&#35760;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Arabic Text Diacritization In The Age Of Transfer Learning: Token Classification Is All You Need. (arXiv:2401.04848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;PTCAD&#65292;&#29992;&#20110;&#38463;&#25289;&#20271;&#25991;&#26412;&#38899;&#26631;&#21270;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#38463;&#25289;&#20271;&#25991;&#26412;&#38899;&#26631;&#21270;&#35270;&#20026;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;GPT-4&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#25289;&#20271;&#25991;&#26412;&#30340;&#33258;&#21160;&#38899;&#26631;&#21270;&#28041;&#21450;&#21521;&#25991;&#26412;&#20013;&#28155;&#21152;&#38899;&#26631;&#31526;&#21495;&#12290;&#36825;&#19968;&#20219;&#21153;&#23545;&#20110;&#35745;&#31639;&#22788;&#29702;&#21644;&#29702;&#35299;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PTCAD&#65288;&#29992;&#20110;&#38463;&#25289;&#20271;&#25991;&#26412;&#38899;&#26631;&#21270;&#30340;&#39044;&#20808;&#24494;&#35843;&#26631;&#35760;&#20998;&#31867;&#26041;&#27861;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#38463;&#25289;&#20271;&#25991;&#26412;&#38899;&#26631;&#21270;&#20219;&#21153;&#30340;&#20840;&#26032;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;PTCAD&#21253;&#25324;&#39044;&#20808;&#24494;&#35843;&#38454;&#27573;&#21644;&#24494;&#35843;&#38454;&#27573;&#65292;&#23558;&#38463;&#25289;&#20271;&#25991;&#26412;&#38899;&#26631;&#21270;&#35270;&#20026;&#38024;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;Tashkeela&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;PTCAD&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23558;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20943;&#23569;&#20102;20&#65285;&#65292;&#24182;&#22312;ATD&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic diacritization of Arabic text involves adding diacritical marks (diacritics) to the text. This task poses a significant challenge with noteworthy implications for computational processing and comprehension. In this paper, we introduce PTCAD (Pre-FineTuned Token Classification for Arabic Diacritization, a novel two-phase approach for the Arabic Text Diacritization task. PTCAD comprises a pre-finetuning phase and a finetuning phase, treating Arabic Text Diacritization as a token classification task for pre-trained models. The effectiveness of PTCAD is demonstrated through evaluations on two benchmark datasets derived from the Tashkeela dataset, where it achieves state-of-the-art results, including a 20\% reduction in Word Error Rate (WER) compared to existing benchmarks and superior performance over GPT-4 in ATD tasks.
&lt;/p&gt;</description></item><item><title>MoSECroT&#26159;&#19968;&#20010;&#32467;&#21512;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#27169;&#22411;&#25340;&#25509;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;&#23427;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26500;&#24314;&#20102;&#28304;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30446;&#26631;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#20849;&#20139;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#36807;&#31616;&#21333;&#20132;&#25442;&#23884;&#20837;&#20174;&#28304;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#36827;&#34892;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2401.04821</link><description>&lt;p&gt;
MoSECroT: &#20351;&#29992;&#38745;&#24577;&#35789;&#21521;&#37327;&#36827;&#34892;&#27169;&#22411;&#25340;&#25509;&#23454;&#29616;&#36328;&#35821;&#35328;&#38646;&#26679;&#20363;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer. (arXiv:2401.04821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04821
&lt;/p&gt;
&lt;p&gt;
MoSECroT&#26159;&#19968;&#20010;&#32467;&#21512;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#27169;&#22411;&#25340;&#25509;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;&#23427;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26500;&#24314;&#20102;&#28304;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30446;&#26631;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#20849;&#20139;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#36807;&#31616;&#21333;&#20132;&#25442;&#23884;&#20837;&#20174;&#28304;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#36827;&#34892;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#32780;&#36825;&#20123;&#36164;&#28304;&#20960;&#20046;&#21482;&#26377;&#39640;&#36164;&#28304;&#35821;&#35328;&#25165;&#33021;&#33719;&#24471;&#12290;&#30456;&#21453;&#65292;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#35757;&#32451;&#26356;&#23481;&#26131;&#65292;&#21487;&#20197;&#26356;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MoSECroT&#65288;Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer&#65289;&#27169;&#22411;&#25340;&#25509;&#19982;&#38745;&#24577;&#35789;&#21521;&#37327;&#32467;&#21512;&#30340;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26500;&#24314;&#28304;&#35821;&#35328;PLM&#23884;&#20837;&#21644;&#30446;&#26631;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#20043;&#38388;&#30340;&#20849;&#20139;&#31354;&#38388;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#28304;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;PLM&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#22320;&#20132;&#25442;&#23884;&#20837;&#23436;&#25104;&#20174;&#28304;&#35821;&#35328;&#21040;&#30446;&#26631;&#35821;&#35328;&#30340;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained language models (PLMs) have achieved remarkable performance in various natural language processing (NLP) tasks. However, pre-training such models can take considerable resources that are almost only available to high-resource languages. On the contrary, static word embeddings are easier to train in terms of computing resources and the amount of data required. In this paper, we introduce MoSECroT Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer), a novel and challenging task that is especially relevant to low-resource languages for which static word embeddings are available. To tackle the task, we present the first framework that leverages relative representations to construct a common space for the embeddings of a source language PLM and the static word embeddings of a target language. In this way, we can train the PLM on source-language training data and perform zero-shot transfer to the target language by simply swapping th
&lt;/p&gt;</description></item><item><title>Translate-Distill &#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32763;&#35793;&#21644;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#36328;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.04810</link><description>&lt;p&gt;
Translate-Distill: &#36890;&#36807;&#32763;&#35793;&#21644;&#33976;&#39311;&#23398;&#20064;&#36328;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation. (arXiv:2401.04810v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04810
&lt;/p&gt;
&lt;p&gt;
Translate-Distill &#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32763;&#35793;&#21644;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#36328;&#35821;&#35328;&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#33521;&#35821;&#21333;&#35821;&#26816;&#32034;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#37327;&#26597;&#35810;-&#25991;&#26723;&#30456;&#20851;&#24615;&#21028;&#26029;&#35757;&#32451;&#30340;&#20132;&#20114;&#32534;&#30721;&#22120;&#21487;&#20197;&#29992;&#20316;&#25945;&#24072;&#27169;&#22411;&#26469;&#35757;&#32451;&#26356;&#39640;&#25928;&#20294;&#21516;&#26679;&#26377;&#25928;&#30340;&#21452;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#26597;&#35810;&#21644;&#25991;&#26723;&#20043;&#38388;&#36827;&#34892;&#36328;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034; (CLIR) &#26102;&#24212;&#29992;&#31867;&#20284;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26597;&#35810;&#21644;&#25991;&#26723;&#35821;&#35328;&#19981;&#21516;&#26102;&#32570;&#20047;&#36275;&#22815;&#22823;&#30340;&#35757;&#32451;&#38598;&#21512;&#12290;&#29616;&#26377;&#30340; CLIR &#25216;&#26415;&#20381;&#36182;&#20110;&#20174;&#24222;&#22823;&#30340;&#33521;&#35821; MS MARCO &#35757;&#32451;&#38598;&#20013;&#32763;&#35793;&#26597;&#35810;&#12289;&#25991;&#26723;&#25110;&#20004;&#32773;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; Translate-Train&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696; Translate-Distill&#65292;&#20854;&#20013;&#20174;&#21333;&#35821;&#20132;&#20114;&#32534;&#30721;&#22120;&#25110; CLIR &#20132;&#20114;&#32534;&#30721;&#22120;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#35757;&#32451;&#21452;&#32534;&#30721;&#22120; CLIR &#23398;&#29983;&#27169;&#22411;&#12290;&#36825;&#31181;&#26356;&#20016;&#23500;&#30340;&#35774;&#35745;&#31354;&#38388;&#20351;&#24471;&#25945;&#24072;&#27169;&#22411;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Prior work on English monolingual retrieval has shown that a cross-encoder trained using a large number of relevance judgments for query-document pairs can be used as a teacher to train more efficient, but similarly effective, dual-encoder student models. Applying a similar knowledge distillation approach to training an efficient dual-encoder model for Cross-Language Information Retrieval (CLIR), where queries and documents are in different languages, is challenging due to the lack of a sufficiently large training collection when the query and document languages differ. The state of the art for CLIR thus relies on translating queries, documents, or both from the large English MS MARCO training set, an approach called Translate-Train. This paper proposes an alternative, Translate-Distill, in which knowledge distillation from either a monolingual cross-encoder or a CLIR cross-encoder is used to train a dual-encoder CLIR student model. This richer design space enables the teacher model to
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04620</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#30340;Agent&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Agent&#36234;&#26469;&#36234;&#22810;&#22320;&#28183;&#36879;&#21040;&#20154;&#31867;&#29983;&#20135;&#21644;&#29983;&#27963;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#20984;&#26174;&#20102;&#23558;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#30446;&#21069;AI&#31995;&#32479;&#30340;&#23545;&#40784;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#20154;&#20026;&#24178;&#39044;&#23545;LLM&#36827;&#34892;&#34987;&#21160;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;Agent&#20855;&#26377;&#25509;&#21463;&#29615;&#22659;&#21453;&#39304;&#21644;&#33258;&#25105;&#36827;&#21270;&#31561;&#29305;&#24615;&#65292;&#20351;&#24471;LLM&#23545;&#40784;&#26041;&#27861;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;Agent&#36827;&#21270;&#21644;&#23545;&#40784;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#12290;&#22312;&#31038;&#20250;&#35268;&#33539;&#19981;&#26029;&#28436;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#65292;&#32780;&#23545;&#40784;&#19981;&#36275;&#30340;Agent&#21017;&#36880;&#28176;&#20943;&#23569;&#12290;&#36890;&#36807;&#22810;&#20010;&#35282;&#24230;&#23545;&#19982;&#31038;&#20250;&#35268;&#33539;&#30456;&#23545;&#40784;&#30340;Agent&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;StepGame&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26144;&#23556;&#21040;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#22312;&#22810;&#36339;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.03991</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65306;&#36890;&#36807;StepGame&#22522;&#20934;&#30340;&#28145;&#20837;&#35780;&#20272;&#21644;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark. (arXiv:2401.03991v1 [cs.AI] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;StepGame&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26144;&#23556;&#21040;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#22312;&#22810;&#36339;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20363;&#22914;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#31354;&#38388;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#25361;&#25112;&#12290;StepGame&#31561;&#22522;&#20934;&#35780;&#20272;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;ChatGPT&#22312;&#20854;&#20013;&#34920;&#29616;&#20986;&#20102;&#19981;&#23613;&#20154;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20934;&#20013;&#23384;&#22312;&#30340;&#27169;&#26495;&#38169;&#35823;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#26495;&#38169;&#35823;&#65292;ChatGPT&#26377;&#28508;&#21147;&#34920;&#29616;&#26356;&#22909;&#65292;&#20174;&#32780;&#33719;&#24471;&#23545;&#20854;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23436;&#21892;&#20102;StepGame&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#27169;&#22411;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;GPT&#22312;&#32463;&#36807;&#20462;&#27491;&#30340;&#22522;&#20934;&#19978;&#30340;&#31354;&#38388;&#25512;&#29702;&#24615;&#33021;&#65292;&#22312;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26144;&#23556;&#21040;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#22312;&#22810;&#36339;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#32570;&#38519;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has made remarkable progress across various domains, with large language models like ChatGPT gaining substantial attention for their human-like text-generation capabilities. Despite these achievements, spatial reasoning remains a significant challenge for these models. Benchmarks like StepGame evaluate AI spatial reasoning, where ChatGPT has shown unsatisfactory performance. However, the presence of template errors in the benchmark has an impact on the evaluation results. Thus there is potential for ChatGPT to perform better if these template errors are addressed, leading to more accurate assessments of its spatial reasoning capabilities. In this study, we refine the StepGame benchmark, providing a more accurate dataset for model evaluation. We analyze GPT's spatial reasoning performance on the rectified benchmark, identifying proficiency in mapping natural language text to spatial relations but limitations in multi-hop reasoning. We provide a flawless solu
&lt;/p&gt;</description></item><item><title>Grimoire&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLEICL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#20363;&#20013;&#23398;&#20064;&#24182;&#23558;&#23398;&#21040;&#30340;&#25216;&#33021;&#20256;&#36882;&#32473;&#24369;&#35821;&#35328;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;ICL&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#24369;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#19982;&#24378;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;ICL&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.03385</link><description>&lt;p&gt;
Grimoire&#26159;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Grimoire is All You Need for Enhancing Large Language Models. (arXiv:2401.03385v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03385
&lt;/p&gt;
&lt;p&gt;
Grimoire&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLEICL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#20363;&#20013;&#23398;&#20064;&#24182;&#23558;&#23398;&#21040;&#30340;&#25216;&#33021;&#20256;&#36882;&#32473;&#24369;&#35821;&#35328;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;ICL&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#24369;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#19982;&#24378;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;ICL&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#36890;&#36807;&#25552;&#20379;&#19968;&#32452;&#23569;&#26679;&#20363;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#20851;&#38190;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#30001;&#20110;&#27169;&#22411;&#26550;&#26500;&#12289;&#23398;&#20064;&#25968;&#25454;&#30340;&#37327;&#21644;&#21442;&#25968;&#30340;&#22823;&#23567;&#31561;&#22240;&#32032;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#27169;&#22411;&#30340;&#21442;&#25968;&#22823;&#23567;&#36234;&#22823;&#65292;&#23398;&#20064;&#25968;&#25454;&#36234;&#24191;&#27867;&#65292;&#20854;ICL&#33021;&#21147;&#36234;&#24378;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;SLEICL&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#24378;&#35821;&#35328;&#27169;&#22411;&#20174;&#31034;&#20363;&#20013;&#23398;&#20064;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#23398;&#21040;&#30340;&#25216;&#33021;&#24635;&#32467;&#21644;&#20256;&#36882;&#32473;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#21644;&#24212;&#29992;&#12290;&#36825;&#30830;&#20445;&#20102;ICL&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#26524;&#12290;&#19982;&#30452;&#25509;&#20351;&#24369;&#35821;&#35328;&#27169;&#22411;&#20174;&#25552;&#31034;&#31034;&#20363;&#20013;&#23398;&#20064;&#30456;&#27604;&#65292;SLEICL&#38477;&#20302;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;ICL&#38590;&#24230;&#12290;&#25105;&#20204;&#22312;&#22810;&#36798;&#20843;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24369;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;SLEICL&#26041;&#27861;&#33719;&#24471;&#19982;&#24378;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;ICL&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context Learning (ICL) is one of the key methods for enhancing the performance of large language models on specific tasks by providing a set of few-shot examples. However, the ICL capability of different types of models shows significant variation due to factors such as model architecture, volume of learning data, and the size of parameters. Generally, the larger the model's parameter size and the more extensive the learning data, the stronger its ICL capability. In this paper, we propose a method SLEICL that involves learning from examples using strong language models and then summarizing and transferring these learned skills to weak language models for inference and application. This ensures the stability and effectiveness of ICL. Compared to directly enabling weak language models to learn from prompt examples, SLEICL reduces the difficulty of ICL for these models. Our experiments, conducted on up to eight datasets with five language models, demonstrate that weak language models a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02987</link><description>&lt;p&gt;
&#20320;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#25913;&#36827;&#21527;&#65311;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#31561;&#39046;&#22495;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#22914;&#20309;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#19982;&#27599;&#20010;&#23454;&#20307;&#30456;&#20851;&#30340;&#20803;&#29305;&#24449;&#20316;&#20026;&#19990;&#30028;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#21644;&#20803;&#29305;&#24449;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20855;&#26377;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained models has significantly impacted from Natural Language Processing (NLP) and Computer Vision to relational datasets. Traditionally, these models are assessed through fine-tuned downstream tasks. However, this raises the question of how to evaluate these models more efficiently and more effectively. In this study, we explore a novel approach where we leverage the meta features associated with each entity as a source of worldly knowledge and employ entity representations from the models. We propose using the consistency between these representations and the meta features as a metric for evaluating pretrained models. Our method's effectiveness is demonstrated across various domains, including models with relational datasets, large language models and images models.
&lt;/p&gt;</description></item><item><title>Cheetah&#26159;&#19968;&#20010;&#38754;&#21521;517&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#24688;&#24403;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#20419;&#36827;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.01053</link><description>&lt;p&gt;
Cheetah: 517&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cheetah: Natural Language Generation for 517 African Languages. (arXiv:2401.01053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01053
&lt;/p&gt;
&lt;p&gt;
Cheetah&#26159;&#19968;&#20010;&#38754;&#21521;517&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#24688;&#24403;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#20419;&#36827;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20219;&#21153;&#26469;&#35828;&#65292;&#38750;&#27954;&#35821;&#35328;&#36164;&#28304;&#31232;&#32570;&#26159;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104; (NLG)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Cheetah&#65292;&#19968;&#20010;&#38754;&#21521;&#38750;&#27954;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;NLG&#35821;&#35328;&#27169;&#22411;&#12290;Cheetah&#25903;&#25345;517&#31181;&#38750;&#27954;&#35821;&#35328;&#21644;&#35821;&#35328;&#21464;&#20307;&#65292;&#35299;&#20915;&#20102;NLG&#36164;&#28304;&#21294;&#20047;&#38382;&#39064;&#65292;&#24182;&#20026;&#20419;&#36827;&#35821;&#35328;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#19971;&#20010;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;&#32508;&#21512;&#35780;&#20272;&#35777;&#26126;&#20102;Cheetah&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#19971;&#20010;&#20219;&#21153;&#20013;&#30340;&#20116;&#20010;&#20219;&#21153;&#20013;&#65292;Cheetah&#30340;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#38750;&#27954;&#35821;&#35328;&#20013;&#29983;&#25104;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#24688;&#24403;&#30340;&#25991;&#26412;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;Cheetah&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;Cheetah&#30340;&#24341;&#20837;&#23545;&#35821;&#35328;&#22810;&#26679;&#24615;&#20855;&#26377;&#28145;&#36828;&#30340;&#30410;&#22788;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#29305;&#23450;&#30340;&#38750;&#27954;&#35821;&#35328;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#30340;&#35821;&#35328;&#29983;&#25104;&#36873;&#25321;&#21644;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-resource African languages pose unique challenges for natural language processing (NLP) tasks, including natural language generation (NLG). In this paper, we develop Cheetah, a massively multilingual NLG language model for African languages. Cheetah supports 517 African languages and language varieties, allowing us to address the scarcity of NLG resources and provide a solution to foster linguistic diversity. We demonstrate the effectiveness of Cheetah through comprehensive evaluations across seven generation downstream tasks. In five of the seven tasks, Cheetah significantly outperforms other models, showcasing its remarkable performance for generating coherent and contextually appropriate text in a wide range of African languages. We additionally conduct a detailed human evaluation to delve deeper into the linguistic capabilities of Cheetah. The introduction of Cheetah has far-reaching benefits for linguistic diversity. By leveraging pretrained models and adapting them to specifi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20316;&#20026;&#38646;-shot&#20851;&#38190;&#35789;&#25552;&#21462;&#22120;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;ChatGPT&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#27169;&#22411;&#20173;&#26377;&#35768;&#22810;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2312.15156</link><description>&lt;p&gt;
&#20316;&#20026;&#38646;-shot&#20851;&#38190;&#35789;&#25552;&#21462;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#21021;&#27493;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Zero-Shot Keyphrase Extractors: A Preliminary Empirical Study. (arXiv:2312.15156v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#20316;&#20026;&#38646;-shot&#20851;&#38190;&#35789;&#25552;&#21462;&#22120;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;ChatGPT&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#27169;&#22411;&#20173;&#26377;&#35768;&#22810;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;-shot&#20851;&#38190;&#35789;&#25552;&#21462;&#26088;&#22312;&#36890;&#36807;&#27809;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#35757;&#32451;&#26469;&#26500;&#24314;&#20851;&#38190;&#35789;&#25552;&#21462;&#22120;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20854;&#20013;&#28041;&#21450;&#21040;&#30340;&#20154;&#24037;&#24178;&#39044;&#26377;&#38480;&#12290;&#38646;-shot&#35774;&#32622;&#33021;&#22815;&#39640;&#25928;&#20943;&#23569;&#25968;&#25454;&#26631;&#27880;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#65292;&#22240;&#27492;&#20855;&#26377;&#25361;&#25112;&#20294;&#20540;&#24471;&#30740;&#31350;&#12290;&#26368;&#36817;&#65292;&#20851;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;ChatGPT&#21644;ChatGLM&#65289;&#22312;&#38646;-shot&#35774;&#32622;&#19978;&#23637;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#25506;&#32034;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21457;&#20986;&#25552;&#31034;&#26469;&#26500;&#24314;&#24378;&#22823;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#27169;&#22411;&#30456;&#27604;&#65292;ChatGPT&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#20013;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot keyphrase extraction aims to build a keyphrase extractor without training by human-annotated data, which is challenging due to the limited human intervention involved. Challenging but worthwhile, zero-shot setting efficiently reduces the time and effort that data labeling takes. Recent efforts on pre-trained large language models (e.g., ChatGPT and ChatGLM) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this paper, we ask whether strong keyphrase extraction models can be constructed by directly prompting the large language model ChatGPT. Through experimental results, it is found that ChatGPT still has a lot of room for improvement in the keyphrase extraction task compared to existing state-of-the-art unsupervised and supervised models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; HyperPIE &#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#36229;&#21442;&#25968;&#20449;&#24687;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#35780;&#20272;&#22810;&#31181;&#27169;&#22411;&#65292;&#21253;&#25324;BERT&#24494;&#35843;&#27169;&#22411;&#21644;&#20116;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20851;&#31995;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#25552;&#21462;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2312.10638</link><description>&lt;p&gt;
HyperPIE: &#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#36229;&#21442;&#25968;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
HyperPIE: Hyperparameter Information Extraction from Scientific Publications. (arXiv:2312.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; HyperPIE &#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#36229;&#21442;&#25968;&#20449;&#24687;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#35780;&#20272;&#22810;&#31181;&#27169;&#22411;&#65292;&#21253;&#25324;BERT&#24494;&#35843;&#27169;&#22411;&#21644;&#20116;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20851;&#31995;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#25552;&#21462;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35770;&#25991;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#26159;&#23454;&#29616;&#31185;&#23398;&#30693;&#35782;&#26426;&#22120;&#21487;&#35835;&#21270;&#30340;&#20851;&#38190;&#12290;&#25552;&#21462;&#20986;&#30340;&#20449;&#24687;&#21487;&#20197;&#20419;&#36827;&#23398;&#26415;&#25628;&#32034;&#12289;&#20915;&#31574;&#21046;&#23450;&#21644;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#12290;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#28085;&#30422;&#30340;&#19968;&#31867;&#37325;&#35201;&#20449;&#24687;&#26159;&#36229;&#21442;&#25968;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36229;&#21442;&#25968;&#20449;&#24687;&#25552;&#21462;&#65288;HyperPIE&#65289;&#24418;&#24335;&#21270;&#20026;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;&#26469;&#28085;&#30422;&#21508;&#31181;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#31185;&#30340;&#35770;&#25991;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#22522;&#20110;BERT&#30340;&#24494;&#35843;&#27169;&#22411;&#20197;&#21450;&#20116;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;GPT-3.5&#12289;GALACTICA&#12289;Falcon&#12289;Vicuna&#21644;WizardLM&#12290;&#23545;&#20110;&#24494;&#35843;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;F1&#20540;&#25552;&#21319;&#20102;29%&#12290;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;YAML&#36755;&#20986;&#36827;&#34892;&#32467;&#26500;&#21270;&#25968;&#25454;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;
&lt;/p&gt;
&lt;p&gt;
Automatic extraction of information from publications is key to making scientific knowledge machine readable at a large scale. The extracted information can, for example, facilitate academic search, decision making, and knowledge graph construction. An important type of information not covered by existing approaches is hyperparameters. In this paper, we formalize and tackle hyperparameter information extraction (HyperPIE) as an entity recognition and relation extraction task. We create a labeled data set covering publications from a variety of computer science disciplines. Using this data set, we train and evaluate BERT-based fine-tuned models as well as five large language models: GPT-3.5, GALACTICA, Falcon, Vicuna, and WizardLM. For fine-tuned models, we develop a relation extraction approach that achieves an improvement of 29% F1 over a state-of-the-art baseline. For large language models, we develop an approach leveraging YAML output for structured data extraction, which achieves a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#65292;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#34892;&#20026;&#20934;&#21017;&#24182;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#65292;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2312.04889</link><description>&lt;p&gt;
KwaiAgents&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#65292;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#34892;&#20026;&#20934;&#21017;&#24182;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#65292;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30001;&#20110;&#22909;&#22855;&#24515;&#30340;&#39537;&#20351;&#65292;&#19981;&#26029;&#25506;&#32034;&#21644;&#29702;&#35299;&#21608;&#22260;&#30340;&#19990;&#30028;&#65292;&#20174;&#32780;&#21457;&#26126;&#20102;&#21508;&#31181;&#24037;&#20855;&#26469;&#28385;&#36275;&#36825;&#31181;&#22909;&#22855;&#24515;&#12290;&#23613;&#31649;&#20154;&#31867;&#26080;&#27861;&#22312;&#22823;&#33041;&#20013;&#22788;&#29702;&#21644;&#35760;&#24518;&#22823;&#37327;&#20449;&#24687;&#65292;&#20294;&#22312;&#25209;&#21028;&#24605;&#32500;&#12289;&#35268;&#21010;&#12289;&#21453;&#24605;&#20197;&#21450;&#21033;&#29992;&#29616;&#26377;&#24037;&#20855;&#19982;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#35299;&#37322;&#26041;&#38754;&#21331;&#36234;&#20986;&#33394;&#65292;&#20351;&#20854;&#33021;&#22815;&#39640;&#25928;&#22320;&#23547;&#25214;&#31572;&#26696;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#27493;&#34920;&#26126;&#65292;&#26426;&#22120;&#21487;&#33021;&#20063;&#20855;&#22791;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#21442;&#25968;&#25968;&#37327;&#21463;&#38480;&#65292;&#20063;&#33021;&#23637;&#31034;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#22312; KwaiAgents &#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLM&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#30340;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#12289;&#34892;&#20026;&#20934;&#21017;&#21644;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#12290;&#26234;&#33021;&#20307;&#36824;&#21487;&#20197;&#26356;&#26032;&#26597;&#35810;&#32467;&#26524;&#65292;&#19982;&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;ASR&#30340;&#25361;&#25112;&#65306;&#19968;&#31181;&#26159;&#21033;&#29992;Bark&#27169;&#22411;&#21644;Meta&#30340;enCodec&#21644;&#39044;&#35757;&#32451;HuBert&#27169;&#22411;&#65292;&#19968;&#31181;&#26159;&#37319;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#38899;&#36716;&#25442;(RVC)&#21644;Ozen&#24037;&#20855;&#21253;&#12290;&#36825;&#20123;&#26041;&#27861;&#20026;ASR&#25216;&#26415;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#20026;&#26500;&#24314;&#23450;&#21046;&#21270;Common Voice&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2311.14836</link><description>&lt;p&gt;
&#33258;&#23450;&#20041;&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;ASR&#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110;Bark&#21644;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Custom Data Augmentation for low resource ASR using Bark and Retrieval-Based Voice Conversion. (arXiv:2311.14836v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;ASR&#30340;&#25361;&#25112;&#65306;&#19968;&#31181;&#26159;&#21033;&#29992;Bark&#27169;&#22411;&#21644;Meta&#30340;enCodec&#21644;&#39044;&#35757;&#32451;HuBert&#27169;&#22411;&#65292;&#19968;&#31181;&#26159;&#37319;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#38899;&#36716;&#25442;(RVC)&#21644;Ozen&#24037;&#20855;&#21253;&#12290;&#36825;&#20123;&#26041;&#27861;&#20026;ASR&#25216;&#26415;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#20026;&#26500;&#24314;&#23450;&#21046;&#21270;Common Voice&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#23450;&#21046;&#21270;&#30340;Common Voice&#25968;&#25454;&#38598;&#65292;&#20197;&#24212;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;&#21360;&#22320;&#35821;&#30340;&#25361;&#25112;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;Suno&#24320;&#21457;&#30340;&#22522;&#20110;Transformer&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;Bark&#65292;&#32467;&#21512;&#20102;Meta&#30340;enCodec&#21644;&#39044;&#35757;&#32451;&#30340;HuBert&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;Bark&#30340;&#24615;&#33021;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#38899;&#36716;&#25442;(RVC)&#65292;&#24182;&#20351;&#29992;Ozen&#24037;&#20855;&#21253;&#36827;&#34892;&#25968;&#25454;&#20934;&#22791;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20026;ASR&#25216;&#26415;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#20026;&#35299;&#20915;&#26500;&#24314;&#23450;&#21046;&#21270;Common Voice&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#20026;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#30340;&#39640;&#36136;&#37327;&#20010;&#24615;&#21270;&#35821;&#38899;&#29983;&#25104;&#25552;&#20379;&#20102;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes two innovative methodologies to construct customized Common Voice datasets for low-resource languages like Hindi. The first methodology leverages Bark, a transformer-based text-to-audio model developed by Suno, and incorporates Meta's enCodec and a pre-trained HuBert model to enhance Bark's performance. The second methodology employs Retrieval-Based Voice Conversion (RVC) and uses the Ozen toolkit for data preparation. Both methodologies contribute to the advancement of ASR technology and offer valuable insights into addressing the challenges of constructing customized Common Voice datasets for under-resourced languages. Furthermore, they provide a pathway to achieving high-quality, personalized voice generation for a range of applications.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#23494;&#24230;&#20272;&#35745;&#30340;&#35282;&#24230;&#35299;&#37322;&#23398;&#20064;&#25104;&#23545;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#27880;&#37322;&#32773;&#30340;&#38544;&#21547;&#20559;&#22909;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2311.14115</link><description>&lt;p&gt;
&#20174;&#25104;&#23545;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#30340;&#23494;&#24230;&#20272;&#35745;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A density estimation perspective on learning from pairwise human preferences. (arXiv:2311.14115v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14115
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#23494;&#24230;&#20272;&#35745;&#30340;&#35282;&#24230;&#35299;&#37322;&#23398;&#20064;&#25104;&#23545;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#27880;&#37322;&#32773;&#30340;&#38544;&#21547;&#20559;&#22909;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;LHF&#65289;--&#23588;&#20854;&#26159;&#20174;&#25104;&#23545;&#20559;&#22909;&#23398;&#20064;--&#26368;&#36817;&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#22823;&#22810;&#23558;&#20854;&#26694;&#26550;&#20026;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#23558;LLM&#35270;&#20026;&#19968;&#20010;&#31574;&#30053;&#65292;&#24182;&#22312;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#32422;&#26463;&#19979;&#36827;&#34892;&#35843;&#25972;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#35299;&#37322;&#65292;&#23427;&#20197;&#25104;&#23545;&#20559;&#22909;&#30340;&#29983;&#25104;&#36807;&#31243;&#20026;&#20013;&#24515;&#65292;&#24182;&#23558;LHF&#35270;&#20026;&#19968;&#20010;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#23545;&#20110;&#36890;&#36807;&#20559;&#22909;&#34892;&#20026;&#20998;&#24067;&#26041;&#31243;&#23450;&#20041;&#30340;&#19968;&#31867;&#29983;&#25104;&#36807;&#31243;&#65292;&#36890;&#36807;&#25104;&#23545;&#20559;&#22909;&#35757;&#32451;&#22870;&#21169;&#20989;&#25968;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#27880;&#37322;&#32773;&#30340;&#38544;&#21547;&#20559;&#22909;&#20998;&#24067;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#8220;&#26631;&#27880;&#32773;&#38169;&#35823;&#8221;&#30340;&#30740;&#31350;&#32467;&#26524;--&#21363;&#38169;&#35823;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on "annotator misspecification" -failure cases where wro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#31867;&#20284;&#20154;&#31867;&#21457;&#23637;&#25968;&#25454;&#35821;&#26009;&#24211;&#23545;LLMs&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#19982;&#20799;&#31461;&#35266;&#30475;&#30340;&#20196;&#29260;&#25968;&#37327;&#30456;&#20284;&#30340;&#26041;&#24335;&#65292;&#35780;&#20272;&#20102;LLMs&#23398;&#20064;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#25552;&#20379;&#24378;&#22823;&#30340;&#22522;&#20934;&#21644;&#23545;&#20219;&#21153;&#32452;&#32455;&#32773;&#25552;&#20379;&#30340;RoBERTa&#22522;&#20934;&#30340;&#22797;&#21046;&#23581;&#35797;&#12290;</title><link>http://arxiv.org/abs/2311.04666</link><description>&lt;p&gt;
&#20351;&#29992;&#31867;&#20284;&#20154;&#31867;&#21457;&#23637;&#25968;&#25454;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
Pre-training LLMs using human-like development data corpus. (arXiv:2311.04666v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#31867;&#20284;&#20154;&#31867;&#21457;&#23637;&#25968;&#25454;&#35821;&#26009;&#24211;&#23545;LLMs&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#19982;&#20799;&#31461;&#35266;&#30475;&#30340;&#20196;&#29260;&#25968;&#37327;&#30456;&#20284;&#30340;&#26041;&#24335;&#65292;&#35780;&#20272;&#20102;LLMs&#23398;&#20064;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#25552;&#20379;&#24378;&#22823;&#30340;&#22522;&#20934;&#21644;&#23545;&#20219;&#21153;&#32452;&#32455;&#32773;&#25552;&#20379;&#30340;RoBERTa&#22522;&#20934;&#30340;&#22797;&#21046;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#25512;&#29702;&#21644;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;LLMs&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#20250;&#26597;&#30475;&#22823;&#37327;&#30340;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#12290;BabyLM&#20849;&#20139;&#20219;&#21153;&#23558;LLM&#30340;&#39044;&#35757;&#32451;&#19982;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#36827;&#34892;&#27604;&#36739;&#65292;13&#23681;&#23401;&#23376;&#30475;&#21040;&#30340;&#20196;&#29260;&#25968;&#37327;&#27604;LLMs&#30475;&#21040;&#30340;&#25968;&#37327;&#35201;&#23567;&#24471;&#22810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;LLMs&#33021;&#22815;&#23398;&#20064;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#26041;&#38754;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20351;&#29992;&#30340;&#20196;&#29260;&#25968;&#37327;&#19982;&#20799;&#31461;&#30475;&#21040;&#30340;&#24046;&#19981;&#22810;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#24378;&#22823;&#30340;&#22522;&#20934;&#65307;&#19981;&#21516;&#30340;&#26550;&#26500;&#12289;&#35780;&#20272;&#19981;&#21516;&#26102;&#26399;&#24615;&#33021;&#21464;&#21270;&#21644;&#25253;&#21578;&#30340;&#39044;&#35757;&#32451;&#25351;&#26631;&#65292;&#20197;&#21450;&#23581;&#35797;&#26494;&#25955;&#22797;&#21046;&#20219;&#21153;&#32452;&#32455;&#32773;&#25552;&#20379;&#30340;RoBERTa&#22522;&#20934;&#20197;&#35266;&#23519;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#22797;&#29616;&#24615;&#23545;&#35757;&#32451;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20005;&#26684;&#21644;&#20005;&#26684;&#23567;&#35268;&#27169;&#36335;&#24452;&#30340;&#25552;&#20132;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Large Language Models (LLMs) have shown success in a diverse set of language inference and understanding tasks. The pre-training stage of LLMs looks at a large corpus of raw textual data. The BabyLM shared task compares LLM pre-training to human language acquisition, where the number of tokens seen by 13-year-old kids is magnitudes smaller than the number of tokens seen by LLMs. In this work, we pre-train and evaluate LLMs on their ability to learn contextual word representations using roughly the same number of tokens as seen by children. We provide a strong set of baselines; with different architectures, evaluation of changes in performance across epochs, and reported pre-training metrics for the strict small and strict tracks of the task. We also try to loosely replicate the RoBERTa baseline given by the task organizers to observe the training robustness to hyperparameter selection and replicability. We provide the submission details to the strict and strict-small tracks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#19978;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02567</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;VQA&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#19978;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#20986;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;8&#24180;&#21518;&#65292;&#20934;&#30830;&#29575;&#20173;&#28982;&#26159;&#33258;&#21160;&#35780;&#20272;&#30340;&#20027;&#35201;&#25351;&#26631;&#12290;&#22312;IID&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;VQA&#20934;&#30830;&#24230;&#19968;&#30452;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#31038;&#21306;&#27491;&#22312;&#36716;&#21521;&#24320;&#25918;&#24335;&#29983;&#25104;&#27169;&#22411;&#21644;OOD&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#33539;&#24335;&#20013;&#65292;&#29616;&#26377;&#30340;VQA&#20934;&#30830;&#24230;&#25351;&#26631;&#36807;&#20110;&#20005;&#26684;&#65292;&#20302;&#20272;&#20102;VQA&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#33258;&#21160;VQA&#24230;&#37327;&#65292;&#20316;&#20026;&#20154;&#31867;&#21028;&#26029;&#30340;&#20195;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;VQA&#24230;&#37327;&#12290;&#25105;&#20204;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#19968;&#20010;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#25351;&#31034;&#26681;&#25454;&#19968;&#32452;&#21442;&#32771;&#31572;&#26696;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#22312;&#20960;&#20010;VQA&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We ho
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#35780;&#20998;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#20154;&#31867;&#24494;&#22937;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;LLMs&#23545;&#38544;&#21947;&#29702;&#35299;&#33021;&#21147;&#26377;&#25152;&#25913;&#21892;&#65292;&#20294;&#23545;&#35773;&#21050;&#29702;&#35299;&#33021;&#21147;&#30340;&#25913;&#36827;&#24182;&#26410;&#35266;&#23519;&#21040;&#12290;</title><link>http://arxiv.org/abs/2309.10744</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#31579;&#36873;&#27979;&#35797;&#29702;&#35299;&#38544;&#21947;&#21644;&#35773;&#21050;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language models' ability to understand metaphor and sarcasm using a screening test for Asperger syndrome. (arXiv:2309.10744v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#35780;&#20998;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#20154;&#31867;&#24494;&#22937;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;LLMs&#23545;&#38544;&#21947;&#29702;&#35299;&#33021;&#21147;&#26377;&#25152;&#25913;&#21892;&#65292;&#20294;&#23545;&#35773;&#21050;&#29702;&#35299;&#33021;&#21147;&#30340;&#25913;&#36827;&#24182;&#26410;&#35266;&#23519;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#21644;&#35773;&#21050;&#26159;&#25105;&#20204;&#39640;&#24230;&#36827;&#21270;&#30340;&#31038;&#20132;&#27807;&#36890;&#25216;&#24039;&#30340;&#29645;&#36149;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#30340;&#20799;&#31461;&#20247;&#25152;&#21608;&#30693;&#22312;&#29702;&#35299;&#35773;&#21050;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#20182;&#20204;&#20855;&#26377;&#36275;&#22815;&#29702;&#35299;&#38544;&#21947;&#30340;&#21475;&#35821;&#26234;&#21830;&#27700;&#24179;&#12290;&#37492;&#20110;&#27492;&#65292;&#24050;&#32463;&#20351;&#29992;&#20102;&#19968;&#20010;&#35780;&#20998;&#27979;&#35797;&#26469;&#35780;&#20272;&#29702;&#35299;&#38544;&#21947;&#21644;&#35773;&#21050;&#30340;&#33021;&#21147;&#65292;&#20197;&#21306;&#20998;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#21644;&#20854;&#20182;&#34920;&#29616;&#30456;&#20284;&#22806;&#37096;&#34892;&#20026;&#30340;&#30151;&#29366;&#65288;&#20363;&#22914;&#27880;&#24847;&#21147;&#32570;&#38519;/&#22810;&#21160;&#38556;&#30861;&#65289;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26631;&#20934;&#21270;&#27979;&#35797;&#26469;&#30740;&#31350;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#20154;&#31867;&#24494;&#22937;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#29702;&#35299;&#38544;&#21947;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#20294;&#24182;&#27809;&#26377;&#35266;&#23519;&#21040;&#23545;&#35773;&#21050;&#29702;&#35299;&#30340;&#25913;&#36827;&#12290;&#36825;&#24847;&#21619;&#30528;&#26377;&#24517;&#35201;&#37319;&#21462;&#20854;&#20182;&#26041;&#27861;&#26469;&#20351;LLMs&#20855;&#22791;&#29702;&#35299;&#35773;&#21050;&#30340;&#33021;&#21147;&#65292;&#36825;&#24050;&#19982;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaphors and sarcasm are precious fruits of our highly-evolved social communication skills. However, children with Asperger syndrome are known to have difficulties in comprehending sarcasm, even if they possess a certain level of verbal IQ sufficient for understanding metaphors. Given that, a screening test that scores the ability to understand metaphor and sarcasm has been used to differentiate Asperger syndrome from other symptoms exhibiting akin external behaviors (e.g., attention-deficit/hyperactivity disorder). This study uses the standardized test to examine the capability of recent large language models (LLMs) in understanding human nuanced communication. The results divulged that, whereas their ability to comprehend metaphors has been improved with the increase of the number of model parameters, the improvement in sarcasm understanding was not observed. This implies that an alternative approach is imperative to imbue LLMs with the capacity to grasp sarcasm, which has been asso
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;SIR&#27169;&#22411;&#30740;&#31350;&#20102;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#20581;&#24247;&#24433;&#21709;&#12289;&#25439;&#22833;&#21644;&#25764;&#31163;&#19977;&#20010;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;SIR&#29702;&#35770;&#25506;&#32034;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.05281</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#26131;&#24863;-&#24863;&#26579;-&#24247;&#22797;&#65288;SIR&#65289;&#27169;&#22411;&#30740;&#31350;&#28798;&#23475;&#21709;&#24212;&#65306;&#20197;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season. (arXiv:2308.05281v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;SIR&#27169;&#22411;&#30740;&#31350;&#20102;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#20581;&#24247;&#24433;&#21709;&#12289;&#25439;&#22833;&#21644;&#25764;&#31163;&#19977;&#20010;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;SIR&#29702;&#35770;&#25506;&#32034;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#28798;&#23475;&#21709;&#24212;&#23545;&#21463;&#24433;&#21709;&#30340;&#31038;&#21306;&#33267;&#20851;&#37325;&#35201;&#12290;&#24212;&#24613;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#22312;&#28798;&#23475;&#26399;&#38388;&#22312;&#20102;&#35299;&#31038;&#21306;&#25152;&#38754;&#20020;&#38382;&#39064;&#30340;&#21487;&#38752;&#21644;&#21450;&#26102;&#30340;&#25351;&#26631;&#19978;&#23558;&#21463;&#30410;&#20110;&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#30340;&#20016;&#23500;&#25968;&#25454;&#26469;&#28304;&#12290;&#31038;&#20132;&#23186;&#20307;&#21487;&#20197;&#21453;&#26144;&#20844;&#20247;&#20851;&#27880;&#21644;&#38656;&#27714;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#65292;&#20197;&#20102;&#35299;&#19981;&#26029;&#28436;&#21464;&#30340;&#24773;&#20917;&#24182;&#20248;&#21270;&#36164;&#28304;&#37197;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#65288;BERT&#65289;&#20027;&#39064;&#24314;&#27169;&#23545;Twitter&#25968;&#25454;&#36827;&#34892;&#20027;&#39064;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#26102;&#38388;-&#31354;&#38388;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;2020&#24180;&#32654;&#22269;&#35199;&#37096;&#28779;&#28798;&#23395;&#26399;&#38388;&#22312;&#19981;&#21516;&#22320;&#21306;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#19977;&#20010;&#20027;&#39064;&#65306;&#8220;&#20581;&#24247;&#24433;&#21709;&#8221;&#65292;&#8220;&#25439;&#22833;&#8221;&#65292;&#8220;&#25764;&#31163;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#26131;&#24863;-&#24863;&#26579;-&#24247;&#22797;&#65288;SIR&#65289;&#29702;&#35770;&#26469;&#25506;&#32034;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;&#32467;&#26524;&#28165;&#26224;&#22320;&#26174;&#31034;&#20102;&#20027;&#39064;&#20256;&#25773;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective disaster response is critical for affected communities. Responders and decision-makers would benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, offering valuable insights for decision-makers to understand evolving situations and optimize resource allocation. We used Bidirectional Encoder Representations from Transformers (BERT) topic modeling to cluster topics from Twitter data. Then, we conducted a temporal-spatial analysis to examine the distribution of these topics across different regions during the 2020 western U.S. wildfire season. Our results show that Twitter users mainly focused on three topics:"health impact," "damage," and "evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter. The results displayed a clear re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LimeAttack&#30340;&#30828;&#26631;&#31614;&#25915;&#20987;&#31639;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21487;&#35299;&#37322;&#26041;&#27861;&#26469;&#36817;&#20284;&#21333;&#35789;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#28982;&#21518;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.00319</link><description>&lt;p&gt;
LimeAttack: &#26412;&#22320;&#21487;&#35299;&#37322;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#30828;&#26631;&#31614;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack. (arXiv:2308.00319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LimeAttack&#30340;&#30828;&#26631;&#31614;&#25915;&#20987;&#31639;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21487;&#35299;&#37322;&#26041;&#27861;&#26469;&#36817;&#20284;&#21333;&#35789;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#28982;&#21518;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#20808;&#21069;&#30340;&#25991;&#26412;&#23545;&#25239;&#24615;&#25915;&#20987;&#37319;&#29992;&#26799;&#24230;&#25110;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#35745;&#31639;&#21333;&#35789;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#24182;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#36825;&#20123;&#20449;&#24687;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#19968;&#20010;&#26356;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#19978;&#65292;&#21517;&#20026;&#30828;&#26631;&#31614;&#25915;&#20987;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#21482;&#33021;&#26597;&#35810;&#27169;&#22411;&#24182;&#33719;&#21462;&#31163;&#25955;&#30340;&#39044;&#27979;&#26631;&#31614;&#12290;&#29616;&#26377;&#30340;&#30828;&#26631;&#31614;&#25915;&#20987;&#31639;&#27861;&#24448;&#24448;&#36890;&#36807;&#38543;&#26426;&#26367;&#25442;&#26469;&#21021;&#22987;&#21270;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#28982;&#21518;&#21033;&#29992;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#20248;&#21270;&#23545;&#25239;&#25200;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#27169;&#22411;&#26597;&#35810;&#65292;&#24182;&#19988;&#25915;&#20987;&#25104;&#21151;&#29575;&#21463;&#21040;&#23545;&#25163;&#21021;&#22987;&#21270;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LimeAttack&#30340;&#26032;&#22411;&#30828;&#26631;&#31614;&#25915;&#20987;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#26412;&#22320;&#21487;&#35299;&#37322;&#26041;&#27861;&#26469;&#36817;&#20284;&#21333;&#35789;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#28982;&#21518;&#37319;&#29992;&#27874;&#26463;&#25628;&#32034;&#26469;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing models are vulnerable to adversarial examples. Previous textual adversarial attacks adopt gradients or confidence scores to calculate word importance ranking and generate adversarial examples. However, this information is unavailable in the real world. Therefore, we focus on a more realistic and challenging setting, named hard-label attack, in which the attacker can only query the model and obtain a discrete prediction label. Existing hard-label attack algorithms tend to initialize adversarial examples by random substitution and then utilize complex heuristic algorithms to optimize the adversarial perturbation. These methods require a lot of model queries and the attack success rate is restricted by adversary initialization. In this paper, we propose a novel hard-label attack algorithm named LimeAttack, which leverages a local explainable method to approximate word importance ranking, and then adopts beam search to find the optimal solution. Extensive experi
&lt;/p&gt;</description></item><item><title>BEER^2&#26159;&#19968;&#31181;&#29992;&#20110;Retriever&#21644;Reader&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#65292;&#20849;&#21516;&#36827;&#27493;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;EL&#12290;</title><link>http://arxiv.org/abs/2306.12245</link><description>&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#30340;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#33539;&#24335;&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking. (arXiv:2306.12245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12245
&lt;/p&gt;
&lt;p&gt;
BEER^2&#26159;&#19968;&#31181;&#29992;&#20110;Retriever&#21644;Reader&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#65292;&#20849;&#21516;&#36827;&#27493;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;EL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#23427;&#30340;&#19968;&#33324;&#24418;&#24335;&#65288;&#21363;&#31471;&#21040;&#31471;EL&#65289;&#26088;&#22312;&#39318;&#20808;&#22312;&#32473;&#23450;&#36755;&#20837;&#25991;&#26723;&#20013;&#25214;&#21040;&#25552;&#21450;&#65292;&#24182;&#23558;&#25552;&#21450;&#38142;&#25509;&#21040;&#29305;&#23450;&#30693;&#35782;&#24211;&#20013;&#30340;&#30456;&#24212;&#23454;&#20307;&#12290;&#26368;&#36817;&#65292;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#33539;&#24335;&#20419;&#36827;&#20102;&#31471;&#21040;&#31471;EL&#30340;&#36827;&#23637;&#65292;&#21463;&#30410;&#20110;&#23494;&#38598;&#30340;&#23454;&#20307;&#26816;&#32034;&#21644;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20165;&#20197;&#27969;&#27700;&#32447;&#26041;&#24335;&#21333;&#29420;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#65292;&#24573;&#30053;&#20102;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20043;&#38388;&#20132;&#20114;&#24102;&#26469;&#30340;&#30410;&#22788;&#12290;&#20026;&#20102;&#20351;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#33539;&#24335;&#26356;&#23436;&#32654;&#22320;&#25191;&#34892;&#31471;&#21040;&#31471;EL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BEER$^2$&#65292;&#19968;&#31181;&#29992;&#20110;Retriever and Reader&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#25105;&#20204;&#35774;&#35745;&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;BEER$^2$&#25351;&#23548;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20114;&#30456;&#23398;&#20064;&#65292;&#20849;&#21516;&#36827;&#27493;&#65292;&#24182;&#26368;&#32456;&#23454;&#29616;&#31471;&#21040;&#31471;EL&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Linking (EL) is a fundamental task for Information Extraction and Knowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first find mentions in the given input document and then link the mentions to corresponding entities in a specific knowledge base. Recently, the paradigm of retriever-reader promotes the progress of end-to-end EL, benefiting from the advantages of dense entity retrieval and machine reading comprehension. However, the existing study only trains the retriever and the reader separately in a pipeline manner, which ignores the benefit that the interaction between the retriever and the reader can bring to the task. To advance the retriever-reader paradigm to perform more perfectly on end-to-end EL, we propose BEER$^2$, a Bidirectional End-to-End training framework for Retriever and Reader. Through our designed bidirectional end-to-end training, BEER$^2$ guides the retriever and the reader to learn from each other, make progress together, and ultimate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#37325;&#28857;&#20851;&#27880; BLIP2 &#27169;&#22411;&#65292;&#26469;&#25552;&#39640;&#38646;&#26679;&#26412; VQA &#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#38382;&#39064;&#27169;&#26495;&#30340;&#26377;&#25928;&#24615;&#12289;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#30340;&#20316;&#29992;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24433;&#21709;&#20197;&#21450;&#23558;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#34701;&#21512;&#30340;&#22909;&#22788;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#25972;&#21512;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#21487;&#20197;&#20419;&#36827; VQA &#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#32467;&#21512;&#20351;&#29992;&#26102;&#12290;</title><link>http://arxiv.org/abs/2306.09996</link><description>&lt;p&gt;
&#25506;&#31350;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#25552;&#31034;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering. (arXiv:2306.09996v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#37325;&#28857;&#20851;&#27880; BLIP2 &#27169;&#22411;&#65292;&#26469;&#25552;&#39640;&#38646;&#26679;&#26412; VQA &#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#38382;&#39064;&#27169;&#26495;&#30340;&#26377;&#25928;&#24615;&#12289;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#30340;&#20316;&#29992;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24433;&#21709;&#20197;&#21450;&#23558;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#34701;&#21512;&#30340;&#22909;&#22788;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#25972;&#21512;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#21487;&#20197;&#20419;&#36827; VQA &#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#32467;&#21512;&#20351;&#29992;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20855;&#22791;&#29702;&#35299;&#21644;&#25512;&#29702;&#35270;&#35273;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36817;&#26399;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;VQA&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22797;&#26434;&#32452;&#21512;&#38382;&#39064;&#21644;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#22914;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#26041;&#38754;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#20351;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;BLIP2&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;VQA&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;VQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#38382;&#39064;&#27169;&#26495;&#30340;&#26377;&#25928;&#24615;&#12289;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#30340;&#20316;&#29992;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24433;&#21709;&#20197;&#21450;&#23558;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#34701;&#21512;&#30340;&#22909;&#22788;&#12290;&#23613;&#31649;&#32467;&#26524;&#21508;&#24322;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#25972;&#21512;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#65288;&#22914;&#22270;&#20687;&#26631;&#39064;&#65289;&#21487;&#20197;&#20419;&#36827;VQA&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#32467;&#21512;&#20351;&#29992;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) is a challenging task that requires the ability to comprehend and reason with visual information. While recent vision-language models have made strides, they continue to struggle with zero-shot VQA, particularly in handling complex compositional questions and adapting to new domains i.e. knowledge-based reasoning. This paper explores the use of various prompting strategies, focusing on the BLIP2 model, to enhance zero-shot VQA performance. We conduct a comprehensive investigation across several VQA datasets, examining the effectiveness of different question templates, the role of few-shot exemplars, the impact of chain-of-thought (CoT) reasoning, and the benefits of incorporating image captions as additional visual cues. Despite the varied outcomes, our findings demonstrate that carefully designed question templates and the integration of additional visual cues, like image captions, can contribute to improved VQA performance, especially when used in conj
&lt;/p&gt;</description></item><item><title>BiomedGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#22312;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;16&#20010;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#21253;&#25324;&#36229;&#36807;&#20102;OpenAI&#30340;GPT-4V&#21644;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#21516;&#26102;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.17100</link><description>&lt;p&gt;
BiomedGPT&#65306;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#32479;&#19968;&#19988;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks. (arXiv:2305.17100v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17100
&lt;/p&gt;
&lt;p&gt;
BiomedGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#22312;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;16&#20010;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#21253;&#25324;&#36229;&#36807;&#20102;OpenAI&#30340;GPT-4V&#21644;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#21516;&#26102;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#21644;&#32500;&#25252;&#20013;&#19981;&#22815;&#28789;&#27963;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#32467;&#21512;&#29616;&#20195;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#20026;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26377;&#28508;&#21147;&#35299;&#37322;&#19981;&#21516;&#30340;&#21307;&#30103;&#27169;&#24577;&#65292;&#24182;&#20135;&#29983;&#22914;&#33258;&#30001;&#25991;&#26412;&#25253;&#21578;&#25110;&#30142;&#30149;&#35786;&#26029;&#31561;&#34920;&#36798;&#24615;&#36755;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BiomedGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#22810;&#26679;&#21270;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#24320;&#28304;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;BiomedGPT&#22312;26&#20010;&#25968;&#25454;&#38598;&#30340;&#20116;&#20010;&#20020;&#24202;&#37325;&#35201;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;16&#20010;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25918;&#23556;&#23398;&#20154;&#21592;&#35780;&#20272;&#20013;&#65292;&#23427;&#36229;&#36234;&#20102;OpenAI&#30340;GPT-4 with vision&#65288;GPT-4V&#65289;&#65292;&#24182;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#36229;&#36807;&#20102;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#27492;&#22806;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional task- and modality-specific artificial intelligence (AI) models are inflexible in real-world deployment and maintenance for biomedicine. At the same time, the growing availability of biomedical data, coupled with the advancements in modern multi-modal multi-task AI techniques, has paved the way for the emergence of generalist biomedical AI solutions. These solutions hold the potential to interpret different medical modalities and produce expressive outputs such as free-text reports or disease diagnosis. Here, we propose BiomedGPT, the first open-source and generalist visual language AI for diverse biomedical tasks. BiomedGPT achieved 16 state-of-the-art results across five clinically significant tasks on 26 datasets. Notably, it outperformed OpenAI's GPT-4 with vision (GPT-4V) in radiology human evaluation and surpassed Google's Med-PaLM M (12B) in breast cancer diagnosis and medical visual question answering. Moreover, BiomedGPT facilitates zero-shot transfer learning, gr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24378;&#35843;&#20102;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20010;&#24615;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LaMP&#65288;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#30340;&#20010;&#24615;&#21270;&#22522;&#20934;&#65289;&#65292;&#24182;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19971;&#39033;&#20010;&#24615;&#21270;&#20219;&#21153;&#20197;&#21450;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21033;&#29992;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#20854;&#29983;&#25104;&#32467;&#26524;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11406</link><description>&lt;p&gt;
LaMP&#65306;&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
LaMP: When Large Language Models Meet Personalization. (arXiv:2304.11406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24378;&#35843;&#20102;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20010;&#24615;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LaMP&#65288;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#30340;&#20010;&#24615;&#21270;&#22522;&#20934;&#65289;&#65292;&#24182;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19971;&#39033;&#20010;&#24615;&#21270;&#20219;&#21153;&#20197;&#21450;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21033;&#29992;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#20854;&#29983;&#25104;&#32467;&#26524;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#22312;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#39046;&#22495;&#30340;&#20010;&#24615;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;LaMP&#22522;&#20934;&#8212;&#8212;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#29983;&#25104;&#20010;&#24615;&#21270;&#36755;&#20986;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20856;&#33539;&#12290;LaMP&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#20219;&#21153;&#21644;&#27599;&#20010;&#29992;&#25143;&#30340;&#22810;&#20010;&#26465;&#30446;&#65292;&#21253;&#25324;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#21644;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#19971;&#20010;&#20010;&#24615;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20174;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#20013;&#26816;&#32034;&#20010;&#24615;&#21270;&#39033;&#30446;&#65292;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#22522;&#32447;&#38646;-shot&#21644;&#24494;&#35843;&#27169;&#22411;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#20010;&#20154;&#36164;&#26009;&#25193;&#23637;&#30340;LM&#20248;&#20110;&#19981;&#32771;&#34385;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper highlights the importance of personalization in the current state of natural language understanding and generation and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three classification and four text generation tasks. We also propose a retrieval augmentation approach that retrieves personalized items from user profiles to construct personalized prompts for large language models. Our baseline zero-shot and fine-tuned model results indicate that LMs utilizing profile augmentation outperform their counterparts that do not factor in profile information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#20010;&#24615;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#65288;PAA&#65289;&#26469;&#29983;&#25104;&#22522;&#20110;&#20010;&#24615;&#30340;&#19968;&#33268;&#24615;&#22238;&#24212;&#65292;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#20010;&#24615;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126; PAA &#26694;&#26550;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.15088</link><description>&lt;p&gt;
&#24102;&#26377;&#20010;&#24615;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Personalized Dialogue Generation with Persona-Adaptive Attention. (arXiv:2210.15088v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#20010;&#24615;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#65288;PAA&#65289;&#26469;&#29983;&#25104;&#22522;&#20110;&#20010;&#24615;&#30340;&#19968;&#33268;&#24615;&#22238;&#24212;&#65292;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#20010;&#24615;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126; PAA &#26694;&#26550;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20010;&#24615;&#21270;&#30340;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#26681;&#25454;&#21382;&#21490;&#19978;&#19979;&#25991;&#21644;&#39044;&#23450;&#20041;&#30340;&#20010;&#24615;&#29983;&#25104;&#19968;&#33268;&#30340;&#22238;&#24212;&#12290;&#19982;&#20256;&#32479;&#30340;&#23545;&#35805;&#29983;&#25104;&#19981;&#21516;&#65292;&#22522;&#20110;&#20010;&#24615;&#30340;&#23545;&#35805;&#38656;&#35201;&#32771;&#34385;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#20010;&#24615;&#20004;&#20010;&#26041;&#38754;&#65292;&#36825;&#23545;&#20110;&#19968;&#33268;&#30340;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#20010;&#24615;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#65288;PAA&#65289;&#65292;&#36890;&#36807;&#25105;&#20204;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#36866;&#24212;&#24615;&#22320;&#25972;&#21512;&#20102;&#26469;&#33258;&#20010;&#24615;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;PAA &#36824;&#24212;&#29992;&#20102;&#21160;&#24577;&#23631;&#34109;&#26426;&#21046;&#65292;&#19981;&#20165;&#21487;&#20197;&#20002;&#24323;&#19978;&#19979;&#25991;&#21644;&#20010;&#24615;&#30340;&#20887;&#20313;&#20449;&#24687;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#27491;&#21017;&#21270;&#26426;&#21046;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340; PAA &#26694;&#26550;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340; PAA &#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#34920;&#29616;&#21516;&#26679;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persona-based dialogue systems aim to generate consistent responses based on historical context and predefined persona. Unlike conventional dialogue generation, the persona-based dialogue needs to consider both dialogue context and persona, posing a challenge for coherent training. Specifically, this requires a delicate weight balance between context and persona. To achieve that, in this paper, we propose an effective framework with Persona-Adaptive Attention (PAA), which adaptively integrates the weights from the persona and context information via our designed attention. In addition, a dynamic masking mechanism is applied to the PAA to not only drop redundant information in context and persona but also serve as a regularization mechanism to avoid overfitting. Experimental results demonstrate the superiority of the proposed PAA framework compared to the strong baselines in both automatic and human evaluation. Moreover, the proposed PAA approach can perform equivalently well in a low-r
&lt;/p&gt;</description></item><item><title>BenchCLAMP&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#21477;&#27861;&#21644;&#35821;&#20041;&#35299;&#26512;&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#25324;&#19971;&#20010;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#21477;&#27861;&#35299;&#26512;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19981;&#21516;&#25968;&#25454;&#31574;&#30053;&#19979;&#30340;&#36164;&#28304;&#21010;&#20998;&#65292;&#24182;&#25903;&#25345;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#21644;&#31934;&#35843;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#36229;&#36807;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2206.10668</link><description>&lt;p&gt;
BenchCLAMP&#65306;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#21477;&#27861;&#21644;&#35821;&#20041;&#35299;&#26512;&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing. (arXiv:2206.10668v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10668
&lt;/p&gt;
&lt;p&gt;
BenchCLAMP&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#21477;&#27861;&#21644;&#35821;&#20041;&#35299;&#26512;&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#25324;&#19971;&#20010;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#21477;&#27861;&#35299;&#26512;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19981;&#21516;&#25968;&#25454;&#31574;&#30053;&#19979;&#30340;&#36164;&#28304;&#21010;&#20998;&#65292;&#24182;&#25903;&#25345;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#21644;&#31934;&#35843;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#36229;&#36807;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#36755;&#20986;&#34987;&#38480;&#21046;&#20026;&#26377;&#25928;&#30340;&#35821;&#20041;&#34920;&#31034;&#26102;&#65292;&#36890;&#36807;&#25552;&#31034;&#25110;&#31934;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#20041;&#35299;&#26512;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;BenchCLAMP&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#32422;&#26463;&#35821;&#35328;&#27169;&#22411;&#35299;&#26512;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19971;&#20010;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#21644;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#36755;&#20986;&#34920;&#31034;&#30340;&#21477;&#27861;&#35299;&#26512;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#21463;&#38480;&#35299;&#30721;&#30028;&#38754;&#65292;&#20165;&#29983;&#25104;&#36825;&#20123;&#25991;&#27861;&#21253;&#21547;&#30340;&#26377;&#25928;&#36755;&#20986;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#20302;&#12289;&#20013;&#12289;&#39640;&#36164;&#28304;&#21010;&#20998;&#65292;&#21487;&#20197;&#20934;&#30830;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#31574;&#30053;&#19979;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#25903;&#25345;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#21644;&#31934;&#35843;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;&#20004;&#20010;&#20165;&#36890;&#36807;API&#21487;&#29992;&#30340;GPT-3&#21464;&#20307;&#22312;&#20869;&#30340;&#20843;&#20010;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#36229;&#36807;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, that includes context-free grammars for seven semantic parsing datasets and two syntactic parsing datasets with varied output representations, as well as a constrained decoding interface to generate only valid outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports evaluation of language models using prompt-based learning as well as fine-tuning. We benchmark eight language models, including two GPT-3 variants available only through an API. Our experiments show that encoder-decoder pretrained language models can achieve similar performance or surpass state-of-the-a
&lt;/p&gt;</description></item></channel></rss>