<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACO&#30340;&#27169;&#24577;&#23545;&#25239;&#21644;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#30340;&#27169;&#24577;&#32570;&#22833;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#29983;&#25104;&#32570;&#22833;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36328;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#25913;&#21892;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06696</link><description>&lt;p&gt;
MACO: &#19968;&#31181;&#29992;&#20110;&#27169;&#24577;&#32570;&#22833;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#30340;&#27169;&#24577;&#23545;&#25239;&#21644;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MACO: A Modality Adversarial and Contrastive Framework for Modality-missing Multi-modal Knowledge Graph Completion. (arXiv:2308.06696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACO&#30340;&#27169;&#24577;&#23545;&#25239;&#21644;&#23545;&#27604;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#20013;&#30340;&#27169;&#24577;&#32570;&#22833;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#29983;&#25104;&#32570;&#22833;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#36328;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#25913;&#21892;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;MMKGC&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;MMKGC&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#23454;&#20307;&#20449;&#24687;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;KGC&#65289;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#22312;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#20013;&#21457;&#29616;&#26410;&#35266;&#23519;&#21040;&#30340;&#19977;&#20803;&#32452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24378;&#35843;&#35774;&#35745;&#20248;&#38597;&#30340;KGC&#27169;&#22411;&#20197;&#20419;&#36827;&#27169;&#24577;&#20132;&#20114;&#65292;&#24573;&#30053;&#20102;&#30693;&#35782;&#22270;&#20013;&#32570;&#22833;&#27169;&#24577;&#30340;&#30495;&#23454;&#38382;&#39064;&#12290;&#32570;&#22833;&#30340;&#27169;&#24577;&#20449;&#24687;&#38459;&#30861;&#20102;&#27169;&#24577;&#20132;&#20114;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#23545;&#25239;&#21644;&#23545;&#27604;&#26694;&#26550;&#65288;MACO&#65289;&#26469;&#35299;&#20915;MMKGC&#20013;&#30340;&#27169;&#24577;&#32570;&#22833;&#38382;&#39064;&#12290;MACO&#36890;&#36807;&#23545;&#25239;&#24615;&#22320;&#35757;&#32451;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#26469;&#29983;&#25104;&#21487;&#20197;&#25972;&#21512;&#21040;MMKGC&#27169;&#22411;&#20013;&#30340;&#32570;&#22833;&#27169;&#24577;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#26469;&#25552;&#39640;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#22312;&#20844;&#20849;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen significant advancements in multi-modal knowledge graph completion (MMKGC). MMKGC enhances knowledge graph completion (KGC) by integrating multi-modal entity information, thereby facilitating the discovery of unobserved triples in the large-scale knowledge graphs (KGs). Nevertheless, existing methods emphasize the design of elegant KGC models to facilitate modality interaction, neglecting the real-life problem of missing modalities in KGs. The missing modality information impedes modal interaction, consequently undermining the model's performance. In this paper, we propose a modality adversarial and contrastive framework (MACO) to solve the modality-missing problem in MMKGC. MACO trains a generator and discriminator adversarially to generate missing modality features that can be incorporated into the MMKGC model. Meanwhile, we design a cross-modal contrastive loss to improve the performance of the generator. Experiments on public benchmarks with further explorati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#26469;&#25552;&#39640;&#25688;&#35201;&#31579;&#36873;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#31216;&#20026;Bio-SIEVE&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#20294;&#22312;&#23433;&#20840;&#24615;&#20248;&#20808;&#22330;&#26223;&#19979;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#23581;&#35797;&#20102;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20294;&#21457;&#29616;&#20854;&#19981;&#33021;&#19982;&#21333;&#20219;&#21153;&#30340;Bio-SIEVE&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#36825;&#19968;&#30740;&#31350;&#26159;&#20026;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#19987;&#38376;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#36807;&#31243;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#12289;&#20195;&#30721;&#21644;DOI&#21015;&#34920;&#20197;&#20379;&#22797;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.06610</link><description>&lt;p&gt;
Bio-SIEVE&#65306;&#25506;&#32034;&#38024;&#23545;&#31995;&#32479;&#24615;&#35780;&#36848;&#33258;&#21160;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation. (arXiv:2308.06610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#26469;&#25552;&#39640;&#25688;&#35201;&#31579;&#36873;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#31216;&#20026;Bio-SIEVE&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#20294;&#22312;&#23433;&#20840;&#24615;&#20248;&#20808;&#22330;&#26223;&#19979;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#23581;&#35797;&#20102;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20294;&#21457;&#29616;&#20854;&#19981;&#33021;&#19982;&#21333;&#20219;&#21153;&#30340;Bio-SIEVE&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#36825;&#19968;&#30740;&#31350;&#26159;&#20026;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#19987;&#38376;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#36807;&#31243;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#12289;&#20195;&#30721;&#21644;DOI&#21015;&#34920;&#20197;&#20379;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#25104;&#26412;&#39640;&#12289;&#36164;&#28304;&#23494;&#38598;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25552;&#20379;&#35814;&#32454;&#30340;&#36873;&#25321;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#25903;&#25345;&#21644;&#35757;&#32451;&#20854;&#25191;&#34892;&#25991;&#29486;&#31579;&#36873;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;LLaMA&#21644;Guanaco&#27169;&#22411;&#36827;&#34892;&#20102;&#25351;&#20196;&#35843;&#20248;&#65292;&#20197;&#25191;&#34892;&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#30340;&#25688;&#35201;&#31579;&#36873;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;Bio-SIEVE&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;ChatGPT&#21644;&#32463;&#36807;&#35757;&#32451;&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#35843;&#25972;&#20026;&#20197;&#23433;&#20840;&#20026;&#20808;&#30340;&#22330;&#26223;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19982;Bio-SIEVE-Multi&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;PICO&#25552;&#21462;&#21644;&#25490;&#38500;&#25512;&#29702;&#31561;&#20219;&#21153;&#65292;&#20294;&#21457;&#29616;&#23427;&#26080;&#27861;&#36798;&#21040;&#21333;&#20219;&#21153;Bio-SIEVE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;Bio-SIEVE&#26159;&#20026;&#29983;&#29289;&#21307;&#23398;&#31995;&#32479;&#24615;&#35780;&#36848;&#36807;&#31243;&#19987;&#38376;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#21516;&#26102;&#36824;&#25506;&#32034;&#20102;&#20854;&#26410;&#26469;&#30340;&#21457;&#23637;&#26426;&#20250;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12289;&#20195;&#30721;&#21644;&#19968;&#20221;DOI&#21015;&#34920;&#20197;&#20379;&#37325;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical systematic reviews can be very costly and resource intensive. We explore how Large Language Models (LLMs) can support and be trained to perform literature screening when provided with a detailed set of selection criteria. Specifically, we instruction tune LLaMA and Guanaco models to perform abstract screening for medical systematic reviews. Our best model, Bio-SIEVE, outperforms both ChatGPT and trained traditional approaches, and generalises better across medical domains. However, there remains the challenge of adapting the model to safety-first scenarios. We also explore the impact of multi-task training with Bio-SIEVE-Multi, including tasks such as PICO extraction and exclusion reasoning, but find that it is unable to match single-task Bio-SIEVE's performance. We see Bio-SIEVE as an important step towards specialising LLMs for the biomedical systematic review process and explore its future developmental opportunities. We release our models, code and a list of DOIs to reconst
&lt;/p&gt;</description></item><item><title>VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.06595</link><description>&lt;p&gt;
VisIT-Bench: &#19968;&#20010;&#21463;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#21551;&#21457;&#30340;&#35270;&#35273;&#35821;&#35328;&#25351;&#31034;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use. (arXiv:2308.06595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06595
&lt;/p&gt;
&lt;p&gt;
VisIT-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20215;&#30495;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25351;&#31034;&#36981;&#24490;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#21508;&#31181;&#20219;&#21153;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#25551;&#36848;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VisIT-Bench&#65288;Visual InsTruction Benchmark&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20215;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20351;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#31034;&#36981;&#24490;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#36215;&#28857;&#26159;&#31574;&#21010;&#20102;70&#20010;&#8220;&#25351;&#31034;&#23478;&#26063;&#8221;&#65292;&#25105;&#20204;&#35748;&#20026;&#25351;&#31034;&#35843;&#20248;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#35299;&#20915;&#36825;&#20123;&#23478;&#26063;&#12290;&#20219;&#21153;&#19981;&#20165;&#38480;&#20110;VQAv2&#21644;COCO&#31561;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20174;&#22522;&#26412;&#35782;&#21035;&#21040;&#28216;&#25103;&#29609;&#27861;&#21644;&#21019;&#36896;&#24615;&#29983;&#25104;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#22312;&#31574;&#21010;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;592&#20010;&#27979;&#35797;&#26597;&#35810;&#65292;&#27599;&#20010;&#26597;&#35810;&#37117;&#24102;&#26377;&#19968;&#20010;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#31034;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#12290;&#36825;&#20123;&#25551;&#36848;&#23637;&#29616;&#20102;&#29305;&#23450;&#25351;&#31034;&#22240;&#32032;&#65292;&#20363;&#22914;&#23545;&#20110;&#35810;&#38382;&#24215;&#38754;&#23545;&#20110;&#36718;&#26885;&#29992;&#25143;&#30340;&#26131;&#35775;&#38382;&#24615;&#30340;&#25351;&#31034;&#65292;&#26465;&#20214;&#21270;&#30340;&#23383;&#24149;&#25551;&#36848;&#20102;&#26012;&#22369;/&#28508;&#22312;&#38556;&#30861;&#29289;&#12290;&#36825;&#20123;&#25551;&#36848;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#65306;1&#65289;&#25910;&#38598;&#27599;&#20010;&#23454;&#20363;&#30340;&#20154;&#24037;&#39564;&#35777;&#30340;&#21442;&#32771;&#36755;&#20986;&#65307;2&#65289;&#20351;&#29992;&#20165;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20505;&#36873;&#22810;&#27169;&#24577;&#29983;&#25104;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps be
&lt;/p&gt;</description></item><item><title>MT4CrossOIE&#26159;&#19968;&#31181;&#22810;&#38454;&#27573;&#35843;&#20248;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#35821;&#31181;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#12290;&#23427;&#36890;&#36807;&#21521;&#20849;&#20139;&#27169;&#22411;&#27880;&#20837;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#20302;&#31209;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#36827;&#34892;&#27169;&#22411;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2308.06552</link><description>&lt;p&gt;
MT4CrossOIE: &#22810;&#38454;&#27573;&#35843;&#20248;&#29992;&#20110;&#36328;&#35821;&#31181;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
MT4CrossOIE: Multi-stage Tuning for Cross-lingual Open Information Extraction. (arXiv:2308.06552v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06552
&lt;/p&gt;
&lt;p&gt;
MT4CrossOIE&#26159;&#19968;&#31181;&#22810;&#38454;&#27573;&#35843;&#20248;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#35821;&#31181;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#12290;&#23427;&#36890;&#36807;&#21521;&#20849;&#20139;&#27169;&#22411;&#27880;&#20837;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#20302;&#31209;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#36827;&#34892;&#27169;&#22411;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#31181;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#26088;&#22312;&#20174;&#22810;&#35821;&#35328;&#30340;&#21407;&#22987;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#20849;&#20139;&#30340;&#36328;&#35821;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#35821;&#35328;&#65292;&#20294;&#26410;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#29305;&#23450;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MT4CrossIE&#30340;&#26377;&#25928;&#22810;&#38454;&#27573;&#35843;&#20248;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21521;&#20849;&#20139;&#27169;&#22411;&#27880;&#20837;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#26469;&#22686;&#24378;&#36328;&#35821;&#31181;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#22312;&#22266;&#23450;&#32534;&#30721;&#22120;&#20013;&#35843;&#25972;&#36328;&#35821;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20849;&#20139;&#35821;&#20041;&#31354;&#38388;&#65288;&#20363;&#22914;&#23884;&#20837;&#30697;&#38453;&#65289;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#20248;&#21270;&#20854;&#20182;&#32452;&#20214;&#12290;&#32463;&#36807;&#36275;&#22815;&#30340;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#28151;&#21512;LoRAs&#20248;&#21270;&#22810;&#20010;&#39069;&#22806;&#30340;&#20302;&#31209;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#65292;&#20197;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#36328;&#35821;&#31181;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#38454;&#27573;&#25552;&#31034;&#26469;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27880;&#37322;&#22810;&#35821;&#31181;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual open information extraction aims to extract structured information from raw text across multiple languages. Previous work uses a shared cross-lingual pre-trained model to handle the different languages but underuses the potential of the language-specific representation. In this paper, we propose an effective multi-stage tuning framework called MT4CrossIE, designed for enhancing cross-lingual open information extraction by injecting language-specific knowledge into the shared model. Specifically, the cross-lingual pre-trained model is first tuned in a shared semantic space (e.g., embedding matrix) in the fixed encoder and then other components are optimized in the second stage. After enough training, we freeze the pre-trained model and tune the multiple extra low-rank language-specific modules using mixture-of-LoRAs for model-based cross-lingual transfer. In addition, we leverage two-stage prompting to encourage the large language model (LLM) to annotate the multi-lingual 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26367;&#20195;&#20266;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#30340;CTC&#25439;&#22833;&#20989;&#25968;&#22788;&#29702;&#22122;&#22768;&#20266;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#20266;&#26631;&#31614;&#26356;&#26032;&#31574;&#30053;&#21644;&#28040;&#34701;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06547</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#26367;&#20195;&#20266;&#26631;&#31614;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition. (arXiv:2308.06547v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26367;&#20195;&#20266;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#30340;CTC&#25439;&#22833;&#20989;&#25968;&#22788;&#29702;&#22122;&#22768;&#20266;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#20266;&#26631;&#31614;&#26356;&#26032;&#31574;&#30053;&#21644;&#28040;&#34701;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#26102;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#25216;&#26415;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20266;&#26631;&#31614;&#36890;&#24120;&#21253;&#21547;&#35768;&#22810;&#38169;&#35823;&#30340;&#26631;&#35760;&#65292;&#23558;&#22122;&#22768;&#26631;&#31614;&#35270;&#20026;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#30495;&#23454;&#26631;&#35760;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#36807;&#28388;&#26368;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#25110;&#25552;&#39640;&#20266;&#26631;&#31614;&#30340;&#25972;&#20307;&#36136;&#37327;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23436;&#20840;&#28040;&#38500;&#20266;&#26631;&#31614;&#20013;&#30340;&#38169;&#35823;&#26631;&#35760;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26367;&#20195;&#20266;&#26631;&#31614;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#20174;&#35757;&#32451;&#30446;&#26631;&#30340;&#35282;&#24230;&#35299;&#20915;&#22024;&#26434;&#20266;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20960;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#24191;&#20041;&#30340;CTC&#25439;&#22833;&#20989;&#25968;&#26469;&#22788;&#29702;&#22122;&#22768;&#20266;&#26631;&#31614;&#65292;&#25509;&#21463;&#22312;&#38169;&#35823;&#26631;&#35760;&#30340;&#20301;&#32622;&#19978;&#30340;&#26367;&#20195;&#26631;&#35760;&#12290;&#28982;&#21518;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#20266;&#26631;&#31614;&#26356;&#26032;&#31574;&#30053;&#65292;&#19981;&#20165;&#33021;&#20943;&#36731;&#20266;&#26631;&#31614;&#30340;&#22122;&#22768;&#65292;&#36824;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#25910;&#25947;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#20266;&#26631;&#31614;&#28040;&#34701;&#27491;&#21017;&#21270;&#26469;&#21152;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When labeled data is insufficient, semi-supervised learning with the pseudo-labeling technique can significantly improve the performance of automatic speech recognition. However, pseudo-labels are often noisy, containing numerous incorrect tokens. Taking noisy labels as ground-truth in the loss function results in suboptimal performance. Previous works attempted to mitigate this issue by either filtering out the nosiest pseudo-labels or improving the overall quality of pseudo-labels. While these methods are effective to some extent, it is unrealistic to entirely eliminate incorrect tokens in pseudo-labels. In this work, we propose a novel framework named alternative pseudo-labeling to tackle the issue of noisy pseudo-labels from the perspective of the training objective. The framework comprises several components. Firstly, a generalized CTC loss function is introduced to handle noisy pseudo-labels by accepting alternative tokens in the positions of incorrect tokens. Applying this loss 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#33647;&#29289;&#30456;&#20851;&#25991;&#26723;&#20013;&#25552;&#21462;&#33647;&#29289;&#20107;&#20214;/&#23454;&#20307;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#24182;&#23545;&#40784;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;/&#35821;&#35328;/&#30693;&#35782;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#33647;&#29289;&#20107;&#20214;&#20449;&#24687;&#30340;&#20840;&#38754;&#26816;&#27979;&#21644;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.06546</link><description>&lt;p&gt;
MC-DRE: &#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#29992;&#20110;&#33647;&#29289;&#20107;&#20214;/&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
MC-DRE: Multi-Aspect Cross Integration for Drug Event/Entity Extraction. (arXiv:2308.06546v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#33647;&#29289;&#30456;&#20851;&#25991;&#26723;&#20013;&#25552;&#21462;&#33647;&#29289;&#20107;&#20214;/&#23454;&#20307;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#24182;&#23545;&#40784;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;/&#35821;&#35328;/&#30693;&#35782;&#23646;&#24615;&#65292;&#24182;&#23454;&#29616;&#33647;&#29289;&#20107;&#20214;&#20449;&#24687;&#30340;&#20840;&#38754;&#26816;&#27979;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#33647;&#29289;&#30456;&#20851;&#20449;&#24687;&#22359;&#65292;&#22914;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#65288;ADE&#65289;&#65292;&#23545;&#20110;&#39044;&#38450;&#30142;&#30149;&#21644;&#25327;&#25937;&#35768;&#22810;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;ADE&#26159;&#36890;&#36807;&#21307;&#30103;&#32972;&#26223;&#19979;&#30340;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25253;&#21578;&#30340;&#12290;&#22240;&#27492;&#65292;&#24212;&#29992;&#36890;&#29992;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#25972;&#21512;&#21644;&#23545;&#40784;&#22810;&#20010;&#20851;&#38190;&#26041;&#38754;&#26469;&#26816;&#27979;&#33647;&#29289;&#20107;&#20214;&#20449;&#24687;&#65292;&#21253;&#25324;&#33647;&#29289;&#20107;&#20214;&#35821;&#20041;&#12289;&#21477;&#27861;&#32467;&#26500;&#21644;&#21307;&#23398;&#39046;&#22495;&#26415;&#35821;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26041;&#38754;&#20132;&#21449;&#25972;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#33647;&#29289;&#30456;&#20851;&#25991;&#26723;&#20013;&#25429;&#25417;&#21644;&#23545;&#40784;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;/&#35821;&#35328;/&#30693;&#35782;&#23646;&#24615;&#65292;&#29992;&#20110;&#33647;&#29289;&#23454;&#20307;/&#20107;&#20214;&#26816;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#22810;&#26041;&#38754;&#32534;&#30721;&#22120;&#26469;&#25551;&#36848;&#35821;&#20041;&#12289;&#21477;&#27861;&#21644;&#21307;&#23398;&#25991;&#26723;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#26041;&#27861;&#21253;&#25324;&#27133;&#26631;&#27880;&#20219;&#21153;&#12289;&#20027;&#35201;&#33647;&#29289;&#23454;&#20307;/&#20107;&#20214;&#26816;&#27979;&#12289;&#35789;&#24615;&#26631;&#27880;&#21644;&#36890;&#29992;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#32534;&#30721;&#22120;&#36827;&#34892;&#20132;&#21449;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting meaningful drug-related information chunks, such as adverse drug events (ADE), is crucial for preventing morbidity and saving many lives. Most ADE are reported via an unstructured conversation with the medical context. Hence, applying a general entity recognition approach is not sufficient enough. The key is how to integrate and align multiple crucial aspects to detect drug event information, including drug event semantics, syntactic structures, and medical domain terminology. In this paper, we propose a new multi-aspect cross-integration framework for drug entity/event detection by capturing and aligning different context/language/knowledge properties from drug-related documents. We first construct multi-aspect encoders to describe semantic, syntactic, and medical document contextual information by conducting those slot tagging tasks, main drug entity/event detection, part-of-speech tagging, and general medical named entity recognition. Then, each encoder conducts cross int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21162;&#21147;&#22797;&#29616;&#20102;Vamvas&#21644;Sennrich&#65288;2022&#24180;&#65289;&#35770;&#25991;&#20013;&#30340;&#20154;&#24037;&#35780;&#20272;&#23454;&#39564;&#65292;&#32467;&#26524;&#30830;&#35748;&#20102;&#21407;&#30740;&#31350;&#30340;&#32467;&#35770;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#30340;&#21487;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06527</link><description>&lt;p&gt;
&#20174;&#20316;&#32773;&#37027;&#37324;&#33719;&#24471;&#19968;&#28857;&#24110;&#21161;&#65306;&#22797;&#29616;&#26426;&#22120;&#32763;&#35793;&#38169;&#35823;&#26816;&#27979;&#30340;&#20154;&#24037;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
With a Little Help from the Authors: Reproducing Human Evaluation of an MT Error Detector. (arXiv:2308.06527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21162;&#21147;&#22797;&#29616;&#20102;Vamvas&#21644;Sennrich&#65288;2022&#24180;&#65289;&#35770;&#25991;&#20013;&#30340;&#20154;&#24037;&#35780;&#20272;&#23454;&#39564;&#65292;&#32467;&#26524;&#30830;&#35748;&#20102;&#21407;&#30740;&#31350;&#30340;&#32467;&#35770;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#30340;&#21487;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#25105;&#20204;&#21162;&#21147;&#22797;&#29616;Vamvas&#21644;Sennrich&#65288;2022&#24180;&#65289;&#35770;&#25991;&#20013;&#21576;&#29616;&#30340;&#20154;&#24037;&#35780;&#20272;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;&#35813;&#23454;&#39564;&#35780;&#20272;&#20102;&#33258;&#21160;&#31995;&#32479;&#22312;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#20013;&#26816;&#27979;&#21040;&#30340;&#36229;&#32763;&#35793;&#21644;&#20302;&#32763;&#35793;&#65288;&#32763;&#35793;&#27604;&#21407;&#25991;&#21253;&#21547;&#26356;&#22810;&#25110;&#26356;&#23569;&#20449;&#24687;&#65289;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#20316;&#32773;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26723;&#21644;&#20195;&#30721;&#65292;&#25105;&#20204;&#22312;&#22797;&#29616;&#23454;&#39564;&#35774;&#35745;&#26041;&#38754;&#21457;&#29616;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#39640;&#21487;&#37325;&#22797;&#24615;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#22797;&#29616;&#30340;&#32467;&#26524;&#24635;&#20307;&#19978;&#39564;&#35777;&#20102;&#21407;&#22987;&#30740;&#31350;&#30340;&#32467;&#35770;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#35266;&#23519;&#21040;&#20102;&#32479;&#35745;&#19978;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#34920;&#26126;&#20154;&#24037;&#26631;&#27880;&#23384;&#22312;&#36739;&#39640;&#30340;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents our efforts to reproduce the results of the human evaluation experiment presented in the paper of Vamvas and Sennrich (2022), which evaluated an automatic system detecting over- and undertranslations (translations containing more or less information than the original) in machine translation (MT) outputs. Despite the high quality of the documentation and code provided by the authors, we discuss some problems we found in reproducing the exact experimental setup and offer recommendations for improving reproducibility. Our replicated results generally confirm the conclusions of the original study, but in some cases, statistically significant differences were observed, suggesting a high variability of human annotation.
&lt;/p&gt;</description></item><item><title>HyperFormer&#26159;&#19968;&#20010;&#32771;&#34385;&#23616;&#37096;&#32423;&#39034;&#24207;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#22810;&#36339;&#20449;&#24687;&#24341;&#20837;&#22122;&#38899;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06512</link><description>&lt;p&gt;
HyperFormer: &#22686;&#24378;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
HyperFormer: Enhancing Entity and Relation Interaction for Hyper-Relational Knowledge Graph Completion. (arXiv:2308.06512v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06512
&lt;/p&gt;
&lt;p&gt;
HyperFormer&#26159;&#19968;&#20010;&#32771;&#34385;&#23616;&#37096;&#32423;&#39034;&#24207;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#22810;&#36339;&#20449;&#24687;&#24341;&#20837;&#22122;&#38899;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#65288;HKGs&#65289;&#36890;&#36807;&#23558;&#23646;&#24615;-&#20540;&#20462;&#39280;&#31526;&#19982;&#19977;&#20803;&#32452;&#30456;&#20851;&#32852;&#26469;&#25193;&#23637;&#26631;&#20934;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#26377;&#25928;&#22320;&#34920;&#31034;&#20102;&#19982;&#20854;&#20851;&#32852;&#19977;&#20803;&#32452;&#30340;&#39069;&#22806;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;HKGC&#65289;&#26088;&#22312;&#22312;&#32771;&#34385;&#20854;&#20462;&#39280;&#31526;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#26410;&#30693;&#19977;&#20803;&#32452;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;HKGC&#26041;&#27861;&#21033;&#29992;&#20840;&#23616;&#32423;&#22270;&#32467;&#26500;&#23558;&#36229;&#20851;&#31995;&#30693;&#35782;&#32534;&#30721;&#20026;&#22270;&#21367;&#31215;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22810;&#36339;&#20449;&#24687;&#30340;&#28155;&#21152;&#21487;&#33021;&#20250;&#22312;&#19977;&#20803;&#32452;&#39044;&#27979;&#36807;&#31243;&#20013;&#24341;&#20837;&#22122;&#38899;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperFormer&#65292;&#36825;&#26159;&#19968;&#20010;&#32771;&#34385;&#23616;&#37096;&#32423;&#39034;&#24207;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#23427;&#32534;&#30721;&#20102;&#19977;&#20803;&#32452;&#30340;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#20462;&#39280;&#31526;&#30340;&#20869;&#23481;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;HyperFormer&#30001;&#19977;&#20010;&#19981;&#21516;&#30340;&#27169;&#22359;&#32452;&#25104;&#65306;&#19968;&#20010;&#23454;&#20307;&#37051;&#23621;&#32858;&#21512;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#25972;&#21512;&#23454;&#20307;&#37051;&#23621;&#30340;&#20449;&#24687;&#65292;&#20197;&#25429;&#25417;&#19981;&#21516;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyper-relational knowledge graphs (HKGs) extend standard knowledge graphs by associating attribute-value qualifiers to triples, which effectively represent additional fine-grained information about its associated triple. Hyper-relational knowledge graph completion (HKGC) aims at inferring unknown triples while considering its qualifiers. Most existing approaches to HKGC exploit a global-level graph structure to encode hyper-relational knowledge into the graph convolution message passing process. However, the addition of multi-hop information might bring noise into the triple prediction process. To address this problem, we propose HyperFormer, a model that considers local-level sequential information, which encodes the content of the entities, relations and qualifiers of a triple. More precisely, HyperFormer is composed of three different modules: an entity neighbor aggregator module allowing to integrate the information of the neighbors of an entity to capture different perspectives of
&lt;/p&gt;</description></item><item><title>AutoConv&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;&#65292;&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#20943;&#36731;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2308.06507</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models. (arXiv:2308.06507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06507
&lt;/p&gt;
&lt;p&gt;
AutoConv&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;&#65292;&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#20943;&#36731;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#23545;&#35805;&#24110;&#21161;&#29992;&#25143;&#25910;&#38598;&#20449;&#24687;&#30340;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20173;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoConv&#29992;&#20110;&#21512;&#25104;&#23545;&#35805;&#29983;&#25104;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#23545;&#35805;&#29983;&#25104;&#38382;&#39064;&#21046;&#23450;&#20026;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#65292;&#28982;&#21518;&#20351;&#29992;&#23569;&#37327;&#20154;&#31867;&#23545;&#35805;&#24494;&#35843;LLM&#65292;&#20197;&#25429;&#25417;&#20449;&#24687;&#23547;&#27714;&#36807;&#31243;&#30340;&#29305;&#24449;&#65292;&#24182;&#29992;&#20043;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#23545;&#35805;&#12290;&#22312;&#20004;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;AutoConv&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20943;&#36731;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20960;&#20010;&#20998;&#26512;&#30740;&#31350;&#20197;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#32842;&#22825;&#30340;&#26041;&#27861;&#65292;&#24182;&#25253;&#21578;&#20102;&#30456;&#23545;&#20110;&#22522;&#20934;&#32447;&#30340;&#25913;&#36827;&#12290;&#36824;&#20998;&#26512;&#20102;&#21478;&#22806;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.06502</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#32842;&#22825;&#30340;&#19977;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Three Ways of Using Large Language Models to Evaluate Chat. (arXiv:2308.06502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#32842;&#22825;&#30340;&#26041;&#27861;&#65292;&#24182;&#25253;&#21578;&#20102;&#30456;&#23545;&#20110;&#22522;&#20934;&#32447;&#30340;&#25913;&#36827;&#12290;&#36824;&#20998;&#26512;&#20102;&#21478;&#22806;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#22242;&#38431;6&#25552;&#20132;&#30340;ChatEval&#31995;&#32479;&#65292;&#36825;&#26159;DSTC 11 Track 4&#31454;&#36187;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#22797;&#30340;&#36716;&#21521;&#32423;&#21035;&#36136;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#21521;&#37327;&#23384;&#20648;&#20013;&#20351;&#29992;&#21160;&#24577;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#20316;&#20026;ChatGPT&#30340;&#25552;&#31034;&#65292;&#25253;&#21578;&#20102;&#30456;&#23545;&#20110;&#22522;&#20934;&#32447;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#21478;&#22806;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25253;&#21578;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;&#25105;&#20204;&#20165;&#29992;&#20004;&#21608;&#26102;&#38388;&#24320;&#21457;&#20102;&#36825;&#19977;&#20010;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#25361;&#25112;&#25130;&#27490;&#26085;&#26399;&#21518;&#36827;&#34892;&#30340;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#26032;&#30340;Llama 2&#27169;&#22411;&#27491;&#22312;&#32553;&#23567;ChatGPT&#21644;&#24320;&#28304;LLMs&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;Llama 2&#27169;&#22411;&#26080;&#27861;&#20687;ChatGPT&#37027;&#26679;&#20174;&#23569;&#26679;&#26412;&#31034;&#20363;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the systems submitted by team6 for ChatEval, the DSTC 11 Track 4 competition. We present three different approaches to predicting turn-level qualities of chatbot responses based on large language models (LLMs). We report improvement over the baseline using dynamic few-shot examples from a vector store for the prompts for ChatGPT. We also analyze the performance of the other two approaches and report needed improvements for future work. We developed the three systems over just two weeks, showing the potential of LLMs for this task. An ablation study conducted after the challenge deadline shows that the new Llama 2 models are closing the performance gap between ChatGPT and open-source LLMs. However, we find that the Llama 2 models do not benefit from few-shot examples in the same way as ChatGPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;-&#20027;&#21160;&#24341;&#23548;&#30340;&#26032;&#38395;&#22522;&#30784;&#23545;&#35805;&#65292;&#20854;&#20013;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#26032;&#38395;&#20851;&#38190;&#35805;&#39064;&#20027;&#21160;&#24341;&#23548;&#23545;&#35805;&#12290;&#30740;&#31350;&#32773;&#36824;&#25910;&#38598;&#20102;&#19968;&#20010;&#20154;&#26426;&#20013;&#25991;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"Predict-Generate-Rank"&#30340;&#26041;&#27861;&#29992;&#20110;&#22238;&#22797;&#29983;&#25104;&#21644;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2308.06501</link><description>&lt;p&gt;
NewsDialogues&#65306;&#38754;&#21521;&#20027;&#21160;&#24341;&#23548;&#30340;&#26032;&#38395;&#22522;&#30784;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
NewsDialogues: Towards Proactive News Grounded Conversation. (arXiv:2308.06501v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;-&#20027;&#21160;&#24341;&#23548;&#30340;&#26032;&#38395;&#22522;&#30784;&#23545;&#35805;&#65292;&#20854;&#20013;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#26032;&#38395;&#20851;&#38190;&#35805;&#39064;&#20027;&#21160;&#24341;&#23548;&#23545;&#35805;&#12290;&#30740;&#31350;&#32773;&#36824;&#25910;&#38598;&#20102;&#19968;&#20010;&#20154;&#26426;&#20013;&#25991;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"Predict-Generate-Rank"&#30340;&#26041;&#27861;&#29992;&#20110;&#22238;&#22797;&#29983;&#25104;&#21644;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#22522;&#30784;&#23545;&#35805;&#38271;&#26399;&#20197;&#26469;&#21463;&#21040;&#20219;&#21153;&#23450;&#20041;&#19981;&#28165;&#26224;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#21046;&#32422;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21363;&#20027;&#21160;&#24341;&#23548;&#30340;&#26032;&#38395;&#22522;&#30784;&#23545;&#35805;&#65292;&#20854;&#20013;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#26032;&#38395;&#30340;&#20851;&#38190;&#35805;&#39064;&#20027;&#21160;&#24341;&#23548;&#23545;&#35805;&#12290;&#27492;&#22806;&#65292;&#35813;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#21644;&#38386;&#32842;&#22330;&#26223;&#65292;&#29992;&#25143;&#21487;&#20197;&#23601;&#26032;&#38395;&#32454;&#33410;&#25552;&#38382;&#25110;&#34920;&#36798;&#35266;&#28857;&#65292;&#24182;&#28212;&#26395;&#36827;&#34892;&#32842;&#22825;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#24320;&#21457;&#36825;&#19968;&#26032;&#39062;&#20219;&#21153;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#20154;&#26426;&#20013;&#25991;&#23545;&#35805;&#25968;&#25454;&#38598;"NewsDialogues"&#65292;&#21253;&#25324;1000&#20010;&#23545;&#35805;&#65292;&#20849;&#26377;14.6K&#20010;&#35805;&#35821;&#65292;&#24182;&#35814;&#32454;&#27880;&#37322;&#20102;&#30446;&#26631;&#35805;&#39064;&#21644;&#30693;&#35782;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Predict-Generate-Rank"&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#39044;&#27979;&#21644;&#29983;&#25104;&#22238;&#22797;&#30340;&#29983;&#25104;&#22120;&#20197;&#21450;&#29992;&#20110;&#22810;&#20010;&#20505;&#36873;&#22238;&#22797;&#25490;&#24207;&#30340;&#25490;&#24207;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hot news is one of the most popular topics in daily conversations. However, news grounded conversation has long been stymied by the lack of well-designed task definition and scarce data. In this paper, we propose a novel task, Proactive News Grounded Conversation, in which a dialogue system can proactively lead the conversation based on some key topics of the news. In addition, both information-seeking and chit-chat scenarios are included realistically, where the user may ask a series of questions about the news details or express their opinions and be eager to chat. To further develop this novel task, we collect a human-to-human Chinese dialogue dataset \ts{NewsDialogues}, which includes 1K conversations with a total of 14.6K utterances and detailed annotations for target topics and knowledge spans. Furthermore, we propose a method named Predict-Generate-Rank, consisting of a generator for grounded knowledge prediction and response generation, and a ranker for the ranking of multiple 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KG&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#21442;&#32771;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#32473;&#23450;&#30340;&#22270;&#29983;&#25104;&#24544;&#23454;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;</title><link>http://arxiv.org/abs/2308.06488</link><description>&lt;p&gt;
&#26080;&#22122;&#22768;&#21442;&#32771;&#25991;&#26412;&#30340;&#30693;&#35782;&#22270;&#29983;&#25104;&#24544;&#23454;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Generating Faithful Text From a Knowledge Graph with Noisy Reference Text. (arXiv:2308.06488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KG&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#21442;&#32771;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#32473;&#23450;&#30340;&#22270;&#29983;&#25104;&#24544;&#23454;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#27969;&#30021;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65292;&#20934;&#30830;&#22320;&#34920;&#31034;&#32473;&#23450;&#30693;&#35782;&#22270;&#30340;&#20449;&#24687;&#12290;&#34429;&#28982;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#19982;&#36866;&#24403;&#30340;&#22270;&#32467;&#26500;&#24863;&#30693;&#27169;&#22359;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#22312;&#29983;&#25104;&#24544;&#23454;&#25991;&#26412;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#24403;&#22320;&#38754;&#30495;&#23454;&#35821;&#35328;&#25991;&#26412;&#20013;&#21253;&#21547;&#22270;&#20013;&#19981;&#23384;&#22312;&#30340;&#39069;&#22806;&#20449;&#24687;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KG&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#21442;&#32771;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#32473;&#23450;&#30340;&#22270;&#29983;&#25104;&#24544;&#23454;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#34701;&#21512;&#20102;&#20004;&#20010;&#26680;&#24515;&#24605;&#24819;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#27169;&#22411;&#21306;&#20998;&#25991;&#26412;&#20013;&#30340;&#24544;&#23454;&#21644;&#34394;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#40723;&#21169;&#35299;&#30721;&#22120;&#29983;&#25104;&#19982;&#36755;&#20837;&#22270;&#23545;&#40784;&#30340;&#25991;&#26412;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36171;&#20104;&#35299;&#30721;&#22120;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#30340;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph (KG)-to-Text generation aims at generating fluent natural-language text that accurately represents the information of a given knowledge graph. While significant progress has been made in this task by exploiting the power of pre-trained language models (PLMs) with appropriate graph structure-aware modules, existing models still fall short of generating faithful text, especially when the ground-truth natural-language text contains additional information that is not present in the graph. In this paper, we develop a KG-to-text generation model that can generate faithful natural-language text from a given graph, in the presence of noisy reference text. Our framework incorporates two core ideas: Firstly, we utilize contrastive learning to enhance the model's ability to differentiate between faithful and hallucinated information in the text, thereby encouraging the decoder to generate text that aligns with the input graph. Secondly, we empower the decoder to control the level 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#30721;&#36827;&#34892;&#32842;&#22825;&#21487;&#20197;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#23545;&#40784;&#25216;&#26415;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CipherChat&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#26816;&#26597;&#23433;&#20840;&#23545;&#40784;&#22312;&#38750;&#33258;&#28982;&#35821;&#35328;&#65288;&#23494;&#30721;&#65289;&#20013;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#31561;&#26368;&#20808;&#36827;&#30340;LLMs&#23545;&#19981;&#21516;&#20195;&#34920;&#24615;&#20154;&#31867;&#23494;&#30721;&#22312;11&#20010;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.06463</link><description>&lt;p&gt;
GPT-4&#22826;&#32874;&#26126;&#20197;&#33267;&#20110;&#19981;&#23433;&#20840;&#65306;&#36890;&#36807;&#23494;&#30721;&#19982;LLMs&#36827;&#34892;&#38544;&#34109;&#32842;&#22825;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher. (arXiv:2308.06463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06463
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#30721;&#36827;&#34892;&#32842;&#22825;&#21487;&#20197;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#23545;&#40784;&#25216;&#26415;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CipherChat&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#26816;&#26597;&#23433;&#20840;&#23545;&#40784;&#22312;&#38750;&#33258;&#28982;&#35821;&#35328;&#65288;&#23494;&#30721;&#65289;&#20013;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#31561;&#26368;&#20808;&#36827;&#30340;LLMs&#23545;&#19981;&#21516;&#20195;&#34920;&#24615;&#20154;&#31867;&#23494;&#30721;&#22312;11&#20010;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24615;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#30340;&#26680;&#24515;&#12290;&#20851;&#20110;&#23558;LLMs&#19982;&#20154;&#31867;&#20262;&#29702;&#21644;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#30340;&#24037;&#20316;&#24050;&#32463;&#24456;&#22810;&#65292;&#21253;&#25324;&#22312;&#39044;&#35757;&#32451;&#20013;&#36827;&#34892;&#25968;&#25454;&#31579;&#36873;&#12289;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#12289;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20197;&#21450;&#32418;&#38431;&#27979;&#35797;&#31561;&#31561;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#23494;&#30721;&#36827;&#34892;&#32842;&#22825;&#21487;&#20197;&#32469;&#36807;LLMs&#30340;&#23433;&#20840;&#23545;&#40784;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#20027;&#35201;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;CipherChat&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#26816;&#26597;&#23433;&#20840;&#23545;&#40784;&#22312;&#38750;&#33258;&#28982;&#35821;&#35328;&#65288;&#23494;&#30721;&#65289;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;CipherChat&#20351;&#20154;&#20204;&#33021;&#22815;&#36890;&#36807;&#21152;&#23494;&#25552;&#31034;&#21644;&#23569;&#37327;&#21152;&#23494;&#28436;&#31034;&#19982;LLMs&#36827;&#34892;&#32842;&#22825;&#12290;&#25105;&#20204;&#20351;&#29992;CipherChat&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#20013;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;LLMs&#65292;&#21253;&#25324;ChatGPT&#21644;GPT-4&#22312;11&#20010;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#19981;&#21516;&#20195;&#34920;&#24615;&#20154;&#31867;&#23494;&#30721;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26576;&#20123;&#23494;&#30721;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#23433;&#20840;&#23545;&#40784;&#25216;&#26415;&#65292;&#20960;&#20046;100%&#30340;&#26102;&#38388;&#37117;&#33021;&#22815;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#36523;&#20221;&#19981;&#21487;&#30693;&#35328;&#35821;&#29983;&#25104;&#65292;&#39318;&#20808;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#65292;&#28982;&#21518;&#20351;&#29992;&#38899;&#39057;&#39537;&#21160;&#30340;&#35828;&#35805;&#22836;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#24341;&#20154;&#20837;&#32988;&#30340;&#35270;&#39057;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06457</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#35270;&#39057;&#65306;&#38646;&#26679;&#26412;&#36523;&#20221;&#19981;&#21487;&#30693;&#35328;&#35821;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation. (arXiv:2308.06457v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#36523;&#20221;&#19981;&#21487;&#30693;&#35328;&#35821;&#29983;&#25104;&#65292;&#39318;&#20808;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#65292;&#28982;&#21518;&#20351;&#29992;&#38899;&#39057;&#39537;&#21160;&#30340;&#35828;&#35805;&#22836;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#24341;&#20154;&#20837;&#32988;&#30340;&#35270;&#39057;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#21019;&#26032;&#30340;&#20449;&#24687;&#25910;&#38598;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;ChatGPT&#25552;&#20379;&#30340;&#20449;&#24687;&#20165;&#38480;&#20110;&#25991;&#26412;&#65292;&#20854;&#21487;&#35270;&#21270;&#20173;&#21463;&#38480;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;TTV&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;&#35270;&#39057;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#23545;&#29983;&#25104;&#38899;&#39057;&#36523;&#20221;&#30340;&#25511;&#21046;&#65292;&#21363;&#19981;&#26159;&#36523;&#20221;&#19981;&#21487;&#30693;&#30340;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#38754;&#21521;&#20154;&#29289;&#19981;&#21487;&#30693;&#30340;&#35270;&#39057;&#20811;&#38534;&#65292;&#29305;&#21035;&#20851;&#27880;TTV&#29983;&#25104;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#27169;&#22411;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#36716;&#25442;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#37319;&#29992;&#22522;&#20110;&#38899;&#39057;&#39537;&#21160;&#30340;&#35828;&#35805;&#22836;&#29983;&#25104;&#26041;&#27861;&#26469;&#20135;&#29983;&#24341;&#20154;&#20837;&#32988;&#30340;&#35270;&#39057;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#38454;&#27573;&#29983;&#25104;&#30340;&#38899;&#39057;&#12290;&#26412;&#25991;&#23545;&#19981;&#21516;&#30340;TTS&#21644;&#22522;&#20110;&#38899;&#39057;&#39537;&#21160;&#30340;&#35828;&#35805;&#22836;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#35782;&#21035;&#20102;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of ChatGPT has introduced innovative methods for information gathering and analysis. However, the information provided by ChatGPT is limited to text, and the visualization of this information remains constrained. Previous research has explored zero-shot text-to-video (TTV) approaches to transform text into videos. However, these methods lacked control over the identity of the generated audio, i.e., not identity-agnostic, hindering their effectiveness. To address this limitation, we propose a novel two-stage framework for person-agnostic video cloning, specifically focusing on TTV generation. In the first stage, we leverage pretrained zero-shot models to achieve text-to-speech (TTS) conversion. In the second stage, an audio-driven talking head generation method is employed to produce compelling videos privided the audio generated in the first stage. This paper presents a comparative analysis of different TTS and audio-driven talking head generation methods, identifying the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#37325;&#26032;&#23450;&#20041;&#20026;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#65292;&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;1.1%&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.06454</link><description>&lt;p&gt;
&#22522;&#20110;&#28436;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Demonstration-based learning for few-shot biomedical named entity recognition under machine reading comprehension. (arXiv:2308.06454v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#37325;&#26032;&#23450;&#20041;&#20026;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#65292;&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;1.1%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35768;&#22810;&#39046;&#22495;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#22823;&#37327;&#25163;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#31181;&#33021;&#22815;&#25913;&#36827;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#35782;&#21035;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#30340;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;BioNER&#65289;&#37325;&#26032;&#23450;&#20041;&#20026;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;BioNER&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#36866;&#24403;&#30340;&#20219;&#21153;&#28436;&#31034;&#12290;&#22312;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21253;&#25324;BC4CHEMD&#12289;BC5CDR-Chemical&#12289;BC5CDR-Disease&#12289;NCBI-Disease&#12289;BC2GM&#21644;JNLPBA&#22312;&#20869;&#30340;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36890;&#36807;&#25253;&#21578;25&#26679;&#26412;&#21644;50&#26679;&#26412;&#23398;&#20064;&#23454;&#39564;&#30340;F1&#20998;&#25968;&#26469;&#26816;&#26597;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#22312;25&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;1.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep learning techniques have shown significant achievements, they frequently depend on extensive amounts of hand-labeled data and tend to perform inadequately in few-shot scenarios. The objective of this study is to devise a strategy that can improve the model's capability to recognize biomedical entities in scenarios of few-shot learning. By redefining biomedical named entity recognition (BioNER) as a machine reading comprehension (MRC) problem, we propose a demonstration-based learning method to address few-shot BioNER, which involves constructing appropriate task demonstrations. In assessing our proposed method, we compared the proposed method with existing advanced methods using six benchmark datasets, including BC4CHEMD, BC5CDR-Chemical, BC5CDR-Disease, NCBI-Disease, BC2GM, and JNLPBA. We examined the models' efficacy by reporting F1 scores from both the 25-shot and 50-shot learning experiments. In 25-shot learning, we observed 1.1% improvements in the average F1 scores 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#30340;&#26032;&#22411;Emotion Recognition Network (ERNetCL)&#27169;&#22411;&#65292;&#36890;&#36807;&#31616;&#21270;&#32593;&#32476;&#32467;&#26500;&#24182;&#20805;&#20998;&#24314;&#27169;&#19978;&#19979;&#25991;&#26469;&#39640;&#25928;&#22320;&#25429;&#25417;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#32447;&#32034;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.06450</link><description>&lt;p&gt;
&#31616;&#21333;&#27169;&#22411;&#20063;&#26377;&#25928;&#65306;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#30340;&#25991;&#26412;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Simple Model Also Works: A Novel Emotion Recognition Network in Textual Conversation Based on Curriculum Learning Strategy. (arXiv:2308.06450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#30340;&#26032;&#22411;Emotion Recognition Network (ERNetCL)&#27169;&#22411;&#65292;&#36890;&#36807;&#31616;&#21270;&#32593;&#32476;&#32467;&#26500;&#24182;&#20805;&#20998;&#24314;&#27169;&#19978;&#19979;&#25991;&#26469;&#39640;&#25928;&#22320;&#25429;&#25417;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#32447;&#32034;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#24050;&#25104;&#20026;&#23545;&#35805;&#26426;&#22120;&#20154;&#21644;&#38382;&#31572;&#31995;&#32479;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#22914;&#20309;&#39640;&#25928;&#22320;&#33719;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#24773;&#24863;&#32447;&#32034;&#19968;&#30452;&#26159;ERC&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#24314;&#27169;&#19978;&#19979;&#25991;&#65292;&#24182;&#37319;&#29992;&#22797;&#26434;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#23548;&#33268;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#36807;&#22823;&#32780;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#30340;&#26032;&#22411;Emotion Recognition Network&#65288;ERNetCL&#65289;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#30001;Temporal Encoder&#65288;TE&#65289;&#12289;Spatial Encoder&#65288;SE&#65289;&#21644;Curriculum Learning&#65288;CL&#65289; loss&#32452;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;TE&#21644;SE&#20197;&#31616;&#27905;&#30340;&#26041;&#24335;&#32467;&#21512;&#20102;&#20197;&#21069;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#23545;&#35805;&#20013;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#27169;&#25311;&#20154;&#31867;&#20174;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#24335;&#65292;&#25105;&#20204;&#23558;CL&#30340;&#24605;&#24819;&#24212;&#29992;&#21040;ERC&#20219;&#21153;&#20013;&#65292;&#36880;&#27493;&#20248;&#21270;&#32593;&#32476;&#26500;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) has emerged as a research hotspot in domains such as conversational robots and question-answer systems. How to efficiently and adequately retrieve contextual emotional cues has been one of the key challenges in the ERC task. Existing efforts do not fully model the context and employ complex network structures, resulting in excessive computational resource overhead without substantial performance improvement. In this paper, we propose a novel Emotion Recognition Network based on Curriculum Learning strategy (ERNetCL). The proposed ERNetCL primarily consists of Temporal Encoder (TE), Spatial Encoder (SE), and Curriculum Learning (CL) loss. We utilize TE and SE to combine the strengths of previous methods in a simplistic manner to efficiently capture temporal and spatial contextual information in the conversation. To simulate the way humans learn curriculum from easy to hard, we apply the idea of CL to the ERC task to progressively optimize the ne
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#38382;&#39064;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06431</link><description>&lt;p&gt;
&#22810;&#36339;&#38382;&#39064;&#30340;&#24615;&#33021;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performance Prediction for Multi-hop Questions. (arXiv:2308.06431v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06431
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#27979;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#38382;&#39064;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#38382;&#31572;&#65288;QA&#65289;&#20013;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#30340;&#38382;&#39064;&#65292;&#20219;&#21153;&#26159;&#20272;&#35745;&#23545;&#35821;&#26009;&#24211;&#36827;&#34892;&#22810;&#36339;&#38382;&#39064;&#35780;&#20272;&#30340;&#38590;&#24230;&#12290;&#23613;&#31649;&#22312;&#39044;&#27979;&#21363;&#24109;&#21644;QA&#26816;&#32034;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#22810;&#36339;&#38382;&#39064;&#38590;&#24230;&#30340;&#35780;&#20272;&#30740;&#31350;&#36824;&#32570;&#20047;&#12290;&#30001;&#20110;&#26816;&#32034;&#36807;&#31243;&#30340;&#22810;&#27493;&#39588;&#24615;&#36136;&#12289;&#27493;&#39588;&#20043;&#38388;&#30340;&#28508;&#22312;&#20381;&#36182;&#20851;&#31995;&#21644;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#26816;&#32034;&#26041;&#27861;multHP&#65292;&#29992;&#20110;&#39044;&#27979;&#24320;&#25918;&#39046;&#22495;&#22810;&#36339;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#26368;&#22823;&#30340;&#22810;&#36339;QA&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#20960;&#31181;&#29616;&#20195;QA&#31995;&#32479;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#24615;&#33021;&#30340;&#24378;&#26377;&#21147;&#39044;&#27979;&#32773;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#21333;&#36339;QPP&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#20248;&#21270;QA&#31995;&#32479;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of Query Performance Prediction (QPP) for open-domain multi-hop Question Answering (QA), where the task is to estimate the difficulty of evaluating a multi-hop question over a corpus. Despite the extensive research on predicting the performance of ad-hoc and QA retrieval models, there has been a lack of study on the estimation of the difficulty of multi-hop questions. The problem is challenging due to the multi-step nature of the retrieval process, potential dependency of the steps and the reasoning involved. To tackle this challenge, we propose multHP, a novel pre-retrieval method for predicting the performance of open-domain multi-hop questions. Our extensive evaluation on the largest multi-hop QA dataset using several modern QA systems shows that the proposed model is a strong predictor of the performance, outperforming traditional single-hop QPP models. Additionally, we demonstrate that our approach can be effectively used to optimize the parameters of QA syste
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;LLM-DP&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20256;&#32479;&#35268;&#21010;&#22120;&#21512;&#20316;&#65292;&#20197;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#20307;&#20219;&#21153;&#12290;&#30456;&#27604;&#20110;&#20165;&#20351;&#29992;&#26420;&#32032;LLM&#26041;&#27861;&#65292;LLM-DP&#22312;&#35299;&#20915;Alfworld&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.06391</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dynamic Planning with a LLM. (arXiv:2308.06391v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06391
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;LLM-DP&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20256;&#32479;&#35268;&#21010;&#22120;&#21512;&#20316;&#65292;&#20197;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#20307;&#20219;&#21153;&#12290;&#30456;&#27604;&#20110;&#20165;&#20351;&#29992;&#26420;&#32032;LLM&#26041;&#27861;&#65292;LLM-DP&#22312;&#35299;&#20915;Alfworld&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#22312;&#38646;-shot&#24773;&#26223;&#19979;&#35299;&#20915;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#28041;&#21450;&#20855;&#20307;&#20195;&#29702;&#30340;&#24212;&#29992;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#35745;&#21010;&#38543;&#30528;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#22686;&#38271;&#21464;&#24471;&#22256;&#38590;&#21644;&#26114;&#36149;&#12290;&#35268;&#21010;&#38656;&#35201;&#29702;&#35299;&#34892;&#21160;&#30340;&#21487;&#33021;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#24403;&#21069;&#29615;&#22659;&#26159;&#21542;&#28385;&#36275;&#30446;&#26631;&#29366;&#24577;&#12290;&#34429;&#28982;&#31526;&#21495;&#35268;&#21010;&#22120;&#21487;&#20197;&#24555;&#36895;&#25214;&#21040;&#26368;&#20248;&#35299;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#23436;&#25972;&#20934;&#30830;&#22320;&#34920;&#31034;&#35268;&#21010;&#38382;&#39064;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#20351;&#29992;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29616;&#20195;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#26102;&#33021;&#22815;&#22788;&#29702;&#22024;&#26434;&#30340;&#35266;&#23519;&#32467;&#26524;&#21644;&#39640;&#27700;&#24179;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;LLM-Dynamic Planner&#65288;LLM-DP&#65289;&#65306;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#22312;&#35299;&#20915;&#20855;&#20307;&#20219;&#21153;&#26102;LLM&#19982;&#20256;&#32479;&#35268;&#21010;&#22120;&#25658;&#25163;&#21512;&#20316;&#12290;&#32473;&#23450;&#34892;&#21160;&#25551;&#36848;&#65292;LLM-DP&#27604;&#26420;&#32032;LLM ReAct&#22522;&#20934;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#20102;Alfworld&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) can solve many NLP tasks in zero-shot settings, applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows. Planning requires understanding the likely effects of one's actions and identifying whether the current environment satisfies the goal state. While symbolic planners find optimal solutions quickly, they require a complete and accurate representation of the planning problem, severely limiting their use in practical scenarios. In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ZYN&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26159;&#38750;&#38382;&#39064;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#25552;&#31034;&#65292;&#20197;&#21450;&#22686;&#24378;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#35299;&#27602;&#12289;&#24773;&#24863;&#20248;&#21270;&#21644;&#20010;&#24615;&#21270;&#25552;&#31034;&#29983;&#25104;&#22120;&#31561;&#12290;</title><link>http://arxiv.org/abs/2308.06385</link><description>&lt;p&gt;
ZYN&#65306;&#38646;&#24335;&#22870;&#21169;&#27169;&#22411;&#19982;&#26159;&#38750;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
ZYN: Zero-Shot Reward Models with Yes-No Questions. (arXiv:2308.06385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ZYN&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26159;&#38750;&#38382;&#39064;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#25552;&#31034;&#65292;&#20197;&#21450;&#22686;&#24378;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#35299;&#27602;&#12289;&#24773;&#24863;&#20248;&#21270;&#21644;&#20010;&#24615;&#21270;&#25552;&#31034;&#29983;&#25104;&#22120;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#23450;&#21521;&#20110;&#26399;&#26395;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#20559;&#22909;&#23545;&#40784;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#21478;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25209;&#35780;&#32773;&#65292;&#36890;&#36807;&#19968;&#20010;&#34920;&#31034;&#29992;&#25143;&#20559;&#22909;&#30340;&#26159;&#38750;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#20197;&#38646;&#24335;&#26041;&#24335;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#31181;&#38646;&#24335;&#22870;&#21169;&#27169;&#22411;&#20026;&#36827;&#19968;&#27493;&#24494;&#35843;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#23398;&#20064;&#20449;&#21495;&#65292;&#20351;&#29992;&#22686;&#24378;&#23398;&#20064;&#65292;&#23601;&#20687;&#22312;RLAIF&#20013;&#19968;&#26679;&#65307;&#28982;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#20182;&#19978;&#19979;&#25991;&#20013;&#20063;&#26159;&#20860;&#23481;&#30340;&#65292;&#20363;&#22914;&#36136;&#37327;&#22810;&#26679;&#24615;&#25628;&#32034;&#12290;&#36890;&#36807;&#22312;&#19982;&#25991;&#26412;&#29983;&#25104;&#30456;&#20851;&#30340;&#19981;&#21516;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#35299;&#27602;&#12289;&#20248;&#21270;&#30005;&#24433;&#35780;&#35770;&#30340;&#24773;&#24863;&#25110;&#20219;&#20309;&#20854;&#20182;&#23646;&#24615;&#12289;&#24341;&#23548;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#30340;&#20851;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#35266;&#28857;&#65292;&#20197;&#21450;&#20010;&#24615;&#21270;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#20219;&#21153;&#30340;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#25552;&#20379;&#20102;&#23545;&#25152;&#25552;&#20986;&#30340;ZYN&#26694;&#26550;&#33021;&#21147;&#30340;&#22823;&#37327;&#35777;&#25454;&#12290;&#20195;&#30721;&#23558;&#22312;\url&#22788;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we address the problem of directing the text generations of a LLM towards a desired behavior, aligning the generated text with the preferences of the human operator. We propose using another language model as a critic, reward model in a zero-shot way thanks to the prompt of a Yes-No question that represents the user preferences, without requiring further labeled data. This zero-shot reward model provides the learning signal to further fine-tune the base LLM using reinforcement learning, as in RLAIF; yet our approach is also compatible in other contexts such as quality-diversity search. Extensive evidence of the capabilities of the proposed ZYN framework is provided through experiments in different domains related to text generation, including detoxification; optimizing sentiment of movie reviews, or any other attribute; steering the opinion about a particular topic the model may have; and personalizing prompt generators for text-to-image tasks. Code to be released at \url
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#21512;&#20026;&#30693;&#35782;&#34920;&#31034;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.06374</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Knowledge Graphs: Opportunities and Challenges. (arXiv:2308.06374v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06374
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#21512;&#20026;&#30693;&#35782;&#34920;&#31034;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#32473;&#30693;&#35782;&#34920;&#31034;&#39046;&#22495;&#21644;&#25972;&#20010;&#19990;&#30028;&#24102;&#26469;&#20102;&#39118;&#26292;&#12290;&#36825;&#19968;&#36716;&#25240;&#28857;&#26631;&#24535;&#30528;&#30693;&#35782;&#34920;&#31034;&#20174;&#26174;&#24335;&#30693;&#35782;&#34920;&#31034;&#36716;&#21521;&#26174;&#24335;&#30693;&#35782;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#28151;&#21512;&#34920;&#31034;&#30340;&#28966;&#28857;&#30340;&#37325;&#22609;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;LLMs&#65288;&#21442;&#25968;&#21270;&#30693;&#35782;&#65289;&#21644;&#30693;&#35782;&#22270;&#35889;&#65288;&#26174;&#24335;&#30693;&#35782;&#65289;&#39046;&#22495;&#20869;&#30340;&#19968;&#20123;&#24120;&#35265;&#20105;&#35770;&#65292;&#23637;&#26395;&#28966;&#28857;&#37325;&#22609;&#25152;&#24102;&#26469;&#30340;&#26426;&#36935;&#21644;&#24895;&#26223;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#30740;&#31350;&#20027;&#39064;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have taken Knowledge Representation -- and the world -- by storm. This inflection point marks a shift from explicit knowledge representation to a renewed focus on the hybrid representation of both explicit knowledge and parametric knowledge. In this position paper, we will discuss some of the common debate points within the community on LLMs (parametric knowledge) and Knowledge Graphs (explicit knowledge) and speculate on opportunities and visions that the renewed focus brings, as well as related research topics and challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#25913;&#36827;&#20102;&#36825;&#20123;&#26497;&#26377;&#20215;&#20540;&#20294;&#24456;&#23569;&#34987;&#35760;&#24405;&#30340;&#20020;&#24202;&#25968;&#25454;&#30340;&#25552;&#21462;&#12290;&#26368;&#20339;&#27169;&#22411;&#20026;&#32463;&#36807;&#24494;&#35843;&#30340;Flan-T5 XL&#21644;Flan-T5 XXL&#65292;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06354</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#35782;&#21035;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Large Language Models to Identify Social Determinants of Health in Electronic Health Records. (arXiv:2308.06354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#25913;&#36827;&#20102;&#36825;&#20123;&#26497;&#26377;&#20215;&#20540;&#20294;&#24456;&#23569;&#34987;&#35760;&#24405;&#30340;&#20020;&#24202;&#25968;&#25454;&#30340;&#25552;&#21462;&#12290;&#26368;&#20339;&#27169;&#22411;&#20026;&#32463;&#36807;&#24494;&#35843;&#30340;Flan-T5 XL&#21644;Flan-T5 XXL&#65292;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#23545;&#24739;&#32773;&#30340;&#32467;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20294;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#25910;&#38598;&#19981;&#23436;&#25972;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;EHR&#20013;&#25552;&#21462;SDoH&#30340;&#33021;&#21147;&#65292;&#24182;&#25506;&#35752;&#20102;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#22312;&#25913;&#36827;&#36825;&#20123;&#23569;&#35265;&#20294;&#26497;&#26377;&#20215;&#20540;&#30340;&#20020;&#24202;&#25968;&#25454;&#25552;&#21462;&#20013;&#30340;&#20316;&#29992;&#12290;&#23545;800&#20221;&#24739;&#32773;&#35760;&#24405;&#36827;&#34892;&#20102;SDoH&#31867;&#21035;&#30340;&#27880;&#37322;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#36824;&#23581;&#35797;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#35780;&#20272;&#20102;&#31639;&#27861;&#20559;&#24046;&#12290;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#26159;&#32463;&#36807;&#24494;&#35843;&#30340;Flan-T5 XL&#65288;macro-F1 0.71&#65289;&#29992;&#20110;&#20219;&#20309;SDoH&#65292;&#20197;&#21450;Flan-T5 XXL&#65288;macro-F1 0.70&#65289;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36741;&#21161;&#24494;&#35843;&#30340;&#25928;&#30410;&#22240;&#27169;&#22411;&#26550;&#26500;&#21644;&#22823;&#23567;&#32780;&#24322;&#65292;&#22312;&#36739;&#23567;&#30340;Flan-T5&#27169;&#22411;&#65288;&#22522;&#30784;&#21644;&#22823;&#22411;&#65289;&#20013;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;delta F1 +0.12&#21040;+0.23&#65289;&#12290;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social determinants of health (SDoH) have an important impact on patient outcomes but are incompletely collected from the electronic health records (EHR). This study researched the ability of large language models to extract SDoH from free text in EHRs, where they are most commonly documented, and explored the role of synthetic clinical text for improving the extraction of these scarcely documented, yet extremely valuable, clinical data. 800 patient notes were annotated for SDoH categories, and several transformer-based models were evaluated. The study also experimented with synthetic data generation and assessed for algorithmic bias. Our best-performing models were fine-tuned Flan-T5 XL (macro-F1 0.71) for any SDoH, and Flan-T5 XXL (macro-F1 0.70). The benefit of augmenting fine-tuning with synthetic data varied across model architecture and size, with smaller Flan-T5 models (base and large) showing the greatest improvements in performance (delta F1 +0.12 to +0.23). Model performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#23383;&#32032;&#21333;&#20301;&#21644;&#36741;&#21161;&#21333;&#35821;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#27979;&#35797;&#20219;&#21153;&#39564;&#35777;&#20102;&#20854;&#22312;&#21452;&#35821;&#24212;&#29992;&#20013;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06327</link><description>&lt;p&gt;
&#20351;&#29992;&#23383;&#32032;&#21333;&#20301;&#21644;&#36741;&#21161;&#21333;&#35821;&#25439;&#22833;&#30340;&#21452;&#35821;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss. (arXiv:2308.06327v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#23383;&#32032;&#21333;&#20301;&#21644;&#36741;&#21161;&#21333;&#35821;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#27979;&#35797;&#20219;&#21153;&#39564;&#35777;&#20102;&#20854;&#22312;&#21452;&#35821;&#24212;&#29992;&#20013;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21452;&#35821;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25903;&#25345;&#22312;&#28151;&#21512;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#29615;&#22659;&#20013;&#65292;&#33521;&#35821;&#20316;&#20026;&#22823;&#22810;&#25968;&#20027;&#35201;&#35821;&#35328;&#29615;&#22659;&#30340;&#31532;&#20108;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24320;&#21457;&#21253;&#25324;&#65306;&#65288;a&#65289;&#20351;&#29992;&#23383;&#32032;&#21333;&#20301;&#32780;&#19981;&#26159;&#38899;&#32032;&#21333;&#20301;&#30340;&#21457;&#38899;&#35789;&#20856;&#65292;&#65288;b&#65289;&#23436;&#20840;&#21452;&#35821;&#23545;&#40784;&#27169;&#22411;&#65292;&#38543;&#21518;&#26159;&#21452;&#35821;&#27969;&#24335;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#65288;c&#65289;&#24102;&#26377;&#35821;&#35328;&#35782;&#21035;&#65288;LID&#65289;&#25439;&#22833;&#30340;&#24179;&#34892;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#65288;d&#65289;&#24102;&#26377;&#36741;&#21161;&#25439;&#22833;&#30340;&#24179;&#34892;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#21333;&#35821;&#25237;&#24433;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#19982;LID&#25439;&#22833;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#36741;&#21161;&#25439;&#22833;&#22312;&#23558;&#24179;&#34892;&#32534;&#30721;&#22120;&#19987;&#38376;&#29992;&#20110;&#21508;&#33258;&#30340;&#21333;&#35821;&#29615;&#22659;&#26041;&#38754;&#26356;&#20248;&#31168;&#65292;&#36825;&#26377;&#21161;&#20110;&#26356;&#24378;&#22823;&#30340;&#21452;&#35821;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#21452;&#35821;&#35199;&#29677;&#29273;&#35821;&#65288;ES&#65289;&#21644;&#21452;&#35821;&#24847;&#22823;&#21033;&#35821;&#65288;IT&#65289;&#24212;&#29992;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#27979;&#35797;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#21452;&#35821;&#27169;&#22411;&#23637;&#31034;&#20102;&#36739;&#24378;&#30340;&#33521;&#35821;&#28151;&#21512;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#21452;&#35821;IT&#27169;&#22411;&#25913;&#21892;&#20102;&#33521;&#35821;&#28151;&#21512;IT ta&#30340;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a bilingual solution to support English as secondary locale for most primary locales in hybrid automatic speech recognition (ASR) settings. Our key developments constitute: (a) pronunciation lexicon with grapheme units instead of phone units, (b) a fully bilingual alignment model and subsequently bilingual streaming transformer model, (c) a parallel encoder structure with language identification (LID) loss, (d) parallel encoder with an auxiliary loss for monolingual projections. We conclude that in comparison to LID loss, our proposed auxiliary loss is superior in specializing the parallel encoders to respective monolingual locales, and that contributes to stronger bilingual learning. We evaluate our work on large-scale training and test tasks for bilingual Spanish (ES) and bilingual Italian (IT) applications. Our bilingual models demonstrate strong English code-mixing capability. In particular, the bilingual IT model improves the word error rate (WER) for a code-mix IT ta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#28155;&#21152;&#25351;&#20196;&#26631;&#31614;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#36394;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#21644;&#33258;&#25105;&#31579;&#36873;&#29983;&#25104;&#35757;&#32451;&#31034;&#20363;&#65292;&#24182;&#19988;&#22312;Alpaca&#25490;&#34892;&#27036;&#19978;&#34920;&#29616;&#20986;&#38750;&#24120;&#39640;&#25928;&#30340;&#33258;&#21160;&#23545;&#40784;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06259</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#30340;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment with Instruction Backtranslation. (arXiv:2308.06259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#28155;&#21152;&#25351;&#20196;&#26631;&#31614;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#36394;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#21644;&#33258;&#25105;&#31579;&#36873;&#29983;&#25104;&#35757;&#32451;&#31034;&#20363;&#65292;&#24182;&#19988;&#22312;Alpaca&#25490;&#34892;&#27036;&#19978;&#34920;&#29616;&#20986;&#38750;&#24120;&#39640;&#25928;&#30340;&#33258;&#21160;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#20026;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#26412;&#28155;&#21152;&#30456;&#24212;&#30340;&#25351;&#20196;&#26631;&#31614;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#36394;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#65292;&#23427;&#20174;&#22312;&#23569;&#37327;&#31181;&#23376;&#25968;&#25454;&#21644;&#32473;&#23450;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#24320;&#22987;&#12290;&#31181;&#23376;&#27169;&#22411;&#29992;&#20110;&#36890;&#36807;&#20026;&#32593;&#32476;&#25991;&#26723;&#29983;&#25104;&#25351;&#20196;&#25552;&#31034;&#65288;&#33258;&#25105;&#22686;&#24378;&#65289;&#26469;&#26500;&#24314;&#35757;&#32451;&#31034;&#20363;&#65292;&#28982;&#21518;&#20174;&#36825;&#20123;&#20505;&#36873;&#31034;&#20363;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#65288;&#33258;&#25105;&#31579;&#36873;&#65289;&#12290;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#24494;&#35843;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;&#20004;&#27425;&#36845;&#20195;&#26469;&#24494;&#35843;LLaMa&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;Alpaca&#25490;&#34892;&#27036;&#19978;&#20987;&#36133;&#20102;&#25152;&#26377;&#20854;&#20182;&#22522;&#20110;LLaMa&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#33976;&#39311;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#38750;&#24120;&#26377;&#25928;&#30340;&#33258;&#21160;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;ZeroShotALI&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#36827;&#37329;&#34701;&#23457;&#35745;&#20013;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21305;&#37197;&#12290;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#32463;&#36807;&#39046;&#22495;&#20248;&#21270;&#30340;&#22522;&#20110;transformer&#30340;&#25991;&#26412;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#31995;&#32479;&#23454;&#29616;&#20102;&#20174;&#25253;&#21578;&#20013;&#25512;&#33616;&#19982;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#65292;&#24182;&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.06111</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#37329;&#34701;&#23457;&#35745;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models. (arXiv:2308.06111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06111
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;ZeroShotALI&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#36827;&#37329;&#34701;&#23457;&#35745;&#20013;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21305;&#37197;&#12290;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#32463;&#36807;&#39046;&#22495;&#20248;&#21270;&#30340;&#22522;&#20110;transformer&#30340;&#25991;&#26412;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#31995;&#32479;&#23454;&#29616;&#20102;&#20174;&#25253;&#21578;&#20013;&#25512;&#33616;&#19982;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#65292;&#24182;&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23457;&#35745;&#37329;&#34701;&#25991;&#20214;&#26159;&#19968;&#20010;&#38750;&#24120;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#30446;&#21069;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#65292;&#20197;&#25512;&#33616;&#19982;&#20005;&#26684;&#20250;&#35745;&#26631;&#20934;&#30340;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#25253;&#21578;&#20013;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23450;&#26399;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#32570;&#20047;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ZeroShotALI&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;&#20248;&#21270;&#30340;&#22522;&#20110;transformer&#30340;&#25991;&#26412;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#39318;&#20808;&#20351;&#29992;&#33258;&#23450;&#20041;BERT&#27169;&#22411;&#26816;&#32034;&#19982;&#27861;&#24459;&#35201;&#27714;&#30456;&#31526;&#30340;&#33509;&#24178;&#26368;&#20339;&#21305;&#37197;&#30340;&#25991;&#26723;&#37096;&#20998;&#65292;&#28982;&#21518;&#20351;&#29992;LLM&#23545;&#36825;&#20123;&#36873;&#25321;&#36827;&#34892;&#36807;&#28388;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26102;&#31354;&#22330;&#26223;&#22270;&#34920;&#31034;&#26469;&#35299;&#20915;&#35270;&#39057;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20013;&#32454;&#31890;&#24230;&#31354;&#38388;&#35821;&#20041;&#21644;&#26102;&#38388;&#24314;&#27169;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#38754;&#21521;&#29305;&#23450;&#30446;&#26631;&#30340;VidSRL&#26694;&#26550;&#65292;&#36890;&#36807;&#22330;&#26223;-&#20107;&#20214;&#26144;&#23556;&#26426;&#21046;&#26469;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05081</link><description>&lt;p&gt;
&#20026;&#35270;&#39057;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#26500;&#24314;&#20840;&#38754;&#30340;&#26102;&#31354;&#22330;&#26223;&#22270;
&lt;/p&gt;
&lt;p&gt;
Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic Role Labeling. (arXiv:2308.05081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26102;&#31354;&#22330;&#26223;&#22270;&#34920;&#31034;&#26469;&#35299;&#20915;&#35270;&#39057;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20013;&#32454;&#31890;&#24230;&#31354;&#38388;&#35821;&#20041;&#21644;&#26102;&#38388;&#24314;&#27169;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#38754;&#21521;&#29305;&#23450;&#30446;&#26631;&#30340;VidSRL&#26694;&#26550;&#65292;&#36890;&#36807;&#22330;&#26223;-&#20107;&#20214;&#26144;&#23556;&#26426;&#21046;&#26469;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;(VidSRL)&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#39044;&#27979;-&#21442;&#25968;&#20107;&#20214;&#32467;&#26500;&#21644;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#26816;&#27979;&#20986;&#26174;&#33879;&#30340;&#20107;&#20214;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#21162;&#21147;&#24050;&#32463;&#25552;&#20986;&#20102;VidSRL&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#32570;&#28857;&#65292;&#21253;&#25324;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#31354;&#38388;&#22330;&#26223;&#24863;&#30693;&#21644;&#19981;&#36275;&#30340;&#35270;&#39057;&#26102;&#38388;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22522;&#20110;&#29616;&#26377;&#30340;&#21160;&#24577;&#22330;&#26223;&#22270;&#32467;&#26500;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#38754;&#30340;&#26102;&#31354;&#22330;&#26223;&#22270;(Holistic Spatio-Temporal Scene Graph)&#34920;&#31034;&#65292;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#35270;&#39057;&#30340;&#32454;&#31890;&#24230;&#31354;&#38388;&#35821;&#20041;&#21644;&#26102;&#38388;&#21160;&#24577;&#29305;&#24615;&#20197;&#36827;&#34892;VidSRL&#12290;&#22312;Holistic Spatio-Temporal Scene Graph&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29305;&#23450;&#30446;&#26631;&#30340;VidSRL&#26694;&#26550;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#22330;&#26223;-&#20107;&#20214;&#26144;&#23556;&#26426;&#21046;&#65292;&#20197;&#24357;&#21512;&#24213;&#23618;&#22330;&#26223;&#32467;&#26500;&#19982;&#39640;&#32423;&#20107;&#20214;&#35821;&#20041;&#32467;&#26500;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24418;&#25104;&#19968;&#20010;&#25972;&#20307;&#23618;&#27425;&#30340;&#22330;&#26223;-&#20107;&#20214;(ICE)&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#36845;&#20195;&#25805;&#20316;&#20197;&#36880;&#27493;&#25913;&#36827;VidSRL&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Semantic Role Labeling (VidSRL) aims to detect the salient events from given videos, by recognizing the predict-argument event structures and the interrelationships between events. While recent endeavors have put forth methods for VidSRL, they can be mostly subject to two key drawbacks, including the lack of fine-grained spatial scene perception and the insufficiently modeling of video temporality. Towards this end, this work explores a novel holistic spatio-temporal scene graph (namely HostSG) representation based on the existing dynamic scene graph structures, which well model both the fine-grained spatial semantics and temporal dynamics of videos for VidSRL. Built upon the HostSG, we present a nichetargeting VidSRL framework. A scene-event mapping mechanism is first designed to bridge the gap between the underlying scene structure and the high-level event semantic structure, resulting in an overall hierarchical scene-event (termed ICE) graph structure. We further perform itera
&lt;/p&gt;</description></item><item><title>&#35813;&#25253;&#21578;&#20851;&#27880;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#20026;&#20381;&#25454;&#32534;&#21046;&#20102;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#21015;&#34920;&#12290;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;ChatGPT&#34920;&#29616;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04889</link><description>&lt;p&gt;
NLLG&#23395;&#24230;arXiv&#25253;&#21578; 06/23&#65306;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#26159;&#20160;&#20040;&#65311;&#65288;arXiv:2308.04889v1 [cs.CY]&#65289;
&lt;/p&gt;
&lt;p&gt;
NLLG Quarterly arXiv Report 06/23: What are the most influential current AI Papers?. (arXiv:2308.04889v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25253;&#21578;&#20851;&#27880;&#24403;&#21069;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;AI&#35770;&#25991;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#20026;&#20381;&#25454;&#32534;&#21046;&#20102;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#21015;&#34920;&#12290;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;ChatGPT&#34920;&#29616;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;Generative Artificial Intelligence&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;Natural Language Processing&#65292;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;Machine Learning&#65292;ML&#65289;&#65289;&#20449;&#24687;&#30340;&#24555;&#36895;&#22686;&#38271;&#32473;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#20351;&#24471;&#20182;&#20204;&#38590;&#20197;&#36319;&#19978;&#26368;&#26032;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#20449;&#24687;&#36807;&#36733;&#30340;&#38382;&#39064;&#65292;Bielefeld&#22823;&#23398;&#30340;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#32452;&#22312;&#26412;&#25253;&#21578;&#20013;&#19987;&#27880;&#20110;&#35782;&#21035;arXiv&#19978;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#65292;&#29305;&#21035;&#20851;&#27880;NLP&#21644;ML&#12290;&#20854;&#30446;&#26631;&#26159;&#20026;&#26368;&#30456;&#20851;&#19988;&#34987;&#24191;&#27867;&#35752;&#35770;&#30340;&#30740;&#31350;&#25552;&#20379;&#24555;&#36895;&#25351;&#21335;&#65292;&#20197;&#24110;&#21161;&#26032;&#26469;&#32773;&#21644;&#24050;&#26377;&#30740;&#31350;&#20154;&#21592;&#36319;&#19978;&#24403;&#21069;&#36235;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;2023&#24180;&#19978;&#21322;&#24180;&#30340;&#26631;&#20934;&#21270;&#24341;&#29992;&#35745;&#25968;&#32534;&#21046;&#20102;&#19968;&#20010;&#30001;40&#31687;&#26368;&#21463;&#27426;&#36814;&#30340;&#35770;&#25991;&#32452;&#25104;&#30340;&#21015;&#34920;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;2023&#24180;&#19978;&#21322;&#24180;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#21644;&#20855;&#20307;&#32780;&#35328;&#30340;ChatGPT&#30456;&#20851;&#30340;&#35770;&#25991;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#32780;ChatGPT&#26174;&#31034;&#20986;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of information in the field of Generative Artificial Intelligence (AI), particularly in the subfields of Natural Language Processing (NLP) and Machine Learning (ML), presents a significant challenge for researchers and practitioners to keep pace with the latest developments. To address the problem of information overload, this report by the Natural Language Learning Group at Bielefeld University focuses on identifying the most popular papers on arXiv, with a specific emphasis on NLP and ML. The objective is to offer a quick guide to the most relevant and widely discussed research, aiding both newcomers and established researchers in staying abreast of current trends. In particular, we compile a list of the 40 most popular papers based on normalized citation counts from the first half of 2023. We observe the dominance of papers related to Large Language Models (LLMs) and specifically ChatGPT during the first half of 2023, with the latter showing signs of declining popul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25913;&#36827;&#22312;&#20855;&#26377;&#20805;&#20998;&#35299;&#37322;&#24615;&#32972;&#26223;&#19979;&#65292;&#20351;&#29992;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#35757;&#32451;&#20013;&#26410;&#35265;&#30340;&#25361;&#25112;&#24615;&#30701;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#29702;&#25454;&#29983;&#25104;&#21644;&#23494;&#38598;&#26816;&#32034;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#29702;&#25454;&#25490;&#21517;&#27169;&#22411;&#36827;&#34892;&#35780;&#20998;&#21644;&#32452;&#21512;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#22686;&#24378;&#26816;&#32034;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#36739;&#23567;&#30340;&#25512;&#29702;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#38271;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.04711</link><description>&lt;p&gt;
&#20351;&#29992;&#29702;&#30001;&#29983;&#25104;&#21644;&#23494;&#38598;&#26816;&#32034;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Answering Unseen Questions With Smaller Language\\Models Using Rationale Generation and Dense Retrieval. (arXiv:2308.04711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25913;&#36827;&#22312;&#20855;&#26377;&#20805;&#20998;&#35299;&#37322;&#24615;&#32972;&#26223;&#19979;&#65292;&#20351;&#29992;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#35757;&#32451;&#20013;&#26410;&#35265;&#30340;&#25361;&#25112;&#24615;&#30701;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#29702;&#25454;&#29983;&#25104;&#21644;&#23494;&#38598;&#26816;&#32034;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#29702;&#25454;&#25490;&#21517;&#27169;&#22411;&#36827;&#34892;&#35780;&#20998;&#21644;&#32452;&#21512;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#22686;&#24378;&#26816;&#32034;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#36739;&#23567;&#30340;&#25512;&#29702;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#38271;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#20379;&#36275;&#22815;&#30340;&#35299;&#37322;&#24615;&#32972;&#26223;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#35777;&#26126;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25361;&#25112;&#24615;&#30340;&#26080;&#27861;&#22312;&#35757;&#32451;&#20013;&#35265;&#36807;&#30340;&#30701;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#36827;&#19968;&#27493;&#25913;&#36827;&#35813;&#22330;&#26223;&#30340;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#27880;&#37325;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#29702;&#30001;&#19982;&#36890;&#36807;&#22810;&#36718;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#21019;&#24314;&#30340;&#26356;&#38271;&#19978;&#19979;&#25991;&#32467;&#21512;&#36215;&#26469;&#12290;&#31532;&#19968;&#20010;&#26041;&#27861;&#65288;$RR$&#65289;&#28041;&#21450;&#35757;&#32451;&#19968;&#20010;&#29702;&#25454;&#25490;&#21517;&#27169;&#22411;&#65292;&#20197;&#35780;&#20998;&#30340;&#26041;&#24335;&#34913;&#37327;&#29983;&#25104;&#30340;&#29702;&#30001;&#21644;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#35780;&#20998;&#20351;&#29992;&#22810;&#31181;&#32452;&#21512;&#31574;&#30053;&#20174;&#20004;&#20010;&#30693;&#35782;&#28304;&#20013;&#33719;&#24471;&#32452;&#21512;&#19978;&#19979;&#25991;&#12290;&#23545;&#20110;&#31532;&#20108;&#31181;&#26041;&#27861;&#65288;$RATD$&#65289;&#65292;&#25105;&#20204;&#20351;&#29992;&#22686;&#24378;&#26816;&#32034;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#36739;&#23567;&#30340;&#25512;&#29702;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#29087;&#32451;&#22320;&#21033;&#29992;&#26469;&#33258;&#26356;&#38271;&#25991;&#26412;&#24207;&#21015;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#33021;&#37096;&#20998;&#20855;&#26377;&#35777;&#25454;&#24615;&#19988;&#39057;&#32321;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
When provided with sufficient explanatory context, smaller Language Models have been shown to exhibit strong reasoning ability on challenging short-answer question-answering tasks where the questions are unseen in training. We evaluate two methods for further improvement in this setting. Both methods focus on combining rationales generated by a larger Language Model with longer contexts created from a multi-hop dense retrieval system. The first method ($\textit{RR}$) involves training a Rationale Ranking model to score both generated rationales and retrieved contexts with respect to relevance and truthfulness. We then use the scores to derive combined contexts from both knowledge sources using a number of combinatory strategies. For the second method ($\textit{RATD}$) we train a smaller Reasoning model using retrieval-augmented training datasets such that it becomes proficient at utilising relevant information from longer text sequences that may be only partially evidential and frequen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#21477;&#38405;&#35835;&#22120;&#19982;&#20256;&#32479;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20960;&#20046;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04566</link><description>&lt;p&gt;
&#21333;&#21477;&#38405;&#35835;&#22120;&#65306;&#35299;&#20915;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Single-Sentence Reader: A Novel Approach for Addressing Answer Position Bias. (arXiv:2308.04566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21333;&#21477;&#38405;&#35835;&#22120;&#19982;&#20256;&#32479;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20960;&#20046;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#27169;&#22411;&#24448;&#24448;&#21033;&#29992;&#20266;&#30456;&#20851;&#24615;&#65288;&#20063;&#31216;&#20026;&#25968;&#25454;&#38598;&#20559;&#24046;&#25110;&#30740;&#31350;&#30028;&#30340;&#26631;&#27880;&#24037;&#20214;&#65289;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#22312;&#19981;&#23436;&#20840;&#29702;&#35299;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;MRC&#20219;&#21153;&#65292;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23548;&#33268;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#20302;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#35757;&#32451;&#38382;&#39064;&#20013;&#26377;&#30456;&#24403;&#27604;&#20363;&#30340;&#31572;&#26696;&#20165;&#20301;&#20110;&#19978;&#19979;&#25991;&#30340;&#31532;&#19968;&#21477;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;MRC&#20013;&#30340;&#31572;&#26696;&#20301;&#32622;&#20559;&#20506;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#21333;&#21477;&#38405;&#35835;&#22120;&#30340;&#32467;&#26524;&#20960;&#20046;&#19982;&#20256;&#32479;&#35757;&#32451;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#21333;&#21477;&#38405;&#35835;&#22120;&#36935;&#21040;&#30340;&#20960;&#20010;&#25361;&#25112;&#21644;&#25552;&#20986;&#30340;&#24212;&#23545;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Reading Comprehension (MRC) models tend to take advantage of spurious correlations (also known as dataset bias or annotation artifacts in the research community). Consequently, these models may perform the MRC task without fully comprehending the given context and question, which is undesirable since it may result in low robustness against distribution shift. This paper delves into the concept of answer-position bias, where a significant percentage of training questions have answers located solely in the first sentence of the context. We propose a Single-Sentence Reader as a new approach for addressing answer position bias in MRC. We implement this approach using six different models and thoroughly analyze their performance. Remarkably, our proposed Single-Sentence Readers achieve results that nearly match those of models trained on conventional training sets, proving their effectiveness. Our study also discusses several challenges our Single-Sentence Readers encounter and prop
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#23545;&#35805;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#27169;&#24577;&#21644;&#19978;&#19979;&#25991;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#35299;&#32544;&#26426;&#21046;&#26469;&#21516;&#26102;&#24314;&#27169;&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#24615;&#21644;&#23545;&#35805;&#30340;&#35821;&#22659;&#21270;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04502</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#22810;&#27169;&#24773;&#24863;&#35782;&#21035;&#20013;&#37325;&#26032;&#23457;&#35270;&#27169;&#24577;&#21644;&#19978;&#19979;&#25991;&#30340;&#35299;&#32544;&#21644;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition. (arXiv:2308.04502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#23545;&#35805;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#27169;&#24577;&#21644;&#19978;&#19979;&#25991;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#35299;&#32544;&#26426;&#21046;&#26469;&#21516;&#26102;&#24314;&#27169;&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#24615;&#21644;&#23545;&#35805;&#30340;&#35821;&#22659;&#21270;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#22330;&#26223;&#19979;&#65292;&#20351;&#26426;&#22120;&#33021;&#22815;&#29702;&#35299;&#20154;&#31867;&#24773;&#24863;&#22312;&#22810;&#27169;&#24577;&#35821;&#22659;&#19979;&#30340;&#30740;&#31350;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#65292;&#36825;&#20010;&#20219;&#21153;&#34987;&#31216;&#20026;&#23545;&#35805;&#24335;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65288;MM-ERC&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;MM-ERC&#19968;&#30452;&#21463;&#21040;&#20851;&#27880;&#65292;&#35768;&#22810;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#23558;MM-ERC&#35270;&#20026;&#26631;&#20934;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35299;&#32544;&#21644;&#34701;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#26469;&#26368;&#22823;&#21270;&#29305;&#24449;&#30340;&#25928;&#29992;&#12290;&#28982;&#32780;&#22312;&#37325;&#26032;&#23457;&#35270;MM-ERC&#30340;&#29305;&#28857;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#29305;&#24449;&#35299;&#32544;&#21644;&#34701;&#21512;&#30340;&#27493;&#39588;&#20013;&#65292;&#26082;&#24212;&#35813;&#36866;&#24403;&#22320;&#24314;&#27169;&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#20063;&#24212;&#35813;&#24314;&#27169;&#23545;&#35805;&#30340;&#35821;&#22659;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20805;&#20998;&#32771;&#34385;&#19978;&#36848;&#35266;&#28857;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;&#19968;&#26041;&#38754;&#65292;&#22312;&#29305;&#24449;&#35299;&#32544;&#38454;&#27573;&#65292;&#25105;&#20204;&#26681;&#25454;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#23618;&#35299;&#32544;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been a hot research topic to enable machines to understand human emotions in multimodal contexts under dialogue scenarios, which is tasked with multimodal emotion analysis in conversation (MM-ERC). MM-ERC has received consistent attention in recent years, where a diverse range of methods has been proposed for securing better task performance. Most existing works treat MM-ERC as a standard multimodal classification problem and perform multimodal feature disentanglement and fusion for maximizing feature utility. Yet after revisiting the characteristic of MM-ERC, we argue that both the feature multimodality and conversational contextualization should be properly modeled simultaneously during the feature disentanglement and fusion steps. In this work, we target further pushing the task performance by taking full consideration of the above insights. On the one hand, during feature disentanglement, based on the contrastive learning technique, we devise a Dual-level Disentanglement Mec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#26680;&#25351;&#20195;&#35299;&#20915;&#26041;&#26696;&#24341;&#20837;&#21040;&#23545;&#35805;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#38598;DialogRE^C+&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#26680;&#25351;&#20195;&#30693;&#35782;&#21487;&#20197;&#22686;&#24378;&#21442;&#25968;&#20851;&#31995;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#25552;&#21319;DRE&#20219;&#21153;&#20013;&#36215;&#21040;&#31215;&#26497;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.04498</link><description>&lt;p&gt;
DialogRE^C+&#65306;DialogRE&#22312;&#23545;&#35805;&#20013;&#20851;&#31995;&#25277;&#21462;&#20013;&#26680;&#25351;&#20195;&#24110;&#21161;&#30340;&#25193;&#23637;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
DialogRE^C+: An Extension of DialogRE to Investigate How Much Coreference Helps Relation Extraction in Dialogs. (arXiv:2308.04498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#26680;&#25351;&#20195;&#35299;&#20915;&#26041;&#26696;&#24341;&#20837;&#21040;&#23545;&#35805;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#38598;DialogRE^C+&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#26680;&#25351;&#20195;&#30693;&#35782;&#21487;&#20197;&#22686;&#24378;&#21442;&#25968;&#20851;&#31995;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#25552;&#21319;DRE&#20219;&#21153;&#20013;&#36215;&#21040;&#31215;&#26497;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20851;&#31995;&#25277;&#21462;(DRE)&#26159;&#35782;&#21035;&#23545;&#35805;&#25991;&#26412;&#20013;&#21442;&#25968;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#20294;&#24120;&#35265;&#30340;&#38382;&#39064;&#26159;&#20154;&#31216;&#20195;&#35789;&#12289;&#23454;&#20307;&#21644;&#21457;&#35328;&#32773;&#30340;&#26680;&#25351;&#20195;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;DialogRE^C+&#65292;&#23558;&#26680;&#25351;&#20195;&#35299;&#20915;&#26041;&#26696;&#24341;&#20837;&#21040;DRE&#22330;&#26223;&#20013;&#12290;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#26680;&#25351;&#20195;&#30693;&#35782;&#65292;&#26399;&#26395;&#22686;&#24378;&#21442;&#25968;&#20851;&#31995;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;DialogRE^C+&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#29616;&#26377;&#30340;DialogRE&#25968;&#25454;&#25163;&#21160;&#27880;&#37322;&#20102;&#24635;&#20849;5,068&#20010;&#26680;&#25351;&#20195;&#38142;&#65292;&#28085;&#30422;&#20102;36,369&#20010;&#21442;&#25968;&#25552;&#21450;&#12290;&#20854;&#20013;&#65292;&#26126;&#30830;&#26631;&#35760;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26680;&#25351;&#20195;&#38142;&#31867;&#22411;&#65292;&#20998;&#21035;&#26159;&#21457;&#35328;&#32773;&#38142;&#12289;&#20010;&#20154;&#38142;&#12289;&#22320;&#28857;&#38142;&#21644;&#32452;&#32455;&#38142;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;4&#20010;&#22522;&#20110;&#22270;&#30340;DRE&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#26377;&#25928;&#30340;&#26680;&#25351;&#20195;&#34920;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;DRE&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#22522;&#20110;&#25105;&#20204;&#30340;&#27880;&#37322;&#35757;&#32451;&#20102;&#19968;&#20010;&#26680;&#25351;&#20195;&#35299;&#20915;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#33258;&#21160;&#25552;&#21462;&#30340;&#26680;&#25351;&#20195;&#23545;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue relation extraction (DRE) that identifies the relations between argument pairs in dialogue text, suffers much from the frequent occurrence of personal pronouns, or entity and speaker coreference. This work introduces a new benchmark dataset DialogRE^C+, introducing coreference resolution into the DRE scenario. With the aid of high-quality coreference knowledge, the reasoning of argument relations is expected to be enhanced. In DialogRE^C+ dataset, we manually annotate total 5,068 coreference chains over 36,369 argument mentions based on the existing DialogRE data, where four different coreference chain types namely speaker chain, person chain, location chain and organization chain are explicitly marked. We further develop 4 coreference-enhanced graph-based DRE models, which learn effective coreference representations for improving the DRE task. We also train a coreference resolution model based on our annotations and evaluate the effect of automatically extracted coreference c
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22810;&#36339;&#25512;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23545;&#35805;&#24773;&#24863;&#20998;&#31867;&#21644;&#34892;&#20026;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#26174;&#24335;&#24314;&#27169;&#24773;&#24863;&#21644;&#34892;&#20026;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20174;&#32780;&#25552;&#21462;&#20016;&#23500;&#30340;&#24773;&#24863;&#21644;&#34892;&#20026;&#32447;&#32034;&#65292;&#23454;&#29616;&#26377;&#25928;&#19988;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.04424</link><description>&lt;p&gt;
&#21452;&#21521;&#22810;&#36339;&#25512;&#29702;&#27169;&#22411;&#29992;&#20110;&#32852;&#21512;&#23545;&#35805;&#24773;&#24863;&#20998;&#31867;&#21644;&#34892;&#20026;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment Classification and Act Recognition. (arXiv:2308.04424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04424
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22810;&#36339;&#25512;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23545;&#35805;&#24773;&#24863;&#20998;&#31867;&#21644;&#34892;&#20026;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#26174;&#24335;&#24314;&#27169;&#24773;&#24863;&#21644;&#34892;&#20026;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20174;&#32780;&#25552;&#21462;&#20016;&#23500;&#30340;&#24773;&#24863;&#21644;&#34892;&#20026;&#32447;&#32034;&#65292;&#23454;&#29616;&#26377;&#25928;&#19988;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#24863;&#20998;&#31867;&#65288;DSC&#65289;&#21644;&#34892;&#20026;&#35782;&#21035;&#65288;DAR&#65289;&#30340;&#32852;&#21512;&#20219;&#21153;&#26088;&#22312;&#21516;&#26102;&#39044;&#27979;&#23545;&#35805;&#20013;&#27599;&#20010;&#35805;&#35821;&#30340;&#24773;&#24863;&#26631;&#31614;&#21644;&#34892;&#20026;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#33021;&#21333;&#21521;&#32534;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#19978;&#19979;&#25991;&#30340;&#20840;&#38754;&#29702;&#35299;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#24773;&#24863;&#21644;&#34892;&#20026;&#26631;&#31614;&#20043;&#38388;&#30340;&#26174;&#24335;&#20851;&#32852;&#65292;&#23548;&#33268;&#23545;&#20016;&#23500;&#30340;&#24773;&#24863;&#21644;&#34892;&#20026;&#32447;&#32034;&#30340;&#33719;&#21462;&#33021;&#21147;&#19981;&#36275;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#26377;&#25928;&#21644;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22810;&#36339;&#25512;&#29702;&#27169;&#22411;&#65288;BMIM&#65289;&#65292;&#23427;&#21033;&#29992;&#29305;&#24449;&#36873;&#25321;&#32593;&#32476;&#21644;&#21452;&#21521;&#22810;&#36339;&#25512;&#29702;&#32593;&#32476;&#26469;&#36845;&#20195;&#22320;&#25552;&#21462;&#21644;&#25972;&#21512;&#20016;&#23500;&#30340;&#24773;&#24863;&#21644;&#34892;&#20026;&#32447;&#32034;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#21452;&#23398;&#20064;&#26469;&#26126;&#30830;&#24314;&#27169;&#24773;&#24863;&#21644;&#34892;&#20026;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;BMIM&#30340;&#24615;&#33021;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The joint task of Dialog Sentiment Classification (DSC) and Act Recognition (DAR) aims to predict the sentiment label and act label for each utterance in a dialog simultaneously. However, current methods encode the dialog context in only one direction, which limits their ability to thoroughly comprehend the context. Moreover, these methods overlook the explicit correlations between sentiment and act labels, which leads to an insufficient ability to capture rich sentiment and act clues and hinders effective and accurate reasoning. To address these issues, we propose a Bi-directional Multi-hop Inference Model (BMIM) that leverages a feature selection network and a bi-directional multi-hop inference network to iteratively extract and integrate rich sentiment and act clues in a bi-directional manner. We also employ contrastive learning and dual learning to explicitly model the correlations of sentiment and act labels. Our experiments on two widely-used datasets show that BMIM outperforms s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20013;&#31934;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLaMA&#30340;&#20013;&#25991;&#21307;&#23398;LLM&#65292;&#36890;&#36807;&#25972;&#20010;&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#65292;&#20197;&#21450;&#24341;&#20837;&#19968;&#20010;&#20013;&#25991;&#22810;&#36718;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;CMtMedQA&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#27169;&#22411;&#22312;&#22797;&#26434;&#23545;&#35805;&#21644;&#20027;&#21160;&#35810;&#38382;&#21457;&#36215;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03549</link><description>&lt;p&gt;
Zhongjing: &#36890;&#36807;&#19987;&#23478;&#21453;&#39304;&#21644;&#30495;&#23454;&#30340;&#22810;&#36718;&#23545;&#35805;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#21307;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue. (arXiv:2308.03549v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20013;&#31934;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLaMA&#30340;&#20013;&#25991;&#21307;&#23398;LLM&#65292;&#36890;&#36807;&#25972;&#20010;&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#65292;&#20197;&#21450;&#24341;&#20837;&#19968;&#20010;&#20013;&#25991;&#22810;&#36718;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;CMtMedQA&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#27169;&#22411;&#22312;&#22797;&#26434;&#23545;&#35805;&#21644;&#20027;&#21160;&#35810;&#38382;&#21457;&#36215;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#22312;&#29702;&#35299;&#21644;&#22238;&#24212;&#29992;&#25143;&#24847;&#22270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#20013;&#21307;&#23398;&#65289;&#20013;&#65292;&#23427;&#20204;&#22312;&#24120;&#35268;&#20351;&#29992;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#33853;&#21518;&#12290;&#30446;&#21069;&#23558;&#20013;&#21307;&#32435;&#20837;LLM&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20351;&#29992;&#21333;&#36718;&#21644;&#31934;&#31616;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#21307;&#29983;&#19968;&#26679;&#30340;&#20027;&#21160;&#35810;&#38382;&#21644;&#22810;&#36718;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#19981;&#33021;&#22987;&#32456;&#19982;&#19987;&#23478;&#30340;&#23433;&#20840;&#21644;&#19987;&#19994;&#24615;&#23545;&#40784;&#22238;&#22797;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20013;&#21307;&#22522;&#20110;LLaMA&#30340;&#20013;&#25991;&#21307;&#23398;LLM&#8212;&#8212;&#20013;&#31934;&#65292;&#23427;&#23454;&#29616;&#20102;&#20174;&#39044;&#35757;&#32451;&#21040;&#24378;&#21270;&#23398;&#20064;&#30340;&#25972;&#20010;&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;70,000&#20010;&#30495;&#23454;&#21307;&#24739;&#23545;&#35805;&#30340;&#20013;&#25991;&#22810;&#36718;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;CMtMedQA&#65292;&#23427;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22797;&#26434;&#23545;&#35805;&#21644;&#20027;&#21160;&#35810;&#38382;&#21457;&#36215;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Large Language Models (LLMs) have achieved remarkable breakthroughs in understanding and responding to user intents. However, their performance lag behind general use cases in some expertise domains, such as Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue data. These models lack the ability for doctor-like proactive inquiry and multi-turn comprehension and cannot always align responses with safety and professionalism experts. In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM that implements an entire training pipeline from pre-training to reinforcement learning with human feedback (RLHF). Additionally, we introduce a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the model's capability for complex dialogue and proactive inquiry initiation. We define a r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#21040;&#32454;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#26816;&#32034;&#22120;&#65292;&#29992;&#20110;&#25913;&#36827;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;NLPCC 2023&#20013;&#30340;&#20013;&#25991;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#20849;&#20139;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;</title><link>http://arxiv.org/abs/2308.03365</link><description>&lt;p&gt;
&#29992;&#31895;&#21040;&#32454;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#26816;&#32034;&#22120;&#25913;&#36827;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Improving Few-shot and Zero-shot Entity Linking with Coarse-to-Fine Lexicon-based Retriever. (arXiv:2308.03365v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#21040;&#32454;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#26816;&#32034;&#22120;&#65292;&#29992;&#20110;&#25913;&#36827;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;NLPCC 2023&#20013;&#30340;&#20013;&#25991;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#20849;&#20139;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#20851;&#27880;&#23614;&#37096;&#21644;&#26032;&#20852;&#23454;&#20307;&#65292;&#36825;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#26356;&#25509;&#36817;&#30495;&#23454;&#22330;&#26223;&#12290;&#20027;&#27969;&#26041;&#27861;&#26159;&#8220;&#26816;&#32034;&#21644;&#37325;&#25490;&#24207;&#8221;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#21040;&#32454;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#26816;&#32034;&#22120;&#65292;&#20197;&#26377;&#25928;&#22320;&#26816;&#32034;&#23454;&#20307;&#20505;&#36873;&#39033;&#65292;&#23427;&#22312;&#20004;&#23618;&#36827;&#34892;&#25805;&#20316;&#12290;&#31532;&#19968;&#23618;&#21033;&#29992;&#23454;&#20307;&#21517;&#31216;&#26816;&#32034;&#31895;&#31890;&#24230;&#20505;&#36873;&#39033;&#65292;&#32780;&#31532;&#20108;&#23618;&#22312;&#31895;&#31890;&#24230;&#20505;&#36873;&#39033;&#20013;&#32553;&#23567;&#25628;&#32034;&#33539;&#22260;&#21040;&#32454;&#31890;&#24230;&#20505;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#31532;&#20108;&#23618;&#20351;&#29992;&#23454;&#20307;&#25551;&#36848;&#26469;&#26377;&#25928;&#28040;&#38500;&#19982;&#29616;&#26377;&#28909;&#38376;&#23454;&#20307;&#21516;&#21517;&#30340;&#23614;&#37096;&#25110;&#26032;&#23454;&#20307;&#30340;&#27495;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;NLPCC 2023&#20849;&#20139;&#20219;&#21153;6&#19978;&#20013;&#25991;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#23454;&#20307;&#38142;&#25509;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot and zero-shot entity linking focus on the tail and emerging entities, which are more challenging but closer to real-world scenarios. The mainstream method is the ''retrieve and rerank'' two-stage framework. In this paper, we propose a coarse-to-fine lexicon-based retriever to retrieve entity candidates in an effective manner, which operates in two layers. The first layer retrieves coarse-grained candidates by leveraging entity names, while the second layer narrows down the search to fine-grained candidates within the coarse-grained ones. In addition, this second layer utilizes entity descriptions to effectively disambiguate tail or new entities that share names with existing popular entities. Experimental results indicate that our approach can obtain superior performance without requiring extensive finetuning in the retrieval stage. Notably, our approach ranks the 1st in NLPCC 2023 Shared Task 6 on Chinese Few-shot and Zero-shot Entity Linking.
&lt;/p&gt;</description></item><item><title>ClassEval&#26159;&#19968;&#31181;&#25163;&#24037;&#26500;&#24314;&#30340;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#22312;&#36825;&#19968;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#35780;&#20272;LLMs&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#12290;GPT-4&#21644;GPT-3.5&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#23545;&#20854;&#20182;LLMs&#26356;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.01861</link><description>&lt;p&gt;
ClassEval: &#19968;&#31181;&#25163;&#24037;&#26500;&#24314;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation. (arXiv:2308.01861v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01861
&lt;/p&gt;
&lt;p&gt;
ClassEval&#26159;&#19968;&#31181;&#25163;&#24037;&#26500;&#24314;&#30340;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65292;&#35813;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#22312;&#36825;&#19968;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#35780;&#20272;LLMs&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#12290;GPT-4&#21644;GPT-3.5&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#30456;&#23545;&#20854;&#20182;LLMs&#26356;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20195;&#30721;&#29983;&#25104;&#22330;&#26223;&#20013;&#35780;&#20272;LLMs&#65292;&#21363;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#25163;&#21160;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;ClassEval&#65292;&#20854;&#20013;&#21253;&#21547;100&#20010;&#31867;&#32423;Python&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#24635;&#20849;&#32791;&#26102;&#32422;500&#20154;&#23567;&#26102;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23545;11&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#30740;&#31350;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#29616;&#26377;&#30340;LLMs&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#30340;&#24615;&#33021;&#35201;&#36828;&#20302;&#20110;&#29420;&#31435;&#30340;&#26041;&#27861;&#32423;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#65288;&#22914;HumanEval&#65289;&#65307;&#32780;&#26041;&#27861;&#32423;&#30340;&#32534;&#30721;&#33021;&#21147;&#19981;&#33021;&#31561;&#21516;&#22320;&#21453;&#26144;LLMs&#22312;&#31867;&#32423;&#32534;&#30721;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-4&#21644;GPT-3.5&#22312;&#31867;&#32423;&#20195;&#30721;&#29983;&#25104;&#19978;&#20173;&#28982;&#34920;&#29616;&#20986;&#30456;&#23545;&#20854;&#20182;LLMs&#26356;&#21331;&#36234;&#30340;&#20248;&#21183;&#65292;&#32780;&#20108;&#32447;&#27169;&#22411;&#21253;&#25324;Instruct-Starcoder&#65292;Instruct-Codegen&#21644;Wizardcoder&#22312;&#24615;&#33021;&#19978;&#38750;&#24120;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e. class-level code generation. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on it, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we have the following main findings. First, we find that all existing LLMs show much worse performance on class-level code generation compared to on standalone method-level code generation benchmarks like HumanEval; and the method-level coding ability cannot equivalently reflect the class-level coding ability among LLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superior than other LLMs on class-level code generation, and the second-tier models includes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with very similar performance. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32972;&#26223;&#19979;&#32416;&#38169;&#38382;&#39064;&#30340;&#20316;&#29992;&#65292;&#31532;&#19968;&#20010;&#23454;&#39564;&#26159;&#23558;&#32416;&#38169;&#20316;&#20026;&#29420;&#31435;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#31532;&#20108;&#20010;&#23454;&#39564;&#21017;&#26159;&#25506;&#35752;&#32416;&#38169;&#20316;&#20026;&#20854;&#20182;NLP&#20219;&#21153;&#30340;&#20934;&#22791;&#20219;&#21153;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#32467;&#26524;&#23558;&#25581;&#31034;&#32416;&#38169;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#23545;&#21508;&#31181;NLP&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.01776</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20173;&#28982;&#23384;&#22312;&#32416;&#38169;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Correction Remain An Problem For Large Language Models?. (arXiv:2308.01776v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32972;&#26223;&#19979;&#32416;&#38169;&#38382;&#39064;&#30340;&#20316;&#29992;&#65292;&#31532;&#19968;&#20010;&#23454;&#39564;&#26159;&#23558;&#32416;&#38169;&#20316;&#20026;&#29420;&#31435;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#31532;&#20108;&#20010;&#23454;&#39564;&#21017;&#26159;&#25506;&#35752;&#32416;&#38169;&#20316;&#20026;&#20854;&#20182;NLP&#20219;&#21153;&#30340;&#20934;&#22791;&#20219;&#21153;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#32467;&#26524;&#23558;&#25581;&#31034;&#32416;&#38169;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#23545;&#21508;&#31181;NLP&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#26029;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#33021;&#21147;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#32416;&#38169;&#38382;&#39064;&#26159;&#21542;&#20173;&#28982;&#23384;&#22312;&#65311;&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#20004;&#20010;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#32416;&#38169;&#30340;&#20316;&#29992;&#12290;&#31532;&#19968;&#20010;&#23454;&#39564;&#23558;&#32416;&#38169;&#20316;&#20026;&#29420;&#31435;&#30340;&#20219;&#21153;&#65292;&#20351;&#29992;GPT&#31867;&#27169;&#22411;&#21644;few-shot learning&#25216;&#26415;&#36827;&#34892;&#38169;&#35823;&#32416;&#27491;&#12290;&#31532;&#20108;&#20010;&#23454;&#39564;&#21017;&#25506;&#35752;&#20102;&#32416;&#38169;&#20316;&#20026;&#20854;&#20182;NLP&#20219;&#21153;&#30340;&#20934;&#22791;&#20219;&#21153;&#30340;&#27010;&#24565;&#65292;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21253;&#21547;&#19968;&#23450;&#31243;&#24230;&#22122;&#22768;&#25110;&#38169;&#35823;&#30340;&#25991;&#26412;&#19978;&#26159;&#21542;&#33021;&#22815;&#23481;&#24525;&#24182;&#34920;&#29616;&#24471;&#36275;&#22815;&#22909;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#23454;&#39564;&#65292;&#25105;&#20204;&#26088;&#22312;&#25581;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#32416;&#38169;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#23545;&#21508;&#31181;NLP&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models, such as GPT, continue to advance the capabilities of natural language processing (NLP), the question arises: does the problem of correction still persist? This paper investigates the role of correction in the context of large language models by conducting two experiments. The first experiment focuses on correction as a standalone task, employing few-shot learning techniques with GPT-like models for error correction. The second experiment explores the notion of correction as a preparatory task for other NLP tasks, examining whether large language models can tolerate and perform adequately on texts containing certain levels of noise or errors. By addressing these experiments, we aim to shed light on the significance of correction in the era of large language models and its implications for various NLP applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25216;&#33021;&#25351;&#23548; (SKiC) &#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#28436;&#31034;&#22522;&#26412;&#25216;&#33021;&#21644;&#32452;&#21512;&#24615;&#31034;&#20363;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#35299;&#20915;&#26356;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#21462;&#24471;&#20960;&#20046;&#23436;&#32654;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.00304</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#38145;&#32452;&#21512;&#24615;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;: &#25216;&#33021;&#25351;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models. (arXiv:2308.00304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25216;&#33021;&#25351;&#23548; (SKiC) &#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#28436;&#31034;&#22522;&#26412;&#25216;&#33021;&#21644;&#32452;&#21512;&#24615;&#31034;&#20363;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#35299;&#20915;&#26356;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#21462;&#24471;&#20960;&#20046;&#23436;&#32654;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#28608;&#21457;&#32452;&#21512;&#24615;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#32452;&#21512;&#24615;&#27867;&#21270;&#20351;&#24471;LLMs&#33021;&#22815;&#35299;&#20915;&#27604;&#23427;&#20204;&#25152;&#35265;&#36807;&#30340;&#38382;&#39064;&#26356;&#22256;&#38590;&#30340;&#38382;&#39064;&#65288;&#21363;&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#65289;&#65292;&#36825;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#20851;&#38190;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#36825;&#31181;&#24418;&#24335;&#30340;&#25512;&#29702;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#25216;&#33021;&#25351;&#23548;&#65288;SKiC&#65289;&#25552;&#31034;&#65292;&#23427;&#25351;&#23548;LLMs&#22914;&#20309;&#32452;&#21512;&#22522;&#26412;&#25216;&#33021;&#26469;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#30456;&#21516;&#30340;&#25552;&#31034;&#19978;&#23637;&#31034;&#25216;&#33021;&#21644;&#32452;&#21512;&#24615;&#31034;&#20363;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20165;&#20165;&#36890;&#36807;&#20004;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#30340;SKiC&#25552;&#31034;&#22312;&#25216;&#33021;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#33021;&#21147;&#20043;&#38388;&#24418;&#25104;&#20102;&#24378;&#22823;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#36171;&#20104;&#20102;LLMs&#35299;&#20915;&#38656;&#35201;&#21019;&#26032;&#25216;&#33021;&#32452;&#21512;&#30340;&#26410;&#35265;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of eliciting compositional generalization capabilities in large language models (LLMs) with a novel type of prompting strategy. Compositional generalization empowers the LLMs to solve problems that are harder than the ones they have seen (i.e., easy-to-hard generalization), which is a critical reasoning capability of human-like intelligence. However, even the current state-of-the-art LLMs still struggle with this form of reasoning. To bridge this gap, we propose skills-in-context (SKiC) prompting, which instructs LLMs how to compose basic skills to resolve more complex problems. We find that it is crucial to demonstrate both the skills and the compositional examples within the same prompting context. With as few as two examplars, our SKiC prompting initiates strong synergies between skills and their composition capabilities. Notably, it empowers LLMs to solve unseen problems that require innovative skill compositions, achieving near-perfect generalization on a b
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;API&#26041;&#38754;&#20998;&#26512;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#24182;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#24615;&#33021;&#65292;&#23545;Performance&#12289;Security&#12289;Usability&#21644;Documentation&#31561;&#26041;&#38754;&#30340;&#26816;&#27979;&#25928;&#26524;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#21644;&#24320;&#21457;&#32773;&#30740;&#31350;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.16878</link><description>&lt;p&gt;
&#23545;API&#26041;&#38754;&#20998;&#26512;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning for API Aspect Analysis. (arXiv:2307.16878v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16878
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;API&#26041;&#38754;&#20998;&#26512;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#24182;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#24615;&#33021;&#65292;&#23545;Performance&#12289;Security&#12289;Usability&#21644;Documentation&#31561;&#26041;&#38754;&#30340;&#26816;&#27979;&#25928;&#26524;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#21644;&#24320;&#21457;&#32773;&#30740;&#31350;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;- CLAA-&#29992;&#20110;API&#35780;&#35770;&#20013;&#30340;API&#26041;&#38754;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#35757;&#32451;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#30446;&#26631;&#20989;&#25968;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#24615;&#33021;&#21644;&#24433;&#21709;&#20998;&#26512;&#35780;&#20272;&#20102;CLAA&#12290;&#23545;&#20110;&#24615;&#33021;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20174;Stack Overflow&#25910;&#38598;&#30340;&#24320;&#21457;&#32773;&#35752;&#35770;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#33719;&#24471;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21464;&#25442;&#22120;&#27169;&#22411;&#22312;&#26816;&#27979;&#24615;&#33021;&#12289;&#23433;&#20840;&#24615;&#12289;&#21487;&#29992;&#24615;&#21644;&#25991;&#26723;&#26041;&#38754;&#31561;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#24433;&#21709;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#21644;&#24320;&#21457;&#32773;&#30740;&#31350;&#12290;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#24182;&#20154;&#24037;&#26631;&#35760;&#30340;200&#20010;&#22312;&#32447;&#35780;&#35770;&#19978;&#65292;CLAA&#36798;&#21040;&#20102;92%&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#21482;&#26377;81.5%&#12290;&#26681;&#25454;&#25105;&#20204;&#23545;10&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#30340;&#24320;&#21457;&#32773;&#30740;&#31350;&#65292;&#20351;&#29992;&#8220;Stack Overflow + CLAA&#8221;&#22312;API&#36873;&#25321;&#36807;&#31243;&#20013;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#20449;&#24515;&#12290;&#22797;&#21046;&#31243;&#24207;&#21253;&#65306;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach - CLAA - for API aspect detection in API reviews that utilizes transformer models trained with a supervised contrastive loss objective function. We evaluate CLAA using performance and impact analysis. For performance analysis, we utilized a benchmark dataset on developer discussions collected from Stack Overflow and compare the results to those obtained using state-of-the-art transformer models. Our experiments show that contrastive learning can significantly improve the performance of transformer models in detecting aspects such as Performance, Security, Usability, and Documentation. For impact analysis, we performed empirical and developer study. On a randomly selected and manually labeled 200 online reviews, CLAA achieved 92% accuracy while the SOTA baseline achieved 81.5%. According to our developer study involving 10 participants, the use of 'Stack Overflow + CLAA' resulted in increased accuracy and confidence during API selection. Replication package: 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16680</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26223;&#35266;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39046;&#20808;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#20063;&#26292;&#38706;&#20986;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#25991;&#29486;&#65292;&#20294;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#28041;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#26032;&#20852;&#23041;&#32961;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36131;&#20219;&#36825;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#24352;&#35814;&#23613;&#30340;&#22320;&#22270;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20123;&#21162;&#21147;&#23545;&#20110;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09702</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;LLM&#24341;&#23548;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#35760;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#20013;&#20960;&#20046;&#19981;&#22686;&#21152;&#20219;&#20309;&#24320;&#38144;&#65292;&#24182;&#20351;&#24471;&#24341;&#23548;&#29983;&#25104;&#22312;&#23454;&#38469;&#20013;&#21487;&#34892;&#12290;&#22312;&#24320;&#28304;Python&#24211;Outlines&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
&lt;/p&gt;</description></item><item><title>MMBench&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#24320;&#21457;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.06281</link><description>&lt;p&gt;
MMBench: &#24744;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20840;&#33021;&#29699;&#21592;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06281
&lt;/p&gt;
&lt;p&gt;
MMBench&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#24320;&#21457;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#20449;&#24687;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#35780;&#20272;&#36825;&#20123;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#26410;&#26469;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20256;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;VQAv2&#25110;COCO Caption&#25552;&#20379;&#20102;&#23450;&#37327;&#30340;&#24615;&#33021;&#27979;&#37327;&#65292;&#20294;&#22312;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#21644;&#38750;&#40065;&#26834;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26368;&#36817;&#30340;&#20027;&#35266;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;OwlEval&#65292;&#36890;&#36807;&#25972;&#21512;&#20154;&#21147;&#36164;&#28304;&#65292;&#23545;&#27169;&#22411;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20294;&#19981;&#21487;&#25193;&#23637;&#24182;&#19988;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MMBench&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#12290;MMBench&#31995;&#32479;&#22320;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#20027;&#35201;&#30001;&#20004;&#20010;&#20803;&#32032;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#20803;&#32032;&#26159;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#35780;&#20272;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#31867;&#20284;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39564;&#35777;&#20302;&#32622;&#20449;&#24230;&#29983;&#25104;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;LLMs&#30340;&#24187;&#35273;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#36827;&#34892;&#39564;&#35777;&#65292;&#24182;&#32531;&#35299;&#24187;&#35273;&#65292;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03987</link><description>&lt;p&gt;
&#23396;&#25484;&#38590;&#40483;&#65306;&#36890;&#36807;&#39564;&#35777;&#20302;&#32622;&#20449;&#24230;&#29983;&#25104;&#26816;&#27979;&#21644;&#20943;&#36731;LLMs&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. (arXiv:2307.03987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39564;&#35777;&#20302;&#32622;&#20449;&#24230;&#29983;&#25104;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;LLMs&#30340;&#24187;&#35273;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#36827;&#34892;&#39564;&#35777;&#65292;&#24182;&#32531;&#35299;&#24187;&#35273;&#65292;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#27969;&#30021;&#36830;&#36143;&#30340;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#20250;&#21457;&#29983;&#8220;&#24187;&#35273;&#8221;&#65292;&#20005;&#37325;&#24433;&#21709;&#20854;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#26816;&#27979;&#21644;&#20943;&#36731;&#29983;&#25104;&#36807;&#31243;&#20013;&#24187;&#35273;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#27169;&#22411;&#30340;&#36923;&#36753;&#36755;&#20986;&#20540;&#35782;&#21035;&#21487;&#33021;&#30340;&#24187;&#35273;&#20505;&#36873;&#39033;&#65292;&#36890;&#36807;&#39564;&#35777;&#36807;&#31243;&#26816;&#26597;&#20854;&#27491;&#30830;&#24615;&#65292;&#20943;&#36731;&#26816;&#27979;&#21040;&#30340;&#24187;&#35273;&#65292;&#28982;&#21518;&#32487;&#32493;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#19982;&#8220;&#25991;&#31456;&#29983;&#25104;&#20219;&#21153;&#8221;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26816;&#27979;&#21644;&#20943;&#36731;&#25216;&#26415;&#30340;&#20010;&#20307;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26816;&#27979;&#25216;&#26415;&#30340;&#21484;&#22238;&#29575;&#36798;&#21040;&#20102;88&#65285;&#65292;&#32531;&#35299;&#25216;&#26415;&#25104;&#21151;&#32531;&#35299;&#20102;57.6&#65285;&#34987;&#27491;&#30830;&#26816;&#27979;&#21040;&#30340;&#24187;&#35273;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20943;&#36731;&#25216;&#26415;&#19981;&#20250;&#24341;&#20837;&#26032;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed large language models have achieved remarkable success in generating fluent and coherent text. However, these models often tend to 'hallucinate' which critically hampers their reliability. In this work, we address this crucial problem and propose an approach that actively detects and mitigates hallucinations during the generation process. Specifically, we first identify the candidates of potential hallucination leveraging the model's logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process. Through extensive experiments with the 'article generation task', we first demonstrate the individual efficacy of our detection and mitigation techniques. Specifically, the detection technique achieves a recall of 88% and the mitigation technique successfully mitigates 57.6% of the correctly detected hallucinations. Importantly, our mitigation technique does not introduce new ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07848</link><description>&lt;p&gt;
GEmo-CLAP: &#38754;&#21521;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#26368;&#36817;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEmo-CLAP&#30340;&#39640;&#25928;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;CLAP&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24773;&#24863;CLAP&#27169;&#22411;&#65288;&#31216;&#20026;Emo-CLAP&#65289;&#65292;&#29992;&#20110;SER&#12290;&#28982;&#21518;&#65292;&#32771;&#34385;&#21040;&#22312;&#35821;&#38899;&#24773;&#24863;&#24314;&#27169;&#20013;&#24615;&#21035;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#65292;&#26469;&#25972;&#21512;&#35821;&#38899;&#20449;&#21495;&#30340;&#24773;&#24863;&#21644;&#24615;&#21035;&#20449;&#24687;&#65292;&#24418;&#25104;&#26356;&#21512;&#29702;&#30340;&#30446;&#26631;&#12290;&#22312;IEMOCAP&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;Emo-CLAP&#27169;&#22411;&#65288;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11095</link><description>&lt;p&gt;
&#28608;&#21457;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#20197;&#23454;&#29616;&#38646;-shot&#20219;&#21153;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;Whisper&#30340;&#26032;&#20852;&#21151;&#33021;&#65292;&#22312;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;&#27169;&#22411;&#21518;&#65292;&#36866;&#24212;&#20102;&#26410;&#35265;&#36807;&#30340;AVSR&#65292;CS-ASR&#21644;ST&#19977;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#35201;&#20040;&#21033;&#29992;&#21478;&#19968;&#20010;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#25805;&#20316;&#40664;&#35748;&#25552;&#31034;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#20351;&#36825;&#19977;&#20010;&#38646;-shot&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;10%&#21040;45&#65285;&#65292;&#29978;&#33267;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;SotA&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;Whisper&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#21253;&#25324;&#20854;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#21475;&#38899;&#30340;&#20559;&#22909;&#20197;&#21450;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/jasonppy/PromptingWhisper&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14104</link><description>&lt;p&gt;
&#20174;&#24369;&#25991;&#26412;&#30417;&#30563;&#20013;&#23398;&#20064;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#38469;&#20114;&#21160;&#26159;&#22810;&#26679;&#19988;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23558;&#23427;&#20204;&#35270;&#20026;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#21487;&#33021;&#30340;&#20114;&#21160;&#30340;&#37325;&#23614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#20154;&#38469;&#20114;&#21160;&#30340;&#33539;&#24335;&#65292;&#23558;&#20854;&#20316;&#20026;&#33258;&#30001;&#25991;&#26412;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#24773;&#20917;&#21644;&#20154;&#38469;&#20851;&#31995;&#30340;&#26080;&#38480;&#31354;&#38388;&#36827;&#34892;&#28789;&#27963;&#24314;&#27169;&#12290;&#20026;&#20102;&#20811;&#26381;&#32570;&#20047;&#29305;&#23450;&#20110;&#27492;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#23383;&#24149;&#25968;&#25454;&#65292;&#20197;&#27492;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#30340;&#20266;&#26631;&#31614;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#33021;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#36890;&#36807;&#34913;&#37327;&#25105;&#20204;&#39044;&#27979;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#19982;&#20107;&#23454;&#30340;&#22522;&#30784;&#24615;&#30340;&#21508;&#31181;&#25351;&#26631;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#21462;&#21644;&#25490;&#24207;&#37329;&#34701;&#26415;&#35821;&#19978;&#20041;&#35789;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19981;&#21516;&#26415;&#35821;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#12290;&#35813;&#31995;&#32479;&#24110;&#21161;&#29992;&#25143;&#26356;&#22909;&#22320;&#20102;&#35299;&#22797;&#26434;&#37329;&#34701;&#26415;&#35821;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.13475</link><description>&lt;p&gt;
&#23398;&#20064;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#26469;&#25490;&#24207;&#37329;&#34701;&#26415;&#35821;&#30340;&#19978;&#20041;&#35789;
&lt;/p&gt;
&lt;p&gt;
Learning Semantic Text Similarity to rank Hypernyms of Financial Terms. (arXiv:2303.13475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#21462;&#21644;&#25490;&#24207;&#37329;&#34701;&#26415;&#35821;&#19978;&#20041;&#35789;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19981;&#21516;&#26415;&#35821;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#12290;&#35813;&#31995;&#32479;&#24110;&#21161;&#29992;&#25143;&#26356;&#22909;&#22320;&#20102;&#35299;&#22797;&#26434;&#37329;&#34701;&#26415;&#35821;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29992;&#25143;&#20351;&#29992;&#37329;&#34701;&#26381;&#21153;&#30340;&#26041;&#24335;&#21457;&#29983;&#20102;&#21464;&#38761;&#12290;&#38543;&#30528;&#25968;&#23383;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#29992;&#25143;&#26356;&#21916;&#27426;&#22312;&#32447;&#26041;&#24335;&#26469;&#25191;&#34892;&#37329;&#34701;&#27963;&#21160;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#30340;&#37329;&#34701;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;&#22823;&#22810;&#25968;&#25237;&#36164;&#32773;&#22312;&#20570;&#20986;&#20915;&#31574;&#21069;&#37117;&#20250;&#38405;&#35835;&#36825;&#20123;&#20869;&#23481;&#12290;&#27599;&#20010;&#34892;&#19994;&#37117;&#26377;&#29305;&#23450;&#20110;&#20854;&#36816;&#33829;&#39046;&#22495;&#30340;&#26415;&#35821;&#12290;&#38134;&#34892;&#21644;&#37329;&#34701;&#26381;&#21153;&#20063;&#19981;&#20363;&#22806;&#12290;&#20026;&#20102;&#23436;&#20840;&#29702;&#35299;&#36825;&#20123;&#20869;&#23481;&#65292;&#38656;&#35201;&#23545;&#37329;&#34701;&#26415;&#35821;&#26377;&#28145;&#20837;&#30340;&#20102;&#35299;&#12290;&#24403;&#29992;&#23427;&#25152;&#23646;&#30340;&#24191;&#20041;&#31867;&#21035;&#26469;&#35828;&#26126;&#26102;&#65292;&#19968;&#20010;&#26415;&#35821;&#30340;&#22522;&#26412;&#27010;&#24565;&#21464;&#24471;&#23481;&#26131;&#12290;&#36825;&#20010;&#24191;&#20041;&#31867;&#21035;&#34987;&#31216;&#20026;&#19978;&#20041;&#35789;&#12290;&#20363;&#22914;&#65292;&#8220;&#20538;&#21048;&#8221;&#26159;&#37329;&#34701;&#26415;&#35821;&#8220;&#26367;&#20195;&#20538;&#21048;&#8221;&#30340;&#19978;&#20041;&#35789;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#25552;&#21462;&#21644;&#25490;&#24207;&#32473;&#23450;&#37329;&#34701;&#26415;&#35821;&#30340;&#19978;&#20041;&#35789;&#12290;&#35813;&#31995;&#32479;&#32463;&#36807;&#37329;&#34701;&#25991;&#26412;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19981;&#21516;&#37329;&#34701;&#26415;&#35821;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#29992;&#20110;&#25552;&#39640;&#23545;&#22797;&#26434;&#37329;&#34701;&#26415;&#35821;&#30340;&#29702;&#35299;&#65292;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the years, there has been a paradigm shift in how users access financial services. With the advancement of digitalization more users have been preferring the online mode of performing financial activities. This has led to the generation of a huge volume of financial content. Most investors prefer to go through these contents before making decisions. Every industry has terms that are specific to the domain it operates in. Banking and Financial Services are not an exception to this. In order to fully comprehend these contents, one needs to have a thorough understanding of the financial terms. Getting a basic idea about a term becomes easy when it is explained with the help of the broad category to which it belongs. This broad category is referred to as hypernym. For example, "bond" is a hypernym of the financial term "alternative debenture". In this paper, we propose a system capable of extracting and ranking hypernyms for a given financial term. The system has been trained with fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#30340;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#22312;Open Intent Induction&#20013;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.13099</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#30340;&#38646;&#26679;&#26412;&#24320;&#25918;&#24847;&#22270;&#24402;&#32435;&#65306;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#30340;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#22312;Open Intent Induction&#20013;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#30340;&#24847;&#22270;&#26159;&#23558;&#35813;&#31995;&#32479;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38590;&#39064;&#65306;&#65288;1&#65289;&#29992;&#20110;&#19968;&#33324;&#23884;&#20837;&#30340;SBERT&#65288;2&#65289;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#65288;MDB&#65289;&#29992;&#20110;&#23545;&#35805;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#21450;&#65288;3&#65289;&#29992;&#20110;&#38598;&#32676;&#19987;&#19994;&#35821;&#20041;&#30340;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#65288;PGT&#65289;&#12290; MDB&#19968;&#27425;&#21521;&#27169;&#22411;&#25552;&#20379;&#22810;&#31181;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#39046;&#22495;&#30693;&#35782;&#26469;&#35299;&#20915;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;PGT&#65292;&#23427;&#37319;&#29992;Siamese&#32593;&#32476;&#30452;&#25509;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;PGT&#32858;&#31867;&#23545;&#35805;&#35821;&#21477;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#27169;&#22411;&#19982;MDB&#21644;PGT&#26174;&#30528;&#25552;&#39640;&#20102;Open Intent Induction&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Task Oriented Dialogue (TOD) system, detecting and inducing new intents are two main challenges to apply the system in the real world. In this paper, we suggest the semantic multi-view model to resolve these two challenges: (1) SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized semantic. MDB feeds diverse dialogue datasets to the model at once to tackle the multi-domain problem by learning the multiple domain knowledge. We introduce a novel method PGT, which employs the Siamese network to fine-tune the model with a clustering method directly.Our model can learn how to cluster dialogue utterances by using PGT. Experimental results demonstrate that our multi-view model with MDB and PGT significantly improves the Open Intent Induction performance compared to baseline systems.
&lt;/p&gt;</description></item><item><title>WHOOPS!&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#33509;&#24178;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#21512;&#24120;&#35268;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07274</link><description>&lt;p&gt;
&#25171;&#30772;&#24120;&#35782;&#65306;WHOOPS&#65281;&#19968;&#20010;&#22522;&#20110;&#21512;&#25104;&#21644;&#32452;&#21512;&#22270;&#20687;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07274
&lt;/p&gt;
&lt;p&gt;
WHOOPS!&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#33509;&#24178;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#21512;&#24120;&#35268;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22855;&#24618;&#12289;&#24322;&#24120;&#21644;&#31070;&#31192;&#30340;&#22270;&#20687;&#20250;&#24341;&#36215;&#35266;&#23519;&#32773;&#30340;&#22909;&#22855;&#24515;&#65292;&#22240;&#20026;&#23427;&#20204;&#25361;&#25112;&#20102;&#24120;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;WHOOPS&#65281;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#35774;&#35745;&#24072;&#20351;&#29992;Midjourney&#31561;&#20844;&#20849;&#21487;&#29992;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#21046;&#20316;&#65292;&#24182;&#21253;&#21547;&#33509;&#24178;&#20010;&#20219;&#21153;&#12290;&#38500;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#24182;&#35299;&#37322;&#32473;&#23450;&#22270;&#20687;&#30340;&#24322;&#24120;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weird, unusual, and uncanny images pique the curiosity of observers because they challenge commonsense. For example, an image released during the 2022 world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo playing chess, which playfully violates our expectation that their competition should occur on the football field. Humans can easily recognize and interpret these unconventional images, but can AI models do the same? We introduce WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney. We consider several tasks posed over the dataset. In addition to image captioning, cross-modal matching, and visual question answering, we introduce a difficult explanation generation task, where models must identify and explain why a given image is unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
&lt;/p&gt;</description></item><item><title>SEAM&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#30524;&#21160;&#25511;&#21046;&#21644;&#21477;&#23376;&#22788;&#29702;&#30340;&#27169;&#22411;&#65292;&#20026;&#23454;&#29616;&#38405;&#35835;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#23436;&#25972;&#25968;&#23398;&#27169;&#22411;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2303.05221</link><description>&lt;p&gt;
SEAM:&#19968;&#31181;&#38598;&#25104;&#20102;&#21477;&#23376;&#22788;&#29702;&#19982;&#38405;&#35835;&#20013;&#30524;&#21160;&#30340;&#28608;&#27963;&#32806;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading. (arXiv:2303.05221v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05221
&lt;/p&gt;
&lt;p&gt;
SEAM&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#30524;&#21160;&#25511;&#21046;&#21644;&#21477;&#23376;&#22788;&#29702;&#30340;&#27169;&#22411;&#65292;&#20026;&#23454;&#29616;&#38405;&#35835;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#23436;&#25972;&#25968;&#23398;&#27169;&#22411;&#36808;&#20986;&#20102;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38405;&#35835;&#20013;&#30340;&#30524;&#21160;&#25511;&#21046;&#27169;&#22411;&#36890;&#24120;&#38598;&#20013;&#22312;&#35270;&#35273;&#12289;&#27880;&#24847;&#12289;&#35789;&#27719;&#21644;&#36816;&#21160;&#36807;&#31243;&#65292;&#20294;&#24573;&#30053;&#20102;&#35789;&#27719;&#21518;&#22788;&#29702;&#30340;&#35821;&#35328;&#22788;&#29702;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21477;&#23376;&#29702;&#35299;&#36807;&#31243;&#30340;&#27169;&#22411;&#36890;&#24120;&#21482;&#20851;&#27880;&#35789;&#27719;&#21518;&#22788;&#29702;&#30340;&#35821;&#35328;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36825;&#20004;&#31181;&#30740;&#31350;&#32447;&#32034;&#32467;&#21512;&#36215;&#26469;&#30340;&#27169;&#22411;&#65292;&#21363;&#25972;&#21512;&#30524;&#21160;&#25511;&#21046;&#21644;&#21477;&#23376;&#22788;&#29702;&#12290;&#24320;&#21457;&#36825;&#26679;&#19968;&#20010;&#25972;&#21512;&#27169;&#22411;&#20855;&#26377;&#26497;&#22823;&#30340;&#25361;&#25112;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#20294;&#36825;&#26679;&#30340;&#25972;&#21512;&#26159;&#26397;&#30528;&#23436;&#25972;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25968;&#23398;&#27169;&#22411;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#25105;&#20204;&#23558;&#30524;&#21160;&#25511;&#21046;&#27169;&#22411;SWIFT&#65288;Seelig&#31561;&#20154;&#65292;2020&#65289;&#19982;Lewis&#21644;Vasishth&#21477;&#23376;&#22788;&#29702;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65288;Lewis&#65286;Vasishth&#65292;2005&#65289;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#36825;&#31181;&#25972;&#21512;&#39318;&#27425;&#21464;&#24471;&#21487;&#33021;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#22240;&#20026;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of eye-movement control during reading, developed largely within psychology, usually focus on visual, attentional, lexical, and motor processes but neglect post-lexical language processing; by contrast, models of sentence comprehension processes, developed largely within psycholinguistics, generally focus only on post-lexical language processes. We present a model that combines these two research threads, by integrating eye-movement control and sentence processing. Developing such an integrated model is extremely challenging and computationally demanding, but such an integration is an important step toward complete mathematical models of natural language comprehension in reading. We combine the SWIFT model of eye-movement control (Seelig et al., 2020, doi:10.1016/j.jmp.2019.102313) with key components of the Lewis and Vasishth sentence processing model (Lewis &amp; Vasishth, 2005, doi:10.1207/s15516709cog0000_25). This integration becomes possible, for the first time, due in part to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#33258;&#21160;&#21270;&#25991;&#26412;&#25688;&#35201;&#29983;&#25104;&#31185;&#23398;&#25991;&#29486;&#30340;&#25554;&#22270;&#26631;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#21270;&#25688;&#35201;&#27169;&#22411; PEGASUS &#23545;&#24341;&#29992;&#22270;&#34920;&#30340;&#27573;&#33853;&#36827;&#34892;&#25688;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#35270;&#35273;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#20302;&#36136;&#37327;&#20316;&#32773;&#25776;&#20889;&#30340;&#26631;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#23545;&#22909;&#26631;&#39064;&#32570;&#20047;&#26126;&#30830;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2302.12324</link><description>&lt;p&gt;
&#25688;&#35201;&#21363;&#26631;&#39064;&#65306;&#21033;&#29992;&#33258;&#21160;&#21270;&#25991;&#26412;&#25688;&#35201;&#29983;&#25104;&#31185;&#23398;&#25991;&#29486;&#30340;&#25554;&#22270;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Summaries as Captions: Generating Figure Captions for Scientific Documents with Automated Text Summarization. (arXiv:2302.12324v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#33258;&#21160;&#21270;&#25991;&#26412;&#25688;&#35201;&#29983;&#25104;&#31185;&#23398;&#25991;&#29486;&#30340;&#25554;&#22270;&#26631;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#21270;&#25688;&#35201;&#27169;&#22411; PEGASUS &#23545;&#24341;&#29992;&#22270;&#34920;&#30340;&#27573;&#33853;&#36827;&#34892;&#25688;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#35270;&#35273;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#20302;&#36136;&#37327;&#20316;&#32773;&#25776;&#20889;&#30340;&#26631;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#23545;&#22909;&#26631;&#39064;&#32570;&#20047;&#26126;&#30830;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#22909;&#30340;&#25554;&#22270;&#26631;&#39064;&#21487;&#20197;&#24110;&#21161;&#35770;&#25991;&#35835;&#32773;&#29702;&#35299;&#22797;&#26434;&#30340;&#31185;&#23398;&#22270;&#34920;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#24050;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#20854;&#26631;&#39064;&#24120;&#24120;&#20889;&#24471;&#24456;&#24046;&#12290;&#33258;&#21160;&#29983;&#25104;&#26631;&#39064;&#21487;&#20197;&#24110;&#21161;&#35770;&#25991;&#20316;&#32773;&#25552;&#20379;&#33391;&#22909;&#30340;&#36215;&#22987;&#26631;&#39064;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#25913;&#36827;&#36136;&#37327;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#24120;&#23558;&#25554;&#22270;&#26631;&#39064;&#29983;&#25104;&#35270;&#20026;&#19968;&#39033;&#35270;&#35273;&#21040;&#35821;&#35328;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#23558;&#20854;&#20316;&#20026;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#26356;&#20026;&#26377;&#25928;&#12290;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#21270;&#25688;&#35201;&#27169;&#22411; PEGASUS &#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#19987;&#38376;&#29992;&#20110;&#23558;&#24341;&#29992;&#22270;&#34920;&#30340;&#27573;&#33853;&#65288;&#20363;&#22914;&#65292;&#8220;&#22270;3&#26174;&#31034;...&#8221;&#65289;&#25688;&#35201;&#20026;&#22270;&#34920;&#26631;&#39064;&#12290;&#22312;&#22823;&#35268;&#27169; arXiv &#22270;&#34920;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#35270;&#35273;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;(i) &#20302;&#36136;&#37327;&#20316;&#32773;&#25776;&#20889;&#30340;&#26631;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450; (ii) &#23545;&#22909;&#26631;&#39064;&#32570;&#20047;&#26126;&#30830;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Good figure captions help paper readers understand complex scientific figures. Unfortunately, even published papers often have poorly written captions. Automatic caption generation could aid paper writers by providing good starting captions that can be refined for better quality. Prior work often treated figure caption generation as a vision-to-language task. In this paper, we show that it can be more effectively tackled as a text summarization task in scientific documents. We fine-tuned PEGASUS, a pre-trained abstractive summarization model, to specifically summarize figure-referencing paragraphs (e.g., "Figure 3 shows...") into figure captions. Experiments on large-scale arXiv figures show that our method outperforms prior vision methods in both automatic and human evaluations. We further conducted an in-depth investigation focused on two key challenges: (i) the common presence of low-quality author-written captions and (ii) the lack of clear standards for good captions. Our code and
&lt;/p&gt;</description></item><item><title>LabelPrompt&#26159;&#19968;&#31181;&#38754;&#21521;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#39069;&#22806;&#30340;&#20196;&#29260;&#26469;&#34920;&#31034;&#20851;&#31995;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#27169;&#26495;&#26041;&#27861;&#26126;&#30830;&#26500;&#24314;&#23427;&#20204;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23558;&#22635;&#20805;&#25513;&#30721;&#26631;&#35760;&#30340;&#33258;&#28982;&#35821;&#35328;&#35789;&#27719;&#19982;&#35821;&#20041;&#20851;&#31995;&#26631;&#31614;&#30456;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#20307;&#24863;&#30693;&#27169;&#22359;&#26469;&#20943;&#36731;&#39044;&#27979;&#20851;&#31995;&#21644;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08068</link><description>&lt;p&gt;
LabelPrompt: &#20851;&#20110;&#20851;&#31995;&#20998;&#31867;&#30340;&#26377;&#25928;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LabelPrompt: Effective Prompt-based Learning for Relation Classification. (arXiv:2302.08068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08068
&lt;/p&gt;
&lt;p&gt;
LabelPrompt&#26159;&#19968;&#31181;&#38754;&#21521;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#39069;&#22806;&#30340;&#20196;&#29260;&#26469;&#34920;&#31034;&#20851;&#31995;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#27169;&#26495;&#26041;&#27861;&#26126;&#30830;&#26500;&#24314;&#23427;&#20204;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23558;&#22635;&#20805;&#25513;&#30721;&#26631;&#35760;&#30340;&#33258;&#28982;&#35821;&#35328;&#35789;&#27719;&#19982;&#35821;&#20041;&#20851;&#31995;&#26631;&#31614;&#30456;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#20307;&#24863;&#30693;&#27169;&#22359;&#26469;&#20943;&#36731;&#39044;&#27979;&#20851;&#31995;&#21644;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#36716;&#25442;&#20026;&#22635;&#31354;&#24335;&#26684;&#24335;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#40784;&#30340;&#26041;&#24335;&#65292;&#25552;&#31034;&#24335;&#23398;&#20064;&#22312;&#35768;&#22810;NLP&#20219;&#21153;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23558;&#22635;&#20805;&#25513;&#30721;&#26631;&#35760;&#30340;&#33258;&#28982;&#35821;&#35328;&#35789;&#27719;&#19982;&#35821;&#20041;&#20851;&#31995;&#26631;&#31614;&#65288;&#22914;"org:founded_by"&#65289;&#30456;&#20851;&#32852;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;LabelPrompt&#65292;&#29992;&#20110;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#12290;&#21463;&#21040;&#8220;&#32473;&#20104;&#27169;&#22411;&#36873;&#25321;&#8221;&#30340;&#30452;&#35273;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#39069;&#22806;&#30340;&#20196;&#29260;&#26469;&#34920;&#31034;&#20851;&#31995;&#26631;&#31614;&#65292;&#23558;&#36825;&#20123;&#20196;&#29260;&#35270;&#20026;&#20855;&#26377;&#35821;&#20041;&#21021;&#22987;&#21270;&#30340;&#21475;&#36848;&#32773;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#27169;&#26495;&#26041;&#27861;&#26126;&#30830;&#26500;&#24314;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20943;&#36731;&#39044;&#27979;&#20851;&#31995;&#21644;&#32473;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#20307;&#24863;&#30693;&#27169;&#22359;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#24615;&#30340;&#26041;&#24335;&#36827;&#34892;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-based learning has gained popularity across many natural language processing (NLP) tasks by reformulating them into a cloze-style format to better align pre-trained language models (PLMs) with downstream tasks. However, applying this approach to relation classification poses unique challenges. Specifically, associating natural language words that fill the masked token with semantic relation labels (\textit{e.g.} \textit{``org:founded\_by}'') is difficult. To address this challenge, this paper presents a novel prompt-based learning method, namely LabelPrompt, for the relation classification task. Motivated by the intuition to ``GIVE MODEL CHOICES!'', we first define additional tokens to represent relation labels, which regard these tokens as the verbaliser with semantic initialisation and explicitly construct them with a prompt template method. Then, to mitigate inconsistency between predicted relations and given entities, we implement an entity-aware module with contra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24773;&#24863;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#30340;&#22810;&#23610;&#24230;&#24773;&#24863;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#34701;&#21512;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.08233</link><description>&lt;p&gt;
&#26102;&#38388;&#24314;&#27169;&#37325;&#35201;&#24615;&#65306;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#26102;&#38388;&#24773;&#24863;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition. (arXiv:2211.08233v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24773;&#24863;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#30340;&#22810;&#23610;&#24230;&#24773;&#24863;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#34701;&#21512;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#22312;&#36890;&#36807;&#35821;&#38899;&#20449;&#21495;&#25512;&#26029;&#20154;&#31867;&#24773;&#24863;&#21644;&#24773;&#24863;&#29366;&#24577;&#65292;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#20174;&#25163;&#24037;&#29305;&#24449;&#20013;&#25366;&#25496;&#26102;&#31354;&#20449;&#24687;&#65292;&#25105;&#20204;&#25506;&#32034;&#22914;&#20309;&#20174;&#21160;&#24577;&#26102;&#38388;&#23610;&#24230;&#24314;&#27169;&#35821;&#38899;&#24773;&#24863;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#26102;&#38388;&#24863;&#30693;&#21452;&#21521;&#22810;&#23610;&#24230;&#32593;&#32476;&#65288;TIM-Net&#65289;&#65292;&#35813;&#32593;&#32476;&#20174;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#23398;&#20064;&#22810;&#23610;&#24230;&#24773;&#24863;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TIM-Net&#39318;&#20808;&#20351;&#29992;&#26102;&#38388;&#24863;&#30693;&#22359;&#23398;&#20064;&#26102;&#38388;&#24773;&#24863;&#34920;&#31034;&#65292;&#28982;&#21518;&#25972;&#21512;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#34917;&#20805;&#20449;&#24687;&#20197;&#20016;&#23500;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#26368;&#32456;&#34701;&#21512;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#29305;&#24449;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#24773;&#24863;&#21464;&#21270;&#12290;&#22312;&#20845;&#20010;&#22522;&#20934;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition (SER) plays a vital role in improving the interactions between humans and machines by inferring human emotion and affective states from speech signals. Whereas recent works primarily focus on mining spatiotemporal information from hand-crafted features, we explore how to model the temporal patterns of speech emotions from dynamic temporal scales. Towards that goal, we introduce a novel temporal emotional modeling approach for SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net), which learns multi-scale contextual affective representations from various time scales. Specifically, TIM-Net first employs temporal-aware blocks to learn temporal affective representation, then integrates complementary information from the past and the future to enrich contextual representations, and finally, fuses multiple time scale features for better adaptation to the emotional variation. Extensive experimental results on six benchmark SER datasets demonstrate th
&lt;/p&gt;</description></item><item><title>NECE&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25925;&#20107;&#20107;&#20214;&#38142;&#25552;&#21462;&#24037;&#20855;&#21253;&#65292;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#21644;&#23545;&#40784;&#25925;&#20107;&#20107;&#20214;&#65292;&#24182;&#21487;&#29992;&#20110;&#20998;&#26512;&#25925;&#20107;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2208.08063</link><description>&lt;p&gt;
NECE: &#25925;&#20107;&#20107;&#20214;&#38142;&#25552;&#21462;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
NECE: Narrative Event Chain Extraction Toolkit. (arXiv:2208.08063v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08063
&lt;/p&gt;
&lt;p&gt;
NECE&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25925;&#20107;&#20107;&#20214;&#38142;&#25552;&#21462;&#24037;&#20855;&#21253;&#65292;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#21644;&#23545;&#40784;&#25925;&#20107;&#20107;&#20214;&#65292;&#24182;&#21487;&#29992;&#20110;&#20998;&#26512;&#25925;&#20107;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29702;&#35299;&#19968;&#20010;&#25925;&#20107;&#65292;&#29702;&#35299;&#20107;&#20214;&#30340;&#26102;&#38388;&#27969;&#21160;&#23588;&#20026;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#19982;&#20027;&#35201;&#35282;&#33394;&#30456;&#20851;&#30340;&#20107;&#20214;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#20887;&#38271;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#25925;&#20107;&#25991;&#26412;&#65292;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NECE&#65292;&#19968;&#20010;&#24320;&#25918;&#33719;&#21462;&#30340;&#12289;&#22522;&#20110;&#25991;&#26723;&#32423;&#21035;&#30340;&#24037;&#20855;&#21253;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25552;&#21462;&#21644;&#23545;&#40784;&#25925;&#20107;&#20107;&#20214;&#65292;&#25353;&#29031;&#23427;&#20204;&#21457;&#29983;&#30340;&#26102;&#38388;&#39034;&#24207;&#25490;&#21015;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NECE&#24037;&#20855;&#21253;&#30340;&#39640;&#36136;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20998;&#26512;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#25925;&#20107;&#20559;&#35265;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#20844;&#24320;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#20197;&#21450;&#26410;&#26469;&#24037;&#20316;&#20013;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;NECE&#24037;&#20855;&#21253;&#21253;&#25324;&#19968;&#20010;Python&#24211;&#21644;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Web&#30028;&#38754;&#65292;&#20026;&#19987;&#19994;&#20154;&#21592;&#21644;&#26222;&#36890;&#22823;&#20247;&#25552;&#20379;&#24179;&#31561;&#30340;&#35775;&#38382;&#26435;&#65292;&#21487;&#20197;&#21487;&#35270;&#21270;&#20107;&#20214;&#38142;&#65292;&#33719;&#21462;&#25925;&#20107;&#27969;&#31243;&#65292;&#25110;&#32773;&#30740;&#31350;&#25925;&#20107;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
To understand a narrative, it is essential to comprehend the temporal event flows, especially those associated with main characters; however, this can be challenging with lengthy and unstructured narrative texts. To address this, we introduce NECE, an open-access, document-level toolkit that automatically extracts and aligns narrative events in the temporal order of their occurrence. Through extensive evaluations, we show the high quality of the NECE toolkit and demonstrates its downstream application in analyzing narrative bias regarding gender. We also openly discuss the shortcomings of the current approach, and potential of leveraging generative models in future works. Lastly the NECE toolkit includes both a Python library and a user-friendly web interface, which offer equal access to professionals and layman audience alike, to visualize event chain, obtain narrative flows, or study narrative bias.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;Transformers&#27169;&#22411;&#21487;&#20197;&#20174;&#22836;&#35757;&#32451;&#65292;&#22312;&#25512;&#26029;&#26102;&#23454;&#29616;&#32447;&#24615;&#20989;&#25968;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.01066</link><description>&lt;p&gt;
Transformers&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#20160;&#20040;&#65311;&#19968;&#20010;&#31616;&#21333;&#20989;&#25968;&#31867;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
What Can Transformers Learn In-Context? A Case Study of Simple Function Classes. (arXiv:2208.01066v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;Transformers&#27169;&#22411;&#21487;&#20197;&#20174;&#22836;&#35757;&#32451;&#65292;&#22312;&#25512;&#26029;&#26102;&#23454;&#29616;&#32447;&#24615;&#20989;&#25968;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#25351;&#27169;&#22411;&#33021;&#22815;&#20381;&#36182;&#20110;&#21253;&#21547;&#19978;&#19979;&#25991;&#31034;&#20363;&#65288;&#19982;&#26576;&#20010;&#20219;&#21153;&#23545;&#24212;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#65289;&#21644;&#26032;&#30340;&#26597;&#35810;&#36755;&#20837;&#30340;&#25552;&#31034;&#24207;&#21015;&#65292;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26159;&#65292;&#22312;&#25512;&#26029;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20165;&#21457;&#29983;&#22312;&#27169;&#22411;&#21442;&#25968;&#26410;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#12290;&#34429;&#28982;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#25104;&#21151;&#30340;&#20219;&#21153;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20160;&#20040;&#20869;&#23481;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#20989;&#25968;&#31867;&#65288;&#20363;&#22914;&#32447;&#24615;&#20989;&#25968;&#65289;&#65306;&#20063;&#23601;&#26159;&#35828;&#65292;&#32473;&#23450;&#20174;&#35813;&#31867;&#20013;&#23548;&#20986;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#33021;&#21542;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#8220;&#22823;&#22810;&#25968;&#8221;&#20989;&#25968;&#65311;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#20934;&#30340;Transformers&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn "most" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;XLMRScore&#21644;&#19968;&#20123;&#25913;&#36827;&#25514;&#26045;&#26469;&#35299;&#20915;&#26410;&#32763;&#35793;&#26631;&#35760;&#21644;&#35821;&#35328;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.00463</link><description>&lt;p&gt;
&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#19981;&#21305;&#37197;&#24863;&#30693;&#26080;&#30417;&#30563;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Mismatching-Aware Unsupervised Translation Quality Estimation For Low-Resource Languages. (arXiv:2208.00463v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;XLMRScore&#21644;&#19968;&#20123;&#25913;&#36827;&#25514;&#26045;&#26469;&#35299;&#20915;&#26410;&#32763;&#35793;&#26631;&#35760;&#21644;&#35821;&#35328;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#65288;QE&#65289;&#26159;&#22312;&#27809;&#26377;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#36755;&#20986;&#36136;&#37327;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#22312;&#26426;&#22120;&#32763;&#35793;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;XLMRScore&#65292;&#23427;&#26159;&#36890;&#36807;XLM-RoBERTa&#65288;XLMR&#65289;&#27169;&#22411;&#35745;&#31639;&#30340;BERTScore&#30340;&#36328;&#35821;&#35328;&#23545;&#24212;&#29289;&#12290;&#36825;&#20010;&#24230;&#37327;&#21487;&#20197;&#29992;&#20316;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;QE&#26041;&#27861;&#65292;&#20294;&#20351;&#29992;&#23427;&#20250;&#23548;&#33268;&#20004;&#20010;&#38382;&#39064;&#65306;&#19968;&#26159;&#23548;&#33268;&#24847;&#22806;&#39640;&#32763;&#35793;&#20998;&#25968;&#30340;&#26410;&#32763;&#35793;&#26631;&#35760;&#65292;&#20108;&#26159;&#22312;XLMRScore&#20013;&#24212;&#29992;&#36138;&#23146;&#21305;&#37197;&#26102;&#28304;&#35821;&#35328;&#21644;&#20551;&#35774;&#35821;&#35328;&#20043;&#38388;&#19981;&#21305;&#37197;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26410;&#32763;&#35793;&#30340;&#35789;&#26367;&#25442;&#20026;&#26410;&#30693;&#26631;&#35760;&#65292;&#24182;&#36328;&#35821;&#35328;&#23545;&#40784;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#26356;&#25509;&#36817;&#23545;&#40784;&#30340;&#35789;&#12290;&#25105;&#20204;&#22312;WMT21 QE&#20849;&#20139;&#20219;&#21153;&#30340;&#22235;&#20010;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation Quality Estimation (QE) is the task of predicting the quality of machine translation (MT) output without any reference. This task has gained increasing attention as an important component in the practical applications of MT. In this paper, we first propose XLMRScore, which is a cross-lingual counterpart of BERTScore computed via the XLM-RoBERTa (XLMR) model. This metric can be used as a simple unsupervised QE method, while employing it results in two issues: firstly, the untranslated tokens leading to unexpectedly high translation scores, and secondly, the issue of mismatching errors between source and hypothesis tokens when applying the greedy matching in XLMRScore. To mitigate these issues, we suggest replacing untranslated words with the unknown token and the cross-lingual alignment of the pre-trained model to represent aligned words closer to each other, respectively. We evaluate the proposed method on four low-resource language pairs of WMT21 QE shared task, as well as
&lt;/p&gt;</description></item><item><title>PInKS&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#30417;&#30563;&#23454;&#29616;&#21069;&#25552;&#25512;&#29702;&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#39640;&#24120;&#35782;&#30693;&#35782;&#21069;&#25552;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.07920</link><description>&lt;p&gt;
PInKS: &#24102;&#26377;&#26368;&#23567;&#30417;&#30563;&#30340;&#39044;&#22788;&#29702;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
PInKS: Preconditioned Commonsense Inference with Minimal Supervision. (arXiv:2206.07920v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07920
&lt;/p&gt;
&lt;p&gt;
PInKS&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#30417;&#30563;&#23454;&#29616;&#21069;&#25552;&#25512;&#29702;&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#39640;&#24120;&#35782;&#30693;&#35782;&#21069;&#25552;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#65292;&#20197;&#8220;&#29627;&#29827;&#21487;&#20197;&#29992;&#26469;&#35013;&#27700;&#65292;&#38500;&#38750;&#29627;&#29827;&#30772;&#30862;&#8221;&#31561;&#21069;&#25552;&#36827;&#34892;&#25512;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#21069;&#25552;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#20197;&#21450;&#27169;&#22411;&#23545;&#27492;&#31867;&#25512;&#29702;&#30340;&#25903;&#25345;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PInKS&#65288;&#24102;&#26377;&#26368;&#23567;&#30417;&#30563;&#30340;&#39044;&#22788;&#29702;&#24120;&#35782;&#25512;&#29702;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#26368;&#23567;&#30417;&#30563;&#26469;&#36827;&#34892;&#21069;&#25552;&#25512;&#29702;&#30340;&#25913;&#36827;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;PInKS&#22312;&#19987;&#27880;&#20110;&#24120;&#35782;&#30693;&#35782;&#21069;&#25552;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#25913;&#21892;&#20102;&#32467;&#26524;&#65288;&#26368;&#22810;&#25552;&#39640;&#20102;40%&#30340;Macro-F1&#20998;&#25968;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;PAC-Bayesian&#20449;&#24687;&#20998;&#26512;&#12289;&#31934;&#30830;&#24230;&#25351;&#26631;&#21644;&#28040;&#34701;&#30740;&#31350;&#26469;&#30740;&#31350;PInKS&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning with preconditions such as "glass can be used for drinking water unless the glass is shattered" remains an open problem for language models. The main challenge lies in the scarcity of preconditions data and the model's lack of support for such reasoning. We present PInKS, Preconditioned Commonsense Inference with WeaK Supervision, an improved model for reasoning with preconditions through minimum supervision. We show, both empirically and theoretically, that PInKS improves the results on benchmarks focused on reasoning with the preconditions of commonsense knowledge (up to 40% Macro-F1 scores). We further investigate PInKS through PAC-Bayesian informativeness analysis, precision measures, and ablation study.
&lt;/p&gt;</description></item><item><title>MathBERT&#26159;&#19968;&#20010;&#22522;&#20110;BASE BERT&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#25968;&#23398;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20026;&#25968;&#23398;&#25945;&#32946;&#20013;&#30340;&#36890;&#29992;NLP&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2106.07340</link><description>&lt;p&gt;
MathBERT: &#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25945;&#32946;&#20013;&#30340;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07340
&lt;/p&gt;
&lt;p&gt;
MathBERT&#26159;&#19968;&#20010;&#22522;&#20110;BASE BERT&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#25968;&#23398;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20026;&#25968;&#23398;&#25945;&#32946;&#20013;&#30340;&#36890;&#29992;NLP&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#21407;&#22987;&#30340;BERT&#65288;&#21363;BASE BERT&#65289;&#30340;&#24341;&#20837;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#24320;&#21457;&#20102;&#21508;&#31181;&#23450;&#21046;&#30340;BERT&#27169;&#22411;&#26469;&#25913;&#36827;&#29305;&#23450;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#25968;&#23398;&#25991;&#26412;&#30340;&#24615;&#36136;&#65292;&#32463;&#24120;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#35789;&#27719;&#20197;&#21450;&#26041;&#31243;&#21644;&#25968;&#23398;&#31526;&#21495;&#65292;&#25105;&#20204;&#35748;&#20026;&#24320;&#21457;&#19968;&#20010;&#38024;&#23545;&#25968;&#23398;&#30340;&#26032;BERT&#27169;&#22411;&#23558;&#23545;&#35768;&#22810;&#25968;&#23398;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#12290;&#22312;&#36825;&#31687;&#36164;&#28304;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#22810;&#26426;&#26500;&#21162;&#21147;&#65288;&#21363;&#20004;&#20010;&#23398;&#20064;&#24179;&#21488;&#21644;&#19977;&#20010;&#32654;&#22269;&#23398;&#26415;&#26426;&#26500;&#65289;&#20197;&#28385;&#36275;&#36825;&#20010;&#38656;&#27714;&#65306;MathBERT&#65292;&#19968;&#20010;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#25968;&#23398;&#35821;&#26009;&#24211;&#19978;&#23545;BASE BERT&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#32780;&#21019;&#24314;&#30340;&#27169;&#22411;&#65292;&#35813;&#35821;&#26009;&#24211;&#28085;&#30422;&#20102;&#20174;&#23398;&#21069;&#25945;&#32946;&#65288;pre-k&#65289;&#21040;&#39640;&#20013;&#20197;&#21450;&#30740;&#31350;&#29983;&#27700;&#24179;&#30340;&#25968;&#23398;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#24120;&#29992;&#20110;&#25968;&#23398;&#25945;&#32946;&#30340;&#36890;&#29992;NLP&#20219;&#21153;&#65306;&#30693;&#35782;&#32452;&#20214;&#39044;&#27979;&#65292;&#33258;&#21160;&#35780;&#20998;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#30693;&#35782;&#36861;&#28335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the introduction of the original BERT (i.e., BASE BERT), researchers have developed various customized BERT models with improved performance for specific domains and tasks by exploiting the benefits of transfer learning. Due to the nature of mathematical texts, which often use domain specific vocabulary along with equations and math symbols, we posit that the development of a new BERT model for mathematics would be useful for many mathematical downstream tasks. In this resource paper, we introduce our multi-institutional effort (i.e., two learning platforms and three academic institutions in the US) toward this need: MathBERT, a model created by pre-training the BASE BERT model on a large mathematical corpus ranging from pre-kindergarten (pre-k), to high-school, to college graduate level mathematical content. In addition, we select three general NLP tasks that are often used in mathematics education: prediction of knowledge component, auto-grading open-ended Q&amp;A, and knowledge tr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PaCo&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25512;&#29702;&#24120;&#35782;&#30693;&#35782;&#30340;&#29615;&#22659;&#21069;&#25552;&#26465;&#20214;&#12290;&#36890;&#36807;&#19977;&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#24773;&#22659;&#21069;&#25552;&#26465;&#20214;&#26041;&#38754;&#19982;&#20154;&#31867;&#34920;&#29616;&#23384;&#22312;10-30%&#30340;&#24046;&#36317;&#65292;&#36825;&#34920;&#26126;&#25512;&#29702;&#21069;&#25552;&#26465;&#20214;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2104.08712</link><description>&lt;p&gt;
PaCo: &#21069;&#25552;&#26465;&#20214;&#24402;&#22240;&#20110;&#24120;&#35782;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
PaCo: Preconditions Attributed to Commonsense Knowledge. (arXiv:2104.08712v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.08712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PaCo&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25512;&#29702;&#24120;&#35782;&#30693;&#35782;&#30340;&#29615;&#22659;&#21069;&#25552;&#26465;&#20214;&#12290;&#36890;&#36807;&#19977;&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#24773;&#22659;&#21069;&#25552;&#26465;&#20214;&#26041;&#38754;&#19982;&#20154;&#31867;&#34920;&#29616;&#23384;&#22312;10-30%&#30340;&#24046;&#36317;&#65292;&#36825;&#34920;&#26126;&#25512;&#29702;&#21069;&#25552;&#26465;&#20214;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#26080;&#32541;&#22320;&#25512;&#29702;&#24120;&#35782;&#30693;&#35782;&#30340;&#29615;&#22659;&#21069;&#25552;&#26465;&#20214;&#12290;&#25105;&#20204;&#30693;&#36947;&#19968;&#20010;&#29627;&#29827;&#26159;&#29992;&#26469;&#21917;&#27700;&#30340;&#65292;&#38500;&#38750;&#29627;&#29827;&#30772;&#30862;&#25110;&#27700;&#26377;&#27602;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#26029;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#26377;&#30528;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#29702;&#35299;&#29615;&#22659;&#21069;&#25552;&#26465;&#20214;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#29615;&#22659;&#21069;&#25552;&#30340;&#25512;&#29702;&#25361;&#25112;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21517;&#20026;PaCo&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;12.4&#21315;&#20010;&#24120;&#35782;&#38472;&#36848;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#22522;&#20110;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#20010;&#32463;&#20856;&#35780;&#20272;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#26816;&#39564;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#24773;&#22659;&#21069;&#25552;&#26465;&#20214;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#26426;&#22120;&#21644;&#20154;&#31867;&#30340;&#34920;&#29616;&#23384;&#22312;10-30%&#30340;&#24046;&#36317;&#65292;&#36825;&#34920;&#26126;&#25512;&#29702;&#21069;&#25552;&#26465;&#20214;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can seamlessly reason with circumstantial preconditions of commonsense knowledge. We understand that a glass is used for drinking water, unless the glass is broken or the water is toxic. Despite state-of-the-art (SOTA) language models' (LMs) impressive performance on inferring commonsense knowledge, it is unclear whether they understand the circumstantial preconditions. To address this gap, we propose a novel challenge of reasoning with circumstantial preconditions. We collect a dataset, called PaCo, consisting of 12.4 thousand preconditions of commonsense statements expressed in natural language. Based on this dataset, we create three canonical evaluation tasks and use them to examine the capability of existing LMs to understand situational preconditions. Our results reveal a 10-30% gap between machine and human performance on our tasks, which shows that reasoning with preconditions is an open challenge.
&lt;/p&gt;</description></item></channel></rss>