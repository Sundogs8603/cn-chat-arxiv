<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2307.06290</link><description>&lt;p&gt;
&#25351;&#20196;&#25366;&#25496;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#32463;&#21382;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#36171;&#20104;&#27169;&#22411;&#24378;&#22823;&#30340;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#22238;&#24212;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#26102;&#20173;&#28982;&#26080;&#27861;&#29702;&#35299;&#20154;&#31867;&#25351;&#20196;&#12290;&#20026;&#20102;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#25351;&#20196;&#24494;&#35843;&#24050;&#25104;&#20026;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#20173;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#36136;&#37327;&#30340;&#32447;&#24615;&#35268;&#21017;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#26469;&#36827;&#34892;InstructMining&#30340;&#24314;&#27169;&#12290;&#20026;&#20102;&#30740;&#31350;&#25968;&#25454;&#36136;&#37327;&#19982;&#36825;&#20123;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32454;&#33268;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
&lt;/p&gt;</description></item><item><title>MMBench&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#24320;&#21457;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.06281</link><description>&lt;p&gt;
MMBench: &#24744;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20840;&#33021;&#29699;&#21592;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06281
&lt;/p&gt;
&lt;p&gt;
MMBench&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#24320;&#21457;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#20449;&#24687;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#35780;&#20272;&#36825;&#20123;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#26410;&#26469;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20256;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;VQAv2&#25110;COCO Caption&#25552;&#20379;&#20102;&#23450;&#37327;&#30340;&#24615;&#33021;&#27979;&#37327;&#65292;&#20294;&#22312;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#21644;&#38750;&#40065;&#26834;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26368;&#36817;&#30340;&#20027;&#35266;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;OwlEval&#65292;&#36890;&#36807;&#25972;&#21512;&#20154;&#21147;&#36164;&#28304;&#65292;&#23545;&#27169;&#22411;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20294;&#19981;&#21487;&#25193;&#23637;&#24182;&#19988;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MMBench&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#12290;MMBench&#31995;&#32479;&#22320;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#20027;&#35201;&#30001;&#20004;&#20010;&#20803;&#32032;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#20803;&#32032;&#26159;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#35780;&#20272;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#31867;&#20284;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluatio
&lt;/p&gt;</description></item><item><title>Ashaar&#26159;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#20998;&#26512;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#35799;&#27468;&#30340;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#38901;&#24459;&#12289;&#20027;&#39064;&#21644;&#26102;&#20195;&#20998;&#31867;&#31561;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#20855;&#22791;&#33258;&#21160;&#35799;&#27468;&#38899;&#26631;&#21270;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06218</link><description>&lt;p&gt;
Ashaar: &#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#20998;&#26512;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#35799;&#27468;
&lt;/p&gt;
&lt;p&gt;
Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning Approaches. (arXiv:2307.06218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06218
&lt;/p&gt;
&lt;p&gt;
Ashaar&#26159;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#20998;&#26512;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#35799;&#27468;&#30340;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#38901;&#24459;&#12289;&#20027;&#39064;&#21644;&#26102;&#20195;&#20998;&#31867;&#31561;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#20855;&#22791;&#33258;&#21160;&#35799;&#27468;&#38899;&#26631;&#21270;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35799;&#27468;&#22312;&#20219;&#20309;&#22269;&#23478;&#30340;&#25991;&#21270;&#21644;&#20256;&#32479;&#20013;&#37117;&#20855;&#26377;&#26497;&#22823;&#30340;&#37325;&#35201;&#24615;&#12290;&#23427;&#26159;&#35799;&#20154;&#34920;&#36798;&#24773;&#24863;&#12289;&#20445;&#30041;&#20064;&#20439;&#21644;&#20256;&#36798;&#25991;&#21270;&#31934;&#39635;&#30340;&#24037;&#20855;&#12290;&#38463;&#25289;&#20271;&#35799;&#27468;&#20063;&#19981;&#20363;&#22806;&#65292;&#22312;&#38463;&#25289;&#20271;&#31038;&#21306;&#30340;&#20256;&#32479;&#20013;&#25198;&#28436;&#30528;&#29645;&#36149;&#30340;&#35282;&#33394;&#65292;&#24182;&#22312;&#24403;&#20170;&#26102;&#20195;&#20445;&#25345;&#20854;&#37325;&#35201;&#24615;&#12290;&#36890;&#24120;&#65292;&#29702;&#35299;&#38463;&#25289;&#20271;&#35799;&#27468;&#38656;&#35201;&#35821;&#35328;&#23398;&#23478;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20182;&#20204;&#21487;&#20197;&#20998;&#26512;&#20854;&#20869;&#23481;&#24182;&#35780;&#20272;&#20854;&#36136;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Ashaar&#8221;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#31995;&#21015;&#29992;&#20110;&#38463;&#25289;&#20271;&#35799;&#27468;&#20998;&#26512;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#27969;&#31243;&#65292;&#21253;&#25324;&#35799;&#27468;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#22914;&#38901;&#24459;&#12289;&#20027;&#39064;&#21644;&#26102;&#20195;&#20998;&#31867;&#12290;&#23427;&#36824;&#21253;&#25324;&#33258;&#21160;&#35799;&#27468;&#38899;&#26631;&#21270;&#65292;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#26356;&#22797;&#26434;&#30340;&#20998;&#26512;&#65292;&#22914;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#35799;&#27468;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poetry holds immense significance within the cultural and traditional fabric of any nation. It serves as a vehicle for poets to articulate their emotions, preserve customs, and convey the essence of their culture. Arabic poetry is no exception, having played a cherished role in the heritage of the Arabic community throughout history and maintaining its relevance in the present era. Typically, comprehending Arabic poetry necessitates the expertise of a linguist who can analyze its content and assess its quality. This paper presents the introduction of a framework called \textit{Ashaar} https://github.com/ARBML/Ashaar, which encompasses a collection of datasets and pre-trained models designed specifically for the analysis and generation of Arabic poetry. The pipeline established within our proposed approach encompasses various aspects of poetry, such as meter, theme, and era classification. It also incorporates automatic poetry diacritization, enabling more intricate analyses like automa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#21040;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#31995;&#32479;&#21644;&#26377;&#25928;&#36890;&#20449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#21464;&#21270;&#30340;&#30417;&#27979;&#12289;&#20998;&#26512;&#12289;&#35745;&#21010;&#21644;&#25191;&#34892;&#31995;&#32479;&#33258;&#36866;&#24212;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2307.06187</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems. (arXiv:2307.06187v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#21040;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#31995;&#32479;&#21644;&#26377;&#25928;&#36890;&#20449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#21464;&#21270;&#30340;&#30417;&#27979;&#12289;&#20998;&#26512;&#12289;&#35745;&#21010;&#21644;&#25191;&#34892;&#31995;&#32479;&#33258;&#36866;&#24212;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#35745;&#31639;&#20013;&#65292;&#33258;&#36866;&#24212;&#34987;&#25552;&#20986;&#20316;&#20026;&#31649;&#29702;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MASs&#65289;&#22797;&#26434;&#24615;&#30340;&#22522;&#26412;&#33539;&#24335;&#12290;&#36890;&#36807;&#28155;&#21152;&#23545;&#31995;&#32479;&#30340;&#30417;&#27979;&#21644;&#33258;&#36866;&#24212;&#25903;&#25345;&#65292;&#20197;&#23454;&#29616;&#29305;&#23450;&#20851;&#27880;&#28857;&#30340;&#30446;&#26631;&#12290;&#22312;&#28041;&#21450;&#26234;&#33021;&#20307;&#20114;&#21160;&#30340;&#22330;&#26223;&#20013;&#65292;&#36890;&#20449;&#26159;&#20851;&#38190;&#65292;&#23427;&#36890;&#36807;&#30452;&#25509;&#12289;&#28165;&#26224;&#30340;&#20449;&#24687;&#20132;&#27969;&#22686;&#24378;&#21512;&#20316;&#24182;&#20943;&#23569;&#21327;&#35843;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#25552;&#39640;&#19982;MASs&#30340;&#20132;&#20114;&#36890;&#20449;&#30340;&#34920;&#36798;&#33021;&#21147;&#24182;&#38750;&#27809;&#26377;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#33258;&#36866;&#24212;&#31995;&#32479;&#23545;&#26377;&#25928;&#36890;&#20449;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#26410;&#26469;MAS&#30340;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22522;&#20110;GPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#21040;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#22522;&#20110;MAPE-K&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20197;&#20854;&#22312;&#21709;&#24212;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#20013;&#30417;&#27979;&#12289;&#20998;&#26512;&#12289;&#35745;&#21010;&#21644;&#25191;&#34892;&#31995;&#32479;&#33258;&#36866;&#24212;&#30340;&#24378;&#22823;&#25903;&#25345;&#32780;&#38395;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomic computing, self-adaptation has been proposed as a fundamental paradigm to manage the complexity of multiagent systems (MASs). This achieved by extending a system with support to monitor and adapt itself to achieve specific concerns of interest. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, improving the expressiveness of the interaction communication with MASs is not without challenges. In this sense, the interplay between self-adaptive systems and effective communication is crucial for future MAS advancements. In this paper, we propose the integration of large language models (LLMs) such as GPT-based technologies into multiagent systems. We anchor our methodology on the MAPE-K model, which is renowned for its robust support in monitoring, analyzing, planning, and executing system adaptations in response to dynami
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21160;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#33889;&#33796;&#29273;&#25163;&#35821;&#21160;&#30011;&#65292;&#37325;&#28857;&#26159;&#25913;&#36827;&#25163;&#35821;&#20043;&#38388;&#30340;&#36807;&#28193;&#65292;&#24182;&#36890;&#36807;&#21253;&#21547;&#22068;&#22411;&#34892;&#20026;&#25552;&#39640;&#20102;&#21021;&#23398;&#32773;&#30340;&#29702;&#35299;&#21644;&#25163;&#35821;&#33258;&#28982;&#24230;&#34920;&#29616;&#12290;&#36825;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#25163;&#35821;&#21160;&#30011;&#30340;&#21512;&#25104;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.06124</link><description>&lt;p&gt;
&#29992;&#21160;&#24577;&#23450;&#26102;&#21644;&#22068;&#22411;&#25552;&#21319;&#33889;&#33796;&#29273;&#25163;&#35821;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Enhancing Portuguese Sign Language Animation with Dynamic Timing and Mouthing. (arXiv:2307.06124v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21160;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#33889;&#33796;&#29273;&#25163;&#35821;&#21160;&#30011;&#65292;&#37325;&#28857;&#26159;&#25913;&#36827;&#25163;&#35821;&#20043;&#38388;&#30340;&#36807;&#28193;&#65292;&#24182;&#36890;&#36807;&#21253;&#21547;&#22068;&#22411;&#34892;&#20026;&#25552;&#39640;&#20102;&#21021;&#23398;&#32773;&#30340;&#29702;&#35299;&#21644;&#25163;&#35821;&#33258;&#28982;&#24230;&#34920;&#29616;&#12290;&#36825;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#25163;&#35821;&#21160;&#30011;&#30340;&#21512;&#25104;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25163;&#35821;&#21160;&#30011;&#32463;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#33258;&#28982;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#33021;&#20934;&#30830;&#22320;&#22797;&#21046;&#20154;&#31867;&#25163;&#35821;&#32773;&#30340;&#21516;&#27493;&#36523;&#20307;&#34892;&#20026;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#25163;&#35821;&#20043;&#38388;&#30340;&#36807;&#28193;&#65292;&#37325;&#28857;&#25918;&#22312;&#33889;&#33796;&#29273;&#25163;&#35821;&#30340;&#22068;&#22411;&#21160;&#30011;&#19978;&#12290;&#34429;&#28982;&#27597;&#35821;&#25163;&#35821;&#32773;&#26356;&#21916;&#27426;&#20855;&#26377;&#21160;&#24577;&#36807;&#28193;&#30340;&#21160;&#30011;&#65292;&#20294;&#25105;&#20204;&#24182;&#26410;&#21457;&#29616;&#22312;&#29702;&#35299;&#21644;&#24863;&#30693;&#33258;&#28982;&#24230;&#35780;&#20998;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#34920;&#26126;&#21253;&#21547;&#22068;&#22411;&#34892;&#20026;&#23545;&#20110;&#21021;&#23398;&#25163;&#35821;&#30340;&#23398;&#20064;&#32773;&#26469;&#35828;&#65292;&#25552;&#39640;&#20102;&#29702;&#35299;&#21644;&#24863;&#30693;&#33258;&#28982;&#24230;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#25163;&#35821;&#21160;&#30011;&#30340;&#21512;&#25104;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current signing avatars are often described as unnatural as they cannot accurately reproduce all the subtleties of synchronized body behaviors of a human signer. In this paper, we propose a new dynamic approach for transitions between signs, focusing on mouthing animations for Portuguese Sign Language. Although native signers preferred animations with dynamic transitions, we did not find significant differences in comprehension and perceived naturalness scores. On the other hand, we show that including mouthing behaviors improved comprehension and perceived naturalness for novice sign language learners. Results have implications in computational linguistics, human-computer interaction, and synthetic animation of signing avatars.
&lt;/p&gt;</description></item><item><title>VELMA&#26159;&#19968;&#20010;&#21475;&#22836;&#21270;&#30340;LLM&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20154;&#31867;&#20889;&#20316;&#30340;&#23548;&#33322;&#25351;&#20196;&#20013;&#30340;&#22320;&#26631;&#24182;&#32467;&#21512;CLIP&#26469;&#36827;&#34892;&#35270;&#35273;&#29615;&#22659;&#30340;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#22312;&#34903;&#26223;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2307.06082</link><description>&lt;p&gt;
VELMA: LLM&#26234;&#33021;&#20307;&#22312;&#34903;&#26223;&#20013;&#36827;&#34892;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#30340;&#21475;&#22836;&#21270;&#20307;&#29616;
&lt;/p&gt;
&lt;p&gt;
VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06082
&lt;/p&gt;
&lt;p&gt;
VELMA&#26159;&#19968;&#20010;&#21475;&#22836;&#21270;&#30340;LLM&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20154;&#31867;&#20889;&#20316;&#30340;&#23548;&#33322;&#25351;&#20196;&#20013;&#30340;&#22320;&#26631;&#24182;&#32467;&#21512;CLIP&#26469;&#36827;&#34892;&#35270;&#35273;&#29615;&#22659;&#30340;&#29702;&#35299;&#65292;&#20197;&#23454;&#29616;&#22312;&#34903;&#26223;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#22686;&#37327;&#20915;&#31574;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20197;&#20307;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20854;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20043;&#19968;&#26159;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;(VLN)&#65292;&#23427;&#38656;&#35201;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20197;&#21450;&#31354;&#38388;&#21644;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20010;&#20307;&#29616;&#26234;&#33021;&#20307;&#38656;&#35201;&#22312;&#34903;&#26223;&#31561;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#35266;&#23519;&#22522;&#30784;&#19978;&#20934;&#30830;&#29702;&#35299;&#23548;&#33322;&#25351;&#20196;&#12290;&#23613;&#31649;LLM&#22312;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#23427;&#20204;&#19982;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#36830;&#25509;&#36215;&#26469;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VELMA&#65292;&#19968;&#31181;&#20351;&#29992;&#36712;&#36857;&#21644;&#35270;&#35273;&#29615;&#22659;&#35266;&#23519;&#30340;&#21475;&#22836;&#21270;&#20316;&#20026;&#19979;&#19968;&#27493;&#25805;&#20316;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;LLM&#26234;&#33021;&#20307;&#12290;&#35270;&#35273;&#20449;&#24687;&#36890;&#36807;&#19968;&#20010;&#27969;&#31243;&#36827;&#34892;&#21475;&#22836;&#21270;&#65292;&#35813;&#27969;&#31243;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#23548;&#33322;&#25351;&#20196;&#20013;&#25552;&#21462;&#22320;&#26631;&#65292;&#24182;&#20351;&#29992;CLIP&#26469;&#30830;&#23450;&#23427;&#20204;&#22312;&#24403;&#21069;&#20840;&#26223;&#35270;&#22270;&#20013;&#30340;&#21487;&#35265;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30142;&#30149;&#36827;&#23637;&#32858;&#31867;&#20013;&#35299;&#35835;&#28145;&#24230;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;2&#22411;&#31958;&#23615;&#30149;&#21442;&#19982;&#32773;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#23545;&#30142;&#30149;&#36827;&#23637;&#27169;&#24335;&#30340;&#20020;&#24202;&#24847;&#20041;&#24615;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.06060</link><description>&lt;p&gt;
&#35299;&#35835;&#30142;&#30149;&#36827;&#23637;&#32858;&#31867;&#20013;&#30340;&#28145;&#24230;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Interpreting deep embeddings for disease progression clustering. (arXiv:2307.06060v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30142;&#30149;&#36827;&#23637;&#32858;&#31867;&#20013;&#35299;&#35835;&#28145;&#24230;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;2&#22411;&#31958;&#23615;&#30149;&#21442;&#19982;&#32773;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#23545;&#30142;&#30149;&#36827;&#23637;&#27169;&#24335;&#30340;&#20020;&#24202;&#24847;&#20041;&#24615;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24739;&#32773;&#32858;&#31867;&#30340;&#32972;&#26223;&#19979;&#35299;&#35835;&#28145;&#24230;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#33521;&#22269;&#29983;&#29289;&#24211;&#30340;2&#22411;&#31958;&#23615;&#30149;&#21442;&#19982;&#32773;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20986;&#23545;&#30142;&#30149;&#36827;&#23637;&#27169;&#24335;&#30340;&#20020;&#24202;&#24847;&#20041;&#24615;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach for interpreting deep embeddings in the context of patient clustering. We evaluate our approach on a dataset of participants with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful insights into disease progression patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992; Heaps &#20989;&#25968;&#21644;&#31867;&#22411;&#26631;&#35760;&#27604;&#26469;&#30830;&#23450;&#33945;&#21476;&#25991;&#36890;&#29992;&#35821;&#26009;&#24211;&#30340;&#21512;&#36866;&#35268;&#27169;&#65292;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#26631;&#35760;&#25968;&#37327;&#36229;&#36807;3900&#19975;&#33267;4200&#19975;&#26102;&#65292;&#31867;&#22411;&#26631;&#35760;&#27604;&#20540;&#20960;&#20046;&#19981;&#21464;&#12290;</title><link>http://arxiv.org/abs/2307.06050</link><description>&lt;p&gt;
&#23545;&#33945;&#21476;&#25991;&#36890;&#29992;&#35821;&#26009;&#24211;&#21512;&#36866;&#35268;&#27169;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Appropriate size of the Mongolian general corpus. (arXiv:2307.06050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992; Heaps &#20989;&#25968;&#21644;&#31867;&#22411;&#26631;&#35760;&#27604;&#26469;&#30830;&#23450;&#33945;&#21476;&#25991;&#36890;&#29992;&#35821;&#26009;&#24211;&#30340;&#21512;&#36866;&#35268;&#27169;&#65292;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#26631;&#35760;&#25968;&#37327;&#36229;&#36807;3900&#19975;&#33267;4200&#19975;&#26102;&#65292;&#31867;&#22411;&#26631;&#35760;&#27604;&#20540;&#20960;&#20046;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#33945;&#21476;&#25991;&#36890;&#29992;&#35821;&#26009;&#24211;&#30340;&#21512;&#36866;&#35268;&#27169;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102; Heaps &#20989;&#25968;&#21644;&#31867;&#22411;&#26631;&#35760;&#27604;&#26469;&#30830;&#23450;&#33945;&#21476;&#25991;&#36890;&#29992;&#35821;&#26009;&#24211;&#30340;&#21512;&#36866;&#35268;&#27169;&#12290;&#26679;&#26412;&#35821;&#26009;&#24211;&#21253;&#21547;&#26469;&#33258;10&#20010;&#39046;&#22495;&#30340;&#25991;&#26412;&#65292;&#21253;&#25324;&#25253;&#32440;&#25919;&#27835;&#12289;&#32463;&#27982;&#12289;&#31038;&#20250;&#12289;&#25991;&#21270;&#12289;&#20307;&#32946;&#12289;&#22269;&#38469;&#25991;&#31456;&#21644;&#27861;&#24459;&#12289;&#20013;&#39640;&#23398;&#26657;&#25991;&#23398;&#25945;&#26448;&#12289;&#35775;&#35848;&#25991;&#31456;&#21644;&#25773;&#23458;&#36716;&#24405;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#26679;&#26412;&#35821;&#26009;&#24211;&#20272;&#35745;&#20102; Heaps &#20989;&#25968;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35266;&#23519;&#20102;&#36890;&#36807;&#20351;&#29992;&#20272;&#35745;&#30340; Heaps &#20989;&#25968;&#22686;&#21152;&#19968;&#30334;&#19975;&#20010;&#26631;&#35760;&#30340;&#26041;&#24335;&#65292;&#31867;&#22411;&#25968;&#37327;&#21644;&#31867;&#22411;&#26631;&#35760;&#27604;&#20540;&#30340;&#21464;&#21270;&#12290;&#35266;&#23519;&#32467;&#26524;&#21457;&#29616;&#65292;&#24403;&#26631;&#35760;&#25968;&#37327;&#36229;&#36807;3900&#19975;&#33267;4200&#19975;&#26102;&#65292;&#31867;&#22411;&#26631;&#35760;&#27604;&#20540;&#20960;&#20046;&#26410;&#21457;&#29983;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#33945;&#21476;&#25991;&#36890;&#29992;&#35821;&#26009;&#24211;&#30340;&#21512;&#36866;&#35268;&#27169;&#20026;3900&#19975;&#33267;4200&#19975;&#20010;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to determine the appropriate size of the Mongolian general corpus. This study used the Heaps function and Type Token Ratio to determine the appropriate size of the Mongolian general corpus. The sample corpus of 906,064 tokens comprised texts from 10 domains of newspaper politics, economy, society, culture, sports, world articles and laws, middle and high school literature textbooks, interview articles, and podcast transcripts. First, we estimated the Heaps function with this sample corpus. Next, we observed changes in the number of types and TTR values while increasing the number of tokens by one million using the estimated Heaps function. As a result of observation, we found that the TTR value hardly changed when the number of tokens exceeded from 39 to 42 million. Thus, we conclude that an appropriate size for a Mongolian general corpus is from 39 to 42 million tokens.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35760;&#24518;&#22686;&#24378;&#30340;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25554;&#25300;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#21487;&#25554;&#25300;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06029</link><description>&lt;p&gt;
&#36890;&#36807;&#35760;&#24518;&#22686;&#24378;&#30340;&#36866;&#37197;&#22120;&#23454;&#29616;&#21487;&#25554;&#25300;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pluggable Neural Machine Translation Models via Memory-augmented Adapters. (arXiv:2307.06029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06029
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35760;&#24518;&#22686;&#24378;&#30340;&#36866;&#37197;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25554;&#25300;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#29983;&#25104;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#21487;&#25554;&#25300;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#22312;&#26222;&#36890;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#25511;&#21046;&#20854;&#29983;&#25104;&#34892;&#20026;&#20197;&#28385;&#36275;&#19981;&#21516;&#29992;&#25143;&#38656;&#27714;&#20173;&#28982;&#20855;&#26377;&#19968;&#23450;&#25361;&#25112;&#24615;&#12290;&#37492;&#20110;&#27599;&#20010;&#29992;&#25143;&#38656;&#27714;&#37117;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#26032;&#27169;&#22411;&#30340;&#39640;&#26114;&#35757;&#32451;&#25104;&#26412;&#21644;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#36866;&#37197;&#22120;&#65292;&#20197;&#21487;&#25554;&#25300;&#30340;&#26041;&#24335;&#24341;&#23548;&#39044;&#35757;&#32451;&#30340;NMT&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#26679;&#26412;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#31890;&#24230;&#35760;&#24518;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#26550;&#26500;&#26469;&#32467;&#21512;&#27169;&#22411;&#34920;&#31034;&#21644;&#26816;&#32034;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35760;&#24518;&#20002;&#24323;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;NMT&#27169;&#22411;&#21644;&#35760;&#24518;&#20043;&#38388;&#30340;&#34394;&#20551;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#39118;&#26684;&#21644;&#39046;&#22495;&#29305;&#23450;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#21487;&#25554;&#25300;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although neural machine translation (NMT) models perform well in the general domain, it remains rather challenging to control their generation behavior to satisfy the requirement of different users. Given the expensive training cost and the data scarcity challenge of learning a new model from scratch for each user requirement, we propose a memory-augmented adapter to steer pretrained NMT models in a pluggable manner. Specifically, we construct a multi-granular memory based on the user-provided text samples and propose a new adapter architecture to combine the model representations and the retrieved results. We also propose a training strategy using memory dropout to reduce spurious dependencies between the NMT model and the memory. We validate our approach on both style- and domain-specific experiments and the results indicate that our method can outperform several representative pluggable baselines.
&lt;/p&gt;</description></item><item><title>PolyLM&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#21452;&#35821;&#25968;&#25454;&#21644;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#22810;&#35821;&#35328;&#33258;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#27169;&#22411;&#24615;&#33021;&#24471;&#21040;&#20102;&#22810;&#20010;&#29616;&#26377;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#30830;&#35748;&#12290;</title><link>http://arxiv.org/abs/2307.06018</link><description>&lt;p&gt;
PolyLM:&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PolyLM: An Open Source Polyglot Large Language Model. (arXiv:2307.06018v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06018
&lt;/p&gt;
&lt;p&gt;
PolyLM&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#21452;&#35821;&#25968;&#25454;&#21644;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#22810;&#35821;&#35328;&#33258;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#27169;&#22411;&#24615;&#33021;&#24471;&#21040;&#20102;&#22810;&#20010;&#29616;&#26377;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#24320;&#21457;&#20027;&#35201;&#38598;&#20013;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;&#33521;&#35821;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#36866;&#29992;&#24615;&#21644;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PolyLM&#65292;&#19968;&#20010;&#35757;&#32451;&#20102;6400&#20159;&#20010;&#26631;&#35760;&#30340;&#22810;&#35821;&#35328;LLM&#65292;&#26377;&#20004;&#31181;&#27169;&#22411;&#22823;&#23567;&#65306;1.7B&#21644;13B&#12290;&#20026;&#20102;&#22686;&#24378;&#20854;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#25105;&#20204;1&#65289;&#23558;&#21452;&#35821;&#25968;&#25454;&#25972;&#21512;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#65307;2&#65289;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#39044;&#35757;&#32451;&#30340;&#31532;&#19968;&#38454;&#27573;&#23558;&#38750;&#33521;&#35821;&#25968;&#25454;&#30340;&#27604;&#20363;&#20174;30%&#22686;&#21152;&#21040;&#26368;&#21518;&#38454;&#27573;&#30340;60%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#33258;&#23398;&#20064;&#26041;&#27861;&#65292;&#33258;&#21160;&#20026;&#27169;&#22411;&#30340;&#24494;&#35843;&#29983;&#25104;&#20102;132.7K&#20010;&#22810;&#35821;&#35328;&#25351;&#20196;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;&#20219;&#21153;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#39064;&#22238;&#31572;&#12289;&#29983;&#25104;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate remarkable ability to comprehend, reason, and generate following nature language instructions. However, the development of LLMs has been primarily focused on high-resource languages, such as English, thereby limiting their applicability and research in other languages. Consequently, we present PolyLM, a multilingual LLM trained on 640 billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its multilingual capabilities, we 1) integrate bilingual data into training data; and 2) adopt a curriculum learning strategy that increases the proportion of non-English data from 30% in the first stage to 60% in the final stage during pre-training. Further, we propose a multilingual self-instruct method which automatically generates 132.7K diverse multilingual instructions for model fine-tuning. To assess the model's performance, we collect several existing multilingual tasks, including multilingual understanding, question answering, gener
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDNAS&#30340;&#31163;&#25955;&#21270;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26494;&#24347;&#30340;&#26550;&#26500;&#34920;&#31034;&#21644;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#31163;&#25955;&#21270;&#23618;&#65292;DDNAS&#22312;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06005</link><description>&lt;p&gt;
DDNAS: &#31163;&#25955;&#21270;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification. (arXiv:2307.06005v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06005
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDNAS&#30340;&#31163;&#25955;&#21270;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#12290;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26494;&#24347;&#30340;&#26550;&#26500;&#34920;&#31034;&#21644;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#31163;&#25955;&#21270;&#23618;&#65292;DDNAS&#22312;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#22312;&#23398;&#20064;&#25991;&#26412;&#34920;&#31034;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;NAS&#26082;&#26410;&#23545;&#26550;&#26500;&#36827;&#34892;&#21487;&#23398;&#20064;&#30340;&#34701;&#21512;&#20197;&#20248;&#21270;&#65292;&#20063;&#26410;&#23545;&#25991;&#26412;&#36755;&#20837;&#32972;&#21518;&#30340;&#28508;&#22312;&#23618;&#32423;&#20998;&#31867;&#36827;&#34892;&#32534;&#30721;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#26041;&#27861;&#65292;&#21363;Discretized Differentiable Neural Architecture Search (DDNAS)&#65292;&#29992;&#20110;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#12290;&#36890;&#36807;&#26550;&#26500;&#34920;&#31034;&#30340;&#36830;&#32493;&#26494;&#24347;&#65292;DDNAS&#21487;&#20197;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26469;&#36827;&#34892;&#25628;&#32034;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#21270;&#23618;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#23558;&#20854;&#26045;&#21152;&#20110;&#27599;&#20010;&#25628;&#32034;&#33410;&#28857;&#19978;&#65292;&#20197;&#23545;&#25991;&#26412;&#34920;&#31034;&#20013;&#30340;&#28508;&#22312;&#23618;&#32423;&#20998;&#31867;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;DDNAS&#22987;&#32456;&#33021;&#22815;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;NAS&#26041;&#27861;&#12290;&#23613;&#31649;DDNAS&#20165;&#20381;&#36182;&#20110;&#21367;&#31215;&#65292;&#27744;&#21270;&#21644;&#26080;&#25805;&#20316;&#36825;&#19977;&#20010;&#22522;&#26412;&#25805;&#20316;&#65292;&#20316;&#20026;&#20505;&#36873;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has shown promising capability in learning text representation. However, existing text-based NAS neither performs a learnable fusion of neural operations to optimize the architecture, nor encodes the latent hierarchical categorization behind text input. This paper presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification. With the continuous relaxation of architecture representation, DDNAS can use gradient descent to optimize the search. We also propose a novel discretization layer via mutual information maximization, which is imposed on every search node to model the latent hierarchical categorization in text representation. Extensive experiments conducted on eight diverse real datasets exhibit that DDNAS can consistently outperform the state-of-the-art NAS methods. While DDNAS relies on only three basic operations, i.e., convolution, pooling, and none, to be the cand
&lt;/p&gt;</description></item><item><title>VoxPoser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#22810;&#31181;&#25805;&#20316;&#20219;&#21153;&#19979;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#21644;&#23545;&#35937;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05973</link><description>&lt;p&gt;
VoxPoser: &#29992;&#20110;&#24102;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#32452;&#21512;&#30340;3D&#20215;&#20540;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05973
&lt;/p&gt;
&lt;p&gt;
VoxPoser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#22312;&#22810;&#31181;&#25805;&#20316;&#20219;&#21153;&#19979;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#21644;&#23545;&#35937;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20016;&#23500;&#30340;&#21487;&#34892;&#21160;&#30693;&#35782;&#65292;&#21487;&#20197;&#20197;&#25512;&#29702;&#21644;&#35268;&#21010;&#30340;&#24418;&#24335;&#25552;&#21462;&#20986;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#36816;&#21160;&#21407;&#35821;&#26469;&#25191;&#34892;&#19982;&#29615;&#22659;&#30340;&#29289;&#29702;&#20132;&#20114;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#24320;&#38598;&#25351;&#20196;&#21644;&#24320;&#38598;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#21512;&#25104;&#26426;&#22120;&#20154;&#36712;&#36857;&#65292;&#21363;&#19968;&#31995;&#21015;&#23494;&#38598;&#30340;6-DoF&#26411;&#31471;&#25191;&#34892;&#22120;&#36335;&#24452;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;LLMs&#22312;&#32473;&#23450;&#33258;&#30001;&#24418;&#24335;&#30340;&#35821;&#35328;&#25351;&#20196;&#26102;&#25797;&#38271;&#25512;&#26029;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#30340;&#20195;&#30721;&#32534;&#20889;&#33021;&#21147;&#65292;&#23427;&#20204;&#21487;&#20197;&#19982;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20132;&#20114;&#65292;&#20197;&#32452;&#21512;3D&#20215;&#20540;&#26144;&#23556;&#23558;&#30693;&#35782;&#25509;&#22320;&#21040;Agent&#30340;&#35266;&#27979;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#26694;&#26550;&#20013;&#20351;&#29992;&#32452;&#21512;&#30340;&#20215;&#20540;&#26144;&#23556;&#26469;&#38646;&#35797;&#21512;&#25104;&#38381;&#29615;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop ro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20107;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#23545;Transformer&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#23398;&#20064;&#37327;&#21270;&#65288;SDQ&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#32047;&#31215;&#37327;&#21270;&#35823;&#24046;&#24182;&#22312;&#22810;&#35821;&#35328;&#27169;&#22411;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;XGLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SDQ&#26041;&#27861;&#21487;&#20197;&#23558;&#27169;&#22411;&#20174;32&#20301;&#28014;&#28857;&#26435;&#37325;&#20943;&#23569;&#21040;8&#20301;&#25972;&#25968;&#26435;&#37325;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05972</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#37327;&#21270;&#65306;&#22312;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#39640;&#21387;&#32553;&#29575;
&lt;/p&gt;
&lt;p&gt;
Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models. (arXiv:2307.05972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20107;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#23545;Transformer&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#23398;&#20064;&#37327;&#21270;&#65288;SDQ&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#32047;&#31215;&#37327;&#21270;&#35823;&#24046;&#24182;&#22312;&#22810;&#35821;&#35328;&#27169;&#22411;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;XGLUE&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SDQ&#26041;&#27861;&#21487;&#20197;&#23558;&#27169;&#22411;&#20174;32&#20301;&#28014;&#28857;&#26435;&#37325;&#20943;&#23569;&#21040;8&#20301;&#25972;&#25968;&#26435;&#37325;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20107;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#23545;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#23398;&#20064;&#37327;&#21270;&#65288;SDQ&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#32047;&#31215;&#37327;&#21270;&#35823;&#24046;&#24182;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;SDQ&#24212;&#29992;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;XLM-R-Base&#21644;InfoXLM-Base&#65292;&#24182;&#35777;&#26126;&#36825;&#20004;&#20010;&#27169;&#22411;&#21487;&#20197;&#20174;32&#20301;&#28014;&#28857;&#26435;&#37325;&#20943;&#23569;&#21040;8&#20301;&#25972;&#25968;&#26435;&#37325;&#65292;&#21516;&#26102;&#22312;XGLUE&#22522;&#20934;&#19978;&#20445;&#25345;&#39640;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#20984;&#26174;&#20102;&#37327;&#21270;&#22810;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#32463;&#24494;&#35843;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the effects of post-training quantization and quantization-aware training on the generalization of Transformer language models. We present a new method called self-distilled quantization (SDQ) that minimizes accumulative quantization errors and outperforms baselines. We apply SDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that both models can be reduced from 32-bit floating point weights to 8-bit integer weights while maintaining a high level of performance on the XGLUE benchmark. Our results also highlight the challenges of quantizing multilingual models, which must generalize to languages they were not fine-tuned on.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#30340;&#21407;&#22411;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;PCTL&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#37325;&#21407;&#22411;&#20363;&#22806;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35782;&#21035;&#30446;&#26631;&#29289;&#20307;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05942</link><description>&lt;p&gt;
&#23545;&#20110;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#30340;&#21407;&#22411;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prototypical Contrastive Transfer Learning for Multimodal Language Understanding. (arXiv:2307.05942v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05942
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#30340;&#21407;&#22411;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;PCTL&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#37325;&#21407;&#22411;&#20363;&#22806;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35782;&#21035;&#30446;&#26631;&#29289;&#20307;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22269;&#20869;&#26381;&#21153;&#26426;&#22120;&#20154;&#34987;&#26399;&#26395;&#33021;&#22815;&#24110;&#21161;&#38656;&#35201;&#25903;&#25345;&#30340;&#20010;&#20154;&#65292;&#20294;&#20182;&#20204;&#30446;&#21069;&#26080;&#27861;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#20204;&#24179;&#28369;&#22320;&#36827;&#34892;&#20132;&#20114;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#25351;&#20196;&#8220;&#20174;&#21416;&#25151;&#32473;&#25105;&#25343;&#19968;&#20010;&#29942;&#23376;&#8221;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#24456;&#38590;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#26126;&#30830;&#25351;&#23450;&#29942;&#23376;&#12290;&#22823;&#22810;&#25968;&#20256;&#32479;&#27169;&#22411;&#37117;&#26159;&#36890;&#36807;&#32791;&#36153;&#22823;&#37327;&#20154;&#21147;&#26469;&#25910;&#38598;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#30340;&#27169;&#25311;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#29702;&#35299;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21407;&#22411;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#65288;PCTL&#65289;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;&#21452;&#37325;&#21407;&#22411;&#20363;&#22806;&#12290;&#25105;&#20204;&#23558;PCTL&#24341;&#20837;&#21040;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35782;&#21035;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#30446;&#26631;&#29289;&#20307;&#30340;&#20219;&#21153;&#20013;&#12290;&#20026;&#20102;&#39564;&#35777;PCTL&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26032;&#30340;&#30495;&#23454;&#19990;&#30028;&#21644;&#27169;&#25311;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;PCTL&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although domestic service robots are expected to assist individuals who require support, they cannot currently interact smoothly with people through natural language. For example, given the instruction "Bring me a bottle from the kitchen," it is difficult for such robots to specify the bottle in an indoor environment. Most conventional models have been trained on real-world datasets that are labor-intensive to collect, and they have not fully leveraged simulation data through a transfer learning framework. In this study, we propose a novel transfer learning approach for multimodal language understanding called Prototypical Contrastive Transfer Learning (PCTL), which uses a new contrastive loss called Dual ProtoNCE. We introduce PCTL to the task of identifying target objects in domestic environments according to free-form natural language instructions. To validate PCTL, we built new real-world and simulation datasets. Our experiment demonstrated that PCTL outperformed existing methods. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#24615;&#27969;&#27700;&#32447;&#35299;&#30721;&#65288;PPD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21551;&#21160;&#21518;&#32493;&#20196;&#29260;&#35299;&#30721;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#36138;&#23146;&#35299;&#30721;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#25345;&#23436;&#20840;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#35299;&#30721;&#24310;&#36831;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;LLM&#35299;&#30721;&#31574;&#30053;&#26435;&#34913;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.05908</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#27969;&#27700;&#32447;&#35299;&#30721;&#65306;&#20934;&#30830;LLM&#35299;&#30721;&#20013;&#30340;&#35745;&#31639;&#24310;&#36831;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding. (arXiv:2307.05908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#24615;&#27969;&#27700;&#32447;&#35299;&#30721;&#65288;PPD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24182;&#34892;&#21551;&#21160;&#21518;&#32493;&#20196;&#29260;&#35299;&#30721;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#36138;&#23146;&#35299;&#30721;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#25345;&#23436;&#20840;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#35299;&#30721;&#24310;&#36831;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;LLM&#35299;&#30721;&#31574;&#30053;&#26435;&#34913;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#39044;&#27979;&#24615;&#27969;&#27700;&#32447;&#35299;&#30721;&#65288;PPD&#65289;"&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#36138;&#23146;&#35299;&#30721;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#21407;&#22987;&#35299;&#30721;&#23436;&#20840;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#19982;&#20256;&#32479;&#31574;&#30053;&#19981;&#21516;&#65292;PPD&#21033;&#29992;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#22312;&#24403;&#21069;&#20196;&#29260;&#35299;&#30721;&#26399;&#38388;&#24182;&#34892;&#21551;&#21160;&#21518;&#32493;&#20196;&#29260;&#35299;&#30721;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#20943;&#23569;&#20102;&#35299;&#30721;&#24310;&#36831;&#65292;&#24182;&#37325;&#26032;&#22609;&#36896;&#20102;LLM&#35299;&#30721;&#31574;&#30053;&#20013;&#30340;&#26435;&#34913;&#29702;&#35299;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#26512;&#35745;&#31639;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35780;&#20272;&#21305;&#37197;&#29575;&#65288;&#34920;&#31034;&#20026;p_correct&#65289;&#26469;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#33021;&#30340;&#24310;&#36831;&#20943;&#23569;&#36827;&#34892;&#20998;&#26512;&#20272;&#35745;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#26377;&#28508;&#21147;&#21152;&#36895;LLM&#30340;&#36138;&#23146;&#35299;&#30721;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents "Predictive Pipelined Decoding (PPD)," an approach that speeds up greedy decoding in Large Language Models (LLMs) while maintaining the exact same output as the original decoding. Unlike conventional strategies, PPD employs additional compute resources to parallelize the initiation of subsequent token decoding during the current token decoding. This innovative method reduces decoding latency and reshapes the understanding of trade-offs in LLM decoding strategies. We have developed a theoretical framework that allows us to analyze the trade-off between computation and latency. Using this framework, we can analytically estimate the potential reduction in latency associated with our proposed method, achieved through the assessment of the match rate, represented as p_correct. The results demonstrate that the use of extra computational resources has the potential to accelerate LLM greedy decoding.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21367;&#31215;&#21644;&#35760;&#24518;&#32593;&#32476;&#65292;&#22312;&#32500;&#22522;&#30334;&#31185;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#12290;&#35813;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#32463;&#36807;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21508;&#20010;&#27169;&#22411;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.05827</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#21644;&#35760;&#24518;&#32593;&#32476;&#22312;&#32500;&#22522;&#30334;&#31185;&#34920;&#26684;&#19978;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks. (arXiv:2307.05827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05827
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#21644;&#35760;&#24518;&#32593;&#32476;&#65292;&#22312;&#32500;&#22522;&#30334;&#31185;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;&#20851;&#31995;&#25277;&#21462;&#12290;&#35813;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#32463;&#36807;&#20840;&#38754;&#30340;&#20998;&#26512;&#21644;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21508;&#20010;&#27169;&#22411;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26159;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#22823;&#37096;&#20998;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#20174;&#33258;&#30001;&#26684;&#24335;&#30340;&#36830;&#32493;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#20016;&#23500;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#27604;&#22914;&#34920;&#26684;&#12290;&#25105;&#20204;&#20174;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22788;&#29702;&#34920;&#26684;&#21270;&#25968;&#25454;&#30340;&#35282;&#24230;&#25506;&#32034;&#20851;&#31995;&#25277;&#21462;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#30001;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;BiLSTM&#65289;&#32593;&#32476;&#32452;&#25104;&#65292;&#20998;&#21035;&#29992;&#20110;&#32534;&#30721;&#23454;&#20307;&#21644;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#19982;&#20043;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38169;&#35823;&#20998;&#26512;&#21644;&#21093;&#31163;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#36129;&#29486;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#26435;&#34913;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) is the task of extracting relations between entities in text. Most RE methods extract relations from free-form running text and leave out other rich data sources, such as tables. We explore RE from the perspective of applying neural methods on tabularly organized data. We introduce a new model consisting of Convolutional Neural Network (CNN) and Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and learn dependencies among them, respectively. We evaluate our model on a large and recent dataset and compare results with previous neural methods. Experimental results show that our model consistently outperforms the previous model for the task of relation extraction on tabular data. We perform comprehensive error analyses and ablation study to show the contribution of various components of our model. Finally, we discuss the usefulness and trade-offs of our approach, and provide suggestions for fostering further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25913;&#36827;&#20020;&#24202;&#35821;&#38899;&#30340;&#35789;&#24615;&#26631;&#27880;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26032;&#38395;&#26641;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#35299;&#26512;&#22120;&#26356;&#36866;&#24212;&#33258;&#28982;&#12289;&#21363;&#20852;&#30340;&#35821;&#38899;&#65292;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.05796</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25913;&#36827;&#20020;&#24202;&#35821;&#38899;&#30340;&#35789;&#24615;&#26631;&#27880;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Improved POS tagging for spontaneous, clinical speech using data augmentation. (arXiv:2307.05796v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25913;&#36827;&#20020;&#24202;&#35821;&#38899;&#30340;&#35789;&#24615;&#26631;&#27880;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26032;&#38395;&#26641;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#35299;&#26512;&#22120;&#26356;&#36866;&#24212;&#33258;&#28982;&#12289;&#21363;&#20852;&#30340;&#35821;&#38899;&#65292;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#25913;&#36827;&#20020;&#24202;&#20154;&#32676;&#35821;&#38899;&#36716;&#24405;&#30340;&#35789;&#24615;&#26631;&#27880;&#38382;&#39064;&#12290;&#19982;&#20043;&#21069;&#20851;&#20110;&#36716;&#24405;&#35821;&#38899;&#30340;&#20998;&#26512;&#21644;&#35789;&#24615;&#26631;&#27880;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20351;&#29992;&#19968;&#20010;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#26641;&#24211;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#19968;&#20010;&#26032;&#38395;&#26641;&#24211;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#36825;&#20123;&#32467;&#26500;&#26356;&#25509;&#36817;&#33258;&#28982;&#12289;&#21363;&#20852;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#24102;&#26377;&#22686;&#24378;&#25968;&#25454;&#21644;&#27809;&#26377;&#22686;&#24378;&#25968;&#25454;&#30340;&#35299;&#26512;&#22120;&#65292;&#24182;&#20351;&#29992;&#25163;&#21160;&#39564;&#35777;&#30340;&#20020;&#24202;&#35821;&#38899;&#35789;&#24615;&#26631;&#31614;&#27979;&#35797;&#20102;&#20854;&#24615;&#33021;&#65292;&#36825;&#20123;&#35821;&#38899;&#26469;&#33258;&#24739;&#26377;&#19981;&#21516;&#31867;&#22411;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of improving POS tagging of transcripts of speech from clinical populations. In contrast to prior work on parsing and POS tagging of transcribed speech, we do not make use of an in domain treebank for training. Instead, we train on an out of domain treebank of newswire using data augmentation techniques to make these structures resemble natural, spontaneous speech. We trained a parser with and without the augmented data and tested its performance using manually validated POS tags in clinical speech produced by patients with various types of neurodegenerative conditions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#21490;&#21644;&#29616;&#29366;&#65292;&#35814;&#32454;&#25551;&#36848;&#20102;&#24213;&#23618;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#35757;&#32451;&#27169;&#22411;&#26469;&#23454;&#29616;&#22810;&#31181;&#26234;&#33021;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05782</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models. (arXiv:2307.05782v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05782
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#21490;&#21644;&#29616;&#29366;&#65292;&#35814;&#32454;&#25551;&#36848;&#20102;&#24213;&#23618;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#35757;&#32451;&#27169;&#22411;&#26469;&#23454;&#29616;&#22810;&#31181;&#26234;&#33021;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#21462;&#24471;&#24778;&#20154;&#30340;&#36827;&#23637;&#65292;&#20854;&#20013;&#19968;&#20010;&#26368;&#22909;&#30340;&#20363;&#23376;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#22914;OpenAI&#30340;GPT&#31995;&#21015;&#12290;&#22312;&#36825;&#20123;&#38024;&#23545;&#20855;&#26377;&#25968;&#23398;&#25110;&#29289;&#29702;&#32972;&#26223;&#30340;&#35835;&#32773;&#32534;&#20889;&#30340;&#35762;&#24231;&#20013;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;LLMs&#30340;&#21382;&#21490;&#21644;&#29616;&#29366;&#65292;&#24182;&#35814;&#32454;&#25551;&#36848;&#20102;&#24213;&#23618;&#30340;Transformer&#26550;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20123;&#20851;&#20110;LLMs&#24037;&#20316;&#21407;&#29702;&#30340;&#24403;&#21069;&#35266;&#28857;&#65292;&#20197;&#21450;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#25991;&#26412;&#20013;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#27169;&#22411;&#22914;&#20309;&#33021;&#22815;&#25191;&#34892;&#26174;&#31034;&#26234;&#33021;&#30340;&#20854;&#20182;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is making spectacular progress, and one of the best examples is the development of large language models (LLMs) such as OpenAI's GPT series. In these lectures, written for readers with a background in mathematics or physics, we give a brief history and survey of the state of the art, and describe the underlying transformer architecture in detail. We then explore some current ideas on how LLMs work and how models trained to predict the next word in a text are able to perform other tasks displaying intelligence.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;ChatGPT&#29983;&#25104;&#21644;&#22686;&#24378;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#65292;&#23613;&#31649;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#24187;&#35937;&#25968;&#25454;&#22312;&#32763;&#35793;&#20449;&#21495;&#26041;&#38754;&#25913;&#21892;&#20102;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05779</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#29983;&#25104;&#21644;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation Data Generation and Augmentation using ChatGPT. (arXiv:2307.05779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05779
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ChatGPT&#29983;&#25104;&#21644;&#22686;&#24378;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#65292;&#23613;&#31649;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#24187;&#35937;&#25968;&#25454;&#22312;&#32763;&#35793;&#20449;&#21495;&#26041;&#38754;&#25913;&#21892;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27169;&#22411;&#24050;&#32463;&#22312;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20294;&#21019;&#24314;&#24179;&#34892;&#35821;&#26009;&#24211;&#32791;&#26102;&#32791;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#25163;&#24037;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;-&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#24187;&#35937;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#26412;&#36523;&#26159;&#22522;&#20110;&#24179;&#34892;&#25968;&#25454;&#35757;&#32451;&#30340;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#21033;&#29992;&#22810;&#35821;&#35328;&#21521;&#37327;&#31354;&#38388;&#26469;&#21019;&#24314;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#34917;&#20805;&#23567;&#35268;&#27169;&#25163;&#24037;&#37319;&#38598;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20004;&#20010;&#20851;&#38190;&#21457;&#29616;-&#23613;&#31649;&#20854;&#36755;&#20986;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#20294;&#24187;&#35937;&#25968;&#25454;&#25913;&#21892;&#20102;&#32763;&#35793;&#20449;&#21495;&#65292;&#21363;&#20351;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#39046;&#22495;&#19981;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural models have revolutionized the field of machine translation, but creating parallel corpora is expensive and time-consuming. We investigate an alternative to manual parallel corpora - hallucinated parallel corpora created by generative language models. Although these models are themselves trained on parallel data, they can leverage a multilingual vector space to create data, and may be able to supplement small manually-procured corpora. Our experiments highlight two key findings - despite a lack of diversity in their output, the hallucinated data improves the translation signal, even when the domain clashes with the original dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23454;&#29616;&#25345;&#32493;&#30340;&#35821;&#35328;&#23398;&#20064;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20219;&#21153;&#24207;&#21015;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#12290;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#20855;&#26377;&#31215;&#26497;&#20256;&#36882;&#28508;&#21147;&#30340;&#20219;&#21153;&#65292;&#24182;&#36991;&#20813;&#36127;&#36801;&#31227;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.05741</link><description>&lt;p&gt;
&#36808;&#21521;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust and Efficient Continual Language Learning. (arXiv:2307.05741v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23454;&#29616;&#25345;&#32493;&#30340;&#35821;&#35328;&#23398;&#20064;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20219;&#21153;&#24207;&#21015;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#12290;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#20855;&#26377;&#31215;&#26497;&#20256;&#36882;&#28508;&#21147;&#30340;&#20219;&#21153;&#65292;&#24182;&#36991;&#20813;&#36127;&#36801;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#24555;&#36895;&#36866;&#24212;&#27169;&#22411;&#21040;&#26032;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;&#25345;&#32493;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;&#36825;&#20010;&#32463;&#20856;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#26032;&#30340;&#20219;&#21153;&#19978;&#32487;&#32493;&#24494;&#35843;&#36807;&#21435;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20197;"&#20256;&#36755;"&#30456;&#20851;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20063;&#23384;&#22312;&#26356;&#22810;&#23475;&#22788;&#30340;&#39118;&#38505;&#65292;&#21363;&#36127;&#36801;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#24207;&#21015;&#22522;&#20934;&#65292;&#38024;&#23545;&#21487;&#33021;&#38754;&#20020;&#30340;&#19981;&#21516;&#20256;&#36882;&#22330;&#26223;&#65292;&#27604;&#22914;&#20855;&#26377;&#31215;&#26497;&#20256;&#36882;&#28508;&#21147;&#30340;&#20219;&#21153;&#24207;&#21015;&#12289;&#20855;&#26377;&#36127;&#36801;&#31227;&#28508;&#21147;&#30340;&#20219;&#21153;&#24207;&#21015;&#12289;&#27809;&#26377;&#39044;&#26399;&#25928;&#26524;&#30340;&#20219;&#21153;&#24207;&#21015;&#25110;&#32773;&#28151;&#21512;&#30340;&#20219;&#21153;&#24207;&#21015;&#12290;&#29702;&#24819;&#30340;&#23398;&#20064;&#32773;&#24212;&#35813;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#20855;&#26377;&#31215;&#26497;&#20256;&#36882;&#28508;&#21147;&#30340;&#20219;&#21153;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#36991;&#20813;&#20219;&#20309;&#21487;&#33021;&#28151;&#28102;&#23427;&#30340;&#24178;&#25200;&#24615;&#20219;&#21153;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23398;&#20064;&#32773;&#65292;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the application space of language models continues to evolve, a natural question to ask is how we can quickly adapt models to new tasks. We approach this classic question from a continual learning perspective, in which we aim to continue fine-tuning models trained on past tasks on new tasks, with the goal of "transferring" relevant knowledge. However, this strategy also runs the risk of doing more harm than good, i.e., negative transfer. In this paper, we construct a new benchmark of task sequences that target different possible transfer scenarios one might face, such as a sequence of tasks with high potential of positive transfer, high potential for negative transfer, no expected effect, or a mixture of each. An ideal learner should be able to maximally exploit information from all tasks that have any potential for positive transfer, while also avoiding the negative effects of any distracting tasks that may confuse it. We then propose a simple, yet effective, learner that satisfies
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#65292;&#21457;&#29616;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.05722</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. (arXiv:2307.05722v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#22270;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#65292;&#21457;&#29616;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#32844;&#20301;&#25512;&#33616;&#20013;&#23545;&#34892;&#20026;&#22270;&#30340;&#29702;&#35299;&#28508;&#21147;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34892;&#20026;&#22270;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#29702;&#35299;&#26469;&#25552;&#21319;&#22312;&#32447;&#25307;&#32856;&#20013;&#30340;&#25512;&#33616;&#65292;&#21253;&#25324;&#20419;&#36827;&#38750;&#20998;&#24067;&#24335;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35821;&#20041;&#34920;&#31034;&#26469;&#20998;&#26512;&#34892;&#20026;&#22270;&#24182;&#25581;&#31034;&#20854;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#36335;&#24452;&#25552;&#31034;&#26500;&#36896;&#22120;&#65292;&#21033;&#29992;LLM&#25512;&#33616;&#22120;&#39318;&#27425;&#29702;&#35299;&#34892;&#20026;&#22270;&#65292;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#36335;&#24452;&#22686;&#24378;&#27169;&#22359;&#26469;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#24207;&#21015;&#36755;&#20837;&#24341;&#20837;&#30340;&#25552;&#31034;&#20559;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23558;LM&#30340;&#29305;&#28857;&#24341;&#20837;&#21040;&#34892;&#20026;&#22270;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks, demonstrating their exceptional capabilities in various domains. However, their potential for behavior graph understanding in job recommendations remains largely unexplored. This paper focuses on unveiling the capability of large language models in understanding behavior graphs and leveraging this understanding to enhance recommendations in online recruitment, including the promotion of out-of-distribution (OOD) application. We present a novel framework that harnesses the rich contextual information and semantic representations provided by large language models to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, we propose a meta-path prompt constructor that leverages LLM recommender to understand behavior graphs for the first time and design a corresponding path augmentation module to alleviate the prompt bias introduced by path-based sequence input. By leveragin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#24635;&#32467;&#26381;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#23618;&#32423;&#20010;&#24615;&#21270;&#22522;&#20110;&#27010;&#24565;&#30340;&#24635;&#32467;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#25968;&#25454;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#32972;&#26223;&#19979;&#65292;&#24110;&#21161;&#29992;&#25143;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.05696</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#24635;&#32467;&#26381;&#21153;&#65306;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Personalized Reinforcement Learning Summarization Service for Learning Structure from Unstructured Data. (arXiv:2307.05696v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#24378;&#21270;&#23398;&#20064;&#24635;&#32467;&#26381;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#23618;&#32423;&#20010;&#24615;&#21270;&#22522;&#20110;&#27010;&#24565;&#30340;&#24635;&#32467;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#25968;&#25454;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#32972;&#26223;&#19979;&#65292;&#24110;&#21161;&#29992;&#25143;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#38656;&#35201;&#24037;&#20855;&#26469;&#24110;&#21161;&#29992;&#25143;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#20256;&#32479;&#30340;&#25991;&#26723;&#25688;&#35201;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#20010;&#20154;&#29992;&#25143;&#38656;&#27714;&#65292;&#24182;&#19988;&#32570;&#20047;&#39640;&#25928;&#20449;&#24687;&#22788;&#29702;&#30340;&#32467;&#26500;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#20010;&#24615;&#21270;&#22522;&#20110;&#27010;&#24565;&#30340;&#24635;&#32467;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#25991;&#26723;&#32508;&#21512;&#25104;&#31616;&#27905;&#30340;&#23618;&#32423;&#27010;&#24565;&#22270;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#21644;&#36866;&#24212;&#29992;&#25143;&#20559;&#22909;&#26469;&#31215;&#26497;&#21442;&#19982;&#29992;&#25143;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#29305;&#23450;&#20027;&#39064;&#30340;&#26410;&#35265;&#25991;&#26723;&#29983;&#25104;&#20010;&#24615;&#21270;&#25688;&#35201;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#29702;&#35299;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#23548;&#33322;&#65292;&#24182;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#33258;&#24049;&#29420;&#29305;&#30340;&#38656;&#27714;&#20174;&#22823;&#37327;&#25991;&#26723;&#38598;&#21512;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of textual data has created a crucial need for tools that assist users in extracting meaningful insights. Traditional document summarization approaches often fail to meet individual user requirements and lack structure for efficient information processing. To address these limitations, we propose Summation, a hierarchical personalized concept-based summarization approach. It synthesizes documents into a concise hierarchical concept map and actively engages users by learning and adapting to their preferences. Using a Reinforcement Learning algorithm, Summation generates personalized summaries for unseen documents on specific topics. This framework enhances comprehension, enables effective navigation, and empowers users to extract meaningful insights from large document collections aligned with their unique requirements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05695</link><description>&lt;p&gt;
&#20197;&#19981;&#21516;&#26041;&#24335;&#22534;&#21472;&#26356;&#22810;&#23618;&#65306;&#36890;&#36807;&#20302;&#31209;&#26356;&#26032;&#36827;&#34892;&#39640;&#31209;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#22312;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#19978;&#30456;&#24403;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#36234;&#22823;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#36234;&#39640;&#65292;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#32593;&#32476;&#25317;&#26377;&#25968;&#30334;&#20159;&#20010;&#21442;&#25968;&#30340;&#35268;&#27169;&#24050;&#32463;&#21344;&#20027;&#23548;&#22320;&#20301;&#24182;&#19988;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#24517;&#35201;&#24615;&#20173;&#28982;&#32570;&#20047;&#28165;&#26224;&#30340;&#29702;&#35299;&#65292;&#32780;&#26367;&#20195;&#26041;&#27861;&#19981;&#19968;&#23450;&#33021;&#22815;&#38477;&#20302;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#20316;&#20026;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;ReLoRA&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20302;&#31209;&#26356;&#26032;&#26469;&#35757;&#32451;&#39640;&#31209;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;ReLoRA&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#37327;&#39640;&#36798;350M&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#19982;&#24120;&#35268;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ReLoRA&#30340;&#25928;&#29575;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#39640;&#25928;&#35757;&#32451;&#21315;&#20159;&#32423;&#21442;&#25968;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20302;&#31209;&#35757;&#32451;&#25216;&#26415;&#30340;&#28508;&#21147;&#21450;&#20854;&#23545;&#20110;&#32553;&#25918;&#23450;&#24459;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39640;&#25512;&#26029;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;&#20013;&#20849;&#25351;&#28040;&#35299;&#30340;&#22788;&#29702;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20849;&#25351;&#28040;&#35299;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#22312;&#20855;&#26377;&#20849;&#25351;&#28040;&#35299;&#30340;&#35780;&#35770;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05646</link><description>&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#20849;&#25351;&#28040;&#35299;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Better Handling Coreference Resolution in Aspect Level Sentiment Classification by Fine-Tuning Language Models. (arXiv:2307.05646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39640;&#25512;&#26029;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;&#20013;&#20849;&#25351;&#28040;&#35299;&#30340;&#22788;&#29702;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20849;&#25351;&#28040;&#35299;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#22312;&#20855;&#26377;&#20849;&#25351;&#28040;&#35299;&#30340;&#35780;&#35770;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20844;&#21496;&#26469;&#35828;&#65292;&#23458;&#25143;&#21453;&#39304;&#26159;&#38750;&#24120;&#23453;&#36149;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20248;&#21270;&#20135;&#21697;&#12290;&#36890;&#36807;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;(ALSC)&#21487;&#20197;&#33258;&#21160;&#30417;&#27979;&#23458;&#25143;&#21453;&#39304;&#65292;&#20174;&#32780;&#20998;&#26512;&#35780;&#35770;&#20013;&#30340;&#20855;&#20307;&#26041;&#38754;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;ALSC&#35299;&#20915;&#26041;&#26696;&#30340;&#26680;&#24515;&#65292;&#20294;&#22312;&#26576;&#20123;&#38656;&#35201;&#20849;&#25351;&#28040;&#35299;(CR)&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#39640;&#25512;&#26029;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;LLM&#22312;&#20855;&#26377;CR&#30340;&#35780;&#35770;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24615;&#33021;&#30340;&#25913;&#36827;&#24456;&#21487;&#33021;&#24402;&#22240;&#20110;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;CR&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#20851;&#20110;ALSC&#20013;CR&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customer feedback is invaluable to companies as they refine their products. Monitoring customer feedback can be automated with Aspect Level Sentiment Classification (ALSC) which allows us to analyse specific aspects of the products in reviews. Large Language Models (LLMs) are the heart of many state-of-the-art ALSC solutions, but they perform poorly in some scenarios requiring Coreference Resolution (CR). In this work, we propose a framework to improve an LLM's performance on CR-containing reviews by fine tuning on highly inferential tasks. We show that the performance improvement is likely attributed to the improved model CR ability. We also release a new dataset that focuses on CR in ALSC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#34917;&#19969;&#32454;&#21270;&#27169;&#22411;&#65288;PatReFormer&#65289;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#21106;&#23884;&#20837;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#25913;&#36827;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#23884;&#20837;&#29305;&#24449;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05627</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#34917;&#19969;&#32454;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Separate-and-Aggregate: A Transformer-based Patch Refinement Model for Knowledge Graph Completion. (arXiv:2307.05627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#34917;&#19969;&#32454;&#21270;&#27169;&#22411;&#65288;PatReFormer&#65289;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#21106;&#23884;&#20837;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#25913;&#36827;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#23884;&#20837;&#29305;&#24449;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26159;&#20174;&#32473;&#23450;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#25512;&#26029;&#32570;&#22833;&#20107;&#23454;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#20026;&#21487;&#35757;&#32451;&#30340;&#36830;&#32493;&#23884;&#20837;&#65292;&#24182;&#23558;&#23454;&#20307;$h$&#65288;&#25110;$t$&#65289;&#21644;&#20851;&#31995;$r$&#30340;&#23884;&#20837;&#34701;&#21512;&#20026;&#26597;&#35810;&#30340;&#38544;&#34255;&#34920;&#31034;$(h, r, ?)$&#65288;&#25110;$(?, r, t$)&#65289;&#20197;&#36817;&#20284;&#32570;&#22833;&#23454;&#20307;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#20182;&#20204;&#35201;&#20040;&#20351;&#29992;&#27973;&#23618;&#32447;&#24615;&#21464;&#25442;&#65292;&#35201;&#20040;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#32447;&#24615;&#21464;&#25442;&#23384;&#22312;&#34920;&#36798;&#33021;&#21147;&#38382;&#39064;&#65292;&#32780;&#28145;&#24230;&#21367;&#31215;&#27169;&#22359;&#24341;&#20837;&#20102;&#19981;&#24517;&#35201;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#34917;&#19969;&#32454;&#21270;&#27169;&#22411;&#65288;PatReFormer&#65289;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;PatReFormer&#39318;&#20808;&#23558;&#23884;&#20837;&#20998;&#21106;&#25104;&#19968;&#31995;&#21015;&#34917;&#19969;&#65292;&#28982;&#21518;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20801;&#35768;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#21452;&#21521;&#23884;&#20837;&#29305;&#24449;&#20132;&#20114;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) is the task of inferencing missing facts from any given knowledge graphs (KG). Previous KGC methods typically represent knowledge graph entities and relations as trainable continuous embeddings and fuse the embeddings of the entity $h$ (or $t$) and relation $r$ into hidden representations of query $(h, r, ?)$ (or $(?, r, t$)) to approximate the missing entities. To achieve this, they either use shallow linear transformations or deep convolutional modules. However, the linear transformations suffer from the expressiveness issue while the deep convolutional modules introduce unnecessary inductive bias, which could potentially degrade the model performance. Thus, we propose a novel Transformer-based Patch Refinement Model (PatReFormer) for KGC. PatReFormer first segments the embedding into a sequence of patches and then employs cross-attention modules to allow bi-directional embedding feature interaction between the entities and relations, leading to a bet
&lt;/p&gt;</description></item><item><title>SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2307.05591</link><description>&lt;p&gt;
SITTA: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05591
&lt;/p&gt;
&lt;p&gt;
SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#20687;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#29702;&#35299;&#23545;&#20110;&#29983;&#25104;&#36866;&#24403;&#30340;&#25551;&#36848;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#38656;&#35201;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35780;&#20272;&#22330;&#26223;&#30340;&#35821;&#20041;&#65292;&#24182;&#23558;&#25552;&#21462;&#30340;&#30693;&#35782;&#34920;&#31034;&#22312;&#35821;&#35328;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#22312;&#20445;&#35777;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#30340;&#21516;&#26102;&#23454;&#29616;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34987;&#26465;&#20214;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;-&#25991;&#26412;&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#20351;&#29992;&#22270;&#20687;&#36755;&#20837;&#12290;&#36825;&#35201;&#27714;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20013;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#20256;&#36882;&#32473;LM&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#36716;&#31227;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#22810;&#27169;&#24577;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#23545;&#24694;&#24847;&#35328;&#35770;&#20013;&#30340;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#21644;&#24694;&#24847;&#35328;&#36766;&#30340;&#24178;&#25200;&#20197;&#21450;&#24694;&#24847;&#35328;&#35770;&#21644;&#38750;&#24694;&#24847;&#35328;&#35770;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#36328;&#24230;&#32423;&#20449;&#24687;&#30340;&#25429;&#25417;&#12290;</title><link>http://arxiv.org/abs/2307.05578</link><description>&lt;p&gt;
&#24694;&#24847;&#35328;&#35770;&#36890;&#36807;&#21452;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hate Speech Detection via Dual Contrastive Learning. (arXiv:2307.05578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#23545;&#24694;&#24847;&#35328;&#35770;&#20013;&#30340;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#21644;&#24694;&#24847;&#35328;&#36766;&#30340;&#24178;&#25200;&#20197;&#21450;&#24694;&#24847;&#35328;&#35770;&#21644;&#38750;&#24694;&#24847;&#35328;&#35770;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#36328;&#24230;&#32423;&#20449;&#24687;&#30340;&#25429;&#25417;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#35328;&#35770;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24555;&#36895;&#20256;&#25773;&#24433;&#21709;&#30528;&#20114;&#32852;&#32593;&#29615;&#22659;&#21644;&#25105;&#20204;&#30340;&#31038;&#20250;&#65292;&#22686;&#21152;&#20102;&#20559;&#35265;&#24182;&#20260;&#23475;&#20102;&#20154;&#20204;&#12290;&#26816;&#27979;&#24694;&#24847;&#35328;&#35770;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#36825;&#20010;&#20219;&#21153;&#20173;&#28982;&#38754;&#20020;&#30528;&#20004;&#20010;&#22266;&#26377;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#24694;&#24847;&#35328;&#35770;&#20013;&#20256;&#36798;&#30340;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#20013;&#20398;&#36785;&#24615;&#35328;&#36766;&#30340;&#24178;&#25200;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#24694;&#24847;&#35328;&#35770;&#21644;&#38750;&#24694;&#24847;&#35328;&#35770;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#20005;&#37325;&#25439;&#23475;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#21452;&#23545;&#27604;&#23398;&#20064;&#65288;DCL&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#21644;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#65292;&#25429;&#25417;&#36229;&#20986;&#29616;&#26377;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#22522;&#20110;&#20196;&#29260;&#32423;&#24773;&#24863;&#35821;&#20041;&#30340;&#36328;&#24230;&#32423;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fast spread of hate speech on social media impacts the Internet environment and our society by increasing prejudice and hurting people. Detecting hate speech has aroused broad attention in the field of natural language processing. Although hate speech detection has been addressed in recent work, this task still faces two inherent unsolved challenges. The first challenge lies in the complex semantic information conveyed in hate speech, particularly the interference of insulting words in hate speech detection. The second challenge is the imbalanced distribution of hate speech and non-hate speech, which may significantly deteriorate the performance of models. To tackle these challenges, we propose a novel dual contrastive learning (DCL) framework for hate speech detection. Our framework jointly optimizes the self-supervised and the supervised contrastive learning loss for capturing span-level information beyond the token-level emotional semantics used in existing models, particularly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QGA-EE&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#30340;&#26041;&#24335;&#36827;&#34892;&#20107;&#20214;&#25552;&#21462;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#20107;&#20214;&#21442;&#25968;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.05567</link><description>&lt;p&gt;
&#20107;&#20214;&#25552;&#21462;&#20316;&#20026;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Event Extraction as Question Generation and Answering. (arXiv:2307.05567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QGA-EE&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#30340;&#26041;&#24335;&#36827;&#34892;&#20107;&#20214;&#25552;&#21462;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#20107;&#20214;&#21442;&#25968;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#20110;&#20107;&#20214;&#25552;&#21462;&#30340;&#30740;&#31350;&#23558;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#38382;&#39064;&#22238;&#31572; (QA)&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#26395;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#20107;&#20214;&#21442;&#25968;&#32780;&#19981;&#26159;&#20808;&#25552;&#21462;&#20505;&#36873;&#39033;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#26631;&#35760;&#30340;&#20998;&#31867;&#26041;&#27861;&#20013;&#30340;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#22522;&#20110;&#22266;&#23450;&#27169;&#26495;&#65292;&#24182;&#19988;&#24456;&#23569;&#21033;&#29992;&#30456;&#20851;&#21442;&#25968;&#31561;&#35821;&#22659;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;QA&#26041;&#27861;&#22312;&#22788;&#29702;&#21516;&#19968;&#35282;&#33394;&#26377;&#22810;&#20010;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QGA-EE&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21253;&#21547;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#32780;&#19981;&#26159;&#20351;&#29992;&#22266;&#23450;&#27169;&#26495;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21160;&#24577;&#27169;&#26495;&#26469;&#36741;&#21161;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;QGA-EE&#22312;ACE05&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#25152;&#26377;&#20043;&#21069;&#21333;&#20219;&#21153;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on Event Extraction has reframed the task as Question Answering (QA), with promising results. The advantage of this approach is that it addresses the error propagation issue found in traditional token-based classification approaches by directly predicting event arguments without extracting candidates first. However, the questions are typically based on fixed templates and they rarely leverage contextual information such as relevant arguments. In addition, prior QA-based approaches have difficulty handling cases where there are multiple arguments for the same role. In this paper, we propose QGA-EE, which enables a Question Generation (QG) model to generate questions that incorporate rich contextual information instead of using fixed templates. We also propose dynamic templates to assist the training of QG model. Experiments show that QGA-EE outperforms all prior single-task-based models on the ACE05 English dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#22686;&#24378;CLIP&#22312;&#22788;&#29702;&#22797;&#21512;&#24615;&#21644;&#27495;&#20041;&#24615;&#26041;&#38754;&#65292;&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20316;&#32773;&#37319;&#29992;Augment-CLIP&#21644;Stable Diffusion Sampling&#65288;SD Sampling&#65289;&#20004;&#20010;&#31995;&#32479;&#26469;&#35299;&#20915;CLIP&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#29983;&#25104;&#21253;&#21547;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;&#21477;&#23376;&#21644;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#31639;&#27861;&#29983;&#25104;&#22810;&#20010;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.05564</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;1&#30340;&#22686;&#24378;&#22120;: &#36890;&#36807;&#25552;&#31034;&#22686;&#24378;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#25913;&#36827;CLIP&#22788;&#29702;&#22797;&#21512;&#24615;&#21644;&#27495;&#20041;&#24615;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#35270;&#35273;VWSD
&lt;/p&gt;
&lt;p&gt;
Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion. (arXiv:2307.05564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#22686;&#24378;CLIP&#22312;&#22788;&#29702;&#22797;&#21512;&#24615;&#21644;&#27495;&#20041;&#24615;&#26041;&#38754;&#65292;&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20316;&#32773;&#37319;&#29992;Augment-CLIP&#21644;Stable Diffusion Sampling&#65288;SD Sampling&#65289;&#20004;&#20010;&#31995;&#32479;&#26469;&#35299;&#20915;CLIP&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#29983;&#25104;&#21253;&#21547;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;&#21477;&#23376;&#21644;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#31639;&#27861;&#29983;&#25104;&#22810;&#20010;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#38024;&#23545;&#33521;&#35821;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#65288;VWSD&#65289;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;CLIP&#23558;&#20505;&#36873;&#22270;&#20687;&#19982;&#30701;&#35821;&#36827;&#34892;&#21305;&#37197;&#30340;&#31616;&#21333;&#26041;&#27861;&#21463;&#21040;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#22810;&#23545;&#22810;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#21512;&#24615;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#30456;&#21453;&#65292;&#30701;&#35821;&#30340;&#25551;&#36848;&#24615;&#28966;&#28857;&#22240;&#23454;&#20363;&#32780;&#24322;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;Augment-CLIP&#21644;Stable Diffusion Sampling&#65288;SD Sampling&#65289;&#12290;Augment-CLIP&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21253;&#21547;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;&#21477;&#23376;&#26469;&#22686;&#24378;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20854;&#20182;&#35821;&#35328;&#30340;CLIP&#27169;&#22411;&#65292;&#22240;&#20026;&#19968;&#20010;&#26377;&#27495;&#20041;&#30340;&#35789;&#21487;&#33021;&#20250;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#20013;&#34987;&#32763;&#35793;&#20026;&#19968;&#20010;&#26080;&#27495;&#20041;&#30340;&#35789;&#12290;SD Sampling&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#31283;&#23450;&#25193;&#25955;&#26469;&#20174;&#32473;&#23450;&#30340;&#30701;&#35821;&#29983;&#25104;&#22810;&#20010;&#22270;&#20687;&#65292;&#22686;&#21152;&#23376;&#38598;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our zero-shot approaches for the Visual Word Sense Disambiguation (VWSD) Task in English. Our preliminary study shows that the simple approach of matching candidate images with the phrase using CLIP suffers from the many-to-many nature of image-text pairs. We find that the CLIP text encoder may have limited abilities in capturing the compositionality in natural language. Conversely, the descriptive focus of the phrase varies from instance to instance. We address these issues in our two systems, Augment-CLIP and Stable Diffusion Sampling (SD Sampling). Augment-CLIP augments the text prompt by generating sentences that contain the context phrase with the help of large language models (LLMs). We further explore CLIP models in other languages, as the an ambiguous word may be translated into an unambiguous one in the other language. SD Sampling uses text-to-image Stable Diffusion to generate multiple images from the given phrase, increasing the likelihood that a subset 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#26234;&#21033;&#20844;&#20849;&#21307;&#30103;&#31995;&#32479;&#20013;&#35774;&#35745;&#21644;&#37096;&#32626;&#30340;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#32534;&#30721;&#36716;&#35786;&#30142;&#30149;&#30340;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;NER&#27169;&#22411;&#35782;&#21035;&#30142;&#30149;&#25552;&#21450;&#65292;&#24182;&#22522;&#20110;Elasticsearch&#30340;&#25628;&#32034;&#24341;&#25806;&#31995;&#32479;&#36827;&#34892;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2307.05560</link><description>&lt;p&gt;
&#22312;&#26234;&#21033;&#20844;&#20849;&#21307;&#30103;&#31995;&#32479;&#20013;&#35774;&#35745;&#21644;&#37096;&#32626;&#19968;&#20010;&#29992;&#20110;&#35268;&#33539;&#36716;&#35786;&#30340;&#20840;&#22269;&#33539;&#22260;&#20869;&#33258;&#21160;&#32534;&#30721;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the Chilean Public Healthcare System. (arXiv:2307.05560v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#26234;&#21033;&#20844;&#20849;&#21307;&#30103;&#31995;&#32479;&#20013;&#35774;&#35745;&#21644;&#37096;&#32626;&#30340;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#32534;&#30721;&#36716;&#35786;&#30142;&#30149;&#30340;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;NER&#27169;&#22411;&#35782;&#21035;&#30142;&#30149;&#25552;&#21450;&#65292;&#24182;&#22522;&#20110;Elasticsearch&#30340;&#25628;&#32034;&#24341;&#25806;&#31995;&#32479;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30142;&#30149;&#32534;&#30721;&#20219;&#21153;&#28041;&#21450;&#20026;&#20020;&#24202;&#25991;&#26723;&#20013;&#25552;&#21040;&#30340;&#27599;&#31181;&#30142;&#30149;&#20998;&#37197;&#19968;&#20010;&#21463;&#25511;&#35789;&#27719;&#20013;&#30340;&#21807;&#19968;&#26631;&#35782;&#31526;&#12290;&#36825;&#20010;&#20219;&#21153;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#20363;&#22914;&#22312;&#29305;&#23450;&#32972;&#26223;&#19979;&#36827;&#34892;&#27969;&#34892;&#30149;&#23398;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#32534;&#30721;&#36807;&#31243;&#23481;&#26131;&#20986;&#38169;&#65292;&#35201;&#27714;&#21307;&#21153;&#20154;&#21592;&#31934;&#36890;&#32534;&#30721;&#35268;&#21017;&#21644;&#26415;&#35821;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#36807;&#31243;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#36825;&#20123;&#36164;&#28304;&#21487;&#20197;&#29992;&#20110;&#26356;&#20855;&#20020;&#24202;&#24847;&#20041;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#22256;&#38590;&#21487;&#20197;&#36890;&#36807;&#24320;&#21457;&#33021;&#22815;&#33258;&#21160;&#20026;&#30142;&#30149;&#20998;&#37197;&#20195;&#30721;&#30340;&#35745;&#31639;&#31995;&#32479;&#26469;&#35299;&#20915;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#27861;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#32534;&#30721;&#26234;&#21033;&#20844;&#20849;&#21307;&#30103;&#31995;&#32479;&#20013;&#30340;&#36716;&#35786;&#30142;&#30149;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;NER&#27169;&#22411;&#26469;&#35782;&#21035;&#30142;&#30149;&#25552;&#21450;&#65292;&#24182;&#22522;&#20110;Elasticsearch&#30340;&#25628;&#32034;&#24341;&#25806;&#31995;&#32479;&#26469;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The disease coding task involves assigning a unique identifier from a controlled vocabulary to each disease mentioned in a clinical document. This task is relevant since it allows information extraction from unstructured data to perform, for example, epidemiological studies about the incidence and prevalence of diseases in a determined context. However, the manual coding process is subject to errors as it requires medical personnel to be competent in coding rules and terminology. In addition, this process consumes a lot of time and energy, which could be allocated to more clinically relevant tasks. These difficulties can be addressed by developing computational systems that automatically assign codes to diseases. In this way, we propose a two-step system for automatically coding diseases in referrals from the Chilean public healthcare system. Specifically, our model uses a state-of-the-art NER model for recognizing disease mentions and a search engine system based on Elasticsearch for 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#20013;&#30340;&#21453;&#39304;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#21644;&#35770;&#25991;&#29305;&#24449;&#65292;&#24182;&#22238;&#39038;&#20102;&#25552;&#20379;&#21453;&#39304;&#30340;&#26368;&#26032;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.05553</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#20013;&#30340;&#21453;&#39304;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of feedback in Automated Essay Scoring. (arXiv:2307.05553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#20013;&#30340;&#21453;&#39304;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#21644;&#35770;&#25991;&#29305;&#24449;&#65292;&#24182;&#22238;&#39038;&#20102;&#25552;&#20379;&#21453;&#39304;&#30340;&#26368;&#26032;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#31995;&#32479;&#35806;&#29983;&#20110;50&#24180;&#21069;&#12290;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#31995;&#32479;&#27491;&#22312;&#21457;&#23637;&#25104;&#20026;&#27604;&#20197;&#21069;&#31616;&#21333;&#35780;&#20998;&#31995;&#32479;&#26356;&#21152;&#21151;&#33021;&#20016;&#23500;&#30340;&#31995;&#32479;&#12290;&#23427;&#30340;&#30446;&#30340;&#19981;&#20165;&#20165;&#26159;&#35780;&#20998;&#65292;&#36824;&#20316;&#20026;&#19968;&#20010;&#23398;&#20064;&#24037;&#20855;&#26469;&#25552;&#39640;&#29992;&#25143;&#30340;&#20889;&#20316;&#33021;&#21147;&#12290;&#21453;&#39304;&#26159;&#20351;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#31995;&#32479;&#22312;&#23454;&#38469;&#29983;&#27963;&#20013;&#26377;&#29992;&#30340;&#26368;&#37325;&#35201;&#26041;&#38754;&#12290;&#22312;&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#31995;&#32479;&#20013;&#24050;&#32463;&#24378;&#35843;&#20102;&#21453;&#39304;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#20851;&#20110;&#33258;&#21160;&#21270;&#35770;&#25991;&#35780;&#20998;&#30340;&#21453;&#39304;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#21453;&#39304;&#21644;&#35770;&#25991;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#25552;&#20379;&#21453;&#39304;&#30340;&#26368;&#26032;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The first automated essay scoring system was developed 50 years ago. Automated essay scoring systems are developing into systems with richer functions than the previous simple scoring systems. Its purpose is not only to score essays but also as a learning tool to improve the writing skill of users. Feedback is the most important aspect of making an automated essay scoring system useful in real life. The importance of feedback was already emphasized in the first AES system. This paper reviews research on feedback including different feedback types and essay traits on automated essay scoring. We also reviewed the latest case studies of the automated essay scoring system that provides feedback.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#24335;&#26469;&#32452;&#32455;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#36890;&#36807;&#25551;&#36848;&#21508;&#31181;&#35843;&#21046;&#31574;&#30053;&#26469;&#23454;&#29616;&#31185;&#23398;&#25991;&#29486;&#30340;&#21487;&#25511;&#29983;&#25104;&#65292;&#20026;&#22522;&#20110;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#30340;&#32452;&#21512;&#25552;&#20379;&#20102;&#26032;&#30340;&#26550;&#26500;&#65292;&#26410;&#26469;&#30740;&#31350;&#23558;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#20197;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05538</link><description>&lt;p&gt;
&#31185;&#23398;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advancements in Scientific Controllable Text Generation Methods. (arXiv:2307.05538v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#24335;&#26469;&#32452;&#32455;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#36890;&#36807;&#25551;&#36848;&#21508;&#31181;&#35843;&#21046;&#31574;&#30053;&#26469;&#23454;&#29616;&#31185;&#23398;&#25991;&#29486;&#30340;&#21487;&#25511;&#29983;&#25104;&#65292;&#20026;&#22522;&#20110;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#30340;&#32452;&#21512;&#25552;&#20379;&#20102;&#26032;&#30340;&#26550;&#26500;&#65292;&#26410;&#26469;&#30740;&#31350;&#23558;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#20197;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#24335;&#26469;&#32452;&#32455;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;&#35813;&#27169;&#24335;&#30001;&#19971;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65292;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#23545;&#20110;&#29983;&#25104;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#31185;&#23398;&#25991;&#29486;&#30340;&#21487;&#25511;&#29983;&#25104;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#29992;&#20110;&#35843;&#21046;&#36825;&#19971;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#21508;&#31181;&#35843;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#29702;&#35770;&#30740;&#31350;&#21644;&#23450;&#24615;&#26816;&#39564;&#12290;&#36825;&#19968;&#27934;&#23519;&#20351;&#24471;&#22522;&#20110;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#25104;&#20026;&#21487;&#33021;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#23558;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#20197;&#20102;&#35299;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The previous work on controllable text generation is organized using a new schema we provide in this study. Seven components make up the schema, and each one is crucial to the creation process. To accomplish controlled generation for scientific literature, we describe the various modulation strategies utilised to modulate each of the seven components. We also offer a theoretical study and qualitative examination of these methods. This insight makes possible new architectures based on combinations of these components. Future research will compare these methods empirically to learn more about their strengths and utility.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#21508;&#20010;&#39033;&#30446;&#22312;&#20195;&#30721;&#12289;&#25968;&#25454;&#12289;&#27169;&#22411;&#31561;&#26041;&#38754;&#30340;&#24320;&#25918;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#26377;&#35768;&#22810;&#39033;&#30446;&#34429;&#33258;&#31216;&#20026;&#24320;&#28304;&#65292;&#21364;&#23384;&#22312;&#19981;&#30830;&#23450;&#21512;&#27861;&#24615;&#30340;&#26410;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#24456;&#23569;&#26377;&#39033;&#30446;&#20998;&#20139;&#37325;&#35201;&#30340;&#25351;&#20196;&#35843;&#25972;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05532</link><description>&lt;p&gt;
&#25171;&#24320;ChatGPT&#65306;&#36319;&#36394;&#25351;&#20196;&#35843;&#25972;&#25991;&#26412;&#29983;&#25104;&#22120;&#30340;&#24320;&#25918;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;
&lt;/p&gt;
&lt;p&gt;
Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators. (arXiv:2307.05532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05532
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#21508;&#20010;&#39033;&#30446;&#22312;&#20195;&#30721;&#12289;&#25968;&#25454;&#12289;&#27169;&#22411;&#31561;&#26041;&#38754;&#30340;&#24320;&#25918;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#26377;&#35768;&#22810;&#39033;&#30446;&#34429;&#33258;&#31216;&#20026;&#24320;&#28304;&#65292;&#21364;&#23384;&#22312;&#19981;&#30830;&#23450;&#21512;&#27861;&#24615;&#30340;&#26410;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#24456;&#23569;&#26377;&#39033;&#30446;&#20998;&#20139;&#37325;&#35201;&#30340;&#25351;&#20196;&#35843;&#25972;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#24320;&#25918;&#24615;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#20013;&#24320;&#25918;&#31243;&#24230;&#30340;&#31185;&#23398;&#25991;&#26723;&#12290;&#36890;&#36807;&#35780;&#20272;&#39033;&#30446;&#30340;&#20195;&#30721;&#12289;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#26435;&#37325;&#12289;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#12289;&#35768;&#21487;&#12289;&#31185;&#23398;&#25991;&#26723;&#21644;&#35775;&#38382;&#26041;&#24335;&#30340;&#24320;&#25918;&#31243;&#24230;&#65292;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#34429;&#28982;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#33258;&#31216;&#20026;&#8220;&#24320;&#28304;&#8221;&#30340;&#39033;&#30446;&#65292;&#20294;&#35768;&#22810;&#39033;&#30446;&#32487;&#25215;&#20102;&#19981;&#30830;&#23450;&#21512;&#27861;&#24615;&#30340;&#26410;&#35760;&#24405;&#30340;&#25968;&#25454;&#65292;&#24456;&#23569;&#26377;&#39033;&#30446;&#20998;&#20139;&#37325;&#35201;&#30340;&#25351;&#20196;&#35843;&#25972;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of OpenAI's ChatGPT, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (LLM+RLHF). We review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. The main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. We evaluate projects in terms of openness of code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods. We find that while there is a fast-growing list of projects billing themselves as 'open source', many inherit undocumented data of dubious legality, few share the all-important instruction-tunin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#23398;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#29983;&#19982;ChatGPT&#21512;&#20316;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#30340;&#26696;&#20363;&#65292;&#24182;&#23545;&#25552;&#31034;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#38750;&#25216;&#26415;&#29992;&#25143;&#22312;&#20026;ChatGPT&#32534;&#20889;&#36866;&#24403;&#30340;&#25552;&#31034;&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#38656;&#35201;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#21644;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2307.05493</link><description>&lt;p&gt;
&#20013;&#23398;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#29983;&#19982;ChatGPT&#21512;&#20316;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT. (arXiv:2307.05493v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#23398;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#29983;&#19982;ChatGPT&#21512;&#20316;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#30340;&#26696;&#20363;&#65292;&#24182;&#23545;&#25552;&#31034;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#38750;&#25216;&#26415;&#29992;&#25143;&#22312;&#20026;ChatGPT&#32534;&#20889;&#36866;&#24403;&#30340;&#25552;&#31034;&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#38656;&#35201;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#21644;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#26368;&#20808;&#36827;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#23613;&#31649;&#23427;&#26377;&#28508;&#21147;&#25903;&#25345;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#29983;&#30340;&#20889;&#20316;&#65292;&#20294;&#35201;&#26377;&#25928;&#22320;&#19982;&#20043;&#21512;&#20316;&#65292;&#23398;&#29983;&#24517;&#39035;&#23398;&#20250;&#35774;&#35745;&#25552;&#31034;&#65292;&#21363;&#21046;&#20316;&#36866;&#24403;&#30340;&#25351;&#20196;&#65292;&#20197;&#20351;ChatGPT&#20135;&#29983;&#26399;&#26395;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#25216;&#26415;&#29992;&#25143;&#26469;&#35828;&#65292;&#20026;ChatGPT&#32534;&#20889;&#36866;&#24403;&#30340;&#25552;&#31034;&#24182;&#38750;&#26131;&#20107;&#65292;&#20182;&#20204;&#32463;&#21382;&#20102;&#21453;&#22797;&#35797;&#38169;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#23398;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#29983;&#22312;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#26102;&#20351;&#29992;ChatGPT&#30340;&#25552;&#31034;&#20869;&#23481;&#65292;&#24182;&#25506;&#35752;&#20102;&#25552;&#31034;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#30340;&#27169;&#24335;&#12290;&#25968;&#25454;&#26469;&#33258;iPad&#23631;&#24149;&#24405;&#20687;&#65292;&#35760;&#24405;&#20102;&#39318;&#27425;&#20351;&#29992;ChatGPT&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#32842;&#22825;&#26426;&#22120;&#20154;&#23436;&#25104;&#30456;&#21516;&#20889;&#20316;&#20219;&#21153;&#30340;&#20013;&#23398;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#23398;&#29983;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#36890;&#36807;&#22235;&#20010;&#19981;&#21516;&#30340;&#36335;&#24452;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#35797;&#38169;&#36807;&#31243;&#21644;&#25552;&#31034;&#20869;&#23481;&#21644;&#25968;&#37327;&#30340;&#19981;&#21516;&#32452;&#21512;&#12290;&#36825;&#20123;&#26696;&#20363;&#20026;&#25552;&#20379;&#25903;&#25345;&#38750;&#25216;&#26415;&#29992;&#25143;&#26377;&#25928;&#20351;&#29992;ChatGPT&#30340;&#35777;&#25454;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a state-of-the-art (SOTA) chatbot. Although it has potential to support English as a foreign language (EFL) students' writing, to effectively collaborate with it, a student must learn to engineer prompts, that is, the skill of crafting appropriate instructions so that ChatGPT produces desired outputs. However, writing an appropriate prompt for ChatGPT is not straightforward for non-technical users who suffer a trial-and-error process. This paper examines the content of EFL students' ChatGPT prompts when completing a writing task and explores patterns in the quality and quantity of the prompts. The data come from iPad screen recordings of secondary school EFL students who used ChatGPT and other SOTA chatbots for the first time to complete the same writing task. The paper presents a case study of four distinct pathways that illustrate the trial-and-error process and show different combinations of prompt content and quantity. The cases contribute evidence for the need to provid
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;GPT&#21644;&#20154;&#31867;&#23457;&#31295;&#20154;&#29983;&#25104;&#30340;&#35780;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;GPT&#22312;&#21516;&#34892;&#35780;&#23457;&#20013;&#20855;&#26377;&#19968;&#23450;&#30340;&#24110;&#21161;&#24615;&#65292;&#20026;&#35299;&#20915;&#21516;&#34892;&#35780;&#23457;&#36164;&#28304;&#38480;&#21046;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2307.05492</link><description>&lt;p&gt;
GPT4&#23545;&#21516;&#34892;&#35780;&#23457;&#23384;&#22312;&#19968;&#23450;&#24110;&#21161;&#65306;&#19968;&#39033;&#23454;&#39564;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study. (arXiv:2307.05492v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05492
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;GPT&#21644;&#20154;&#31867;&#23457;&#31295;&#20154;&#29983;&#25104;&#30340;&#35780;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;GPT&#22312;&#21516;&#34892;&#35780;&#23457;&#20013;&#20855;&#26377;&#19968;&#23450;&#30340;&#24110;&#21161;&#24615;&#65292;&#20026;&#35299;&#20915;&#21516;&#34892;&#35780;&#23457;&#36164;&#28304;&#38480;&#21046;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#23454;&#39564;&#24615;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;GPT4&#22312;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20551;&#35774;&#26159;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#23457;&#31295;&#20154;&#29983;&#25104;&#30340;&#35780;&#35770;&#36827;&#34892;&#27604;&#36739;&#65292;GPT&#25152;&#29983;&#25104;&#30340;&#35780;&#35770;&#21487;&#20197;&#36798;&#21040;&#30456;&#20284;&#30340;&#26377;&#29992;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#22312;&#19968;&#20010;&#37325;&#35201;&#30340;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#25552;&#20132;&#30340;&#23398;&#26415;&#35770;&#25991;&#30340;&#20154;&#31867;&#23457;&#31295;&#20154;&#21644;GPT&#27169;&#22411;&#29983;&#25104;&#30340;&#35780;&#35770;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#65292;&#34920;&#26126;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26377;&#25928;&#22320;&#20026;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23545;&#25554;&#20837;&#20102;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20851;&#27880;&#35770;&#25991;&#30340;&#21738;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#35299;&#20915;&#21516;&#34892;&#35780;&#23457;&#36164;&#28304;&#38480;&#21046;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;&#35813;&#32467;&#26524;&#36824;&#20026;&#25913;&#36827;&#35780;&#23457;&#36807;&#31243;&#25552;&#20379;&#20102;&#21551;&#31034;&#65292;&#24182;&#20026;&#22312;&#20154;&#31867;&#21453;&#39304;&#36164;&#28304;&#31232;&#32570;&#30340;&#39046;&#22495;&#20013;&#36827;&#19968;&#27493;&#30740;&#31350;&#25193;&#22823;&#30417;&#30563;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this pilot study, we investigate the use of GPT4 to assist in the peer-review process. Our key hypothesis was that GPT-generated reviews could achieve comparable helpfulness to human reviewers. By comparing reviews generated by both human reviewers and GPT models for academic papers submitted to a major machine learning conference, we provide initial evidence that artificial intelligence can contribute effectively to the peer-review process. We also perform robustness experiments with inserted errors to understand which parts of the paper the model tends to focus on. Our findings open new avenues for leveraging machine learning tools to address resource constraints in peer review. The results also shed light on potential enhancements to the review process and lay the groundwork for further research on scaling oversight in a domain where human-feedback is increasingly a scarce resource.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SICCK&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22797;&#26434;&#32452;&#21512;&#30693;&#35782;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#32467;&#26500;&#21644;&#35821;&#20041;&#32452;&#21512;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.05034</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22797;&#26434;&#32452;&#21512;&#30693;&#35782;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference. (arXiv:2307.05034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SICCK&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22797;&#26434;&#32452;&#21512;&#30693;&#35782;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#32467;&#26500;&#21644;&#35821;&#20041;&#32452;&#21512;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Sentences Involving Complex Compositional Knowledge (SICCK)&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#23545;&#36923;&#36753;&#32452;&#25104;&#24615;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;SICK&#25968;&#25454;&#38598;&#20013;&#30340;15&#20010;&#31034;&#20363;&#65292;&#29983;&#25104;&#20102;1,304&#20010;&#21477;&#23376;&#23545;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#30701;&#35821; - &#19982;&#33258;&#28982;&#36923;&#36753;&#20013;&#30340;&#26222;&#36941;&#37327;&#35789;&#12289;&#23384;&#22312;&#37327;&#35789;&#12289;&#21542;&#23450;&#21644;&#20854;&#20182;&#27010;&#24565;&#20462;&#39280;&#31526;&#30456;&#23545;&#24212;&#30340;&#20462;&#39280;&#31526; - &#20462;&#25913;&#20102;&#21407;&#22987;&#25991;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#30701;&#35821;&#20462;&#25913;&#21069;&#25552;&#21644;&#20551;&#35774;&#30340;&#20027;&#35821;&#12289;&#35859;&#35821;&#21644;&#23486;&#35821;&#37096;&#20998;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#33258;&#28982;&#36923;&#36753;&#35268;&#21017;&#20026;&#36825;&#20123;&#20462;&#25913;&#21518;&#30340;&#25991;&#26412;&#26631;&#27880;&#30456;&#24212;&#30340;&#21253;&#21547;&#20851;&#31995;&#26631;&#31614;&#12290;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#24773;&#20917;&#19979;&#23545;&#32467;&#26500;&#21644;&#35821;&#20041;&#32452;&#21512;&#21464;&#21270;&#30340;&#25429;&#25417;&#33021;&#21147;&#36827;&#34892;&#20102;&#21021;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;NLI&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic. We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014). To this end, we modify the original texts using a set of phrases - modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis. Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules. We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios. We found that the performance of NLI models under th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#35789;&#27719;&#20135;&#29983;&#24847;&#20041;&#30340;&#36807;&#31243;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#20799;&#31461;&#35821;&#35328;&#20064;&#24471;&#21644;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#12290;&#20316;&#32773;&#24378;&#35843;&#20102;&#20307;&#39564;&#24863;&#30693;&#12289;&#24773;&#24863;&#21644;&#35748;&#30693;&#22312;&#35821;&#35328;&#20064;&#24471;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#31867;&#20284;&#20799;&#31461;&#29615;&#22659;&#20013;&#35821;&#35328;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.04518</link><description>&lt;p&gt;
&#35770;&#24847;&#20041;&#30340;&#35745;&#31639;&#24314;&#27169;&#65306;&#34701;&#21512;&#24773;&#24863;&#30340;&#20307;&#39564;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
On the Computational Modeling of Meaning: Embodied Cognition Intertwined with Emotion. (arXiv:2307.04518v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#35789;&#27719;&#20135;&#29983;&#24847;&#20041;&#30340;&#36807;&#31243;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#20799;&#31461;&#35821;&#35328;&#20064;&#24471;&#21644;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#12290;&#20316;&#32773;&#24378;&#35843;&#20102;&#20307;&#39564;&#24863;&#30693;&#12289;&#24773;&#24863;&#21644;&#35748;&#30693;&#22312;&#35821;&#35328;&#20064;&#24471;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#31867;&#20284;&#20799;&#31461;&#29615;&#22659;&#20013;&#35821;&#35328;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35760;&#24405;&#20102;&#20316;&#32773;&#25506;&#32034;&#35789;&#27719;&#20135;&#29983;&#24847;&#20041;&#30340;&#36807;&#31243;&#65292;&#37325;&#28857;&#20851;&#27880;&#20799;&#31461;&#35821;&#35328;&#20064;&#24471;&#20197;&#21450;&#23545;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25991;&#20013;&#35299;&#37322;&#20102;&#20799;&#31461;&#35821;&#35328;&#20064;&#24471;&#30340;&#32972;&#26223;&#65292;&#35828;&#26126;&#20102;&#20307;&#39564;&#24863;&#30693;&#21644;&#23454;&#36341;&#22312;&#35748;&#30693;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#24773;&#24863;&#19982;&#35748;&#30693;&#22914;&#20309;&#30456;&#20114;&#20851;&#32852;&#20197;&#21450;&#19982;&#35821;&#35328;&#20064;&#24471;&#36807;&#31243;&#30340;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#38416;&#36848;&#20102;&#20316;&#32773;&#23545;&#22312;&#31867;&#20284;&#20799;&#31461;&#29615;&#22659;&#20013;&#23398;&#20064;&#35821;&#35328;&#30340;&#35821;&#35328;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#21487;&#20316;&#20026;&#26410;&#26469;&#23545;&#35821;&#35328;&#24314;&#27169;&#24037;&#20316;&#30340;&#28508;&#22312;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document chronicles this author's attempt to explore how words come to mean what they do, with a particular focus on child language acquisition and what that means for models of language understanding.\footnote{I say \emph{historical} because I synthesize the ideas based on when I discovered them and how those ideas influenced my later thinking.} I explain the setting for child language learning, how embodiment -- being able to perceive and enact in the world, including knowledge of concrete and abstract concepts -- is crucial, and how emotion and cognition relate to each other and the language learning process. I end with what I think are some of the requirements for a language-learning agent that learns language in a setting similar to that of children. This paper can act as a potential guide for ongoing and future work in modeling language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#35757;&#32451;&#20102;&#19968;&#20010;&#19987;&#38376;&#36866;&#37197;&#20020;&#24202;&#39046;&#22495;&#30340;LLaMA-LoRA&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20854;&#19982;Downstream LLaMA-LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.03042</link><description>&lt;p&gt;
LLaMA&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#35757;&#32451;&#20102;&#19968;&#20010;&#19987;&#38376;&#36866;&#37197;&#20020;&#24202;&#39046;&#22495;&#30340;LLaMA-LoRA&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20854;&#19982;Downstream LLaMA-LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22914;&#20020;&#24202;&#24212;&#29992;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25152;&#26377;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#24040;&#22823;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#36341;&#24615;&#36234;&#26469;&#36234;&#34987;&#35777;&#26126;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#19968;&#20010;&#23567;&#30340;&#38468;&#21152;&#21442;&#25968;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#39046;&#22495;&#36866;&#24212;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;LLaMA-LoRA&#65292;&#36825;&#26159;&#19968;&#20010;&#26500;&#24314;&#22312;&#24320;&#28304;LLaMA&#27169;&#22411;&#19978;&#30340;PEFT&#36866;&#37197;&#22120;&#23618;&#12290;&#20020;&#24202;LLaMA-LoRA&#20351;&#29992;&#20174;MIMIC-IV&#25968;&#25454;&#24211;&#20013;&#33719;&#21462;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#20026;&#20020;&#24202;&#39046;&#22495;&#35774;&#35745;&#30340;&#19987;&#29992;&#36866;&#37197;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#23558;&#20020;&#24202;LLaMA-LoRA&#19982;Downstream LLaMA-LoRA&#36827;&#34892;&#34701;&#21512;&#65292;&#21518;&#32773;&#26159;&#21478;&#19968;&#20010;&#19987;&#20026;&#19979;&#28216;&#20219;&#21153;&#35774;&#35745;&#30340;PEFT&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting pretrained language models to novel domains, such as clinical applications, traditionally involves retraining their entire set of parameters. However, this approach is increasingly proven to be impractical owing to the substantial computational requirements associated with training such large language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a viable solution by selectively fine-tuning a small subset of additional parameters, significantly reducing the computational requirements for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is trained using clinical notes obtained from the MIMIC-IV database, thereby creating a specialised adapter designed for the clinical domain. Additionally, we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks. W
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#19981;&#20351;&#29992;&#35270;&#39057;&#21644;&#26631;&#27880;&#65292;&#32780;&#26159;&#22312;&#27979;&#35797;&#26102;&#20165;&#20248;&#21270;&#36755;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#30340;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#25991;&#26412;&#21644;&#26102;&#21051;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#35270;&#39057;&#20013;&#20934;&#30830;&#22320;&#23450;&#20301;&#21644;&#25551;&#36848;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2307.02682</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#25991;&#26412;&#21644;&#26102;&#21051;&#23454;&#29616;&#38646;&#26679;&#26412;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment. (arXiv:2307.02682v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02682
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#19981;&#20351;&#29992;&#35270;&#39057;&#21644;&#26631;&#27880;&#65292;&#32780;&#26159;&#22312;&#27979;&#35797;&#26102;&#20165;&#20248;&#21270;&#36755;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#30340;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#25991;&#26412;&#21644;&#26102;&#21051;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#35270;&#39057;&#20013;&#20934;&#30830;&#22320;&#23450;&#20301;&#21644;&#25551;&#36848;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26159;&#19968;&#39033;&#23558;&#26377;&#24847;&#20041;&#30340;&#26102;&#21051;&#23450;&#20301;&#21644;&#30456;&#20851;&#23383;&#24149;&#29983;&#25104;&#24212;&#29992;&#20110;&#35270;&#39057;&#20013;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#26114;&#36149;&#30340;&#24102;&#26377;&#26631;&#27880;&#30340;&#35270;&#39057;&#29255;&#27573;&#21644;&#25991;&#26412;&#30340;&#35821;&#26009;&#24211;&#12290;&#20026;&#20102;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;ZeroTA&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#19981;&#38656;&#35201;&#20219;&#20309;&#35270;&#39057;&#25110;&#26631;&#27880;&#65292;&#32780;&#26159;&#36890;&#36807;&#20165;&#22312;&#36755;&#20837;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#27979;&#35797;&#26102;&#23450;&#20301;&#21644;&#25551;&#36848;&#27599;&#20010;&#36755;&#20837;&#35270;&#39057;&#20013;&#30340;&#20107;&#20214;&#12290;&#36825;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#34920;&#31034;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#27573;&#30340;&#36719;&#26102;&#21051;&#25513;&#30721;&#65292;&#24182;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21069;&#32512;&#21442;&#25968;&#36827;&#34892;&#32852;&#21512;&#20248;&#21270;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#32852;&#21512;&#20248;&#21270;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#25991;&#26412;&#19982;&#35270;&#39057;&#20013;&#30340;&#26576;&#20010;&#26102;&#21051;&#20043;&#38388;&#30340;&#21305;&#37197;&#20998;&#25968;&#65292;&#23558;&#22266;&#23450;&#30340;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;&#21363;GPT-2&#65289;&#19982;&#22266;&#23450;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#27604;&#27169;&#22411;&#65288;&#21363;CLIP&#65289;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#23545;&#19968;&#30340;&#26102;&#38388;IoU&#25439;&#22833;&#65292;&#20351;&#19968;&#32452;&#36719;&#26102;&#21051;&#25513;&#30721;&#20043;&#38388;&#36827;&#34892;&#27604;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense video captioning, a task of localizing meaningful moments and generating relevant captions for videos, often requires a large, expensive corpus of annotated video segments paired with text. In an effort to minimize the annotation cost, we propose ZeroTA, a novel method for dense video captioning in a zero-shot manner. Our method does not require any videos or annotations for training; instead, it localizes and describes events within each input video at test time by optimizing solely on the input. This is accomplished by introducing a soft moment mask that represents a temporal segment in the video and jointly optimizing it with the prefix parameters of a language model. This joint optimization aligns a frozen language generation model (i.e., GPT-2) with a frozen vision-language contrastive model (i.e., CLIP) by maximizing the matching score between the generated text and a moment within the video. We also introduce a pairwise temporal IoU loss to let a set of soft moment masks c
&lt;/p&gt;</description></item><item><title>PatternGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#23454;&#29616;&#27169;&#24335;&#20849;&#20139;&#65292;&#26368;&#32456;&#36890;&#36807;&#25628;&#32034;&#39640;&#36136;&#37327;&#27169;&#24335;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.00470</link><description>&lt;p&gt;
PatternGPT: &#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation. (arXiv:2307.00470v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00470
&lt;/p&gt;
&lt;p&gt;
PatternGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#23454;&#29616;&#27169;&#24335;&#20849;&#20139;&#65292;&#26368;&#32456;&#36890;&#36807;&#25628;&#32034;&#39640;&#36136;&#37327;&#27169;&#24335;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#29983;&#25104;&#27969;&#30021;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#38190;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PatternGPT&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#20016;&#23500;&#22810;&#26679;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#20511;&#37492;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#23454;&#29616;&#20849;&#20139;&#20197;&#33719;&#21462;&#26356;&#22810;&#26679;&#30340;&#27169;&#24335;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#29992;&#21028;&#26029;&#26631;&#20934;&#21644;&#20248;&#21270;&#31639;&#27861;&#25628;&#32034;&#39640;&#36136;&#37327;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#25628;&#32034;&#21040;&#30340;&#27169;&#24335;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMS) have shown excellent text generation capabilities,capable of generating fluent responses for many downstream tasks. However,applying large language models to real-world critical tasks remains challenging due to their susceptibility to hallucinations and inability to directly use external knowledge. To address the above challenges,this paper proposes PatternGPT, a pattern-driven text generation framework for large language models. First,the framework utilizes the extraction capabilities of large language models to generate rich and diverse patterns and later draws on the idea of federated learning. Using multiple agents to achieve sharing to obtain more diverse patterns. Finally, it searches for high-quality patterns using judgment criteria and optimization algorithms and uses the searched patterns to guide the model for generation. This framework has the advantages of generating diversified patterns, protecting data privacy,combining external knowledge, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#23450;&#37327;&#25991;&#26412;&#21644;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#26368;&#22823;&#30340;&#40657;&#27934;incels&#35770;&#22363;&#22914;&#20309;&#35752;&#35770;&#36523;&#20221;&#32676;&#20307;&#12290;&#30740;&#31350;&#21457;&#29616;&#35813;&#31038;&#21306;&#20135;&#29983;&#20102;&#35768;&#22810;&#26032;&#30340;&#36523;&#20221;&#26415;&#35821;&#65292;&#23384;&#22312;&#29289;&#36136;&#20027;&#20041;&#30340;&#24847;&#35782;&#24418;&#24577;&#12290;&#23545;&#27492;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#33258;&#21160;&#21270; misogynist hate speech &#26816;&#27979;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.15745</link><description>&lt;p&gt;
&#12298;&#22312;&#19968;&#20010;&#23545;&#22899;&#24615;&#21388;&#24694;&#30340;incels&#35770;&#22363;&#20013;&#30340;&#36523;&#20221;&#24314;&#26500;&#12299;
&lt;/p&gt;
&lt;p&gt;
Identity Construction in a Misogynist Incels Forum. (arXiv:2306.15745v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23450;&#37327;&#25991;&#26412;&#21644;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#26368;&#22823;&#30340;&#40657;&#27934;incels&#35770;&#22363;&#22914;&#20309;&#35752;&#35770;&#36523;&#20221;&#32676;&#20307;&#12290;&#30740;&#31350;&#21457;&#29616;&#35813;&#31038;&#21306;&#20135;&#29983;&#20102;&#35768;&#22810;&#26032;&#30340;&#36523;&#20221;&#26415;&#35821;&#65292;&#23384;&#22312;&#29289;&#36136;&#20027;&#20041;&#30340;&#24847;&#35782;&#24418;&#24577;&#12290;&#23545;&#27492;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#33258;&#21160;&#21270; misogynist hate speech &#26816;&#27979;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23450;&#37327;&#25991;&#26412;&#21644;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;incels.is&#65292;&#21363;&#26368;&#22823;&#30340;&#40657;&#27934;incels&#35770;&#22363;&#22914;&#20309;&#35752;&#35770;&#36523;&#20221;&#32676;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#31038;&#21306;&#20135;&#29983;&#20102;&#35768;&#22810;&#26032;&#30340;&#36523;&#20221;&#26415;&#35821;&#65292;&#23613;&#31649;&#22899;&#24615;&#30340;&#26415;&#35821;&#26368;&#24120;&#35265;&#65292;&#20294;&#20854;&#20182;&#23569;&#25968;&#32676;&#20307;&#30340;&#25552;&#21450;&#20063;&#22312;&#22686;&#21152;&#12290;&#23545;&#36523;&#20221;&#32676;&#20307;&#30340;&#20851;&#32852;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20010;&#31038;&#21306;&#23384;&#22312;&#30528;&#29289;&#36136;&#20027;&#20041;&#30340;&#24847;&#35782;&#24418;&#24577;&#65292;&#20854;&#20013;&#36523;&#20307;&#22806;&#35980;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#20915;&#23450;&#20102;&#20154;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#33258;&#21160;&#21270; misogynist hate speech &#26816;&#27979;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online communities of involuntary celibates (incels) are a prominent source of misogynist hate speech. In this paper, we use quantitative text and network analysis approaches to examine how identity groups are discussed on incels.is, the largest black-pilled incels forum. We find that this community produces a wide range of novel identity terms and, while terms for women are most common, mentions of other minoritized identities are increasing. An analysis of the associations made with identity groups suggests an essentialist ideology where physical appearance, as well as gender and racial hierarchies, determine human value. We discuss implications for research into automated misogynist hate speech detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#25351;&#23548;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#26032;&#39062;&#20294;&#21512;&#36866;&#30340;&#25351;&#23548;&#25514;&#36766;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11270</link><description>&lt;p&gt;
&#35780;&#20272;&#25351;&#23548;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Zero-shot Robustness of Instruction-tuned Language Models. (arXiv:2306.11270v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#25351;&#23548;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#26032;&#39062;&#20294;&#21512;&#36866;&#30340;&#25351;&#23548;&#25514;&#36766;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#24494;&#35843;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#19978;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#25913;&#21892;&#20013;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#29305;&#21035;&#30340;&#20248;&#21183;&#65292;&#26377;&#26102;&#29978;&#33267;&#19982;&#26356;&#22823;&#30340;&#27169;&#22411;&#21464;&#31181;&#30456;&#31454;&#20105;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#25351;&#23548;&#24494;&#35843;&#30340;&#27169;&#22411;&#23545;&#25351;&#23548;&#30340;&#29305;&#23450;&#25514;&#36766;&#26377;&#22810;&#25935;&#24863;&#65292;&#65288;2&#65289;&#22914;&#20309;&#20351;&#23427;&#20204;&#26356;&#33021;&#25269;&#25239;&#33258;&#28982;&#35821;&#35328;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#22238;&#31572;&#21069;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#30001;NLP&#20174;&#19994;&#32773;&#25163;&#24037;&#32534;&#20889;&#30340;319&#20010;&#25351;&#23548;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;80&#22810;&#20010;&#29420;&#29305;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#25351;&#23548;&#19982;&#25351;&#23548;&#24494;&#35843;&#26399;&#38388;&#35266;&#23519;&#21040;&#30340;&#25351;&#23548;&#25514;&#36766;&#20043;&#38388;&#30340;&#26041;&#24046;&#21644;&#24179;&#22343;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#26032;&#39062;&#65288;&#26410;&#35266;&#23519;&#21040;&#30340;&#65289;&#20294;&#21512;&#36866;&#30340;&#25351;&#23548;&#25514;&#36766;&#20250;&#19968;&#33268;&#22320;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#29978;&#33267;&#20250;&#22823;&#24133;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA) &#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#21160;&#24577;&#22320;&#31232;&#30095;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.11197</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22359;&#28608;&#27963;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA) &#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#21160;&#24577;&#22320;&#31232;&#30095;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411; (SSM) &#22312;&#21508;&#31181;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#24490;&#29615;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#32508;&#21512;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#20173;&#28982;&#20248;&#20110;SSM&#12290;&#21516;&#26102;&#20351;&#29992;SSM&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#28151;&#21512;&#27169;&#22411;&#36890;&#24120;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#27169;&#22359;&#38745;&#24577;&#19988;&#22343;&#21248;&#22320;&#24212;&#29992;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#20803;&#32032;&#65292;&#23548;&#33268;&#20102;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#27425;&#20248;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA)&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#31232;&#30095;&#22320;&#21160;&#24577;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#12290;&#36890;&#36807;&#20801;&#35768;&#27599;&#20010;&#20803;&#32032;&#36339;&#36807;&#38750;&#28608;&#27963;&#30340;&#23376;&#27169;&#22359;&#65292;SMA&#21487;&#20197;&#22312;&#24207;&#21015;&#24314;&#27169;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#38477;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#20316;&#20026;SMA&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24503;&#35821;&#21644;&#32599;&#26364;&#20160;&#35821;&#30340;&#24179;&#34892;&#21477;&#23376;&#20013;&#20351;&#29992;mBERT&#21644;XLM-R&#30340;&#35789;&#23884;&#20837;&#65292;&#32467;&#21512;&#30456;&#20284;&#24615;&#23545;&#40784;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;mBERT&#23545;&#32599;&#26364;&#20160;&#35821;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;mBERT&#30340;&#35789;&#23884;&#20837;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#32599;&#26364;&#20160;&#35821;&#36827;&#34892;&#35789;&#35821;&#23545;&#40784;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#21644;&#36866;&#29992;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.08702</link><description>&lt;p&gt;
mBERT&#26159;&#21542;&#29702;&#35299;&#32599;&#26364;&#20160;&#35821;&#65311;&#20351;&#29992;&#35789;&#35821;&#23545;&#40784;&#35780;&#20272;&#35789;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Does mBERT understand Romansh? Evaluating word embeddings using word alignment. (arXiv:2306.08702v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24503;&#35821;&#21644;&#32599;&#26364;&#20160;&#35821;&#30340;&#24179;&#34892;&#21477;&#23376;&#20013;&#20351;&#29992;mBERT&#21644;XLM-R&#30340;&#35789;&#23884;&#20837;&#65292;&#32467;&#21512;&#30456;&#20284;&#24615;&#23545;&#40784;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;mBERT&#23545;&#32599;&#26364;&#20160;&#35821;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;mBERT&#30340;&#35789;&#23884;&#20837;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#32599;&#26364;&#20160;&#35821;&#36827;&#34892;&#35789;&#35821;&#23545;&#40784;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#21644;&#36866;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24503;&#35821;&#21644;&#32599;&#26364;&#20160;&#35821;&#20043;&#38388;&#30340;&#24179;&#34892;&#21477;&#23376;&#20013;&#65292;&#20351;&#29992;mBERT&#21644;XLM-R&#30340;&#35789;&#23884;&#20837;&#65292;&#32467;&#21512;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#35789;&#35821;&#23545;&#40784;&#27169;&#22411;&#65288;SimAlign&#21644;awesome-align&#65289;&#36827;&#34892;&#27979;&#35797;&#12290;&#30001;&#20110;&#32599;&#26364;&#20160;&#35821;&#26159;&#19968;&#31181;&#26410;&#30693;&#35821;&#35328;&#65292;&#25105;&#20204;&#22788;&#20110;&#38646;&#26679;&#26412;&#30340;&#24773;&#20917;&#12290;&#20351;&#29992;mBERT&#30340;&#35789;&#23884;&#20837;&#65292;&#20004;&#20010;&#27169;&#22411;&#30340;&#23545;&#40784;&#38169;&#35823;&#29575;&#20026;0.22&#65292;&#20248;&#20110;&#32479;&#35745;&#27169;&#22411;fast_align&#65292;&#24182;&#19988;&#19982;&#23545;&#20110;&#24050;&#30693;&#35821;&#35328;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#35789;&#35821;&#23545;&#40784;&#30456;&#24403;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#35299;&#37322;&#20026;mBERT&#21253;&#21547;&#20102;&#23545;&#32599;&#26364;&#20160;&#35821;&#26377;&#24847;&#20041;&#21644;&#36866;&#29992;&#30340;&#20449;&#24687;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We test similarity-based word alignment models (SimAlign and awesome-align) in combination with word embeddings from mBERT and XLM-R on parallel sentences in German and Romansh. Since Romansh is an unseen language, we are dealing with a zero-shot setting. Using embeddings from mBERT, both models reach an alignment error rate of 0.22, which outperforms fast_align, a statistical model, and is on par with similarity-based word alignment for seen languages. We interpret these results as evidence that mBERT contains information that can be meaningful and applicable to Romansh.  To evaluate performance, we also present a new trilingual corpus, which we call the DERMIT (DE-RM-IT) corpus, containing press releases made by the Canton of Grisons in German, Romansh and Italian in the past 25 years. The corpus contains 4 547 parallel documents and approximately 100 000 sentence pairs in each language combination. We additionally present a gold standard for German-Romansh word alignment. The data i
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05685</link><description>&lt;p&gt;
&#29992;MT-Bench&#21644;Chatbot Arena&#35780;&#20272;&#20197;LLM&#20026;&#22522;&#30784;&#30340;&#32842;&#22825;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05685
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32842;&#22825;&#21161;&#25163;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#32780;&#29616;&#26377;&#30340;&#22522;&#20934;&#26080;&#27861;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#22312;&#26356;&#21152;&#24320;&#25918;&#30340;&#38382;&#39064;&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#30340;&#20351;&#29992;&#21644;&#23616;&#38480;&#24615;&#65292;&#22914;&#20301;&#32622;&#21644;&#20887;&#20313;&#20559;&#35265;&#20197;&#21450;&#26377;&#38480;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#26469;&#36801;&#31227;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#65288;&#19968;&#20010;&#22810;&#36718;&#38382;&#31572;&#38598;&#21644;&#19968;&#20010;&#20247;&#21253;&#31454;&#25216;&#24179;&#21488;&#65289;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#24378;&#22823;LLM&#35780;&#21028;&#21592;&#21487;&#20197;&#24456;&#22909;&#22320;&#21305;&#37197;&#21463;&#25511;&#21644;&#20247;&#21253;&#20154;&#31867;&#20559;&#22909;&#65292;&#36798;&#21040;&#20102;80&#65285;&#20197;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#19982;&#20154;&#31867;&#19968;&#33268;&#24615;&#27700;&#24179;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#36924;&#36817;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#24335;&#65292;&#32780;&#36825;&#20123;&#20559;&#22909;&#26159;&#38750;&#24120;&#26114;&#36149;&#33719;&#21462;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, such as position and verbosity biases and limited reasoning ability, and propose solutions to migrate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20026;IWSLT 2023&#22810;&#35821;&#35328;&#32763;&#35793;&#36129;&#29486;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#20854;&#37325;&#28857;&#22312;&#20110;&#32763;&#35793;&#31185;&#23398;&#20250;&#35758;&#28436;&#35762;&#12290;&#29992;&#8220;&#26816;&#32034;&#24335;&#26041;&#27861;&#8221;&#65288;kNN-MT&#65289;&#36827;&#34892;&#26377;&#25928;&#30340;&#36866;&#24212;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#36866;&#37197;&#22120;&#36731;&#26494;&#38598;&#25104;&#26469;&#33258;&#25968;&#25454;&#22686;&#24378;&#30340;&#22686;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#32423;&#32852;&#31995;&#32479;&#26356;&#23481;&#26131;&#36866;&#24212;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.05320</link><description>&lt;p&gt;
KIT&#30340;&#22810;&#35821;&#35328;&#28436;&#35762;&#32763;&#35793;&#31995;&#32479;&#22312;IWSLT 2023&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
KIT's Multilingual Speech Translation System for IWSLT 2023. (arXiv:2306.05320v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20026;IWSLT 2023&#22810;&#35821;&#35328;&#32763;&#35793;&#36129;&#29486;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#20854;&#37325;&#28857;&#22312;&#20110;&#32763;&#35793;&#31185;&#23398;&#20250;&#35758;&#28436;&#35762;&#12290;&#29992;&#8220;&#26816;&#32034;&#24335;&#26041;&#27861;&#8221;&#65288;kNN-MT&#65289;&#36827;&#34892;&#26377;&#25928;&#30340;&#36866;&#24212;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#36866;&#37197;&#22120;&#36731;&#26494;&#38598;&#25104;&#26469;&#33258;&#25968;&#25454;&#22686;&#24378;&#30340;&#22686;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#32423;&#32852;&#31995;&#32479;&#26356;&#23481;&#26131;&#36866;&#24212;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#30340;&#35821;&#38899;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#37117;&#38024;&#23545;&#39640;&#21697;&#36136;&#24405;&#38899;&#26465;&#20214;&#19979;&#30340;&#20197;&#33521;&#35821;&#20026;&#27597;&#35821;&#30340;&#35821;&#38899;&#65292;&#36825;&#36890;&#24120;&#19982;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#20013;&#30340;&#26465;&#20214;&#19981;&#31526;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;IWSLT 2023&#22810;&#35821;&#35328;&#36712;&#36947;&#35774;&#35745;&#30340;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#37325;&#28857;&#32763;&#35793;&#31185;&#23398;&#20250;&#35758;&#28436;&#35762;&#12290;&#27979;&#35797;&#26465;&#20214;&#21253;&#25324;&#21475;&#38899;&#37325;&#30340;&#36755;&#20837;&#35821;&#38899;&#21644;&#26415;&#35821;&#23494;&#38598;&#30340;&#20869;&#23481;&#65292;&#24182;&#19988;&#38656;&#35201;&#32763;&#35793;&#25104;10&#31181;&#36164;&#28304;&#25968;&#37327;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;&#22312;&#27809;&#26377;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26816;&#32034;&#24335;&#26041;&#27861;&#65288;kNN-MT&#65289;&#36827;&#34892;&#26377;&#25928;&#30340;&#36866;&#24212;&#65288;&#35821;&#38899;&#32763;&#35793;+0.8 BLEU&#65289;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36866;&#37197;&#22120;&#36731;&#26494;&#38598;&#25104;&#26469;&#33258;&#25968;&#25454;&#22686;&#24378;&#30340;&#22686;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20854;&#19982;&#37325;&#26032;&#35757;&#32451;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#32423;&#32852;&#31995;&#32479;&#26356;&#23481;&#26131;&#36866;&#24212;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#30001;&#22810;&#20010;&#29420;&#31435;&#27169;&#22359;&#32452;&#25104;&#30340;&#12290;&#25105;&#20204;&#30340;&#32423;&#32852;&#35821;&#38899;&#31995;&#32479;&#36828;&#36828;&#20248;&#20110;&#20854;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system substantially outperforms its end-to-end 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.18703</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#20102;&#39640;&#24230;&#23454;&#29992;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#22522;&#30784;&#12290;LLMs &#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20419;&#20351;&#20154;&#20204;&#23558;&#20854;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21161;&#25163;&#29978;&#33267;&#26367;&#20195;&#29305;&#23450;&#39046;&#22495;&#30340;&#19987;&#23478;&#21644;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23558;LLMs&#30452;&#25509;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#20250;&#36935;&#21040;&#35768;&#22810;&#22256;&#38590;&#65292;&#21253;&#25324;&#39046;&#22495;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#39046;&#22495;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12289;&#39046;&#22495;&#30446;&#26631;&#30340;&#29420;&#29305;&#24615;&#20197;&#21450;&#32422;&#26463;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#31181;&#24046;&#36317;&#65292;&#26368;&#36817;&#20960;&#24180;&#36827;&#34892;&#20102;&#24613;&#21095;&#22686;&#21152;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#33268;&#21147;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#28982;&#32780;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#24635;&#32467;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20559;&#32622;&#25439;&#22833;&#20197;&#24110;&#21161;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#31181;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;WER&#38477;&#20302;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#30456;&#23545;WER&#25552;&#39640;&#20102;12.1&#65285;&#65292;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;40.5&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.12493</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#30340;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20559;&#32622;&#25439;&#22833;&#20197;&#24110;&#21161;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#31181;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;WER&#38477;&#20302;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#30456;&#23545;WER&#25552;&#39640;&#20102;12.1&#65285;&#65292;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;40.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#20449;&#24687;&#22312;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23558;&#20854;&#34701;&#20837;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#32570;&#20047;&#20559;&#32622;&#20219;&#21153;&#30340;&#26174;&#24335;&#30417;&#30563;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#19978;&#19979;&#25991;&#23884;&#20837;&#39044;&#27979;&#21457;&#38899;&#20013;&#30340;&#19978;&#19979;&#25991;&#30701;&#35821;&#65292;&#24182;&#35745;&#31639;&#20559;&#32622;&#25439;&#22833;&#20197;&#24110;&#21161;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;(WER)&#38477;&#20302;&#12290;&#23545;LibriSpeech&#35821;&#26009;&#24211;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22522;&#32447;&#27169;&#22411;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;WER&#25552;&#39640;&#20102;12.1&#65285;&#65292;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;40.5&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24212;&#29992;&#19978;&#19979;&#25991;&#30701;&#35821;&#36807;&#28388;&#31574;&#30053;&#65292;&#25105;&#20204;&#36824;&#26377;&#25928;&#28040;&#38500;&#20102;&#20351;&#29992;&#26356;&#22823;&#30340;&#20559;&#32622;&#21015;&#34920;&#26102;&#30340;WER&#38477;&#32423;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual information plays a crucial role in speech recognition technologies and incorporating it into the end-to-end speech recognition models has drawn immense interest recently. However, previous deep bias methods lacked explicit supervision for bias tasks. In this study, we introduce a contextual phrase prediction network for an attention-based deep bias method. This network predicts context phrases in utterances using contextual embeddings and calculates bias loss to assist in the training of the contextualized model. Our method achieved a significant word error rate (WER) reduction across various end-to-end speech recognition models. Experiments on the LibriSpeech corpus show that our proposed model obtains a 12.1% relative WER improvement over the baseline model, and the WER of the context phrases decreases relatively by 40.5%. Moreover, by applying a context phrase filtering strategy, we also effectively eliminate the WER degradation when using a larger biasing list.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;RL4F&#65288;Reinforcement Learning for Feedback&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#20462;&#22797;&#27169;&#22411;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#40657;&#30418;&#25110;&#20165;&#26377;&#26377;&#38480;&#35775;&#38382;&#26435;&#38480;&#30340;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#31354;&#38388;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08844</link><description>&lt;p&gt;
RL4F: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20462;&#22797;&#27169;&#22411;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs. (arXiv:2305.08844v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;RL4F&#65288;Reinforcement Learning for Feedback&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#20462;&#22797;&#27169;&#22411;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#40657;&#30418;&#25110;&#20165;&#26377;&#26377;&#38480;&#35775;&#38382;&#26435;&#38480;&#30340;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#31354;&#38388;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#20250;&#20986;&#38169;&#12290;&#19982;&#20154;&#31867;&#36890;&#36807;&#21453;&#39304;&#23398;&#20064;&#21644;&#25913;&#36827;&#31867;&#20284;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65292;&#20197;&#25351;&#23548;&#23427;&#20204;&#20462;&#22797;&#36755;&#20986;&#12290;&#30001;&#20110;&#20154;&#24037;&#29983;&#25104;&#30340;&#25209;&#35780;&#22312;&#33719;&#21462;&#19978;&#36739;&#20026;&#26114;&#36149;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#23398;&#20064;&#25209;&#35780;&#29983;&#25104;&#22120;&#26469;&#26367;&#20195;&#20154;&#31867;&#25209;&#35780;&#32773;&#65292;&#24182;&#20551;&#35774;&#21487;&#20197;&#35757;&#32451;&#19979;&#28216;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#36866;&#29992;&#20110;&#40657;&#30418;&#25110;&#20165;&#26377;&#26377;&#38480;&#35775;&#38382;&#26435;&#38480;&#30340;&#27169;&#22411;&#65292;&#27604;&#22914;ChatGPT&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#22312;&#22823;&#22411;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#24494;&#35843;&#26082;&#19981;&#20855;&#22791;&#35745;&#31639;&#25928;&#29575;&#20063;&#19981;&#20855;&#22791;&#31354;&#38388;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20250;&#23548;&#33268;&#32593;&#32476;&#30340;&#22810;&#20010;&#21103;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RL4F&#65288;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#20854;&#20013;&#25209;&#35780;&#29983;&#25104;&#22120;&#30340;&#35757;&#32451;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;GPT-3&#30340;&#32456;&#31471;&#20219;&#21153;&#24615;&#33021;&#65292;GPT-3&#26159;&#19968;&#20010;&#22266;&#23450;&#27169;&#22411;&#65292;&#27604;&#21069;&#20195;&#27169;&#22411;&#26356;&#21152;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 time
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.04003</link><description>&lt;p&gt;
ANTONIO:&#38754;&#21521;NLP&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39564;&#35777;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#24120;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;NLP&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#19968;&#38382;&#39064;&#30340;&#25216;&#26415;&#21407;&#22240;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#23558;NLP&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20934;&#22791;&#20026;&#36866;&#21512;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#30340;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20026;&#19968;&#20010;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#35813;&#24211;&#36830;&#25509;&#21040;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;ERAN&#21644;Marabou&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;R-U-A-Robot&#30340;NLP&#25968;&#25454;&#38598;&#23545;&#24037;&#20855;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;&#25552;&#35758;&#20316;&#20026;&#39564;&#35777;&#20855;&#26377;&#27861;&#24459;&#37325;&#35201;&#24615;&#30340;NLP&#24212;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24076;&#26395;&#65292;&#30001;&#20110;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
&lt;/p&gt;</description></item><item><title>DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2304.14108</link><description>&lt;p&gt;
DataComp&#65306;&#23547;&#25214;&#19979;&#19968;&#20195;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14108
&lt;/p&gt;
&lt;p&gt;
DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22312;&#36817;&#26399;&#30340;&#31361;&#30772;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#27604;&#22914;CLIP&#12289;Stable Diffusion&#21644;GPT-4&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25968;&#25454;&#38598;&#24456;&#23569;&#24471;&#21040;&#19982;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#31639;&#27861;&#21516;&#31561;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataComp&#65292;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#35757;&#32451;&#20195;&#30721;&#26159;&#22266;&#23450;&#30340;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Common Crawl&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#20854;&#20013;&#21253;&#21547;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#12290;&#21442;&#21152;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#65292;&#24182;&#36890;&#36807;&#36816;&#34892;&#25105;&#20204;&#26631;&#20934;&#21270;&#30340;CLIP&#35757;&#32451;&#20195;&#30721;&#24182;&#22312;38&#20010;&#19979;&#28216;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#20182;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#22810;&#20010;&#35268;&#27169;&#65292;&#22235;&#20010;&#20505;&#36873;&#27744;&#22823;&#23567;&#21644;&#30456;&#24212;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#28085;&#30422;&#20102;&#20174;12.8M&#21040;12.8B&#20010;&#26679;&#26412;&#12290;&#36825;&#31181;&#22810;&#35268;&#27169;&#35774;&#35745;&#26377;&#21161;&#20110;&#30740;&#31350;&#35268;&#27169;&#36235;&#21183;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#36873;&#25321;&#20313;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.02819</link><description>&lt;p&gt;
GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#25512;&#24191;&#24102;&#26469;&#20102;&#25968;&#23383;&#36890;&#20449;&#26041;&#38754;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#26469;&#21306;&#20998;AI&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#33521;&#35821;&#27597;&#35821;&#21644;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#30340;&#20889;&#20316;&#26679;&#26412;&#35780;&#20272;&#20102;&#20960;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;GPT&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#25345;&#32493;&#23558;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20889;&#20316;&#26679;&#26412;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#32780;&#21407;&#29983;&#20889;&#20316;&#26679;&#26412;&#21017;&#33021;&#22815;&#34987;&#20934;&#30830;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#26080;&#24847;&#20013;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21628;&#21505;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23646;&#24615;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#25805;&#32437;&#23548;&#33268;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#25552;&#20379;&#20102;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#20984;&#26174;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2302.09582</link><description>&lt;p&gt;
&#35821;&#35328;&#29305;&#23450;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#23545;&#24773;&#32490;&#25512;&#26029;&#30340;&#22240;&#26524;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23646;&#24615;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#25805;&#32437;&#23548;&#33268;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#25552;&#20379;&#20102;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#20984;&#26174;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#32490;&#31185;&#23398;&#20013;&#65292;&#22914;&#20309;&#29702;&#35299;&#35821;&#35328;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#20173;&#28982;&#26159;&#19968;&#20010;&#20105;&#35758;&#30340;&#35805;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#34893;&#29983;&#30340;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#34920;&#31034;&#65292;&#35843;&#26597;&#20102;&#35821;&#35328;&#26159;&#21542;&#20250;&#22240;&#26524;&#25903;&#25345;&#24773;&#32490;&#25512;&#26029;&#12290;&#20351;&#29992;&#25552;&#31034;&#25216;&#26415;&#65292;&#21457;&#29616;&#20102;14&#20010;&#24773;&#32490;&#27010;&#24565;&#30340;&#23646;&#24615;&#30001;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#32676;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#25805;&#32437;&#36825;&#20123;&#23646;&#24615;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#65292;&#19982;&#38543;&#26426;&#25805;&#32437;&#30456;&#27604;&#65292;&#22823;&#22810;&#25968;&#24773;&#32490;&#25512;&#26029;&#20219;&#21153;&#30340;&#34920;&#29616;&#20986;&#29616;&#20102;&#19979;&#38477;&#12290;&#23646;&#24615;&#29305;&#23450;&#30340;&#34920;&#29616;&#19979;&#38477;&#19982;&#20154;&#31867;&#24515;&#29702;&#31354;&#38388;&#20013;&#19981;&#21516;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#25903;&#25345;&#22522;&#20110;&#35821;&#35328;&#30340;&#24773;&#32490;&#25512;&#26029;&#26426;&#21046;&#30340;&#22240;&#26524;&#35777;&#25454;&#65292;&#24182;&#24378;&#35843;&#20102;&#24773;&#32490;&#27010;&#24565;&#30693;&#35782;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23545;&#33678;&#22763;&#27604;&#20122;&#30340;&#20316;&#21697;&#36827;&#34892;&#20102;&#36830;&#32493;&#20998;&#26512;&#65292;&#21457;&#29616;&#20182;&#30340;&#20889;&#20316;&#39118;&#26684;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#21477;&#23376;&#38271;&#24230;&#12289;&#24418;&#23481;&#35789;&#21644;&#21103;&#35789;&#30340;&#39057;&#29575;&#20197;&#21450;&#25991;&#26412;&#20013;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#37096;&#20998;&#25103;&#21095;&#30340;&#39118;&#26684;&#29305;&#24449;&#26356;&#31867;&#20284;&#20110;&#20854;&#20889;&#20316;&#26102;&#38388;&#20043;&#21069;&#25110;&#20043;&#21518;&#30340;&#20316;&#21697;&#12290;</title><link>http://arxiv.org/abs/2301.06024</link><description>&lt;p&gt;
&#23545;&#33678;&#22763;&#27604;&#20122;&#25103;&#21095;&#30340;&#36830;&#32493;&#20998;&#26512;&#30340;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A data science and machine learning approach to continuous analysis of Shakespeare's plays. (arXiv:2301.06024v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06024
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23545;&#33678;&#22763;&#27604;&#20122;&#30340;&#20316;&#21697;&#36827;&#34892;&#20102;&#36830;&#32493;&#20998;&#26512;&#65292;&#21457;&#29616;&#20182;&#30340;&#20889;&#20316;&#39118;&#26684;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21457;&#29983;&#20102;&#21464;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#21477;&#23376;&#38271;&#24230;&#12289;&#24418;&#23481;&#35789;&#21644;&#21103;&#35789;&#30340;&#39057;&#29575;&#20197;&#21450;&#25991;&#26412;&#20013;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#37096;&#20998;&#25103;&#21095;&#30340;&#39118;&#26684;&#29305;&#24449;&#26356;&#31867;&#20284;&#20110;&#20854;&#20889;&#20316;&#26102;&#38388;&#20043;&#21069;&#25110;&#20043;&#21518;&#30340;&#20316;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#37327;&#21270;&#30340;&#25991;&#26412;&#20998;&#26512;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#20026;&#25991;&#23398;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#23558;&#32508;&#21512;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#24212;&#29992;&#20110;&#23041;&#24265;&#183;&#33678;&#22763;&#27604;&#20122;&#30340;&#20316;&#21697;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#33678;&#22763;&#27604;&#20122;&#30340;&#20889;&#20316;&#39118;&#26684;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21457;&#29983;&#20102;&#26126;&#26174;&#30340;&#21464;&#21270;&#65292;&#20854;&#20013;&#26368;&#26174;&#33879;&#30340;&#21464;&#21270;&#21253;&#25324;&#21477;&#23376;&#38271;&#24230;&#12289;&#24418;&#23481;&#35789;&#21644;&#21103;&#35789;&#30340;&#39057;&#29575;&#20197;&#21450;&#25991;&#26412;&#20013;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#23545;&#25103;&#21095;&#24180;&#20221;&#30340;&#39118;&#26684;&#39044;&#27979;&#34920;&#26126;&#65292;&#23454;&#38469;&#24180;&#20221;&#21644;&#39044;&#27979;&#24180;&#20221;&#20043;&#38388;&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#20026;0.71&#65292;&#34920;&#26126;&#33678;&#22763;&#27604;&#20122;&#30340;&#20889;&#20316;&#39118;&#26684;&#22312;&#25968;&#37327;&#21270;&#27979;&#37327;&#26041;&#38754;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#26576;&#20123;&#25103;&#21095;&#30340;&#39118;&#26684;&#29305;&#24449;&#26356;&#31867;&#20284;&#20110;&#23427;&#20204;&#30340;&#20889;&#20316;&#24180;&#20221;&#20043;&#21069;&#25110;&#20043;&#21518;&#30340;&#20316;&#21697;&#12290;&#20363;&#22914;&#65292;"&#32599;&#23494;&#27431;&#19982;&#26417;&#20029;&#21494;"&#30340;&#26085;&#26399;&#20026;1596&#24180;&#65292;&#20294;&#22312;&#39118;&#26684;&#29305;&#24449;&#19978;&#26356;&#31867;&#20284;&#20110;1600&#24180;&#20043;&#21518;&#33678;&#22763;&#27604;&#20122;&#30340;&#20854;&#20182;&#20316;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of quantitative text analysis methods has provided new ways of analyzing literature in a manner that was not available in the pre-information era. Here we apply comprehensive machine learning analysis to the work of William Shakespeare. The analysis shows clear changes in the style of writing over time, with the most significant changes in the sentence length, frequency of adjectives and adverbs, and the sentiments expressed in the text. Applying machine learning to make a stylometric prediction of the year of the play shows a Pearson correlation of 0.71 between the actual and predicted year, indicating that Shakespeare's writing style as reflected by the quantitative measurements changed over time. Additionally, it shows that the stylometrics of some of the plays is more similar to plays written either before or after the year they were written. For instance, Romeo and Juliet is dated 1596, but is more similar in stylometrics to plays written by Shakespeare after 1600
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#31574;&#30053;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#31614;&#24179;&#28369;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#22312;&#23545;&#25239;&#26679;&#26412;&#19978;&#20986;&#29616;&#30340;&#36807;&#24230;&#33258;&#20449;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2212.10258</link><description>&lt;p&gt;
&#21033;&#29992;&#26631;&#31614;&#24179;&#28369;&#23454;&#29616;&#39046;&#22495;&#20869;&#22806;&#25991;&#26412;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
In and Out-of-Domain Text Adversarial Robustness via Label Smoothing. (arXiv:2212.10258v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#31574;&#30053;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#31614;&#24179;&#28369;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#22312;&#23545;&#25239;&#26679;&#26412;&#19978;&#20986;&#29616;&#30340;&#36807;&#24230;&#33258;&#20449;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21363;&#23545;&#36755;&#20837;&#36827;&#34892;&#32454;&#24494;&#20462;&#25913;&#65288;&#22914;&#21516;&#20041;&#35789;&#26367;&#25442;&#65289;&#20250;&#26497;&#22823;&#22320;&#25913;&#21464;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;&#25991;&#26412;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38450;&#24481;&#25216;&#26415;&#65292;&#24182;&#23558;&#20854;&#35843;&#25972;&#33267;&#31163;&#25955;&#24615;&#36136;&#30340;&#25991;&#26412;&#25968;&#25454;&#19978;&#65292;&#20294;&#26159;&#32508;&#21512;&#24615;&#30340;&#35268;&#21017;&#21270;&#26041;&#27861;&#65292;&#22914;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#24179;&#28369;&#23545;&#20110;&#25991;&#26412;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#30340;&#25928;&#26524;&#36824;&#27809;&#26377;&#34987;&#30740;&#31350;&#36807;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26631;&#31614;&#24179;&#28369;&#31574;&#30053;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#23545;&#22810;&#26679;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#26631;&#31614;&#24179;&#28369;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#22312;&#21508;&#31181;&#27969;&#34892;&#25915;&#20987;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#39044;&#27979;&#21487;&#20449;&#24230;&#19982;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26174;&#31034;&#26631;&#31614;&#24179;&#28369;&#20943;&#23569;&#20102;&#22312;&#23545;&#25239;&#26679;&#26412;&#19978;&#20986;&#29616;&#30340;&#36807;&#24230;&#33258;&#20449;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to the discrete nature of text adversarial attacks, the benefits of general-purpose regularization methods such as label smoothing for language models, have not been studied. In this paper, we study the adversarial robustness provided by various label smoothing strategies in foundational models for diverse NLP tasks in both in-domain and out-of-domain settings. Our experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks. We also analyze the relationship between prediction confidence and robustness, showing that label smoothing reduces over-confident errors on adversarial examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;CoNLL-2003&#19978;&#36229;&#36807;20&#31181;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;NER&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21508;&#24322;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#20960;&#21313;&#24180;&#21069;&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20173;&#28982;&#20445;&#25345;&#30528;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#19982;&#19979;&#28216;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#26102;&#38388;&#19981;&#21305;&#37197;&#26159;&#23548;&#33268;&#24615;&#33021;&#24694;&#21270;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2212.09747</link><description>&lt;p&gt;
CoNLL-2003&#21629;&#21517;&#23454;&#20307;&#26631;&#27880;&#22120;&#22312;2023&#24180;&#20173;&#28982;&#26377;&#25928;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?. (arXiv:2212.09747v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;CoNLL-2003&#19978;&#36229;&#36807;20&#31181;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;NER&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21508;&#24322;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#20960;&#21313;&#24180;&#21069;&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20173;&#28982;&#20445;&#25345;&#30528;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#19982;&#19979;&#28216;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#26102;&#38388;&#19981;&#21305;&#37197;&#26159;&#23548;&#33268;&#24615;&#33021;&#24694;&#21270;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CoNLL-2003&#33521;&#25991;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#25968;&#25454;&#38598;&#24050;&#32463;&#34987;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;NER&#27169;&#22411;&#20960;&#20046;20&#24180;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#19981;&#28165;&#26970;&#22312;&#29616;&#20195;&#25968;&#25454;&#19978;&#24212;&#29992;&#22312;&#27492;20&#24180;&#21069;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#24182;&#32463;&#36807;&#20960;&#21313;&#24180;&#21457;&#23637;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#22914;&#20309;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#36229;&#36807;20&#31181;&#22312;CoNLL-2003&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;NER&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#24322;&#24456;&#22823;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#20351;&#29992;&#20960;&#21313;&#24180;&#21069;&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#22914;RoBERTa&#21644;T5&#31561;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20173;&#28982;&#27809;&#26377;&#24615;&#33021;&#19979;&#38477;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26032;&#25968;&#25454;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#19981;&#33021;&#65292;&#24182;&#35797;&#22270;&#35299;&#37322;&#30001;&#20110;&#27979;&#35797;&#38598;&#37325;&#29992;&#32780;&#23548;&#33268;&#30340;&#26102;&#38388;&#28418;&#31227;&#21644;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22823;&#37096;&#20998;&#24694;&#21270;&#26159;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#19982;&#19979;&#28216;&#27979;&#35797;&#38598;&#20043;&#38388;&#30340;&#26102;&#38388;&#19981;&#21305;&#37197;&#25152;&#33268;&#12290;&#25105;&#20204;&#21457;&#29616;&#22235;&#20010;&#22240;&#32032;&#23545;&#20110;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CoNLL-2003 English named entity recognition (NER) dataset has been widely used to train and evaluate NER models for almost 20 years. However, it is unclear how well models that are trained on this 20-year-old data and developed over a period of decades using the same test set will perform when applied on modern data. In this paper, we evaluate the generalization of over 20 different models trained on CoNLL-2003, and show that NER models have very different generalization. Surprisingly, we find no evidence of performance degradation in pre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using decades-old data. We investigate why some models generalize well to new data while others do not, and attempt to disentangle the effects of temporal drift and overfitting due to test reuse. Our analysis suggests that most deterioration is due to temporal mismatch between the pre-training corpora and the downstream test sets. We found that four factors are important for good g
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35780;&#20272;&#20154;&#26426;&#20132;&#20114;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;HALIE&#65292;&#35813;&#26694;&#26550;&#25429;&#25417;&#20102;&#20132;&#20114;&#36807;&#31243;&#12289;&#20027;&#35266;&#20307;&#39564;&#21644;&#20559;&#22909;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#20116;&#20010;&#20219;&#21153;&#26469;&#28085;&#30422;&#19981;&#21516;&#24418;&#24335;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2212.09746</link><description>&lt;p&gt;
&#35780;&#20272;&#20154;&#26426;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Evaluating Human-Language Model Interaction. (arXiv:2212.09746v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09746
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#20154;&#26426;&#20132;&#20114;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;HALIE&#65292;&#35813;&#26694;&#26550;&#25429;&#25417;&#20102;&#20132;&#20114;&#36807;&#31243;&#12289;&#20027;&#35266;&#20307;&#39564;&#21644;&#20559;&#22909;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#20116;&#20010;&#20219;&#21153;&#26469;&#28085;&#30422;&#19981;&#21516;&#24418;&#24335;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#20889;&#20316;&#36741;&#21161;&#21644;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#65292;&#28041;&#21450;&#21040;&#20154;&#26426;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#37117;&#26159;&#38750;&#20132;&#20114;&#24335;&#30340;&#65292;&#27169;&#22411;&#22312;&#27809;&#26377;&#20154;&#31867;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#36755;&#20986;&#12290;&#20026;&#20102;&#35780;&#20272;&#20154;&#26426;&#20132;&#20114;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20154;&#26426;&#35821;&#35328;&#20132;&#20114;&#35780;&#20272;&#65288;HALIE&#65289;&#65292;&#35813;&#26694;&#26550;&#23450;&#20041;&#20102;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#21644;&#35774;&#35745;&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#32771;&#34385;&#30340;&#32500;&#24230;&#12290;&#19982;&#26631;&#20934;&#30340;&#38750;&#20132;&#20114;&#24335;&#35780;&#20272;&#30456;&#27604;&#65292;HALIE&#25429;&#25417;&#21040;&#20102;&#65288;i&#65289;&#20132;&#20114;&#36807;&#31243;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26368;&#32456;&#36755;&#20986;&#65307;&#65288;ii&#65289;&#31532;&#19968;&#20154;&#31216;&#20027;&#35266;&#20307;&#39564;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#31532;&#19977;&#26041;&#35780;&#20272;&#65307;&#65288;iii&#65289;&#38500;&#20102;&#36136;&#37327;&#20043;&#22806;&#30340;&#20559;&#22909;&#27010;&#24565;&#65288;&#20363;&#22914;&#20139;&#21463;&#21644;&#25152;&#26377;&#26435;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20116;&#20010;&#20219;&#21153;&#65292;&#28085;&#30422;&#19981;&#21516;&#24418;&#24335;&#30340;&#20132;&#20114;&#65306;&#31038;&#20132;&#23545;&#35805;&#12289;&#38382;&#31572;&#12289;&#22635;&#23383;&#28216;&#25103;&#12289;&#25688;&#35201;&#21644;&#38544;&#21947;&#29983;&#25104;&#12290;&#20351;&#29992;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;LM&#65288;OpenAI&#30340;GPT-3&#30340;&#19977;&#20010;&#21464;&#20307;&#21644;AI21 Labs&#30340;Jurass&#65289;
&lt;/p&gt;
&lt;p&gt;
Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction. However, most benchmarks are non-interactive in that a model produces output without human involvement. To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard, non-interactive evaluation, HALIE captures (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality (e.g., enjoyment and ownership). We then design five tasks to cover different forms of interaction: social dialogue, question answering, crossword puzzles, summarization, and metaphor generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21 Labs' Jurass
&lt;/p&gt;</description></item><item><title>SpeechBlender&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#21457;&#38899;&#38169;&#35823;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#22312;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;&#30340;&#38899;&#32032;&#32423;&#19978;&#33719;&#24471;&#20102;ASR&#30456;&#20851;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#20855;&#26377;&#26356;&#26377;&#25928;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2211.00923</link><description>&lt;p&gt;
SpeechBlender: &#29992;&#20110;&#21457;&#38899;&#38169;&#35823;&#25968;&#25454;&#29983;&#25104;&#30340;&#35821;&#38899;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation. (arXiv:2211.00923v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00923
&lt;/p&gt;
&lt;p&gt;
SpeechBlender&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#21457;&#38899;&#38169;&#35823;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#22312;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;&#30340;&#38899;&#32032;&#32423;&#19978;&#33719;&#24471;&#20102;ASR&#30456;&#20851;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#20855;&#26377;&#26356;&#26377;&#25928;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#32570;&#20047;&#26631;&#35760;&#30340;&#31532;&#20108;&#35821;&#35328;&#65288;L2&#65289;&#35821;&#38899;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SpeechBlender - &#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#21457;&#38899;&#38169;&#35823;&#30340;&#32454;&#31890;&#24230;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#65292;&#20197;&#20811;&#26381;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#24615;&#12290;SpeechBlender&#21033;&#29992;&#21508;&#31181;&#25513;&#27169;&#20197;&#38024;&#23545;&#35821;&#38899;&#21333;&#20803;&#30340;&#19981;&#21516;&#21306;&#22495;&#65292;&#24182;&#22312;&#22686;&#24378;&#21457;&#38899;&#26102;&#20351;&#29992;&#28151;&#21512;&#22240;&#23376;&#20197;&#32447;&#24615;&#25554;&#20540;&#21407;&#22987;&#35821;&#38899;&#20449;&#21495;&#12290;&#36825;&#20123;&#25513;&#27169;&#26377;&#21161;&#20110;&#24179;&#28369;&#28151;&#21512;&#20449;&#21495;&#65292;&#20135;&#29983;&#27604;&#8220;&#21098;&#20999;/&#31896;&#36148;&#8221;&#26041;&#27861;&#26356;&#26377;&#25928;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#24314;&#35758;&#25216;&#26415;&#22312;&#19982;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;[1]&#30456;&#27604;&#30340;ASR&#20381;&#36182;&#24615;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#27169;&#22411;&#30340;&#38899;&#32032;&#32423;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;Speechocean762&#65292;Pearson&#30456;&#20851;&#31995;&#25968;&#65288;PCC&#65289;&#25552;&#39640;&#20102;2.0&#65285;&#12290;&#27492;&#22806;&#65292;&#19982;&#25105;&#20204;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#38899;&#32032;&#32423;&#21035;&#19978;&#23637;&#31034;&#20102;5.0&#65285;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;AraVoiceL2&#27979;&#35797;&#38598;&#19978;&#30340;F1&#24471;&#20998;&#25552;&#39640;&#20102;4.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of labeled second language (L2) speech data is a major challenge in designing mispronunciation detection models. We introduce SpeechBlender - a fine-grained data augmentation pipeline for generating mispronunciation errors to overcome such data scarcity. The SpeechBlender utilizes varieties of masks to target different regions of phonetic units, and use the mixing factors to linearly interpolate raw speech signals while augmenting pronunciation. The masks facilitate smooth blending of the signals, generating more effective samples than the `Cut/Paste' method. Our proposed technique achieves state-of-the-art results, with Speechocean762, on ASR dependent mispronunciation detection models at phoneme level, with a 2.0% gain in Pearson Correlation Coefficient (PCC) compared to the previous state-of-the-art [1]. Additionally, we demonstrate a 5.0% improvement at the phoneme level compared to our baseline. We also observed a 4.6% increase in F1-score with Arabic AraVoiceL2 testset.
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#35299;&#30721;&#26159;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20248;&#21270;&#23545;&#27604;&#30446;&#26631;&#26469;&#29983;&#25104;&#21512;&#29702;&#19988;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#19982;&#20165;&#20351;&#29992;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#30721;&#30456;&#27604;&#65292;&#23545;&#27604;&#35299;&#30721;&#33021;&#22815;&#36991;&#20813;&#30701;&#32780;&#37325;&#22797;&#30340;&#25991;&#26412;&#21644;&#19981;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#12290;</title><link>http://arxiv.org/abs/2210.15097</link><description>&lt;p&gt;
&#23545;&#27604;&#35299;&#30721;&#65306;&#23558;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#35270;&#20026;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15097
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35299;&#30721;&#26159;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20248;&#21270;&#23545;&#27604;&#30446;&#26631;&#26469;&#29983;&#25104;&#21512;&#29702;&#19988;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#19982;&#20165;&#20351;&#29992;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35299;&#30721;&#30456;&#27604;&#65292;&#23545;&#27604;&#35299;&#30721;&#33021;&#22815;&#36991;&#20813;&#30701;&#32780;&#37325;&#22797;&#30340;&#25991;&#26412;&#21644;&#19981;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#65292;&#26368;&#22823;&#27010;&#29575;&#26159;&#24320;&#25918;&#24335;&#29983;&#25104;&#30340;&#36739;&#24046;&#35299;&#30721;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#20250;&#20135;&#29983;&#30701;&#32780;&#37325;&#22797;&#30340;&#25991;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#37319;&#26679;&#24448;&#24448;&#20250;&#20135;&#29983;&#19982;&#21407;&#22987;&#20027;&#39064;&#20559;&#31163;&#30340;&#19981;&#36830;&#36143;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#35299;&#30721;&#65288;CD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#22312;&#28385;&#36275;&#21512;&#29702;&#24615;&#32422;&#26463;&#26465;&#20214;&#30340;&#21069;&#25552;&#19979;&#20248;&#21270;&#23545;&#27604;&#30446;&#26631;&#12290;&#23545;&#27604;&#30446;&#26631;&#36820;&#22238;&#19968;&#20010;&#22823;&#22411;LM&#65288;&#34987;&#31216;&#20026;&#19987;&#23478;&#65292;&#20363;&#22914;OPT-13B&#65289;&#21644;&#19968;&#20010;&#23567;&#22411;LM&#65288;&#34987;&#31216;&#20026;&#19994;&#20313;&#32773;&#65292;&#20363;&#22914;OPT-125M&#65289;&#20043;&#38388;&#30340;&#20284;&#28982;&#24046;&#24322;&#65292;&#24182;&#19988;&#32422;&#26463;&#26465;&#20214;&#30830;&#20445;&#36755;&#20986;&#26159;&#21512;&#29702;&#30340;&#12290;CD&#30340;&#28789;&#24863;&#26469;&#33258;&#20110;&#36825;&#26679;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#36739;&#22823;&#30340;LM&#65288;&#20363;&#22914;&#37325;&#22797;&#12289;&#19981;&#36830;&#36143;&#65289;&#22312;&#36739;&#23567;&#30340;LM&#20013;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#36825;&#31181;&#24046;&#24322;&#34920;&#26126;&#21738;&#20123;&#25991;&#26412;&#24212;&#20248;&#20808;&#32771;&#34385;&#12290;CD&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#22521;&#35757;&#65292;&#24182;&#19988;&#27604;&#20165;&#20174;&#36739;&#22823;&#30340;LM&#36827;&#34892;&#35299;&#30721;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;&#23427;&#36824;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#35268;&#27169;&#65288;OPT-13B&#21644;GPT2-1.5B&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#23567;&#23616;&#37096;&#22522;&#20110;&#35821;&#27861;&#30340;&#32534;&#30721;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#31616;&#21333;&#12289;&#26356;&#26222;&#36941;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#20855;&#26377;&#24378;&#22823;&#30340;&#26222;&#36941;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23454;&#39564;&#20063;&#34920;&#26126;&#65292;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#20013;&#30340;&#35268;&#21017;&#25968;&#37327;&#19981;&#33021;&#26126;&#30830;&#21306;&#20998;&#38271;&#35760;&#24518;&#21644;&#26080;&#35760;&#24518;&#30340;&#28304;&#12290;</title><link>http://arxiv.org/abs/2209.13636</link><description>&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#23616;&#37096;&#22522;&#20110;&#35821;&#27861;&#30340;&#32534;&#30721;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Local Grammar-Based Coding Revisited. (arXiv:2209.13636v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#23567;&#23616;&#37096;&#22522;&#20110;&#35821;&#27861;&#30340;&#32534;&#30721;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#31616;&#21333;&#12289;&#26356;&#26222;&#36941;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#20855;&#26377;&#24378;&#22823;&#30340;&#26222;&#36941;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23454;&#39564;&#20063;&#34920;&#26126;&#65292;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#20013;&#30340;&#35268;&#21017;&#25968;&#37327;&#19981;&#33021;&#26126;&#30830;&#21306;&#20998;&#38271;&#35760;&#24518;&#21644;&#26080;&#35760;&#24518;&#30340;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#23567;&#23616;&#37096;&#22522;&#20110;&#35821;&#27861;&#30340;&#32534;&#30721;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#23616;&#37096;&#22522;&#20110;&#35821;&#27861;&#30340;&#32534;&#30721;&#22120;&#36880;&#20010;&#31526;&#21495;&#22320;&#23545;&#35821;&#27861;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#26368;&#23567;&#35821;&#27861;&#21464;&#25442;&#36890;&#36807;&#23616;&#37096;&#35821;&#27861;&#32534;&#30721;&#30340;&#38271;&#24230;&#22312;&#39044;&#35774;&#30340;&#35821;&#27861;&#31867;&#21035;&#20013;&#26368;&#23567;&#21270;&#35821;&#27861;&#38271;&#24230;&#12290;&#24050;&#30693;&#65292;&#36825;&#26679;&#30340;&#26368;&#23567;&#32534;&#30721;&#23545;&#20110;&#20005;&#26684;&#27491;&#29109;&#29575;&#30340;&#24773;&#20917;&#20855;&#26377;&#24378;&#22823;&#30340;&#26222;&#36941;&#24615;&#65292;&#32780;&#26368;&#23567;&#35821;&#27861;&#20013;&#30340;&#35268;&#21017;&#25968;&#37327;&#26500;&#25104;&#20102;&#28304;&#30340;&#20114;&#20449;&#24687;&#30340;&#19978;&#30028;&#12290;&#23613;&#31649;&#23436;&#20840;&#26368;&#23567;&#32534;&#30721;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20294;&#21463;&#38480;&#30340;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#21487;&#20197;&#26377;&#25928;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#31616;&#21333;&#12289;&#26356;&#26222;&#36866;&#30340;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#24378;&#22823;&#26222;&#36941;&#24615;&#30340;&#35777;&#26126;&#26041;&#27861;&#65292;&#19981;&#21463;&#29109;&#29575;&#30340;&#38480;&#21046;&#12290;&#35813;&#35777;&#26126;&#22522;&#20110;&#23545;&#25490;&#21517;&#27010;&#29575;&#30340;&#31616;&#21333;&#30340;Zipfian&#30028;&#38480;&#12290;&#39034;&#20415;&#25552;&#19968;&#19979;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#26368;&#23567;&#20998;&#22359;&#32534;&#30721;&#20013;&#30340;&#35268;&#21017;&#25968;&#37327;&#19981;&#33021;&#26126;&#30830;&#21306;&#20998;&#38271;&#35760;&#24518;&#21644;&#26080;&#35760;&#24518;&#30340;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the problem of minimal local grammar-based coding. In this setting, the local grammar encoder encodes grammars symbol by symbol, whereas the minimal grammar transform minimizes the grammar length in a preset class of grammars as given by the length of local grammar encoding. It has been known that such minimal codes are strongly universal for a strictly positive entropy rate, whereas the number of rules in the minimal grammar constitutes an upper bound for the mutual information of the source. Whereas the fully minimal code is likely intractable, the constrained minimal block code can be efficiently computed. In this article, we present a new, simpler, and more general proof of strong universality of the minimal block code, regardless of the entropy rate. The proof is based on a simple Zipfian bound for ranked probabilities. By the way, we also show empirically that the number of rules in the minimal block code cannot clearly discriminate between long-memory and memoryless s
&lt;/p&gt;</description></item><item><title>Kencorpus&#39033;&#30446;&#26088;&#22312;&#25910;&#38598;&#21644;&#23384;&#20648;&#32943;&#23612;&#20122;&#26031;&#29926;&#24076;&#37324;&#35821;&#12289;&#22810;&#21346;&#22885;&#35821;&#21644;&#21346;&#38597;&#35821;&#30340;&#25991;&#26412;&#21644;&#35821;&#38899;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#21407;&#29983;&#38750;&#27954;&#35821;&#35328;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25968;&#23383;&#21253;&#23481;&#24615;&#21644;&#20449;&#24687;&#35775;&#38382;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;5,000&#20010;&#39033;&#30446;&#65292;&#20026;&#22810;&#21346;&#22885;&#35821;&#21644;&#21346;&#38597;&#35821;&#25552;&#20379;&#20102;&#35789;&#24615;&#26631;&#27880;&#38598;&#12290;</title><link>http://arxiv.org/abs/2208.12081</link><description>&lt;p&gt;
Kencorpus: &#19968;&#20221;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#32943;&#23612;&#20122;&#26031;&#29926;&#24076;&#37324;&#35821;&#12289;&#22810;&#21346;&#22885;&#35821;&#21644;&#21346;&#38597;&#35821;&#30340;&#32943;&#23612;&#20122;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for Natural Language Processing Tasks. (arXiv:2208.12081v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12081
&lt;/p&gt;
&lt;p&gt;
Kencorpus&#39033;&#30446;&#26088;&#22312;&#25910;&#38598;&#21644;&#23384;&#20648;&#32943;&#23612;&#20122;&#26031;&#29926;&#24076;&#37324;&#35821;&#12289;&#22810;&#21346;&#22885;&#35821;&#21644;&#21346;&#38597;&#35821;&#30340;&#25991;&#26412;&#21644;&#35821;&#38899;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#21407;&#29983;&#38750;&#27954;&#35821;&#35328;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25968;&#23383;&#21253;&#23481;&#24615;&#21644;&#20449;&#24687;&#35775;&#38382;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;5,000&#20010;&#39033;&#30446;&#65292;&#20026;&#22810;&#21346;&#22885;&#35821;&#21644;&#21346;&#38597;&#35821;&#25552;&#20379;&#20102;&#35789;&#24615;&#26631;&#27880;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#29983;&#38750;&#27954;&#35821;&#35328;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34987;&#24402;&#31867;&#20026;&#19981;&#21463;&#20851;&#27880;&#30340;&#35821;&#35328;&#65292;&#22312;&#25968;&#23383;&#21253;&#23481;&#24615;&#21644;&#20449;&#24687;&#35775;&#38382;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#20123;&#35821;&#35328;&#30340;&#22788;&#29702;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#22312;&#27809;&#26377;&#24517;&#35201;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;Kencorpus&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25910;&#38598;&#21644;&#23384;&#20648;&#36275;&#22815;&#22909;&#30340;&#25991;&#26412;&#21644;&#35821;&#38899;&#25968;&#25454;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20197;&#24212;&#29992;&#20110;&#22810;&#35821;&#35328;&#31038;&#21306;&#20013;&#30340;&#26426;&#22120;&#32763;&#35793;&#12289;&#38382;&#31572;&#21644;&#36716;&#24405;&#31561;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;Kencorpus&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;&#32943;&#23612;&#20122;&#19977;&#31181;&#20027;&#35201;&#20351;&#29992;&#35821;&#35328;&#65288;&#26031;&#29926;&#24076;&#37324;&#35821;&#12289;&#22810;&#21346;&#22885;&#35821;&#21644;&#21346;&#38597;&#35821;&#65289;&#30340;&#25991;&#26412;&#21644;&#35821;&#38899;&#35821;&#26009;&#24211;&#12290;&#30740;&#31350;&#20154;&#21592;&#20174;&#31038;&#21306;&#12289;&#23398;&#26657;&#12289;&#23186;&#20307;&#21644;&#20986;&#29256;&#21830;&#25910;&#38598;&#20102;&#36825;&#20123;&#25968;&#25454;&#12290;Kencorpus&#25968;&#25454;&#38598;&#21253;&#21547;5,594&#20010;&#39033;&#30446;&#65292;&#20854;&#20013;&#21253;&#25324;4,442&#20010;&#25991;&#26412;&#65288;5.6&#30334;&#19975;&#35789;&#65289;&#21644;1,152&#20010;&#35821;&#38899;&#25991;&#20214;&#65288;177&#23567;&#26102;&#65289;&#12290;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#65292;&#20026;&#22810;&#21346;&#22885;&#35821;&#21644;&#21346;&#38597;&#35821;&#20998;&#21035;&#35774;&#32622;&#20102;5&#19975;&#21644;9.3&#19975;&#20010;&#35789;&#30340;&#35789;&#24615;&#26631;&#27880;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indigenous African languages are categorized as under-served in Natural Language Processing. They therefore experience poor digital inclusivity and information access. The processing challenge with such languages has been how to use machine learning and deep learning models without the requisite data. The Kencorpus project intends to bridge this gap by collecting and storing text and speech data that is good enough for data-driven solutions in applications such as machine translation, question answering and transcription in multilingual communities. The Kencorpus dataset is a text and speech corpus for three languages predominantly spoken in Kenya: Swahili, Dholuo and Luhya. Data collection was done by researchers from communities, schools, media, and publishers. The Kencorpus' dataset has a collection of 5,594 items - 4,442 texts (5.6M words) and 1,152 speech files (177hrs). Based on this data, Part of Speech tagging sets for Dholuo and Luhya (50,000 and 93,000 words respectively) wer
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#23450;&#21046;&#25991;&#26412;&#28040;&#27602;&#26426;&#21046; (CusText) &#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#19982;&#25928;&#29992;&#26435;&#34913;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#24182;&#22312;&#26631;&#35760;&#32423;&#21035;&#23454;&#29616;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2207.01193</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#23450;&#21046;&#25991;&#26412;&#28040;&#27602;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Customized Text Sanitization Mechanism with Differential Privacy. (arXiv:2207.01193v2 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01193
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#23450;&#21046;&#25991;&#26412;&#28040;&#27602;&#26426;&#21046; (CusText) &#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#19982;&#25928;&#29992;&#26435;&#34913;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#24182;&#22312;&#26631;&#35760;&#32423;&#21035;&#23454;&#29616;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38544;&#31169;&#38382;&#39064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#31038;&#21306;&#20013;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24050;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#25353;&#29031;&#24046;&#20998;&#38544;&#31169;&#21407;&#21017;&#23545;&#25991;&#26412;&#36827;&#34892;&#28040;&#27602;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24230;&#37327;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169; (MLDP) &#30340;&#26368;&#20808;&#36827;&#25991;&#26412;&#28040;&#27602;&#26426;&#21046;&#19981;&#36866;&#29992;&#20110;&#38750;&#24230;&#37327;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#24182;&#19988;&#26080;&#27861;&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987; &#949;-&#24046;&#20998;&#38544;&#31169; (DP) &#23450;&#20041;&#30340;&#26032;&#22411;&#23450;&#21046;&#25991;&#26412; (CusText) &#28040;&#27602;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#19982;&#20219;&#20309;&#30456;&#20284;&#24230;&#24230;&#37327;&#20860;&#23481;&#12290;&#27492;&#22806;&#65292;CusText&#20026;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#20998;&#37197;&#19968;&#20010;&#23450;&#21046;&#30340;&#36755;&#20986;&#26631;&#35760;&#38598;&#65292;&#20197;&#22312;&#26631;&#35760;&#32423;&#21035;&#25552;&#20379;&#26356;&#39640;&#32423;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;CusText&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/sai4july/CusT &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
As privacy issues are receiving increasing attention within the Natural Language Processing (NLP) community, numerous methods have been proposed to sanitize texts subject to differential privacy. However, the state-of-the-art text sanitization mechanisms based on metric local differential privacy (MLDP) do not apply to non-metric semantic similarity measures and cannot achieve good trade-offs between privacy and utility. To address the above limitations, we propose a novel Customized Text (CusText) sanitization mechanism based on the original $\epsilon$-differential privacy (DP) definition, which is compatible with any similarity measure. Furthermore, CusText assigns each input token a customized output set of tokens to provide more advanced privacy protection at the token level. Extensive experiments on several benchmark datasets show that CusText achieves a better trade-off between privacy and utility than existing mechanisms. The code is available at https://github.com/sai4july/CusT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#21028;&#21035;&#27169;&#22411;&#22312;&#20960;&#20046;&#25152;&#26377;&#35774;&#32622;&#20013;&#20173;&#28982;&#21487;&#20197;&#20248;&#20110;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2206.02892</link><description>&lt;p&gt;
&#21028;&#21035;&#27169;&#22411;&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#20173;&#28982;&#20248;&#20110;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discriminative Models Can Still Outperform Generative Models in Aspect Based Sentiment Analysis. (arXiv:2206.02892v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#21028;&#21035;&#27169;&#22411;&#22312;&#20960;&#20046;&#25152;&#26377;&#35774;&#32622;&#20013;&#20173;&#28982;&#21487;&#20197;&#20248;&#20110;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26377;&#21161;&#20110;&#35299;&#37322;&#29992;&#25143;&#23545;&#20135;&#21697;&#21644;&#26381;&#21153;&#30340;&#24847;&#35265;&#12290;&#36807;&#21435;&#65292;ABSA&#27169;&#22411;&#26159;&#21028;&#21035;&#24615;&#30340;&#65292;&#20294;&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#30452;&#25509;&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#26041;&#38754;&#21644;&#26497;&#24615;&#12290;&#30456;&#21453;&#65292;&#21028;&#21035;&#27169;&#22411;&#36890;&#24120;&#39318;&#20808;&#20174;&#25991;&#26412;&#20013;&#36873;&#25321;&#26041;&#38754;&#65292;&#28982;&#21518;&#23545;&#26041;&#38754;&#30340;&#26497;&#24615;&#36827;&#34892;&#20998;&#31867;&#12290;&#20197;&#21069;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#33521;&#35821;ABSA&#25968;&#25454;&#38598;&#19978;&#65292;&#29983;&#25104;&#27169;&#22411;&#32988;&#36807;&#21028;&#21035;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#35774;&#32622;&#20013;&#35780;&#20272;&#21644;&#23545;&#27604;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#21028;&#21035;&#21644;&#29983;&#25104;&#27169;&#22411;&#65306;&#36328;&#35821;&#35328;&#12289;&#36328;&#39046;&#22495;&#21644;&#36328;&#35821;&#35328;&#21644;&#39046;&#22495;&#65292;&#20197;&#20102;&#35299;&#38500;&#20102;&#33521;&#35821;&#21333;&#19968;&#39046;&#22495;&#20043;&#22806;&#30340;&#35774;&#32622;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#30340;&#35774;&#32622;&#20013;&#65292;&#21028;&#21035;&#27169;&#22411;&#20173;&#28982;&#21487;&#20197;&#20248;&#20110;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based Sentiment Analysis (ABSA) helps to explain customers' opinions towards products and services. In the past, ABSA models were discriminative, but more recently generative models have been used to generate aspects and polarities directly from text. In contrast, discriminative models commonly first select aspects from the text, and then classify the aspect's polarity. Previous results showed that generative models outperform discriminative models on several English ABSA datasets. Here, we evaluate and contrast two state-of-the-art discriminative and generative models in several settings: cross-lingual, cross-domain, and cross-lingual and domain, to understand generalizability in settings other than English mono-lingual in-domain. Our more thorough evaluation shows that, contrary to previous studies, discriminative models can still outperform generative models in almost all settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2108.08614</link><description>&lt;p&gt;
UNIQORN&#65306;&#32479;&#19968;&#30340;RDF&#30693;&#35782;&#22270;&#35889;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#22312;&#30693;&#35782;&#22270;&#35889;&#21644;&#20854;&#20182;RDF&#25968;&#25454;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#35768;&#22810;&#20248;&#31168;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#25110;&#30005;&#25253;&#26597;&#35810;&#25552;&#20379;&#28165;&#26224;&#30340;&#31572;&#26696;&#12290;&#20854;&#20013;&#19968;&#20123;&#31995;&#32479;&#23558;&#25991;&#26412;&#28304;&#20316;&#20026;&#38468;&#21152;&#35777;&#25454;&#32435;&#20837;&#22238;&#31572;&#36807;&#31243;&#65292;&#20294;&#19981;&#33021;&#35745;&#31639;&#20165;&#23384;&#22312;&#20110;&#25991;&#26412;&#20013;&#30340;&#31572;&#26696;&#12290;&#30456;&#21453;&#65292;IR&#21644;NLP&#31038;&#21306;&#30340;&#31995;&#32479;&#24050;&#32463;&#35299;&#20915;&#20102;&#26377;&#20851;&#25991;&#26412;&#30340;QA&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#31995;&#32479;&#20960;&#20046;&#19981;&#21033;&#29992;&#35821;&#20041;&#25968;&#25454;&#21644;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#26080;&#32541;&#25805;&#20316;&#28151;&#21512;RDF&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#35821;&#26009;&#24211;&#25110;&#21333;&#20010;&#26469;&#28304;&#30340;&#22797;&#26434;&#38382;&#39064;&#30340;&#31995;&#32479;&#65292;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;UNIQORN&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;BERT&#27169;&#22411;&#20174;RDF&#25968;&#25454;&#21644;/&#25110;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35777;&#25454;&#26469;&#21160;&#24577;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#12290;&#32467;&#26524;&#22270;&#36890;&#24120;&#38750;&#24120;&#20016;&#23500;&#20294;&#39640;&#24230;&#22024;&#26434;&#12290;UNIQORN&#36890;&#36807;&#29992;&#20110;&#32452;Steiner&#26641;&#30340;&#22270;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#36755;&#20837;&#65292;&#20174;&#32780;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#65292;&#36827;&#32780;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, systems from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents the first system for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph is typically rich but highly noisy. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifi
&lt;/p&gt;</description></item></channel></rss>