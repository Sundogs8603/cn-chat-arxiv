<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04634</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#22312;&#28151;&#21512;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24320;&#22987;&#24212;&#29992;&#20110;&#26085;&#24120;&#20351;&#29992;&#65292;&#24182;&#26377;&#33021;&#21147;&#22312;&#26410;&#26469;&#30340;&#21313;&#24180;&#20869;&#20135;&#29983;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#21462;&#20195;&#20114;&#32852;&#32593;&#19978;&#30340;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#65292;&#24182;&#26377;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#65292;&#22914;&#38035;&#40060;&#25915;&#20987;&#21644;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12290;&#27700;&#21360;&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#26816;&#27979;&#21644;&#21487;&#35760;&#24405;&#65292;&#26469;&#38477;&#20302;&#36825;&#20123;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#22312;&#29616;&#23454;&#20013;&#28151;&#21512;&#20102;&#20854;&#20182;&#30340;&#25991;&#26412;&#26469;&#28304;&#65292;&#34987;&#20154;&#31867;&#20889;&#20316;&#32773;&#25110;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#25913;&#20889;&#65292;&#34987;&#29992;&#20110;&#31038;&#20132;&#21644;&#25216;&#26415;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#26102;&#65292;&#27700;&#21360;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#30340;&#21487;&#38752;&#24615;&#22914;&#20309;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#37327;&#21270;&#20102;&#23427;&#20204;&#26816;&#27979;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#22312;&#27599;&#20010;&#24773;&#20917;&#19979;&#38656;&#35201;&#35266;&#23519;&#22810;&#23569;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#25165;&#33021;&#21487;&#38752;&#22320;&#26816;&#27979;&#27700;&#21360;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20102;&#24403;&#27700;&#21360;&#19982;&#20854;&#20182;&#25991;&#26412;&#26469;&#28304;&#28151;&#21512;&#26102;&#27700;&#21360;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#27700;&#21360;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#22871;&#20214;&#65292;&#20316;&#32773;&#20204;&#21457;&#29616;OOD&#19982;ID&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04618</link><description>&lt;p&gt;
&#37325;&#28201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;: &#22522;&#20934;&#65292;&#20998;&#26512;&#21644;LLMs&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations. (arXiv:2306.04618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#22871;&#20214;&#65292;&#20316;&#32773;&#20204;&#21457;&#29616;OOD&#19982;ID&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#20013;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;(OOD)&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#20197;&#24448;&#30740;&#31350;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#35774;&#32622;&#26222;&#36941;&#32570;&#20047;&#36275;&#22815;&#30340;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#23545;OOD&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#26500;&#24314;&#26041;&#26696;&#65292;&#30830;&#20445;&#20102;&#26126;&#30830;&#30340;&#21306;&#20998;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BOSS&#65292;&#19968;&#20010;&#28085;&#30422;5&#20010;&#20219;&#21153;&#21644;20&#20010;&#25968;&#25454;&#38598;&#30340;&#29992;&#20110;&#35780;&#20272;OOT&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#22871;&#20214;&#12290;&#22522;&#20110;BOSS&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;&#20998;&#26512;&#21644;&#35780;&#20272;OOD&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39321;&#33609;&#24494;&#35843;&#30340;ID&#21644;OOD&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#20856;&#22411;&#31867;&#22411;&#25581;&#31034;&#20102;&#20869;&#22312;&#30340;&#23398;&#20064;&#26426;&#21046;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#39044;&#27979;OOD&#40065;&#26834;&#24615;&#65292;&#24182;&#19982;ID&#25968;&#25454;&#38598;&#19978;&#30340;&#36827;&#23637;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;BOSS&#19978;&#35780;&#20272;&#20102;5&#31181;&#32463;&#20856;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;OOD&#24615;&#33021;&#24182;&#19981;&#24635;&#26159;&#19982;ID&#24615;&#33021;&#19968;&#33268;&#65292;&#36825;&#34920;&#26126;&#20102;&#29305;&#21035;&#35780;&#20272;OOD&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#65288;&#28508;&#22312;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;&#8220;&#20004;&#20010;&#35789;&#27979;&#35797;&#8221;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#12290;&#27979;&#35797;&#38656;&#35201;&#23545;1768&#20010;&#21517;&#35789;&#32452;&#21512;&#36827;&#34892;&#24847;&#20041;&#24615;&#21028;&#26029;&#65292;&#24182;&#21487;&#29992;&#20110;&#35780;&#20272;0-4&#37327;&#34920;&#19978;&#30340;&#26377;&#24847;&#20041;&#35780;&#20998;&#21644;&#20108;&#36827;&#21046;&#21028;&#26029;&#12290;</title><link>http://arxiv.org/abs/2306.04610</link><description>&lt;p&gt;
&#19968;&#20010;&#35821;&#20041;&#22522;&#20934;&#27979;&#35797;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20004;&#20010;&#35789;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
The Two Word Test: A Semantic Benchmark for Large Language Models. (arXiv:2306.04610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04610
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;&#8220;&#20004;&#20010;&#35789;&#27979;&#35797;&#8221;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#12290;&#27979;&#35797;&#38656;&#35201;&#23545;1768&#20010;&#21517;&#35789;&#32452;&#21512;&#36827;&#34892;&#24847;&#20041;&#24615;&#21028;&#26029;&#65292;&#24182;&#21487;&#29992;&#20110;&#35780;&#20272;0-4&#37327;&#34920;&#19978;&#30340;&#26377;&#24847;&#20041;&#35780;&#20998;&#21644;&#20108;&#36827;&#21046;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36817;&#26469;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#36890;&#36807;&#39640;&#32423;&#19987;&#19994;&#32771;&#35797;&#21644;&#33499;&#21051;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#31181;&#24615;&#33021;&#20351;&#35768;&#22810;&#20154;&#35748;&#20026;&#23427;&#20204;&#25509;&#36817;&#20110;&#23454;&#29616;&#20154;&#31867;&#25110;&#8220;&#30495;&#27491;&#30340;&#8221;&#35821;&#35328;&#29702;&#35299;&#65292;&#29978;&#33267;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#20351;&#29992;&#20004;&#20010;&#21333;&#35789;&#30701;&#35821;&#35780;&#20272;LLMs&#30340;&#35821;&#20041;&#33021;&#21147;&#65292;&#35813;&#20219;&#21153;&#21487;&#20197;&#30456;&#23545;&#23481;&#26131;&#22320;&#30001;&#27809;&#26377;&#39640;&#32423;&#22521;&#35757;&#30340;&#20154;&#31867;&#23436;&#25104;&#12290;&#23558;&#22810;&#20010;&#35789;&#32452;&#21512;&#25104;&#19968;&#20010;&#27010;&#24565;&#26159;&#20154;&#31867;&#35821;&#35328;&#21644;&#26234;&#33021;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#35813;&#27979;&#35797;&#38656;&#35201;&#23545;1768&#20010;&#35780;&#20026;&#26377;&#24847;&#20041;&#65288;&#20363;&#22914;baby boy&#65289;&#25110;&#19981;&#20855;&#26377;&#24847;&#20041;&#65288;&#20363;&#22914;goat sky&#65289;&#30340;&#21517;&#35789;&#32452;&#21512;&#36827;&#34892;&#26377;&#24847;&#20041;&#24615;&#21028;&#26029;&#65292;&#30001;150&#20010;&#20154;&#31867;&#35780;&#23450;&#32773;&#36827;&#34892;&#35780;&#23450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;0-4&#37327;&#34920;&#19978;&#25506;&#27979;&#26377;&#24847;&#20041;&#35780;&#20998;&#20197;&#21450;&#20108;&#36827;&#21046;&#21028;&#26029;&#30340;&#20219;&#21153;&#29256;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;TWT&#22312;GPT-4&#12289;GPT-3.5&#21644;Bard&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable abilities recently, including passing advanced professional exams and demanding benchmark tests. This performance has led many to suggest that they are close to achieving humanlike or 'true' understanding of language, and even Artificial General Intelligence (AGI). Here, we provide a new open-source benchmark that can assess semantic abilities of LLMs using two-word phrases using a task that can be performed relatively easily by humans without advanced training. Combining multiple words into a single concept is a fundamental aspect of human language and intelligence. The test requires meaningfulness judgments of 1768 noun-noun combinations that have been rated as meaningful (e.g., baby boy) or not meaningful (e.g., goat sky). by 150 human raters. We provide versions of the task that probe meaningfulness ratings on a 0-4 scale as well as binary judgments. We conducted a series of experiments using the TWT on GPT-4, GPT-3.5, and Bard, wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#24178;&#39044;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#26497;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#20943;&#23569;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#65292;&#26377;&#25928;&#38477;&#20302;&#23545;&#20219;&#20309;&#24615;&#21035;&#30340;&#20559;&#22909;&#20542;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.04597</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#21435;&#20559;&#32622;&#65306;&#23569;&#26679;&#26412;&#25968;&#25454;&#24178;&#39044;&#26041;&#27861;&#38477;&#20302;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions. (arXiv:2306.04597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04597
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#24178;&#39044;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#26497;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#20943;&#23569;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#65292;&#26377;&#25928;&#38477;&#20302;&#23545;&#20219;&#20309;&#24615;&#21035;&#30340;&#20559;&#22909;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20027;&#27969;&#30340;&#21435;&#20559;&#32622;&#25216;&#26415;&#22823;&#22810;&#38598;&#20013;&#22312;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#24178;&#39044;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#26159;&#19968;&#31181;&#24378;&#22823;&#32780;&#31616;&#21333;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20943;&#23569;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#21482;&#35201;&#23548;&#20837;10&#20010;&#21435;&#20559;&#32622;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21363;&#21487;&#26174;&#33879;&#20943;&#23569;&#20219;&#20309;&#24615;&#21035;&#30340;&#20559;&#22909;&#20542;&#21521;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#22240;&#27492;&#26159;&#39640;&#24230;&#21487;&#34892;&#21644;&#23454;&#29992;&#30340;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 de-biased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, our few-shot debiasing approach is highly feasible and practical. Through extensive experimentation,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#28304;&#35821;&#35328;&#32570;&#20047;&#26126;&#30830;&#24615;&#21035;&#26631;&#35760;&#30340;&#24773;&#20917;&#65292;&#29305;&#21035;&#20851;&#27880;&#21253;&#21547;&#20154;&#21517;&#30340;&#36755;&#20837;&#65292;&#22312;&#24615;&#21035;&#20559;&#35265;&#21644;&#32531;&#35299;&#26041;&#38754;&#25552;&#20986;&#20102;&#25509;&#21463;&#24615;&#21035;&#21644;&#32763;&#35793;&#30340;&#27169;&#31946;&#24615;&#30340;&#19968;&#31181;&#28508;&#22312;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2306.04573</link><description>&lt;p&gt;
&#24615;&#21035;&#12289;&#22995;&#21517;&#21644;&#20854;&#20182;&#22885;&#31192;&#65306;&#36208;&#21521;&#23545;&#21253;&#23481;&#24615;&#21035;&#30340;&#32763;&#35793;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gender, names and other mysteries: Towards the ambiguous for gender-inclusive translation. (arXiv:2306.04573v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04573
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#28304;&#35821;&#35328;&#32570;&#20047;&#26126;&#30830;&#24615;&#21035;&#26631;&#35760;&#30340;&#24773;&#20917;&#65292;&#29305;&#21035;&#20851;&#27880;&#21253;&#21547;&#20154;&#21517;&#30340;&#36755;&#20837;&#65292;&#22312;&#24615;&#21035;&#20559;&#35265;&#21644;&#32531;&#35299;&#26041;&#38754;&#25552;&#20986;&#20102;&#25509;&#21463;&#24615;&#21035;&#21644;&#32763;&#35793;&#30340;&#27169;&#31946;&#24615;&#30340;&#19968;&#31181;&#28508;&#22312;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MT&#65288;&#26426;&#22120;&#32763;&#35793;&#65289;&#20013;&#20851;&#20110;&#24615;&#21035;&#30340;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#8220;&#26126;&#30830;&#24615;&#8221;&#36755;&#20837;&#19978;&#65292;&#21363;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#26631;&#35760;&#24212;&#22312;&#36755;&#20986;&#20013;&#24471;&#21040;&#35299;&#20915;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#28304;&#21477;&#23376;&#32570;&#20047;&#26174;&#24335;&#24615;&#21035;&#26631;&#35760;&#65292;&#20294;&#30446;&#26631;&#21477;&#23376;&#21547;&#26377;&#26356;&#20016;&#23500;&#35821;&#27861;&#24615;&#21035;&#30340;&#26222;&#36941;&#24773;&#20917;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21253;&#21547;&#20154;&#21517;&#30340;&#36755;&#20837;&#12290;&#30740;&#31350;&#36825;&#20123;&#21477;&#23376;&#23545;&#25581;&#31034;&#20102;MT&#24615;&#21035;&#20559;&#35265;&#21450;&#20854;&#32531;&#35299;&#30340;&#26032;&#35748;&#35782;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;MT&#25968;&#25454;&#20013;&#30340;&#35768;&#22810;&#22995;&#21517;-&#24615;&#21035;&#20849;&#29616;&#24182;&#19981;&#33021;&#36890;&#36807;&#28304;&#35821;&#35328;&#30340;&#8220;&#26126;&#30830;&#24615;&#21035;&#8221;&#26469;&#35299;&#20915;&#65292;&#24182;&#19988;&#24615;&#21035;&#19981;&#26126;&#30830;&#30340;&#20363;&#23376;&#21487;&#20197;&#21344;&#21040;&#22823;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25509;&#21463;&#24615;&#21035;&#21644;&#32763;&#35793;&#19978;&#30340;&#27169;&#31946;&#24615;&#65292;&#26397;&#21521;&#21253;&#23481;&#24615;&#21035;&#30340;&#32763;&#35793;&#30340;&#28508;&#22312;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vast majority of work on gender in MT focuses on 'unambiguous' inputs, where gender markers in the source language are expected to be resolved in the output. Conversely, this paper explores the widespread case where the source sentence lacks explicit gender markers, but the target sentence contains them due to richer grammatical gender. We particularly focus on inputs containing person names.  Investigating such sentence pairs casts a new light on research into MT gender bias and its mitigation. We find that many name-gender co-occurrences in MT data are not resolvable with 'unambiguous gender' in the source language, and that gender-ambiguous examples can make up a large proportion of training examples. From this, we discuss potential steps toward gender-inclusive translation which accepts the ambiguity in both gender and translation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;OpenAI&#30340;ChatGPT&#27169;&#22411;&#22312;&#24189;&#40664;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#30340;&#24189;&#40664;&#24182;&#19981;&#26159;&#30828;&#32534;&#30721;&#30340;&#65292;&#20294;&#22823;&#37096;&#20998;&#29983;&#25104;&#30340;&#31505;&#35805;&#37117;&#19981;&#26159;&#26032;&#30340;&#65292;&#20960;&#20046;&#26159;&#23569;&#37327;&#20960;&#32452;&#37325;&#22797;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.04563</link><description>&lt;p&gt;
ChatGPT&#24456;&#26377;&#36259;&#65292;&#20294;&#24182;&#19981;&#22909;&#31505;&#65281;&#24189;&#40664;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#20381;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models. (arXiv:2306.04563v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04563
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;OpenAI&#30340;ChatGPT&#27169;&#22411;&#22312;&#24189;&#40664;&#35782;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#30340;&#24189;&#40664;&#24182;&#19981;&#26159;&#30828;&#32534;&#30721;&#30340;&#65292;&#20294;&#22823;&#37096;&#20998;&#29983;&#25104;&#30340;&#31505;&#35805;&#37117;&#19981;&#26159;&#26032;&#30340;&#65292;&#20960;&#20046;&#26159;&#23569;&#37327;&#20960;&#32452;&#37325;&#22797;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#20132;&#27969;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#34987;&#35299;&#20915;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#33021;&#22815;&#25429;&#25417;&#38544;&#21547;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;OpenAI&#30340;ChatGPT&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20844;&#20247;&#20851;&#27880;&#12290;&#22522;&#20110;GPT3&#30340;&#27169;&#22411;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#19982;&#20154;&#31867;&#20132;&#27969;&#30340;&#27700;&#24179;&#65292;&#29978;&#33267;&#33021;&#22815;&#35762;&#31505;&#35805;&#12290;&#20294;&#26159;&#65292;ChatGPT&#30495;&#30340;&#24456;&#26377;&#36259;&#21527;&#65311;&#20316;&#32773;&#38024;&#23545;&#27169;&#22411;&#30340;&#24189;&#40664;&#24863;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;&#29983;&#25104;&#12289;&#35299;&#37322;&#21644;&#26816;&#27979;&#31561;&#29615;&#33410;&#65292;&#35797;&#22270;&#20102;&#35299;ChatGPT&#29702;&#35299;&#24182;&#20877;&#29616;&#20154;&#31867;&#24189;&#40664;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#27169;&#22411;&#26412;&#36523;&#19981;&#21487;&#35775;&#38382;&#65292;&#20316;&#32773;&#37319;&#29992;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#23454;&#39564;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#22823;&#37096;&#20998;&#29983;&#25104;&#30340;&#31505;&#35805;&#24182;&#19981;&#26159;&#30001;&#35813;&#27169;&#22411;&#26032;&#29983;&#25104;&#30340;&#65292;&#32780;&#26159;&#37325;&#22797;&#20102;&#23569;&#37327;&#30340;&#20960;&#32452;&#31505;&#35805;&#12290;&#34429;&#28982;&#31995;&#32479;&#21487;&#20197;&#20934;&#30830;&#22320;&#35299;&#37322;&#26377;&#25928;&#30340;&#31505;&#35805;&#65292;&#20294;&#20063;&#20250;&#25552;&#20986;&#34394;&#26500;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humor is a central aspect of human communication that has not been solved for artificial agents so far. Large language models (LLMs) are increasingly able to capture implicit and contextual information. Especially, OpenAI's ChatGPT recently gained immense public attention. The GPT3-based model almost seems to communicate on a human level and can even tell jokes. Humor is an essential component of human communication. But is ChatGPT really funny? We put ChatGPT's sense of humor to the test. In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT's capability to grasp and reproduce human humor. Since the model itself is not accessible, we applied prompt-based experiments. Our empirical evidence indicates that jokes are not hard-coded but mostly also not newly generated by the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system accurately explains valid jokes but also comes up with fictional ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#20869;&#19982;&#39046;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22810;&#20219;&#21153;&#19982;&#21333;&#20219;&#21153;&#35757;&#32451;&#30340;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#20020;&#24202;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35786;&#26029;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04551</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#35757;&#32451;&#32467;&#21512;&#39046;&#22495;&#20869;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35786;&#26029;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning. (arXiv:2306.04551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#20869;&#19982;&#39046;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22810;&#20219;&#21153;&#19982;&#21333;&#20219;&#21153;&#35757;&#32451;&#30340;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#20020;&#24202;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#35786;&#26029;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26159;&#22686;&#24378;&#20020;&#24202;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#21644;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#35786;&#26029;&#25512;&#29702;&#22522;&#20934;&#65288;DR.BENCH&#65289;&#20316;&#20026;&#20840;&#38754;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#30001;&#20845;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#20195;&#34920;&#20020;&#24202;&#25512;&#29702;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#39046;&#22495;&#20869;&#19982;&#39046;&#22495;&#22806;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22810;&#20219;&#21153;&#19982;&#21333;&#20219;&#21153;&#35757;&#32451;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880; DR.BENCH &#30340;&#38382;&#39064;&#24635;&#32467;&#20219;&#21153;&#65288;Gao &#31561;&#65292;2023&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20020;&#24202;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20248;&#20110;&#20854;&#19968;&#33324;&#39046;&#22495;&#30340;&#23545;&#24212;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292; ROUGE-L &#24471;&#20998;&#20026; 28.55&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#22312;&#20248;&#21270;&#20020;&#24202;&#35786;&#26029;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative artificial intelligence (AI) is a promising direction for augmenting clinical diagnostic decision support and reducing diagnostic errors, a leading contributor to medical errors. To further the development of clinical AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a comprehensive generative AI framework, comprised of six tasks representing key components in clinical reasoning. We present a comparative analysis of in-domain versus out-of-domain language models as well as multi-task versus single task training with a focus on the problem summarization task in DR.BENCH (Gao et al., 2023). We demonstrate that a multi-task, clinically trained language model outperforms its general domain counterpart by a large margin, establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55. This research underscores the value of domain-specific training for optimizing clinical diagnostic reasoning tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#32858;&#31867;&#33258;&#20030;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#20351;&#29992;&#31895;&#30053;&#27880;&#37322;&#21644;&#26144;&#23556;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#32454;&#21270;&#31243;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04544</link><description>&lt;p&gt;
&#23545;&#27604;&#33258;&#20030;&#29992;&#20110;&#26631;&#31614;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Contrastive Bootstrapping for Label Refinement. (arXiv:2306.04544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#32858;&#31867;&#33258;&#20030;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#20351;&#29992;&#31895;&#30053;&#27880;&#37322;&#21644;&#26144;&#23556;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#32454;&#21270;&#31243;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25991;&#26412;&#20998;&#31867;&#36890;&#24120;&#23558;&#25991;&#26412;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#31895;&#31890;&#24230;&#31867;&#21035;&#20013;&#65292;&#30001;&#27492;&#29983;&#25104;&#30340;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#20013;&#23450;&#26399;&#20986;&#29616;&#30340;&#26356;&#31934;&#32454;&#30340;&#31867;&#21035;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#26381;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20165;&#20351;&#29992;&#31895;&#31890;&#24230;&#31867;&#21035;&#30340;&#27880;&#37322;&#21644;&#20174;&#31895;&#21040;&#32454;&#30340;&#26144;&#23556;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#31867;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#23545;&#27604;&#32858;&#31867;&#33258;&#20030;&#26041;&#27861;&#65292;&#26469;&#36845;&#20195;&#22320;&#20248;&#21270;&#27573;&#33853;&#30340;&#26631;&#31614;&#12290;&#22312;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#23427;&#20174;&#20840;&#23616;&#21644;&#26412;&#22320;&#30340;&#35282;&#24230;&#19979;&#38754;&#26397;&#36127;&#26679;&#26412;-&#21407;&#22411;&#32452;&#20043;&#38388;&#20445;&#25345;&#30456;&#23545;&#36317;&#31163;&#12290;&#22312; NYT &#21644; 20News &#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional text classification typically categorizes texts into pre-defined coarse-grained classes, from which the produced models cannot handle the real-world scenario where finer categories emerge periodically for accurate services. In this work, we investigate the setting where fine-grained classification is done only using the annotation of coarse-grained categories and the coarse-to-fine mapping. We propose a lightweight contrastive clustering-based bootstrapping method to iteratively refine the labels of passages. During clustering, it pulls away negative passage-prototype pairs under the guidance of the mapping from both global and local perspectives. Experiments on NYT and 20News show that our method outperforms the state-of-the-art methods by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#21482;&#26377;&#24102;&#26631;&#31614;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#28982;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#19979;&#30028;&#21644;&#19968;&#20010;&#19978;&#30028;&#26469;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04539</link><description>&lt;p&gt;
&#26080;&#26631;&#35760;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#20445;&#35777;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications. (arXiv:2306.04539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#21482;&#26377;&#24102;&#26631;&#31614;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#28982;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#19979;&#30028;&#21644;&#19968;&#20010;&#19978;&#30028;&#26469;&#37327;&#21270;&#22810;&#27169;&#24577;&#20132;&#20114;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20849;&#21516;&#23398;&#20064;&#22810;&#20010;&#27169;&#24577;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#19968;&#20010;&#26680;&#24515;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#29702;&#35299;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26412;&#36136;&#65306;&#22312;&#20174;&#20004;&#20010;&#37117;&#27809;&#26377;&#30340;&#27169;&#24577;&#23398;&#20064;&#26102;&#20986;&#29616;&#20102;&#26032;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#30740;&#31350;&#36825;&#19968;&#20132;&#20114;&#37327;&#21270;&#30340;&#25361;&#25112;&#65292;&#21482;&#20351;&#29992;&#24102;&#26631;&#31614;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#21644;&#33258;&#28982;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#26080;&#26631;&#31614;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#65292;&#35270;&#39057;&#21644;&#30456;&#24212;&#30340;&#38899;&#39057;&#65289;&#12290;&#21033;&#29992;&#31934;&#30830;&#30340;&#20449;&#24687;&#35770;&#20132;&#20114;&#23450;&#20041;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25512;&#23548;&#19979;&#30028;&#21644;&#19978;&#30028;&#65292;&#37327;&#21270;&#36825;&#31181;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#37327;&#21644;&#21333;&#29420;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#30340;&#20004;&#20010;&#19979;&#30028;&#65292;&#24182;&#36890;&#36807;&#36830;&#25509;&#21040;&#36817;&#20284;&#31639;&#27861;&#26469;&#25512;&#23548;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24515;&#29702;&#35821;&#35328;&#23398;&#26041;&#27861;&#35780;&#20272;&#20102;&#26469;&#33258; chatGPT &#21644;&#20154;&#31867;&#30340;&#38271;&#24418;&#27604;&#21947;&#65292;&#21457;&#29616; chatGPT &#29983;&#25104;&#30340;&#27604;&#21947;&#32570;&#20047;&#20154;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04537</link><description>&lt;p&gt;
ChatGPT &#29983;&#25104;&#30340;&#38271;&#24418;&#27604;&#21947;&#32570;&#20047;&#31867;&#20154;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-form analogies generated by chatGPT lack human-like psycholinguistic properties. (arXiv:2306.04537v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24515;&#29702;&#35821;&#35328;&#23398;&#26041;&#27861;&#35780;&#20272;&#20102;&#26469;&#33258; chatGPT &#21644;&#20154;&#31867;&#30340;&#38271;&#24418;&#27604;&#21947;&#65292;&#21457;&#29616; chatGPT &#29983;&#25104;&#30340;&#27604;&#21947;&#32570;&#20047;&#20154;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#35821;&#35328;&#23398;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36755;&#20986;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30340;&#25163;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#29992;&#20110;&#34920;&#24449;LLM&#36755;&#20986;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24615;&#65292;&#24182;&#35828;&#26126;LLM&#22312;&#21738;&#20123;&#39046;&#22495;&#19982;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#30456;&#27604;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#25991;&#22312;&#29983;&#29289;&#21270;&#23398;&#27010;&#24565;&#30340;&#38271;&#24418;&#27604;&#21947;&#20013;&#24212;&#29992;&#24515;&#29702;&#35821;&#35328;&#23398;&#26041;&#27861;&#26469;&#35780;&#20272;&#21333;&#20010;&#21477;&#23376;&#12290;&#25105;&#20204;&#23558;&#20837;&#38376;&#29983;&#29289;&#21270;&#23398;&#35838;&#31243;&#20013;&#30340;&#20154;&#31867;&#23545;&#35937;&#29983;&#25104;&#30340;&#27604;&#21947;&#19982;chatGPT&#29983;&#25104;&#30340;&#27604;&#21947;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;Coh-metrix&#20013;&#25552;&#21462;&#30340;78&#20010;&#29305;&#24449;&#36827;&#34892;&#30417;&#30563;&#20998;&#31867;&#20998;&#26512;&#65292;&#20998;&#26512;&#25991;&#26412;&#30340;&#36830;&#36143;&#24615;&#65292;&#35821;&#35328;&#21644;&#21487;&#35835;&#24615;&#65288;Graesser&#31561;&#65292;2004&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20998;&#31867;&#23398;&#29983;&#29983;&#25104;&#30340;&#27604;&#21947;&#21644;chatGPT&#29983;&#25104;&#30340;&#27604;&#21947;&#30340;&#24615;&#33021;&#24456;&#39640;&#12290;&#20026;&#20102;&#35780;&#20272;&#21738;&#20123;&#29305;&#24449;&#23545;&#27169;&#22411;&#24615;&#33021;&#20570;&#20986;&#20102;&#26368;&#22823;&#36129;&#29486;&#65292;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#32858;&#31867;&#26041;&#27861;&#12290;&#20174;&#36825;&#20010;&#20998;&#26512;&#20013;&#24471;&#20986;&#30340;&#32467;&#26524;&#35828;&#26126;&#20102;chatGPT&#29983;&#25104;&#30340;&#27604;&#21947;&#32570;&#20047;&#20154;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Psycholinguistic analyses provide a means of evaluating large language model (LLM) output and making systematic comparisons to human-generated text. These methods can be used to characterize the psycholinguistic properties of LLM output and illustrate areas where LLMs fall short in comparison to human-generated text. In this work, we apply psycholinguistic methods to evaluate individual sentences from long-form analogies about biochemical concepts. We compare analogies generated by human subjects enrolled in introductory biochemistry courses to analogies generated by chatGPT. We perform a supervised classification analysis using 78 features extracted from Coh-metrix that analyze text cohesion, language, and readability (Graesser et. al., 2004). Results illustrate high performance for classifying student-generated and chatGPT-generated analogies. To evaluate which features contribute most to model performance, we use a hierarchical clustering approach. Results from this analysis illustr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#25506;&#27979;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120; (DST) &#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#22686;&#24378; DST &#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04535</link><description>&lt;p&gt;
PromptAttack: &#20351;&#29992;&#23545;&#25239;&#24615;&#25552;&#31034;&#25506;&#27979;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts. (arXiv:2306.04535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#25506;&#27979;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120; (DST) &#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#22686;&#24378; DST &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120; (DST)&#65292;&#23427;&#21487;&#20197;&#24314;&#27169;&#29992;&#25143;&#30340;&#30446;&#26631;&#21644;&#38656;&#27714;&#12290;&#20026;&#20102;&#24314;&#31435;&#26356;&#21152;&#24378;&#20581;&#21644;&#21487;&#38752;&#30340; DST&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#25506;&#27979; DST &#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#26377;&#20004;&#20010;&#20851;&#38190;&#29305;&#28857;&#65306; (i) &#21482;&#38656;&#35201; DST &#30340;&#36755;&#20986;&#65292;&#26080;&#38656;&#27169;&#22411;&#21442;&#25968;&#65292; (ii) &#23427;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#21487;&#20197;&#38024;&#23545;&#20219;&#20309; DST &#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340; DST &#36827;&#34892;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20445;&#25345;&#33391;&#22909;&#27969;&#30021;&#24615;&#21644;&#20302;&#25200;&#21160;&#27604;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#30340;&#20934;&#30830;&#24230;&#19979;&#38477;&#21644;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#33258;&#21160;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#22914;&#20309;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#22686;&#24378; DST&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340; DST &#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#20063;&#20026;&#25345;&#32493;&#25913;&#36827;&#30041;&#19979;&#20102;&#24320;&#25918;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key component of modern conversational systems is the Dialogue State Tracker (or DST), which models a user's goals and needs. Toward building more robust and reliable DSTs, we introduce a prompt-based learning approach to automatically generate effective adversarial examples to probe DST models. Two key characteristics of this approach are: (i) it only needs the output of the DST with no need for model parameters, and (ii) it can learn to generate natural language utterances that can target any DST. Through experiments over state-of-the-art DSTs, the proposed framework leads to the greatest reduction in accuracy and the best attack success rate while maintaining good fluency and a low perturbation ratio. We also show how much the generated adversarial examples can bolster a DST through adversarial training. These results indicate the strength of prompt-based attacks on DSTs and leave open avenues for continued refinement.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23485;&#26494;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#33021;&#22815;&#26356;&#30495;&#23454;&#22320;&#35780;&#20272;&#26085;&#35821;&#35821;&#38899;&#35782;&#21035;&#65292;&#35299;&#20915;&#20102;&#26085;&#35821;&#32570;&#20047;&#25340;&#20889;&#35268;&#33539;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04530</link><description>&lt;p&gt;
&#26085;&#35821;&#35821;&#38899;&#35782;&#21035;&#30340;&#23485;&#26494;&#35780;&#20272;:&#24314;&#27169;&#33258;&#28982;&#20986;&#29616;&#30340;&#25340;&#20889;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Lenient Evaluation of Japanese Speech Recognition: Modeling Naturally Occurring Spelling Inconsistency. (arXiv:2306.04530v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23485;&#26494;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#33021;&#22815;&#26356;&#30495;&#23454;&#22320;&#35780;&#20272;&#26085;&#35821;&#35821;&#38899;&#35782;&#21035;&#65292;&#35299;&#20915;&#20102;&#26085;&#35821;&#32570;&#20047;&#25340;&#20889;&#35268;&#33539;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#35823;&#29575;&#65288;WER&#65289;&#21644;&#23383;&#31526;&#35823;&#29575;&#65288;CER&#65289;&#26159;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#30340;&#26631;&#20934;&#25351;&#26631;&#65292;&#20294;&#26159;&#20854;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#22987;&#32456;&#26159;&#26367;&#20195;&#25340;&#20889;&#65306;&#22914;&#26524;&#19968;&#20010;&#31995;&#32479;&#23558;adviser&#36716;&#24405;&#20026;advisor&#65292;&#37027;&#20040;&#21363;&#20351;&#36825;&#20004;&#20010;&#25340;&#20889;&#23454;&#38469;&#19978;&#20195;&#34920;&#30456;&#21516;&#30340;&#21333;&#35789;&#65292;&#23427;&#20063;&#20250;&#34987;&#35745;&#20026;&#38169;&#35823;&#12290;&#26085;&#35821;&#20197;&#8220;&#32570;&#20047;&#25340;&#20889;&#35268;&#33539;&#8221;&#32780;&#38395;&#21517;&#65306;&#22823;&#22810;&#25968;&#21333;&#35789;&#21487;&#20197;&#29992;&#22810;&#31181;&#26041;&#24335;&#25340;&#20889;&#65292;&#36825;&#23545;&#20934;&#30830;&#30340;ASR&#35780;&#20272;&#26500;&#25104;&#20102;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23485;&#26494;&#35780;&#20272;&#24230;&#37327;&#20316;&#20026;&#26356;&#21487;&#38752;&#30340;&#26085;&#35821;ASR CER&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#20351;&#29992;&#35789;&#27719;&#36164;&#28304;&#12289;&#26085;&#35821;&#25991;&#26412;&#22788;&#29702;&#31995;&#32479;&#21644;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21019;&#24314;&#21442;&#32771;&#36716;&#24405;&#30340;&#21487;&#33021;&#37325;&#25340;&#26500;&#25104;&#30340;&#22270;&#65292;&#29992;&#20110;&#23558;&#20551;&#21517;&#25110;&#29255;&#20551;&#21517;&#37325;&#24314;&#25104;&#27721;&#23383;&#12290;&#22312;&#25163;&#21160;&#35780;&#20272;&#20013;&#65292;&#35780;&#20272;&#20154;&#21592;&#23558;95.4%&#30340;&#25340;&#20889;&#21464;&#20307;&#35780;&#20026;&#21487;&#20449;&#12290;ASR&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20250;&#24809;&#32602;&#31995;&#32479;&#36873;&#25321;&#26377;&#25928;&#20294;&#38750;&#26631;&#20934;&#30340;&#25340;&#20889;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#26085;&#35821;&#35821;&#38899;&#35782;&#21035;&#30340;&#26356;&#30495;&#23454;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word error rate (WER) and character error rate (CER) are standard metrics in Speech Recognition (ASR), but one problem has always been alternative spellings: If one's system transcribes adviser whereas the ground truth has advisor, this will count as an error even though the two spellings really represent the same word.  Japanese is notorious for ``lacking orthography'': most words can be spelled in multiple ways, presenting a problem for accurate ASR evaluation. In this paper we propose a new lenient evaluation metric as a more defensible CER measure for Japanese ASR. We create a lattice of plausible respellings of the reference transcription, using a combination of lexical resources, a Japanese text-processing system, and a neural machine translation model for reconstructing kanji from hiragana or katakana. In a manual evaluation, raters rated 95.4% of the proposed spelling variants as plausible. ASR results show that our method, which does not penalize the system for choosing a vali
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#24503;&#35821;&#35821;&#24207;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#23545;&#25239;&#24615;&#25968;&#25454;&#38598;WOGLI&#65292;&#35777;&#26126;&#20102;&#22312;&#24403;&#21069;&#32763;&#35793;&#21518;&#30340;NLI&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#24503;&#35821;&#33258;&#32534;&#30721;&#27169;&#22411;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#24046;&#65292;&#38656;&#35201;&#20351;&#29992;&#24418;&#24577;&#26631;&#35760;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.04523</link><description>&lt;p&gt;
&#24403;&#21069;&#30340;NLI&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#24503;&#35821;&#35821;&#24207;&#21527;&#65311;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#20010;&#26032;&#30340;&#24503;&#35821;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65288;arXiv:2306.04523v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Can current NLI systems handle German word order? Investigating language model performance on a new German challenge set of minimal pairs. (arXiv:2306.04523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#24503;&#35821;&#35821;&#24207;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#23545;&#25239;&#24615;&#25968;&#25454;&#38598;WOGLI&#65292;&#35777;&#26126;&#20102;&#22312;&#24403;&#21069;&#32763;&#35793;&#21518;&#30340;NLI&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#24503;&#35821;&#33258;&#32534;&#30721;&#27169;&#22411;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#24046;&#65292;&#38656;&#35201;&#20351;&#29992;&#24418;&#24577;&#26631;&#35760;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#33521;&#35821;&#30456;&#27604;&#65292;&#24503;&#35821;&#30340;&#35821;&#24207;&#26356;&#20026;&#33258;&#30001;&#65292;&#22240;&#27492;&#32473;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;(NLI)&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;WOGLI(&#24503;&#35821;&#35821;&#24207;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#23545;&#25239;&#24615;&#25968;&#25454;&#38598;)&#65292;&#26159;&#31532;&#19968;&#20010;&#25317;&#26377;&#20197;&#19979;&#29305;&#24615;&#30340;&#24503;&#35821;&#23545;&#25239;&#24615;NLI&#25968;&#25454;&#38598;&#65306;(i)&#27599;&#20010;&#21069;&#25552;&#37117;&#26377;&#19968;&#20010;&#34164;&#21547;&#21644;&#19968;&#20010;&#38750;&#34164;&#21547;&#30340;&#20551;&#35774;&#65307;(ii)&#21069;&#25552;&#21644;&#20551;&#35774;&#20165;&#22312;&#35821;&#24207;&#21644;&#24517;&#35201;&#30340;&#24418;&#24577;&#21464;&#21270;&#20197;&#26631;&#35760;&#26684;&#21644;&#25968;&#26041;&#38754;&#26377;&#25152;&#19981;&#21516;&#12290;&#29305;&#21035;&#22320;&#65292;&#27599;&#20010;&#21069;&#25552;&#21644;&#20854;&#20004;&#20010;&#20551;&#35774;&#21253;&#21547;&#23436;&#20840;&#30456;&#21516;&#30340;&#35789;&#26681;&#12290;&#25105;&#20204;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#35201;&#27714;&#27169;&#22411;&#20351;&#29992;&#24418;&#24577;&#26631;&#35760;&#26469;&#35782;&#21035;&#25110;&#25298;&#32477;&#34164;&#21547;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30446;&#21069;&#22312;&#32763;&#35793;&#21518;&#30340;NLI&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#24503;&#35821;&#33258;&#32534;&#30721;&#27169;&#22411;&#22312;&#36825;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#20250;&#36935;&#21040;&#22256;&#38590;&#65292;&#21453;&#26144;&#20102;&#32763;&#35793;&#21518;&#30340;NLI&#25968;&#25454;&#38598;&#19981;&#33021;&#21453;&#26144;&#30446;&#26631;&#35821;&#35328;&#20013;&#25152;&#26377;&#24517;&#35201;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;&#25105;&#20204;&#36824;&#22312;&#25968;&#25454;&#22686;&#24378;&#21518;&#21644;&#30456;&#20851;&#30340;&#35821;&#24207;&#29616;&#35937;&#19978;&#26816;&#26597;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to English, German word order is freer and therefore poses additional challenges for natural language inference (NLI). We create WOGLI (Word Order in German Language Inference), the first adversarial NLI dataset for German word order that has the following properties: (i) each premise has an entailed and a non-entailed hypothesis; (ii) premise and hypotheses differ only in word order and necessary morphological changes to mark case and number. In particular, each premise andits two hypotheses contain exactly the same lemmata. Our adversarial examples require the model to use morphological markers in order to recognise or reject entailment. We show that current German autoencoding models fine-tuned on translated NLI data can struggle on this challenge set, reflecting the fact that translated NLI datasets will not mirror all necessary language phenomena in the target language. We also examine performance after data augmentation as well as on related word order phenomena derived 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31572;&#26696;&#21453;&#39304;&#26469;&#22686;&#24378;&#22810;&#36328;&#24230;&#38382;&#31572;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20256;&#32479;&#30340;&#28436;&#31034;&#31034;&#20363;&#25193;&#23637;&#20102;&#21453;&#39304;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04508</link><description>&lt;p&gt;
&#36890;&#36807;&#31572;&#26696;&#21453;&#39304;&#22686;&#24378;&#22810;&#36328;&#24230;&#38382;&#31572;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering. (arXiv:2306.04508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31572;&#26696;&#21453;&#39304;&#26469;&#22686;&#24378;&#22810;&#36328;&#24230;&#38382;&#31572;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20256;&#32479;&#30340;&#28436;&#31034;&#31034;&#20363;&#25193;&#23637;&#20102;&#21453;&#39304;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36890;&#29992;&#24615;&#33021;&#21147;&#65292;&#20294;&#22312;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#22810;&#36328;&#24230;&#38382;&#31572;&#65289;&#19978;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#28436;&#31034;&#31034;&#20363;&#26500;&#24314;&#23569;&#37327;&#36861;&#38382;&#25552;&#31034;&#65292;&#21487;&#20197;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26377;&#25928;&#22320;&#21033;&#29992;LLM&#12290;&#19968;&#31181;&#27969;&#34892;&#30340;&#23454;&#29616;&#26041;&#24335;&#26159;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#26495;&#23558;&#20960;&#20010;&#38382;&#39064;&#21450;&#20854;&#27491;&#30830;&#31572;&#26696;&#36827;&#34892;&#36830;&#25509;&#65292;&#36890;&#30693;LLM&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#20043;&#20063;&#25552;&#20379;&#23545;LLM&#19968;&#20123;&#19981;&#26399;&#26395;&#30340;&#36755;&#20986;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#25193;&#23637;&#28436;&#31034;&#31034;&#20363;&#20197;&#21453;&#39304;&#39044;&#27979;&#27169;&#22411;&#39044;&#27979;&#30340;&#31572;&#26696;&#65292;&#20363;&#22914;&#65292;&#27491;&#30830;&#12289;&#19981;&#27491;&#30830;&#25110;&#19981;&#23436;&#25972;&#12290;&#22312;&#19977;&#20010;&#22810;&#36328;&#24230;&#38382;&#31572;&#25968;&#25454;&#38598;&#20197;&#21450;&#19968;&#20010;&#20851;&#38190;&#35789;&#25552;&#21462;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#25552;&#31034;&#31574;&#30053;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whereas the recent emergence of large language models (LLMs) like ChatGPT has exhibited impressive general performance, it still has a large gap with fully-supervised models on specific tasks such as multi-span question answering. Previous researches found that in-context learning is an effective approach to exploiting LLM, by using a few task-related labeled data as demonstration examples to construct a few-shot prompt for answering new questions. A popular implementation is to concatenate a few questions and their correct answers through simple templates, informing LLM of the desired output. In this paper, we propose a novel way of employing labeled data such that it also informs LLM of some undesired output, by extending demonstration examples with feedback about answers predicted by an off-the-shelf model, e.g., correct, incorrect, or incomplete. Experiments on three multi-span question answering datasets as well as a keyphrase extraction dataset show that our new prompting strateg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#35757;&#32451;&#26679;&#26412;&#36739;&#23567;&#26102;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#30001;&#27492;&#34920;&#26126;ChatGPT&#20855;&#26377;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25104;&#20026;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04504</link><description>&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#35780;&#20272;ChatGPT&#65306;&#19982;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#30340;&#38646;&#26679;&#20363;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#35757;&#32451;&#26679;&#26412;&#36739;&#23567;&#26102;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#30001;&#27492;&#34920;&#26126;ChatGPT&#20855;&#26377;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25104;&#20026;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23578;&#26410;&#30740;&#31350;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#21508;&#31181;&#22522;&#20934;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#22914;&#20851;&#31995;&#25552;&#21462;&#12289;&#25991;&#26723;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#25688;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23545;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#24037;&#20316;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#22914;BioGPT&#21644;BioBART&#12290;&#36825;&#34920;&#26126;ChatGPT&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#39044;&#35757;&#32451;&#20351;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#30456;&#24403;&#30340;&#19987;&#19994;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#25104;&#20026;&#21508;&#31181;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#20221;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#32508;&#36848;&#65292;&#36890;&#36807;&#20998;&#31867;&#35752;&#35770;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#65292;&#24182;&#32508;&#21512;&#35780;&#36848;&#20102;&#30456;&#20851;&#30340;&#37327;&#21270;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.04459</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;: &#26469;&#28304;&#12289;&#37327;&#21270;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Uncertainty in Natural Language Processing: Sources, Quantification, and Applications. (arXiv:2306.04459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#20221;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#32508;&#36848;&#65292;&#36890;&#36807;&#20998;&#31867;&#35752;&#35770;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#65292;&#24182;&#32508;&#21512;&#35780;&#36848;&#20102;&#30456;&#20851;&#30340;&#37327;&#21270;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#30340;&#20027;&#35201;&#39046;&#22495;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#24050;&#32463;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#22823;&#37327;&#30340; NLP &#20219;&#21153;&#24050;&#32463;&#20197;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#24335;&#24471;&#21040;&#35299;&#20915;&#65292;&#21508;&#31181;&#20219;&#21153;&#36890;&#36807;&#20849;&#20139;&#30456;&#21516;&#30340;&#33539;&#20363;&#30456;&#20114;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#26159;&#40657;&#21283;&#23376;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#27010;&#29575;&#35745;&#31639;&#12290;&#29359;&#38169;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24615;&#65288;&#25442;&#21477;&#35805;&#35828;&#65292;&#19981;&#30830;&#23450;&#24615;&#65289;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#22312;&#38477;&#20302;&#27169;&#22411;&#39118;&#38505;&#21644;&#20570;&#20986;&#26356;&#22909;&#20915;&#31574;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#27425;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#32508;&#21512;&#35780;&#36848;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19982;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#30340;&#24037;&#20316;&#12290;&#22522;&#20110;&#25968;&#25454;&#21644;&#33539;&#20363;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#33258;&#28982;&#35821;&#35328;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65292;&#21253;&#25324;&#36755;&#20837;&#12289;&#31995;&#32479;&#21644;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#21644;&#20851;&#38190;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
As a main field of artificial intelligence, natural language processing (NLP) has achieved remarkable success via deep neural networks. Plenty of NLP tasks have been addressed in a unified manner, with various tasks being associated with each other through sharing the same paradigm. However, neural networks are black boxes and rely on probability computation. Making mistakes is inevitable. Therefore, estimating the reliability and trustworthiness (in other words, uncertainty) of neural networks becomes a key research direction, which plays a crucial role in reducing models' risks and making better decisions. Therefore, in this survey, we provide a comprehensive review of uncertainty-relevant works in the NLP field. Considering the data and paradigms characteristics, we first categorize the sources of uncertainty in natural language into three types, including input, system, and output. Then, we systemically review uncertainty quantification approaches and the main applications. Finally
&lt;/p&gt;</description></item><item><title>STEPS&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39034;&#24207;&#25512;&#29702;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#39564;&#35777;&#24403;&#21069;&#31070;&#32463;&#27169;&#22411;&#22312;&#39034;&#24207;&#20219;&#21153;&#20013;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#20915;&#39034;&#24207;&#20219;&#21153;&#20013;&#30340;&#24120;&#35782;&#25512;&#29702;&#25361;&#25112;&#38656;&#35201;&#20351;&#29992;LLMs&#30340;&#38646;-shot&#25552;&#31034;&#25110;&#23569;&#37327;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#32780;&#25552;&#31034;&#26041;&#27861;&#20173;&#28982;&#26126;&#26174;&#33853;&#21518;&#20110;&#19978;&#19979;&#25991;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04441</link><description>&lt;p&gt;
STEPS&#65306;&#39034;&#24207;&#20219;&#21153;&#20013;&#39034;&#24207;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
STEPS: A Benchmark for Order Reasoning in Sequential Tasks. (arXiv:2306.04441v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04441
&lt;/p&gt;
&lt;p&gt;
STEPS&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39034;&#24207;&#25512;&#29702;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#39564;&#35777;&#24403;&#21069;&#31070;&#32463;&#27169;&#22411;&#22312;&#39034;&#24207;&#20219;&#21153;&#20013;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#20915;&#39034;&#24207;&#20219;&#21153;&#20013;&#30340;&#24120;&#35782;&#25512;&#29702;&#25361;&#25112;&#38656;&#35201;&#20351;&#29992;LLMs&#30340;&#38646;-shot&#25552;&#31034;&#25110;&#23569;&#37327;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#32780;&#25552;&#31034;&#26041;&#27861;&#20173;&#28982;&#26126;&#26174;&#33853;&#21518;&#20110;&#19978;&#19979;&#25991;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#20154;&#31867;&#27963;&#21160;&#21487;&#20197;&#25277;&#35937;&#20026;&#33258;&#28982;&#25991;&#26412;&#20013;&#30340;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#20363;&#22914;&#28921;&#39274;&#65292;&#32500;&#20462;&#65292;&#21046;&#36896;&#31561;&#12290;&#36825;&#26679;&#30340;&#34892;&#21160;&#24207;&#21015;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25191;&#34892;&#39034;&#24207;&#65292;&#32780;&#24207;&#21015;&#20013;&#30340;&#28151;&#20081;&#20250;&#23548;&#33268;&#26426;&#22120;&#20154;&#25110;AI&#20195;&#29702;&#26080;&#27861;&#36827;&#19968;&#27493;&#25191;&#34892;&#20219;&#21153;&#22833;&#36133;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#39564;&#35777;&#24403;&#21069;&#31070;&#32463;&#27169;&#22411;&#22312;&#39034;&#24207;&#20219;&#21153;&#20013;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21517;&#20026;STEPS&#12290;STEPS&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#35774;&#32622;&#65292;&#20998;&#21035;&#20851;&#27880;&#30830;&#23450;&#32473;&#23450;&#39135;&#35889;&#20013;&#19979;&#19968;&#20010;&#27493;&#39588;&#30340;&#21512;&#29702;&#24615;&#21644;&#20174;&#22810;&#39033;&#36873;&#25321;&#39064;&#20013;&#36873;&#25321;&#21512;&#29702;&#30340;&#27493;&#39588;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25968;&#25454;&#26500;&#24314;&#21644;&#20219;&#21153;&#20844;&#24335;&#65292;&#24182;&#22522;&#20934;&#27979;&#35797;&#20102;&#22823;&#37096;&#20998;&#26174;&#33879;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;&#35299;&#20915;&#39034;&#24207;&#20219;&#21153;&#20013;&#30340;&#24120;&#35782;&#25512;&#29702;&#25361;&#25112;&#38656;&#35201;&#20351;&#29992;LLMs&#30340;&#38646;-shot&#25552;&#31034;&#25110;&#23569;&#37327;&#19978;&#19979;&#25991;&#23398;&#20064;&#65307;2&#65289;&#25552;&#31034;&#26041;&#27861;&#20173;&#28982;&#26126;&#26174;&#33853;&#21518;&#20110;&#19978;&#19979;&#25991;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various human activities can be abstracted into a sequence of actions in natural text, i.e. cooking, repairing, manufacturing, etc. Such action sequences heavily depend on the executing order, while disorder in action sequences leads to failure of further task execution by robots or AI agents. Therefore, to verify the order reasoning capability of current neural models in sequential tasks, we propose a challenging benchmark , named STEPS. STEPS involves two subtask settings, focusing on determining the rationality of given next step in recipes and selecting the reasonable step from the multi-choice question, respectively. We describe the data construction and task formulations, and benchmark most of significant Large Language Models (LLMs). The experimental results demonstrate 1) The commonsense reasoning of action orders in sequential tasks are challenging to resolve via zero-shot prompting or few-shot in-context learning for LLMs; 2) Prompting method still significantly lags behind t
&lt;/p&gt;</description></item><item><title>Zambezi Voice&#26159;&#38024;&#23545;&#36190;&#27604;&#20122;&#35821;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#21547;&#26377;160&#23567;&#26102;&#26080;&#26631;&#31614;&#38899;&#39057;&#21644;80&#23567;&#26102;&#26377;&#26631;&#31614;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#21644;&#22810;&#35821;&#31181;&#35821;&#38899;&#22788;&#29702;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20026;&#36190;&#27604;&#20122;&#35821;&#21019;&#36896;&#30340;&#31532;&#19968;&#20010;&#22810;&#35821;&#31181;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04428</link><description>&lt;p&gt;
Zambezi Voice: &#19968;&#31181;&#36190;&#27604;&#20122;&#35821;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages. (arXiv:2306.04428v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04428
&lt;/p&gt;
&lt;p&gt;
Zambezi Voice&#26159;&#38024;&#23545;&#36190;&#27604;&#20122;&#35821;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#21547;&#26377;160&#23567;&#26102;&#26080;&#26631;&#31614;&#38899;&#39057;&#21644;80&#23567;&#26102;&#26377;&#26631;&#31614;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#21644;&#22810;&#35821;&#31181;&#35821;&#38899;&#22788;&#29702;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20026;&#36190;&#27604;&#20122;&#35821;&#21019;&#36896;&#30340;&#31532;&#19968;&#20010;&#22810;&#35821;&#31181;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; Zambezi Voice&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#36190;&#27604;&#20122;&#35821;&#30340;&#24320;&#28304;&#22810;&#35821;&#31181;&#35821;&#38899;&#36164;&#28304;&#12290;&#23427;&#21253;&#21547;&#20004;&#20010;&#25968;&#25454;&#38598;&#21512;&#65306;&#19968;&#32452;&#26080;&#26631;&#31614;&#30340;&#25910;&#38899;&#26426;&#26032;&#38395;&#21644;&#35848;&#35805;&#33410;&#30446;&#30340;&#38899;&#39057;&#24405;&#38899;&#65288;160&#23567;&#26102;&#65289;&#65292;&#20197;&#21450;&#19968;&#32452;&#34987;&#26631;&#27880;&#30340;&#25968;&#25454;&#65288;&#36229;&#36807;80&#23567;&#26102;&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#20174;&#20844;&#20849;&#21487;&#24471;&#30340;&#25991;&#23398;&#20070;&#31821;&#20013;&#28304;&#33258;&#30340;&#25991;&#26412;&#30340;&#26391;&#35835;&#35821;&#38899;&#12290;&#35813;&#25968;&#25454;&#38598;&#34987;&#21019;&#24314;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#65292;&#20294;&#21487;&#20197;&#25193;&#23637;&#21040;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#22788;&#29702;&#30740;&#31350;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#20026;&#36190;&#27604;&#20122;&#35821;&#21019;&#36896;&#30340;&#31532;&#19968;&#20010;&#22810;&#35821;&#31181;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;Wav2Vec2.0&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21033;&#29992;&#39044;&#35757;&#32451;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#26469;&#26500;&#24314;&#22522;&#32447;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#20844;&#20849;&#39046;&#22495;&#19979;&#21457;&#24067;&#65292;&#24182;&#21487;&#36890;&#36807;&#39033;&#30446;&#20195;&#30721;&#24211;&#36827;&#34892;&#35775;&#38382;&#12290;&#35831;&#21442;&#35265; https://github.com/unza-speech-lab/zambezi-v&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces Zambezi Voice, an open-source multilingual speech resource for Zambian languages. It contains two collections of datasets: unlabelled audio recordings of radio news and talk shows programs (160 hours) and labelled data (over 80 hours) consisting of read speech recorded from text sourced from publicly available literature books. The dataset is created for speech recognition but can be extended to multilingual speech processing research for both supervised and unsupervised learning approaches. To our knowledge, this is the first multilingual speech dataset created for Zambian languages. We exploit pretraining and cross-lingual transfer learning by finetuning the Wav2Vec2.0 large-scale multilingual pre-trained model to build end-to-end (E2E) speech recognition models for our baseline models. The dataset is released publicly under a Creative Commons BY-NC-ND 4.0 license and can be accessed through the project repository. See https://github.com/unza-speech-lab/zambezi-v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#35266;&#28857;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#25506;&#31350;&#24847;&#35265;&#24635;&#32467;&#20013;&#30340;&#20559;&#35265;&#65292;&#36890;&#36807;&#23545;COVID-19&#19979;&#19977;&#20010;&#26377;&#20105;&#35758;&#30340;&#35758;&#39064;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#24847;&#35265;&#30456;&#20284;&#24230;&#30340;&#39640;&#24230;&#24182;&#19981;&#24847;&#21619;&#30528;&#24456;&#22909;&#30340;&#22810;&#26679;&#24615;&#25110;&#20844;&#24179;&#22320;&#28085;&#30422;&#21407;&#22987;&#25991;&#26723;&#20013;&#21576;&#29616;&#30340;&#21508;&#31181;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.04424</link><description>&lt;p&gt;
&#36890;&#36807;&#35266;&#28857;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#26816;&#39564;&#24847;&#35265;&#24635;&#32467;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Examining Bias in Opinion Summarisation Through the Perspective of Opinion Diversity. (arXiv:2306.04424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#35266;&#28857;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#25506;&#31350;&#24847;&#35265;&#24635;&#32467;&#20013;&#30340;&#20559;&#35265;&#65292;&#36890;&#36807;&#23545;COVID-19&#19979;&#19977;&#20010;&#26377;&#20105;&#35758;&#30340;&#35758;&#39064;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#24847;&#35265;&#30456;&#20284;&#24230;&#30340;&#39640;&#24230;&#24182;&#19981;&#24847;&#21619;&#30528;&#24456;&#22909;&#30340;&#22810;&#26679;&#24615;&#25110;&#20844;&#24179;&#22320;&#28085;&#30422;&#21407;&#22987;&#25991;&#26723;&#20013;&#21576;&#29616;&#30340;&#21508;&#31181;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#35265;&#24635;&#32467;&#26159;&#19968;&#39033;&#26088;&#22312;&#27010;&#25324;&#28304;&#25991;&#26723;&#20013;&#21576;&#29616;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#26680;&#24515;&#20449;&#24687;&#21644;&#35266;&#28857;&#30340;&#20219;&#21153;&#12290;&#20165;&#20195;&#34920;&#22823;&#22810;&#25968;&#35266;&#28857;&#30340;&#25688;&#35201;&#23558;&#20351;&#23569;&#25968;&#35266;&#28857;&#26410;&#34987;&#25688;&#35201;&#21253;&#21547;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23545;&#19968;&#20010;&#29305;&#23450;&#30446;&#26631;&#30340;&#31435;&#22330;&#20316;&#20026;&#19968;&#31181;&#35266;&#28857;&#12290;&#25105;&#20204;&#20174;&#35266;&#28857;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#24847;&#35265;&#24635;&#32467;&#20013;&#30340;&#20559;&#35265;&#65292;&#35813;&#22810;&#26679;&#24615;&#27979;&#37327;&#27169;&#22411;&#29983;&#25104;&#30340;&#24635;&#32467;&#26159;&#21542;&#33021;&#28085;&#30422;&#22810;&#26679;&#30340;&#35266;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35266;&#28857;&#30456;&#20284;&#24615;&#65292;&#21363;&#20004;&#31181;&#35266;&#28857;&#22312;&#23545;&#26576;&#20010;&#20027;&#39064;&#30340;&#31435;&#22330;&#26041;&#38754;&#26377;&#22810;&#23494;&#20999;&#30456;&#20851;&#65292;&#20197;&#21450;&#20854;&#19982;&#35266;&#28857;&#22810;&#26679;&#24615;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;COVID-19&#19979;&#19977;&#20010;&#26377;&#20105;&#35758;&#30340;&#35758;&#39064;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#31435;&#22330;&#30340;&#35282;&#24230;&#26469;&#32771;&#23519;&#35266;&#28857;&#22810;&#26679;&#24615;&#21644;&#30456;&#20284;&#24615;&#12290;&#36825;&#20123;&#35805;&#39064;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24847;&#35265;&#30456;&#20284;&#24230;&#30340;&#39640;&#24230;&#24182;&#19981;&#24847;&#21619;&#30528;&#24456;&#22909;&#30340;&#22810;&#26679;&#24615;&#25110;&#20844;&#24179;&#22320;&#28085;&#30422;&#21407;&#22987;&#25991;&#26723;&#20013;&#21576;&#29616;&#30340;&#21508;&#31181;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinion summarisation is a task that aims to condense the information presented in the source documents while retaining the core message and opinions. A summary that only represents the majority opinions will leave the minority opinions unrepresented in the summary. In this paper, we use the stance towards a certain target as an opinion. We study bias in opinion summarisation from the perspective of opinion diversity, which measures whether the model generated summary can cover a diverse set of opinions. In addition, we examine opinion similarity, a measure of how closely related two opinions are in terms of their stance on a given topic, and its relationship with opinion diversity. Through the lens of stances towards a topic, we examine opinion diversity and similarity using three debatable topics under COVID-19. Experimental results on these topics revealed that a higher degree of similarity of opinions did not indicate good diversity or fairly cover the various opinions originally p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;Transfer learning&#30340;&#26041;&#27861;&#65292;&#23558;&#25463;&#20811;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#21040;&#26031;&#27931;&#20240;&#20811;&#35821;&#30340;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22312;&#26031;&#27931;&#20240;&#20811;&#35821;&#20219;&#21153;&#20013;&#30340;&#36739;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.04399</link><description>&lt;p&gt;
&#20174;&#25463;&#20811;&#35821;&#21040;&#26031;&#27931;&#20240;&#20811;&#35821;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning of Transformer-based Speech Recognition Models from Czech to Slovak. (arXiv:2306.04399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;Transfer learning&#30340;&#26041;&#27861;&#65292;&#23558;&#25463;&#20811;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#21040;&#26031;&#27931;&#20240;&#20811;&#35821;&#30340;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22312;&#26031;&#27931;&#20240;&#20811;&#35821;&#20219;&#21153;&#20013;&#30340;&#36739;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;Transformer&#32467;&#26500;&#30340;&#20960;&#31181;&#26031;&#27931;&#20240;&#20811;&#35821;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20174;&#29616;&#26377;&#25463;&#20811;&#35821;&#39044;&#35757;&#32451;&#30340;Wav2Vec 2.0&#27169;&#22411;&#21521;&#26031;&#27931;&#20240;&#20811;&#35821;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#26031;&#27931;&#20240;&#20811;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24320;&#22987;&#26102;&#65292;&#23558;&#25463;&#20811;&#35821;&#27169;&#22411;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21040;&#26031;&#27931;&#20240;&#20811;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26368;&#20339;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25463;&#20811;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25152;&#23384;&#20648;&#30340;&#30693;&#35782;&#21487;&#20197;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#26031;&#27931;&#20240;&#20811;&#35821;&#20219;&#21153;&#20013;&#65292;&#29978;&#33267;&#32988;&#36807;&#26356;&#22823;&#12289;&#20844;&#20849;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we are comparing several methods of training the Slovak speech recognition models based on the Transformers architecture. Specifically, we are exploring the approach of transfer learning from the existing Czech pre-trained Wav2Vec 2.0 model into Slovak. We are demonstrating the benefits of the proposed approach on three Slovak datasets. Our Slovak models scored the best results when initializing the weights from the Czech model at the beginning of the pre-training phase. Our results show that the knowledge stored in the Cezch pre-trained model can be successfully reused to solve tasks in Slovak while outperforming even much larger public multilingual models.
&lt;/p&gt;</description></item><item><title>M$^3$IT&#25968;&#25454;&#38598;&#26088;&#22312;&#20248;&#21270;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#19982;&#20154;&#31867;&#25351;&#20196;&#30340;&#23545;&#40784;&#65292;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#31181;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04387</link><description>&lt;p&gt;
M$^3$IT: &#22810;&#27169;&#24577;&#22810;&#35821;&#31181;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning. (arXiv:2306.04387v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04387
&lt;/p&gt;
&lt;p&gt;
M$^3$IT&#25968;&#25454;&#38598;&#26088;&#22312;&#20248;&#21270;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#19982;&#20154;&#31867;&#25351;&#20196;&#30340;&#23545;&#40784;&#65292;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#31181;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#26174;&#33879;&#25512;&#36827;&#20102;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#25351;&#20196;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#65292;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#36827;&#23637;&#19968;&#30452;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#20419;&#36827;&#35270;&#35273;&#35821;&#35328;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#25972;&#65288;M$^3$IT&#65289;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20248;&#21270;VLM&#19982;&#20154;&#31867;&#25351;&#20196;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;M$^3$IT&#25968;&#25454;&#38598;&#21253;&#25324;40&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;240&#19975;&#20010;&#23454;&#20363;&#21644;400&#20010;&#25163;&#21160;&#32534;&#20889;&#30340;&#20219;&#21153;&#25351;&#20196;&#65292;&#26684;&#24335;&#21270;&#20026;&#35270;&#35273;&#21040;&#25991;&#26412;&#32467;&#26500;&#12290;&#37325;&#35201;&#20219;&#21153;&#34987;&#32763;&#35793;&#25104;80&#31181;&#35821;&#35328;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#30830;&#20445;&#26356;&#24191;&#27867;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;M$^3$IT&#22312;&#20219;&#21153;&#35206;&#30422;&#33539;&#22260;&#12289;&#25351;&#20196;&#25968;&#37327;&#21644;&#23454;&#20363;&#35268;&#27169;&#26041;&#38754;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Ying-VLM&#65292;&#23427;&#26159;&#22312;&#25105;&#20204;&#30340;M$^3$IT&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;VLM&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#28508;&#22312;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has significantly advanced large language models (LLMs) such as ChatGPT, enabling them to align with human instructions across diverse tasks. However, progress in open vision-language models (VLMs) has been limited due to the scarcity of high-quality instruction datasets. To tackle this challenge and promote research in the vision-language field, we introduce the Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises 40 carefully curated datasets, including 2.4 million instances and 400 manually written task instructions, reformatted into a vision-to-text structure. Key tasks are translated into 80 languages with an advanced translation system, ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding task coverage, instruction number and instance scale. Moreover, we develop Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#32763;&#35793;&#21644;&#36328;&#35821;&#35328;&#36801;&#31227;&#20004;&#31181;&#26041;&#27861;&#26469;&#25191;&#34892;&#20020;&#24202;&#39046;&#22495;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#24182;&#35777;&#26126;&#36328;&#35821;&#35328;&#36801;&#31227;&#27604;&#36825;&#20004;&#31181;&#32763;&#35793;&#26041;&#27861;&#22312;&#27861;&#35821;&#21644;&#24503;&#35821;&#20013;&#37117;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04384</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#65306;&#32763;&#35793;&#36824;&#26159;&#36328;&#35821;&#35328;&#36801;&#31227;&#65311;
&lt;/p&gt;
&lt;p&gt;
Multilingual Clinical NER: Translation or Cross-lingual Transfer?. (arXiv:2306.04384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#32763;&#35793;&#21644;&#36328;&#35821;&#35328;&#36801;&#31227;&#20004;&#31181;&#26041;&#27861;&#26469;&#25191;&#34892;&#20020;&#24202;&#39046;&#22495;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#24182;&#35777;&#26126;&#36328;&#35821;&#35328;&#36801;&#31227;&#27604;&#36825;&#20004;&#31181;&#32763;&#35793;&#26041;&#27861;&#22312;&#27861;&#35821;&#21644;&#24503;&#35821;&#20013;&#37117;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39046;&#22495;&#38750;&#33521;&#35821;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#31561;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#21644;&#26114;&#36149;&#65292;&#22240;&#20026;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#12290;&#36328;&#35821;&#35328;&#36801;&#31227;&#65288;CLT&#65289;&#26159;&#19968;&#31181;&#32469;&#36807;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#20010;&#35821;&#35328;&#19978;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#21478;&#19968;&#20010;&#35821;&#35328;&#19978;&#25552;&#20379;&#39640;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#36824;&#26377;&#20854;&#20182;&#21033;&#29992;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#25191;&#34892;NER&#65292;&#32780;&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#65292;&#21487;&#20197;&#36890;&#36807;&#32763;&#35793;&#35757;&#32451;&#38598;&#25110;&#27979;&#35797;&#38598;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#19982;&#36825;&#20004;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#22312;&#27861;&#35821;&#21644;&#24503;&#35821;&#20013;&#25191;&#34892;&#20020;&#24202;NER&#65292;&#32780;&#19981;&#38656;&#35201;&#36825;&#20123;&#35821;&#35328;&#30340;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;MedNERF&#65292;&#36825;&#26159;&#20174;&#27861;&#22269;&#33647;&#29289;&#22788;&#26041;&#20013;&#25552;&#21462;&#30340;&#21307;&#23398;NER&#27979;&#35797;&#38598;&#65292;&#24182;&#20351;&#29992;&#19982;&#33521;&#35821;&#25968;&#25454;&#38598;&#30456;&#21516;&#30340;&#25351;&#21335;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#36890;&#36807;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#24503;&#35821;&#21307;&#23398;&#35821;&#26009;&#24211;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36328;&#35821;&#35328;&#36801;&#31227;&#27604;&#20004;&#31181;&#32763;&#35793;&#26041;&#27861;&#22312;&#20004;&#31181;&#30446;&#26631;&#35821;&#35328;&#20013;&#37117;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language tasks like Named Entity Recognition (NER) in the clinical domain on non-English texts can be very time-consuming and expensive due to the lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language. However, other methods leveraging translation models can be used to perform NER without annotated data in the target language, by either translating the training set or test set. This paper compares cross-lingual transfer with these two alternative methods, to perform clinical NER in French and in German without any training data in those languages. To this end, we release MedNERF a medical NER test set extracted from French drug prescriptions and annotated with the same guidelines as an English dataset. Through extensive experiments on this dataset and on a German medica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#26631;&#31614;&#24863;&#30693;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65288;LASR&#65289;&#26694;&#26550;&#65292;&#23558;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21644;&#35821;&#35328;&#26631;&#31614;&#20449;&#24687;&#30456;&#32467;&#21512;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#19977;&#20803;&#32452;&#30340;&#30446;&#26631;&#20989;&#25968;&#23558;&#20108;&#32773;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#35821;&#31181;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#24212;&#23545;&#22122;&#22768;/&#20002;&#22833;&#26631;&#31614;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.04374</link><description>&lt;p&gt;
&#22522;&#20110;&#26631;&#31614;&#24863;&#30693;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#35821;&#31181;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Label Aware Speech Representation Learning For Language Identification. (arXiv:2306.04374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#26631;&#31614;&#24863;&#30693;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65288;LASR&#65289;&#26694;&#26550;&#65292;&#23558;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21644;&#35821;&#35328;&#26631;&#31614;&#20449;&#24687;&#30456;&#32467;&#21512;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#19977;&#20803;&#32452;&#30340;&#30446;&#26631;&#20989;&#25968;&#23558;&#20108;&#32773;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#35821;&#31181;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#24212;&#23545;&#22122;&#22768;/&#20002;&#22833;&#26631;&#31614;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#38750;&#35821;&#20041;&#20219;&#21153;&#65292;&#20363;&#22914;&#35821;&#35328;&#35782;&#21035;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35201;&#20040;&#25506;&#32034;&#20351;&#29992;&#20998;&#31867;&#22120;&#27169;&#22411;&#30340;&#30417;&#30563;&#23884;&#20837;&#25552;&#21462;&#26041;&#27861;&#65292;&#35201;&#20040;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#30340;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#19982;&#35821;&#35328;&#26631;&#31614;&#20449;&#24687;&#30456;&#32467;&#21512;&#36827;&#34892;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#31216;&#20026;&#22522;&#20110;&#26631;&#31614;&#24863;&#30693;&#30340;&#35821;&#38899;&#34920;&#31034;&#65288;LASR&#65289;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#19977;&#20803;&#32452;&#30340;&#30446;&#26631;&#20989;&#25968;&#23558;&#35821;&#35328;&#26631;&#31614;&#19982;&#33258;&#25105;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#30456;&#32467;&#21512;&#12290;&#35821;&#38899;&#34920;&#31034;&#36827;&#19968;&#27493;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#35821;&#35328;&#35782;&#21035;&#23454;&#39564;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598; - FLEURS&#21644;Dhwani&#19978;&#36827;&#34892;&#12290;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35828;&#26126;&#25152;&#25552;&#20986;&#30340;LASR&#26694;&#26550;&#22312;&#35821;&#35328;&#35782;&#21035;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;LASR&#26041;&#27861;&#23545;&#20110;&#22122;&#22768;/&#20002;&#22833;&#26631;&#31614;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22810;&#38454;&#27573;&#22686;&#24378;&#26041;&#27861;&#25913;&#21892;&#38463;&#25289;&#20271;&#35821;&#21457;&#38899;&#38556;&#30861;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#38463;&#25289;&#20271;&#35821;Conformer&#30340;&#24494;&#35843;&#21644;&#26356;&#27491;&#31574;&#30053;&#65292;&#21462;&#24471;&#20102;&#36739;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04368</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#25239;&#32593;&#32476;&#21644;&#20449;&#21495;&#22686;&#24378;&#25216;&#26415;&#30340;&#38463;&#25289;&#20271;&#35821;&#21457;&#38899;&#38556;&#30861;&#32773;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Arabic Dysarthric Speech Recognition Using Adversarial and Signal-Based Augmentation. (arXiv:2306.04368v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#38454;&#27573;&#22686;&#24378;&#26041;&#27861;&#25913;&#21892;&#38463;&#25289;&#20271;&#35821;&#21457;&#38899;&#38556;&#30861;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#38463;&#25289;&#20271;&#35821;Conformer&#30340;&#24494;&#35843;&#21644;&#26356;&#27491;&#31574;&#30053;&#65292;&#21462;&#24471;&#20102;&#36739;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#24050;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;ASR&#31995;&#32479;&#22312;&#22788;&#29702;&#21463;&#25439;&#35821;&#38899;&#26102;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#29978;&#33267;&#23545;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#38463;&#25289;&#20271;&#35821;&#20013;&#65292;&#30001;&#20110;&#20174;&#21457;&#38899;&#38556;&#30861;&#24739;&#32773;&#37027;&#37324;&#25910;&#38598;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#19968;&#25361;&#25112;&#26356;&#21152;&#20005;&#23803;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22810;&#38454;&#27573;&#22686;&#24378;&#26041;&#27861;&#25913;&#21892;&#38463;&#25289;&#20271;&#35821;&#21457;&#38899;&#38556;&#30861;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#34920;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#36895;&#24230;&#21644;&#33410;&#22863;&#20174;&#20581;&#24247;&#30340;&#38463;&#25289;&#20271;&#35821;&#38899;&#20013;&#29983;&#25104;&#21457;&#38899;&#38556;&#30861;&#38463;&#25289;&#20271;&#35821;&#38899;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31532;&#20108;&#38454;&#27573;&#24182;&#34892;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;&#65288;PWG&#65289;&#65292;&#22312;&#35757;&#32451;&#33521;&#25991;&#21457;&#38899;&#38556;&#30861;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#65292;&#25429;&#25417;&#35821;&#35328;&#26080;&#20851;&#30340;&#21457;&#38899;&#38556;&#30861;&#35821;&#38899;&#27169;&#24335;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20449;&#21495;&#35843;&#25972;&#30340;&#35821;&#38899;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#38463;&#25289;&#20271;&#35821;Conformer&#30340;&#24494;&#35843;&#21644;&#25991;&#26412;&#26356;&#27491;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#31243;&#24230;&#30340;&#21457;&#38899;&#38556;&#30861;&#12290;&#25105;&#20204;&#24494;&#35843;&#30340;Conformer&#22312;&#19968;&#39033;&#32463;&#36807;&#26631;&#35760;&#30340;&#21457;&#38899;&#38556;&#30861;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;18%&#30340;&#23383;&#35789;&#38169;&#35823;&#29575;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite major advancements in Automatic Speech Recognition (ASR), the state-of-the-art ASR systems struggle to deal with impaired speech even with high-resource languages. In Arabic, this challenge gets amplified, with added complexities in collecting data from dysarthric speakers. In this paper, we aim to improve the performance of Arabic dysarthric automatic speech recognition through a multi-stage augmentation approach. To this effect, we first propose a signal-based approach to generate dysarthric Arabic speech from healthy Arabic speech by modifying its speed and tempo. We also propose a second stage Parallel Wave Generative (PWG) adversarial model that is trained on an English dysarthric dataset to capture language-independant dysarthric speech patterns and further augment the signal-adjusted speech samples. Furthermore, we propose a fine-tuning and text-correction strategies for Arabic Conformer at different dysarthric speech severity levels. Our fine-tuned Conformer achieved 18
&lt;/p&gt;</description></item><item><title>Youku-mPLUG&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20221;10M&#30340;&#20013;&#25991;&#35270;&#39057;-&#25991;&#26412;&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;45&#31181;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#24182;&#26500;&#24314;&#20102;&#26368;&#22823;&#30340;&#20154;&#24037;&#27880;&#37322;&#20013;&#25991;&#22522;&#20934;&#20197;&#28085;&#30422;&#19977;&#20010;&#27969;&#34892;&#30340;&#35270;&#39057;&#35821;&#35328;&#20219;&#21153;&#65306;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#35270;&#39057;&#23383;&#24149;&#21644;&#35270;&#39057;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.04362</link><description>&lt;p&gt;
Youku-mPLUG&#65306;&#19968;&#20221;1000&#19975;&#22823;&#35268;&#27169;&#30340;&#20013;&#25991;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks. (arXiv:2306.04362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04362
&lt;/p&gt;
&lt;p&gt;
Youku-mPLUG&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20221;10M&#30340;&#20013;&#25991;&#35270;&#39057;-&#25991;&#26412;&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;45&#31181;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#24182;&#26500;&#24314;&#20102;&#26368;&#22823;&#30340;&#20154;&#24037;&#27880;&#37322;&#20013;&#25991;&#22522;&#20934;&#20197;&#28085;&#30422;&#19977;&#20010;&#27969;&#34892;&#30340;&#35270;&#39057;&#35821;&#35328;&#20219;&#21153;&#65306;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#35270;&#39057;&#23383;&#24149;&#21644;&#35270;&#39057;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#20013;&#22269;&#31038;&#21306;&#20013;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#39318;&#20808;&#21457;&#24067;&#20102;&#26368;&#22823;&#30340;&#20844;&#20849;&#20013;&#25991;&#39640;&#36136;&#37327;&#35270;&#39057;&#35821;&#35328;&#25968;&#25454;&#38598;Youku-mPLUG&#12290;&#23427;&#20174;&#20248;&#37239;&#20013;&#25910;&#38598;&#65292;&#20855;&#26377;&#20005;&#26684;&#30340;&#23433;&#20840;&#12289;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#26631;&#20934;&#12290;Youku-mPLUG&#21253;&#21547;1000&#19975;&#31687;&#20013;&#25991;&#35270;&#39057;&#25991;&#26412;&#23545;&#65292;&#26159;&#20174;45&#31181;&#19981;&#21516;&#30340;&#31867;&#21035;&#20013;&#31579;&#36873;&#20986;&#30340;400&#19975;&#20010;&#21407;&#22987;&#35270;&#39057;&#20013;&#25552;&#21462;&#20986;&#26469;&#30340;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20415;&#20110;&#20840;&#38754;&#35780;&#20272;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#35748;&#30495;&#26500;&#24314;&#20102;&#26368;&#22823;&#30340;&#20154;&#24037;&#27880;&#37322;&#20013;&#25991;&#22522;&#20934;&#65292;&#28085;&#30422;&#19977;&#20010;&#27969;&#34892;&#30340;&#35270;&#39057;&#35821;&#35328;&#20219;&#21153;&#65306;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#35270;&#39057;&#23383;&#24149;&#21644;&#35270;&#39057;&#20998;&#31867;&#12290;Youku-mPLUG&#21487;&#20197;&#20351;&#30740;&#31350;&#20154;&#21592;&#22312;&#26410;&#26469;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#22810;&#27169;&#24577;&#30740;&#31350;&#21644;&#24320;&#21457;&#26356;&#22909;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#27969;&#34892;&#30340;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;AL&#12290;
&lt;/p&gt;
&lt;p&gt;
To promote the development of Vision-Language Pre-training (VLP) and multimodal Large Language Model (LLM) in the Chinese community, we firstly release the largest public Chinese high-quality video-language dataset named Youku-mPLUG, which is collected from Youku, a well-known Chinese video-sharing website, with strict criteria of safety, diversity, and quality. Youku-mPLUG contains 10 million Chinese video-text pairs filtered from 400 million raw videos across a wide range of 45 diverse categories for large-scale pre-training. In addition, to facilitate a comprehensive evaluation of video-language models, we carefully build the largest human-annotated Chinese benchmarks covering three popular video-language tasks of cross-modal retrieval, video captioning, and video category classification. Youku-mPLUG can enable researchers to conduct more in-depth multimodal research and develop better applications in the future. Furthermore, we release popular video-language pre-training models, AL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04357</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#26816;&#32034;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#29992;&#25143;&#21644;&#31995;&#32479;&#35805;&#35821;&#21382;&#21490;&#35760;&#24405;&#20174;&#20960;&#20010;&#20505;&#36873;&#21709;&#24212;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21518;&#35757;&#32451;&#22823;&#22810;&#20381;&#36182;&#20110;&#21333;&#32431;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#24320;&#21457;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;IR&#31038;&#21306;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#25991;&#26412;&#34920;&#31034;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#23545;&#35805;&#35821;&#20041;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Dial-MAE&#65288;&#23545;&#35805;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;&#12290; Dial-MAE&#20351;&#29992;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23398;&#20064;&#23558;&#23545;&#35805;&#30340;&#35821;&#20041;&#26356;&#22909;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#21521;&#37327;&#20013;&#12290; Dial-MAE&#30340;&#36807;&#31243;&#21253;&#25324;&#30001;&#28145;&#24230;&#32534;&#30721;&#22120;&#21019;&#24314;&#24102;&#26377;&#25513;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#23884;&#20837;&#65292;&#28982;&#21518;&#26159;&#27973;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#20351;&#29992;&#27492;&#23884;&#20837;&#20197;&#21450;&#19978;&#19979;&#25991;&#21521;&#37327;&#26469;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25968;&#25454;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;-&#24674;&#22797;&#33539;&#24335;&#19982;GPT&#30340;&#19968;&#27425;&#24615;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#26631;&#27880;&#30340;&#35206;&#30422;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#26131;&#35835;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04349</link><description>&lt;p&gt;
GPT&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#25968;&#25454;&#26631;&#27880;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
GPT Self-Supervision for a Better Data Annotator. (arXiv:2306.04349v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25968;&#25454;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;-&#24674;&#22797;&#33539;&#24335;&#19982;GPT&#30340;&#19968;&#27425;&#24615;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#26631;&#27880;&#30340;&#35206;&#30422;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#26131;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23558;&#25968;&#25454;&#27880;&#37322;&#20026;&#31616;&#27905;&#30340;&#25688;&#35201;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#24120;&#24120;&#38656;&#35201;&#20154;&#31867;&#19987;&#23478;&#25237;&#20837;&#22823;&#37327;&#26102;&#38388;&#21644;&#19987;&#19994;&#30693;&#35782;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#24050;&#32463;&#26377;&#25152;&#23581;&#35797;&#65292;&#20294;&#20173;&#23384;&#22312;&#35832;&#22914;&#19981;&#36866;&#29992;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#12289;&#32570;&#20047;&#33258;&#30417;&#30563;&#26041;&#27861;&#12289;&#32570;&#20047;&#23545;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20851;&#27880;&#31561;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25968;&#25454;&#26631;&#27880;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#29983;&#25104;-&#24674;&#22797;&#33539;&#24335;&#65292;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#20013;&#30340;&#19968;&#27425;&#24615;&#23398;&#20064;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#27425;&#24615;&#35843;&#25972;&#38454;&#27573;&#21644;&#29983;&#25104;&#38454;&#27573;&#12290;&#22312;&#19968;&#27425;&#24615;&#35843;&#25972;&#38454;&#27573;&#65292;&#25105;&#20204;&#20174;&#25903;&#25345;&#38598;&#20013;&#25277;&#21462;&#25968;&#25454;&#20316;&#20026;GPT&#29983;&#25104;&#25991;&#26412;&#25688;&#35201;&#30340;&#19968;&#37096;&#20998;&#65292;&#28982;&#21518;&#29992;&#35813;&#25688;&#35201;&#24674;&#22797;&#21407;&#22987;&#25968;&#25454;&#12290;&#24674;&#22797;&#25968;&#25454;&#19982;&#21407;&#22987;&#25968;&#25454;&#30340;&#23545;&#40784;&#20998;&#25968;&#29992;&#20110;&#24494;&#35843;GPT&#26435;&#37325;&#12290;&#22312;&#29983;&#25104;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#22522;&#20110;GPT&#30340;&#26041;&#27861;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#26032;&#30340;&#25991;&#26412;&#25688;&#35201;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35206;&#30422;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#26131;&#35835;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#27604;&#29616;&#26377;&#30340;&#26631;&#27880;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of annotating data into concise summaries poses a significant challenge across various domains, frequently requiring the allocation of significant time and specialized knowledge by human experts. Despite existing efforts to use large language models for annotation tasks, significant problems such as limited applicability to unlabeled data, the absence of self-supervised methods, and the lack of focus on complex structured data still persist. In this work, we propose a GPT self-supervision annotation method. This method embodies a generating-recovering paradigm that leverages the capabilities of one-shot learning capabilities in Generative Pretrained Transformer (GPT). The proposed approach comprises a one-shot tuning phase followed by a generation phase. In the one-shot tuning phase, we sample a data from the support set as part of the prompt for GPT to generate a textual summary, which is then used to recover the original data. The alignment score between the recovered and or
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#24418;&#30340;&#35821;&#20041;&#35268;&#33539;MathWorld&#65292;&#21487;&#20197;&#23558;&#19990;&#30028;&#27169;&#22411;&#20998;&#37197;&#32473;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#20379;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;NLP&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.04347</link><description>&lt;p&gt;
&#25968;&#23398;&#35299;&#39064;&#30340;&#19990;&#30028;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
World Models for Math Story Problems. (arXiv:2306.04347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#24418;&#30340;&#35821;&#20041;&#35268;&#33539;MathWorld&#65292;&#21487;&#20197;&#23558;&#19990;&#30028;&#27169;&#22411;&#20998;&#37197;&#32473;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#20379;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;NLP&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23398;&#29983;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#32780;&#35328;&#65292;&#35299;&#20915;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20182;&#20204;&#29702;&#35299;&#25925;&#20107;&#20013;&#25152;&#25551;&#36848;&#30340;&#19990;&#30028;&#24182;&#23545;&#20854;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#35745;&#31639;&#20986;&#31572;&#26696;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#21019;&#26032;&#25216;&#26415;&#24050;&#32463;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#33258;&#21160;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25968;&#23398;&#27010;&#24565;&#30340;&#20934;&#30830;&#34920;&#31034;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#23548;&#33268;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#20174;&#32780;&#24433;&#21709;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#26412;&#25991;&#23558;&#20043;&#21069;&#30340;&#24037;&#20316;&#25972;&#21512;&#21040;&#20998;&#31867;&#21644;&#34920;&#36798;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#19978;&#65292;&#24182;&#24320;&#21457;&#20986;&#38024;&#23545;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#39046;&#22495;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#35821;&#20041;&#35268;&#33539;MathWorld&#12290;&#21033;&#29992;MathWorld&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#20998;&#37197;&#19990;&#30028;&#27169;&#22411;&#65292;&#23427;&#20204;&#34920;&#31034;&#22312;&#25991;&#26412;&#20013;&#20171;&#32461;&#30340;&#24773;&#20917;&#21644;&#34892;&#21160;&#20197;&#21450;&#23427;&#20204;&#30340;&#25968;&#23398;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#26469;&#33258;&#20960;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#25968;&#23398;&#25925;&#20107;&#38382;&#39064;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#22312;&#25105;&#20204;&#25910;&#38598;&#30340;&#26032;&#25968;&#25454;&#38598;Story-Gen Math&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MathWorld&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving math story problems is a complex task for students and NLP models alike, requiring them to understand the world as described in the story and reason over it to compute an answer. Recent years have seen impressive performance on automatically solving these problems with large pre-trained language models and innovative techniques to prompt them. However, it remains unclear if these models possess accurate representations of mathematical concepts. This leads to lack of interpretability and trustworthiness which impedes their usefulness in various applications. In this paper, we consolidate previous work on categorizing and representing math story problems and develop MathWorld, which is a graph-based semantic formalism specific for the domain of math story problems. With MathWorld, we can assign world models to math story problems which represent the situations and actions introduced in the text and their mathematical relationships. We combine math story problems from several exis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25552;&#21462;&#30340;&#21327;&#21516;&#28436;&#21270;&#22270;&#25512;&#29702;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#31435;&#24322;&#26500;&#22270;&#25429;&#25417;&#22240;&#26524;&#20851;&#31995;&#21644;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25552;&#21462;&#30340;&#26032;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04340</link><description>&lt;p&gt;
&#38754;&#21521;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25552;&#21462;&#30340;&#21327;&#21516;&#28436;&#21270;&#22270;&#25512;&#29702;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Co-evolving Graph Reasoning Network for Emotion-Cause Pair Extraction. (arXiv:2306.04340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25552;&#21462;&#30340;&#21327;&#21516;&#28436;&#21270;&#22270;&#25512;&#29702;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#31435;&#24322;&#26500;&#22270;&#25429;&#25417;&#22240;&#26524;&#20851;&#31995;&#21644;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25552;&#21462;&#30340;&#26032;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;-&#21407;&#22240;&#23545;&#25552;&#21462;&#65288;ECPE&#65289;&#26088;&#22312;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#25152;&#26377;&#24773;&#24863;&#23376;&#21477;&#21450;&#20854;&#30456;&#24212;&#30340;&#21407;&#22240;&#23376;&#21477;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#20026;ECPE&#25552;&#20379;&#20102;&#25351;&#31034;&#24615;&#30340;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;MTL&#26694;&#26550;&#20165;&#32771;&#34385;&#20102;&#19968;&#36718;&#22810;&#20219;&#21153;&#25512;&#29702;&#65292;&#24573;&#30053;&#20102;&#20174;ECPE&#21040;&#23376;&#20219;&#21153;&#30340;&#21453;&#39304;&#12290;&#27492;&#22806;&#65292;&#20854;&#22810;&#20219;&#21153;&#25512;&#29702;&#20165;&#20381;&#36182;&#20110;&#35821;&#20041;&#27700;&#24179;&#30340;&#20132;&#20114;&#65292;&#26080;&#27861;&#25429;&#25417;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#32534;&#30721;&#22120;&#20849;&#20139;&#21644;&#22810;&#20219;&#21153;&#38544;&#34255;&#29366;&#24577;&#32423;&#32852;&#20063;&#38590;&#20197;&#25429;&#25417;&#22240;&#26524;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#28436;&#21270;&#25512;&#29702;&#30340;&#26032;MTL&#26694;&#26550;&#12290;&#23427;&#65288;1&#65289;&#27169;&#25311;&#20102;ECPE&#19982;&#20854;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#21452;&#21521;&#21453;&#39304;&#65307;&#65288;2&#65289;&#20801;&#35768;&#19977;&#20010;&#20219;&#21153;&#19968;&#36215;&#28436;&#21270;&#65292;&#24182;&#30456;&#20114;&#24341;&#23548;&#65307;&#65288;3&#65289;&#38598;&#25104;&#39044;&#27979;&#32423;&#21035;&#30340;&#20132;&#20114;&#26469;&#25429;&#25417;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Co-evolving Graph Reasoning Network&#65288;CGRN&#65289;&#65292;&#26469;&#23454;&#29616;Co-evolving Reasoning MTL&#26694;&#26550;&#12290;CGRN&#22312;&#19977;&#20010;&#20219;&#21153;&#21450;&#20854;&#36755;&#20837;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24322;&#26500;&#22270;&#65292;&#36890;&#36807;&#22270;&#25512;&#29702;&#36807;&#31243;&#33258;&#28982;&#22320;&#25429;&#25417;&#20102;&#22240;&#26524;&#20851;&#31995;&#21644;&#26174;&#24335;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#20004;&#20010;ECPE&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;CGRN&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion-Cause Pair Extraction (ECPE) aims to extract all emotion clauses and their corresponding cause clauses from a document. Existing approaches tackle this task through multi-task learning (MTL) framework in which the two subtasks provide indicative clues for ECPE. However, the previous MTL framework considers only one round of multi-task reasoning and ignores the reverse feedbacks from ECPE to the subtasks. Besides, its multi-task reasoning only relies on semantics-level interactions, which cannot capture the explicit dependencies, and both the encoder sharing and multi-task hidden states concatenations can hardly capture the causalities. To solve these issues, we first put forward a new MTL framework based on Co-evolving Reasoning. It (1) models the bidirectional feedbacks between ECPE and its subtasks; (2) allows the three tasks to evolve together and prompt each other recurrently; (3) integrates prediction-level interactions to capture explicit dependencies. Then we propose a n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#28155;&#21152;&#21644;&#20943;&#23569;&#22122;&#22768;&#65292;&#20197;&#19968;&#31181;&#26356;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#65292;&#21487;&#35270;&#21270;&#21644;&#27604;&#36739;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#27169;&#22411;&#65292;&#26356;&#28165;&#26970;&#22320;&#20102;&#35299;&#33258;&#21160;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#30340;&#27169;&#24335;&#65292;&#24182;&#20026;&#24314;&#31435;&#21487;&#38752;&#30340;&#33258;&#21160;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#31995;&#32479;&#25552;&#20379;&#20102;&#35748;&#35782;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04337</link><description>&lt;p&gt;
&#33258;&#21160;&#21457;&#38899;&#38556;&#30861;&#35821;&#38899;&#35780;&#20272;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Reliability of Automatic Dysarthric Speech Assessments. (arXiv:2306.04337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#28155;&#21152;&#21644;&#20943;&#23569;&#22122;&#22768;&#65292;&#20197;&#19968;&#31181;&#26356;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#65292;&#21487;&#35270;&#21270;&#21644;&#27604;&#36739;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#27169;&#22411;&#65292;&#26356;&#28165;&#26970;&#22320;&#20102;&#35299;&#33258;&#21160;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#30340;&#27169;&#24335;&#65292;&#24182;&#20026;&#24314;&#31435;&#21487;&#38752;&#30340;&#33258;&#21160;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#31995;&#32479;&#25552;&#20379;&#20102;&#35748;&#35782;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#25552;&#20379;&#20102;&#24320;&#21457;&#26377;&#25928;&#20302;&#25104;&#26412;&#24037;&#20855;&#30340;&#26426;&#20250;&#65292;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#35299;&#20915;&#25163;&#21160;&#21644;&#20027;&#35266;&#35780;&#20272;&#30340;&#29616;&#26377;&#38480;&#21046;&#12290; &#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#20381;&#36182;&#20110;&#21457;&#38899;&#38556;&#30861;&#30456;&#20851;&#30340;&#35821;&#38899;&#27169;&#24335;&#25110;&#22806;&#37096;&#22240;&#32032;&#23578;&#19981;&#28165;&#26970;&#12290; &#25105;&#20204;&#26088;&#22312;&#26356;&#28165;&#26970;&#22320;&#20102;&#35299;&#21457;&#38899;&#38556;&#30861;&#27169;&#24335;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#21644;&#20943;&#23569;&#22122;&#22768;&#65292;&#35774;&#35745;&#21644;&#23454;&#26045;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21487;&#35270;&#21270;&#21644;&#27604;&#36739;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#27169;&#22411;&#65292;&#22312;&#24739;&#32773;&#23618;&#38754;&#19978;&#36827;&#34892;&#26356;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;UA-Speech&#25968;&#25454;&#38598;&#65292;&#23558;&#25968;&#25454;&#20197;&#35828;&#35805;&#20154;&#20026;&#21333;&#20301;&#36827;&#34892;&#25286;&#20998;&#12290; &#30446;&#21069;&#30340;&#25991;&#29486;&#20013;&#25253;&#21578;&#30340;&#32467;&#26524;&#20284;&#20046;&#26080;&#35770;&#26159;&#21542;&#36827;&#34892;&#20102;&#36825;&#31181;&#25286;&#20998;&#65292;&#37117;&#20250;&#23548;&#33268;&#30001;&#20110;&#25968;&#25454;&#27844;&#28431;&#32780;&#21487;&#33021;&#36807;&#20110;&#33258;&#20449;&#30340;&#27169;&#22411;&#12290; &#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#32467;&#26524;&#25552;&#39640;&#30740;&#31350;&#30028;&#23545;&#24314;&#31435;&#21487;&#38752;&#30340;&#33258;&#21160;&#21457;&#38899;&#38556;&#30861;&#35780;&#20272;&#31995;&#32479;&#30340;&#35201;&#27714;&#30340;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating dysarthria assessments offers the opportunity to develop effective, low-cost tools that address the current limitations of manual and subjective assessments. Nonetheless, it is unclear whether current approaches rely on dysarthria-related speech patterns or external factors. We aim toward obtaining a clearer understanding of dysarthria patterns. To this extent, we study the effects of noise in recordings, both through addition and reduction. We design and implement a new method for visualizing and comparing feature extractors and models, at a patient level, in a more interpretable way. We use the UA-Speech dataset with a speaker-based split of the dataset. Results reported in the literature appear to have been done irrespective of such split, leading to models that may be overconfident due to data-leakage. We hope that these results raise awareness in the research community regarding the requirements for establishing reliable automatic dysarthria assessment systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;"Echoes from Alexandria"&#30340;&#22823;&#22411;&#36164;&#28304;&#65292;&#23427;&#21253;&#25324;&#19977;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#22810;&#35821;&#20840;&#20070;&#25688;&#35201;&#65292;&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;&#36164;&#28304;&#65292;&#20063;&#26159;&#26368;&#22823;&#30340;&#36164;&#28304;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25277;&#21462;-&#27010;&#25324;&#22522;&#32447;&#65292;&#24182;&#30830;&#23450;&#20102;&#20840;&#20070;&#25688;&#35201;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.04334</link><description>&lt;p&gt;
&#20122;&#21382;&#23665;&#22823;&#30340;&#22238;&#22768;&#65306;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#20840;&#20070;&#25688;&#35201;&#30340;&#22823;&#22411;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Echoes from Alexandria: A Large Resource for Multilingual Book Summarization. (arXiv:2306.04334v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;"Echoes from Alexandria"&#30340;&#22823;&#22411;&#36164;&#28304;&#65292;&#23427;&#21253;&#25324;&#19977;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#22810;&#35821;&#20840;&#20070;&#25688;&#35201;&#65292;&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;&#36164;&#28304;&#65292;&#20063;&#26159;&#26368;&#22823;&#30340;&#36164;&#28304;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25277;&#21462;-&#27010;&#25324;&#22522;&#32447;&#65292;&#24182;&#30830;&#23450;&#20102;&#20840;&#20070;&#25688;&#35201;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#25688;&#35201;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26032;&#38395;&#39046;&#22495;&#65292;&#20854;&#20013;&#25991;&#26412;&#36890;&#24120;&#24456;&#30701;&#24182;&#20855;&#26377;&#24378;&#28872;&#30340;&#24067;&#23616;&#29305;&#24449;&#12290;&#20840;&#20070;&#25688;&#35201;&#30340;&#20219;&#21153;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#24456;&#38590;&#36890;&#36807;&#24403;&#21069;&#30340;&#36164;&#28304;&#26469;&#22788;&#29702;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35268;&#27169;&#26377;&#38480;&#19988;&#20165;&#22312;&#33521;&#35821;&#20013;&#25552;&#20379;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"Echoes from Alexandria"&#65288;&#25110;&#31616;&#31216;&#20026;"Echoes"&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#20840;&#20070;&#25688;&#35201;&#30340;&#22823;&#22411;&#36164;&#28304;&#12290;Echoes&#21253;&#25324;&#19977;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65306;i) Echo-Wiki&#65292;&#29992;&#20110;&#22810;&#35821;&#20840;&#20070;&#25688;&#35201;&#65307;ii&#65289;Echo-XSum&#65292;&#29992;&#20110;&#26497;&#24230;&#21387;&#32553;&#30340;&#22810;&#35821;&#20840;&#20070;&#25688;&#35201;&#65307;iii&#65289;Echo-FairySum&#65292;&#29992;&#20110;&#25277;&#21462;&#24335;&#20840;&#20070;&#25688;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;Echoes&#26159;&#26368;&#22823;&#30340;&#36164;&#28304;&#65292;&#20063;&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;&#36164;&#28304;&#65292;&#21253;&#25324;5&#31181;&#35821;&#35328;&#21644;25&#31181;&#35821;&#35328;&#23545;&#12290;&#38500;&#20102;Echoes&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25277;&#21462;-&#27010;&#25324;&#22522;&#32447;&#65292;&#24182;&#26681;&#25454;&#25105;&#20204;&#20351;&#29992;Echoes&#30340;&#32463;&#39564;&#65292;&#30830;&#23450;&#20102;&#20840;&#20070;&#25688;&#35201;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, research in text summarization has mainly focused on the news domain, where texts are typically short and have strong layout features. The task of full-book summarization presents additional challenges which are hard to tackle with current resources, due to their limited size and availability in English only. To overcome these limitations, we present "Echoes from Alexandria", or in shortened form, "Echoes", a large resource for multilingual book summarization. Echoes features three novel datasets: i) Echo-Wiki, for multilingual book summarization, ii) Echo-XSum, for extremely-compressive multilingual book summarization, and iii) Echo-FairySum, for extractive book summarization. To the best of our knowledge, Echoes, with its thousands of books and summaries, is the largest resource, and the first to be multilingual, featuring 5 languages and 25 language pairs. In addition to Echoes, we also introduce a new extractive-then-abstractive baseline, and, supported by our expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#24202;&#23545;&#35805;&#30340;&#22810;&#23618;&#27425;&#25688;&#35201;&#26041;&#27861;&#65292;&#21457;&#29616;&#38024;&#23545;&#27599;&#20010;&#37096;&#20998;&#20351;&#29992;&#21333;&#29420;&#25688;&#35201;&#27169;&#22411;&#30340;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#26159;&#22810;&#23618;/&#38454;&#27573;&#26041;&#27861;&#24182;&#19981;&#20250;&#25913;&#21892;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04328</link><description>&lt;p&gt;
IUTEAM1&#22312;MEDIQA-Chat 2023&#27604;&#36187;&#20013;&#65306;&#31616;&#21333;&#24494;&#35843;&#23545;&#20020;&#24202;&#23545;&#35805;&#30340;&#22810;&#23618;&#27425;&#25688;&#35201;&#26377;&#25928;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
IUTEAM1 at MEDIQA-Chat 2023: Is simple fine tuning effective for multilayer summarization of clinical conversations?. (arXiv:2306.04328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#24202;&#23545;&#35805;&#30340;&#22810;&#23618;&#27425;&#25688;&#35201;&#26041;&#27861;&#65292;&#21457;&#29616;&#38024;&#23545;&#27599;&#20010;&#37096;&#20998;&#20351;&#29992;&#21333;&#29420;&#25688;&#35201;&#27169;&#22411;&#30340;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#26159;&#22810;&#23618;/&#38454;&#27573;&#26041;&#27861;&#24182;&#19981;&#20250;&#25913;&#21892;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#23545;&#35805;&#30340;&#25688;&#35201;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;&#25688;&#35201;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#30340;&#21307;&#23398;&#25253;&#21578;&#8220;&#30149;&#21382;&#25688;&#35201;&#8221;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#20174;&#21333;&#20010;&#30340;&#25688;&#35201;&#27169;&#22411;&#21019;&#24314;&#22522;&#32447;&#24320;&#22987;&#65292;&#28982;&#21518;&#23548;&#33268;&#35768;&#22810;&#38024;&#23545;&#30149;&#21382;&#25688;&#35201;&#30340;&#27169;&#22411;&#38598;&#25104;&#65292;&#26368;&#32456;&#37319;&#29992;&#22810;&#23618;/&#38454;&#27573;&#26041;&#24335;&#23558;&#29983;&#25104;&#30340;&#32467;&#26524;&#20256;&#36882;&#32473;&#21478;&#19968;&#20010;&#25688;&#35201;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29983;&#25104;&#25991;&#26412;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#27599;&#20010;&#37096;&#20998;&#19987;&#29992;&#30340;&#25688;&#35201;&#27169;&#22411;&#38598;&#25104;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#22810;&#23618;/&#38454;&#27573;&#26041;&#27861;&#24182;&#19981;&#20250;&#25913;&#21892;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical conversation summarization has become an important application of Natural language Processing. In this work, we intend to analyze summarization model ensembling approaches, that can be utilized to improve the overall accuracy of the generated medical report called chart note. The work starts with a single summarization model creating the baseline. Then leads to an ensemble of summarization models trained on a separate section of the chart note. This leads to the final approach of passing the generated results to another summarization model in a multi-layer/stage fashion for better coherency of the generated text. Our results indicate that although an ensemble of models specialized in each section produces better results, the multi-layer/stage approach does not improve accuracy. The code for the above paper is available at https://github.com/dhananjay-srivastava/MEDIQA-Chat-2023-iuteam1.git
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#25552;&#39640;&#35770;&#35777;&#25366;&#25496;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#22686;&#21152;&#35805;&#35821;&#26631;&#35760;&#20197;&#26126;&#30830;&#26631;&#31034;&#25152;&#26377;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#24494;&#35843;&#35757;&#32451;&#21518;&#30340;&#27969;&#34892;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.04314</link><description>&lt;p&gt;
&#36328;&#25991;&#20307;&#35770;&#35777;&#25366;&#25496;&#65306;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#33258;&#21160;&#22635;&#34917;&#32570;&#22833;&#30340;&#35805;&#35821;&#26631;&#35760;&#65311;
&lt;/p&gt;
&lt;p&gt;
Cross-Genre Argument Mining: Can Language Models Automatically Fill in Missing Discourse Markers?. (arXiv:2306.04314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04314
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#35770;&#35777;&#25366;&#25496;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#22686;&#21152;&#35805;&#35821;&#26631;&#35760;&#20197;&#26126;&#30830;&#26631;&#31034;&#25152;&#26377;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#24494;&#35843;&#35757;&#32451;&#21518;&#30340;&#27969;&#34892;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#25366;&#25496;&#30340;&#21487;&#29992;&#35821;&#26009;&#24211;&#22312;&#22810;&#20010;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#24046;&#24322;&#26159;&#35805;&#35821;&#26631;&#35760;&#30340;&#23384;&#22312;&#65288;&#25110;&#32570;&#22833;&#65289;&#20197;&#34920;&#26126;&#35770;&#35777;&#20869;&#23481;&#12290;&#20174;&#19981;&#21516;&#30340;&#35805;&#35821;&#20998;&#26512;&#20219;&#21153;&#20013;&#25506;&#32034;&#26377;&#25928;&#20351;&#29992;&#35805;&#35821;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#36825;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#20013;&#24341;&#20986;&#30340;&#32467;&#35770;&#26159;&#35805;&#35821;&#26631;&#35760;&#26159;&#35805;&#35821;&#20851;&#31995;&#30340;&#24378;&#26377;&#21147;&#25351;&#26631;&#12290;&#20026;&#20102;&#25552;&#39640;&#36328;&#19981;&#21516;&#25991;&#20307;&#30340;&#35770;&#35777;&#25366;&#25496;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#33258;&#21160;&#36890;&#36807;&#22686;&#21152;&#35805;&#35821;&#26631;&#35760;&#26469;&#22686;&#24378;&#32473;&#23450;&#25991;&#26412;&#65292;&#20174;&#32780;&#26126;&#30830;&#22320;&#26631;&#31034;&#20986;&#25152;&#26377;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;&#26469;&#65292;&#30452;&#25509;&#20351;&#29992;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65307;&#28982;&#32780;&#65292;&#24403;&#20854;&#22312;&#25105;&#20204;&#26500;&#24314;&#30340;&#26032;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#65288;&#21253;&#25324;&#21512;&#25104;&#21644;&#30495;&#23454;&#31034;&#20363;&#65289;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35780;&#20272;&#30340;&#35770;&#35777;&#25366;&#25496;&#19979;&#28216;&#20219;&#21153;&#26469;&#23637;&#31034;&#65292;&#35828;&#26126;&#21487;&#20197;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#33258;&#21160;&#23436;&#25104;&#35770;&#35777;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;
Available corpora for Argument Mining differ along several axes, and one of the key differences is the presence (or absence) of discourse markers to signal argumentative content. Exploring effective ways to use discourse markers has received wide attention in various discourse parsing tasks, from which it is well-known that discourse markers are strong indicators of discourse relations. To improve the robustness of Argument Mining systems across different genres, we propose to automatically augment a given text with discourse markers such that all relations are explicitly signaled. Our analysis unveils that popular language models taken out-of-the-box fail on this task; however, when fine-tuned on a new heterogeneous dataset that we construct (including synthetic and real examples), they perform considerably better. We demonstrate the impact of our approach on an Argument Mining downstream task, evaluated on different corpora, showing that language models can be trained to automaticall
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-3 Davinci-003&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#29305;&#36136;&#65292;&#20294;&#22312;&#19981;&#21516;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.04308</link><description>&lt;p&gt;
GPT-3&#30340;&#20154;&#26684;&#27979;&#35797;&#65306;&#26102;&#38388;&#21487;&#38752;&#24615;&#26377;&#38480;&#65292;&#20294;&#20984;&#26174;&#20102;&#31038;&#20132;&#28212;&#26395;&#30340;&#20154;&#26684;&#24037;&#20855;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results. (arXiv:2306.04308v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-3 Davinci-003&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#29305;&#36136;&#65292;&#20294;&#22312;&#19981;&#21516;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#32842;&#22825;&#26426;&#22120;&#20154;GPT-3 Davinci-003&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#21450;&#20854;&#20010;&#24615;&#21270;&#36164;&#26009;&#30340;&#20154;&#26684;&#38382;&#21367;&#30340;&#26102;&#38388;&#21487;&#38752;&#24615;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22330;&#21512;&#65292;&#24515;&#29702;&#38382;&#21367;&#34987;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#28982;&#21518;&#23558;&#22238;&#31572;&#19982;&#20154;&#31867;&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#65292;&#26377;&#20123;&#37327;&#34920;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#26377;&#20123;&#21017;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#19968;&#33268;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;Davinci-003&#26174;&#31034;&#20986;&#19968;&#20010;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#23588;&#20854;&#26159;&#22312;&#20146;&#21644;&#21147;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#31572;&#30340;&#22522;&#30784;&#65292;&#26080;&#35770;&#26159;&#30001;&#20027;&#35266;&#33258;&#25105;&#21453;&#24605;&#36824;&#26159;&#39044;&#23450;&#31639;&#27861;&#39537;&#21160;&#65292;&#23578;&#19981;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
To assess the potential applications and limitations of chatbot GPT-3 Davinci-003, this study explored the temporal reliability of personality questionnaires applied to the chatbot and its personality profile. Psychological questionnaires were administered to the chatbot on two separate occasions, followed by a comparison of the responses to human normative data. The findings revealed varying levels of agreement in the chatbot's responses over time, with some scales displaying excellent while others demonstrated poor agreement. Overall, Davinci-003 displayed a socially desirable and pro-social personality profile, particularly in the domain of communion. However, the underlying basis of the chatbot's responses, whether driven by conscious self-reflection or predetermined algorithms, remains uncertain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#31995;&#32479;Allophant&#65292;&#32467;&#21512;&#32452;&#25104;&#24615;&#38899;&#32032;&#23884;&#20837;&#26041;&#27861;&#21644;&#20010;&#21035;&#30417;&#30563;&#30340;&#35821;&#38899;&#23646;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#32467;&#26500;&#65292;&#20351;&#24471;&#35813;&#31995;&#32479;&#33021;&#22815;&#20302;&#36164;&#28304;&#36827;&#34892;&#35821;&#38899;&#35782;&#21035;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#22788;&#29702;&#38476;&#29983;&#38899;&#32032;&#21644;&#38899;&#32032;&#24211;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04306</link><description>&lt;p&gt;
Allophant: &#24102;&#26377;&#21457;&#38899;&#23646;&#24615;&#30340;&#36328;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes. (arXiv:2306.04306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#31995;&#32479;Allophant&#65292;&#32467;&#21512;&#32452;&#25104;&#24615;&#38899;&#32032;&#23884;&#20837;&#26041;&#27861;&#21644;&#20010;&#21035;&#30417;&#30563;&#30340;&#35821;&#38899;&#23646;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#32467;&#26500;&#65292;&#20351;&#24471;&#35813;&#31995;&#32479;&#33021;&#22815;&#20302;&#36164;&#28304;&#36827;&#34892;&#35821;&#38899;&#35782;&#21035;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#22788;&#29702;&#38476;&#29983;&#38899;&#32032;&#21644;&#38899;&#32032;&#24211;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#31995;&#32479;Allophant&#12290;&#23427;&#21482;&#38656;&#35201;&#36328;&#35821;&#35328;&#30446;&#26631;&#35821;&#31181;&#30340;&#38899;&#32032;&#28165;&#21333;&#21363;&#21487;&#36827;&#34892;&#20302;&#36164;&#28304;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#20102;&#32452;&#25104;&#24615;&#38899;&#32032;&#23884;&#20837;&#26041;&#27861;&#20197;&#21450;&#20010;&#21035;&#30417;&#30563;&#30340;&#35821;&#38899;&#23646;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;Allophoible&#25968;&#25454;&#24211;&#30340;&#25193;&#23637;&#12290;&#36890;&#36807;&#23558;&#35813;&#25968;&#25454;&#24211;&#19982;&#22522;&#20110;&#36317;&#31163;&#30340;&#22270;&#38899;&#36716;&#25442;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;PHOIBLE&#28165;&#21333;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#23545;34&#31181;&#35821;&#35328;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#21152;&#20837;&#25552;&#39640;&#20102;&#35813;&#27169;&#22411;&#22788;&#29702;&#38476;&#29983;&#38899;&#32032;&#21644;&#38899;&#32032;&#24211;&#30340;&#33021;&#21147;&#12290;&#22312;&#21463;&#30417;&#30563;&#35821;&#35328;&#19978;&#65292;&#19982;&#27809;&#26377;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;11&#20010;&#30334;&#20998;&#28857;&#30340;&#38899;&#32032;&#35823;&#24046;&#29575;&#25913;&#36827;&#12290;&#22312;84&#31181;&#38646;-shot&#36801;&#31227;&#35821;&#35328;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;PER&#19979;&#38477;&#20102;2.63&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes Allophant, a multilingual phoneme recognizer. It requires only a phoneme inventory for cross-lingual transfer to a target language, allowing for low-resource recognition. The architecture combines a compositional phone embedding approach with individually supervised phonetic attribute classifiers in a multi-task architecture. We also introduce Allophoible, an extension of the PHOIBLE database. When combined with a distance based mapping approach for grapheme-to-phoneme outputs, it allows us to train on PHOIBLE inventories directly. By training and evaluating on 34 languages, we found that the addition of multi-task learning improves the model's capability of being applied to unseen phonemes and phoneme inventories. On supervised languages we achieve phoneme error rate improvements of 11 percentage points (pp.) compared to a baseline without multi-task learning. Evaluation of zero-shot transfer on 84 languages yielded a decrease in PER of 2.63 pp. over the baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26816;&#32034;&#30340;&#26041;&#27861;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#22810;&#36718;&#23545;&#35805;&#38382;&#31572;&#20013;&#20256;&#32479;&#26041;&#27861;&#30340;&#28431;&#27934;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#23545;&#35805;&#20381;&#36182;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04293</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#20250;&#35805;&#20381;&#36182;&#24314;&#27169;&#30340;&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#38382;&#31572;&#30340;&#30701;&#35821;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Phrase Retrieval for Open-Domain Conversational Question Answering with Conversational Dependency Modeling via Contrastive Learning. (arXiv:2306.04293v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26816;&#32034;&#30340;&#26041;&#27861;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#22810;&#36718;&#23545;&#35805;&#38382;&#31572;&#20013;&#20256;&#32479;&#26041;&#27861;&#30340;&#28431;&#27934;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#23545;&#35805;&#20381;&#36182;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#38382;&#31572;(ODConvQA)&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#19968;&#20010;&#36861;&#28335;-&#38405;&#35835;&#22120;&#27169;&#22411;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26816;&#32034;&#30340;&#26041;&#27861;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#20854;&#22312;ODConvQA&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#35805;&#20381;&#36182;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-Domain Conversational Question Answering (ODConvQA) aims at answering questions through a multi-turn conversation based on a retriever-reader pipeline, which retrieves passages and then predicts answers with them. However, such a pipeline approach not only makes the reader vulnerable to the errors propagated from the retriever, but also demands additional effort to develop both the retriever and the reader, which further makes it slower since they are not runnable in parallel. In this work, we propose a method to directly predict answers with a phrase retrieval scheme for a sequence of words, reducing the conventional two distinct subtasks into a single one. Also, for the first time, we study its capability for ODConvQA tasks. However, simply adopting it is largely problematic, due to the dependencies between previous and current turns in a conversation. To address this problem, we further introduce a novel contrastive learning strategy, making sure to reflect previous turns when 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#20998;&#26512;&#20013;&#22830;&#38134;&#34892;&#21457;&#24067;&#30340;&#25991;&#20214;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#35780;&#20272;&#20013;&#22830;&#38134;&#34892;&#25919;&#31574;&#24577;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04277</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20998;&#31867;&#25991;&#26412;&#34164;&#21547;&#27169;&#22411;&#20998;&#26512;Fed&#30340;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Analysis of the Fed's communication by using textual entailment model of Zero-Shot classification. (arXiv:2306.04277v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#20998;&#26512;&#20013;&#22830;&#38134;&#34892;&#21457;&#24067;&#30340;&#25991;&#20214;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#35780;&#20272;&#20013;&#22830;&#38134;&#34892;&#25919;&#31574;&#24577;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#20998;&#26512;&#20013;&#22830;&#38134;&#34892;&#21457;&#24067;&#30340;&#25991;&#20214;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#35780;&#20272;&#20013;&#22830;&#38134;&#34892;&#25919;&#31574;&#24577;&#24230;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#21508;&#22823;&#20013;&#22830;&#38134;&#34892;&#30340;&#36135;&#24065;&#25919;&#31574;&#23545;&#37329;&#34701;&#24066;&#22330;&#36235;&#21183;&#12289;&#39640;&#39118;&#38505;&#36164;&#20135;&#30340;&#23450;&#20215;&#21644;&#23454;&#20307;&#32463;&#27982;&#20135;&#29983;&#24191;&#27867;&#24433;&#21709;&#65292;&#24066;&#22330;&#21442;&#19982;&#32773;&#35797;&#22270;&#26356;&#20934;&#30830;&#22320;&#25429;&#25417;&#20013;&#22830;&#38134;&#34892;&#26410;&#26469;&#36135;&#24065;&#25919;&#31574;&#30340;&#21464;&#21270;&#12290;&#37492;&#20110;&#21457;&#24067;&#30340;&#25991;&#20214;&#20063;&#26159;&#20013;&#22830;&#38134;&#34892;&#19982;&#24066;&#22330;&#27807;&#36890;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#35821;&#27861;&#21477;&#27861;&#21644;&#25514;&#36766;&#19978;&#37117;&#35201;&#31934;&#24515;&#21046;&#20316;&#65292;&#40723;&#21169;&#25237;&#36164;&#32773;&#26356;&#20934;&#30830;&#22320;&#38405;&#35835;&#20013;&#22830;&#38134;&#34892;&#30340;&#25919;&#31574;&#31435;&#22330;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;&#23545;&#20013;&#22830;&#38134;&#34892;&#25991;&#20214;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#24050;&#32463;&#34987;&#24320;&#23637;&#65292;&#20294;&#24456;&#38590;&#20934;&#30830;&#35299;&#37322;&#25991;&#20214;&#30340;&#21547;&#20041;&#65292;&#29978;&#33267;&#26080;&#24847;&#20013;&#25913;&#21464;&#30340;&#24494;&#22937;&#24046;&#24322;&#20063;&#38590;&#20197;&#26126;&#30830;&#25429;&#25417;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#35780;&#20272;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#25152;&#28085;&#30422;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we analyze documents published by central banks using text mining techniques and propose a method to evaluate the policy tone of central banks. Since the monetary policies of major central banks have a broad impact on financial market trends, the pricing of risky assets, and the real economy, market participants are attempting to more accurately capture changes in the outlook for central banks' future monetary policies. Since the published documents are also an important tool for the central bank to communicate with the market, they are meticulously elaborated on grammatical syntax and wording, and investors are urged to read more accurately about the central bank's policy stance. Sentiment analysis on central bank documents has long been carried out, but it has been difficult to interpret the meaning of the documents accurately and to explicitly capture even the intentional change in nuance. This study attempts to evaluate the implication of the zero-shot text classific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;&#40614;&#20811;&#39118;&#36828;&#31243;&#35821;&#38899;&#22330;&#26223;&#19979;&#30340;&#19977;&#20010;&#20998;&#21106;&#20219;&#21153;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#24490;&#29615;&#35856;&#27874;&#29305;&#24449;&#30340;&#31354;&#38388;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#26631;&#20934;&#22768;&#23398;&#29305;&#24449;&#26469;&#25552;&#39640;&#20998;&#21106;&#25928;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04268</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#35856;&#27874;&#29305;&#24449;&#30340;&#22810;&#40614;&#20811;&#39118;&#20250;&#35758;&#35821;&#38899;&#33258;&#21160;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multi-microphone Automatic Speech Segmentation in Meetings Based on Circular Harmonics Features. (arXiv:2306.04268v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;&#40614;&#20811;&#39118;&#36828;&#31243;&#35821;&#38899;&#22330;&#26223;&#19979;&#30340;&#19977;&#20010;&#20998;&#21106;&#20219;&#21153;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#24490;&#29615;&#35856;&#27874;&#29305;&#24449;&#30340;&#31354;&#38388;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#26631;&#20934;&#22768;&#23398;&#29305;&#24449;&#26469;&#25552;&#39640;&#20998;&#21106;&#25928;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#20154;&#20998;&#31163;&#26159;&#22312;&#35821;&#38899;&#27969;&#20013;&#22238;&#31572;&#8220;&#35841;&#35762;&#20102;&#20160;&#20040;&#8221;&#30340;&#20219;&#21153;&#12290;&#27969;&#27700;&#32447;&#31995;&#32479;&#20381;&#36182;&#20110;&#35821;&#38899;&#20998;&#21106;&#26469;&#25552;&#21462;&#35828;&#35805;&#32773;&#30340;&#29255;&#27573;&#24182;&#23454;&#29616;&#24378;&#20581;&#30340;&#35828;&#35805;&#20154;&#20998;&#31163;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36828;&#31243;&#35821;&#38899;&#24773;&#20917;&#19979;&#30340;&#19977;&#20010;&#20998;&#21106;&#20219;&#21153;&#65306;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;(VAD)&#65292;&#37325;&#21472;&#35821;&#38899;&#26816;&#27979;(OSD)&#21644;&#35828;&#35805;&#20154;&#26356;&#25913;&#26816;&#27979;(SCD)&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#26377;&#23569;&#37327;&#30740;&#31350;&#35843;&#26597;&#20102;&#22810;&#40614;&#20811;&#39118;&#36828;&#31243;&#35821;&#38899;&#22330;&#26223;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#31354;&#38388;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22522;&#20110;&#24490;&#29615;&#35856;&#27874;&#22495;(CH-DOA)&#20013;&#30340;&#21040;&#36798;&#26041;&#21521;&#20272;&#35745;&#12290;&#36825;&#20123;&#31354;&#38388;&#29305;&#24449;&#20174;&#22810;&#40614;&#20811;&#39118;&#38899;&#39057;&#25968;&#25454;&#20013;&#25552;&#21462;&#65292;&#24182;&#19982;&#26631;&#20934;&#22768;&#23398;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#22312;AMI&#20250;&#35758;&#35821;&#26009;&#24211;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CH-DOA&#21487;&#20197;&#25913;&#21892;&#20998;&#21106;&#32467;&#26524;&#24182;&#19988;&#22312;&#40614;&#20811;&#39118;&#20572;&#29992;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speaker diarization is the task of answering Who spoke and when? in an audio stream. Pipeline systems rely on speech segmentation to extract speakers' segments and achieve robust speaker diarization. This paper proposes a common framework to solve three segmentation tasks in the distant speech scenario: Voice Activity Detection (VAD), Overlapped Speech Detection (OSD), and Speaker Change Detection (SCD). In the literature, a few studies investigate the multi-microphone distant speech scenario. In this work, we propose a new set of spatial features based on direction-of-arrival estimations in the circular harmonic domain (CH-DOA). These spatial features are extracted from multi-microphone audio data and combined with standard acoustic features. Experiments on the AMI meeting corpus show that CH-DOA can improve the segmentation while being robust in the case of deactivated microphones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#20102;&#20811;&#26381;&#31471;&#21040;&#31471;&#35821;&#38899;&#25688;&#35201;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#19981;&#33258;&#28982;&#36755;&#20986;&#21477;&#23376;&#30340;&#38382;&#39064;&#65292;&#39318;&#27425;&#25552;&#20986;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#35299;&#30721;&#22120;&#20013;&#65292;&#20854;&#23454;&#39564;&#32467;&#26524;&#20248;&#20110;&#22522;&#32447;&#21644;&#25968;&#25454;&#22686;&#24378;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.04233</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#25552;&#21319;&#20102;&#31471;&#21040;&#31471;&#35821;&#38899;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization. (arXiv:2306.04233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20102;&#20811;&#26381;&#31471;&#21040;&#31471;&#35821;&#38899;&#25688;&#35201;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#19981;&#33258;&#28982;&#36755;&#20986;&#21477;&#23376;&#30340;&#38382;&#39064;&#65292;&#39318;&#27425;&#25552;&#20986;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#35299;&#30721;&#22120;&#20013;&#65292;&#20854;&#23454;&#39564;&#32467;&#26524;&#20248;&#20110;&#22522;&#32447;&#21644;&#25968;&#25454;&#22686;&#24378;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#35821;&#38899;&#25688;&#35201;(E2E SSum)&#36890;&#36807;&#21333;&#20010;&#27169;&#22411;&#30452;&#25509;&#23558;&#36755;&#20837;&#35821;&#38899;&#24635;&#32467;&#20026;&#26131;&#20110;&#38405;&#35835;&#30340;&#30701;&#21477;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#26377;&#21069;&#36884;&#65292;&#22240;&#20026;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#32423;&#32852;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#20840;&#37096;&#22768;&#23398;&#20449;&#24687;&#24182;&#20943;&#36731;&#36716;&#24405;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25910;&#38598;&#35821;&#38899;-&#25688;&#35201;&#23545;&#30340;&#25104;&#26412;&#36739;&#39640;&#65292;E2E SSum&#27169;&#22411;&#24448;&#24448;&#20250;&#22240;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#19988;&#36755;&#20986;&#21477;&#23376;&#19981;&#33258;&#28982;&#32780;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#32570;&#28857;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LM)&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#25972;&#21512;&#21040;E2E SSum&#35299;&#30721;&#22120;&#20013;&#65292;LM&#33021;&#22815;&#29983;&#25104;&#33258;&#28982;&#21477;&#23376;&#30340;&#33021;&#21147;&#24456;&#24378;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#29420;&#31435;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#36801;&#31227;&#22522;&#32447;E2E SSum&#32534;&#30721;&#22120;&#32780;&#19981;&#26159;&#36890;&#24120;&#20351;&#29992;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#22522;&#32447;&#21644;&#25968;&#25454;&#22686;&#24378;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end speech summarization (E2E SSum) directly summarizes input speech into easy-to-read short sentences with a single model. This approach is promising because it, in contrast to the conventional cascade approach, can utilize full acoustical information and mitigate to the propagation of transcription errors. However, due to the high cost of collecting speech-summary pairs, an E2E SSum model tends to suffer from training data scarcity and output unnatural sentences. To overcome this drawback, we propose for the first time to integrate a pre-trained language model (LM), which is highly capable of generating natural sentences, into the E2E SSum decoder via transfer learning. In addition, to reduce the gap between the independently pre-trained encoder and decoder, we also propose to transfer the baseline E2E SSum encoder instead of the commonly used automatic speech recognition encoder. Experimental results show that the proposed model outperforms baseline and data augmented models.
&lt;/p&gt;</description></item><item><title>ECRTM&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#23884;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#65288;ECR&#65289;&#26469;&#35299;&#20915;&#20027;&#39064;&#23849;&#22604;&#38382;&#39064;&#65292;&#20351;&#24471;&#27599;&#20010;&#20135;&#29983;&#30340;&#20027;&#39064;&#37117;&#21253;&#21547;&#19981;&#21516;&#30340;&#21333;&#35789;&#35821;&#20041;&#65292;&#20174;&#32780;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#36830;&#36143;&#30340;&#20027;&#39064;&#20197;&#21450;&#39640;&#36136;&#37327;&#30340;&#25991;&#26723;&#20027;&#39064;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.04217</link><description>&lt;p&gt;
&#24102;&#26377;&#23884;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#30340;&#26377;&#25928;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Effective Neural Topic Modeling with Embedding Clustering Regularization. (arXiv:2306.04217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04217
&lt;/p&gt;
&lt;p&gt;
ECRTM&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#23884;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#65288;ECR&#65289;&#26469;&#35299;&#20915;&#20027;&#39064;&#23849;&#22604;&#38382;&#39064;&#65292;&#20351;&#24471;&#27599;&#20010;&#20135;&#29983;&#30340;&#20027;&#39064;&#37117;&#21253;&#21547;&#19981;&#21516;&#30340;&#21333;&#35789;&#35821;&#20041;&#65292;&#20174;&#32780;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#36830;&#36143;&#30340;&#20027;&#39064;&#20197;&#21450;&#39640;&#36136;&#37327;&#30340;&#25991;&#26723;&#20027;&#39064;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#20027;&#39064;&#27169;&#22411;&#36890;&#24120;&#23384;&#22312;&#24694;&#21517;&#26157;&#24432;&#30340;&#20027;&#39064;&#23849;&#22604;&#65306;&#21457;&#29616;&#30340;&#20027;&#39064;&#22312;&#35821;&#20041;&#19978;&#21521;&#24444;&#27492;&#25240;&#21472;&#65292;&#23548;&#33268;&#39640;&#24230;&#37325;&#22797;&#30340;&#20027;&#39064;&#12289;&#19981;&#36275;&#30340;&#20027;&#39064;&#21457;&#29616;&#21644;&#25439;&#22351;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21363;&#23884;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#20027;&#39064;&#27169;&#22411; (ECRTM)&#12290;&#38500;&#20102;&#29616;&#26377;&#30340;&#37325;&#26500;&#35823;&#24046;&#20043;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;(ECR)&#65292;&#23427;&#24378;&#21046;&#27599;&#20010;&#20027;&#39064;&#23884;&#20837;&#25104;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#21333;&#29420;&#32858;&#21512;&#30340;&#21333;&#35789;&#23884;&#20837;&#38598;&#32676;&#30340;&#20013;&#24515;&#12290;&#36825;&#20351;&#24471;&#27599;&#20010;&#20135;&#29983;&#30340;&#20027;&#39064;&#37117;&#21253;&#21547;&#19981;&#21516;&#30340;&#21333;&#35789;&#35821;&#20041;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#20027;&#39064;&#23849;&#22604;&#12290;&#22312;ECR&#30340;&#27491;&#21017;&#21270;&#19979;&#65292;&#25105;&#20204;&#30340;ECRTM&#29983;&#25104;&#20102;&#22810;&#26679;&#21270;&#21644;&#36830;&#36143;&#30340;&#20027;&#39064;&#65292;&#20197;&#21450;&#39640;&#36136;&#37327;&#30340;&#25991;&#26723;&#20027;&#39064;&#20998;&#24067;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;ECRTM&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20027;&#39064;&#23849;&#22604;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models have been prevalent for decades with various applications. However, existing topic models commonly suffer from the notorious topic collapsing: discovered topics semantically collapse towards each other, leading to highly repetitive topics, insufficient topic discovery, and damaged model interpretability. In this paper, we propose a new neural topic model, Embedding Clustering Regularization Topic Model (ECRTM). Besides the existing reconstruction error, we propose a novel Embedding Clustering Regularization (ECR), which forces each topic embedding to be the center of a separately aggregated word embedding cluster in the semantic space. This enables each produced topic to contain distinct word semantics, which alleviates topic collapsing. Regularized by ECR, our ECRTM generates diverse and coherent topics together with high-quality topic distributions of documents. Extensive experiments on benchmark datasets demonstrate that ECRTM effectively addresses the topic collapsing 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#25110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#26009;&#24211;&#20013;&#23454;&#20307;&#30340;&#20998;&#23618;&#32467;&#26500;&#21644;&#20851;&#31995;&#20998;&#24067;&#65292;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#21477;&#23376;&#32423;&#19978;&#19979;&#25991;&#34920;&#31034;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04203</link><description>&lt;p&gt;
&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#22686;&#24378;&#20851;&#31995;&#25552;&#21462;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Leveraging Knowledge Graph Embeddings to Enhance Contextual Representations for Relation Extraction. (arXiv:2306.04203v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#25110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#26009;&#24211;&#20013;&#23454;&#20307;&#30340;&#20998;&#23618;&#32467;&#26500;&#21644;&#20851;&#31995;&#20998;&#24067;&#65292;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#21477;&#23376;&#32423;&#19978;&#19979;&#25991;&#34920;&#31034;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22312;&#35299;&#20915;&#20219;&#21153;&#26041;&#38754;&#30340;&#26174;&#30528;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#22823;&#22810;&#25968;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#25110;&#39044;&#20808;&#35757;&#32451;&#22312;&#28023;&#37327;&#35821;&#26009;&#24211;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#37327;&#25968;&#25454;&#12290;&#26412;&#25991;&#20851;&#27880;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#35821;&#26009;&#24211;&#25552;&#20379;&#30340;&#30693;&#35782;&#26469;&#21019;&#24314;&#39640;&#24615;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23637;&#31034;&#65292;&#22312;&#19981;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#26009;&#24211;&#20013;&#23454;&#20307;&#30340;&#20998;&#23618;&#32467;&#26500;&#21644;&#20851;&#31995;&#20998;&#24067;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#21477;&#23376;&#32423;&#19978;&#19979;&#25991;&#34920;&#31034;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21457;&#29616;&#32467;&#26524;&#20196;&#20154;&#20852;&#22859;&#19988;&#24456;&#26377;&#24847;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction task is a crucial and challenging aspect of Natural Language Processing. Several methods have surfaced as of late, exhibiting notable performance in addressing the task; however, most of these approaches rely on vast amounts of data from large-scale knowledge graphs or language models pretrained on voluminous corpora. In this paper, we hone in on the effective utilization of solely the knowledge supplied by a corpus to create a high-performing model. Our objective is to showcase that by leveraging the hierarchical structure and relational distribution of entities within a corpus without introducing external knowledge, a relation extraction model can achieve significantly enhanced performance. We therefore proposed a relation extraction approach based on the incorporation of pretrained knowledge graph embeddings at the corpus scale into the sentence-level contextual representation. We conducted a series of experiments which revealed promising and very interesting res
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;ASR&#31995;&#32479;&#24182;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20248;&#21270;ASR&#31995;&#32479;&#25552;&#20379;&#32473;&#23567;&#23398;&#19968;&#24180;&#32423;&#23398;&#29983;&#30340;&#21453;&#39304;&#21487;&#20197;&#25552;&#39640;&#20182;&#20204;&#30340;&#38405;&#35835;&#36827;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.04190</link><description>&lt;p&gt;
&#22522;&#20110;ASR&#30340;&#38405;&#35835;&#25945;&#23398;&#36741;&#23548;&#31995;&#32479;&#65306;&#23545;&#23567;&#23398;&#29983;&#21453;&#39304;&#36827;&#34892;&#20248;&#21270;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
An ASR-Based Tutor for Learning to Read: How to Optimize Feedback to First Graders. (arXiv:2306.04190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04190
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;ASR&#31995;&#32479;&#24182;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20248;&#21270;ASR&#31995;&#32479;&#25552;&#20379;&#32473;&#23567;&#23398;&#19968;&#24180;&#32423;&#23398;&#29983;&#30340;&#21453;&#39304;&#21487;&#20197;&#25552;&#39640;&#20182;&#20204;&#30340;&#38405;&#35835;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#24212;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20110;&#38405;&#35835;&#32451;&#20064;&#20013;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#22312;&#19968;&#39033;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#27454;&#22522;&#20110;ASR&#30340;&#33655;&#20848;&#35821;&#38405;&#35835;&#36741;&#23548;&#36719;&#20214;&#65292;&#26088;&#22312;&#24110;&#21161;&#27491;&#22312;&#23398;&#20064;&#38405;&#35835;&#30340;&#23567;&#23398;&#19968;&#24180;&#32423;&#23398;&#29983;&#21450;&#26102;&#33719;&#24471;&#21453;&#39304;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ASR&#22312;&#38405;&#35835;&#30340;&#21021;&#32423;&#38454;&#27573;&#26377;&#28508;&#21147;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#38405;&#35835;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#35821;&#26009;&#24211;&#65288;JASMIN&#65289;&#20013;&#30340;&#20799;&#31461;&#35821;&#38899;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;ASR&#31995;&#32479;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992; Cohen's Kappa&#12289;Matthews&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F-measures&#31561;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21333;&#35789;&#20026;&#21333;&#20301;&#20998;&#26512;ASR&#31995;&#32479;&#30340;&#27491;&#30830;/&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#19982;&#20154;&#24037;&#21028;&#26029;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#26032;&#24320;&#21457;&#30340;ASR&#31995;&#32479;&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#21644;&#27491;&#30830;&#30340;&#25298;&#32477;&#38169;&#35823;&#35821;&#38899;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;ASR&#31995;&#32479;&#25552;&#20379;&#32473;&#23567;&#23398;&#19968;&#24180;&#32423;&#23398;&#29983;&#30340;&#21453;&#39304;&#21487;&#20197;&#25552;&#39640;&#20182;&#20204;&#30340;&#38405;&#35835;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interest in employing automatic speech recognition (ASR) in applications for reading practice has been growing in recent years. In a previous study, we presented an ASR-based Dutch reading tutor application that was developed to provide instantaneous feedback to first-graders learning to read. We saw that ASR has potential at this stage of the reading process, as the results suggested that pupils made progress in reading accuracy and fluency by using the software. In the current study, we used children's speech from an existing corpus (JASMIN) to develop two new ASR systems, and compared the results to those of the previous study. We analyze correct/incorrect classification of the ASR systems using human transcripts at word level, by means of evaluation measures such as Cohen's Kappa, Matthews Correlation Coefficient (MCC), precision, recall and F-measures. We observe improvements for the newly developed ASR systems regarding the agreement with human-based judgment and correct reje
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26032;&#30340;&#20013;&#25991;&#21477;&#23376;&#31616;&#21270;&#25968;&#25454;&#38598;CSS&#65292;&#24182;&#27979;&#35797;&#20102;&#20960;&#31181;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#21477;&#23376;&#31616;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.04188</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#23454;&#35777;&#30740;&#31350;&#8212;&#8212;&#20013;&#25991;&#21477;&#23376;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
A New Dataset and Empirical Study for Sentence Simplification in Chinese. (arXiv:2306.04188v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26032;&#30340;&#20013;&#25991;&#21477;&#23376;&#31616;&#21270;&#25968;&#25454;&#38598;CSS&#65292;&#24182;&#27979;&#35797;&#20102;&#20960;&#31181;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#21477;&#23376;&#31616;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#31616;&#21270;&#26159;&#19968;&#39033;&#26377;&#30410;&#20110;&#35821;&#35328;&#23398;&#20064;&#32773;&#21644;&#20799;&#31461;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#26356;&#22810;&#22320;&#20851;&#27880;&#33521;&#25991;&#21477;&#23376;&#30340;&#31616;&#21270;&#12290;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#65292;&#20013;&#25991;&#21477;&#23376;&#31616;&#21270;&#30340;&#21457;&#23637;&#30456;&#23545;&#36739;&#24930;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;CSS&#65292;&#19968;&#20010;&#35780;&#20272;&#20013;&#25991;&#21477;&#23376;&#31616;&#21270;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20154;&#31867;&#26631;&#27880;&#30340;&#25163;&#21160;&#31616;&#21270;&#65292;&#24182;&#36827;&#34892;&#20102;&#25968;&#25454;&#20998;&#26512;&#20197;&#23637;&#31034;&#20013;&#33521;&#25991;&#21477;&#23376;&#31616;&#21270;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;CSS&#19978;&#27979;&#35797;&#20102;&#20960;&#31181;&#26080;&#30417;&#30563;&#21644;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20316;&#20026;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#21477;&#23376;&#31616;&#21270;&#31995;&#32479;&#65292;&#24182;&#22312;CSS&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence Simplification is a valuable technique that can benefit language learners and children a lot. However, current research focuses more on English sentence simplification. The development of Chinese sentence simplification is relatively slow due to the lack of data. To alleviate this limitation, this paper introduces CSS, a new dataset for assessing sentence simplification in Chinese. We collect manual simplifications from human annotators and perform data analysis to show the difference between English and Chinese sentence simplifications. Furthermore, we test several unsupervised and zero/few-shot learning methods on CSS and analyze the automatic evaluation and human evaluation results. In the end, we explore whether Large Language Models can serve as high-quality Chinese sentence simplification systems by evaluating them on CSS.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#36947;&#22914;&#20309;&#19982;&#30693;&#36947;&#20160;&#20040;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#22238;&#31572;&#20851;&#20110;&#29992;&#25143;&#25163;&#20876;&#30340;&#22522;&#26412;&#20107;&#23454;&#12289;&#27969;&#31243;&#65292;&#24182;&#35299;&#20915;&#19968;&#20123;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;&#20182;&#20204;&#37319;&#29992;&#22270;(TARA)&#26469;&#32852;&#21512;&#34920;&#31034;&#27493;&#39588;&#21644;&#20107;&#23454;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#22312;&#22238;&#31572;&#23454;&#38469;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04187</link><description>&lt;p&gt;
&#30693;&#36947;&#22914;&#20309;&#19982;&#30693;&#36947;&#20160;&#20040;&#65306;&#29992;&#25143;&#25163;&#20876;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#26032;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Knowing-how &amp; Knowing-that: A New Task for Machine Reading Comprehension of User Manuals. (arXiv:2306.04187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30693;&#36947;&#22914;&#20309;&#19982;&#30693;&#36947;&#20160;&#20040;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#22238;&#31572;&#20851;&#20110;&#29992;&#25143;&#25163;&#20876;&#30340;&#22522;&#26412;&#20107;&#23454;&#12289;&#27969;&#31243;&#65292;&#24182;&#35299;&#20915;&#19968;&#20123;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;&#20182;&#20204;&#37319;&#29992;&#22270;(TARA)&#26469;&#32852;&#21512;&#34920;&#31034;&#27493;&#39588;&#21644;&#20107;&#23454;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#22312;&#22238;&#31572;&#23454;&#38469;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#25163;&#20876;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20855;&#26377;&#24040;&#22823;&#30340;&#23458;&#25143;&#26381;&#21153;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#30693;&#36947;&#22914;&#20309;&#19982;&#30693;&#36947;&#20160;&#20040;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#22238;&#31572;&#20851;&#20110;&#29992;&#25143;&#25163;&#20876;&#30340;&#22522;&#26412;&#20107;&#23454;&#12289;&#27969;&#31243;&#65292;&#24182;&#35299;&#20915;&#19968;&#20123;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;(TARA)&#20013;&#32852;&#21512;&#34920;&#31034;&#27493;&#39588;&#21644;&#20107;&#23454;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25903;&#25345;&#21508;&#31181;&#38382;&#39064;&#30340;&#32479;&#19968;&#25512;&#29702;&#12290;&#20026;&#20102;&#36827;&#34892;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#35780;&#20272;&#30740;&#31350;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#33258;&#21160;&#23558;&#29992;&#25143;&#25163;&#20876;&#35299;&#26512;&#25104;TARA&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#27979;&#35797;&#27169;&#22411;&#22312;&#22238;&#31572;&#23454;&#38469;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#29992;&#25143;&#25163;&#20876;&#34920;&#31034;&#20026;TARA&#26159;&#29992;&#25143;&#25163;&#20876;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#29702;&#24819;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;TARA&#30340;&#28145;&#20837;&#30740;&#31350;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#26410;&#26469;&#29992;&#25143;&#25163;&#20876;&#34920;&#31034;&#30340;&#38382;&#39064;&#21644;&#24191;&#27867;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#23558;&#29992;&#25143;&#25163;&#20876;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25512;&#21521;&#26356;&#23454;&#29992;&#21644;&#26377;&#25928;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine reading comprehension (MRC) of user manuals has huge potential in customer service. However,current methods have trouble answering complex questions. Therefore, we introduce the Knowing-how &amp; Knowing-that task that requires the model to answer factoid-style, procedure-style, and inconsistent questions about user manuals. We resolve this task by jointly representing the steps and facts in a graph (TARA), which supports a unified inference of various questions. Towards a systematical benchmarking study, we design a heuristic method to automatically parse user manuals into TARAs and build an annotated dataset to test the model's ability in answering real-world questions. Empirical results demonstrate that representing user manuals as TARAs is a desired solution for the MRC of user manuals. An in-depth investigation of TARA further sheds light on the issues and broader impacts of future representations of user manuals. We hope our work can move the MRC of user manuals to a more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#65292;&#21487;&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#36807;&#21435;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#20013;&#30340;&#27979;&#35797;&#27844;&#28431;&#21644;&#35780;&#20272;&#33258;&#21160;&#21270;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#26131;&#20110;&#25193;&#23637;&#65292;&#21487;&#20197;&#37319;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#12290;</title><link>http://arxiv.org/abs/2306.04181</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#32771;&#23448;&#8221;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Foundation Models with Language-Model-as-an-Examiner. (arXiv:2306.04181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#65292;&#21487;&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#36807;&#21435;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#20013;&#30340;&#27979;&#35797;&#27844;&#28431;&#21644;&#35780;&#20272;&#33258;&#21160;&#21270;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#26131;&#20110;&#25193;&#23637;&#65292;&#21487;&#20197;&#37319;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#38382;&#31572;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23427;&#26159;&#27979;&#35797;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;&#20840;&#38754;&#27979;&#35797;&#12290;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#25552;&#20986;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#20013;&#30475;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21363;&#27979;&#35797;&#27844;&#28431;&#21644;&#35780;&#20272;&#33258;&#21160;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#65288;LMAE&#65289;&#65292;&#20854;&#20013;LM&#20316;&#20026;&#30693;&#35782;&#28170;&#21338;&#30340;&#32771;&#23448;&#65292;&#26681;&#25454;&#20854;&#30693;&#35782;&#21046;&#23450;&#38382;&#39064;&#24182;&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#26131;&#20110;&#25193;&#23637;&#65292;&#22240;&#20026;&#21487;&#20197;&#37319;&#29992;&#21508;&#31181;LM&#20316;&#20026;&#32771;&#23448;&#65292;&#24182;&#19988;&#21487;&#20197;&#19981;&#26029;&#26356;&#26032;&#38382;&#39064;&#65292;&#32473;&#20104;&#26356;&#22810;&#26679;&#21270;&#30340;&#35302;&#21457;&#20027;&#39064;&#12290;&#20026;&#20102;&#26356;&#20840;&#38754;&#21644;&#20844;&#27491;&#22320;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#31574;&#30053;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25351;&#31034;LM&#32771;&#23448;&#22312;&#35768;&#22810;&#39046;&#22495;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20915;&#23450;&#22312;&#20309;&#26102;&#20351;&#29992;&#25991;&#26723;&#25110;QA-pair&#22238;&#31572;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04176</link><description>&lt;p&gt;
&#20309;&#26102;&#26597;&#38405;&#25991;&#26723;&#25110;QA&#21382;&#21490;&#35760;&#24405;&#65306;&#32479;&#19968;&#21644;&#26377;&#36873;&#25321;&#30340;&#24320;&#25918;&#39046;&#22495;QA&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
When to Read Documents or QA History: On Unified and Selective Open-domain QA. (arXiv:2306.04176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20915;&#23450;&#22312;&#20309;&#26102;&#20351;&#29992;&#25991;&#26723;&#25110;QA-pair&#22238;&#31572;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#38382;&#39064;&#65292;&#33268;&#21147;&#20110;&#21033;&#29992;&#30693;&#35782;&#36164;&#28304;&#22238;&#31572;&#21508;&#31181;&#21508;&#26679;&#30340;&#38382;&#39064;&#12290;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;QA-pair&#26159;&#20004;&#31181;&#24120;&#29992;&#30340;&#20449;&#24687;&#28304;&#65292;&#21069;&#32773;&#22312;&#22788;&#29702;&#24050;&#30693;&#38382;&#39064;&#26102;&#38750;&#24120;&#20934;&#30830;&#65292;&#32780;&#21518;&#32773;&#22312;&#22788;&#29702;&#26410;&#30693;&#38382;&#39064;&#26102;&#26356;&#20855;&#24191;&#27867;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#39044;&#27979;&#31572;&#26696;&#32622;&#20449;&#24230;&#36827;&#34892;&#20998;&#26512;&#30340;&#26041;&#26696;&#65292;&#20197;&#20915;&#23450;&#20309;&#26102;&#20351;&#29992;&#25991;&#26723;&#25110;QA-pair&#12290;&#35813;&#26041;&#27861;&#22312;Natural Questions&#21644;TriviaQA&#31561;&#24191;&#27867;&#37319;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of open-domain question answering, with the aim of answering a diverse range of questions leveraging knowledge resources. Two types of sources, QA-pair and document corpora, have been actively leveraged with the following complementary strength. The former is highly precise when the paraphrase of given question $q$ was seen and answered during training, often posed as a retrieval problem, while the latter generalizes better for unseen questions. A natural follow-up is thus leveraging both models, while a naive pipelining or integration approaches have failed to bring additional gains over either model alone. Our distinction is interpreting the problem as calibration, which estimates the confidence of predicted answers as an indicator to decide when to use a document or QA-pair corpus. The effectiveness of our method was validated on widely adopted benchmarks such as Natural Questions and TriviaQA.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#12290;&#23427;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#20107;&#23454;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#28155;&#21152;&#21040;&#38382;&#39064;&#20043;&#21069;&#65292;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.04136</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#29992;&#20110;&#38646;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering. (arXiv:2306.04136v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#12290;&#23427;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#20107;&#23454;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#28155;&#21152;&#21040;&#38382;&#39064;&#20043;&#21069;&#65292;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#22522;&#20110;&#20854;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23384;&#20648;&#30340;&#20869;&#37096;&#30693;&#35782;&#25191;&#34892;&#38646;&#26679;&#26412;&#23553;&#38381;&#20070;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20869;&#37096;&#21270;&#30340;&#30693;&#35782;&#21487;&#33021;&#19981;&#36275;&#25110;&#19981;&#27491;&#30830;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20107;&#23454;&#19978;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#20013;&#30452;&#25509;&#22686;&#24378;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#38382;&#39064;&#19982;&#30456;&#20851;&#20107;&#23454;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20197;&#25552;&#31034;&#30340;&#24418;&#24335;&#23558;&#26816;&#32034;&#21040;&#30340;&#20107;&#23454;&#38468;&#21152;&#21040;&#36755;&#20837;&#38382;&#39064;&#20043;&#21069;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;KAPING&#65288;Knowledge-Augmented language model PromptING&#65289;&#26080;&#38656;&#27169;&#22411;&#35757;&#32451;&#65292;&#23436;&#20840;&#38646;&#26679;&#26412;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;KAPING&#26694;&#26550;&#22312;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#65292;&#23454;&#29616;&#20102;&#26082;&#37325;&#35270;&#23545;&#40784;&#21448;&#37325;&#35270;&#31354;&#23545;&#40784;&#30340;&#19981;&#24179;&#34913;&#21333;&#35821;&#35789;&#23545;&#40784;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.04116</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#30340;&#26368;&#20248;&#36755;&#36816;&#22312;&#19981;&#24179;&#34913;&#30340;&#21333;&#35821;&#35789;&#23545;&#40784;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Unbalanced Optimal Transport for Unbalanced Word Alignment. (arXiv:2306.04116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#65292;&#23454;&#29616;&#20102;&#26082;&#37325;&#35270;&#23545;&#40784;&#21448;&#37325;&#35270;&#31354;&#23545;&#40784;&#30340;&#19981;&#24179;&#34913;&#21333;&#35821;&#35789;&#23545;&#40784;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35821;&#35789;&#23545;&#40784;&#23545;&#20110;&#27169;&#22411;&#21270;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#22320;&#65292;&#31354;&#23545;&#40784;&#26159;&#19968;&#31181;&#26222;&#36941;&#19988;&#20851;&#38190;&#30340;&#29616;&#35937;&#65292;&#29992;&#20110;&#22788;&#29702;&#35821;&#20041;&#19978;&#19981;&#30456;&#20284;&#30340;&#21477;&#23376;&#12290;&#35782;&#21035;&#31354;&#23545;&#40784;&#26412;&#36523;&#23601;&#26377;&#21161;&#20110;&#25512;&#26029;&#21477;&#23376;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#22240;&#20026;&#23427;&#34920;&#26126;&#23384;&#22312;&#20449;&#24687;&#19981;&#24179;&#31561;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#26368;&#20248;&#36755;&#36816;&#30340;&#23478;&#26063;&#65288;&#24179;&#34913;&#12289;&#37096;&#20998;&#21644;&#19981;&#24179;&#34913;&#36755;&#36816;&#65289;&#26159;&#33258;&#28982;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#27809;&#26377;&#37327;&#36523;&#23450;&#21046;&#30340;&#25216;&#26415;&#20063;&#33021;&#23454;&#29616;&#26082;&#37325;&#35270;&#23545;&#40784;&#21448;&#37325;&#35270;&#31354;&#23545;&#40784;&#30340;&#19981;&#24179;&#34913;&#21333;&#35821;&#35789;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#28085;&#30422;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#35774;&#32622;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#36890;&#29992;OT-based&#23545;&#40784;&#26041;&#27861;&#22312;&#20855;&#26377;&#39640;&#31354;&#23545;&#40784;&#39057;&#29575;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#26159;&#33021;&#31454;&#20105;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monolingual word alignment is crucial to model semantic interactions between sentences. In particular, null alignment, a phenomenon in which words have no corresponding counterparts, is pervasive and critical in handling semantically divergent sentences. Identification of null alignment is useful on its own to reason about the semantic similarity of sentences by indicating there exists information inequality. To achieve unbalanced word alignment that values both alignment and null alignment, this study shows that the family of optimal transport (OT), i.e., balanced, partial, and unbalanced OT, are natural and powerful approaches even without tailor-made techniques. Our extensive experiments covering unsupervised and supervised settings indicate that our generic OT-based alignment methods are competitive against the state-of-the-arts specially designed for word alignment, remarkably on challenging datasets with high null alignment frequencies.
&lt;/p&gt;</description></item><item><title>Gotta&#26159;&#19968;&#20010;&#22522;&#20110;&#22635;&#31354;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#24335;&#22635;&#31354;&#20219;&#21153;&#19982;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04101</link><description>&lt;p&gt;
Gotta: &#22522;&#20110;&#25552;&#31034;&#24335;&#22635;&#31354;&#25968;&#25454;&#22686;&#24378;&#30340;&#29983;&#25104;&#24335;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation. (arXiv:2306.04101v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04101
&lt;/p&gt;
&lt;p&gt;
Gotta&#26159;&#19968;&#20010;&#22522;&#20110;&#22635;&#31354;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#24335;&#22635;&#31354;&#20219;&#21153;&#19982;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;&#26088;&#22312;&#20165;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19978;&#19979;&#25991;&#27573;&#33853;&#20013;&#31934;&#30830;&#22320;&#25214;&#21040;&#19968;&#32452;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#24182;&#19988;&#36890;&#24120;&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#29702;&#35299;&#28145;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Gotta&#65292;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24335;&#25552;&#31034;&#24335;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#32531;&#35299;&#19978;&#36848;&#25361;&#25112;&#12290;&#21463;&#21040;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#25552;&#31034;&#24335;&#22635;&#31354;&#20219;&#21153;&#19982;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#23569;&#26679;&#26412;&#38382;&#39064;&#22238;&#31572;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#26368;&#36817;&#25552;&#31034;&#35843;&#25972;&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#25105;&#20204;&#20197;&#19982;&#20027;&#35201;&#30340;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30456;&#21516;&#30340;&#26684;&#24335;&#25552;&#20986;&#20102;&#22635;&#31354;&#20219;&#21153;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#22320;&#23398;&#20064;&#20004;&#20010;&#20219;&#21153;&#65292;&#20805;&#20998;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#30340;&#33021;&#21147;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;Gotta&#22987;&#32456;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#22635;&#31354;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#19981;&#20165;&#25552;&#39640;&#20102;&#24615;&#33021;&#36824;&#22686;&#24378;&#20102;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot question answering (QA) aims at precisely discovering answers to a set of questions from context passages while only a few training samples are available. Although existing studies have made some progress and can usually achieve proper results, they suffer from understanding deep semantics for reasoning out the questions. In this paper, we develop Gotta, a Generative prOmpT-based daTa Augmentation framework to mitigate the challenge above. Inspired by the human reasoning process, we propose to integrate the cloze task to enhance few-shot QA learning. Following the recent success of prompt-tuning, we present the cloze task in the same format as the main QA task, allowing the model to learn both tasks seamlessly together to fully take advantage of the power of prompt-tuning. Extensive experiments on widely used benchmarks demonstrate that Gotta consistently outperforms competitive baselines, validating the effectiveness of our proposed prompt-tuning-based cloze task, which not o
&lt;/p&gt;</description></item><item><title>XSemPLR&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#36328;&#35821;&#20041;&#35299;&#26512;&#22522;&#20934;&#65292;&#35206;&#30422;&#20102;22&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;8&#31181;&#24847;&#20041;&#34920;&#31034;&#65292;&#33021;&#22815;&#20840;&#38754;&#22320;&#35780;&#20215;&#22810;&#31181;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04085</link><description>&lt;p&gt;
XSemPLR&#65306;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;&#24847;&#20041;&#34920;&#31034;&#19979;&#30340;&#36328;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations. (arXiv:2306.04085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04085
&lt;/p&gt;
&lt;p&gt;
XSemPLR&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#36328;&#35821;&#20041;&#35299;&#26512;&#22522;&#20934;&#65292;&#35206;&#30422;&#20102;22&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;8&#31181;&#24847;&#20041;&#34920;&#31034;&#65292;&#33021;&#22815;&#20840;&#38754;&#22320;&#35780;&#20215;&#22810;&#31181;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#20041;&#35299;&#26512;&#26088;&#22312;&#23558;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#30340;&#26597;&#35810;&#36716;&#25442;&#20026;&#35832;&#22914;SQL&#12289;lambda&#28436;&#31639;&#21644;&#36923;&#36753;&#24418;&#24335;&#31561;&#24847;&#20041;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#37117;&#26159;&#20998;&#21035;&#25552;&#20986;&#24182;&#22312;&#26377;&#38480;&#30340;&#20219;&#21153;&#21644;&#24212;&#29992;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#38459;&#30861;&#20102;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;&#24847;&#20041;&#34920;&#31034;&#19979;&#20840;&#38754;&#21644;&#32479;&#19968;&#30340;&#35780;&#20215;&#36328;&#35821;&#20041;&#35299;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;XSemPLR&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#36328;&#35821;&#20041;&#35299;&#26512;&#22522;&#20934;&#65292;&#21253;&#25324;22&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;8&#31181;&#24847;&#20041;&#34920;&#31034;&#65292;&#35206;&#30422;&#20102;5&#20010;&#20219;&#21153;&#21644;164&#20010;&#22495;&#65292;&#36873;&#25321;&#20102;9&#20010;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#21644;&#31579;&#36873;&#12290;&#25105;&#20204;&#20351;&#29992;XSemPLR&#23545;&#22810;&#31181;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21253;&#25324;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#65288;mBERT&#12289;XLM-R&#65289;&#12289;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65288;mBART&#12289;mT5&#65289;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#65288;Codex&#12289;BLOOM&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;6&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#28085;&#30422;&#21508;&#31181;&#35821;&#35328;&#32452;&#21512;&#65288;&#21333;&#35821;&#35328;&#12289;&#22810;&#35821;&#35328;&#12289;&#36328;&#35821;&#35328;&#65289;&#21644;&#23398;&#20064;&#26679;&#26412;&#25968;&#37327;&#65288;f&#65289;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#19968;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#22312;&#36716;&#25442;&#22120;&#20013;&#36827;&#34892;&#20351;&#29992;&#32431;&#25991;&#26412;&#35821;&#26009;&#30340;&#24555;&#36895;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;TTS&#26041;&#27861;&#21644;&#25991;&#26412;&#22270;&#26041;&#27861;&#65292;&#33021;&#23558;&#30446;&#26631;&#39046;&#22495;&#30340;&#35789;&#38169;&#35823;&#29575;&#30456;&#23545;&#20943;&#23569;44&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.04076</link><description>&lt;p&gt;
&#20351;&#29992;&#32479;&#19968;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#22312;&#36716;&#25442;&#22120;&#20013;&#36827;&#34892;&#32431;&#25991;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Text-only Domain Adaptation using Unified Speech-Text Representation in Transducer. (arXiv:2306.04076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#19968;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#22312;&#36716;&#25442;&#22120;&#20013;&#36827;&#34892;&#20351;&#29992;&#32431;&#25991;&#26412;&#35821;&#26009;&#30340;&#24555;&#36895;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;TTS&#26041;&#27861;&#21644;&#25991;&#26412;&#22270;&#26041;&#27861;&#65292;&#33021;&#23558;&#30446;&#26631;&#39046;&#22495;&#30340;&#35789;&#38169;&#35823;&#29575;&#30456;&#23545;&#20943;&#23569;44&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#20351;&#29992;&#32431;&#25991;&#26412;&#35821;&#26009;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36890;&#36807;TTS&#20174;&#25991;&#26412;&#20013;&#21512;&#25104;&#38899;&#39057;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#26159;&#32791;&#36153;&#36164;&#28304;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#23398;&#20064;&#32479;&#19968;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#22312;Conformer Transducer&#65288;USTR-CT&#65289;&#20013;&#20197;&#23454;&#29616;&#20351;&#29992;&#32431;&#25991;&#26412;&#35821;&#26009;&#30340;&#24555;&#36895;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#19982;&#20043;&#21069;&#30340;&#25991;&#26412;&#22270;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#25991;&#26412;&#34920;&#31034;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23558;&#20854;&#31227;&#38500;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#22312;&#32447;&#37096;&#32626;&#26102;&#20462;&#25913;&#12290;&#20026;&#20102;&#25552;&#39640;&#33258;&#36866;&#24212;&#25928;&#29575;&#65292;&#36824;&#25506;&#32034;&#20102;&#21333;&#27493;&#21644;&#22810;&#27493;&#33258;&#36866;&#24212;&#12290;&#23558;LibriSpeech&#33258;&#36866;&#24212;&#21040;SPGISpeech&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#30446;&#26631;&#39046;&#22495;&#23558;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#30456;&#23545;&#20943;&#23569;&#20102;44&#65285;&#65292;&#27604;TTS&#26041;&#27861;&#21644;&#25991;&#26412;&#22270;&#26041;&#27861;&#26356;&#22909;&#22320;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#20272;&#35745;&#65288;ILME&#65289;&#30456;&#32467;&#21512;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation using text-only corpus is challenging in end-to-end(E2E) speech recognition. Adaptation by synthesizing audio from text through TTS is resource-consuming. We present a method to learn Unified Speech-Text Representation in Conformer Transducer(USTR-CT) to enable fast domain adaptation using the text-only corpus. Different from the previous textogram method, an extra text encoder is introduced in our work to learn text representation and is removed during inference, so there is no modification for online deployment. To improve the efficiency of adaptation, single-step and multi-step adaptations are also explored. The experiments on adapting LibriSpeech to SPGISpeech show the proposed method reduces the word error rate(WER) by relatively 44% on the target domain, which is better than those of TTS method and textogram method. Also, it is shown the proposed method can be combined with internal language model estimation(ILME) to further improve the performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;(CDA)&#32467;&#21512;&#26368;&#36817;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#26469;&#36827;&#34892;&#20559;&#35265;&#32531;&#35299;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#23545;&#20943;&#36731;&#24615;&#21035;&#20559;&#35265;&#26377;&#25928;&#65292;&#20854;&#20013;&#36866;&#37197;&#22120;&#35843;&#25972;&#25928;&#26524;&#26368;&#22909;&#65292;&#25552;&#31034;&#35843;&#25972;&#36866;&#21512;&#20110;GPT-2&#12290;&#20294;&#22312;&#20943;&#36731;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#25928;&#26524;&#19981;&#22826;&#26126;&#26174;&#65292;&#21487;&#33021;&#22240;&#20026;CDA&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04067</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models. (arXiv:2306.04067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;(CDA)&#32467;&#21512;&#26368;&#36817;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#26469;&#36827;&#34892;&#20559;&#35265;&#32531;&#35299;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#23545;&#20943;&#36731;&#24615;&#21035;&#20559;&#35265;&#26377;&#25928;&#65292;&#20854;&#20013;&#36866;&#37197;&#22120;&#35843;&#25972;&#25928;&#26524;&#26368;&#22909;&#65292;&#25552;&#31034;&#35843;&#25972;&#36866;&#21512;&#20110;GPT-2&#12290;&#20294;&#22312;&#20943;&#36731;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#25928;&#26524;&#19981;&#22826;&#26126;&#26174;&#65292;&#21487;&#33021;&#22240;&#20026;CDA&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#30340;&#23610;&#23544;&#19981;&#20165;&#20250;&#35753;&#23427;&#20204;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#32487;&#25215;&#26356;&#22810;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#20559;&#35265;&#65292;&#32780;&#19988;&#20063;&#20250;&#35753;&#32531;&#35299;&#36825;&#20123;&#20559;&#35265;&#21464;&#24471;&#35745;&#31639;&#19978;&#26114;&#36149;&#12290;&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;(CDA)&#32467;&#21512;&#26368;&#36817;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#26469;&#36827;&#34892;&#20559;&#35265;&#32531;&#35299;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20559;&#35265;&#31867;&#22411;&#19978;&#36827;&#34892;&#21069;&#32512;&#35843;&#25972;&#12289;&#25552;&#31034;&#35843;&#25972;&#21644;&#36866;&#37197;&#22120;&#35843;&#25972;&#26041;&#38754;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#21435;&#20559;&#35265;&#24615;&#33021;&#21644;&#20445;&#25345;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;(i)&#22312;&#20943;&#36731;&#24615;&#21035;&#20559;&#35265;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;&#36866;&#37197;&#22120;&#35843;&#25972;&#22987;&#32456;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#25552;&#31034;&#35843;&#25972;&#26356;&#36866;&#21512;&#20110;GPT-2&#32780;&#19981;&#26159;BERT&#65307;(ii)&#22312;&#20943;&#36731;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#19981;&#22826;&#26377;&#25928;&#65292;&#36825;&#21487;&#33021;&#24402;&#22240;&#20110;CDA&#30340;&#23616;&#38480;&#24615;&#65307;(iii)&#22312;&#26377;&#26102;&#21487;&#20197;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#27169;&#22411;&#34920;&#29616;&#31867;&#20284;&#25110;&#26356;&#22909;&#26102;&#25928;&#26524;&#20063;&#19981;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly large size of modern pretrained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. We find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for GPT-2 than BERT, (ii) are less effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA, and (iii) can perform similarly to or sometimes better than full fine-tuni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;NLP&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#20581;&#24247;&#32500;&#24230;&#30340;&#39044;&#31579;&#36873;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.04059</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;Reddit&#24086;&#23376;&#20197;&#30830;&#23450;&#24433;&#21709;&#24515;&#29702;&#20581;&#24247;&#30340;&#20581;&#24247;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Augmenting Reddit Posts to Determine Wellness Dimensions impacting Mental Health. (arXiv:2306.04059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;NLP&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#20581;&#24247;&#32500;&#24230;&#30340;&#39044;&#31579;&#36873;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#20581;&#24247;&#21361;&#26426;&#20013;&#65292;&#26377;&#24517;&#35201;&#35782;&#21035;&#20986;&#22312;&#33258;&#36848;&#25991;&#26412;&#20013;&#34920;&#29616;&#20986;&#30340;&#20581;&#24247;&#32500;&#24230;&#65288;WD&#65289;&#30340;&#21487;&#33021;&#36857;&#35937;&#12290;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#30340;WD&#20998;&#24067;&#20869;&#22312;&#22320;&#19981;&#24179;&#34913;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#29983;&#25104;NLP&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#20415;&#22312;&#20998;&#31867;WD&#30340;&#39044;&#31579;&#36873;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25552;&#31034;&#30340;&#29983;&#25104;NLP&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#35299;&#37322;&#21644;&#22686;&#24378;&#25968;&#25454;&#20043;&#38388;&#30340;ROUGE&#24471;&#20998;&#20197;&#21450;&#21477;&#27861;/&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#27169;&#22411;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#22312;&#22522;&#32447;&#65288;&#22914;Easy-Data Augmentation&#21644;Backtranslation&#65289;&#19978;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;&#24341;&#20837;&#25968;&#25454;&#22686;&#24378;&#20197;&#29983;&#25104;&#26356;&#22810;&#30340;&#35757;&#32451;&#26679;&#26412;&#21644;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;F-score&#21644;Matthew's Correlation Coefficient&#24471;&#20998;&#20998;&#21035;&#25552;&#39640;&#20102;13.11%&#21644;15.95%&#12290;
&lt;/p&gt;
&lt;p&gt;
Amid ongoing health crisis, there is a growing necessity to discern possible signs of Wellness Dimensions (WD) manifested in self-narrated text. As the distribution of WD on social media data is intrinsically imbalanced, we experiment the generative NLP models for data augmentation to enable further improvement in the pre-screening task of classifying WD. To this end, we propose a simple yet effective data augmentation approach through prompt-based Generative NLP models, and evaluate the ROUGE scores and syntactic/semantic similarity among existing interpretations and augmented data. Our approach with ChatGPT model surpasses all the other methods and achieves improvement over baselines such as Easy-Data Augmentation and Backtranslation. Introducing data augmentation to generate more training samples and balanced dataset, results in the improved F-score and the Matthew's Correlation Coefficient for upto 13.11% and 15.95%, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CAVEN - &#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#22238;&#31572;&#20197;&#21327;&#21161;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#36827;&#34892;&#24341;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.04047</link><description>&lt;p&gt;
&#29992;&#20110;&#25913;&#36827;&#35270;&#21548;&#34701;&#21512;&#23548;&#33322;&#30340;&#20027;&#21160;&#31232;&#30095;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Active Sparse Conversations for Improved Audio-Visual Embodied Navigation. (arXiv:2306.04047v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CAVEN - &#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#22238;&#31572;&#20197;&#21327;&#21161;&#33258;&#20027;&#23548;&#33322;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#36827;&#34892;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39640;&#25928;&#22320;&#23548;&#33322;&#21040;&#19968;&#20010;&#21548;&#35273;&#30446;&#26631;&#65292;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#33258;&#20027;&#26435;&#30340;&#23454;&#20307;&#24517;&#39035;&#19981;&#20165;&#35201;&#26377;&#33021;&#21147;&#26377;&#25928;&#22320;&#20351;&#29992;&#35270;&#21548;&#32447;&#32034;, &#32780;&#19988;&#36824;&#35201;&#26377;&#33021;&#21147;&#22312;&#19981;&#29306;&#29298;&#33258;&#20027;&#24615;&#30340;&#24773;&#20917;&#19979;&#20027;&#21160;&#23547;&#27714;&#20154;&#31867;/&#31070;&#35861;&#30340;&#24110;&#21161;&#65292;&#20363;&#22914;&#65292;&#24403;&#19981;&#30830;&#23450;&#23548;&#33322;&#21040;&#21738;&#37324;&#23547;&#25214;&#22024;&#26434;&#25110;&#38388;&#27463;&#24615;&#21548;&#35273;&#30446;&#26631;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAVEN-&#19968;&#31181;&#20855;&#26377;&#23545;&#35805;&#21151;&#33021;&#30340;&#38899;&#39057;&#35270;&#35273;&#23548;&#33322;&#20195;&#29702;&#65292;&#33021;&#22815;&#21521;&#20154;&#31867;/&#31070;&#35861;&#25552;&#20986;&#23548;&#33322;&#38382;&#39064;&#24182;&#22788;&#29702;&#31070;&#35861;&#30340;&#33258;&#30001;&#24418;&#24335;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#12290;&#22312;CAVEN&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;(RL)&#35774;&#32622;&#65292;&#23427;&#37197;&#22791;&#20102;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#27599;&#19968;&#27493;&#20174;&#19977;&#20010;&#20302;&#32423;&#31574;&#30053;&#20013;&#36873;&#25321;&#19968;&#20010;&#65292;&#21363;&#65306;(i)&#20351;&#29992;&#35270;&#21548;&#32447;&#32034;&#36827;&#34892;&#23548;&#33322;&#65292;&#25110;(ii)&#21521;&#31070;&#35861;&#25552;&#20986;&#38382;&#39064;&#24182;&#25509;&#25910;&#30701;&#25110;&#35814;&#32454;&#30340;&#22238;&#31572;&#65292;&#25110;(iii)&#25552;&#38382;&#26222;&#36941;&#38382;&#39064;(&#24403;&#19981;&#30830;&#23450;&#35813;&#38382;&#20160;&#20040;&#26102;)&#24182;&#33719;&#24471;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Efficient navigation towards an audio-goal necessitates an embodied agent to not only possess the ability to use audio-visual cues effectively, but also be equipped to actively (but occasionally) seek human/oracle assistance without sacrificing autonomy, e.g., when it is uncertain of where to navigate towards locating a noisy or sporadic audio goal. To this end, we present CAVEN -- a conversational audio-visual embodied navigation agent that is capable of posing navigation questions to a human/oracle and processing the oracle responses; both in free-form natural language. At the core of CAVEN is a multimodal hierarchical reinforcement learning (RL) setup that is equipped with a high-level policy that is trained to choose from one of three low-level policies (at every step), namely: (i) to navigate using audio-visual cues, or (ii) to frame a question to the oracle and receive a short or detailed response, or (iii) ask generic questions (when unsure of what to ask) and receive instructio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30524;&#21160;&#36861;&#36394;&#12289;&#21477;&#23376;&#32423;&#29305;&#24449;&#26631;&#27880;&#21644;&#24635;&#20307;&#21442;&#19982;&#24230;&#35843;&#26597;&#65292;&#22312;&#20998;&#26512;23&#20010;&#35835;&#32773;&#23545;&#20004;&#31687;&#30701;&#31687;&#23567;&#35828;&#30340;&#21453;&#24212;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#39044;&#27979;&#35835;&#32773;&#21442;&#19982;&#24230;&#26041;&#38754;&#30340;&#19968;&#20123;&#20851;&#38190;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04043</link><description>&lt;p&gt;
&#36890;&#36807;&#30524;&#21160;&#36861;&#36394;&#21644;&#35821;&#35328;&#29305;&#24449;&#20998;&#26512;&#25991;&#23398;&#23567;&#35828;&#20013;&#30340;&#35835;&#32773;&#21442;&#19982;&#24230;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Reader Engagement in Literary Fiction through Eye Tracking and Linguistic Features. (arXiv:2306.04043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04043
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30524;&#21160;&#36861;&#36394;&#12289;&#21477;&#23376;&#32423;&#29305;&#24449;&#26631;&#27880;&#21644;&#24635;&#20307;&#21442;&#19982;&#24230;&#35843;&#26597;&#65292;&#22312;&#20998;&#26512;23&#20010;&#35835;&#32773;&#23545;&#20004;&#31687;&#30701;&#31687;&#23567;&#35828;&#30340;&#21453;&#24212;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#39044;&#27979;&#35835;&#32773;&#21442;&#19982;&#24230;&#26041;&#38754;&#30340;&#19968;&#20123;&#20851;&#38190;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25429;&#25417;&#35835;&#32773;&#22312;&#23567;&#35828;&#20013;&#30340;&#21442;&#19982;&#24230;&#26159;&#21465;&#20107;&#29702;&#35299;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#26497;&#20026;&#37325;&#35201;&#30340;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#30524;&#21160;&#36861;&#36394;&#12289;&#21477;&#23376;&#32423;&#21035;&#30340;&#26631;&#27880;&#21644;&#24635;&#20307;&#21442;&#19982;&#24230;&#35843;&#26597;&#65292;&#25910;&#38598;&#20102;23&#20010;&#35835;&#32773;&#23545;&#20004;&#31687;&#30701;&#31687;&#23567;&#35828;&#30340;&#21453;&#24212;&#65292;&#24182;&#20998;&#26512;&#20102;&#25991;&#26412;&#21508;&#31181;&#29305;&#24615;&#22312;&#39044;&#27979;&#35835;&#32773;&#21442;&#19982;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#24615;&#12290;&#30001;&#20110;&#23567;&#35828;&#30340;&#20048;&#36259;&#39640;&#24230;&#20381;&#36182;&#20110;&#35821;&#22659;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#25968;&#25454;&#20013;&#30340;&#20010;&#20307;&#24046;&#24322;&#12290;&#28145;&#20837;&#20102;&#35299;&#23567;&#35828;&#20013;&#24341;&#20154;&#20837;&#32988;&#30340;&#22240;&#32032;&#23558;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#25351;&#23548;&#29992;&#20110;&#21019;&#24847;&#21465;&#36848;&#29983;&#25104;&#21644;&#21327;&#20316;&#20889;&#20316;&#24037;&#20855;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capturing readers' engagement in fiction is a challenging but important aspect of narrative understanding. In this study, we collected 23 readers' reactions to 2 short stories through eye tracking, sentence-level annotations, and an overall engagement scale survey. We analyzed the significance of various qualities of the text in predicting how engaging a reader is likely to find it. As enjoyment of fiction is highly contextual, we also investigated individual differences in our data. Furthering our understanding of what captivates readers in fiction will help better inform models used in creative narrative generation and collaborative writing tools.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#25552;&#31034;&#21644;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#27604;&#26631;&#20934;&#35843;&#25972;&#26041;&#27861;&#26356;&#22823;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.04009</link><description>&lt;p&gt;
&#20351;&#29992;&#36719;&#25552;&#31034;&#21644;&#38543;&#26426;&#28216;&#36208;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#35302;&#21457;&#22810;&#36339;&#25512;&#29702;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks. (arXiv:2306.04009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#36719;&#25552;&#31034;&#21644;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#27604;&#26631;&#20934;&#35843;&#25972;&#26041;&#27861;&#26356;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21487;&#20197;&#36731;&#26494;&#22320;&#35760;&#24518;&#26377;&#20851;&#23454;&#20307;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24448;&#24448;&#22312;&#32452;&#21512;&#20004;&#20010;&#25110;&#22810;&#20010;&#20107;&#23454;&#20197;&#25191;&#34892;&#22810;&#36339;&#25512;&#29702;&#30340;&#38382;&#31572;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#25913;&#21892;&#36825;&#20010;&#38480;&#21046;&#65292;&#36825;&#20123;&#25216;&#26415;&#20381;&#38752;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#36719;&#25552;&#31034;&#26469;&#24341;&#23548;LM&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#22810;&#36339;&#38382;&#39064;&#26144;&#23556;&#21040;&#36890;&#21521;&#31572;&#26696;&#30340;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#26469;&#38142;&#24335;&#32534;&#30721;&#23427;&#20204;&#30340;&#30693;&#35782;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;T5 LM&#19978;&#65292;&#22312;&#22238;&#31572;&#38656;&#35201;2&#36339;&#25512;&#29702;&#30340;&#38382;&#39064;&#26041;&#38754;&#65292;&#34920;&#29616;&#20986;&#20102;&#27604;&#26631;&#20934;&#35843;&#25972;&#26041;&#27861;&#26356;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks. In this work, we propose techniques that improve upon this limitation by relying on random walks over structured knowledge graphs. Specifically, we use soft prompts to guide LMs to chain together their encoded knowledge by learning to map multi-hop questions to random walk paths that lead to the answer. Applying our methods on two T5 LMs shows substantial improvements over standard tuning approaches in answering questions that require 2-hop reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XLex&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#35789;&#20856;&#21644;Transformer&#27169;&#22411;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20013;&#25552;&#20379;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03997</link><description>&lt;p&gt;
&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#65306;&#20174;Transformer&#22238;&#24402;&#21040;&#21487;&#35299;&#37322;&#24615;&#35789;&#20856;(XLex)
&lt;/p&gt;
&lt;p&gt;
Sentiment Analysis in Finance: From Transformers Back to eXplainable Lexicons (XLex). (arXiv:2306.03997v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XLex&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#35789;&#20856;&#21644;Transformer&#27169;&#22411;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20013;&#25552;&#20379;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35789;&#20856;&#30340;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#21033;&#29992;&#20154;&#24037;&#19987;&#23478;&#21019;&#24314;&#30340;&#19987;&#38376;&#27880;&#37322;&#30340;&#35789;&#20856;&#20174;&#37329;&#34701;&#25991;&#26412;&#20013;&#25552;&#21462;&#24773;&#24863;&#12290;&#34429;&#28982;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#23454;&#29616;&#31616;&#21333;&#19988;&#36895;&#24230;&#24555;&#65292;&#20294;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#25163;&#21160;&#27880;&#37322;&#24037;&#20316;&#26469;&#21019;&#24314;&#12289;&#32500;&#25252;&#21644;&#26356;&#26032;&#35789;&#20856;&#12290;&#36825;&#20123;&#26041;&#27861;&#20063;&#34987;&#35748;&#20026;&#19981;&#22914;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;&#22914;Transformer&#27169;&#22411;&#65289;&#20248;&#36234;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#12290;&#28982;&#32780;&#65292;Transformer&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#28041;&#21450;&#26174;&#33879;&#30340;&#39044;&#27979;&#26102;&#38388;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#23454;&#26102;&#29983;&#20135;&#29615;&#22659;&#25110;&#22788;&#29702;&#33021;&#21147;&#21463;&#38480;&#30340;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;eXplainable Lexicons&#65288;XLex&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#21644;Transformer&#27169;&#22411;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexicon-based sentiment analysis (SA) in finance leverages specialized, manually annotated lexicons created by human experts to extract sentiment from financial texts. Although lexicon-based methods are simple to implement and fast to operate on textual data, they require considerable manual annotation efforts to create, maintain, and update the lexicons. These methods are also considered inferior to the deep learning-based approaches, such as transformer models, which have become dominant in various NLP tasks due to their remarkable performance. However, transformers require extensive data and computational resources for both training and testing. Additionally, they involve significant prediction times, making them unsuitable for real-time production environments or systems with limited processing capabilities. In this paper, we introduce a novel methodology named eXplainable Lexicons (XLex) that combines the advantages of both lexicon-based methods and transformer models. We propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#36136;&#37327;&#27880;&#37322;&#24037;&#20316;&#27969;&#31243;&#31216;&#20026;DQA&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#22320;&#35780;&#20272;&#23545;&#35805;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#19968;&#20123;&#23458;&#35266;&#23545;&#35805;&#23646;&#24615;&#30340;&#21028;&#26029;&#12290;</title><link>http://arxiv.org/abs/2306.03984</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#30340;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#30340;&#35780;&#20272;&#25351;&#26631;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs. (arXiv:2306.03984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#36136;&#37327;&#27880;&#37322;&#24037;&#20316;&#27969;&#31243;&#31216;&#20026;DQA&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#22320;&#35780;&#20272;&#23545;&#35805;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#19968;&#20123;&#23458;&#35266;&#23545;&#35805;&#23646;&#24615;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20132;&#20114;&#36136;&#37327;&#23545;&#20110;&#25913;&#36827;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#23545;&#35805;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#35201;&#20040;&#20391;&#37325;&#20110;&#35780;&#20272;&#21333;&#20010;&#23545;&#35805;&#36718;&#27425;&#30340;&#36136;&#37327;&#65292;&#35201;&#20040;&#20174;&#32456;&#31471;&#29992;&#25143;&#31435;&#21363;&#22312;&#20132;&#20114;&#20043;&#21518;&#25910;&#38598;&#23545;&#35805;&#32423;&#21035;&#30340;&#36136;&#37327;&#27979;&#37327;&#25968;&#25454;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#32423;&#21035;&#27880;&#37322;&#24037;&#20316;&#27969;&#31243;&#31216;&#20026;&#23545;&#35805;&#36136;&#37327;&#27880;&#37322;&#65288;DQA&#65289;&#12290;DQA&#19987;&#23478;&#27880;&#37322;&#21592;&#35780;&#20272;&#25972;&#20010;&#23545;&#35805;&#30340;&#36136;&#37327;&#65292;&#24182;&#26631;&#35760;&#23545;&#35805;&#30340;&#30446;&#26631;&#23436;&#25104;&#21644;&#29992;&#25143;&#24773;&#24863;&#31561;&#23646;&#24615;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;&#65288;i&#65289;&#23613;&#31649;&#23545;&#35805;&#36136;&#37327;&#19981;&#33021;&#23436;&#20840;&#20998;&#35299;&#25104;&#23545;&#35805;&#32423;&#21035;&#23646;&#24615;&#65292;&#20294;&#26576;&#20123;&#23458;&#35266;&#23545;&#35805;&#23646;&#24615;&#19982;&#23545;&#35805;&#36136;&#37327;&#30340;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#30528;&#24378;&#20851;&#31995;&#65307;&#65288;ii&#65289;&#23545;&#20110;&#23545;&#35805;&#32423;&#21035;&#36136;&#37327;&#20272;&#35745;&#20219;&#21153;&#65292;&#19968;&#20010;&#22312;&#23545;&#35805;&#32423;&#21035;&#27880;&#37322;&#19978;&#35757;&#32451;&#30340;&#30417;&#30563;&#27169;&#22411;&#20248;&#20110;&#20165;&#22522;&#20110;&#32858;&#21512;&#36718;&#27425;&#32423;&#21035;&#29305;&#24449;&#30340;&#26041;&#27861;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#20351;&#29992;DQA&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#30340;&#23545;&#35805;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measurement of interaction quality is a critical task for the improvement of spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialog-level attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#22303;&#32819;&#20854;&#35821;&#24320;&#25918;&#35775;&#38382;&#27169;&#22411;&#19981;&#36275;&#30340;&#26041;&#27861;&#65306;&#21019;&#24314;&#22823;&#22411;&#22303;&#32819;&#20854;&#35821;&#25968;&#25454;&#38598;&#65292;&#29992;&#36825;&#20123;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20197;&#33719;&#24471;&#29305;&#23450;&#20219;&#21153;&#30340;&#19987;&#19994;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.03978</link><description>&lt;p&gt;
&#29992;&#22303;&#32819;&#20854;&#35821;&#25968;&#25454;&#35757;&#32451;&#21644;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
B\"{u}y\"{u}k dil modellerinin T\"{u}rk\c{c}e verisetleri ile e\u{g}itilmesi ve ince ayarlanmas\i. (arXiv:2306.03978v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#22303;&#32819;&#20854;&#35821;&#24320;&#25918;&#35775;&#38382;&#27169;&#22411;&#19981;&#36275;&#30340;&#26041;&#27861;&#65306;&#21019;&#24314;&#22823;&#22411;&#22303;&#32819;&#20854;&#35821;&#25968;&#25454;&#38598;&#65292;&#29992;&#36825;&#20123;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20197;&#33719;&#24471;&#29305;&#23450;&#20219;&#21153;&#30340;&#19987;&#19994;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#25552;&#21319;&#21644;&#21560;&#24341;&#21147;&#65292;&#24182;&#22788;&#20110;&#23494;&#38598;&#30740;&#31350;&#38454;&#27573;&#12290;&#20854;&#20013;&#24320;&#25918;&#35775;&#38382;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#24050;&#20844;&#24320;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#20351;&#29992;&#19968;&#20123;&#25216;&#26415;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#29305;&#23450;&#20219;&#21153;&#30340;&#19987;&#19994;&#27169;&#22411;&#12290;&#23601;&#22303;&#32819;&#20854;&#35821;&#32780;&#35328;&#65292;&#24320;&#25918;&#35775;&#38382;&#30340;&#27169;&#22411;&#19981;&#25552;&#20379;&#28385;&#24847;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#24819;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65306;&#21019;&#24314;&#22823;&#22411;&#22303;&#32819;&#20854;&#35821;&#25968;&#25454;&#38598;&#65292;&#29992;&#36825;&#20123;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#29992;&#22303;&#32819;&#20854;&#35821;&#36755;&#20837;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#22303;&#32819;&#20854;&#35821;&#35757;&#32451;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#32467;&#35770;&#20013;&#25552;&#20986;&#20102;&#36825;&#20123;&#23454;&#39564;&#30340;&#32467;&#26524;&#21644;&#36827;&#19968;&#27493;&#24037;&#20316;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have advanced enormously, gained vast attraction and are having a phase of intensed research. Some of the developed models and training datasets have been made open-accessible. Hence these may be further fine-tuned with some techniques to obtain specialized models for specific tasks. When it comes to Turkish language, open-access models do not provide satisfactory coverage. This is also observed over published datasets. In this work, we propose some ideas to mitigate this issue: creating large Turkish datasets, training LLMs with these and fine-tuning pre-trained models with Turkish inputs. We report our findings on Turkish-based trainings with the problems encountered along the way. We conclude with outcomes of these experiments and propose ideas for further works.  - B\"uy\"uk dil modelleri inan{\i}lmaz \"ol\c{c}\"ude geli\c{s}mekte, b\"uy\"uk ilgi toplayarak ve \"uzerlerinde yo\u{g}un ara\c{s}tirmalarin yapildi\u{g}i bir d\"onemdedirler. Geli\c{s}tirilen mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#35805;&#35805;&#35821;&#29305;&#24449;&#22686;&#24378;&#23545;&#35805;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#22270;&#34920;&#31034;&#21644;&#24341;&#20837;&#22686;&#24378;&#20998;&#31163;&#30446;&#26631;&#65292;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#23545;&#35805;&#35821;&#22659;&#21644;&#21033;&#29992;&#20869;&#22312;&#30340;&#35805;&#35821;&#32467;&#26500;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23545;&#35805;&#20998;&#31163;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03975</link><description>&lt;p&gt;
&#37325;&#22609;&#35805;&#35821;&#35821;&#31687;&#29702;&#35299;&#20197;&#20419;&#36827;&#23545;&#35805;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Revisiting Conversation Discourse for Dialogue Disentanglement. (arXiv:2306.03975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#35805;&#35805;&#35821;&#29305;&#24449;&#22686;&#24378;&#23545;&#35805;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#22270;&#34920;&#31034;&#21644;&#24341;&#20837;&#22686;&#24378;&#20998;&#31163;&#30446;&#26631;&#65292;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#23545;&#35805;&#35821;&#22659;&#21644;&#21033;&#29992;&#20869;&#22312;&#30340;&#35805;&#35821;&#32467;&#26500;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23545;&#35805;&#20998;&#31163;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20998;&#31163;&#26088;&#22312;&#23558;&#26102;&#38388;&#39034;&#24207;&#25490;&#21015;&#30340;&#35805;&#35821;&#20998;&#38548;&#25104;&#20960;&#20010;&#29420;&#31435;&#30340;&#20250;&#35805;&#12290;&#23545;&#35805;&#35805;&#35821;&#26412;&#36136;&#19978;&#26159;&#30001;&#24213;&#23618;&#35821;&#31687;&#32452;&#32455;&#21644;&#25551;&#36848;&#30340;&#65292;&#22240;&#27492;&#23545;&#35805;&#20998;&#31163;&#38656;&#35201;&#23436;&#20840;&#29702;&#35299;&#21644;&#21033;&#29992;&#20869;&#22312;&#30340;&#35805;&#35821;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#23545;&#35805;&#35805;&#35821;&#29305;&#24449;&#20840;&#38754;&#22686;&#24378;&#23545;&#35805;&#20998;&#31163;&#12290;&#22312;&#29305;&#24449;&#32534;&#30721;&#38454;&#27573;&#65292;&#25105;&#20204;&#26500;&#24314;&#24322;&#26500;&#22270;&#34920;&#31034;&#26469;&#27169;&#25311;&#21508;&#31181;&#23545;&#35805;&#29305;&#23450;&#30340;&#35805;&#35821;&#32467;&#26500;&#29305;&#24449;&#65292;&#21253;&#25324;&#38745;&#24577;&#30340;&#35762;&#35805;&#32773;&#35282;&#33394;&#32467;&#26500;&#65288;&#21363;&#35762;&#35805;&#32773;&#35805;&#35821;&#21644;&#35762;&#35805;&#32773;&#25552;&#21450;&#32467;&#26500;&#65289;&#21644;&#21160;&#24577;&#30340;&#19978;&#19979;&#25991;&#32467;&#26500;&#65288;&#21363;&#35805;&#35821;&#36317;&#31163;&#21644;&#37096;&#20998;&#22238;&#22797;&#32467;&#26500;&#65289;&#12290;&#25105;&#20204;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#32467;&#26500;&#24863;&#30693;&#26694;&#26550;&#65292;&#20197;&#38598;&#25104;&#20016;&#23500;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#26356;&#22909;&#22320;&#24314;&#27169;&#23545;&#35805;&#35821;&#22659;&#12290;&#20854;&#27425;&#65292;&#22312;&#27169;&#22411;&#32763;&#35793;&#38454;&#27573;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#20998;&#31163;&#30446;&#26631;&#65292;&#20197;&#21033;&#29992;&#20869;&#22312;&#30340;&#35805;&#35821;&#32467;&#26500;&#20449;&#24687;&#26469;&#36827;&#34892;&#20998;&#31163;&#36807;&#31243;&#12290;&#22312;&#19981;&#21516;&#30340;&#23545;&#35805;&#20998;&#31163;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue disentanglement aims to detach the chronologically ordered utterances into several independent sessions. Conversation utterances are essentially organized and described by the underlying discourse, and thus dialogue disentanglement requires the full understanding and harnessing of the intrinsic discourse attribute. In this paper, we propose enhancing dialogue disentanglement by taking full advantage of the dialogue discourse characteristics. First of all, \textbf{in feature encoding stage}, we construct the heterogeneous graph representations to model the various dialogue-specific discourse structural features, including the static speaker-role structures (i.e., speaker-utterance and speaker-mentioning structure) and the dynamic contextual structures (i.e., the utterance-distance and partial-replying structure). We then develop a structure-aware framework to integrate the rich structural features for better modeling the conversational semantic context. Second, \textbf{in model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; TKDP &#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#20013;&#36890;&#36807;&#25972;&#21512;&#19977;&#31181;&#19981;&#21516;&#26469;&#28304;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290; &#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#23545;&#20110;&#21407;&#22987;&#30340;&#28145;&#24230;&#25552;&#31034;&#26041;&#27861;&#25552;&#39640;&#20102;&#26368;&#22810; 11.53% &#30340; F1&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110; 8 &#31181;&#34920;&#29616;&#24378;&#21170;&#30340; few-shot NER &#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03974</link><description>&lt;p&gt;
TKDP: &#19977;&#37325;&#30693;&#35782;&#22686;&#24378;&#30340;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#22312;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TKDP: Threefold Knowledge-enriched Deep Prompt Tuning for Few-shot Named Entity Recognition. (arXiv:2306.03974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; TKDP &#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#20013;&#36890;&#36807;&#25972;&#21512;&#19977;&#31181;&#19981;&#21516;&#26469;&#28304;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290; &#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#23545;&#20110;&#21407;&#22987;&#30340;&#28145;&#24230;&#25552;&#31034;&#26041;&#27861;&#25552;&#39640;&#20102;&#26368;&#22810; 11.53% &#30340; F1&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110; 8 &#31181;&#34920;&#29616;&#24378;&#21170;&#30340; few-shot NER &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36890;&#36807;&#26377;&#38480;&#27880;&#37322;&#31034;&#20363;&#26469;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#36716;&#31227;&#20869;&#37096;&#25110;&#22806;&#37096;&#36164;&#28304;&#25104;&#20026;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#20013;&#25972;&#21512;&#20016;&#23500;&#30340;&#30693;&#35782;&#20197;&#23454;&#29616;&#26356;&#24378;&#30340;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#26694;&#26550;&#19982;&#19977;&#37325;&#30693;&#35782;&#65288;&#21363; TKDP&#65289;&#65292;&#21253;&#25324;&#20869;&#37096;&#30340; 1&#65289;&#19978;&#19979;&#25991;&#30693;&#35782;&#21644;&#22806;&#37096;&#30340; 2&#65289;&#26631;&#31614;&#30693;&#35782;&#21644; 3&#65289;&#20041;&#21407;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;TKDP &#23545;&#19977;&#20010;&#29305;&#24449;&#28304;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#36719;&#25552;&#31034;&#23884;&#20837;&#20013;&#65292;&#36827;&#32780;&#27880;&#20837;&#21040;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20197;&#20419;&#36827;&#39044;&#27979;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#30693;&#35782;&#22686;&#24378;&#27169;&#22411;&#30456;&#23545;&#20110;&#21407;&#22987;&#30340;&#28145;&#24230;&#25552;&#31034;&#26041;&#27861;&#25552;&#39640;&#20102;&#26368;&#22810; 11.53% &#30340; F1&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110; 8 &#31181;&#34920;&#29616;&#24378;&#21170;&#30340; few-shot NER &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot named entity recognition (NER) exploits limited annotated instances to identify named mentions. Effectively transferring the internal or external resources thus becomes the key to few-shot NER. While the existing prompt tuning methods have shown remarkable few-shot performances, they still fail to make full use of knowledge. In this work, we investigate the integration of rich knowledge to prompt tuning for stronger few-shot NER. We propose incorporating the deep prompt tuning framework with threefold knowledge (namely TKDP), including the internal 1) context knowledge and the external 2) label knowledge &amp; 3) sememe knowledge. TKDP encodes the three feature sources and incorporates them into the soft prompt embeddings, which are further injected into an existing pre-trained language model to facilitate predictions. On five benchmark datasets, our knowledge-enriched model boosts by at most 11.53% F1 over the raw deep prompt method, and significantly outperforms 8 strong-perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;-&#21407;&#22240;&#22235;&#20803;&#32452;&#25277;&#21462;&#20219;&#21153;(ECQED)&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#21644;&#21407;&#22240;&#26816;&#27979;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03969</link><description>&lt;p&gt;
ECQED&#65306;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;-&#21407;&#22240;&#22235;&#20803;&#32452;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
ECQED: Emotion-Cause Quadruple Extraction in Dialogs. (arXiv:2306.03969v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;-&#21407;&#22240;&#22235;&#20803;&#32452;&#25277;&#21462;&#20219;&#21153;(ECQED)&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#21644;&#21407;&#22240;&#26816;&#27979;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25277;&#21462;(ECPE)&#20219;&#21153;&#36951;&#25022;&#22320;&#24573;&#30053;&#20102;&#24773;&#24863;&#31867;&#22411;&#21644;&#21407;&#22240;&#31867;&#22411;&#30340;&#25552;&#21462;&#65292;&#32780;&#36825;&#20123;&#32454;&#31890;&#24230;&#30340;&#20803;&#20449;&#24687;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#38750;&#24120;&#26377;&#29992;&#65292;&#20363;&#22914;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;ECPE&#20165;&#38480;&#20110;&#21333;&#20010;&#25991;&#26412;&#29255;&#27573;&#30340;&#22330;&#26223;&#65292;&#32780;&#24573;&#30053;&#20102;&#24212;&#35813;&#20855;&#26377;&#26356;&#29616;&#23454;&#20215;&#20540;&#30340;&#23545;&#35805;&#32423;&#21035;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26356;&#24191;&#27867;&#30340;&#23450;&#20041;&#21644;&#22330;&#26223;&#25193;&#23637;&#20102;ECPE&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65306;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;-&#21407;&#22240;&#22235;&#20803;&#32452;&#25277;&#21462;(ECQED)&#65292;&#38656;&#35201;&#26816;&#27979;&#24773;&#24863;-&#21407;&#22240;&#35805;&#35821;&#23545;&#21644;&#24773;&#24863;&#21644;&#21407;&#22240;&#31867;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32467;&#26500;&#21644;&#35821;&#20041;&#24322;&#26500;&#22270;&#20197;&#21450;&#24179;&#34892;&#26631;&#35760;&#26041;&#26696;&#30340;ECQED&#27169;&#22411;&#65292;&#36825;&#25552;&#39640;&#20102;&#26377;&#25928;&#22320;&#32467;&#21512;&#23545;&#35805;&#19978;&#19979;&#25991;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20196;&#20154;&#22256;&#25200;&#30340;&#37325;&#21472;&#22235;&#20803;&#32452;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#24341;&#20837;&#32454;&#31890;&#24230;&#24773;&#24863;&#21644;&#21407;&#22240;&#26816;&#27979;&#24182;&#32771;&#34385;&#23545;&#35805;&#19978;&#19979;&#25991;&#23545;&#20110;&#23454;&#29616;ECQED&#20219;&#21153;&#30340;&#26356;&#22909;&#25928;&#26524;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existing emotion-cause pair extraction (ECPE) task, unfortunately, ignores extracting the emotion type and cause type, while these fine-grained meta-information can be practically useful in real-world applications, i.e., chat robots and empathic dialog generation. Also the current ECPE is limited to the scenario of single text piece, while neglecting the studies at dialog level that should have more realistic values. In this paper, we extend the ECPE task with a broader definition and scenario, presenting a new task, Emotion-Cause Quadruple Extraction in Dialogs (ECQED), which requires detecting emotion-cause utterance pairs and emotion and cause types. We present an ECQED model based on a structural and semantic heterogeneous graph as well as a parallel grid tagging scheme, which advances in effectively incorporating the dialog context structure, meanwhile solving the challenging overlapped quadruple issue. Via experiments we show that introducing the fine-grained emotion and caus
&lt;/p&gt;</description></item><item><title>KADS&#26159;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#23545;&#35805;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20174;&#20195;&#29702;&#25351;&#21335;&#20013;&#24471;&#20986;&#30340;&#26126;&#30830;&#35828;&#26126;&#26469;&#26500;&#24314;&#22797;&#26434;&#30340;&#12289;&#22810;&#27493;&#39588;&#30340;&#23545;&#35805;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03959</link><description>&lt;p&gt;
&#21033;&#29992;&#26174;&#24335;&#36807;&#31243;&#35828;&#26126;&#36827;&#34892;&#25968;&#25454;&#26377;&#25928;&#30340;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction. (arXiv:2306.03959v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03959
&lt;/p&gt;
&lt;p&gt;
KADS&#26159;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#23545;&#35805;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20174;&#20195;&#29702;&#25351;&#21335;&#20013;&#24471;&#20986;&#30340;&#26126;&#30830;&#35828;&#26126;&#26469;&#26500;&#24314;&#22797;&#26434;&#30340;&#12289;&#22810;&#27493;&#39588;&#30340;&#23545;&#35805;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#36890;&#24120;&#35201;&#27714;&#20195;&#29702;&#25353;&#29031;&#22797;&#26434;&#30340;&#12289;&#22810;&#27493;&#39588;&#30340;&#31243;&#24207;&#26469;&#28385;&#36275;&#29992;&#25143;&#35831;&#27714;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21463;&#38480;&#29615;&#22659;&#19979;&#33258;&#21160;&#21270;&#36825;&#20123;&#23545;&#35805;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#24191;&#27867;&#37096;&#32626;&#21463;&#21040;&#25152;&#38656;&#30340;&#22823;&#37327;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#26500;&#24314;&#23545;&#35805;&#31995;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20174;&#20195;&#29702;&#25351;&#21335;&#20013;&#24471;&#20986;&#30340;&#26126;&#30830;&#35828;&#26126;&#65292;&#20363;&#22914;&#20844;&#21496;&#25919;&#31574;&#25110;&#23458;&#25143;&#26381;&#21153;&#25163;&#20876;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#30693;&#35782;&#22686;&#24378;&#23545;&#35805;&#31995;&#32479; (KADS) &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#26816;&#32034;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#35813;&#27169;&#22359;&#20174;&#39044;&#23450;&#20041;&#30340;&#25919;&#31574;&#38598;&#21512;&#20013;&#26816;&#32034;&#27010;&#36848;&#30456;&#20851;&#31243;&#24207;&#30340;&#25991;&#26723;&#65292;&#24182;&#32473;&#20986;&#29992;&#25143;&#20195;&#29702;&#20132;&#20114;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20010;&#31995;&#32479;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21322;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;&#29992;&#23545;&#35805;&#25991;&#26723;&#21305;&#37197;&#21644;&#38754;&#21521;&#21160;&#20316;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#19982;&#37096;&#20998;&#21442;&#25968;&#20923;&#32467;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogues often require agents to enact complex, multi-step procedures in order to meet user requests. While large language models have found success automating these dialogues in constrained environments, their widespread deployment is limited by the substantial quantities of task-specific data required for training. The following paper presents a data-efficient solution to constructing dialogue systems, leveraging explicit instructions derived from agent guidelines, such as company policies or customer service manuals. Our proposed Knowledge-Augmented Dialogue System (KADS) combines a large language model with a knowledge retrieval module that pulls documents outlining relevant procedures from a predefined set of policies, given a user-agent interaction. To train this system, we introduce a semi-supervised pre-training scheme that employs dialogue-document matching and action-oriented masked language modeling with partial parameter freezing. We evaluate the effectivenes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;&#24191;&#21463;&#27426;&#36814;&#30340;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#20351;&#29992;&#33521;&#35821;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#21644;&#26032;&#20195;&#35789;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#32771;&#34385;&#21040;&#38750;&#20108;&#20803;&#24615;&#21035;&#36523;&#20221;&#30340;&#37325;&#35201;&#24615;&#65292;&#24341;&#20837;&#20102;MISGENDERED&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#20351;&#29992;&#39318;&#36873;&#20195;&#35789;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03950</link><description>&lt;p&gt;
MISGENDERED&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20195;&#35789;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
MISGENDERED: Limits of Large Language Models in Understanding Pronouns. (arXiv:2306.03950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;&#24191;&#21463;&#27426;&#36814;&#30340;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#20351;&#29992;&#33521;&#35821;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#21644;&#26032;&#20195;&#35789;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#32771;&#34385;&#21040;&#38750;&#20108;&#20803;&#24615;&#21035;&#36523;&#20221;&#30340;&#37325;&#35201;&#24615;&#65292;&#24341;&#20837;&#20102;MISGENDERED&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#20351;&#29992;&#39318;&#36873;&#20195;&#35789;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#35686;&#21578;:&#26412;&#25991;&#21253;&#21547;&#21487;&#33021;&#20196;&#20154;&#19981;&#24742;&#21644;&#28508;&#22312;&#24341;&#21457;&#24773;&#24863;&#38382;&#39064;&#30340;&#38169;&#35823;&#31216;&#21628;&#21644;&#25273;&#26432;&#30340;&#20363;&#23376;. &#24615;&#21035;&#20559;&#35265;&#22312;&#35821;&#35328;&#25216;&#26415;&#20013;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#30740;&#31350;&#22823;&#22810;&#38480;&#20110;&#20108;&#20803;&#24615;&#21035;&#33539;&#24335;&#12290;&#32771;&#34385;&#38750;&#20108;&#20803;&#24615;&#21035;&#36523;&#20221;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#25490;&#38500;&#20182;&#20204;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#20260;&#23475;&#36825;&#20010;&#24050;&#32463;&#34987;&#36793;&#32536;&#21270;&#30340;&#32676;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#24191;&#21463;&#27426;&#36814;&#30340;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#20351;&#29992;&#33521;&#35821;&#24615;&#21035;&#20013;&#31435;&#20195;&#35789;&#65288;&#20363;&#22914;&#21333;&#25968;they&#12289;them&#65289;&#21644;&#26032;&#20195;&#35789;&#65288;&#20363;&#22914;ze&#12289;xe&#12289;thon&#65289;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20195;&#35789;&#26159;&#30001;&#37027;&#20123;&#24615;&#21035;&#35748;&#21516;&#19981;&#20026;&#20108;&#20803;&#24615;&#21035;&#25152;&#20195;&#34920;&#30340;&#20010;&#20307;&#20351;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MISGENDERED&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#30830;&#20351;&#29992;&#39318;&#36873;&#20195;&#35789;&#30340;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;(i)&#38472;&#36848;&#20010;&#20307;&#20195;&#35789;&#30340;&#23454;&#20363;&#65292;&#21518;&#38754;&#36319;&#30528;&#19968;&#20010;&#32570;&#23569;&#20195;&#35789;&#30340;&#21477;&#23376;&#65292;&#20197;&#21450;(ii)&#35780;&#20272;&#25513;&#30422;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#30340;&#23454;&#39564;&#35774;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering.  Gender bias in language technologies has been widely studied, but research has mostly been restricted to a binary paradigm of gender. It is essential also to consider non-binary gender identities, as excluding them can cause further harm to an already marginalized group. In this paper, we comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns. We introduce MISGENDERED, a framework for evaluating large language models' ability to correctly use preferred pronouns, consisting of (i) instances declaring an individual's pronoun, followed by a sentence with a missing pronoun, and (ii) an experimental setup for evaluating masked and auto-regressive languag
&lt;/p&gt;</description></item><item><title>ChatDB&#39033;&#30446;&#23558;SQL&#25968;&#25454;&#24211;&#20316;&#20026;&#31526;&#21495;&#20869;&#23384;&#65292;&#22686;&#24378;LLMs&#30340;&#22797;&#26434;&#22810;&#36339;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03901</link><description>&lt;p&gt;
ChatDB: &#23558;&#25968;&#25454;&#24211;&#20316;&#20026;&#31526;&#21495;&#20869;&#23384;&#22686;&#24378;LLMs
&lt;/p&gt;
&lt;p&gt;
ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory. (arXiv:2306.03901v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03901
&lt;/p&gt;
&lt;p&gt;
ChatDB&#39033;&#30446;&#23558;SQL&#25968;&#25454;&#24211;&#20316;&#20026;&#31526;&#21495;&#20869;&#23384;&#65292;&#22686;&#24378;LLMs&#30340;&#22797;&#26434;&#22810;&#36339;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20869;&#23384;&#26159;&#35745;&#31639;&#36890;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;LLMs&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20869;&#23384;&#65292;&#24182;&#19988;&#35774;&#35745;&#21463;&#21040;&#29983;&#29289;&#22823;&#33041;&#30340;&#20005;&#37325;&#24433;&#21709;&#12290;&#30001;&#20110;&#20854;&#36817;&#20284;&#24615;&#36136;&#21644;&#38169;&#35823;&#32047;&#31215;&#20542;&#21521;&#65292;&#20256;&#32479;&#31070;&#32463;&#20869;&#23384;&#26426;&#21046;&#19981;&#33021;&#25903;&#25345;LLMs&#27169;&#25311;&#22797;&#26434;&#25512;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29616;&#20195;&#35745;&#31639;&#26426;&#26550;&#26500;&#20013;&#23547;&#27714;&#28789;&#24863;&#65292;&#20026;&#22797;&#26434;&#30340;&#22810;&#36339;&#25512;&#29702;&#22686;&#24378;LLMs&#31526;&#21495;&#20869;&#23384;&#12290;&#36825;&#26679;&#30340;&#31526;&#21495;&#20869;&#23384;&#26694;&#26550;&#34987;&#23454;&#20363;&#21270;&#20026;&#19968;&#20010;LLM&#21644;&#19968;&#32452;SQL&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;LLM&#29983;&#25104;SQL&#25351;&#20196;&#20197;&#25805;&#20316;SQL&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#20869;&#23384;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290; &#39033;&#30446;&#32593;&#31449;&#20301;&#20110;https://chatdatabase.github.io/&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#20998;&#26512;&#20102;Transformer&#27169;&#22411;&#22312;Winograd Schema Challenge&#19978;&#30340;&#34920;&#29616;&#65292;&#30830;&#23450;&#20102;&#36127;&#36131;&#25351;&#23548;&#20195;&#35789;&#35299;&#37322;&#30340;&#30456;&#23545;&#36739;&#23567;&#30340;&#27880;&#24847;&#21147;&#22836;&#30005;&#36335;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#22312;&#8220;&#35821;&#27861;&#8221;&#25511;&#21046;&#19979;&#30340;&#34892;&#20026;&#26041;&#24335;&#65292;&#36825;&#25581;&#31034;&#20102;&#26500;&#24314;&#38544;&#21547;&#24773;&#26223;&#27169;&#22411;&#30340;&#19981;&#21516;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2306.03882</link><description>&lt;p&gt;
&#22240;&#26524;&#24178;&#39044;&#25581;&#31034;&#20102;&#36890;&#35782;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#38544;&#21547;&#24773;&#26223;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Causal interventions expose implicit situation models for commonsense language understanding. (arXiv:2306.03882v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#20998;&#26512;&#20102;Transformer&#27169;&#22411;&#22312;Winograd Schema Challenge&#19978;&#30340;&#34920;&#29616;&#65292;&#30830;&#23450;&#20102;&#36127;&#36131;&#25351;&#23548;&#20195;&#35789;&#35299;&#37322;&#30340;&#30456;&#23545;&#36739;&#23567;&#30340;&#27880;&#24847;&#21147;&#22836;&#30005;&#36335;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#22312;&#8220;&#35821;&#27861;&#8221;&#25511;&#21046;&#19979;&#30340;&#34892;&#20026;&#26041;&#24335;&#65292;&#36825;&#25581;&#31034;&#20102;&#26500;&#24314;&#38544;&#21547;&#24773;&#26223;&#27169;&#22411;&#30340;&#19981;&#21516;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#35328;&#21152;&#24037;&#30340;&#29702;&#35770;&#19968;&#30452;&#22312;&#24378;&#35843;&#38544;&#21547;&#30340;&#8220;&#24773;&#26223;&#27169;&#22411;&#8221;&#65292;&#20197;&#20016;&#23500;&#29702;&#35299;&#30340;&#21516;&#26102;&#34917;&#20805;&#30456;&#20851;&#20294;&#26410;&#26126;&#30830;&#34920;&#36848;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#26412;&#25991;&#24212;&#29992;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#23545;&#26368;&#36817;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#20854;&#22312;Winograd Schema Challenge&#65288;WSC&#65289;&#19978;&#30340;&#34920;&#29616;&#12290;WSC&#25552;&#20379;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#65292;&#29992;&#20110;&#36716;&#25442;&#26377;&#27495;&#20041;&#30340;&#20195;&#35789;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#30456;&#23545;&#36739;&#23567;&#30340;&#27880;&#24847;&#21147;&#22836;&#30005;&#36335;&#65292;&#36127;&#36131;&#20174;&#19978;&#19979;&#25991;&#35789;&#20256;&#25773;&#20449;&#24687;&#65292;&#20197;&#25351;&#23548;&#20195;&#35789;&#26368;&#32456;&#25152;&#28041;&#21450;&#30340;&#20505;&#36873;&#21517;&#35789;&#30701;&#35821;&#30340;&#36873;&#25321;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22312;&#20005;&#26684;&#19981;&#38656;&#35201;&#24773;&#26223;&#27169;&#22411;&#30340;&#8220;&#35821;&#27861;&#8221;&#25511;&#21046;&#19979;&#30005;&#36335;&#30340;&#34892;&#20026;&#26041;&#24335;&#12290;&#36825;&#20123;&#20998;&#26512;&#34920;&#26126;&#65292;&#26500;&#24314;&#25351;&#23548;&#20195;&#35789;&#35299;&#20915;&#38382;&#39064;&#30340;&#38544;&#21547;&#24773;&#26223;&#27169;&#22411;&#30340;&#36884;&#24452;&#26159;&#19981;&#21516;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accounts of human language processing have long appealed to implicit ``situation models'' that enrich comprehension with relevant but unstated world knowledge. Here, we apply causal intervention techniques to recent transformer models to analyze performance on the Winograd Schema Challenge (WSC), where a single context cue shifts interpretation of an ambiguous pronoun. We identify a relatively small circuit of attention heads that are responsible for propagating information from the context word that guides which of the candidate noun phrases the pronoun ultimately attends to. We then compare how this circuit behaves in a closely matched ``syntactic'' control where the situation model is not strictly necessary. These analyses suggest distinct pathways through which implicit situation models are constructed to guide pronoun resolution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03872</link><description>&lt;p&gt;
&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#39564;&#35777;&#24605;&#32500;&#38142;&#30340;&#25512;&#29702;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deductive Verification of Chain-of-Thought Reasoning. (arXiv:2306.03872v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21463;&#30410;&#21290;&#27973;&#65292;&#29305;&#21035;&#26159;&#24212;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21487;&#20197;&#20351;&#27169;&#22411;&#20135;&#29983;&#26356;&#20840;&#38754;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24605;&#32500;&#38142;&#30340;&#24378;&#35843;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#21487;&#33021;&#20250;&#19981;&#24910;&#23548;&#33268;&#20135;&#29983;&#24187;&#35273;&#21644;&#32047;&#31215;&#38169;&#35823;&#65292;&#20174;&#32780;&#38480;&#21046;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#22914;&#20309;&#36827;&#34892;&#32454;&#33268;&#30340;&#28436;&#32462;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#26469;&#35299;&#20915;&#20219;&#21153;&#65292;&#25105;&#20204;&#26088;&#22312;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#39564;&#35777;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#20687;ChatGPT&#36825;&#26679;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#30452;&#25509;&#39564;&#35777;&#25972;&#20010;&#28436;&#32462;&#25512;&#29702;&#36807;&#31243;&#30340;&#26377;&#25928;&#24615;&#20063;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25512;&#29702;&#39564;&#35777;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#36880;&#27493;&#30340;&#23376;&#36807;&#31243;&#65292;&#27599;&#20010;&#36807;&#31243;&#21482;&#25509;&#25910;&#20854;&#24517;&#35201;&#30340;&#19978;&#19979;&#25991;&#21644;&#21069;&#25552;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03763</link><description>&lt;p&gt;
ChatGPT&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24050;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20174;&#26102;&#38388;&#25991;&#26412;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#36130;&#32463;&#26032;&#38395;&#65289;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#32467;&#26500;&#30340;&#28508;&#21147;&#20173;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#22270;&#25512;&#26029;&#33021;&#21147;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24039;&#22937;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#32467;&#26500;&#34701;&#21512;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36827;&#34892;&#21518;&#32493;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#27169;&#22411;&#30340;&#20135;&#20986;&#26500;&#24314;&#30340;&#32452;&#21512;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#24180;&#21270;&#32047;&#35745;&#22238;&#25253;&#12289;&#26356;&#20302;&#30340;&#27874;&#21160;&#24615;&#21644;&#26368;&#22823;&#22238;&#25764;&#12290;&#36825;&#31181;&#21331;&#36234;&#34920;&#29616;&#31361;&#26174;&#20102;ChatGPT&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#32593;&#32476;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#32469;&#21475;&#20196;&#29983;&#25104;&#30340;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;TwistList&#25968;&#25454;&#38598;&#21644;TwisterMisters&#22522;&#20934;&#31995;&#32479;&#65292;&#24182;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#21644;&#26174;&#24335;&#38899;&#38901;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03457</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#35821;&#35328;&#29983;&#25104;&#65306;&#20197;&#32469;&#21475;&#20196;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Phonetically-Grounded Language Generation: The Case of Tongue Twisters. (arXiv:2306.03457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#32469;&#21475;&#20196;&#29983;&#25104;&#30340;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;TwistList&#25968;&#25454;&#38598;&#21644;TwisterMisters&#22522;&#20934;&#31995;&#32479;&#65292;&#24182;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#21644;&#26174;&#24335;&#38899;&#38901;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#38899;&#38901;&#23398;&#35821;&#35328;&#29983;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#35789;&#27468;&#21644;&#35799;&#27468;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22260;&#32469;&#32469;&#21475;&#20196;&#29983;&#25104;&#23637;&#24320;&#30340;&#24037;&#20316;&#65292;&#32469;&#21475;&#20196;&#38656;&#35201;&#22312;&#20445;&#25345;&#35821;&#20041;&#27491;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26368;&#22823;&#21270;&#38899;&#39057;&#37325;&#21472;&#24182;&#20445;&#25345;&#35821;&#27861;&#27491;&#30830;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;TwistList&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;2.1K&#20154;&#24037;&#32534;&#20889;&#30340;&#32469;&#21475;&#20196;&#30340;&#22823;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#32469;&#21475;&#20196;&#29983;&#25104;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20934;&#31995;&#32479;(TwisterMisters)&#65292;&#21253;&#25324;&#38656;&#35201;&#21644;&#19981;&#38656;&#35201;&#22312;&#22495;&#20869;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#30340;&#32467;&#26524;&#26469;&#35777;&#26126;&#29616;&#26377;&#20027;&#27969;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#24615;&#33021;&#20248;&#33391;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#21644;&#26174;&#24335;&#38899;&#38901;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32469;&#21475;&#20196;&#29983;&#25104;&#30340;&#20219;&#21153;&#26159;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work in phonetically-grounded language generation has mainly focused on domains such as lyrics and poetry. In this paper, we present work on the generation of tongue twisters - a form of language that is required to be phonetically conditioned to maximise sound overlap, whilst maintaining semantic consistency with an input topic, and still being grammatically correct. We present \textbf{TwistList}, a large annotated dataset of tongue twisters, consisting of 2.1K+ human-authored examples. We additionally present several benchmark systems (referred to as TwisterMisters) for the proposed task of tongue twister generation, including models that both do and do not require training on in-domain data. We present the results of automatic and human evaluation to demonstrate the performance of existing mainstream pre-trained models in this task with limited (or no) task specific training and data, and no explicit phonetic knowledge. We find that the task of tongue twister generation is 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;bgGLUE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#20445;&#21152;&#21033;&#20122;&#35821;&#19978;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#38024;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#20107;&#23454;&#26816;&#26597;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#38382;&#31572;&#31561;&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;NLU&#20219;&#21153;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#36824;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.02349</link><description>&lt;p&gt;
bgGLUE&#65306;&#20445;&#21152;&#21033;&#20122;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark. (arXiv:2306.02349v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02349
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;bgGLUE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#20445;&#21152;&#21033;&#20122;&#35821;&#19978;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#38024;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#20107;&#23454;&#26816;&#26597;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#38382;&#31572;&#31561;&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;NLU&#20219;&#21153;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#36824;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;bgGLUE&#65288;&#20445;&#21152;&#21033;&#20122;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#20445;&#21152;&#21033;&#20122;&#35821;&#19978;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#19978;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;&#38024;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#20107;&#23454;&#26816;&#26597;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#38382;&#31572;&#31561;&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65288;&#24207;&#21015;&#26631;&#35760;&#12289;&#25991;&#26723;&#32423;&#20998;&#31867;&#21644;&#22238;&#24402;&#65289;&#30340;NLU&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#35780;&#20272;&#20445;&#21152;&#21033;&#20122;&#35821;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#36328;&#36275;&#20102;&#20061;&#20010;&#20219;&#21153;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#32467;&#26524;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#36824;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;bgGLUE&#19982;&#24494;&#35843;&#21644;&#35780;&#20272;&#20195;&#30721;&#19968;&#36215;&#20844;&#24320;&#25552;&#20379;&#65292;&#20197;&#21450;&#22312;https://bgglue.github.io/&#19978;&#25552;&#20379;&#20844;&#20849;&#25490;&#34892;&#27036;&#65292;&#24076;&#26395;&#23427;&#33021;&#20419;&#36827;&#26356;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present bgGLUE(Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic evaluation of pre-trained language models for Bulgarian, comparing and contrasting results across the nine tasks in the benchmark. The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning. We make bgGLUE publicly available together with the fine-tuning and the evaluation code, as well as a public leaderboard at https://bgglue.github.io/, and we hope that it will enable further advancements in developi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;APA&#65292;&#20854;&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65288;PPO&#65289;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#30340;&#23849;&#28291;&#19982;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02231</link><description>&lt;p&gt;
&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#30340;Fine-Tuning&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;APA&#65292;&#20854;&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65288;PPO&#65289;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#30340;&#23849;&#28291;&#19982;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#21487;&#38752;&#26041;&#27861;&#12290;&#22312;&#20247;&#22810;RLHF&#25216;&#26415;&#20013;&#65292;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;PPO&#24456;&#27969;&#34892;&#65292;&#20294;&#23427;&#21487;&#33021;&#20250;&#36973;&#21463;&#27169;&#24335;&#23849;&#28291;&#12289;&#19981;&#31283;&#23450;&#21644;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;--&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#65288;APA&#65289;&#65292;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#26102;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#22987;&#32456;&#27604;PPO&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;PPO&#30456;&#27604;&#65292;APA&#21487;&#20197;&#26356;&#31283;&#23450;&#22320;&#25511;&#21046;&#27169;&#22411;&#19982;&#21021;&#22987;&#31574;&#30053;&#30340;&#20559;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#20250;&#23849;&#28291;&#20026;&#30830;&#23450;&#24615;&#36755;&#20986;&#12290;&#38500;&#20102;&#32463;&#39564;&#32467;&#26524;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;APA&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#30456;&#23545;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#20855;&#26377;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#21512;&#25104;&#31639;&#26415;&#20219;&#21153;&#65288;MsAT&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;LM&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01707</link><description>&lt;p&gt;
&#20174;&#31639;&#26415;&#20219;&#21153;&#20013;&#23398;&#20064;&#22810;&#27493;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-step Reasoning from Arithmetic Task. (arXiv:2306.01707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#30456;&#23545;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#20855;&#26377;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#21512;&#25104;&#31639;&#26415;&#20219;&#21153;&#65288;MsAT&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;LM&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#34987;&#35748;&#20026;&#26159;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24517;&#35201;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;LM&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25104;&#21151;&#24402;&#22240;&#20110;&#23427;&#20204;&#30340;&#36830;&#32493;&#24605;&#32771;&#65288;CoT&#65289;&#25512;&#29702;&#33021;&#21147;&#65292;&#21363;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#25104;&#36880;&#27493;&#25512;&#29702;&#38142;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#33021;&#21147;&#20284;&#20046;&#21482;&#20986;&#29616;&#22312;&#20855;&#26377;&#20016;&#23500;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#22914;&#20309;&#23558;&#30456;&#23545;&#36739;&#23567;&#30340;LM&#19982;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;MsAT&#65288;&#22810;&#27493;&#31639;&#26415;&#20219;&#21153;&#65289;&#36827;&#34892;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#26469;&#27880;&#20837;&#36825;&#31181;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#22686;&#24378;LM&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning is regarded as a necessary ability for Language Models (LMs). Recent works demonstrate large LMs' impressive performance in solving math problems. The success is attributed to their Chain-of-Thought (CoT) reasoning abilities, i.e., the ability to decompose complex questions into step-by-step reasoning chains, but such ability seems only to emerge from models with abundant parameters. This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning. We propose to inject such abilities by continually pre-training LMs on a synthetic dataset MsAT, which stands for Multi-step Arithmetic Task. Our experiments on four math word problem datasets show the effectiveness of the proposed method in enhancing LMs' math reasoning abilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#20351;&#29992;&#20102;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;Spider&#19978;&#23454;&#29616;&#20102;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#24182;&#26174;&#30528;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00739</link><description>&lt;p&gt;
SQL-PaLM&#65306;&#38024;&#23545;Text-to-SQL&#30340;&#25913;&#36827;&#22823;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. (arXiv:2306.00739v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#20351;&#29992;&#20102;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;Spider&#19978;&#23454;&#29616;&#20102;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#24182;&#26174;&#30528;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19968;&#20010;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#21151;&#33021;&#26159;&#29983;&#25104;&#20195;&#30721;&#65292;&#21253;&#25324;&#29992;&#20110;&#25968;&#25454;&#24211;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#12290;&#23545;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#36716;&#25442;&#20026;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#21363;Text-to-SQL&#65292;LLMs&#30340;&#36866;&#24212;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20351;&#29992;&#30340;&#36866;&#24212;&#24615;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;Text-to-SQL&#27169;&#22411;SQL-PaLM&#65292;&#21033;&#29992;&#20102;PaLM-2&#65292;&#25512;&#21160;&#20102;&#20004;&#31181;&#35774;&#32622;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;Few-shot SQL-PaLM&#22522;&#20110;&#38754;&#21521;Text-to-SQL&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#33258;&#19968;&#33268;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#22312;Spider&#19978;&#23454;&#29616;77.3%&#30340;&#27979;&#35797;&#22871;&#20214;&#20934;&#30830;&#24230;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#26174;&#30528;&#36739;&#22823;&#30340;&#24494;&#35843;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;SQL-PALM&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;1%&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;SQL-PaLM&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#20854;&#23545;&#20854;&#20182;&#25361;&#25112;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One impressive emergent capability of large language models (LLMs) is generation of code, including Structured Query Language (SQL) for databases. For the task of converting natural language text to SQL queries, Text-to-SQL, adaptation of LLMs is of paramount importance, both in in-context learning and fine-tuning settings, depending on the amount of adaptation data used. In this paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is based on an execution-based self-consistency prompting approach designed for Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our best knowledge is the first to outperform previous state-of-the-art with fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying SQL-PaLM to real-world scenarios we further evaluate its robustness on other chall
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#21508;&#21521;&#24322;&#24615;&#21644;&#24322;&#24120;&#20540;&#32500;&#24230;&#65292;&#38024;&#23545;&#36328;&#35821;&#35328;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;&#21457;&#29616;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#30446;&#26631;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#34920;&#31034;&#30340;&#21508;&#21521;&#21516;&#24615;&#65292;&#20294;&#19981;&#20250;&#32479;&#19968;&#20943;&#23569;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.00458</link><description>&lt;p&gt;
&#25506;&#31350;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21508;&#21521;&#24322;&#24615;&#21644;&#24322;&#24120;&#20540;&#65292;&#38024;&#23545;&#36328;&#35821;&#35328;&#35821;&#20041;&#21477;&#23376;&#30456;&#20284;&#24230;&#36827;&#34892;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity. (arXiv:2306.00458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00458
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#21508;&#21521;&#24322;&#24615;&#21644;&#24322;&#24120;&#20540;&#32500;&#24230;&#65292;&#38024;&#23545;&#36328;&#35821;&#35328;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;&#21457;&#29616;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#30446;&#26631;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#34920;&#31034;&#30340;&#21508;&#21521;&#21516;&#24615;&#65292;&#20294;&#19981;&#20250;&#32479;&#19968;&#20943;&#23569;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#34920;&#31034;&#27604;&#38745;&#24577;&#31867;&#22411;&#23884;&#20837;&#26356;&#20855;&#21508;&#21521;&#24322;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#26174;&#31034;&#24322;&#24120;&#20540;&#32500;&#24230;&#12290;&#34429;&#28982;&#23545;&#20110;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#37117;&#26159;&#22914;&#27492;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;&#24773;&#22659;&#19979;&#30340;&#30740;&#31350;&#36824;&#36828;&#19981;&#22815;&#12290;&#20026;&#20160;&#20040;&#20250;&#20986;&#29616;&#36825;&#20123;&#24322;&#24120;&#20540;&#24182;&#19988;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#34920;&#31034;&#20173;&#26159;&#30740;&#31350;&#30340;&#27963;&#36291;&#39046;&#22495;&#12290;&#25105;&#20204;&#35843;&#26597;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#24322;&#24120;&#20540;&#32500;&#24230;&#21450;&#20854;&#19982;&#21508;&#21521;&#24322;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36328;&#35821;&#35328;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#26159;&#35780;&#20272;&#22810;&#35821;&#35328;&#34920;&#31034;&#33258;&#28982;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#21477;&#23376;&#34920;&#31034;&#12290;&#22312;&#24179;&#34892;&#36164;&#28304;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#21477;&#23376;&#36716;&#25442;&#22120;&#22312;&#27492;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#25105;&#20204;&#23637;&#31034;&#23427;&#20204;&#30340;&#34920;&#31034;&#26356;&#21508;&#21521;&#21516;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24635;&#20307;&#25913;&#21892;&#22810;&#35821;&#35328;&#34920;&#31034;&#12290;&#25105;&#20204;&#35843;&#26597;&#36890;&#36807;&#22312;&#36328;&#35821;&#35328;&#30446;&#26631;&#19978;&#35757;&#32451;&#21487;&#20943;&#23569;&#22810;&#23569;&#21508;&#21521;&#24322;&#24615;&#21644;&#24322;&#24120;&#20540;&#65292;&#20197;&#21450;&#36825;&#26679;&#20570;&#22914;&#20309;&#24433;&#21709;&#36328;&#35821;&#35328;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#30446;&#26631;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#34920;&#31034;&#30340;&#21508;&#21521;&#21516;&#24615;&#65292;&#20294;&#19981;&#20250;&#32479;&#19968;&#20943;&#23569;&#24322;&#24120;&#20540;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#20854;&#20013;&#19968;&#20123;&#35821;&#35328;&#26174;&#31034;&#20986;&#27604;&#20854;&#20182;&#35821;&#35328;&#26356;&#22823;&#30340;&#24322;&#24120;&#20540;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the multilingual context. Why these outliers occur and how they affect the representations is still an active area of research. We investigate outlier dimensions and their relationship to anisotropy in multiple pre-trained multilingual language models. We focus on cross-lingual semantic similarity tasks, as these are natural tasks for evaluating multilingual representations. Specifically, we examine sentence representations. Sentence transformers which are fine-tuned on parallel resources (that are not always available) perform better on this task, and we show that their representations are more isotropic. However, we aim to improve multilingual representations in general. We investigate how much of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#28508;&#22312;&#22996;&#23113;&#35821;&#28040;&#27495;&#20219;&#21153;&#65292;&#39318;&#20808;&#27880;&#37322;&#20102;&#27169;&#31946;&#24615;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#22996;&#23113;&#35821;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#65292;&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#21021;&#27493;&#30340;&#32467;&#26524;&#65292;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2306.00217</link><description>&lt;p&gt;
FEED PETs&#65306;&#20851;&#20110;&#28508;&#22312;&#22996;&#23113;&#35828;&#35821;&#30340;&#28040;&#27495;&#26356;&#22810;&#23454;&#39564;&#19982;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
FEED PETs: Further Experimentation and Expansion on the Disambiguation of Potentially Euphemistic Terms. (arXiv:2306.00217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#28508;&#22312;&#22996;&#23113;&#35821;&#28040;&#27495;&#20219;&#21153;&#65292;&#39318;&#20808;&#27880;&#37322;&#20102;&#27169;&#31946;&#24615;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#22996;&#23113;&#35821;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#65292;&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#21021;&#27493;&#30340;&#32467;&#26524;&#65292;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;Transformer&#22312;&#33521;&#35821;&#22996;&#23113;&#35821;&#28040;&#27495;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21363;&#22312;&#29305;&#23450;&#35821;&#22659;&#20013;&#23558;&#28508;&#22312;&#30340;&#22996;&#23113;&#35821;&#65288;PET&#65289;&#20998;&#31867;&#20026;&#22996;&#23113;&#35821;&#25110;&#38750;&#22996;&#23113;&#35821;&#12290;&#26412;&#30740;&#31350;&#20174;&#20004;&#20010;&#26041;&#38754;&#25193;&#23637;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#27169;&#31946;&#24615;&#27880;&#37322;PET&#65292;&#36825;&#26159;&#19982;&#22996;&#23113;&#35821;&#30456;&#20851;&#30340;&#35821;&#35328;&#23646;&#24615;&#65292;&#24182;&#21457;&#29616;Transformer&#36890;&#24120;&#26356;&#25797;&#38271;&#20998;&#31867;&#27169;&#31946;&#30340;PET&#65292;&#36825;&#34920;&#26126;&#24433;&#21709;&#24615;&#33021;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#35821;&#35328;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#26032;&#39062;&#22996;&#23113;&#35821;&#35821;&#26009;&#24211;&#65306;&#32422;&#40065;&#24052;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#20013;&#25991;&#26222;&#36890;&#35805;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;mBERT&#21644;XLM-RoBERTa&#22312;&#27599;&#31181;&#35821;&#35328;&#20013;&#25191;&#34892;&#22996;&#23113;&#35821;&#28040;&#27495;&#23454;&#39564;&#65292;&#24314;&#31435;&#21021;&#27493;&#32467;&#26524;&#65292;&#20026;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#20102;&#22522;&#30784;&#21644;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have been shown to work well for the task of English euphemism disambiguation, in which a potentially euphemistic term (PET) is classified as euphemistic or non-euphemistic in a particular context. In this study, we expand on the task in two ways. First, we annotate PETs for vagueness, a linguistic property associated with euphemisms, and find that transformers are generally better at classifying vague PETs, suggesting linguistic differences in the data that impact performance. Second, we present novel euphemism corpora in three different languages: Yoruba, Spanish, and Mandarin Chinese. We perform euphemism disambiguation experiments in each language using multilingual transformer models mBERT and XLM-RoBERTa, establishing preliminary results from which to launch future work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#24494;&#35843;&#23569;&#37327;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18466</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18466
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#24494;&#35843;&#23569;&#37327;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35768;&#22810;&#24037;&#20316;&#37117;&#26088;&#22312;&#22312;&#27979;&#35797;&#26102;&#20174;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#20854;&#26631;&#20934;&#35757;&#32451;&#35774;&#32622;&#23545;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#38656;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#8220;Pile&#8221;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#23884;&#20837;&#30340;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26368;&#36817;&#37051;&#32034;&#24341;&#12290;&#32473;&#23450;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#26816;&#32034;&#26597;&#35810;&#30340;&#37051;&#23621;&#65292;&#24182;&#22312;&#23545;&#24212;&#20110;&#36825;&#20123;&#37051;&#23621;&#30340;&#25991;&#26412;&#25968;&#25454;&#19978;&#24494;&#35843;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26816;&#32034;&#21644;&#35757;&#32451;&#20165;20&#20010;&#37051;&#23621;&#65292;&#27599;&#20010;&#37051;&#23621;&#20165;&#36827;&#34892;&#19968;&#27425;&#26799;&#24230;&#36845;&#20195;&#65292;&#23601;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#8220;Pile&#8221;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20108;&#21313;&#20010;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26174;&#33879;&#32553;&#23567;&#20102;&#23567;&#22411;GPT2&#27169;&#22411;&#21644;GPTNeo&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21518;&#32773;&#26159;&#19987;&#38376;&#23545;&#8220;Pile&#8221;&#36827;&#34892;&#25910;&#25947;&#35757;&#32451;&#30340;&#65292;&#20307;&#31215;&#21364;&#26159;&#21069;&#32773;&#30340;&#21313;&#20493;&#20197;&#19978;&#12290;&#28982;&#32780;&#65292;&#20854;&#26041;&#27861;&#30340;&#25104;&#21151;&#36824;&#21462;&#20915;&#20110;&#20805;&#20998;&#30340;&#32034;&#24341;&#36136;&#37327;&#21644;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent efforts aim to augment language models with relevant information retrieved from a database at test time. We avoid the need for prompt engineering by directly fine-tuning the model on data retrieved at test time using its standard training setup. For this purpose, we build a large-scale distributed nearest neighbor index based on text embeddings of the Pile dataset. Given a query to a language model, our system retrieves the neighbors of the query and fine-tunes the model on the text data corresponding to those neighbors. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than twenty language modeling tasks in the Pile benchmark. For example, test-time training significantly narrows the performance gap between a small GPT2 model and a GPTNeo model, more than ten times larger, that was specifically trained to convergence on the Pile. Sufficient index quality and size, however, are
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#25968;&#25454;&#30340; AI &#29983;&#25104;&#30340;&#22823;&#23398;&#29983;&#20316;&#19994;&#26816;&#27979;&#26041;&#27861; HowkGPT&#65292;&#36890;&#36807;&#35745;&#31639;&#22256;&#24785;&#24230;&#24471;&#20998;&#26469;&#21306;&#20998;&#23398;&#29983;&#25552;&#20132;&#21644; ChatGPT &#29983;&#25104;&#30340;&#20316;&#19994;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#26512;&#30340;&#31934;&#24230;&#65292;&#20197;&#24110;&#21161;&#32500;&#25252;&#23398;&#26415;&#35802;&#20449;&#21644;&#38450;&#27490;&#20316;&#24330;&#12290;</title><link>http://arxiv.org/abs/2305.18226</link><description>&lt;p&gt;
HowkGPT: &#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#22256;&#24785;&#24230;&#20998;&#26512;&#30340; ChatGPT &#29983;&#25104;&#30340;&#22823;&#23398;&#29983;&#20316;&#19994;&#26816;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis. (arXiv:2305.18226v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#25968;&#25454;&#30340; AI &#29983;&#25104;&#30340;&#22823;&#23398;&#29983;&#20316;&#19994;&#26816;&#27979;&#26041;&#27861; HowkGPT&#65292;&#36890;&#36807;&#35745;&#31639;&#22256;&#24785;&#24230;&#24471;&#20998;&#26469;&#21306;&#20998;&#23398;&#29983;&#25552;&#20132;&#21644; ChatGPT &#29983;&#25104;&#30340;&#20316;&#19994;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#26512;&#30340;&#31934;&#24230;&#65292;&#20197;&#24110;&#21161;&#32500;&#25252;&#23398;&#26415;&#35802;&#20449;&#21644;&#38450;&#27490;&#20316;&#24330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20154;&#20204;&#25285;&#24515;&#23427;&#20204;&#21487;&#33021;&#20250;&#21361;&#21450;&#23398;&#26415;&#35802;&#20449;&#12290;&#25945;&#32946;&#37096;&#38376;&#30446;&#21069;&#27491;&#22312;&#21162;&#21147;&#21306;&#20998;&#23398;&#29983;&#25552;&#20132;&#30340;&#23478;&#24237;&#20316;&#19994;&#21644;AI&#29983;&#25104;&#30340;&#20316;&#19994;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837; HowkGPT &#26631;&#35782;&#30001; AI &#29983;&#25104;&#30340;&#20316;&#19994;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;HowkGPT &#22522;&#20110;&#19968;&#32452;&#23398;&#26415;&#20316;&#19994;&#21644;&#30456;&#24212;&#20803;&#25968;&#25454;&#26500;&#24314;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340; LLM &#35745;&#31639;&#23398;&#29983;&#25552;&#20132;&#21644; ChatGPT &#29983;&#25104;&#30340;&#22238;&#31572;&#30340;&#22256;&#24785;&#24230;&#24471;&#20998;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#24471;&#20998;&#26377;&#21161;&#20110;&#24314;&#31435;&#21306;&#20998;&#25552;&#20132;&#20316;&#19994;&#26469;&#28304;&#30340;&#38408;&#20540;&#12290;&#37492;&#20110;&#23398;&#26415;&#24037;&#20316;&#30340;&#29305;&#27530;&#24615;&#21644;&#19978;&#19979;&#25991;&#24615;&#36136;&#65292;HowkGPT &#36824;&#36890;&#36807;&#23450;&#20041;&#20174;&#20803;&#25968;&#25454;&#20013;&#23548;&#20986;&#30340;&#31867;&#21035;&#29305;&#23450;&#30340;&#38408;&#20540;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#26512;&#30340;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312; LLM &#25991;&#26412;&#29983;&#25104;&#26102;&#26399;&#32500;&#25252;&#23398;&#26415;&#35802;&#20449;&#21644;&#38450;&#27490;&#20316;&#24330;&#30340;&#26377;&#25928;&#31574;&#30053;&#30340;&#20851;&#38190;&#24615;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of Large Language Models (LLMs) in text generation tasks proliferates, concerns arise over their potential to compromise academic integrity. The education sector currently tussles with distinguishing student-authored homework assignments from AI-generated ones. This paper addresses the challenge by introducing HowkGPT, designed to identify homework assignments generated by AI. HowkGPT is built upon a dataset of academic assignments and accompanying metadata [17] and employs a pretrained LLM to compute perplexity scores for student-authored and ChatGPT-generated responses. These scores then assist in establishing a threshold for discerning the origin of a submitted assignment. Given the specificity and contextual nature of academic work, HowkGPT further refines its analysis by defining category-specific thresholds derived from the metadata, enhancing the precision of the detection. This study emphasizes the critical need for effective strategies to uphold academic integrity a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NER&#35821;&#26009;&#24211;&#65292;&#20174;&#26410;&#27880;&#37322;&#30340;&#21382;&#21490;&#25991;&#26412;&#20013;&#24341;&#23548;&#27880;&#37322;&#31649;&#36947;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;NER&#27169;&#22411;&#26469;&#35782;&#21035;&#21382;&#21490;&#25991;&#29486;&#20013;&#30340;&#20154;&#21517;&#21644;&#22320;&#21517;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.16718</link><description>&lt;p&gt;
&#21382;&#21490;&#27431;&#27954;&#30340;&#20154;&#19982;&#22320;&#26041;&#65306;&#24341;&#23548;&#27880;&#37322;&#31649;&#36947;&#21644;&#19968;&#20010;&#26032;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#22312;&#26202;&#26399;&#20013;&#19990;&#32426;&#25991;&#26412;&#20013;
&lt;/p&gt;
&lt;p&gt;
People and Places of Historical Europe: Bootstrapping Annotation Pipeline and a New Corpus of Named Entities in Late Medieval Texts. (arXiv:2305.16718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NER&#35821;&#26009;&#24211;&#65292;&#20174;&#26410;&#27880;&#37322;&#30340;&#21382;&#21490;&#25991;&#26412;&#20013;&#24341;&#23548;&#27880;&#37322;&#31649;&#36947;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;NER&#27169;&#22411;&#26469;&#35782;&#21035;&#21382;&#21490;&#25991;&#29486;&#20013;&#30340;&#20154;&#21517;&#21644;&#22320;&#21517;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#39044;&#20808;&#35757;&#32451;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#23545;&#29616;&#20195;&#35821;&#26009;&#24211;&#38750;&#24120;&#20934;&#30830;&#65292;&#20294;&#30001;&#20110;OCR&#38169;&#35823;&#21644;&#35821;&#35328;&#24046;&#24322;&#65292;&#23427;&#20204;&#22312;&#21382;&#21490;&#25991;&#26412;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;NER&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;36&#19975;&#20010;&#21477;&#23376;&#65292;&#20027;&#35201;&#26159;&#20351;&#29992;&#25463;&#20811;&#35821;&#12289;&#25289;&#19969;&#35821;&#21644;&#24503;&#35821;&#32534;&#20889;&#30340;&#26202;&#26399;&#20013;&#19990;&#32426;&#25991;&#20070;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#20174;&#24050;&#30693;&#30340;&#21382;&#21490;&#20154;&#29289;&#21644;&#22320;&#28857;&#21015;&#34920;&#20197;&#21450;&#26410;&#27880;&#37322;&#30340;&#21382;&#21490;&#25991;&#26412;&#20013;&#24320;&#22987;&#65292;&#24182;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#33258;&#21160;&#24341;&#23548;NER&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;NER&#27169;&#22411;&#65292;&#23427;&#22312;&#25163;&#21160;&#27880;&#37322;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;72.81-93.98&#65285;&#30340;&#23454;&#20307;&#32423;&#31934;&#24230;&#21644;58.14-81.77&#65285;&#30340;&#21484;&#22238;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#24110;&#21161;&#24212;&#23545;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#12290;&#20026;&#20102;&#20351;&#20854;&#20182;&#20154;&#33021;&#22815;&#37325;&#29616;&#21644;&#24314;&#31435;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#65292;&#27169;&#22411;&#21644;&#23454;&#39564;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although pre-trained named entity recognition (NER) models are highly accurate on modern corpora, they underperform on historical texts due to differences in language OCR errors. In this work, we develop a new NER corpus of 3.6M sentences from late medieval charters written mainly in Czech, Latin, and German.  We show that we can start with a list of known historical figures and locations and an unannotated corpus of historical texts, and use information retrieval techniques to automatically bootstrap a NER-annotated corpus. Using our corpus, we train a NER model that achieves entity-level Precision of 72.81-93.98% with 58.14-81.77% Recall on a manually-annotated test dataset. Furthermore, we show that using a weighted loss function helps to combat class imbalance in token classification tasks. To make it easy for others to reproduce and build upon our work, we publicly release our corpus, models, and experimental code.
&lt;/p&gt;</description></item><item><title>DataFinder&#33021;&#22815;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25512;&#33616;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#31185;&#23398;&#23478;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#23547;&#25214;&#21512;&#36866;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.16636</link><description>&lt;p&gt;
DataFinder: &#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25512;&#33616;&#31185;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions. (arXiv:2305.16636v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16636
&lt;/p&gt;
&lt;p&gt;
DataFinder&#33021;&#22815;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25512;&#33616;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#31185;&#23398;&#23478;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#23547;&#25214;&#21512;&#36866;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#21644;&#39564;&#35777;&#30740;&#31350;&#24819;&#27861;&#12290;&#37492;&#20110;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#30340;&#22686;&#38271;&#65292;&#25214;&#21040;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20219;&#20309;&#30740;&#31350;&#38382;&#39064;&#23545;&#33021;&#22815;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#30340;&#35201;&#27714;&#37117;&#26377;&#26126;&#30830;&#21644;&#38544;&#21547;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#27169;&#24577;&#21644;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#19968;&#20010;&#30740;&#31350;&#24819;&#27861;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#25512;&#33616;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#25214;&#21040;&#31526;&#21512;&#20182;&#20204;&#38656;&#27714;&#30340;&#30456;&#20851;&#25968;&#25454;&#38598;&#12290;&#25968;&#25454;&#38598;&#25512;&#33616;&#23384;&#22312;&#29420;&#29305;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#65292;&#25968;&#25454;&#38598;&#24456;&#38590;&#30452;&#25509;&#32034;&#24341;&#36827;&#34892;&#25628;&#32034;&#65292;&#20063;&#27809;&#26377;&#29616;&#25104;&#30340;&#35821;&#26009;&#24211;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;DataFinder&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#33258;&#21160;&#26500;&#24314;&#30340;&#36739;&#22823;&#35757;&#32451;&#38598;&#65288;17500&#20010;&#26597;&#35810;&#65289;&#21644;&#19968;&#20010;&#36739;&#23567;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#35780;&#20272;&#38598;&#65288;392&#20010;&#26597;&#35810;&#65289;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21508;&#31181;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We introduce a new task of recommending relevant datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To operationalize this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.16259</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#29616;&#29366;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#37319;&#29992;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#38271;&#25991;&#26412;&#20998;&#26512;&#30340;&#38656;&#27714;&#19982;&#30701;&#25991;&#26412;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#32780;&#32593;&#32476;&#19978;&#20256;&#36755;&#30340;&#25991;&#26723;&#22823;&#23567;&#19981;&#26029;&#22686;&#21152;&#65292;&#20351;&#38271;&#25991;&#26412;&#30340;&#33258;&#21160;&#29702;&#35299;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;a&#65289;&#27010;&#36848;&#30456;&#20851;&#30340;&#31070;&#32463;&#26500;&#24314;&#27169;&#22359;&#65292;&#20316;&#20026;&#30701;&#25945;&#31243;&#65307;b&#65289;&#24635;&#32467;&#38271;&#25991;&#26412;NLP&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#26680;&#24515;&#20219;&#21153;&#65306;&#25991;&#26723;&#20998;&#31867;&#21644;&#25991;&#26723;&#25688;&#35201;&#12290;&#24773;&#24863;&#20998;&#26512;&#20063;&#28085;&#30422;&#22312;&#20869;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#34987;&#35270;&#20026;&#25991;&#26723;&#20998;&#31867;&#30340;&#29305;&#20363;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#38271;&#25991;&#26412;NLP&#30456;&#20851;&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#20844;&#24320;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded on-line renders automated understanding of long texts a critical area of research. This article has two goals: a) it overviews the relevant neural building blocks, thus serving as a short tutorial, and b) it surveys the state-of-the-art in long document NLP, mainly focusing on two central tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Additionally, this article discusses the main challenges, issues and current solutions related to long document NLP. Finally, the relevant, publicly available, annotated datasets are presented, in order to facilitate further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#21644;&#20998;&#31867;&#20102;Text-to-SQL&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#24357;&#34917;&#20559;&#35265;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16253</link><description>&lt;p&gt;
&#25581;&#31034;&#21644;&#20998;&#31867;Text-to-SQL&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Uncovering and Categorizing Social Biases in Text-to-SQL. (arXiv:2305.16253v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#21644;&#20998;&#31867;&#20102;Text-to-SQL&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#24357;&#34917;&#20559;&#35265;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Text-to-SQL&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#20998;&#31867;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#25513;&#30422;&#20102;&#27169;&#22411;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#24357;&#34917;&#20559;&#35265;&#25968;&#25454;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#20559;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content Warning: This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.} Large pre-trained language models are acknowledged to carry social biases towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm. Text-to-SQL is an important task, models of which are mainly adopted by administrative industries, where unfair decisions may lead to catastrophic consequences. However, existing Text-to-SQL models are trained on clean, neutral datasets, such as Spider and WikiSQL. This, to some extent, cover up social bias in models under ideal conditions, which nevertheless may emerge in real application scenarios. In this work, we aim to uncover and categorize social biases in Text-to-SQL models. We summarize the categories of social biases that may occur in structured data for Text-to-SQL models. We build test benchmarks and revea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#20869;&#37096;&#35780;&#20272;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24402;&#32435;&#33021;&#21147;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#20110;&#27169;&#22411;&#26159;&#21542;&#23398;&#20064;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.14070</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24402;&#32435;&#33021;&#21147;&#65306;&#19968;&#20221;&#38754;&#21521;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Assessing Linguistic Generalisation in Language Models: A Dataset for Brazilian Portuguese. (arXiv:2305.14070v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#20869;&#37096;&#35780;&#20272;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24402;&#32435;&#33021;&#21147;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#20110;&#27169;&#22411;&#26159;&#21542;&#23398;&#20064;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#37327;&#30340;&#30740;&#31350;&#31934;&#21147;&#34987;&#25237;&#20837;&#21040;&#21019;&#24314;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#30446;&#21069;&#26368;&#20027;&#27969;&#30340;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;BERT&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24448;&#24448;&#34987;&#35270;&#20026;&#40657;&#21283;&#23376;&#12290;&#36825;&#19981;&#20165;&#24433;&#21709;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#20063;&#24433;&#21709;&#20102;&#19981;&#21516;&#26550;&#26500;&#29978;&#33267;&#20351;&#29992;&#19981;&#21516;&#35821;&#26009;&#24211;&#25110;&#36229;&#21442;&#25968;&#35757;&#32451;&#30340;&#21516;&#19968;&#27169;&#22411;&#30340;&#21487;&#27604;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#20869;&#22312;&#35780;&#20272;&#20219;&#21153;&#65292;&#26816;&#26597;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#36825;&#20123;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#24402;&#32435;&#19982;&#35821;&#27861;&#32467;&#26500;&#21644;&#22810;&#35789;&#34920;&#36798;&#24335;&#65288;MWE&#65289;&#30456;&#20851;&#30340;&#19981;&#21516;&#35821;&#35328;&#29616;&#35937;&#65292;&#20174;&#32780;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#23398;&#20064;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;&#20026;&#36825;&#20123;&#20219;&#21153;&#24320;&#21457;&#30340;&#25968;&#25454;&#38598;&#30001;&#19968;&#31995;&#21015;&#20855;&#26377;&#21333;&#20010;&#23631;&#34109;&#35789;&#21644;&#25552;&#31034;&#30701;&#35821;&#30340;&#21477;&#23376;&#32452;&#25104;&#65292;&#36825;&#26377;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
Much recent effort has been devoted to creating large-scale language models. Nowadays, the most prominent approaches are based on deep neural networks, such as BERT. However, they lack transparency and interpretability, and are often seen as black boxes. This affects not only their applicability in downstream tasks but also the comparability of different architectures or even of the same model trained using different corpora or hyperparameters. In this paper, we propose a set of intrinsic evaluation tasks that inspect the linguistic information encoded in models developed for Brazilian Portuguese. These tasks are designed to evaluate how different language models generalise information related to grammatical structures and multiword expressions (MWEs), thus allowing for an assessment of whether the model has learned different linguistic phenomena. The dataset that was developed for these tasks is composed of a series of sentences with a single masked word and a cue phrase that helps in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07622</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;LMMs&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PALR&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29992;&#25143;/&#29289;&#21697;&#20114;&#21160;&#20316;&#20026;&#20505;&#36873;&#26816;&#32034;&#30340;&#25351;&#23548;&#65292;&#28982;&#21518;&#37319;&#29992;&#22522;&#20110;LLMs&#30340;&#25490;&#24207;&#27169;&#22411;&#29983;&#25104;&#25512;&#33616;&#29289;&#21697;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#30693;&#35782;&#32858;&#21512;&#36807;&#31243;&#19982;&#30456;&#20851;&#30693;&#35782;&#22270;&#30340;&#20840;&#23616;&#29305;&#24449;&#26377;&#25928;&#34701;&#21512;&#65292;&#23558;&#22686;&#24378;&#30340;&#22270;&#32467;&#26500;&#30693;&#35782;&#38598;&#25104;&#21040;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#33258;&#21160;&#24230;&#37327;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06294</link><description>&lt;p&gt;
CADGE&#65306;&#22522;&#20110;&#22270;&#32467;&#26500;&#30693;&#35782;&#32858;&#21512;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured Knowledge Aggregation. (arXiv:2305.06294v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#30693;&#35782;&#32858;&#21512;&#36807;&#31243;&#19982;&#30456;&#20851;&#30693;&#35782;&#22270;&#30340;&#20840;&#23616;&#29305;&#24449;&#26377;&#25928;&#34701;&#21512;&#65292;&#23558;&#22686;&#24378;&#30340;&#22270;&#32467;&#26500;&#30693;&#35782;&#38598;&#25104;&#21040;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#33258;&#21160;&#24230;&#37327;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#30693;&#35782;&#65288;commonsense knowledge&#65289;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#22270;&#30693;&#35782;&#19982;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#65292;&#23548;&#33268;&#25991;&#26412;&#21644;&#22270;&#30693;&#35782;&#32534;&#30721;&#36807;&#31243;&#22312;&#20018;&#34892;&#27969;&#27700;&#32447;&#20013;&#34987;&#20998;&#31163;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#20123;&#20998;&#31163;&#30340;&#34920;&#31034;&#23398;&#20064;&#38454;&#27573;&#21487;&#33021;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21253;&#21547;&#22312;&#20004;&#31181;&#36755;&#20837;&#30693;&#35782;&#31867;&#22411;&#20013;&#30340;&#25972;&#20307;&#19978;&#19979;&#25991;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65288;Context-aware GAT&#65289;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#30693;&#35782;&#32858;&#21512;&#36807;&#31243;&#26377;&#25928;&#22320;&#34701;&#21512;&#30456;&#20851;&#30693;&#35782;&#22270;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#24322;&#26500;&#29305;&#24449;&#8212;&#8212;&#23558;&#22270;&#30693;&#35782;&#19982;&#25991;&#26412;&#30456;&#32467;&#21512;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#22312;&#36830;&#25509;&#23376;&#22270;&#19978;&#20998;&#23618;&#24212;&#29992;&#22270;&#30693;&#35782;&#32858;&#21512;&#20197;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#23558;&#22686;&#24378;&#30340;&#22270;&#32467;&#26500;&#30693;&#35782;&#38598;&#25104;&#21040;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#33258;&#21160;&#24230;&#37327;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense knowledge is crucial to many natural language processing tasks. Existing works usually incorporate graph knowledge with conventional graph neural networks (GNNs), leading to the text and graph knowledge encoding processes being separated in a serial pipeline. We argue that these separate representation learning stages may be suboptimal for neural networks to learn the overall context contained in both types of input knowledge. In this paper, we propose a novel context-aware graph-attention model (Context-aware GAT), which can effectively incorporate global features of relevant knowledge graphs based on a context-enhanced knowledge aggregation process. Specifically, our framework leverages a novel representation learning approach to process heterogeneous features - combining flattened graph knowledge with text. To the best of our knowledge, this is the first attempt at hierarchically applying graph knowledge aggregation on a connected subgraph in addition to contextual infor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#23567;&#22411;&#30284;&#30151;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#31934;&#24230;&#20998;&#31867;&#30284;&#30151;&#26631;&#24535;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;94.45%&#30340;&#31934;&#24230;&#65292;&#27604;&#20197;&#24448;&#25991;&#29486;&#30340;&#30740;&#31350;&#32467;&#26524;&#39640;&#20986;&#33267;&#23569;8.04%&#12290;</title><link>http://arxiv.org/abs/2305.03501</link><description>&lt;p&gt;
&#21033;&#29992;Transformers&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#24449;&#30340;&#30284;&#30151;&#26631;&#24535;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cancer Hallmark Classification Using Bidirectional Encoder Representations From Transformers. (arXiv:2305.03501v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#23567;&#22411;&#30284;&#30151;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#31934;&#24230;&#20998;&#31867;&#30284;&#30151;&#26631;&#24535;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;94.45%&#30340;&#31934;&#24230;&#65292;&#27604;&#20197;&#24448;&#25991;&#29486;&#30340;&#30740;&#31350;&#32467;&#26524;&#39640;&#20986;&#33267;&#23569;8.04%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#31934;&#30830;&#22320;&#20998;&#31867;&#30284;&#30151;&#30340;&#26631;&#24535;&#65292;&#36825;&#26159;&#30284;&#30151;&#30740;&#31350;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#24449;&#30340;Transformers&#65288;BERT&#65289;&#26550;&#26500;&#65292;&#22312;&#23567;&#22411;&#30284;&#30151;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;94.45%&#30340;&#26174;&#33879;&#31934;&#24230;&#65292;&#36825;&#27604;&#25991;&#29486;&#20013;&#20960;&#20046;&#25152;&#26377;&#20043;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#37117;&#35201;&#39640;&#20986;&#33267;&#23569;8.04%&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#20998;&#31867;&#21644;&#29702;&#35299;&#30284;&#30151;&#30740;&#31350;&#25991;&#26412;&#25991;&#29486;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#22240;&#27492;&#23545;&#35813;&#39046;&#22495;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;&#30001;&#20110;&#30284;&#30151;&#20173;&#28982;&#26159;&#20840;&#29699;&#21313;&#22823;&#27515;&#22240;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to accurately classify the hallmarks of cancer, which is a crucial task in cancer research. Our proposed method utilizes the Bidirectional Encoder Representations from Transformers (BERT) architecture, which has shown exceptional performance in various downstream applications. By applying transfer learning, we fine-tuned the pre-trained BERT model on a small corpus of biomedical text documents related to cancer. The outcomes of our experimental investigations demonstrate that our approach attains a noteworthy accuracy of 94.45%, surpassing almost all prior findings with a substantial increase of at least 8.04% as reported in the literature. These findings highlight the effectiveness of our proposed model in accurately classifying and comprehending text documents for cancer research, thus contributing significantly to the field. As cancer remains one of the top ten leading causes of death globally, our approach holds great promise in advancing cancer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#38656;&#35201;&#20855;&#22791;&#30340;&#25216;&#33021;&#65292;&#21457;&#29616;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#21487;&#20197;&#22312;&#36825;&#20010;&#39046;&#22495;&#24102;&#26469;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.13835</link><description>&lt;p&gt;
&#22810;&#26041;&#32842;&#22825;&#65306;&#20154;&#31867;&#21644;&#27169;&#22411;&#20013;&#30340;&#32676;&#32842;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models. (arXiv:2304.13835v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22312;&#32676;&#20307;&#23545;&#35805;&#20013;&#38656;&#35201;&#20855;&#22791;&#30340;&#25216;&#33021;&#65292;&#21457;&#29616;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#21487;&#20197;&#22312;&#36825;&#20010;&#39046;&#22495;&#24102;&#26469;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#23545;&#35805;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#25104;&#23545;&#65288;&#21452;&#26041;&#65289;&#23545;&#35805;&#65292;&#24182;&#27809;&#26377;&#28041;&#21450;&#21040;&#22810;&#20110;&#20004;&#20010;&#20154;&#22312;&#19968;&#36215;&#23545;&#35805;&#30340;&#26085;&#24120;&#24773;&#26223;&#12290;&#26412;&#25991;&#20351;&#29992;LIGHT&#29615;&#22659;&#26500;&#24314;&#25509;&#22320;&#23545;&#35805;&#26469;&#25910;&#38598;&#21644;&#35780;&#20272;&#22810;&#26041;&#23545;&#35805;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#27604;&#22312;&#26032;&#25968;&#25454;&#38598;MultiLIGHT&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#29616;&#26377;&#30340;&#25104;&#23545;&#35757;&#32451;&#30340;&#23545;&#35805;&#27169;&#22411;&#20197;&#21450;&#24102;&#26377;&#23569;&#37327;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#23558;&#20844;&#24320;&#21457;&#24067;MultiLIGHT&#25968;&#25454;&#38598;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#22312;&#32676;&#20307;&#35774;&#32622;&#20013;&#24102;&#26469;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07438</link><description>&lt;p&gt;
&#21487;&#25805;&#20316;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#29983;&#25104;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#22238;&#24402;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29983;&#25104;&#28385;&#36275;&#22797;&#26434;&#38480;&#21046;&#30340;&#25991;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65306;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35789;&#27719;&#38480;&#21046;&#20063;&#20351;&#26465;&#20214;&#20998;&#24067;$\Pr(\text{text} | \alpha)$&#30340;&#37319;&#26679;&#21464;&#24471;&#19981;&#21487;&#35745;&#31639;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#25805;&#20316;&#30340;&#27010;&#29575;&#27169;&#22411;&#23558;&#35789;&#27719;&#38480;&#21046;&#24378;&#21152;&#20110;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026; GeLaTo&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#31934;&#31616;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26469;&#25511;&#21046;&#20174;GPT2&#21040;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#12290;GeLaTo&#22312;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;CommonGen&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22823;&#24133;&#20987;&#36133;&#20102;&#21508;&#31181;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#20026;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#36824;&#28608;&#21169;&#20154;&#20204;&#24320;&#21457;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution $\Pr(\text{text} | \alpha)$ is intractable for even the simplest lexical constraints $\alpha$. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17491</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#35745;&#31639;&#26426;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#36890;&#29992;&#20219;&#21153;&#30340;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#21644;&#21327;&#21161;&#22797;&#26434;&#38382;&#39064;&#30340;&#35299;&#20915;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#29983;&#20135;&#21147;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#35299;&#20915;&#26032;&#30340;&#35745;&#31639;&#26426;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#19987;&#23478;&#31034;&#33539;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#26032;&#20219;&#21153;&#26469;&#35828;&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#65288;RCI&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#24182;&#22312;&#25209;&#35780;&#21644;&#25913;&#36827;&#36755;&#20986;&#30340;&#36807;&#31243;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;RCI&#26041;&#27861;&#22312;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;RCI&#26041;&#27861;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#20165;&#26377;&#30340;&#23569;&#25968;&#31034;&#33539;&#65292;&#19982;&#26368;&#26032;&#30340;SL+RL&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICE&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22806;&#25512;&#25511;&#21046;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#25511;&#21046;&#32534;&#36753;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#33258;&#21160;&#35774;&#35745;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33647;&#29289;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#36739;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.04562</link><description>&lt;p&gt;
&#36845;&#20195;&#20462;&#27491;&#30340;&#22806;&#25512;&#25511;&#21046;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Extrapolative Controlled Sequence Generation via Iterative Refinement. (arXiv:2303.04562v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICE&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22806;&#25512;&#25511;&#21046;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#25511;&#21046;&#32534;&#36753;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#33258;&#21160;&#35774;&#35745;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33647;&#29289;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#36739;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22806;&#25512;&#25511;&#21046;&#29983;&#25104;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#23646;&#24615;&#20540;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#24207;&#21015;&#12290;&#22312;&#33258;&#21160;&#35774;&#35745;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#33647;&#29289;&#30740;&#31350;&#39046;&#22495;&#65292;&#36825;&#20010;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#30446;&#26631;&#26159;&#35774;&#35745;&#20986;&#27604;&#29616;&#26377;&#24207;&#21015;&#26356;&#22909;&#65288;&#20363;&#22914;&#26356;&#31283;&#23450;&#65289;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#12290;&#22240;&#27492;&#65292;&#25353;&#29031;&#23450;&#20041;&#65292;&#30446;&#26631;&#24207;&#21015;&#21450;&#20854;&#23646;&#24615;&#20540;&#36229;&#20986;&#35757;&#32451;&#20998;&#24067;&#65292;&#25361;&#25112;&#29616;&#26377;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#24207;&#21015;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36845;&#20195;&#25511;&#21046;&#22806;&#25512;&#65288;ICE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#23545;&#24207;&#21015;&#36827;&#34892;&#23616;&#37096;&#32534;&#36753;&#26469;&#23454;&#29616;&#22806;&#25512;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#30340;&#24207;&#21015;&#23545;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#28436;&#31034;&#24494;&#23567;&#30340;&#23646;&#24615;&#20540;&#25913;&#36827;&#12290;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65288;&#24773;&#24863;&#20998;&#26512;&#65289;&#21644;&#20004;&#20010;&#34507;&#30333;&#36136;&#24037;&#31243;&#20219;&#21153;&#65288;ACE2&#31283;&#23450;&#24615;&#21644;AAV&#36866;&#24212;&#24615;&#65289;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ICE&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of extrapolative controlled generation, i.e., generating sequences with attribute values beyond the range seen in training. This task is of significant importance in automated design, especially drug discovery, where the goal is to design novel proteins that are \textit{better} (e.g., more stable) than existing sequences. Thus, by definition, the target sequences and their attribute values are out of the training distribution, posing challenges to existing methods that aim to directly generate the target sequence. Instead, in this work, we propose Iterative Controlled Extrapolation (ICE) which iteratively makes local edits to a sequence to enable extrapolation. We train the model on synthetically generated sequence pairs that demonstrate small improvement in the attribute value. Results on one natural language task (sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV fitness) show that ICE considerably outperforms state-of-the-art approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#23545;&#40784;&#12289;&#26080;&#23545;&#40784;&#21644;&#28151;&#21512;&#26041;&#27861;&#22788;&#29702;&#21796;&#37266;&#35789;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#30446;&#26631;&#25805;&#20316;&#28857;&#19978;&#26080;&#23545;&#40784;&#31995;&#32479;&#27604;&#22522;&#20110;&#23545;&#40784;&#30340;&#26041;&#27861;&#26356;&#22909;&#65292;&#32780;&#20351;&#29992;&#23569;&#37327;&#23545;&#40784;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#23545;&#40784;&#25968;&#25454;&#30340;&#28151;&#21512;&#26041;&#27861;&#22312;&#28385;&#36275;&#21021;&#22987;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20986;&#20102;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.08950</link><description>&lt;p&gt;
&#22788;&#29702;&#21796;&#37266;&#35789;&#26816;&#27979;&#30340;&#23545;&#40784;&#65306;&#22522;&#20110;&#23545;&#40784;&#12289;&#26080;&#23545;&#40784;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-Based, Alignment-Free and Hybrid Approaches. (arXiv:2302.08950v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#23545;&#40784;&#12289;&#26080;&#23545;&#40784;&#21644;&#28151;&#21512;&#26041;&#27861;&#22788;&#29702;&#21796;&#37266;&#35789;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#30446;&#26631;&#25805;&#20316;&#28857;&#19978;&#26080;&#23545;&#40784;&#31995;&#32479;&#27604;&#22522;&#20110;&#23545;&#40784;&#30340;&#26041;&#27861;&#26356;&#22909;&#65292;&#32780;&#20351;&#29992;&#23569;&#37327;&#23545;&#40784;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#23545;&#40784;&#25968;&#25454;&#30340;&#28151;&#21512;&#26041;&#27861;&#22312;&#28385;&#36275;&#21021;&#22987;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20986;&#20102;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21796;&#37266;&#35789;&#26816;&#27979;&#23384;&#22312;&#20110;&#22823;&#22810;&#25968;&#26234;&#33021;&#23478;&#23621;&#21644;&#20415;&#25658;&#35774;&#22791;&#20013;&#12290;&#23427;&#20026;&#36825;&#20123;&#35774;&#22791;&#25552;&#20379;&#20102;&#22312;&#21484;&#21796;&#26102;&#8220;&#21796;&#37266;&#8221;&#30340;&#33021;&#21147;&#65292;&#24182;&#33410;&#30465;&#20102;&#21151;&#29575;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#23545;&#40784;&#22312;&#24320;&#21457;&#22238;&#31572;&#36890;&#29992;&#30701;&#35821;&#30340;&#21796;&#37266;&#35789;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19977;&#31181;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26159;&#22522;&#20110;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#27169;&#22411;&#20351;&#29992;&#36880;&#24103;&#20132;&#21449;&#29109;&#36827;&#34892;&#35757;&#32451;&#12290;&#31532;&#20108;&#31181;&#26159;&#26080;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#27169;&#22411;&#20351;&#29992;CTC&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31532;&#19977;&#31181;&#26159;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#27492;&#26041;&#26696;&#20013;&#65292;&#27169;&#22411;&#20351;&#29992;&#23569;&#37327;&#23545;&#40784;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#22823;&#37327;&#26410;&#23545;&#40784;&#30340;&#25968;&#25454;&#36827;&#34892;&#35843;&#20248;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#23545;&#40784;&#21040;&#26410;&#23545;&#40784;&#27604;&#29575;&#23545;&#28151;&#21512;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30446;&#26631;&#25805;&#20316;&#28857;&#19978;&#65292;&#26080;&#23545;&#40784;&#31995;&#32479;&#27604;&#22522;&#20110;&#23545;&#40784;&#26356;&#22909;&#65292;&#24182;&#19988;&#22312;&#20165;&#20351;&#29992;20&#65285;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#31526;&#21512;&#25105;&#20204;&#30340;&#21021;&#22987;&#32422;&#26463;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wake word detection exists in most intelligent homes and portable devices. It offers these devices the ability to "wake up" when summoned at a low cost of power and computing. This paper focuses on understanding alignment's role in developing a wake-word system that answers a generic phrase. We discuss three approaches. The first is alignment-based, where the model is trained with frame-wise cross-entropy. The second is alignment-free, where the model is trained with CTC. The third, proposed by us, is a hybrid solution in which the model is trained with a small set of aligned data and then tuned with a sizeable unaligned dataset. We compare the three approaches and evaluate the impact of the different aligned-to-unaligned ratios for hybrid training. Our results show that the alignment-free system performs better than the alignment-based for the target operating point, and with a small fraction of the data (20%), we can train a model that complies with our initial constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GINSEW &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31192;&#23494;&#20449;&#21495;&#27880;&#20837;&#21040;&#27599;&#20010;&#30446;&#26631;&#26631;&#35760;&#30340;&#35299;&#30721;&#27493;&#39588;&#30340;&#27010;&#29575;&#21521;&#37327;&#20013;&#65292;&#20445;&#25252;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#26377;&#25928;&#35782;&#21035;&#20986;&#20405;&#26435;&#34892;&#20026;&#65292;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2302.03162</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24418;&#27700;&#21360;&#20445;&#25252;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Protecting Language Generation Models via Invisible Watermarking. (arXiv:2302.03162v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GINSEW &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31192;&#23494;&#20449;&#21495;&#27880;&#20837;&#21040;&#27599;&#20010;&#30446;&#26631;&#26631;&#35760;&#30340;&#35299;&#30721;&#27493;&#39588;&#30340;&#27010;&#29575;&#21521;&#37327;&#20013;&#65292;&#20445;&#25252;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#26377;&#25928;&#35782;&#21035;&#20986;&#20405;&#26435;&#34892;&#20026;&#65292;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#26159;&#35768;&#22810;&#24212;&#29992;&#30340;&#26377;&#21147;&#25903;&#25345;&#32773;&#12290;&#35768;&#22810;&#36825;&#26679;&#30340;&#27169;&#22411;&#25552;&#20379;&#20813;&#36153;&#25110;&#32463;&#27982;&#23454;&#24800;&#30340; API &#35775;&#38382;&#65292;&#36825;&#20351;&#23427;&#20204;&#21487;&#33021;&#21463;&#21040;&#27169;&#22411;&#25277;&#21462;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#27491;&#20351;&#29992;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#20363;&#22914;&#35789;&#27719;&#27700;&#21360;&#21644;&#21516;&#20041;&#35789;&#26367;&#25442;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#34987;&#26126;&#26174;&#30340;&#23545;&#31574;&#22914;&#8220;&#21516;&#20041;&#35789;&#38543;&#26426;&#21270;&#8221;&#31561;&#25152;&#25269;&#28040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; GINSEW&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#33976;&#39311;&#20445;&#25252;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#31192;&#23494;&#20449;&#21495;&#27880;&#20837;&#21040;&#27599;&#20010;&#30446;&#26631;&#26631;&#35760;&#30340;&#35299;&#30721;&#27493;&#39588;&#30340;&#27010;&#29575;&#21521;&#37327;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25506;&#27979;&#23244;&#30097;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;&#31192;&#23494;&#28040;&#24687;&#26159;&#21542;&#30001;&#21463;&#20445;&#25252;&#30340;&#27169;&#22411;&#33976;&#39311;&#32780;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GINSEW &#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#20405;&#26435;&#34892;&#20026;&#65292;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;&#26497;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as "synonym randomization". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#30495;&#23454;&#29992;&#25143;&#35821;&#21477;&#26469;&#24110;&#21161;&#22686;&#21152;&#31995;&#32479;&#30340;&#35821;&#35328;&#21644;&#21151;&#33021;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#19981;&#20250;&#21361;&#21450;&#23454;&#38469;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2212.10520</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#39046;&#22495;&#33258;&#36866;&#24212;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Domain Adaptation of Semantic Parsers. (arXiv:2212.10520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#30495;&#23454;&#29992;&#25143;&#35821;&#21477;&#26469;&#24110;&#21161;&#22686;&#21152;&#31995;&#32479;&#30340;&#35821;&#35328;&#21644;&#21151;&#33021;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#19981;&#20250;&#21361;&#21450;&#23454;&#38469;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#36741;&#21161;&#29992;&#25143;&#22788;&#29702;&#20010;&#20154;&#25110;&#26426;&#23494;&#20107;&#21153;&#12290;&#22240;&#27492;&#65292;&#27492;&#31867;&#31995;&#32479;&#30340;&#24320;&#21457;&#20154;&#21592;&#36890;&#24120;&#34987;&#31105;&#27490;&#35266;&#23519;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#12290;&#37027;&#20040;&#65292;&#20182;&#20204;&#22914;&#20309;&#30693;&#36947;&#31995;&#32479;&#22312;&#21738;&#20123;&#26041;&#38754;&#23384;&#22312;&#22833;&#36133;&#24182;&#38656;&#35201;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#26032;&#21151;&#33021;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#21512;&#25104;&#30495;&#23454;&#29992;&#25143;&#35821;&#21477;&#26469;&#24110;&#21161;&#22686;&#21152;&#31995;&#32479;&#30340;&#35821;&#35328;&#21644;&#21151;&#33021;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#19981;&#20250;&#21361;&#21450;&#23454;&#38469;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#24046;&#20998;&#38544;&#31169;&#29983;&#25104;&#26041;&#27861;&#65292;&#39318;&#20808;&#29983;&#25104;&#28508;&#22312;&#30340;&#35821;&#20041;&#35299;&#26512;&#65292;&#28982;&#21518;&#26681;&#25454;&#35299;&#26512;&#29983;&#25104;&#35821;&#21477;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31169;&#26377;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#30456;&#23545;&#20110;&#24403;&#21069;&#26041;&#27861;&#25552;&#39640;&#20102;MAUVE 2.5&#20493;&#21644;&#35821;&#20041;&#35206;&#30422;1.3&#20493;&#65292;&#25552;&#39640;&#20102;&#27969;&#30021;&#24615;&#21644;&#35821;&#20041;&#35206;&#30422;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#28155;&#21152;&#26032;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue systems often assist users with personal or confidential matters. For this reason, the developers of such a system are generally prohibited from observing actual usage. So how can they know where the system is failing and needs more training data or new functionality? In this work, we study ways in which realistic user utterances can be generated synthetically, to help increase the linguistic and functional coverage of the system, without compromising the privacy of actual users. To this end, we propose a two-stage Differentially Private (DP) generation method which first generates latent semantic parses, and then generates utterances based on the parses. Our proposed approach improves MAUVE by 2.5X and parse tree function type overlap by 1.3X relative to current approaches for private synthetic data generation, improving both on fluency and semantic coverage. We further validate our approach on a realistic domain adaptation task of adding new functionality from 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20840;&#26032;&#30340;&#28151;&#21512;&#21487;&#25511;&#21046;&#25688;&#35201;&#30340;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20154;&#24037;&#27880;&#37322;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;MACSum&#12290;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#22522;&#20110;&#30828;&#25552;&#31034;&#24494;&#35843;&#21644;&#36719;&#21069;&#32512;&#24494;&#35843;&#12290;&#36825;&#19968;&#24037;&#20316;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#23427;&#35299;&#20915;&#20102;&#25511;&#21046;&#28151;&#21512;&#23646;&#24615;&#30340;&#25688;&#35201;&#29983;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.05041</link><description>&lt;p&gt;
MACSum: &#28151;&#21512;&#23646;&#24615;&#21487;&#25511;&#21046;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MACSum: Controllable Summarization with Mixed Attributes. (arXiv:2211.05041v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20840;&#26032;&#30340;&#28151;&#21512;&#21487;&#25511;&#21046;&#25688;&#35201;&#30340;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20154;&#24037;&#27880;&#37322;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;MACSum&#12290;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#22522;&#20110;&#30828;&#25552;&#31034;&#24494;&#35843;&#21644;&#36719;&#21069;&#32512;&#24494;&#35843;&#12290;&#36825;&#19968;&#24037;&#20316;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#23427;&#35299;&#20915;&#20102;&#25511;&#21046;&#28151;&#21512;&#23646;&#24615;&#30340;&#25688;&#35201;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#21046;&#30340;&#25688;&#35201;&#29983;&#25104;&#21487;&#20197;&#35753;&#29992;&#25143;&#29983;&#25104;&#24102;&#26377;&#25351;&#23450;&#23646;&#24615;&#30340;&#33258;&#23450;&#20041;&#25688;&#35201;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32570;&#20047;&#21463;&#25511;&#25688;&#35201;&#30340;&#25351;&#23450;&#27880;&#37322;&#65292;&#29616;&#26377;&#24037;&#20316;&#19981;&#24471;&#19981;&#36890;&#36807;&#36866;&#24212;&#36890;&#29992;&#25688;&#35201;&#22522;&#20934;&#26469;&#21019;&#24314;&#20266;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#19987;&#27880;&#20110;&#21333;&#20010;&#23646;&#24615;&#30340;&#25511;&#21046;&#65288;&#20363;&#22914;&#65292;&#30701;&#25688;&#35201;&#25110;&#39640;&#24230;&#25277;&#35937;&#30340;&#25688;&#35201;&#65289;&#65292;&#32780;&#19981;&#26159;&#25511;&#21046;&#28151;&#21512;&#23646;&#24615;&#19968;&#36215;&#65288;&#20363;&#22914;&#65292;&#30701;&#19988;&#39640;&#24230;&#25277;&#35937;&#30340;&#25688;&#35201;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MACSum&#65292;&#29992;&#20110;&#25511;&#21046;&#28151;&#21512;&#23646;&#24615;&#30340;&#31532;&#19968;&#20010;&#20154;&#24037;&#27880;&#37322;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290; &#23427;&#21253;&#21547;&#26469;&#33258;&#20004;&#20010;&#22495;&#65288;&#26032;&#38395;&#25991;&#31456;&#21644;&#23545;&#35805;&#65289;&#30340;&#28304;&#25991;&#26412;&#65292;&#30001;&#20116;&#20010;&#35774;&#35745;&#23646;&#24615;&#65288;&#38271;&#24230;&#12289;&#25552;&#21462;&#12289;&#29305;&#23450;&#24615;&#12289;&#20027;&#39064;&#21644;&#35828;&#35805;&#20154;&#65289;&#25511;&#21046;&#30340;&#20154;&#24037;&#27880;&#37322;&#25688;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#26032;&#30340;&#28151;&#21512;&#21487;&#25511;&#21046;&#25688;&#35201;&#20219;&#21153;&#65292;&#22522;&#20110;&#30828;&#25552;&#31034;&#24494;&#35843;&#21644;&#36719;&#21069;&#32512;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable summarization allows users to generate customized summaries with specified attributes. However, due to the lack of designated annotations of controlled summaries, existing works have to craft pseudo datasets by adapting generic summarization benchmarks. Furthermore, most research focuses on controlling single attributes individually (e.g., a short summary or a highly abstractive summary) rather than controlling a mix of attributes together (e.g., a short and highly abstractive summary). In this paper, we propose MACSum, the first human-annotated summarization dataset for controlling mixed attributes. It contains source texts from two domains, news articles and dialogues, with human-annotated summaries controlled by five designed attributes (Length, Extractiveness, Specificity, Topic, and Speaker). We propose two simple and effective parameter-efficient approaches for the new task of mixed controllable summarization based on hard prompt tuning and soft prefix tuning. Result
&lt;/p&gt;</description></item><item><title>&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#22312;&#22823;&#35268;&#27169;&#19978;&#25918;&#22823;&#20102;&#20154;&#21475;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#21363;&#20351;&#25552;&#31034;&#19981;&#26126;&#30830;&#25552;&#21040;&#36523;&#20221;&#21644;&#20154;&#21475;&#32479;&#35745;&#35821;&#35328;&#25110;&#37319;&#21462;&#32531;&#35299;&#31574;&#30053;&#20063;&#26080;&#27861;&#28040;&#38500;&#36825;&#31181;&#21360;&#35937;&#12290;</title><link>http://arxiv.org/abs/2211.03759</link><description>&lt;p&gt;
&#26131;&#20110;&#35775;&#38382;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#22312;&#22823;&#35268;&#27169;&#19978;&#25918;&#22823;&#20102;&#20154;&#21475;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale. (arXiv:2211.03759v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03759
&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#22312;&#22823;&#35268;&#27169;&#19978;&#25918;&#22823;&#20102;&#20154;&#21475;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#21363;&#20351;&#25552;&#31034;&#19981;&#26126;&#30830;&#25552;&#21040;&#36523;&#20221;&#21644;&#20154;&#21475;&#32479;&#35745;&#35821;&#35328;&#25110;&#37319;&#21462;&#32531;&#35299;&#31574;&#30053;&#20063;&#26080;&#27861;&#28040;&#38500;&#36825;&#31181;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#29992;&#25143;&#32534;&#20889;&#30340;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;&#22270;&#20687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29616;&#22312;&#22312;&#32593;&#19978;&#24191;&#27867;&#21487;&#29992;&#65292;&#24182;&#34987;&#25968;&#30334;&#19975;&#29992;&#25143;&#29992;&#20110;&#27599;&#22825;&#29983;&#25104;&#25968;&#30334;&#19975;&#24352;&#22270;&#20687;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#27169;&#22411;&#25918;&#22823;&#21361;&#38505;&#21644;&#22797;&#26434;&#21051;&#26495;&#21360;&#35937;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#24191;&#27867;&#30340;&#26222;&#36890;&#25552;&#31034;&#20250;&#20135;&#29983;&#21051;&#26495;&#21360;&#35937;&#65292;&#21253;&#25324;&#20165;&#25552;&#21040;&#29305;&#24449;&#12289;&#25551;&#36848;&#31526;&#12289;&#32844;&#19994;&#25110;&#23545;&#35937;&#30340;&#25552;&#31034;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#22522;&#26412;&#29305;&#24449;&#25110;&#31038;&#20250;&#35282;&#33394;&#30340;&#24773;&#20917;&#19979;&#65292;&#32467;&#26524;&#29983;&#25104;&#20102;&#24378;&#35843;&#30333;&#20154;&#20316;&#20026;&#29702;&#24819;&#30340;&#24418;&#35937;&#65292;&#25552;&#31034;&#32844;&#19994;&#30340;&#24773;&#20917;&#19979;&#23548;&#33268;&#31181;&#26063;&#21644;&#24615;&#21035;&#24046;&#24322;&#30340;&#25918;&#22823;&#65292;&#25552;&#31034;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#23548;&#33268;&#32654;&#22269;&#35268;&#33539;&#30340;&#20877;&#29616;&#12290;&#21051;&#26495;&#21360;&#35937;&#23384;&#22312;&#65292;&#26080;&#35770;&#25552;&#31034;&#26159;&#21542;&#26126;&#30830;&#25552;&#21040;&#36523;&#20221;&#21644;&#20154;&#21475;&#32479;&#35745;&#35821;&#35328;&#25110;&#36991;&#20813;&#27492;&#31867;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#37319;&#21462;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#21051;&#26495;&#21360;&#35937;&#20173;&#28982;&#23384;&#22312;&#65307;&#29992;&#25143;&#35797;&#22270;&#36890;&#36807;&#35831;&#27714;&#20855;&#26377;&#29305;&#23450;&#21453;&#21051;&#26495;&#21360;&#35937;&#30340;&#22270;&#20687;&#26469;&#25171;&#30772;&#21051;&#26495;&#21360;&#35937;&#65292;&#25110;&#32773;&#26426;&#26500;&#35797;&#22270;&#36807;&#28388;&#25552;&#31034;&#22312;&#22266;&#26377;&#25928;&#24212;&#19978;&#37117;&#19981;&#33021;&#25104;&#21151;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25216;&#26415;&#22312;&#35268;&#27169;&#19978;&#21487;&#20197;&#25918;&#22823;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#24182;&#24378;&#35843;&#20102;&#23567;&#24515;&#35880;&#24910;&#22320;&#22788;&#29702;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models that convert user-written text descriptions into images are now widely available online and used by millions of users to generate millions of images a day. We investigate the potential for these models to amplify dangerous and complex stereotypes. We find a broad range of ordinary prompts produce stereotypes, including prompts simply mentioning traits, descriptors, occupations, or objects. For example, we find cases of prompting for basic traits or social roles resulting in images reinforcing whiteness as ideal, prompting for occupations resulting in amplification of racial and gender disparities, and prompting for objects resulting in reification of American norms. Stereotypes are present regardless of whether prompts explicitly mention identity and demographic language or avoid such language. Moreover, stereotypes persist despite mitigation strategies; neither user attempts to counter stereotypes by requesting images with specific counter-stereotypes nor insti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26367;&#20195;&#30828;&#36127;&#20363;&#25366;&#25496;&#30340;&#20840;&#23616;&#23545;&#27604;&#25209;&#37327;&#37319;&#26679;&#26041;&#27861;GCBS&#65292;&#33021;&#22815;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26131;&#20110;&#23454;&#29616;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.12874</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#26412;&#25490;&#21015;&#20248;&#21270;&#30340;&#20840;&#23616;&#23545;&#27604;&#25209;&#37327;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Global Contrastive Batch Sampling via Optimization on Sample Permutations. (arXiv:2210.12874v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26367;&#20195;&#30828;&#36127;&#20363;&#25366;&#25496;&#30340;&#20840;&#23616;&#23545;&#27604;&#25209;&#37327;&#37319;&#26679;&#26041;&#27861;GCBS&#65292;&#33021;&#22815;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26131;&#20110;&#23454;&#29616;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26368;&#36817;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35768;&#22810;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#25366;&#25496;&#30340;&#30828;&#36127;&#20363;&#26469;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#25209;&#22788;&#29702;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23427;&#20204;&#22686;&#21152;&#20102;&#19982;&#25366;&#25496;&#36127;&#20363;&#25968;&#25104;&#27604;&#20363;&#30340;&#32426;&#20803;&#38271;&#24230;&#65292;&#24182;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;&#26368;&#36817;&#37051;&#23621;&#32034;&#24341;&#25110;&#20174;&#26368;&#36817;&#30340;&#25209;&#27425;&#20013;&#36827;&#34892;&#25366;&#25496;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#30828;&#36127;&#20363;&#25366;&#25496;&#30340;&#26367;&#20195;&#26041;&#26696;&#65306;&#20840;&#23616;&#23545;&#27604;&#25209;&#37327;&#37319;&#26679;&#65288;GCBS&#65289;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#36817;&#20284;&#25209;&#22788;&#29702;&#20998;&#37197;&#38382;&#39064;&#65292;&#23427;&#19978;&#30028;&#20102;&#23545;&#27604;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#20840;&#23616;&#25439;&#22833;&#21644;&#35757;&#32451;&#25439;&#22833;&#20043;&#38388;&#30340;&#24046;&#36317;$\mathcal{L}^{Global} - \mathcal{L}^{Train}$&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;GCBS&#25913;&#21892;&#20102;&#21477;&#23376;&#23884;&#20837;&#21644;&#20195;&#30721;&#25628;&#32034;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;GCBS&#26131;&#20110;&#23454;&#29616;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23569;&#37327;&#38468;&#21152;&#20195;&#30721;&#65292;&#19981;&#38656;&#35201;&#32500;&#25252;&#22806;&#37096;&#25968;&#25454;&#32467;&#26500;&#65292;&#22914;&#26368;&#36817;&#37051;&#23621;&#32034;&#24341;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning has recently achieved state-of-the-art performance in a wide range of tasks. Many contrastive learning approaches use mined hard negatives to make batches more informative during training but these approaches are inefficient as they increase epoch length proportional to the number of mined negatives and require frequent updates of nearest neighbor indices or mining from recent batches. In this work, we provide an alternative to hard negative mining, Global Contrastive Batch Sampling (GCBS), an efficient approximation to the batch assignment problem that upper bounds the gap between the global and training losses, $\mathcal{L}^{Global} - \mathcal{L}^{Train}$, in contrastive learning settings. Through experimentation we find GCBS improves state-of-the-art performance in sentence embedding and code-search tasks. Additionally, GCBS is easy to implement as it requires only a few additional lines of code, does not maintain external data structures such as nearest neighbo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#35782;&#21035;&#20020;&#24202;&#35760;&#24405;&#20013;&#30340;&#30284;&#30151;&#65292;&#37319;&#29992;&#29942;&#39048;&#36866;&#37197;&#22120;&#21644;&#25552;&#31034;&#24494;&#35843;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#23427;&#26041;&#27861;&#65292;&#21487;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.09440</link><description>&lt;p&gt;
&#21033;&#29992;&#29942;&#39048;&#36866;&#37197;&#22120;&#22312;&#20302;&#36164;&#28304;&#38480;&#21046;&#19979;&#35782;&#21035;&#20020;&#24202;&#35760;&#24405;&#20013;&#30340;&#30284;&#30151;
&lt;/p&gt;
&lt;p&gt;
Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints. (arXiv:2210.09440v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#35782;&#21035;&#20020;&#24202;&#35760;&#24405;&#20013;&#30340;&#30284;&#30151;&#65292;&#37319;&#29992;&#29942;&#39048;&#36866;&#37197;&#22120;&#21644;&#25552;&#31034;&#24494;&#35843;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#23427;&#26041;&#27861;&#65292;&#21487;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#23384;&#20648;&#22312;&#20020;&#24202;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#20449;&#24687;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26159;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#20010;&#27963;&#36291;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#22312;&#19968;&#20010;&#21547;&#26377;&#20020;&#24202;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#31616;&#21333;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21040;&#19987;&#19994;&#30340;&#36716;&#25442;&#22120;&#65292;&#20363;&#22914; BioBERT&#65292;&#24182;&#38468;&#26377;&#25351;&#31034;&#26679;&#26412;&#26159;&#21542;&#19982;&#30284;&#30151;&#30456;&#20851;&#30340;&#19968;&#32452;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29305;&#21035;&#37319;&#29992;&#20102;&#26469;&#33258;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21363;&#29942;&#39048;&#36866;&#37197;&#22120;&#21644;&#25552;&#31034;&#35843;&#25972;&#65292;&#20197;&#36866;&#24212;&#25105;&#20204;&#30340;&#19987;&#19994;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20923;&#32467;&#30340;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29942;&#39048;&#36866;&#37197;&#22120;&#24494;&#35843;&#65292;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#31574;&#30053;&#65292;&#21253;&#25324;&#20840;&#38754;&#24494;&#35843;&#19987;&#29992;&#30340;BioBERT&#27169;&#22411;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#29942;&#39048;&#36866;&#37197;&#22120;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#25110;&#22788;&#29702;&#33021;&#21147;&#26102;&#65292;&#21487;&#33021;&#26159;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30340;&#21487;&#34892;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP. In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not.  Furthermore, we specifically employ efficient fine-tuning methods from NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. Our evaluations suggest that fine-tuning a frozen BERT model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised BioBERT model. Based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedical text mining. The
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#26174;&#33879;&#22270;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#24182;&#25351;&#23548;&#20102;GPT-3.5&#29983;&#25104;&#26174;&#33879;&#22270;&#35328;&#35821;&#21270;&#65292;&#24471;&#21040;&#26368;&#39640;&#20154;&#31867;&#35780;&#20998;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.07222</link><description>&lt;p&gt;
&#26174;&#33879;&#22270;&#32763;&#35793;&#65306;&#27169;&#22411;&#26080;&#20851;&#21644;&#22522;&#20110;&#25351;&#20196;&#26041;&#27861;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#34920;&#31034;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods. (arXiv:2210.07222v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#26174;&#33879;&#22270;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#24182;&#25351;&#23548;&#20102;GPT-3.5&#29983;&#25104;&#26174;&#33879;&#22270;&#35328;&#35821;&#21270;&#65292;&#24471;&#21040;&#26368;&#39640;&#20154;&#31867;&#35780;&#20998;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#22270;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#30340;&#36755;&#20837;&#29305;&#24449;&#26469;&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#24456;&#38590;&#34987;&#38750;&#19987;&#19994;&#20154;&#22763;&#35299;&#37322;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#35768;&#22810;&#29305;&#24449;&#30340;&#23454;&#20363;&#12290;&#20026;&#20102;&#20351;&#23427;&#20204;&#26356;&#26131;&#20110;&#29702;&#35299;&#65292;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#23558;&#26174;&#33879;&#22270;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#20219;&#21153;&#65292;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#20851;&#38190;&#25361;&#25112;&#30340;&#26041;&#27861;&#65306;&#20160;&#20040;&#21644;&#22914;&#20309;&#34920;&#36798;&#12290;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#26631;&#35760;&#32423;&#23646;&#24615;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65288;&#22522;&#20110;&#25628;&#32034;&#21644;&#22522;&#20110;&#25351;&#20196;&#30340;&#34920;&#36798;&#65289;&#19982;&#20256;&#32479;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#34920;&#31034;&#65288;&#28909;&#22270;&#21487;&#35270;&#21270;&#21644;&#25277;&#21462;&#29702;&#24615;&#65289;&#65292;&#35780;&#20272;&#21487;&#27169;&#25311;&#24615;&#12289;&#24544;&#35802;&#24230;&#12289;&#24110;&#21161;&#24615;&#21644;&#26131;&#29702;&#35299;&#24615;&#12290;&#36890;&#36807;&#25351;&#23548; GPT-3.5 &#29983;&#25104;&#26174;&#33879;&#22270;&#35328;&#35821;&#21270;&#65292;&#21487;&#20197;&#24471;&#21040;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#21253;&#25324;&#20851;&#32852;&#12289;&#25277;&#35937;&#27010;&#25324;&#21644;&#24120;&#35782;&#25512;&#29702;&#65292;&#20174;&#32780;&#21462;&#24471;&#20102;&#36804;&#20170;&#26368;&#39640;&#30340;&#20154;&#31867;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saliency maps can explain a neural model's predictions by identifying important input features. They are difficult to interpret for laypeople, especially for instances with many features. In order to make them more accessible, we formalize the underexplored task of translating saliency maps into natural language and compare methods that address two key challenges of this approach -- what and how to verbalize. In both automatic and human evaluation setups, using token-level attributions from text classification tasks, we compare two novel methods (search-based and instruction-based verbalizations) against conventional feature importance representations (heatmap visualizations and extractive rationales), measuring simulatability, faithfulness, helpfulness and ease of understanding. Instructing GPT-3.5 to generate saliency map verbalizations yields plausible explanations which include associations, abstractive summarization and commonsense reasoning, achieving by far the highest human rat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;Z-Code++&#65292;&#23427;&#20351;&#29992;&#20004;&#31181;&#39044;&#35757;&#32451;&#38454;&#27573;&#21644;&#19977;&#31181;&#25216;&#26415;&#36827;&#34892;&#20248;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#35299;&#32806;&#30340;&#27880;&#24847;&#21147;&#23618;&#21644;&#34701;&#21512;&#32534;&#30721;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21442;&#25968;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2208.09770</link><description>&lt;p&gt;
Z-Code++&#65306;&#19968;&#31181;&#38024;&#23545;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20248;&#21270;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization. (arXiv:2208.09770v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;Z-Code++&#65292;&#23427;&#20351;&#29992;&#20004;&#31181;&#39044;&#35757;&#32451;&#38454;&#27573;&#21644;&#19977;&#31181;&#25216;&#26415;&#36827;&#34892;&#20248;&#21270;&#65292;&#20854;&#20013;&#21253;&#25324;&#35299;&#32806;&#30340;&#27880;&#24847;&#21147;&#23618;&#21644;&#34701;&#21512;&#32534;&#30721;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#22312;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#21442;&#25968;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Z-Code++&#65292;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20248;&#21270;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#36816;&#29992;&#20102;&#19977;&#31181;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#38454;&#27573;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#20351;&#29992;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#30340;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#25688;&#35201;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#36830;&#32493;&#30340;&#39044;&#35757;&#32451;&#20197;&#25552;&#39640;&#20854;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#29992;&#35299;&#32806;&#30340;&#27880;&#24847;&#21147;&#23618;&#21462;&#20195;&#32534;&#30721;&#22120;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#20854;&#20013;&#27599;&#20010;&#21333;&#35789;&#20998;&#21035;&#20351;&#29992;&#20004;&#20010;&#21521;&#37327;&#26469;&#34920;&#31034;&#20854;&#20869;&#23481;&#21644;&#20301;&#32622;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#32534;&#30721;&#22120;&#20013;&#30340;&#34701;&#21512;&#32534;&#30721;&#26041;&#27861;&#65292;&#20197;&#19968;&#31181;&#20998;&#23618;&#30340;&#26041;&#24335;&#23545;&#38271;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;Z-Code++&#22312;5&#31181;&#35821;&#35328;&#30340;13&#20010;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#26377;9&#20010;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21442;&#25968;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;XSum&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#27604;&#20854;&#22823;600&#20493;&#30340;PaLM-540B&#65292;&#20197;&#21450;&#27604;&#20854;&#22823;200&#20493;&#30340;FeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state of the art encoder-decoder model using three techniques. First, we use a two-phase pre-training process to improve model's performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, and then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ creates new state of the art on 9 out of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM-540B on XSum, and the finetuned 200x l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#21333;&#35789;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979; troll &#25512;&#25991;&#65292;&#32467;&#26524;&#34920;&#26126;&#37319;&#29992;ELMo&#21644;BERT&#23884;&#20837;&#26041;&#27861;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#26368;&#20339;&#34920;&#29616;&#26041;&#27861;&#20026;&#22522;&#20110;ELMo&#30340;&#26550;&#26500;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;GRU&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;0.929&#30340;AUC&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2207.08230</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#21333;&#35789;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#24694;&#24847;&#25512;&#25991;
&lt;/p&gt;
&lt;p&gt;
A Context-Sensitive Word Embedding Approach for The Detection of Troll Tweets. (arXiv:2207.08230v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#21333;&#35789;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979; troll &#25512;&#25991;&#65292;&#32467;&#26524;&#34920;&#26126;&#37319;&#29992;ELMo&#21644;BERT&#23884;&#20837;&#26041;&#27861;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#26368;&#20339;&#34920;&#29616;&#26041;&#27861;&#20026;&#22522;&#20110;ELMo&#30340;&#26550;&#26500;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;GRU&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;0.929&#30340;AUC&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#21644;&#35780;&#20272;&#19968;&#32452;&#27169;&#22411;&#26550;&#26500;&#26469;&#33258;&#21160;&#26816;&#27979; troll &#25512;&#25991;&#65292;&#20174;&#32780;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#28363;&#25200;&#34892;&#20026;&#26085;&#36235;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#21333;&#35789;&#23884;&#20837;&#26041;&#27861;&#65292;&#22914;BERT&#12289;ELMo&#21644;GloVe&#65292;&#20351;&#29992;&#20998;&#31867;&#20934;&#30830;&#24230;&#12289;F1&#24471;&#20998;&#12289;AUC&#21644;&#31934;&#30830;&#24230;&#31561;&#25351;&#26631;&#35780;&#20272;&#20102;&#27599;&#20010;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;BERT&#21644;ELMo&#23884;&#20837;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;GloVe&#26041;&#27861;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#22320;&#25429;&#25417;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#20351;&#29992;&#32454;&#24494;&#24046;&#21035;&#30340;&#19978;&#19979;&#25991;&#21270;&#21333;&#35789;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;CNN&#21644;GRU&#32534;&#30721;&#22120;&#22312;F1&#20998;&#25968;&#21644;AUC&#26041;&#38754;&#34920;&#29616;&#30456;&#20284;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;&#26368;&#20339;&#34920;&#29616;&#26041;&#27861;&#26159;&#22522;&#20110;ELMo&#30340;&#26550;&#26500;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;GRU&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;0.929&#30340;AUC&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aimed to address the growing concern of trolling behavior on social media by developing and evaluating a set of model architectures for the automatic detection of troll tweets. Utilizing deep learning techniques and pre-trained word embedding methods such as BERT, ELMo, and GloVe, we evaluated the performance of each architecture using metrics such as classification accuracy, F1 score, AUC, and precision. Our results indicate that BERT and ELMo embedding methods performed better than the GloVe method, likely due to their ability to provide contextualized word embeddings that better capture the nuances and subtleties of language use in online social media. Additionally, we found that CNN and GRU encoders performed similarly in terms of F1 score and AUC, suggesting their effectiveness in extracting relevant information from input text. The best-performing method was found to be an ELMo-based architecture that employed a GRU classifier, with an AUC score of 0.929. This r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EEPT&#30340;&#22312;&#32447;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35821;&#20041;&#30456;&#20284;&#24230;&#22312;Twitter&#19978;&#21457;&#29616;&#26032;&#20852;&#23454;&#20307;&#12290;&#36890;&#36807;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#34920;&#26126;EEPT&#26159;&#26377;&#21069;&#36884;&#30340;&#24182;&#33021;&#22815;&#22312;&#23454;&#20307;&#24314;&#31435;&#20043;&#21069;&#21457;&#29616;&#37325;&#35201;&#30340;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2207.02434</link><description>&lt;p&gt;
&#26089;&#26399;&#21033;&#29992;&#35821;&#20041;&#30456;&#20284;&#24230;&#22312;&#27874;&#26031;&#35821;Twitter&#19978;&#21457;&#29616;&#26032;&#20852;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Early Discovery of Emerging Entities in Persian Twitter with Semantic Similarity. (arXiv:2207.02434v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EEPT&#30340;&#22312;&#32447;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35821;&#20041;&#30456;&#20284;&#24230;&#22312;Twitter&#19978;&#21457;&#29616;&#26032;&#20852;&#23454;&#20307;&#12290;&#36890;&#36807;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#34920;&#26126;EEPT&#26159;&#26377;&#21069;&#36884;&#30340;&#24182;&#33021;&#22815;&#22312;&#23454;&#20307;&#24314;&#31435;&#20043;&#21069;&#21457;&#29616;&#37325;&#35201;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#26032;&#20852;&#23454;&#20307;&#65288;EEs&#65289;&#26159;&#25351;&#22312;&#23427;&#20204;&#34987;&#35748;&#21487;&#20043;&#21069;&#23601;&#25214;&#21040;&#23427;&#20204;&#30340;&#36807;&#31243;&#12290;&#36825;&#20123;&#23454;&#20307;&#23545;&#20010;&#20154;&#12289;&#20844;&#21496;&#21644;&#25919;&#24220;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#35768;&#22810;&#23454;&#20307;&#21487;&#20197;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#21457;&#29616;&#65292;&#20363;&#22914;Twitter&#12290;&#36817;&#24180;&#26469;&#65292;&#36825;&#20123;&#23454;&#20307;&#24050;&#25104;&#20026;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#19982;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#19968;&#26679;&#65292;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#26159;&#36825;&#19968;&#38382;&#39064;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EEPT&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32447;&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;EEs&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#34913;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EEPT&#26159;&#26377;&#21069;&#36884;&#30340;&#65292;&#24182;&#33021;&#22312;&#23454;&#20307;&#24314;&#31435;&#20043;&#21069;&#21457;&#29616;&#37325;&#35201;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering emerging entities (EEs) is the problem of finding entities before their establishment. These entities can be critical for individuals, companies, and governments. Many of these entities can be discovered on social media platforms, e.g. Twitter. These identities have been the spot of research in academia and industry in recent years. Similar to any machine learning problem, data availability is one of the major challenges in this problem. This paper proposes EEPT. That is an online clustering method able to discover EEs without any need for training on a dataset. Additionally, due to the lack of a proper evaluation metric, this paper uses a new metric to evaluate the results. The results show that EEPT is promising and finds significant entities before their establishment.
&lt;/p&gt;</description></item></channel></rss>