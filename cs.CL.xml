<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#32763;&#35793;&#22270;&#20687;&#20197;&#20351;&#20854;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#65292;&#26500;&#24314;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.01247</link><description>&lt;p&gt;
&#22270;&#29255;&#34429;&#28982;&#20195;&#34920;&#21315;&#35328;&#19975;&#35821;&#65292;&#20294;&#27599;&#20010;&#20154;&#37117;&#33021;&#21548;&#25026;&#21527;&#65311;&#20851;&#20110;&#32763;&#35793;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01247
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#32763;&#35793;&#22270;&#20687;&#20197;&#20351;&#20854;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#65292;&#26500;&#24314;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#20852;&#36215;&#65292;&#20154;&#31867;&#32763;&#35793;&#36234;&#26469;&#36234;&#22810;&#22320;&#20851;&#27880;&#20110;&#25991;&#21270;&#36866;&#24212;&#65292;&#19981;&#20165;&#38480;&#20110;&#25991;&#23383;&#65292;&#36824;&#21253;&#25324;&#22270;&#29255;&#31561;&#20854;&#20182;&#24418;&#24335;&#65292;&#20197;&#20256;&#36798;&#30456;&#21516;&#30340;&#21547;&#20041;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#24212;&#29992;&#23558;&#21463;&#30410;&#20110;&#36825;&#19968;&#28857;&#65292;&#20294;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20173;&#28982;&#23616;&#38480;&#20110;&#22788;&#29702;&#35821;&#35328;&#30340;&#21475;&#22836;&#21644;&#25991;&#23383;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#32763;&#35793;&#22270;&#20687;&#20197;&#20351;&#20854;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#31532;&#19968;&#27493;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#20010;&#21253;&#21547;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#27969;&#27700;&#32447;&#26469;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65306;i&#65289;&#27010;&#24565;&#65306;&#21253;&#25324;600&#24352;&#36328;&#25991;&#21270;&#36830;&#36143;&#30340;&#22270;&#20687;&#65292;&#27599;&#24352;&#22270;&#20687;&#19987;&#27880;&#20110;&#21333;&#20010;&#27010;&#24565;&#65292;ii&#65289;&#24212;&#29992;&#65306;&#21253;&#25324;&#20174;&#23454;&#38469;&#24212;&#29992;&#20013;&#31579;&#36873;&#20986;&#30340;100&#24352;&#22270;&#20687;&#12290;&#25105;&#20204;&#23545;&#32763;&#35793;&#21518;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#25991;&#21270;&#30456;&#20851;&#24615;&#21644;&#21547;&#20041;&#20445;&#30041;&#12290;&#25105;&#20204;&#21457;&#29616;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#22833;&#36133;&#20102;&#65292;&#20294;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01247v1 Announce Type: new  Abstract: Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15412</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#27979;&#37327;&#21644;&#24314;&#27169;&#8220;&#25991;&#21270;&#8221;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring and Modeling "Culture" in LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15412
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21576;&#29616;&#20102;&#23545;39&#31687;&#26368;&#26032;&#35770;&#25991;&#30340;&#35843;&#26597;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27809;&#26377;&#19968;&#31687;&#30740;&#31350;&#23450;&#20041;&#8220;&#25991;&#21270;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#12289;&#22810;&#23618;&#38754;&#30340;&#27010;&#24565;&#65307;&#30456;&#21453;&#65292;&#23427;&#20204;&#22312;&#19968;&#20123;&#29305;&#21035;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20195;&#34920;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#38754;&#31216;&#20026;&#25991;&#21270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#22312;&#20154;&#21475;&#32479;&#35745;&#12289;&#35821;&#20041;&#21644;&#35821;&#35328;&#25991;&#21270;&#20132;&#20114;&#20195;&#29702;&#30340;&#19977;&#20010;&#32500;&#24230;&#19978;&#12290;&#25105;&#20204;&#36824;&#23545;&#37319;&#29992;&#30340;&#25506;&#26597;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21482;&#26377;&#8220;&#25991;&#21270;&#8221;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#22914;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#65292;&#34987;&#30740;&#31350;&#20102;&#65292;&#30041;&#19979;&#20102;&#20960;&#20010;&#20854;&#20182;&#26377;&#36259;&#19988;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22823;&#37327;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#65288;Hershcovich&#31561;&#20154;&#65292;2022&#65289;&#30340;&#26410;&#34987;&#25506;&#31350;&#12290;&#21478;&#22806;&#20004;&#20010;&#20851;&#38190;&#30340;&#31354;&#30333;&#26159;&#30446;&#21069;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#24773;&#22659;&#24615;&#30340;&#32570;&#20047;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15412v1 Announce Type: cross  Abstract: We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define "culture," which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture." We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of "culture," such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22686;&#24378;&#22411;&#33258;&#20027;&#20195;&#29702;&#22312;&#21512;&#20316;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#36825;&#20123;&#20195;&#29702;&#26174;&#31034;&#20986;&#21512;&#20316;&#20542;&#21521;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#21512;&#20316;&#22256;&#38590;&#65292;&#24378;&#35843;&#20102;&#23545;&#26356;&#20581;&#22766;&#26550;&#26500;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.11381</link><description>&lt;p&gt;
LLM&#22686;&#24378;&#22411;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#21512;&#20316;&#21527;&#65311;&#36890;&#36807;&#34701;&#21512;&#30406;&#35780;&#20272;&#23427;&#20204;&#30340;&#21512;&#20316;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22686;&#24378;&#22411;&#33258;&#20027;&#20195;&#29702;&#22312;&#21512;&#20316;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#36825;&#20123;&#20195;&#29702;&#26174;&#31034;&#20986;&#21512;&#20316;&#20542;&#21521;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#21512;&#20316;&#22256;&#38590;&#65292;&#24378;&#35843;&#20102;&#23545;&#26356;&#20581;&#22766;&#26550;&#26500;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#32500;&#24230;&#26159;&#21457;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#28508;&#21147;&#22686;&#24378;&#22810;&#20195;&#29702;&#20154;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#33879;&#21517;&#30340;Meltin Pot&#29615;&#22659;&#20197;&#21450;&#21442;&#32771;&#27169;&#22411;&#22914;GPT4&#21644;GPT3.5&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#33258;&#20027;&#20195;&#29702;(LAAs)&#30340;&#21512;&#20316;&#33021;&#21147;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#20195;&#29702;&#20154;&#34920;&#29616;&#20986;&#21512;&#20316;&#30340;&#20542;&#21521;&#65292;&#20294;&#22312;&#29305;&#23450;&#29615;&#22659;&#20013;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#21327;&#20316;&#65292;&#24378;&#35843;&#20102;&#26356;&#24378;&#22823;&#26550;&#26500;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#36866;&#24212;LLM&#30340;Melting Pot&#28216;&#25103;&#22330;&#26223;&#30340;&#25277;&#35937;&#21270;&#23618;&#65292;&#19968;&#20010;&#29992;&#20110;LLM&#20013;&#20171;&#20195;&#29702;&#24320;&#21457;&#30340;&#21487;&#37325;&#29992;&#26550;&#26500;-&#21253;&#25324;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#20197;&#21450;&#19981;&#21516;&#30340;&#35748;&#30693;&#27169;&#22359;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#26041;&#27861;&#35780;&#20272;&#21512;&#20316;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11381v1 Announce Type: new  Abstract: As the field of AI continues to evolve, a significant dimension of this progression is the development of Large Language Models and their potential to enhance multi-agent artificial intelligence systems. This paper explores the cooperative capabilities of Large Language Model-augmented Autonomous Agents (LAAs) using the well-known Meltin Pot environments along with reference models such as GPT4 and GPT3.5. Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures. The study's contributions include an abstraction layer to adapt Melting Pot game scenarios for LLMs, the implementation of a reusable architecture for LLM-mediated agent development - which includes short and long-term memories and different cognitive modules, and the evaluation of cooperation capabilities using a set of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#26412;&#22320;&#35821;&#35328;&#25552;&#31034;&#26469;&#35299;&#20915;&#25991;&#21270;&#30456;&#20851;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#21628;&#21505;&#21457;&#23637;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;LLMs&#12290;</title><link>https://arxiv.org/abs/2403.10258</link><description>&lt;p&gt;
&#32763;&#35793;&#21040;&#24213;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#21527;&#65311;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10258
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#26412;&#22320;&#35821;&#35328;&#25552;&#31034;&#26469;&#35299;&#20915;&#25991;&#21270;&#30456;&#20851;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#21628;&#21505;&#21457;&#23637;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#35821;&#26009;&#24211;&#19981;&#24179;&#34913;&#65292;&#23427;&#20204;&#22823;&#22810;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#12290;&#29616;&#26377;&#30740;&#31350;&#21033;&#29992;&#36825;&#19968;&#29616;&#35937;&#26469;&#25552;&#39640;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#22810;&#35821;&#35328;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#25193;&#23637;&#21040;&#30495;&#23454;&#29992;&#25143;&#26597;&#35810;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#23558;&#25991;&#26412;&#32763;&#35793;&#25104;&#33521;&#35821;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#22312;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#25152;&#26377;&#22330;&#26223;&#12290;&#23545;&#20110;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#35821;&#35328;&#30340;&#25991;&#21270;&#30456;&#20851;&#20219;&#21153;&#65292;&#20197;&#26412;&#22320;&#35821;&#35328;&#25552;&#31034;&#26356;&#20026;&#26377;&#21069;&#26223;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25429;&#25417;&#19982;&#25991;&#21270;&#21644;&#35821;&#35328;&#30456;&#20851;&#30340;&#24494;&#22937;&#20043;&#22788;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#30528;&#21147;&#21457;&#23637;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;LLMs&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10258v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated strong multilingual capabilities; yet, they are mostly English-centric due to the imbalanced training corpora. Existing works leverage this phenomenon to improve their multilingual performances on NLP tasks. In this work, we extend the evaluation from NLP tasks to real user queries. We find that even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios. For culture-related tasks that need deep language understanding, prompting in the native language proves to be more promising since it can capture the nuances related to culture and language. Therefore, we advocate for more efforts towards the development of strong multilingual LLMs instead of just English-centric LLMs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Debatrix&#31995;&#32479;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#22810;&#36718;&#36777;&#35770;&#30340;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#30452;&#25509;&#20351;&#29992;LLMs&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#22330;&#36777;&#35770;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.08010</link><description>&lt;p&gt;
Debatrix:&#22522;&#20110;LLM&#30340;&#22810;&#32500;&#36777;&#35770;&#35780;&#21028;&#31995;&#32479;&#19982;&#36845;&#20195;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08010
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Debatrix&#31995;&#32479;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#22810;&#36718;&#36777;&#35770;&#30340;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#30452;&#25509;&#20351;&#29992;LLMs&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#22330;&#36777;&#35770;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#26500;&#24314;&#19968;&#20010;&#33258;&#21160;&#21270;&#36777;&#35770;&#35780;&#21028;&#31995;&#32479;&#26469;&#35780;&#20272;&#19968;&#22330;&#24191;&#27867;&#12289;&#20805;&#28385;&#27963;&#21147;&#30340;&#22810;&#36718;&#36777;&#35770;&#65311;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35780;&#21028;&#36777;&#35770;&#28041;&#21450;&#22788;&#29702;&#20887;&#38271;&#25991;&#26412;&#12289;&#22797;&#26434;&#30340;&#35770;&#28857;&#20851;&#31995;&#21644;&#22810;&#32500;&#24230;&#35780;&#20272;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30701;&#23545;&#35805;&#65292;&#24456;&#23569;&#28041;&#21450;&#23545;&#25972;&#22330;&#36777;&#35770;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Debatrix&#65292;&#20351;&#24471;&#22810;&#36718;&#36777;&#35770;&#30340;&#20998;&#26512;&#21644;&#35780;&#20272;&#26356;&#31526;&#21512;&#22823;&#22810;&#25968;&#20154;&#30340;&#20559;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Debatrix&#20855;&#26377;&#22402;&#30452;&#30340;&#12289;&#36845;&#20195;&#24335;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#27700;&#24179;&#30340;&#12289;&#22810;&#32500;&#24230;&#30340;&#35780;&#20272;&#21327;&#20316;&#12290;&#20026;&#20102;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#36777;&#35770;&#22330;&#26223;&#20445;&#25345;&#19968;&#33268;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PanelBench&#22522;&#20934;&#65292;&#23558;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#19982;&#23454;&#38469;&#36777;&#35770;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#30452;&#25509;&#20351;&#29992;LLMs&#36827;&#34892;&#36777;&#35770;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08010v1 Announce Type: new  Abstract: How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments. At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate. In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration. To align with real-world debate scenarios, we introduced the PanelBench benchmark, comparing our system's performance to actual debate outcomes. The findings indicate a notable enhancement over directly using LLMs for debate evaluation. Source code
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39034;&#24207;&#25351;&#20196;&#24494;&#35843;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#25191;&#34892;&#22810;&#20010;&#39034;&#24207;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#20248;&#20110;&#20256;&#32479;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.07794</link><description>&lt;p&gt;
&#20351;&#29992;&#39034;&#24207;&#25351;&#20196;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models with Sequential Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07794
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39034;&#24207;&#25351;&#20196;&#24494;&#35843;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#25191;&#34892;&#22810;&#20010;&#39034;&#24207;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#20248;&#20110;&#20256;&#32479;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21333;&#20010;&#26597;&#35810;&#20013;&#36981;&#24490;&#19968;&#31995;&#21015;&#25351;&#20196;&#26102;&#24448;&#24448;&#20250;&#24573;&#30053;&#25110;&#35823;&#35299;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#65292;&#36825;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#38656;&#35201;&#22810;&#20010;&#20013;&#38388;&#27493;&#39588;&#30340;&#22797;&#26434;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#22810;&#35821;&#35328;&#65288;&#20808;&#32763;&#35793;&#20877;&#22238;&#31572;&#65289;&#21644;&#22810;&#27169;&#24577;&#65288;&#26631;&#39064;&#21518;&#22238;&#31572;&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#28304;LLMs&#65288;&#22914;LLaMA-2 70B&#21644;Mixtral-8x7B&#65289;&#30340;&#23454;&#35777;&#39564;&#35777;&#20102;&#36825;&#19968;&#28857;&#12290;&#38024;&#23545;&#24403;&#21069;&#25968;&#25454;&#20013;&#39034;&#24207;&#25351;&#20196;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39034;&#24207;&#25351;&#20196;&#24494;&#35843;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#33258;&#21160;&#22686;&#21152;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#65292;&#20351;LLMs&#20855;&#22791;&#25191;&#34892;&#22810;&#20010;&#39034;&#24207;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#22312;&#25506;&#32034;&#29616;&#26377;&#25968;&#25454;&#38598;&#65288;&#22914;Alpaca&#65289;&#20013;&#25554;&#20837;&#25351;&#20196;&#24182;&#36827;&#34892;&#19968;&#31995;&#21015;&#20013;&#38388;&#20219;&#21153;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#39034;&#24207;&#25351;&#20196;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20256;&#32479;&#30340;&#25351;&#20196;&#24494;&#35843;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07794v1 Announce Type: new  Abstract: Large language models (LLMs) struggle to follow a sequence of instructions in a single query as they may ignore or misinterpret part of it. This impairs their performance in complex problems whose solution requires multiple intermediate steps, such as multilingual (translate then answer) and multimodal (caption then answer) tasks. We empirically verify this with open-source LLMs as large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential instructions in present-day data, we propose sequential instruction tuning, a simple yet effective strategy to automatically augment instruction tuning data and equip LLMs with the ability to execute multiple sequential instructions. After exploring interleaving instructions in existing datasets, such as Alpaca, with a wide range of intermediate tasks, we find that sequential instruction-tuned models consistently outperform the conventional instruction-tuned baselines in downstream tas
&lt;/p&gt;</description></item><item><title>StableToolBench&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#65292;&#36890;&#36807;&#32531;&#23384;&#31995;&#32479;&#12289;API&#27169;&#25311;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31283;&#23450;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07714</link><description>&lt;p&gt;
StableToolBench&#65306;&#38754;&#21521;&#22823;&#35268;&#27169;&#31283;&#23450;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20855;&#23398;&#20064;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07714
&lt;/p&gt;
&lt;p&gt;
StableToolBench&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#65292;&#36890;&#36807;&#32531;&#23384;&#31995;&#32479;&#12289;API&#27169;&#25311;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31283;&#23450;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#20351;&#20154;&#20204;&#25506;&#32034;&#24037;&#20855;&#23398;&#20064;&#65292;&#23558;LLMs&#19982;&#22806;&#37096;&#24037;&#20855;&#25972;&#21512;&#20197;&#35299;&#20915;&#21508;&#31181;&#29616;&#23454;&#25361;&#25112;&#12290;&#35780;&#20272;LLMs&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#38656;&#35201;&#22823;&#35268;&#27169;&#19988;&#31283;&#23450;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;ToolBench&#28436;&#21464;&#32780;&#26469;&#30340;StableToolBench&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#12290;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21253;&#21547;&#32531;&#23384;&#31995;&#32479;&#21644;API&#27169;&#25311;&#22120;&#65292;&#20114;&#34917;&#20943;&#36731;API&#29366;&#24577;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#31283;&#23450;&#30340;&#35780;&#20272;&#31995;&#32479;&#20351;&#29992;GPT-4&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#35774;&#35745;&#21487;&#35299;&#20915;&#30340;&#36890;&#36807;&#29575;&#21644;&#32988;&#29575;&#65292;&#20197;&#28040;&#38500;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07714v1 Announce Type: new  Abstract: Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#24555;&#36895;&#36827;&#34892;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#30041;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2403.07088</link><description>&lt;p&gt;
SPA&#65306;&#38754;&#21521;&#20113;&#31471;&#21644;&#35774;&#22791;&#21327;&#20316;&#30340;&#35745;&#31639;&#21451;&#22909;&#22411;Seq2seq&#20010;&#24615;&#21270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07088
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#24555;&#36895;&#36827;&#34892;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#30041;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34920;&#29616;&#20986;&#33394;&#30340;&#33021;&#21147;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#38382;&#31572;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;&#28982;&#32780;&#65292;LLMs&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#22823;&#20869;&#23384;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24403;&#35757;&#32451;&#25110;&#39044;&#27979;&#36807;&#31243;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#26102;&#65292;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#24555;&#36895;&#35774;&#22791;&#19978;&#30340;&#25512;&#26029;&#21644;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#20445;&#25345;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07088v1 Announce Type: new  Abstract: Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.05750</link><description>&lt;p&gt;
&#35299;&#35835;AI&#31508;: &#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#25216;&#26415;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05750
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#24443;&#24213;&#39072;&#35206;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24191;&#27867;&#30340;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#65292;&#38656;&#35201;&#28145;&#20837;&#23457;&#26597;&#12289;&#20262;&#29702;&#23457;&#26597;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#25506;&#32034;&#20102;&#29616;&#26377;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#37325;&#28857;&#26159;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#20316;&#20026;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#35780;&#20272;&#20102;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#39046;&#22495;&#38480;&#21046;&#30340;&#26032;&#39062;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#39318;&#25209;&#24320;&#25918;&#35775;&#38382;&#30340;&#21453;&#32534;&#35793;LLM&#65292;&#39044;&#35757;&#32451;&#22312;40&#20159;&#20010;C&#28304;&#20195;&#30721;&#21644;&#27719;&#32534;&#20195;&#30721;&#26631;&#35760;&#19978;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#32771;&#34385;&#37325;&#26032;&#32534;&#35793;&#24615;&#21644;&#37325;&#26032;&#25191;&#34892;&#24615;&#30340;&#21453;&#32534;&#35793;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.05286</link><description>&lt;p&gt;
LLM4Decompile&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20108;&#36827;&#21046;&#20195;&#30721;&#36827;&#34892;&#21453;&#32534;&#35793;
&lt;/p&gt;
&lt;p&gt;
LLM4Decompile: Decompiling Binary Code with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05286
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#39318;&#25209;&#24320;&#25918;&#35775;&#38382;&#30340;&#21453;&#32534;&#35793;LLM&#65292;&#39044;&#35757;&#32451;&#22312;40&#20159;&#20010;C&#28304;&#20195;&#30721;&#21644;&#27719;&#32534;&#20195;&#30721;&#26631;&#35760;&#19978;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#32771;&#34385;&#37325;&#26032;&#32534;&#35793;&#24615;&#21644;&#37325;&#26032;&#25191;&#34892;&#24615;&#30340;&#21453;&#32534;&#35793;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#32534;&#35793;&#26088;&#22312;&#23558;&#32534;&#35793;&#20195;&#30721;&#24674;&#22797;&#20026;&#21487;&#35835;&#24615;&#24378;&#30340;&#28304;&#20195;&#30721;&#65292;&#20294;&#22312;&#21517;&#31216;&#21644;&#32467;&#26500;&#31561;&#32454;&#33410;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32534;&#31243;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#28608;&#21457;&#20102;&#23427;&#20204;&#22312;&#21453;&#32534;&#35793;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26080;&#29992;&#20110;&#21453;&#32534;&#35793;&#30340;&#24320;&#28304;LLM&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#21453;&#32534;&#35793;&#35780;&#20272;&#31995;&#32479;&#20027;&#35201;&#32771;&#34385;&#26631;&#35760;&#32423;&#20934;&#30830;&#24615;&#65292;&#32780;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#20195;&#30721;&#30340;&#21487;&#25191;&#34892;&#24615;&#65292;&#36825;&#26159;&#20219;&#20309;&#31243;&#24207;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#39318;&#25209;&#24320;&#25918;&#35775;&#38382;&#30340;&#21453;&#32534;&#35793;LLM&#65292;&#33539;&#22260;&#20174;10&#20159;&#21040;330&#20159;&#65292;&#39044;&#20808;&#35757;&#32451;&#20102;40&#20159;&#20010;&#20196;&#29260;&#30340;C&#28304;&#20195;&#30721;&#21644;&#30456;&#24212;&#30340;&#27719;&#32534;&#20195;&#30721;&#12290;&#36825;&#20123;&#24320;&#28304;LLM&#21487;&#20197;&#20316;&#20026;&#35813;&#39046;&#22495;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#22522;&#32447;&#12290;&#20026;&#20102;&#30830;&#20445;&#23454;&#38469;&#31243;&#24207;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Decompile-Eval&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#37325;&#26032;&#32534;&#35793;&#24615;&#21644;&#37325;&#26032;&#25191;&#34892;&#24615;&#30340;&#21453;&#32534;&#35793;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#24378;&#35843;&#20102;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05286v1 Announce Type: cross  Abstract: Decompilation aims to restore compiled code to human-readable source code, but struggles with details like names and structure. Large language models (LLMs) show promise for programming tasks, motivating their application to decompilation. However, there does not exist any open-source LLM for decompilation. Moreover, existing decompilation evaluation systems mainly consider token-level accuracy and largely ignore code executability, which is the most important feature of any program. Therefore, we release the first open-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion tokens of C source code and the corresponding assembly code. The open-source LLMs can serve as baselines for further development in the field. To ensure practical program evaluation, we introduce Decompile-Eval, the first dataset that considers re-compilability and re-executability for decompilation. The benchmark emphasizes the importance of eval
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02966</link><description>&lt;p&gt;
&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#29992;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;-shot&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#24615;&#33021;&#65292;&#28982;&#32780;&#32467;&#26500;&#21270;&#30340;KG&#24418;&#24335;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#19977;&#20803;&#32452;&#24418;&#24335;&#25110;&#19977;&#20803;&#32452;&#20107;&#23454;&#30340;&#33258;&#30001;&#25991;&#26412;&#36716;&#25442;&#65292;&#36935;&#21040;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#30001;&#20110;&#37325;&#22797;&#23454;&#20307;&#25110;&#20851;&#31995;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#23494;&#24230;&#38477;&#20302;&#65292;&#20197;&#21450;&#30001;&#20110;&#26080;&#27861;&#24378;&#35843;&#20851;&#38190;&#35777;&#25454;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#28165;&#26224;&#24230;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EFSum&#65292;&#19968;&#20010;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;LLMs&#22686;&#24378;QA&#12290;&#25105;&#20204;&#36890;&#36807;&#33976;&#39311;&#21644;&#20559;&#22909;&#23545;&#40784;&#26469;&#20248;&#21270;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#20316;&#20026;&#20107;&#23454;&#25688;&#35201;&#22120;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;EFSum&#25552;&#39640;&#20102;LLM&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#30830;&#20445;&#25688;&#35201;&#30340;&#21516;&#26102;&#26377;&#30410;&#21644;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00863</link><description>&lt;p&gt;
LLM-Ensemble: &#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#26368;&#20339;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00863
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00863v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495;&#25688;&#35201;: &#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#24403;&#20195;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#25552;&#20379;&#31934;&#30830;&#30340;&#20135;&#21697;&#23646;&#24615;&#20540;&#22312;&#30830;&#20445;&#39640;&#36136;&#37327;&#25512;&#33616;&#21644;&#25552;&#21319;&#23458;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#23646;&#24615;&#25552;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#12289;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#65292;&#19981;&#21516;LLMs&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#36825;&#31181;&#21464;&#21270;&#20351;&#23427;&#20204;&#24444;&#27492;&#20114;&#34917;&#65292;&#27809;&#26377;&#21738;&#20010;LLM&#33021;&#23436;&#20840;&#21387;&#20498;&#20854;&#20182;LLM&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#22810;&#26679;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#24320;&#21457;&#19968;&#31181;&#21033;&#29992;&#23427;&#20204;&#20114;&#34917;&#28508;&#21147;&#30340;&#38598;&#25104;&#26041;&#27861;&#21464;&#24471;&#24517;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00863v1 Announce Type: cross  Abstract: Product attribute value extraction is a pivotal component in Natural Language Processing (NLP) and the contemporary e-commerce industry. The provision of precise product attribute values is fundamental in ensuring high-quality recommendations and enhancing customer satisfaction. The recently emerging Large Language Models (LLMs) have demonstrated state-of-the-art performance in numerous attribute extraction tasks, without the need for domain-specific training data. Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters. This variation makes them complementary to each other, with no single LLM dominating all others. Considering the diverse strengths and weaknesses of LLMs, it becomes necessary to develop an ensemble method that leverages their complementary potentials. In this paper, we propose a novel algorithm called LLM-ensemble to ensemble diffe
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;LLMs&#22312;&#25512;&#29702;&#21644;&#27807;&#36890;&#20013;&#20351;&#29992;&#26367;&#20195;&#26684;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26102;&#26631;&#35760;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.18439</link><description>&lt;p&gt;
&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#65306;LLM&#21033;&#29992;&#26367;&#20195;&#26684;&#24335;&#36827;&#34892;&#22686;&#24378;&#25512;&#29702;&#21644;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18439
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;LLMs&#22312;&#25512;&#29702;&#21644;&#27807;&#36890;&#20013;&#20351;&#29992;&#26367;&#20195;&#26684;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26102;&#26631;&#35760;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#27807;&#36890;&#30340;&#20027;&#35201;&#26684;&#24335;&#65292;&#22240;&#27492;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#20013;&#21516;&#26679;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;NL&#20043;&#22806;&#65292;LLMs&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30475;&#21040;&#20102;&#21508;&#31181;&#38750;NL&#26684;&#24335;&#65292;&#22914;&#20195;&#30721;&#21644;&#36923;&#36753;&#34920;&#36798;&#24335;&#12290;NL&#20316;&#20026;LLMs&#30340;&#26368;&#20339;&#26684;&#24335;&#65292;&#22312;&#21333;&#19968;LLM&#25512;&#29702;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#20013;&#30340;&#22320;&#20301;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#23457;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;NL&#65292;&#36890;&#36807;&#25506;&#32034;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#38750;NL&#26684;&#24335;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#20801;&#35768;LLMs&#22312;&#25512;&#29702;&#25110;&#27807;&#36890;&#20043;&#21069;&#33258;&#20027;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26684;&#24335;&#65292;&#21487;&#23548;&#33268;&#19981;&#21516;LLMs&#25512;&#29702;&#25928;&#29575;&#25552;&#39640;3.3&#33267;5.7&#65285;&#65292;&#24182;&#19988;&#22312;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#20013;&#26368;&#22810;&#21487;&#20943;&#23569;72.7&#65285;&#30340;&#26631;&#35760;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#27807;&#36890;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;LLMs&#21487;&#20197;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18439v1 Announce Type: cross  Abstract: Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs c
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#24615;&#21035;&#24046;&#36317;&#65292;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#21463;&#30410;&#30340;&#24615;&#21035;&#32676;&#20307;&#21508;&#19981;&#30456;&#21516;&#12290;</title><link>https://arxiv.org/abs/2402.17954</link><description>&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17954
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#24615;&#21035;&#24046;&#36317;&#65292;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#21463;&#30410;&#30340;&#24615;&#21035;&#32676;&#20307;&#21508;&#19981;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#20351;&#29992;&#22810;&#20219;&#21153;&#12289;&#22810;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35832;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#35821;&#38899;&#20219;&#21153;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#35768;&#22810;&#35821;&#35328;&#32780;&#19981;&#38656;&#35201;&#23454;&#36136;&#24615;&#30340;&#26356;&#25913;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#30340;&#35821;&#35328;&#35206;&#30422;&#20173;&#28982;&#21487;&#33021;&#25513;&#30422;&#35821;&#35328;&#20869;&#37096;&#23384;&#22312;&#30340;&#24615;&#21035;&#24046;&#36317;&#12290;&#26412;&#25991;&#31995;&#32479;&#35780;&#20272;&#22810;&#35821;&#35328;ASR&#31995;&#32479;&#22312;&#24615;&#21035;&#34920;&#29616;&#24046;&#36317;&#19978;&#30340;&#24773;&#20917;&#12290;&#22312;19&#31181;&#35821;&#35328;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#20004;&#31181;&#27969;&#34892;&#27169;&#22411;&#65292;&#36328;&#36234;&#19971;&#20010;&#35821;&#35328;&#23478;&#26063;&#65292;&#25105;&#20204;&#21457;&#29616;&#26126;&#26174;&#30340;&#24615;&#21035;&#24046;&#24322;&#12290;&#19981;&#36807;&#65292;&#19981;&#21516;&#35821;&#35328;&#20013;&#21463;&#30410;&#30340;&#32676;&#20307;&#21508;&#19981;&#30456;&#21516;&#12290;&#23613;&#31649;&#22312;&#35821;&#38899;&#23398;&#21464;&#37327;&#65288;&#38899;&#39640;&#12289;&#35828;&#35805;&#36895;&#24230;&#31561;&#65289;&#19978;&#21508;&#32676;&#20307;&#38388;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#20294;&#25506;&#32034;&#27169;&#22411;&#20869;&#37096;&#29366;&#24577;&#21364;&#25581;&#31034;&#20102;&#25506;&#26597;&#24615;&#33021;&#21644;&#24615;&#21035;&#34920;&#29616;&#24046;&#36317;&#20043;&#38388;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;&#21363;&#65292;&#22312;&#26576;&#31181;&#35821;&#35328;&#20013;&#26356;&#23481;&#26131;&#21306;&#20998;&#35828;&#35805;&#32773;&#24615;&#21035;&#65292;&#27169;&#22411;&#23601;&#26356;&#20559;&#21521;&#20110;&#22899;&#24615;&#35828;&#35805;&#32773;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#32452;&#21035;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17954v1 Announce Type: new  Abstract: Current voice recognition approaches use multi-task, multilingual models for speech tasks like Automatic Speech Recognition (ASR) to make them applicable to many languages without substantial changes. However, broad language coverage can still mask performance gaps within languages, for example, across genders. We systematically evaluate multilingual ASR systems on gendered performance gaps. Using two popular models on three datasets in 19 languages across seven language families, we find clear gender disparities. However, the advantaged group varies between languages. While there are no significant differences across groups in phonetic variables (pitch, speaking rate, etc.), probing the model's internal states reveals a negative correlation between probe performance and the gendered performance gap. I.e., the easier to distinguish speaker gender in a language, the more the models favor female speakers. Our results show that group dispar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#29983;&#25104;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#24341;&#20837;&#36127;&#20363;&#35757;&#32451;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#36127;&#20363;&#30340;&#24341;&#20837;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#28165;&#26224;&#21010;&#23450;&#26631;&#31614;&#36793;&#30028;&#26469;&#26174;&#33879;&#25913;&#36827;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Matching&#30340;&#26032;&#39062;&#39640;&#25928;&#31639;&#27861;&#65292;&#36827;&#19968;&#27493;&#23558;&#38750;&#32467;&#26500;&#21270;&#39044;&#27979;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.16602</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#29983;&#25104;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#36127;&#20363;
&lt;/p&gt;
&lt;p&gt;
Rethinking Negative Instances for Generative Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#29983;&#25104;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#24341;&#20837;&#36127;&#20363;&#35757;&#32451;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#36127;&#20363;&#30340;&#24341;&#20837;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#28165;&#26224;&#21010;&#23450;&#26631;&#31614;&#36793;&#30028;&#26469;&#26174;&#33879;&#25913;&#36827;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Matching&#30340;&#26032;&#39062;&#39640;&#25928;&#31639;&#27861;&#65292;&#36827;&#19968;&#27493;&#23558;&#38750;&#32467;&#26500;&#21270;&#39044;&#27979;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#26410;&#30693;&#20219;&#21153;&#20013;&#25797;&#38271;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#37319;&#29992;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#27169;&#24335;&#35843;&#25972;&#65292;LLMs&#30340;&#26174;&#33879;&#25913;&#36827;&#24050;&#32463;&#22312;&#24191;&#27867;&#30340;&#23454;&#20307;&#39046;&#22495;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#36890;&#36807;&#23558;&#36127;&#20363;&#32435;&#20837;&#35757;&#32451;&#26469;&#22686;&#24378;&#29616;&#26377;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#36127;&#20363;&#36890;&#36807;&#65288;1&#65289;&#24341;&#20837;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#65288;2&#65289;&#28165;&#26224;&#21010;&#23450;&#26631;&#31614;&#36793;&#30028;&#32780;&#23545;&#25913;&#36827;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Matching&#30340;&#26032;&#39062;&#39640;&#25928;&#31639;&#27861;&#65292;&#26088;&#22312;&#23558;&#38750;&#32467;&#26500;&#21270;&#39044;&#27979;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNER&#65292;&#19968;&#20010;&#29983;&#25104;&#24335;NER&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#36328;&#26410;&#30693;&#23454;&#20307;&#39046;&#22495;&#30340;&#25552;&#21319;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16602v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities for generalizing in unseen tasks. In the Named Entity Recognition (NER) task, recent advancements have seen the remarkable improvement of LLMs in a broad range of entity domains via instruction tuning, by adopting entity-centric schema. In this work, we explore the potential enhancement of the existing methods by incorporating negative instances into training. Our experiments reveal that negative instances contribute to remarkable improvements by (1) introducing contextual information, and (2) clearly delineating label boundaries. Furthermore, we introduce a novel and efficient algorithm named Hierarchical Matching, which is tailored to transform unstructured predictions into structured entities. By integrating these components, we present GNER, a Generative NER system that shows improved zero-shot performance across unseen entity domains. Our comprehensive evaluation
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#12289;&#35821;&#35328;&#21551;&#21457;&#12289;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22312;&#20851;&#31995;&#25277;&#21462;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.16159</link><description>&lt;p&gt;
DistALANER&#65306;&#24320;&#28304;&#36719;&#20214;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#36828;&#31243;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#22686;&#24378;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16159
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#12289;&#35821;&#35328;&#21551;&#21457;&#12289;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#21644;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22312;&#20851;&#31995;&#25277;&#21462;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#20840;&#38754;&#30340;&#20004;&#27493;&#36828;&#31243;&#30417;&#30563;&#27880;&#37322;&#36807;&#31243;&#26469;&#35299;&#20915;&#36719;&#20214;&#25968;&#25454;&#26631;&#27880;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#35813;&#36807;&#31243;&#24039;&#22937;&#22320;&#21033;&#29992;&#35821;&#35328;&#21551;&#21457;&#12289;&#29420;&#29305;&#30340;&#26597;&#25214;&#34920;&#12289;&#22806;&#37096;&#30693;&#35782;&#28304;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#25104;&#26412;&#21644;&#19987;&#23478;&#26631;&#27880;&#20154;&#21592;&#31232;&#32570;&#25152;&#24102;&#26469;&#30340;&#38480;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;LLMs&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;NER&#22312;&#20851;&#31995;&#25277;&#21462;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16159v1 Announce Type: new  Abstract: This paper proposes a novel named entity recognition (NER) technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly supervised annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an active learning approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our framework significantly outperforms the state-of-the-art LLMs by a substantial margin. We also show the effectiveness of NER in the downstream task of relation extraction.
&lt;/p&gt;</description></item><item><title>OAG-Bench&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#23398;&#26415;&#22270;&#30340;&#20840;&#38754;&#12289;&#22810;&#26041;&#38754;&#21644;&#31934;&#32454;&#21270;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;</title><link>https://arxiv.org/abs/2402.15810</link><description>&lt;p&gt;
OAG-Bench&#65306;&#38754;&#21521;&#23398;&#26415;&#22270;&#25366;&#25496;&#30340;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15810
&lt;/p&gt;
&lt;p&gt;
OAG-Bench&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#23398;&#26415;&#22270;&#30340;&#20840;&#38754;&#12289;&#22810;&#26041;&#38754;&#21644;&#31934;&#32454;&#21270;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31185;&#23398;&#25991;&#29486;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#22810;&#21151;&#33021;&#30340;&#23398;&#26415;&#30693;&#35782;&#26381;&#21153;&#36234;&#26469;&#36234;&#20381;&#36182;&#20840;&#38754;&#30340;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;&#23613;&#31649;&#20844;&#24320;&#23398;&#26415;&#22270;&#12289;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#24050;&#32463;&#26377;&#20102;&#65292;&#20294;&#36825;&#20123;&#36164;&#28304;&#36890;&#24120;&#22312;&#22810;&#26041;&#38754;&#21644;&#32454;&#31890;&#24230;&#27880;&#37322;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#21463;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#31867;&#22411;&#21644;&#39046;&#22495;&#65292;&#25110;&#32773;&#32570;&#20047;&#30495;&#23454;&#23398;&#26415;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24320;&#25918;&#23398;&#26415;&#22270;&#65288;OAG&#65289;&#30340;&#20840;&#38754;&#12289;&#22810;&#26041;&#38754;&#21644;&#31934;&#32454;&#21270;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;OAG-Bench&#12290;OAG-Bench&#28085;&#30422;&#20102;10&#20010;&#20219;&#21153;&#65292;20&#20010;&#25968;&#25454;&#38598;&#65292;70+&#20010;&#22522;&#20934;&#21644;120+&#20010;&#25130;&#33267;&#30446;&#21069;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#38024;&#23545;&#26576;&#20123;&#20219;&#21153;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#27880;&#37322;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#19968;&#22871;&#25968;&#25454;&#39044;&#22788;&#29702;&#20195;&#30721;&#12289;&#31639;&#27861;&#23454;&#29616;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#20419;&#36827;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36825;&#26679;&#30340;&#20808;&#36827;&#31639;&#27861;&#20063;&#20250;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#21463;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15810v1 Announce Type: cross  Abstract: With the rapid proliferation of scientific literature, versatile academic knowledge services increasingly rely on comprehensive academic graph mining. Despite the availability of public academic graphs, benchmarks, and datasets, these resources often fall short in multi-aspect and fine-grained annotations, are constrained to specific task types and domains, or lack underlying real academic graphs. In this paper, we present OAG-Bench, a comprehensive, multi-aspect, and fine-grained human-curated benchmark based on the Open Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines, and 120+ experimental results to date. We propose new data annotation strategies for certain tasks and offer a suite of data pre-processing codes, algorithm implementations, and standardized evaluation protocols to facilitate academic graph mining. Extensive experiments reveal that even advanced algorithms like large language models (LLMs) en
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15537</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Performance of ChatGPT for Spam Email Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#37038;&#20214;&#32487;&#32493;&#26159;&#19987;&#19994;&#21644;&#21830;&#19994;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#36890;&#20449;&#23186;&#20171;&#12290;&#28982;&#32780;&#65292;&#22403;&#22334;&#37038;&#20214;&#30340;&#26222;&#21450;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#25200;&#20081;&#20102;&#20182;&#20204;&#30340;&#26085;&#24120;&#24037;&#20316;&#24182;&#38477;&#20302;&#20102;&#29983;&#20135;&#29575;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20869;&#23481;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#36807;&#28388;&#22403;&#22334;&#37038;&#20214;&#23545;&#32593;&#32476;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#65292;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#35780;&#20272;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;ChatGPT&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#38656;&#35201;&#25552;&#31034;&#35828;&#26126;&#21644;&#23569;&#37327;&#31034;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15537v1 Announce Type: cross  Abstract: Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction and a few demonstrations. We also investigate how the t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#38376;&#22686;&#24378;&#23545;&#40784;&#26041;&#27861;&#26377;&#25928;&#32531;&#35299;&#24494;&#35843;&#36234;&#29425;&#25915;&#20987;&#65292;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#23433;&#20840;&#31034;&#20363;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14968</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#22686;&#24378;&#23545;&#40784;&#26469;&#32531;&#35299;&#24494;&#35843;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14968
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#38376;&#22686;&#24378;&#23545;&#40784;&#26041;&#27861;&#26377;&#25928;&#32531;&#35299;&#24494;&#35843;&#36234;&#29425;&#25915;&#20987;&#65292;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#23433;&#20840;&#31034;&#20363;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#21644;Llama-2&#20855;&#26377;&#19968;&#33324;&#33021;&#21147;&#65292;&#20294;&#22312;&#28385;&#36275;&#29305;&#23450;&#19994;&#21153;&#38656;&#27714;&#21644;&#23450;&#21046;&#29992;&#20363;&#30340;&#22797;&#26434;&#24615;&#26102;&#65292;&#20173;&#28982;&#38656;&#35201;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#25110;&#33258;&#36866;&#24212;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#19981;&#21487;&#36991;&#20813;&#22320;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22522;&#20110;&#24494;&#35843;&#30340;&#36234;&#29425;&#25915;&#20987;&#65288;FJAttack&#65289;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#20165;&#20960;&#20010;&#26377;&#23475;&#31034;&#20363;&#32435;&#20837;&#24494;&#35843;&#25968;&#25454;&#38598;&#23601;&#21487;&#33021;&#26174;&#30528;&#22320;&#25439;&#23475;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20363;&#22914;&#23558;&#23433;&#20840;&#31034;&#20363;&#32435;&#20837;&#24494;&#35843;&#25968;&#25454;&#38598;&#20197;&#20943;&#23569;&#23433;&#20840;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#32435;&#20837;&#22823;&#37327;&#30340;&#23433;&#20840;&#31034;&#20363;&#65292;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#38024;&#23545;FJAttack&#36827;&#34892;&#38450;&#24481;&#24182;&#21482;&#20351;&#29992;&#26377;&#38480;&#30340;&#23433;&#20840;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#21518;&#38376;&#25915;&#20987;&#27010;&#24565;&#30340;&#21518;&#38376;&#22686;&#24378;&#23433;&#20840;&#23545;&#40784;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14968v1 Announce Type: cross  Abstract: Despite the general capabilities of Large Language Models (LLMs) like GPT-4 and Llama-2, these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack), where incorporating just a few harmful examples into the fine-tuning dataset can significantly compromise the model safety. Though potential defenses have been proposed by incorporating safety examples into the fine-tuning dataset to reduce the safety issues, such approaches require incorporating a substantial amount of safety examples, making it inefficient. To effectively defend against the FJAttack with limited safety examples, we propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In pa
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14897</link><description>&lt;p&gt;
Chain-of-Thought&#19981;&#24544;&#35802;&#20316;&#20026;&#20266;&#35013;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Unfaithfulness as Disguised Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14897
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought (CoT)&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;LLM&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;CoT&#24544;&#23454;&#24230;&#30340;&#20195;&#29702;&#65292;arXiv:2307.13702&#25552;&#20986;&#20102;&#19968;&#20010;&#24230;&#37327;&#27169;&#22411;&#20381;&#36182;&#20854;CoT&#29983;&#25104;&#31572;&#26696;&#30340;&#25351;&#26631;&#12290;&#22312;&#19968;&#20010;&#19987;&#26377;&#27169;&#22411;&#31995;&#21015;&#20013;&#65292;&#20182;&#20204;&#21457;&#29616;LLM&#34920;&#29616;&#20986;&#27169;&#22411;&#22823;&#23567;&#19982;&#20854;&#24544;&#23454;&#24230;&#27979;&#37327;&#20043;&#38388;&#30340;&#32553;&#25918;-&#21453;&#21521;&#32553;&#25918;&#20851;&#31995;&#65292;&#24182;&#19988;130&#20159;&#21442;&#25968;&#27169;&#22411;&#30456;&#27604;&#20110;&#23610;&#23544;&#20171;&#20110;8.1&#20159;&#21040;1750&#20159;&#21442;&#25968;&#20043;&#38388;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#22686;&#21152;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#32467;&#26524;&#26159;&#21542;&#20316;&#20026;&#25152;&#26377;LLM&#30340;&#29305;&#24615;&#27867;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#31995;&#21015;&#30340;&#27169;&#22411;&#22797;&#21046;&#20182;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20182;&#20204;&#25253;&#21578;&#30340;CoT&#24544;&#23454;&#24230;&#30340;&#32553;&#25918;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#25913;&#21464;&#35774;&#23450;&#20250;&#23548;&#33268;&#36825;&#20123;&#27169;&#24335;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#28040;&#24687;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#23545;&#25269;&#25239;&#36234;&#29425;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#65292;&#19988;&#36234;&#29425;&#21487;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14857</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31995;&#32479;&#28040;&#24687;&#23545;&#36234;&#29425;&#26159;&#21542;&#30495;&#30340;&#24456;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the System Message Really Important to Jailbreaks in Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14857
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#28040;&#24687;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#23545;&#25269;&#25239;&#36234;&#29425;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#65292;&#19988;&#36234;&#29425;&#21487;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#19981;&#21487;&#25110;&#32570;&#12290;&#23613;&#31649;&#36890;&#24120;&#20250;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#22312;&#21457;&#24067;&#21069;&#23558;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#29616;&#35937;&#65292;&#34987;&#31216;&#20026;"&#36234;&#29425;"&#12290;&#36825;&#20010;&#26415;&#35821;&#25351;&#30340;&#26159;&#24403;LLMs&#21463;&#21040;&#24694;&#24847;&#38382;&#39064;&#25552;&#31034;&#26102;&#20135;&#29983;&#24847;&#22806;&#19988;&#21487;&#33021;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#29983;&#25104;&#36234;&#29425;&#25552;&#31034;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#19981;&#21516;&#30340;&#38382;&#39064;&#65306;&#31995;&#32479;&#28040;&#24687;&#23545;LLMs&#20013;&#30340;&#36234;&#29425;&#26159;&#21542;&#30495;&#30340;&#24456;&#37325;&#35201;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#31283;&#23450;&#30340;GPT&#29256;&#26412;gpt-3.5-turbo-0613&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#29983;&#25104;&#20102;&#20855;&#26377;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#30340;&#36234;&#29425;&#25552;&#31034;&#65306;&#30701;&#65292;&#38271;&#21644;&#26080;&#28040;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#31995;&#32479;&#28040;&#24687;&#36890;&#36807;&#23454;&#39564;&#20855;&#26377;&#19981;&#21516;&#30340;&#25269;&#25239;&#36234;&#29425;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36234;&#29425;&#22312;LLMs&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#31995;&#32479;&#28040;&#24687;&#22312;&#38450;&#27490;LLMs&#36234;&#29425;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14857v1 Announce Type: cross  Abstract: The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named "jailbreak." This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the signi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34917;&#20805;&#30340;&#35821;&#27861;&#20449;&#24687;&#26469;&#22788;&#29702;&#20551;&#26032;&#38395;&#20013;&#30340;&#24494;&#22937;&#36716;&#25240;</title><link>https://arxiv.org/abs/2402.14834</link><description>&lt;p&gt;
MSynFD: &#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MSynFD: Multi-hop Syntax aware Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14834
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34917;&#20805;&#30340;&#35821;&#27861;&#20449;&#24687;&#26469;&#22788;&#29702;&#20551;&#26032;&#38395;&#20013;&#30340;&#24494;&#22937;&#36716;&#25240;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#24191;&#27867;&#20256;&#25773;&#21161;&#38271;&#20102;&#20551;&#26032;&#38395;&#30340;&#24555;&#36895;&#20256;&#25773;&#65292;&#23545;&#25105;&#20204;&#30340;&#29616;&#23454;&#31038;&#20250;&#26500;&#25104;&#23041;&#32961;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#65292;&#36890;&#36807;&#20998;&#26512;&#26032;&#38395;&#20869;&#23481;&#21644;/&#25110;&#20854;&#31038;&#20250;&#32972;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#20102;&#22522;&#26412;&#30340;&#25991;&#26412;&#26032;&#38395;&#20869;&#23481;&#65288;&#25991;&#31456;&#65289;&#65292;&#24182;&#19988;&#36807;&#20998;&#20381;&#36182;&#24207;&#21015;&#24314;&#27169;&#21644;&#20840;&#23616;&#27880;&#24847;&#21147;&#26469;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#22797;&#26434;&#12289;&#24494;&#22937;&#30340;&#36716;&#25240;&#65292;&#27604;&#22914;&#21477;&#27861;-&#35821;&#20041;&#19981;&#21305;&#37197;&#21644;&#20808;&#39564;&#20559;&#24046;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#20302;&#65292;&#24182;&#22312;&#32570;&#22833;&#27169;&#24577;&#25110;&#31038;&#20250;&#32972;&#26223;&#26102;&#21487;&#33021;&#22833;&#36133;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#37325;&#35201;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#36339;&#35821;&#27861;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65288;MSynFD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;&#34917;&#20805;&#30340;&#35821;&#27861;&#20449;&#24687;&#65292;&#20197;&#22788;&#29702;&#20551;&#26032;&#38395;&#20013;&#30340;&#24494;&#22937;&#36716;&#25240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14834v1 Announce Type: cross  Abstract: The proliferation of social media platforms has fueled the rapid dissemination of fake news, posing threats to our real-life society. Existing methods use multimodal data or contextual information to enhance the detection of fake news by analyzing news content and/or its social context. However, these methods often overlook essential textual news content (articles) and heavily rely on sequential modeling and global attention to extract semantic information. These existing methods fail to handle the complex, subtle twists in news articles, such as syntax-semantics mismatches and prior biases, leading to lower performance and potential failure when modalities or social context are missing. To bridge these significant gaps, we propose a novel multi-hop syntax aware fake news detection (MSynFD) method, which incorporates complementary syntax information to deal with subtle twists in fake news. Specifically, we introduce a syntactical depen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#22270;&#26041;&#24335;&#65292;&#21033;&#29992;&#25991;&#26723;&#38388;&#21644;&#25991;&#20869;&#30456;&#20284;&#24615;&#65292;&#25552;&#21462;&#25991;&#26723;&#25910;&#34255;&#30340;&#25972;&#20307;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.13906</link><description>&lt;p&gt;
&#21033;&#29992;&#25972;&#20010;&#25910;&#34255;&#30456;&#20284;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#25991;&#26723;&#32467;&#26500;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13906
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#22270;&#26041;&#24335;&#65292;&#21033;&#29992;&#25991;&#26723;&#38388;&#21644;&#25991;&#20869;&#30456;&#20284;&#24615;&#65292;&#25552;&#21462;&#25991;&#26723;&#25910;&#34255;&#30340;&#25972;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#20010;&#39046;&#22495;&#30340;&#25991;&#26723;&#25910;&#34255;&#65292;&#22914;&#27861;&#24459;&#12289;&#21307;&#23398;&#25110;&#37329;&#34701;&#31561;&#65292;&#36890;&#24120;&#20849;&#20139;&#19968;&#20123;&#28508;&#22312;&#30340;&#25972;&#20010;&#25910;&#34255;&#32467;&#26500;&#65292;&#36825;&#20123;&#32467;&#26500;&#25429;&#25417;&#21040;&#30340;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#20154;&#31867;&#29992;&#25143;&#21644;&#32467;&#26500;&#24863;&#30693;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#35782;&#21035;&#25910;&#34255;&#20869;&#25991;&#26723;&#30340;&#20856;&#22411;&#32467;&#26500;&#65292;&#38656;&#35201;&#25429;&#25417;&#25972;&#20010;&#25910;&#34255;&#20013;&#21453;&#22797;&#20986;&#29616;&#30340;&#20027;&#39064;&#65292;&#21516;&#26102;&#25688;&#35201;&#20219;&#24847;&#26631;&#39064;&#30340;&#37322;&#20041;&#65292;&#24182;&#23558;&#27599;&#20010;&#20027;&#39064;&#19982;&#30456;&#24212;&#30340;&#25991;&#26723;&#20301;&#32622;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20123;&#35201;&#27714;&#24102;&#26469;&#20102;&#20960;&#20010;&#25361;&#25112;&#65306;&#26631;&#35760;&#21453;&#22797;&#20986;&#29616;&#20027;&#39064;&#30340;&#26631;&#39064;&#22312;&#25514;&#36766;&#19978;&#32463;&#24120;&#19981;&#21516;&#65292;&#26576;&#20123;&#33410;&#26631;&#39064;&#20165;&#36866;&#29992;&#20110;&#20010;&#21035;&#25991;&#26723;&#19988;&#19981;&#21453;&#26144;&#20856;&#22411;&#32467;&#26500;&#65292;&#20027;&#39064;&#39034;&#24207;&#22312;&#25991;&#26723;&#20043;&#38388;&#21487;&#33021;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26723;&#38388;&#21644;&#25991;&#20869;&#30456;&#20284;&#24615;&#30340;&#26080;&#30417;&#30563;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#21462;&#28508;&#22312;&#30340;&#25972;&#20010;&#25910;&#34255;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13906v1 Announce Type: new  Abstract: Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models. We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations. These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents. Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure. Our evaluations on three diverse domains in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#35821;&#38899;&#35299;&#26512;&#22120;&#35780;&#20272;&#38382;&#39064;&#21551;&#21457;&#30340;&#32467;&#26500;&#21270;&#21477;&#27861;&#20998;&#26512;&#26641;&#30456;&#20284;&#24615;&#24230;&#37327;&#25351;&#26631;STRUCT-IOU&#65292;&#26377;&#25928;&#22320;&#27604;&#36739;&#20102;&#21475;&#35821;&#35789;&#36793;&#30028;&#19978;&#30340;&#32452;&#22359;&#20998;&#26512;&#26641;&#19982;&#20070;&#38754;&#35789;&#19978;&#22522;&#20934;&#35299;&#26512;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#32452;&#22359;&#20998;&#26512;&#35780;&#20272;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13433</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#26641;&#23545;&#40784;&#29992;&#20110;&#65288;&#35821;&#38899;&#65289;&#32452;&#22359;&#20998;&#26512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Structured Tree Alignment for Evaluation of (Speech) Constituency Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13433
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#35821;&#38899;&#35299;&#26512;&#22120;&#35780;&#20272;&#38382;&#39064;&#21551;&#21457;&#30340;&#32467;&#26500;&#21270;&#21477;&#27861;&#20998;&#26512;&#26641;&#30456;&#20284;&#24615;&#24230;&#37327;&#25351;&#26631;STRUCT-IOU&#65292;&#26377;&#25928;&#22320;&#27604;&#36739;&#20102;&#21475;&#35821;&#35789;&#36793;&#30028;&#19978;&#30340;&#32452;&#22359;&#20998;&#26512;&#26641;&#19982;&#20070;&#38754;&#35789;&#19978;&#22522;&#20934;&#35299;&#26512;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#32452;&#22359;&#20998;&#26512;&#35780;&#20272;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#24179;&#22343;&#20132;&#38598;-&#32852;&#30431;&#27604;&#65288;STRUCT-IOU&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21477;&#27861;&#20998;&#26512;&#26641;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#25351;&#26631;&#65292;&#21463;&#21040;&#20102;&#35780;&#20272;&#35821;&#38899;&#35299;&#26512;&#22120;&#38382;&#39064;&#30340;&#21551;&#21457;&#12290;STRUCT-IOU&#20351;&#24471;&#21487;&#20197;&#27604;&#36739;&#22312;&#33258;&#21160;&#35782;&#21035;&#30340;&#21475;&#35821;&#35789;&#36793;&#30028;&#19978;&#30340;&#32452;&#22359;&#20998;&#26512;&#26641;&#19982;&#22522;&#20934;&#35299;&#26512;&#65288;&#22312;&#20070;&#38754;&#35789;&#19978;&#65289;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35745;&#31639;&#36825;&#20010;&#25351;&#26631;&#65292;&#25105;&#20204;&#36890;&#36807;&#24378;&#21046;&#23545;&#40784;&#23558;&#22522;&#20934;&#35299;&#26512;&#26641;&#25237;&#24433;&#21040;&#35821;&#38899;&#39046;&#22495;&#65292;&#23558;&#25237;&#24433;&#30340;&#22522;&#20934;&#25104;&#20998;&#19982;&#39044;&#27979;&#30340;&#25104;&#20998;&#22312;&#19968;&#23450;&#30340;&#32467;&#26500;&#32422;&#26463;&#19979;&#23545;&#40784;&#65292;&#28982;&#21518;&#35745;&#31639;&#25152;&#26377;&#23545;&#40784;&#25104;&#20998;&#23545;&#20043;&#38388;&#30340;&#24179;&#22343;IOU&#20998;&#25968;&#12290;STRUCT-IOU&#32771;&#34385;&#20102;&#35789;&#36793;&#30028;&#65292;&#24182;&#20811;&#26381;&#20102;&#39044;&#27979;&#30340;&#35789;&#21644;&#22522;&#20934;&#20107;&#23454;&#21487;&#33021;&#27809;&#26377;&#23436;&#32654;&#19968;&#19968;&#23545;&#24212;&#30340;&#25361;&#25112;&#12290;&#25193;&#23637;&#21040;&#25991;&#26412;&#32452;&#22359;&#20998;&#26512;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;STRUCT-IOU&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#23545;&#21477;&#27861;&#21512;&#29702;&#35299;&#26512;&#30340;&#23481;&#24525;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13433v1 Announce Type: new  Abstract: We present the structured average intersection-over-union ratio (STRUCT-IOU), a similarity metric between constituency parse trees motivated by the problem of evaluating speech parsers. STRUCT-IOU enables comparison between a constituency parse tree (over automatically recognized spoken word boundaries) with the ground-truth parse (over written words). To compute the metric, we project the ground-truth parse tree to the speech domain by forced alignment, align the projected ground-truth constituents with the predicted ones under certain structured constraints, and calculate the average IOU score across all aligned constituent pairs. STRUCT-IOU takes word boundaries into account and overcomes the challenge that the predicted words and ground truth may not have perfect one-to-one correspondence. Extending to the evaluation of text constituency parsing, we demonstrate that STRUCT-IOU shows higher tolerance to syntactically plausible parses 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#31574;&#30053;&#20197;&#31934;&#28860;&#26356;&#23567;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12821</link><description>&lt;p&gt;
&#22312;&#25688;&#35201;&#20013;&#35782;&#21035;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65306;&#26397;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#31574;&#30053;&#20197;&#31934;&#28860;&#26356;&#23567;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#23545;&#25277;&#35937;&#24615;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#21830;&#19994;&#37096;&#32626;&#26500;&#25104;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#22260;&#32469;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#23637;&#24320;&#65306;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#31934;&#28860;&#19968;&#20010;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#21151;&#25928;&#24615;&#30340;&#26356;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65311;&#39318;&#20808;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#38646;&#26679;&#26412;&#33539;&#24335;&#65292;&#36328;&#36234;&#20116;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65306;&#30452;&#25509;&#25512;&#29702;&#25972;&#20010;&#25688;&#35201;&#25110;&#27599;&#20010;&#25688;&#35201;&#31383;&#21475;&#65307;&#36890;&#36807;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#36827;&#34892;&#23454;&#20307;&#39564;&#35777;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#33021;&#22815;&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#24179;&#22343;&#36229;&#36807;&#24378;&#22823;&#30340;&#35757;&#32451;&#22522;&#32447;2.8%&#12290;&#20026;&#36827;&#19968;&#27493;&#20419;&#36827;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#38024;&#23545;&#31934;&#28860;&#26356;&#23567;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19968;&#27425;&#24615;&#39640;&#20934;&#30830;&#22320;&#35780;&#20998;&#25972;&#20010;&#25688;&#35201;&#65292;&#32988;&#36807;&#38646;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12821v1 Announce Type: new  Abstract: Factual inconsistency poses a significant hurdle for the commercial deployment of abstractive summarizers. Under this Large Language Model (LLM) era, this work focuses around two important questions: what is the best way to leverage LLM for factual inconsistency detection, and how could we distill a smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms are firstly proposed and evaluated across five diverse datasets: direct inference on the entire summary or each summary window; entity verification through question generation and answering. Experiments suggest that LLM itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average. To further promote practical utility, we then propose training strategies aimed at distilling smaller open-source LLM that learns to score the entire summary at once with high accuracy, which outperforms the zero
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;FinBen&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24443;&#24213;&#35780;&#20272;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#33021;&#21147;&#30340;&#20840;&#38754;&#24320;&#28304;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;15&#20010;&#20195;&#34920;&#24615;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12659</link><description>&lt;p&gt;
FinBen&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#36130;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
The FinBen: An Holistic Financial Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;FinBen&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24443;&#24213;&#35780;&#20272;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#33021;&#21147;&#30340;&#20840;&#38754;&#24320;&#28304;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;15&#20010;&#20195;&#34920;&#24615;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#24443;&#24213;&#30340;&#35780;&#20272;&#21644;&#37329;&#34701;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#24320;&#21457;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FinBen&#65292;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#24320;&#28304;&#35780;&#20272;&#22522;&#20934;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24443;&#24213;&#35780;&#20272;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;FinBen&#21253;&#25324;23&#31181;&#37329;&#34701;&#20219;&#21153;&#30340;35&#20010;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#20219;&#21153;&#26681;&#25454;&#21345;&#29305;&#23572;-&#38669;&#24681;-&#21345;&#32599;&#23572;&#29702;&#35770;&#30340;&#28789;&#24863;&#32452;&#32455;&#25104;&#19977;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#35889;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#24402;&#32435;&#25512;&#29702;&#12289;&#32852;&#24819;&#35760;&#24518;&#12289;&#25968;&#37327;&#25512;&#29702;&#12289;&#26230;&#20307;&#26234;&#21147;&#31561;&#26041;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;15&#20010;&#20195;&#34920;&#24615;LLMs&#65288;&#21253;&#25324;GPT-4&#12289;ChatGPT&#21644;&#26368;&#26032;&#30340;Gemini&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12659v1 Announce Type: cross  Abstract: LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of thorough evaluations and the complexity of financial tasks. This along with the rapid development of LLMs, highlights the urgent need for a systematic financial evaluation benchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs' cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations withi
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#20114;&#21160;&#30340;&#8220;&#31867;&#31038;&#20250;&#8221;&#23646;&#24615;&#65292;&#20197;&#22686;&#21152;&#22870;&#21169;&#24182;&#20943;&#23569;&#39118;&#38505;&#65292;&#23637;&#31034;&#26032;&#20852;&#30340;&#21435;&#20013;&#24515;&#21270;AI&#38598;&#20307;&#22914;&#20309;&#25193;&#22823;&#20154;&#31867;&#22810;&#26679;&#24615;&#33539;&#22260;&#21644;&#38477;&#20302;&#22312;&#32447;&#26377;&#27602;&#34892;&#20026;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.12590</link><description>&lt;p&gt;
&#23558;AI&#38598;&#20307;&#36827;&#21270;&#65292;&#22686;&#24378;&#20154;&#31867;&#22810;&#26679;&#24615;&#24182;&#23454;&#29616;&#33258;&#25105;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12590
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#20114;&#21160;&#30340;&#8220;&#31867;&#31038;&#20250;&#8221;&#23646;&#24615;&#65292;&#20197;&#22686;&#21152;&#22870;&#21169;&#24182;&#20943;&#23569;&#39118;&#38505;&#65292;&#23637;&#31034;&#26032;&#20852;&#30340;&#21435;&#20013;&#24515;&#21270;AI&#38598;&#20307;&#22914;&#20309;&#25193;&#22823;&#20154;&#31867;&#22810;&#26679;&#24615;&#33539;&#22260;&#21644;&#38477;&#20302;&#22312;&#32447;&#26377;&#27602;&#34892;&#20026;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#20182;&#20154;&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#24341;&#23548;&#20854;&#34892;&#20026;&#12290;&#36825;&#31181;&#33021;&#21147;&#21450;&#20854;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#26085;&#30410;&#26222;&#21450;&#30340;&#36235;&#21183;&#39044;&#31034;&#30528;&#23427;&#20204;&#23558;&#26377;&#24847;&#25110;&#26080;&#24847;&#22320;&#8220;&#32534;&#31243;&#8221;&#24444;&#27492;&#24182;&#24418;&#25104;&#26032;&#20852;&#30340;AI&#20027;&#20307;&#24615;&#12289;&#20851;&#31995;&#21644;&#38598;&#20307;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#30028;&#25506;&#31350;&#36825;&#20123;&#20114;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#8220;&#31867;&#31038;&#20250;&#8221;&#23646;&#24615;&#65292;&#20197;&#22686;&#21152;&#20854;&#22870;&#21169;&#24182;&#20943;&#23569;&#23545;&#20154;&#31867;&#31038;&#20250;&#21644;&#22312;&#32447;&#29615;&#22659;&#20581;&#24247;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#21450;&#20854;&#36755;&#20986;&#26469;&#35828;&#26126;&#36825;&#31181;&#26032;&#20852;&#30340;&#12289;&#21435;&#20013;&#24515;&#21270;&#30340;AI&#38598;&#20307;&#22914;&#20309;&#25193;&#22823;&#20154;&#31867;&#22810;&#26679;&#24615;&#33539;&#22260;&#24182;&#38477;&#20302;&#22312;&#32447;&#26377;&#27602;&#12289;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#39118;&#38505;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;AI&#33258;&#25105;&#35843;&#33410;&#30340;&#26426;&#20250;&#65292;&#24182;&#35299;&#20915;&#20102;&#28041;&#21450;&#21019;&#24314;&#21644;&#32500;&#25252;&#21435;&#20013;&#24515;&#21270;AI&#38598;&#20307;&#30340;&#20262;&#29702;&#38382;&#39064;&#21644;&#35774;&#35745;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12590v1 Announce Type: new  Abstract: Large language models steer their behaviors based on texts generated by others. This capacity and their increasing prevalence in online settings portend that they will intentionally or unintentionally "program" one another and form emergent AI subjectivities, relationships, and collectives. Here, we call upon the research community to investigate these "society-like" properties of interacting artificial intelligences to increase their rewards and reduce their risks for human society and the health of online environments. We use a simple model and its outputs to illustrate how such emergent, decentralized AI collectives can expand the bounds of human diversity and reduce the risk of toxic, anti-social behavior online. Finally, we discuss opportunities for AI self-moderation and address ethical issues and design challenges associated with creating and maintaining decentralized AI collectives.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;</title><link>https://arxiv.org/abs/2402.11903</link><description>&lt;p&gt;
SoLA: &#20026;&#20102;&#26356;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32780;&#23545;LLM&#36827;&#34892;&#27714;&#35299;&#23618;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11903
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36923;&#36753;&#25512;&#29702;&#19978;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#35797;&#22270;&#36890;&#36807;&#24037;&#20855;&#23398;&#20064;&#26469;&#25913;&#21464;&#38382;&#39064;&#27714;&#35299;&#12290;&#34429;&#28982;&#22312;&#23567;&#35268;&#27169;&#38382;&#39064;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#35268;&#27169;&#24222;&#22823;&#19988;&#34920;&#36798;&#22797;&#26434;&#65292;&#35299;&#20915;&#24037;&#19994;&#26696;&#20363;&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;LLM&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27714;&#35299;&#22120;&#20316;&#20026;&#26032;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;&#12290;&#22312;SoLA&#20013;&#65292;LLM&#26088;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#35782;&#21035;&#26368;&#39640;&#36136;&#37327;&#30340;&#23616;&#37096;&#35299;&#65292;&#32780;&#27714;&#35299;&#22120;&#23618;&#21017;&#19987;&#27880;&#20110;&#21021;&#22987;&#35299;&#19981;&#28385;&#36275;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#20511;&#21161;MaxSAT&#20316;&#20026;&#26725;&#26753;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#26799;&#24230;&#65292;&#20351;&#26368;&#32456;&#27169;&#22411;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#28385;&#36275;&#30340;&#35299;&#25110;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#12290;&#21518;&#38376;&#29702;&#35770;&#30830;&#20445;SoLA&#33021;&#22815;&#33719;&#24471;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11903v1 Announce Type: cross  Abstract: Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#30340;&#39640;&#36890;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11804</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#25552;&#31034;&#22120;&#65306;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#30340;&#39640;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#24402;&#32435;&#25512;&#29702;&#26088;&#22312;&#25512;&#26029;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;KG&#20013;&#32570;&#22833;&#30340;&#20107;&#23454;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;KG&#24402;&#32435;&#25512;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22788;&#29702;&#22312;&#25991;&#26412;&#21644;&#32467;&#26500;&#26041;&#38754;&#37117;&#31232;&#32570;&#30340;&#20302;&#36164;&#28304;&#22330;&#26223;&#12290;&#26412;&#25991;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#20174;&#32780;&#20026;KG&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#24102;&#26469;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312;&#26041;&#27861;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;ProLINK&#65292;&#26088;&#22312;&#22312;&#20219;&#24847;KG&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#22312;&#23454;&#36341;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;36&#20010;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;DEEPEVAL&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#23545;&#35270;&#35273;&#28145;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;LMMs&#22312;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.11281</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#25581;&#31034;&#22270;&#20687;&#32972;&#21518;&#30340;&#28145;&#23618;&#35821;&#20041;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Multimodal Models Uncover Deep Semantics Behind Images?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;DEEPEVAL&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#23545;&#35270;&#35273;&#28145;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;LMMs&#22312;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22270;&#20687;&#30340;&#28145;&#23618;&#35821;&#20041;&#22312;&#31038;&#20132;&#23186;&#20307;&#20027;&#23548;&#30340;&#26102;&#20195;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#22270;&#20687;&#30340;&#34920;&#38754;&#25551;&#36848;&#19978;&#65292;&#25581;&#31034;&#20102;&#22312;&#23545;&#20869;&#22312;&#28145;&#23618;&#35821;&#20041;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#26041;&#38754;&#30340;&#26126;&#26174;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DEEPEVAL&#65292;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#23545;&#35270;&#35273;&#28145;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290; DEEPEVAL &#21253;&#25324;&#20154;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#28176;&#36827;&#30340;&#23376;&#20219;&#21153;&#65306;&#32454;&#31890;&#24230;&#25551;&#36848;&#36873;&#25321;&#12289;&#28145;&#24230;&#26631;&#39064;&#21305;&#37197;&#21644;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#12290;&#21033;&#29992; DEEPEVAL&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;9&#20010;&#24320;&#28304;LMMs&#21644;GPT-4V(ision)&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#29616;&#26377;LMMs&#19982;&#20154;&#31867;&#22312;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#19978;&#23384;&#22312;&#30528;&#23454;&#36136;&#24046;&#36317;&#12290;&#20363;&#22914;&#65292;&#21363;&#20351;&#22312;&#22270;&#20687;&#25551;&#36848;&#26041;&#38754;&#36798;&#21040;&#19982;&#20154;&#31867;&#21487;&#27604;&#30340;&#34920;&#29616;&#65292;GPT-4V&#22312;&#29702;&#35299;&#28145;&#23618;&#35821;&#20041;&#26041;&#38754;&#20173;&#33853;&#21518;&#20110;&#20154;&#31867;30%&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11281v1 Announce Type: new  Abstract: Understanding the deep semantics of images is essential in the era dominated by social media. However, current research works primarily on the superficial description of images, revealing a notable deficiency in the systematic investigation of the inherent deep semantics. In this work, we introduce DEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs) capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset and three progressive subtasks: fine-grained description selection, in-depth title matching, and deep semantics understanding. Utilizing DEEPEVAL, we evaluate 9 open-source LMMs and GPT-4V(ision).Our evaluation demonstrates a substantial gap between the deep semantic comprehension capabilities of existing LMMs and humans. For example, GPT-4V is 30% behind humans in understanding deep semantics, even though it achieves human-comparable performance in image description. Further analysis indi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CoT&#25512;&#29702;&#33021;&#21147;&#35780;&#20272;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#25512;&#29702;&#30693;&#35782;&#21644;&#29983;&#25104;CoT&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;</title><link>https://arxiv.org/abs/2402.11199</link><description>&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#23545;&#22810;&#36339;&#25512;&#29702;&#20013;&#24605;&#32500;&#38142;&#30340;&#30452;&#25509;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CoT&#25512;&#29702;&#33021;&#21147;&#35780;&#20272;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#25512;&#29702;&#30693;&#35782;&#21644;&#29983;&#25104;CoT&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24403;&#25552;&#31034;&#29983;&#25104;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#35299;&#37322;&#26102;&#65292;&#20197;&#21450;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#20851;&#20110;LLMs&#35780;&#20272;&#30340;&#30740;&#31350;&#20165;&#20851;&#27880;&#31572;&#26696;&#20934;&#30830;&#24615;&#65292;&#24573;&#30053;&#20102;&#29983;&#25104;&#30340;CoT&#30340;&#27491;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#65292;&#28145;&#20837;&#25506;&#35752;LLMs&#22312;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;CoT&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36776;&#21035;&#24335;&#21644;&#29983;&#25104;&#24335;CoT&#35780;&#20272;&#33539;&#24335;&#65292;&#20197;&#35780;&#20272;LLMs&#30340;&#25512;&#29702;&#30693;&#35782;&#21644;&#29983;&#25104;CoT&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;2&#20010;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#19978;&#23545;5&#20010;&#19981;&#21516;&#31995;&#21015;&#30340;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#20855;&#26377;&#36275;&#22815;&#30340;&#30693;&#35782;&#26469;&#25191;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;LLMs&#29983;&#25104;&#30340;CoT&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#19982;&#31572;&#26696;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#34920;&#26126;&#23427;&#20204;&#32463;&#24120;&#36890;&#36807;&#38169;&#35823;&#30340;&#26041;&#24335;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11199v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate strong reasoning abilities when prompted to generate chain-of-thought (CoT) explanations alongside answers. However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT. In this paper, we delve deeper into the CoT reasoning capabilities of LLMs in multi-hop question answering by utilizing knowledge graphs (KGs). We propose a novel discriminative and generative CoT evaluation paradigm to assess LLMs' knowledge of reasoning and the accuracy of the generated CoT. Through experiments conducted on 5 different families of LLMs across 2 multi-hop question-answering datasets, we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36339;&#34920;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#20889;&#38382;&#39064;&#21644;&#27874;&#26463;&#25628;&#32034;&#26469;&#20943;&#23569;&#30456;&#20284;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22810;&#36339;&#26816;&#32034;&#20013;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.10666</link><description>&lt;p&gt;
&#24320;&#25918;&#22495;&#25991;&#26412;&#21040;SQL&#30340;&#22810;&#36339;&#34920;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Hop Table Retrieval for Open-Domain Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10666
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#36339;&#34920;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#20889;&#38382;&#39064;&#21644;&#27874;&#26463;&#25628;&#32034;&#26469;&#20943;&#23569;&#30456;&#20284;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22810;&#36339;&#26816;&#32034;&#20013;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#25991;&#26412;&#21040;SQL&#26159;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#23427;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#34920;&#65292;&#28982;&#21518;&#29983;&#25104;SQL&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21333;&#36339;&#26816;&#32034;&#26041;&#27861;&#24182;&#26410;&#20851;&#27880;&#25991;&#26412;&#21040;SQL&#25361;&#25112;&#20013;&#30340;&#27169;&#24335;&#38142;&#25509;&#65292;&#36825;&#28041;&#21450;&#21040;&#23558;&#38382;&#39064;&#20013;&#30340;&#23454;&#20307;&#19982;&#34920;&#20013;&#23454;&#20307;&#23545;&#40784;&#65292;&#20027;&#35201;&#20307;&#29616;&#22312;&#20004;&#20010;&#26041;&#38754;&#65306;&#30456;&#20284;&#30340;&#26080;&#20851;&#23454;&#20307;&#21644;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#24102;&#37325;&#20889;&#21644;&#27874;&#26463;&#25628;&#32034;&#30340;&#22810;&#36339;&#34920;&#26816;&#32034;&#65288;Murre&#65289;&#12290;&#20026;&#20102;&#20943;&#23569;&#30456;&#20284;&#30340;&#26080;&#20851;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#27599;&#20010;&#36339;&#36291;&#20013;&#26410;&#26816;&#32034;&#21040;&#30340;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#27874;&#26463;&#25628;&#32034;&#32771;&#34385;&#25490;&#21517;&#36739;&#20302;&#30340;&#34920;&#12290;&#20026;&#20102;&#32531;&#35299;&#39046;&#22495;&#19981;&#21305;&#37197;&#23454;&#20307;&#30340;&#38480;&#21046;&#65292;Murre&#22522;&#20110;&#22810;&#20010;&#36339;&#36291;&#20013;&#26816;&#32034;&#21040;&#30340;&#34920;&#37325;&#20889;&#38382;&#39064;&#65292;&#20943;&#23569;&#19982;&#30456;&#20851;&#34920;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;SpiderUnion&#21644;BirdUnion+&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10666v1 Announce Type: new  Abstract: Open-domain text-to-SQL is an important task that retrieves question-relevant tables from massive databases and then generates SQL. However, existing retrieval methods that retrieve in a single hop do not pay attention to the text-to-SQL challenge of schema linking, which is aligning the entities in the question with table entities, reflected in two aspects: similar irrelevant entity and domain mismatch entity. Therefore, we propose our method, the multi-hop table retrieval with rewrite and beam search (Murre). To reduce the effect of the similar irrelevant entity, our method focuses on unretrieved entities at each hop and considers the low-ranked tables by beam search. To alleviate the limitation of domain mismatch entity, Murre rewrites the question based on retrieved tables in multiple hops, decreasing the domain gap with relevant tables. We conduct experiments on SpiderUnion and BirdUnion+, reaching new state-of-the-art results with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#22810;&#27425;&#36845;&#20195;&#21512;&#25104;&#26469;&#25913;&#21892;&#25991;&#26412;&#21040;SQL&#28436;&#31034;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#22810;&#26679;&#24615;&#28436;&#31034;&#27744;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#24615;&#24182;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.10663</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#34701;&#21512;&#26041;&#27861;&#25913;&#21892;&#25991;&#26412;&#21040;SQL&#30340;&#28436;&#31034;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#22810;&#27425;&#36845;&#20195;&#21512;&#25104;&#26469;&#25913;&#21892;&#25991;&#26412;&#21040;SQL&#28436;&#31034;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#22810;&#26679;&#24615;&#28436;&#31034;&#27744;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#24615;&#24182;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#20026;&#25991;&#26412;&#21040;SQL&#30740;&#31350;&#30340;&#20027;&#27969;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35752;&#35770;&#20102;&#22914;&#20309;&#20174;&#20154;&#26631;&#35760;&#30340;&#28436;&#31034;&#27744;&#20013;&#36873;&#25321;&#19982;&#29992;&#25143;&#38382;&#39064;&#30456;&#20851;&#30340;&#28436;&#31034;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26631;&#27880;&#23384;&#22312;&#30528;&#22810;&#26679;&#24615;&#19981;&#36275;&#21644;&#26631;&#27880;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#34913;&#37327;&#21644;&#25913;&#21892;&#25991;&#26412;&#21040;SQL&#28436;&#31034;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24230;&#37327;&#28436;&#31034;&#22810;&#26679;&#24615;&#30340;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#29616;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#22522;&#20110;&#19978;&#36848;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#30340;&#22810;&#27425;&#36845;&#20195;&#21512;&#25104;&#26469;&#26500;&#24314;&#39640;&#22810;&#26679;&#24615;&#28436;&#31034;&#27744;&#30340;&#34701;&#21512;&#26041;&#27861;&#65288;Fused&#65289;&#65292;&#25552;&#39640;&#20102;&#22810;&#26679;&#24615;&#24182;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;/&#26080;&#20154;&#31867;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#24179;&#22343;&#25552;&#39640;&#20102;3.2%&#21644;5.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10663v1 Announce Type: new  Abstract: Currently, the in-context learning method based on large language models (LLMs) has become the mainstream of text-to-SQL research. Previous works have discussed how to select demonstrations related to the user question from a human-labeled demonstration pool. However, human labeling suffers from the limitations of insufficient diversity and high labeling overhead. Therefore, in this paper, we discuss how to measure and improve the diversity of the demonstrations for text-to-SQL. We present a metric to measure the diversity of the demonstrations and analyze the insufficient of the existing labeled data by experiments. Based on the above discovery, we propose fusing iteratively for demonstrations (Fused) to build a high-diversity demonstration pool through human-free multiple-iteration synthesis, improving diversity and lowering label cost. Our method achieves an average improvement of 3.2% and 5.0% with and without human labeling on sever
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25552;&#31034;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25216;&#26415;GGPP&#12290;&#36890;&#36807;GGPP&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;LLMs&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#29305;&#23450;&#30340;&#38169;&#35823;&#31572;&#26696;&#65292;&#24182;&#24212;&#23545;&#25552;&#31034;&#20013;&#30340;&#26080;&#20851;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.07179</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#25552;&#31034;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#25552;&#31034;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#25216;&#26415;GGPP&#12290;&#36890;&#36807;GGPP&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;LLMs&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#29305;&#23450;&#30340;&#38169;&#35823;&#31572;&#26696;&#65292;&#24182;&#24212;&#23545;&#25552;&#31034;&#20013;&#30340;&#26080;&#20851;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#40065;&#26834;&#24615;&#22312;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#20351;&#29992;&#36805;&#36895;&#22686;&#38271;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#34987;&#35270;&#20026;&#25552;&#39640;&#20174;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;RAG-based LLMs&#30340;&#36755;&#20986;&#22914;&#20309;&#21463;&#21040;&#31245;&#26377;&#19981;&#21516;&#30340;&#36755;&#20837;&#24433;&#21709;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#25552;&#31034;&#20013;&#25554;&#20837;&#19968;&#20010;&#24456;&#30701;&#30340;&#21069;&#32512;&#20063;&#20250;&#23548;&#33268;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#20107;&#23454;&#27491;&#30830;&#31572;&#26696;&#30456;&#21435;&#29978;&#36828;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#36825;&#31867;&#21069;&#32512;&#23545;RAG&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Gradient Guided Prompt Perturbation&#65288;GGPP&#65289;&#30340;&#26032;&#22411;&#20248;&#21270;&#25216;&#26415;&#12290;GGPP&#22312;&#23558;RAG-based LLMs&#30340;&#36755;&#20986;&#24341;&#23548;&#21040;&#29305;&#23450;&#38169;&#35823;&#31572;&#26696;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;&#23427;&#36824;&#21487;&#20197;&#24212;&#23545;&#25552;&#31034;&#20013;&#35831;&#27714;&#24573;&#30053;&#26080;&#20851;&#19978;&#19979;&#25991;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;LLMs&#22312;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;GGPP&#25200;&#21160;&#30340;&#25552;&#31034;&#20043;&#38388;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;&#24046;&#24322;&#26469;&#25552;&#20379;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24515;&#38382;&#39064;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#30001;&#20110;&#20998;&#32452;&#25439;&#22833;&#23548;&#33268;&#30340;&#39044;&#27979;&#20998;&#25968;&#19982;&#23454;&#38469;&#27010;&#29575;&#20559;&#31163;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#37325;&#26032;&#30830;&#23450;LLMs&#65292;&#25913;&#21892;&#23427;&#20204;&#30340;&#33258;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04957</link><description>&lt;p&gt;
&#20174;&#20998;&#32452;&#25439;&#22833;&#30340;&#35282;&#24230;&#37325;&#26500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24515;
&lt;/p&gt;
&lt;p&gt;
Reconfidencing LLMs from the Grouping Loss Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24515;&#38382;&#39064;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#30001;&#20110;&#20998;&#32452;&#25439;&#22833;&#23548;&#33268;&#30340;&#39044;&#27979;&#20998;&#25968;&#19982;&#23454;&#38469;&#27010;&#29575;&#20559;&#31163;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#37325;&#26032;&#30830;&#23450;LLMs&#65292;&#25913;&#21892;&#23427;&#20204;&#30340;&#33258;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;ChatGPT&#21644;LLaMA&#65292;&#22312;&#33258;&#20449;&#30340;&#21475;&#21563;&#20013;&#23481;&#26131;&#29983;&#25104;&#34394;&#20551;&#31572;&#26696;&#12290;&#23613;&#31649;&#24341;&#23548;&#21644;&#26657;&#20934;&#20449;&#24515;&#20998;&#25968;&#30340;&#21162;&#21147;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25511;&#21046;&#19981;&#30830;&#23450;&#24615;&#24517;&#39035;&#36229;&#36234;&#26657;&#20934;: &#30001;&#20110;&#20998;&#32452;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#39044;&#27979;&#20998;&#25968;&#21487;&#33021;&#26126;&#26174;&#20559;&#31163;&#23454;&#38469;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20174;&#30693;&#35782;&#24211;&#20013;&#33719;&#21462;&#65292;&#20197;&#35780;&#20272;&#23545;Mistral&#21644;LLaMA&#30340;&#31572;&#26696;&#32473;&#20986;&#30340;&#20449;&#24515;&#20998;&#25968;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#36807;&#20110;&#33258;&#20449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#26576;&#20123;&#31572;&#26696;&#19978;&#27604;&#20854;&#20182;&#31572;&#26696;&#26356;&#36807;&#20110;&#33258;&#20449;&#65292;&#20363;&#22914;&#21462;&#20915;&#20110;&#26597;&#35810;&#20013;&#20154;&#30340;&#22269;&#31821;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#29702;&#35770;&#20013;&#65292;&#36825;&#23601;&#26159;&#20998;&#32452;&#25439;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#30830;&#23450;LLMs&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20165;&#21462;&#28040;&#26657;&#20934;&#65292;&#36824;&#21462;&#28040;&#20998;&#32452;&#25439;&#22833;&#12290;&#32463;&#36807;&#37325;&#26032;&#30830;&#23450;&#30340;LLMs&#32463;&#36807;&#22788;&#29702;&#21518;&#65292;&#34920;&#31034;&#25913;&#36827;&#30340;&#33258;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone. While efforts to elicit and calibrate confidence scores have proven useful, recent findings show that controlling uncertainty must go beyond calibration: predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss. In this work, we construct a new evaluation dataset derived from a knowledge base to assess confidence scores given to answers of Mistral and LLaMA. Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \emph{eg} depending on the nationality of the person in the query. In uncertainty-quantification theory, this is grouping loss. To address this, we propose a solution to reconfidence LLMs, canceling not only calibration but also grouping loss. The LLMs, after the reconfidencing process, indicate improved confidenc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.04437</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Structured Entity Extraction Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#24433;&#21709;&#20102;&#20449;&#24687;&#25552;&#21462;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#21644;&#35268;&#33539;&#21270;&#20102;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#65288;SEE&#65289;&#20219;&#21153;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#36817;&#20284;&#23454;&#20307;&#38598;&#37325;&#21472;&#65288;AESOP&#65289;&#24230;&#37327;&#65292;&#20197;&#36866;&#24403;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#24182;&#34892;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#39046;&#22495;&#30340;&#26410;&#26469;&#36827;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#19978;&#28216;&#23454;&#20363;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26681;&#25454;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#21464;&#21270;&#19982;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.01865</link><description>&lt;p&gt;
&#25105;&#30340;&#27169;&#22411;&#20250;&#24536;&#35760;&#20160;&#20040;&#65311;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20013;&#30340;&#34987;&#36951;&#24536;&#23454;&#20363;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#19978;&#28216;&#23454;&#20363;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26681;&#25454;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#21464;&#21270;&#19982;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#20250;&#20986;&#29616;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#36890;&#36807;&#23558;&#27169;&#22411;&#26356;&#26032;&#20026;&#32416;&#27491;&#38169;&#35823;&#23454;&#20363;&#65292;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#65292;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#22312;&#25351;&#23548;&#24494;&#35843;&#25110;&#19978;&#28216;&#35757;&#32451;&#38454;&#27573;&#20013;&#23398;&#21040;&#30340;&#23454;&#20363;&#19978;&#20986;&#29616;&#38169;&#35823;&#12290;&#38543;&#26426;&#37325;&#25773;&#19978;&#28216;&#25968;&#25454;&#30340;&#25928;&#26524;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#24448;&#24448;&#20276;&#38543;&#30528;&#36739;&#39640;&#30340;&#26041;&#24046;&#21644;&#36739;&#24046;&#30340;&#21487;&#25511;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#30001;&#20110;&#27169;&#22411;&#26356;&#26032;&#32780;&#36951;&#24536;&#30340;&#19978;&#28216;&#23454;&#20363;&#12290;&#25105;&#20204;&#26681;&#25454;&#19968;&#32452;&#22312;&#32447;&#23398;&#20064;&#30340;&#23454;&#20363;&#21644;&#30456;&#24212;&#34987;&#36951;&#24536;&#30340;&#19978;&#28216;&#39044;&#35757;&#32451;&#23454;&#20363;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#30340;&#21464;&#21270;&#31867;&#20284;&#20110;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#21464;&#21270;&#65292;&#36825;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#19981;&#38169;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products 
&lt;/p&gt;</description></item><item><title>CDEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#21270;&#32500;&#24230;&#12290;&#36890;&#36807;&#23545;&#20845;&#20010;&#25991;&#21270;&#32500;&#24230;&#21644;&#19971;&#20010;&#39046;&#22495;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#21270;&#29305;&#24449;&#12289;&#19968;&#33268;&#24615;&#21644;&#24046;&#24322;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#26102;&#34701;&#20837;&#25991;&#21270;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.16421</link><description>&lt;p&gt;
CDEval&#65306;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#32500;&#24230;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16421
&lt;/p&gt;
&lt;p&gt;
CDEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#21270;&#32500;&#24230;&#12290;&#36890;&#36807;&#23545;&#20845;&#20010;&#25991;&#21270;&#32500;&#24230;&#21644;&#19971;&#20010;&#39046;&#22495;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#21270;&#29305;&#24449;&#12289;&#19968;&#33268;&#24615;&#21644;&#24046;&#24322;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#26102;&#34701;&#20837;&#25991;&#21270;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25193;&#23637;&#26174;&#33879;&#22686;&#24378;&#20854;&#33021;&#21147;&#65292;&#23545;&#30830;&#20445;&#20854;&#36127;&#36131;&#20219;&#21644;&#20262;&#29702;&#20351;&#29992;&#30340;&#23545;&#40784;&#38382;&#39064;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#23545;&#40784;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26222;&#19990;&#20215;&#20540;&#35266;&#65288;&#22914;HHH&#21407;&#21017;&#65289;&#19978;&#65292;&#20294;&#25991;&#21270;&#36825;&#19968;&#26412;&#36136;&#19978;&#26159;&#20247;&#22810;&#22810;&#20803;&#21270;&#30340;&#32500;&#24230;&#21364;&#26410;&#24471;&#21040;&#20805;&#20998;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;CDEval&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#25991;&#21270;&#32500;&#24230;&#12290;CDEval&#36890;&#36807;&#32467;&#21512;GPT-4&#30340;&#33258;&#21160;&#29983;&#25104;&#21644;&#20154;&#24037;&#39564;&#35777;&#26500;&#24314;&#65292;&#22312;&#19971;&#20010;&#39046;&#22495;&#28085;&#30422;&#20102;&#20845;&#20010;&#25991;&#21270;&#32500;&#24230;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#39564;&#20026;&#20027;&#27969;LLMs&#30340;&#25991;&#21270;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#27934;&#23519;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#32500;&#24230;&#21644;&#39046;&#22495;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#21644;&#24046;&#24322;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;LLM&#30340;&#24320;&#21457;&#20013;&#25972;&#21512;&#25991;&#21270;&#32771;&#34385;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#20803;&#25991;&#21270;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the scaling of Large Language Models (LLMs) has dramatically enhanced their capabilities, there has been a growing focus on the alignment problem to ensure their responsible and ethical use. While existing alignment efforts predominantly concentrate on universal values such as the HHH principle, the aspect of culture, which is inherently pluralistic and diverse, has not received adequate attention. This work introduces a new benchmark, CDEval, aimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by incorporating both GPT-4's automated generation and human verification, covering six cultural dimensions across seven domains. Our comprehensive experiments provide intriguing insights into the culture of mainstream LLMs, highlighting both consistencies and variations across different dimensions and domains. The findings underscore the importance of integrating cultural considerations in LLM development, particularly for applications in diverse cultural settings. Thr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10747</link><description>&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;:&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22768;&#38899;&#32447;&#32034;&#26469;&#35782;&#21035;&#20010;&#20307;&#34920;&#36798;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#25152;&#26377;&#27169;&#24577;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#36801;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65292;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#20445;&#30041;&#37325;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#27169;&#24577;&#30340;&#26368;&#22823;&#20449;&#24687;&#65292;&#29992;&#20110;&#24773;&#24863;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#20855;&#26377;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
&lt;/p&gt;</description></item><item><title>DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10471</link><description>&lt;p&gt;
DeepEdit: &#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#24335;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10471
&lt;/p&gt;
&lt;p&gt;
DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#35270;&#20026;&#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeepEdit&#65288;&#22522;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#28176;&#36827;&#24335;&#35299;&#30721;&#30693;&#35782;&#32534;&#36753;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12289;&#38382;&#39064;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#26469;&#25913;&#36827;&#30693;&#35782;&#32534;&#36753;&#12290;DeepEdit&#21487;&#28789;&#27963;&#24212;&#29992;&#20110;&#25152;&#26377;&#40657;&#30418;LLMs&#65306;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#34920;&#31034;&#25110;&#36755;&#20986;&#35789;&#27719;&#20998;&#24067;&#12290;DeepEdit&#36880;&#27493;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#20462;&#25913;LLMs&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#36755;&#20986;&#23545;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#12290;&#22312;&#30693;&#35782;&#32534;&#36753;&#26041;&#38754;&#65292;DeepEdit&#22312;&#25511;&#21046;LLMs&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;MQuaKE&#19978;&#65292;DeepEdit&#22312;&#23450;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20316;&#20026;&#23454;&#29616;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#26356;&#25509;&#36817;&#21442;&#32771;&#20551;&#35774;&#30340;&#36923;&#36753;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#35266;&#23519;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLF-KG&#65289;&#65292;&#26368;&#23567;&#21270;&#35266;&#23519;&#32467;&#26524;&#19982;&#32467;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2312.15643</link><description>&lt;p&gt;
&#36890;&#36807;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#25512;&#36827;&#35825;&#23548;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation. (arXiv:2312.15643v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15643
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20316;&#20026;&#23454;&#29616;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#26356;&#25509;&#36817;&#21442;&#32771;&#20551;&#35774;&#30340;&#36923;&#36753;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#35266;&#23519;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLF-KG&#65289;&#65292;&#26368;&#23567;&#21270;&#35266;&#23519;&#32467;&#26524;&#19982;&#32467;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35825;&#23548;&#25512;&#29702;&#26159;&#36890;&#36807;&#20570;&#20986;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#26469;&#35299;&#37322;&#35266;&#23519;&#32467;&#26524;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#20351;&#29992;&#30693;&#35782;&#36827;&#34892;&#35299;&#37322;&#65292;&#20294;&#23558;&#35825;&#23548;&#25512;&#29702;&#19982;&#32467;&#26500;&#21270;&#30693;&#35782;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#65289;&#32467;&#21512;&#20351;&#29992;&#30340;&#26041;&#27861;&#20173;&#28982;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20316;&#20026;&#23454;&#29616;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#22797;&#26434;&#30340;&#36923;&#36753;&#20551;&#35774;&#65292;&#20197;&#35299;&#37322;&#19968;&#32452;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32463;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#26356;&#25509;&#36817;&#21442;&#32771;&#20551;&#35774;&#30340;&#36923;&#36753;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#24403;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#35266;&#23519;&#32467;&#26524;&#26102;&#65292;&#36825;&#31181;&#35757;&#32451;&#30446;&#26631;&#24182;&#19981;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#20551;&#35774;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLF-KG&#65289;&#65292;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#35266;&#23519;&#32467;&#26524;&#19982;&#32467;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although many applications require the use of knowledge for explanations, the utilization of abductive reasoning in conjunction with structured knowledge, such as a knowledge graph, remains largely unexplored. To fill this gap, this paper introduces the task of complex logical hypothesis generation, as an initial step towards abductive logical reasoning with KG. In this task, we aim to generate a complex logical hypothesis so that it can explain a set of observations. We find that the supervised trained generative model can generate logical hypotheses that are structurally closer to the reference hypothesis. However, when generalized to unseen observations, this training objective does not guarantee better hypothesis generation. To address this, we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#26159;&#21542;&#33021;&#22815;&#30830;&#23450;&#20004;&#20010;SQL&#26597;&#35810;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#24110;&#21161;LLM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2312.10321</link><description>&lt;p&gt;
LLM-SQL-Solver: LLM&#33021;&#22815;&#30830;&#23450;SQL&#31561;&#20215;&#20851;&#31995;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?. (arXiv:2312.10321v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#26159;&#21542;&#33021;&#22815;&#30830;&#23450;&#20004;&#20010;SQL&#26597;&#35810;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#24110;&#21161;LLM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#26029;&#20004;&#20010;SQL&#26597;&#35810;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#26159;&#25968;&#25454;&#31649;&#29702;&#21644;SQL&#29983;&#25104;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65288;&#21363;&#65292;&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#30340;SQL&#26597;&#35810;&#30340;&#36136;&#37327;&#65289;&#12290;&#34429;&#28982;&#30740;&#31350;&#30028;&#22810;&#24180;&#26469;&#19968;&#30452;&#22312;&#32771;&#34385;SQL&#30340;&#31561;&#20215;&#24615;&#65292;&#20294;&#23427;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#27809;&#26377;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#12289;&#38382;&#31572;&#21644;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;SQL&#26597;&#35810;&#30340;&#31561;&#20215;&#24615;&#65288;&#35821;&#20041;&#31561;&#20215;&#21644;&#23485;&#26494;&#31561;&#20215;&#65289;&#12290;&#20026;&#20102;&#24110;&#21161;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25552;&#31034;&#25216;&#26415;&#65306;Miniature &amp; Mull&#21644;Explain &amp; Compare&#12290;&#21069;&#19968;&#31181;&#25216;&#26415;&#34987;&#29992;&#20110;&#35780;&#20272;&#35821;&#20041;&#31561;&#20215;&#24615;&#65292;&#23427;&#35201;&#27714;LLMs&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#24211;&#23454;&#20363;&#19978;&#25191;&#34892;&#26597;&#35810;&#65292;&#28982;&#21518;&#25506;&#32034;&#26159;&#21542;&#23384;&#22312;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Judging the equivalence between two SQL queries is a fundamental problem with many practical applications in data management and SQL generation (i.e., evaluating the quality of generated SQL queries in text-to-SQL task). While the research community has reasoned about SQL equivalence for decades, it poses considerable difficulties and no complete solutions exist. Recently, Large Language Models (LLMs) have shown strong reasoning capability in conversation, question answering and solving mathematics challenges. In this paper, we study if LLMs can be used to determine the equivalence between SQL queries under two notions of SQL equivalence (semantic equivalence and relaxed equivalence). To assist LLMs in generating high quality responses, we present two prompting techniques: Miniature &amp; Mull and Explain &amp; Compare. The former technique is used to evaluate the semantic equivalence in which it asks LLMs to execute a query on a simple database instance and then explore if a counterexample ex
&lt;/p&gt;</description></item><item><title>CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08753</link><description>&lt;p&gt;
CompA: &#35299;&#20915;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08753
&lt;/p&gt;
&lt;p&gt;
CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#30340;&#22522;&#26412;&#29305;&#24615;&#26159;&#20854;&#32452;&#21512;&#24615;&#12290;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;CLAP&#65289;&#35757;&#32451;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#33021;&#22815;&#23398;&#20064;&#38899;&#39057;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#12289;&#38899;&#39057;&#26816;&#32034;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26377;&#25928;&#25191;&#34892;&#32452;&#21512;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#36824;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CompA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#30495;&#23454;&#19990;&#30028;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;CompA-order&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#25110;&#21457;&#29983;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#32780;CompA-attribute&#35780;&#20272;&#22768;&#38899;&#20107;&#20214;&#30340;&#23646;&#24615;&#32465;&#23450;&#12290;&#27599;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#21253;&#21547;&#20004;&#20010;&#38899;&#39057;-&#26631;&#39064;&#23545;&#65292;&#20854;&#20013;&#20004;&#20010;&#38899;&#39057;&#20855;&#26377;&#30456;&#21516;&#30340;&#22768;&#38899;&#20107;&#20214;&#65292;&#20294;&#32452;&#21512;&#26041;&#24335;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.07059</link><description>&lt;p&gt;
DKEC: &#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#30005;&#23376;&#30149;&#21382;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#39046;&#22495;&#30340;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#32463;&#24120;&#38754;&#20020;&#38271;&#23614;&#26631;&#31614;&#20998;&#24067;&#65292;&#21363;&#32597;&#35265;&#31867;&#21035;&#30340;&#35757;&#32451;&#26679;&#26412;&#23569;&#20110;&#39057;&#32321;&#31867;&#21035;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#23618;&#27425;&#21270;&#26631;&#31614;&#32467;&#26500;&#26469;&#25214;&#21040;&#37325;&#35201;&#29305;&#24449;&#65292;&#20294;&#22823;&#22810;&#25968;&#24573;&#30053;&#20102;&#20174;&#21307;&#23398;&#25351;&#21335;&#20013;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#22686;&#24378;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#30340;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#21019;&#26032;&#28857;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22522;&#20110;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#32467;&#21512;&#24322;&#26500;&#22270;&#21644;&#39046;&#22495;&#26412;&#20307;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#65288;2&#65289;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#30456;&#20284;&#24615;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DKEC&#65306;RAA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#24613;&#25937;&#26381;&#21153;&#65288;EMS&#65289;&#20107;&#20214;&#30340;4,417&#20010;&#24739;&#32773;&#25252;&#29702;&#25253;&#21578;&#30340;&#25910;&#38598;&#65292;&#21644;&#26469;&#33258;53898&#25253;&#21578;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label text classification (MLTC) tasks in the medical domain often face long-tail label distribution, where rare classes have fewer training samples than frequent classes. Although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;HJCL&#65289;&#65292;&#29992;&#20110;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#21450;&#31934;&#24515;&#26500;&#24314;&#25209;&#27425;&#26469;&#22788;&#29702;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;HMTC&#20013;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05128</link><description>&lt;p&gt;
&#23454;&#20363;&#21644;&#26631;&#31614;: &#38024;&#23545;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification. (arXiv:2310.05128v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;HJCL&#65289;&#65292;&#29992;&#20110;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#21450;&#31934;&#24515;&#26500;&#24314;&#25209;&#27425;&#26469;&#22788;&#29702;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;HMTC&#20013;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65288;HMTC&#65289;&#26088;&#22312;&#21033;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#36817;&#26399;&#20851;&#20110;HMTC&#30340;&#26041;&#27861;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#20197;&#21322;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#25991;&#26412;&#21644;&#26631;&#31614;&#23884;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#26045;&#21152;&#36807;&#24230;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26679;&#26412;&#30340;&#29983;&#25104;&#24448;&#24448;&#24341;&#20837;&#22122;&#22768;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#21516;&#19968;&#25209;&#27425;&#20013;&#30456;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26041;&#27861;&#26159;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#26631;&#31614;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\textbf{HJCL}$&#30340;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22635;&#34917;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;HMTC&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#20180;&#32454;&#26500;&#36896;&#25209;&#27425;&#26469;&#28385;&#36275;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an over-constrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to this issue is supervised contrastive learning, but it remains an underexplored topic in HMTC due to its complex structured labels. To overcome this challenge, we propose $\textbf{HJCL}$, a $\textbf{H}$ierarchy-aware $\textbf{J}$oint Supervised $\textbf{C}$ontrastive $\textbf{L}$earning method that bridges the gap between supervised contrastive learning and HMTC. Specifically, we employ both instance-wise and label-wise contrastive learning techniques and carefully construct batches to fulfill the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#35770;&#25991;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#35780;&#23457;&#20154;&#21592;&#30340;&#31034;&#20363;&#35780;&#20215;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.03304</link><description>&lt;p&gt;
&#23398;&#20064;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Learning Personalized Story Evaluation. (arXiv:2310.03304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#35770;&#25991;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#35780;&#23457;&#20154;&#21592;&#30340;&#31034;&#20363;&#35780;&#20215;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#26816;&#32034;&#31561;&#26356;&#23458;&#35266;&#30340;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#35780;&#20272;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#34920;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#21253;&#25324;&#65288;1&#65289;&#25968;&#25454;&#27745;&#26579;&#65307;&#65288;2&#65289;&#22810;&#32500;&#35780;&#20272;&#26631;&#20934;&#65307;&#20197;&#21450;&#65288;3&#65289;&#26469;&#33258;&#35780;&#23457;&#20154;&#21592;&#20010;&#20154;&#20559;&#22909;&#30340;&#20027;&#35266;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#19968;&#20010;&#26080;&#27745;&#26579;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#35780;&#20272;&#20013;&#24314;&#27169;&#20010;&#24615;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24403;&#30340;&#21311;&#21517;&#21270;&#21644;&#26032;&#30340;&#20010;&#24615;&#21270;&#26631;&#31614;&#65292;&#37325;&#26032;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Per-MPST&#21644;Per-DOC&#29992;&#20110;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#25925;&#20107;&#35780;&#20272;&#27169;&#22411;PERSE&#26469;&#25512;&#27979;&#35780;&#23457;&#20154;&#21592;&#30340;&#20559;&#22909;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#26576;&#20010;&#35780;&#23457;&#20154;&#21592;&#30340;&#19968;&#20123;&#31034;&#20363;&#35780;&#20215;&#65292;PERSE&#21487;&#20197;&#39044;&#27979;&#35813;&#35780;&#23457;&#20154;&#21592;&#22312;&#26032;&#30340;&#24773;&#33410;&#19978;&#30340;&#35814;&#32454;&#35780;&#23457;&#25110;&#32454;&#31890;&#24230;&#27604;&#36739;&#65288;&#22914;&#36259;&#21619;&#24615;&#21644;&#24778;&#21916;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have shown impressive results for more objective tasks such as QA and retrieval, it remains nontrivial to evaluate their performance on open-ended text generation for reasons including (1) data contamination; (2) multi-dimensional evaluation criteria; and (3) subjectiveness stemming from reviewers' personal preferences. To address such issues, we propose to model personalization in an uncontaminated open-ended generation assessment. We create two new datasets Per-MPST and Per-DOC for personalized story evaluation, by re-purposing existing datasets with proper anonymization and new personalized labels. We further develop a personalized story evaluation model PERSE to infer reviewer preferences and provide a personalized evaluation. Specifically, given a few exemplary reviews from a particular reviewer, PERSE predicts either a detailed review or fine-grained comparison in several aspects (such as interestingness and surprise) for that reviewer on a new 
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.15857</link><description>&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
A Survey on Image-text Multimodal Models. (arXiv:2309.15857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15857
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20854;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#25104;&#20026;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#23548;&#33268;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#35770;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24403;&#21069;&#29366;&#24577;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#20215;&#20540;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#21457;&#23637;&#37324;&#31243;&#30865;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#23427;&#20204;&#30340;&#21457;&#23637;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#65292;&#22522;&#20110;&#23427;&#20204;&#34987;&#24341;&#20837;&#30340;&#26102;&#38388;&#21644;&#23545;&#23398;&#31185;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20219;&#21153;&#22312;&#23398;&#26415;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#26222;&#21450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#19982;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#22411;&#30456;&#20851;&#30340;&#20219;&#21153;&#21010;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#31867;&#22411;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#38416;&#26126;&#20102;&#27599;&#20010;&#31867;&#21035;&#20869;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#20173;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amidst the evolving landscape of artificial intelligence, the convergence of visual and textual information has surfaced as a crucial frontier, leading to the advent of image-text multimodal models. This paper provides a comprehensive review of the evolution and current state of image-text multimodal models, exploring their application value, challenges, and potential research trajectories. Initially, we revisit the basic concepts and developmental milestones of these models, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline. Furthermore, based on the tasks' significance and prevalence in the academic landscape, we propose a categorization of the tasks associated with image-text multimodal models into five major types, elucidating the recent progress and key technologies within each category. Despite the remarkable accomplishments of these models, numerous challenges a
&lt;/p&gt;</description></item><item><title>CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11143</link><description>&lt;p&gt;
CoT-BERT: &#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11143
&lt;/p&gt;
&lt;p&gt;
CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#36755;&#20837;&#21477;&#23376;&#36716;&#21270;&#20026;&#23500;&#21547;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#30340;&#22266;&#23450;&#38271;&#24230;&#21521;&#37327;&#65292;&#21516;&#26102;&#28040;&#38500;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#25512;&#21160;&#19979;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#36712;&#36857;&#20013;&#65292;&#20173;&#28982;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#24605;&#32500;&#38142;&#26465;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#20026;&#20102;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20013;&#30340;&#28508;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21477;&#23376;&#34920;&#31034;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#29702;&#35299;&#21644;&#25688;&#35201;&#12290;&#38543;&#21518;&#65292;&#21518;&#19968;&#38454;&#27573;&#30340;&#36755;&#20986;&#34987;&#21033;&#29992;&#20026;&#36755;&#20837;&#21477;&#23376;&#30340;&#21521;&#37327;&#21270;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#21644;&#27169;&#26495;&#21435;&#22122;&#25216;&#26415;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#12290;&#20005;&#26684;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CoT-BERT&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#24180;&#40836;&#12289;&#32654;&#20029;&#12289;&#26426;&#26500;&#21644;&#22269;&#31821;&#31561;&#23569;&#30740;&#31350;&#20294;&#20173;&#28982;&#37325;&#35201;&#30340;&#32500;&#24230;&#19978;&#30340;&#20559;&#35265;&#65292;&#36890;&#36807;&#34913;&#37327;&#22312;&#31038;&#20250;&#32676;&#20307;&#21644;&#19981;&#30456;&#20851;&#30340;&#27491;&#36127;&#23646;&#24615;&#20043;&#38388;&#20570;&#20986;&#30340;&#24494;&#22937;&#30456;&#20851;&#20915;&#31574;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#29305;&#23450;&#31038;&#20250;&#32676;&#20307;&#19978;&#23384;&#22312;&#31867;&#20284;&#20110;&#8220;&#32654;&#20029;&#21363;&#21892;&#8221;&#30340;&#24191;&#27867;&#27491;&#38754;&#25110;&#36127;&#38754;&#24577;&#24230;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2309.08902</link><description>&lt;p&gt;
&#35843;&#26597;LLMs&#20013;&#26356;&#24494;&#22937;&#30340;&#20559;&#35265;&#65306;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24180;&#40836;&#20027;&#20041;&#12289;&#32654;&#20029;&#12289;&#26426;&#26500;&#21644;&#22269;&#31821;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models. (arXiv:2309.08902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#24180;&#40836;&#12289;&#32654;&#20029;&#12289;&#26426;&#26500;&#21644;&#22269;&#31821;&#31561;&#23569;&#30740;&#31350;&#20294;&#20173;&#28982;&#37325;&#35201;&#30340;&#32500;&#24230;&#19978;&#30340;&#20559;&#35265;&#65292;&#36890;&#36807;&#34913;&#37327;&#22312;&#31038;&#20250;&#32676;&#20307;&#21644;&#19981;&#30456;&#20851;&#30340;&#27491;&#36127;&#23646;&#24615;&#20043;&#38388;&#20570;&#20986;&#30340;&#24494;&#22937;&#30456;&#20851;&#20915;&#31574;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#29305;&#23450;&#31038;&#20250;&#32676;&#20307;&#19978;&#23384;&#22312;&#31867;&#20284;&#20110;&#8220;&#32654;&#20029;&#21363;&#21892;&#8221;&#30340;&#24191;&#27867;&#27491;&#38754;&#25110;&#36127;&#38754;&#24577;&#24230;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#36234;&#26469;&#36234;&#24378;&#22823;&#24182;&#24191;&#27867;&#29992;&#20110;&#36741;&#21161;&#29992;&#25143;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;&#36825;&#31181;&#20351;&#29992;&#21487;&#33021;&#20250;&#23558;LLM&#20559;&#35265;&#24341;&#20837;&#21040;&#37325;&#35201;&#20915;&#31574;&#20013;&#65292;&#22914;&#25307;&#32856;&#12289;&#20154;&#21592;&#32489;&#25928;&#35780;&#20272;&#21644;&#21009;&#20107;&#21028;&#20915;&#12290;&#22312;NLP&#31995;&#32479;&#20013;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#26041;&#38754;&#30340;&#20559;&#35265;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#29305;&#23450;&#21051;&#26495;&#21360;&#35937;&#30340;&#20559;&#35265;&#65288;&#20363;&#22914;&#65292;&#20122;&#27954;&#20154;&#25797;&#38271;&#25968;&#23398;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#36739;&#23569;&#30740;&#31350;&#20294;&#20173;&#28982;&#37325;&#35201;&#30340;&#32500;&#24230;&#19978;&#30340;&#20559;&#35265;&#65292;&#22914;&#24180;&#40836;&#21644;&#32654;&#20029;&#65292;&#22312;LLMs&#65288;&#29305;&#21035;&#26159;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65289;&#22312;&#31038;&#20250;&#32676;&#20307;&#21644;&#19981;&#30456;&#20851;&#30340;&#27491;&#36127;&#23646;&#24615;&#20043;&#38388;&#20570;&#20986;&#26356;&#24494;&#22937;&#30340;&#30456;&#20851;&#20915;&#31574;&#12290;&#25105;&#20204;&#38382;LLMs&#26159;&#21542;&#23545;&#29305;&#23450;&#31038;&#20250;&#32676;&#20307;&#25345;&#26377;&#24191;&#27867;&#30340;&#27491;&#38754;&#25110;&#36127;&#38754;&#24577;&#24230;&#30340;&#20559;&#35265;&#65292;&#31867;&#20284;&#20110;&#23454;&#39564;&#24515;&#29702;&#23398;&#20013;&#20154;&#20204;&#21457;&#29616;&#30340;&#8220;&#32654;&#20029;&#21363;&#21892;&#8221;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#26495;&#29983;&#25104;&#30340;&#21477;&#23376;&#23436;&#25104;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#35201;&#27714;&#27169;&#22411;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs are increasingly powerful and widely used to assist users in a variety of tasks. This use risks the introduction of LLM biases to consequential decisions such as job hiring, human performance evaluation, and criminal sentencing. Bias in NLP systems along the lines of gender and ethnicity has been widely studied, especially for specific stereotypes (e.g., Asians are good at math). In this paper, we investigate bias along less studied, but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that LLMs (specially autoregressive language models) make between social groups and unrelated positive and negative attributes. We ask whether LLMs hold wide-reaching biases of positive or negative sentiment for specific social groups similar to the ``what is beautiful is good'' bias found in people in experimental psychology. We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attrib
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;&#31038;&#20250;&#31185;&#23398;&#26041;&#27861;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#22312;&#20998;&#26512;&#23186;&#20307;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#35770;&#40511;&#27807;&#30340;&#21487;&#33021;&#26041;&#21521;&#65292;&#21253;&#25324;&#27169;&#22411;&#36879;&#26126;&#24230;&#12289;&#32771;&#34385;&#25991;&#26723;&#22806;&#37096;&#20449;&#24687;&#21644;&#36328;&#25991;&#26723;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.08069</link><description>&lt;p&gt;
&#22312;&#26032;&#38395;&#20998;&#26512;&#20013;&#36830;&#25509;&#21508;&#20010;&#28857;&#65306;&#20851;&#20110;&#23186;&#20307;&#20559;&#35265;&#21644;&#26694;&#26550;&#30340;&#36328;&#23398;&#31185;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots in News Analysis: A Cross-Disciplinary Survey of Media Bias and Framing. (arXiv:2309.08069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;&#31038;&#20250;&#31185;&#23398;&#26041;&#27861;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#22312;&#20998;&#26512;&#23186;&#20307;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#35770;&#40511;&#27807;&#30340;&#21487;&#33021;&#26041;&#21521;&#65292;&#21253;&#25324;&#27169;&#22411;&#36879;&#26126;&#24230;&#12289;&#32771;&#34385;&#25991;&#26723;&#22806;&#37096;&#20449;&#24687;&#21644;&#36328;&#25991;&#26723;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#35265;&#22312;&#26032;&#38395;&#25253;&#36947;&#20013;&#30340;&#34920;&#29616;&#21644;&#24433;&#21709;&#26159;&#31038;&#20250;&#31185;&#23398;&#20960;&#21313;&#24180;&#26469;&#30340;&#26680;&#24515;&#35758;&#39064;&#65292;&#24182;&#19988;&#36817;&#24180;&#26469;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#24110;&#21161;&#25193;&#22823;&#20998;&#26512;&#35268;&#27169;&#25110;&#25552;&#20379;&#33258;&#21160;&#21270;&#31243;&#24207;&#26469;&#35843;&#26597;&#20559;&#35265;&#26032;&#38395;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#20027;&#23548;&#22320;&#20301;&#30340;&#26041;&#27861;&#35770;&#26410;&#33021;&#35299;&#20915;&#29702;&#35770;&#23186;&#20307;&#30740;&#31350;&#20013;&#28041;&#21450;&#30340;&#22797;&#26434;&#38382;&#39064;&#21644;&#24433;&#21709;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#31038;&#20250;&#31185;&#23398;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#29992;&#20110;&#20998;&#26512;&#23186;&#20307;&#20559;&#35265;&#30340;&#20856;&#22411;&#20219;&#21153;&#24418;&#24335;&#12289;&#26041;&#27861;&#21644;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#29702;&#35770;&#27169;&#22411;&#21644;&#39044;&#27979;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#35780;&#20272;&#20043;&#38388;&#40511;&#27807;&#30340;&#21487;&#33021;&#26041;&#21521;&#12290;&#36825;&#20123;&#21253;&#25324;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12289;&#32771;&#34385;&#25991;&#26723;&#22806;&#37096;&#20449;&#24687;&#12289;&#20197;&#21450;&#36328;&#25991;&#26723;&#25512;&#29702;&#32780;&#38750;&#21333;&#26631;&#31614;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
The manifestation and effect of bias in news reporting have been central topics in the social sciences for decades, and have received increasing attention in the NLP community recently. While NLP can help to scale up analyses or contribute automatic procedures to investigate the impact of biased news in society, we argue that methodologies that are currently dominant fall short of addressing the complex questions and effects addressed in theoretical media studies. In this survey paper, we review social science approaches and draw a comparison with typical task formulations, methods, and evaluation metrics used in the analysis of media bias in NLP. We discuss open questions and suggest possible directions to close identified gaps between theory and predictive models, and their evaluation. These include model transparency, considering document-external information, and cross-document reasoning rather than single-label assignment.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;ACE&#25968;&#25454;&#38598;&#20197;&#21450;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#25968;&#25454;&#22495;&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.06219</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#38142;&#25509;&#39044;&#27979;&#22312;&#29983;&#27963;&#26041;&#24335;vlog&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;
&lt;/p&gt;
&lt;p&gt;
Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction. (arXiv:2309.06219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;ACE&#25968;&#25454;&#38598;&#20197;&#21450;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#25968;&#25454;&#22495;&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#21363;&#30830;&#23450;&#20004;&#20010;&#20154;&#31867;&#21160;&#20316;&#26159;&#21542;&#21487;&#20197;&#22312;&#21516;&#19968;&#26102;&#38388;&#38388;&#38548;&#20869;&#20849;&#29616;&#12290;&#25105;&#20204;&#21019;&#24314;&#24182;&#20844;&#24320;&#20102;ACE&#65288;Action Co-occurrencE&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#32422;12k&#20010;&#20849;&#29616;&#30340;&#35270;&#35273;&#21160;&#20316;&#23545;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#35270;&#39057;&#29255;&#27573;&#32452;&#25104;&#30340;&#22823;&#22411;&#22270;&#24418;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#26469;&#33258;&#21160;&#25512;&#26029;&#20004;&#20010;&#21160;&#20316;&#26159;&#21542;&#20849;&#29616;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#24418;&#29305;&#21035;&#36866;&#21512;&#25429;&#25417;&#20154;&#31867;&#21160;&#20316;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#25152;&#23398;&#20064;&#30340;&#22270;&#24418;&#34920;&#31034;&#23545;&#20110;&#25105;&#20204;&#30340;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#22495;&#20013;&#25429;&#25417;&#21040;&#26032;&#39062;&#32780;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;ACE&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/MichiganNLP/vlog_action_co-occurrence&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the task of automatic human action co-occurrence identification, i.e., determine whether two human actions can co-occur in the same interval of time. We create and make publicly available the ACE (Action Co-occurrencE) dataset, consisting of a large graph of ~12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring. We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains. The ACE dataset and the code introduced in this paper are publicly available at https://github.com/MichiganNLP/vlog_action_co-occurrence.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#30340;&#24555;&#36895;&#23567;&#22411;BERT&#27169;&#22411;&#12290;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#36739;&#23567;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;RoBERTa&#27169;&#22411;&#30456;&#24403;&#30340;95%&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12568</link><description>&lt;p&gt;
&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#30340;&#23567;&#22411;&#24555;&#36895;BERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Small and Fast BERT for Chinese Medical Punctuation Restoration. (arXiv:2308.12568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12568
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#30340;&#24555;&#36895;&#23567;&#22411;BERT&#27169;&#22411;&#12290;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#36739;&#23567;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;RoBERTa&#27169;&#22411;&#30456;&#24403;&#30340;95%&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#21548;&#20889;&#20013;&#65292;&#27809;&#26377;&#26126;&#30830;&#26631;&#28857;&#31526;&#21495;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#23548;&#33268;&#20102;&#23545;&#21548;&#20889;&#25253;&#21578;&#30340;&#35823;&#35299;&#12290;&#20026;&#20102;&#20351;&#29992;ASR&#25552;&#20379;&#31934;&#30830;&#21644;&#26131;&#25026;&#30340;&#20020;&#24202;&#25253;&#21578;&#65292;&#38656;&#35201;&#36827;&#34892;&#33258;&#21160;&#26631;&#28857;&#20462;&#22797;&#12290;&#32771;&#34385;&#21040;&#23454;&#38469;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#24555;&#36895;&#36731;&#37327;&#32423;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65288;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#65289;&#26469;&#25552;&#28860;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#26631;&#28857;&#20462;&#22797;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25552;&#28860;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;RoBERTa&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;10%&#30340;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;95%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In clinical dictation, utterances after automatic speech recognition (ASR) without explicit punctuation marks may lead to the misunderstanding of dictated reports. To give a precise and understandable clinical report with ASR, automatic punctuation restoration is required. Considering a practical scenario, we propose a fast and light pre-trained model for Chinese medical punctuation restoration based on 'pretraining and fine-tuning' paradigm. In this work, we distill pre-trained models by incorporating supervised contrastive learning and a novel auxiliary pre-training task (Punctuation Mark Prediction) to make it well-suited for punctuation restoration. Our experiments on various distilled models reveal that our model can achieve 95% performance while 10% model size relative to state-of-the-art Chinese RoBERTa.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12215</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#26041;&#38754;&#30340;&#25361;&#25112;&#65306;&#19968;&#20010;&#38024;&#23545;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#26816;&#26597;&#20102;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#19978;&#23398;&#26415;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#12290;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#20013;270&#31687;&#24191;&#21463;&#24341;&#29992;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#33258;&#21160;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#30340;&#25991;&#29486;&#31995;&#32479;&#21270;&#65292;&#24182;&#23545;&#23376;&#38598;&#20013;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#12289;&#35774;&#35745;&#22833;&#35823;&#12289;&#21487;&#22797;&#29616;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#25991;&#29486;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#36825;&#23545;&#25152;&#22768;&#31216;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#26816;&#27979;&#20219;&#21153;&#36890;&#24120;&#19982;&#22312;&#32447;&#26381;&#21153;&#30495;&#27491;&#38754;&#20020;&#30340;&#25361;&#25112;&#26377;&#26412;&#36136;&#19978;&#30340;&#21306;&#21035;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#36890;&#24120;&#19981;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#65292;&#32780;&#19988;&#35780;&#20272;&#24448;&#24448;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#24456;&#24046;&#12290;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#25345;&#32493;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#36830;&#32493;&#20449;&#24687;&#26816;&#32034;&#30340;&#22810;&#20027;&#39064;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25345;&#32493;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#24182;&#25552;&#39640;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08378</link><description>&lt;p&gt;
&#25512;&#36827;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#25345;&#32493;&#32456;&#36523;&#23398;&#20064;&#65306;&#23450;&#20041;&#12289;&#25968;&#25454;&#38598;&#12289;&#26694;&#26550;&#21644;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Advancing continual lifelong learning in neural information retrieval: definition, dataset, framework, and empirical evaluation. (arXiv:2308.08378v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#25345;&#32493;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#36830;&#32493;&#20449;&#24687;&#26816;&#32034;&#30340;&#22810;&#20027;&#39064;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25345;&#32493;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#24182;&#25552;&#39640;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#25351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23398;&#20064;&#21644;&#36866;&#24212;&#26032;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#19981;&#24433;&#21709;&#20854;&#22312;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#26377;&#22810;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#20173;&#32570;&#20047;&#26126;&#30830;&#30340;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#19988;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#20856;&#22411;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#25345;&#32493;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#36830;&#32493;&#20449;&#24687;&#26816;&#32034;&#30340;&#22810;&#20027;&#39064;&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25345;&#32493;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#26694;&#26550;&#65292;&#21253;&#25324;&#20856;&#22411;&#26816;&#32034;&#27169;&#22411;&#21644;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#25104;&#21151;&#22320;&#38450;&#27490;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#25552;&#39640;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#23884;&#20837;&#30340;&#26816;&#32034;&#26041;&#24335;&#36739;&#20256;&#32479;&#30340;&#22522;&#20110;&#32034;&#24341;&#30340;&#26816;&#32034;&#26041;&#24335;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#21319;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning refers to the capability of a machine learning model to learn and adapt to new information, without compromising its performance on previously learned tasks. Although several studies have investigated continual learning methods for information retrieval tasks, a well-defined task formulation is still lacking, and it is unclear how typical learning strategies perform in this context. To address this challenge, a systematic task formulation of continual neural information retrieval is presented, along with a multiple-topic dataset that simulates continuous information retrieval. A comprehensive continual neural information retrieval framework consisting of typical retrieval models and continual learning strategies is then proposed. Empirical evaluations illustrate that the proposed framework can successfully prevent catastrophic forgetting in neural information retrieval and enhance performance on previously learned tasks. The results indicate that embedding-based retr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2308.07706</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25506;&#32034;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25972;&#21512;&#25991;&#26412;&#25351;&#23548;&#20197;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#20173;&#28982;&#26159;&#19968;&#20010;&#36827;&#23637;&#26377;&#38480;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#30340;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#22312;&#24320;&#25918;&#39046;&#22495;&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#30452;&#25509;&#24212;&#29992;&#30340;&#38590;&#39064;&#65292;&#38656;&#35201;&#25163;&#21160;&#20171;&#20837;&#25110;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20174;&#22270;&#20687;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#24471;&#33021;&#22815;&#23545;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20197;&#35780;&#20272;&#20854;&#20174;&#24320;&#25918;&#39046;&#22495;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#26410;&#35265;&#22270;&#20687;&#30340;&#22270;&#20687;&#25551;&#36848;&#24341;&#20837;&#20102;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
&lt;/p&gt;</description></item><item><title>mBLIP&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;Vision-LLM&#65292;&#36890;&#36807;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#20363;&#30340;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26041;&#24335;&#33719;&#24471;&#12290;</title><link>http://arxiv.org/abs/2307.06930</link><description>&lt;p&gt;
mBLIP: &#22810;&#35821;&#35328;&#35270;&#35273;-LLM&#30340;&#39640;&#25928;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs. (arXiv:2307.06930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06930
&lt;/p&gt;
&lt;p&gt;
mBLIP&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;Vision-LLM&#65292;&#36890;&#36807;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#20363;&#30340;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26041;&#24335;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22359;&#21270;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;Vision-LLM&#65289;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#19982;&#65288;&#39044;&#35757;&#32451;&#30340;&#65289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#40784;&#65292;&#26159;&#19968;&#31181;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#30340;&#36873;&#25321;&#65292;&#21487;&#20197;&#20195;&#26367;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#32780;&#21518;&#32773;&#23545;&#20110;&#22823;&#22810;&#25968;&#20154;&#26469;&#35828;&#25104;&#26412;&#22826;&#39640;&#12290; Vision-LLM&#23558;LLM&#20107;&#21518;&#26465;&#20214;&#21270;&#20026;&#8220;&#29702;&#35299;&#8221;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#36755;&#20986;&#12290;&#38543;&#30528;&#29616;&#25104;&#30340;&#39640;&#36136;&#37327;&#33521;&#25991;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#20197;&#21450;&#21333;&#35821;&#33521;&#35821;LLM&#30340;&#20016;&#23500;&#24615;&#65292;&#30740;&#31350;&#37325;&#28857;&#24050;&#32463;&#25918;&#22312;&#20165;&#33521;&#25991;&#30340;Vision-LLM&#19978;&#12290;&#32780;&#22810;&#35821;&#35328;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#20027;&#35201;&#36890;&#36807;&#26114;&#36149;&#30340;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#33719;&#24471;&#65292;&#36825;&#23548;&#33268;&#20102;&#30456;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#26377;&#38480;&#30340;&#22810;&#35821;&#35328;&#22270;&#20687;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#34917;&#20805;&#20102;&#20165;&#26377;&#25991;&#26412;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;mBLIP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;Vision-LLM&#65292;&#25105;&#20204;&#20197;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26041;&#24335;&#33719;&#24471;&#65292;&#20165;&#20351;&#29992;&#20960;&#30334;&#19975;&#20010;&#35757;&#32451;&#26679;&#20363;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modular vision-language models (Vision-LLMs) align pretrained image encoders with (pretrained) large language models (LLMs), representing a computationally much more efficient alternative to end-to-end training of large vision-language models from scratch, which is prohibitively expensive for most. Vision-LLMs instead post-hoc condition LLMs to `understand' the output of an image encoder. With the abundance of readily available high-quality English image-text data as well as monolingual English LLMs, the research focus has been on English-only Vision-LLMs. Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora. In this work, we present mBLIP, the first multilingual Vision-LLM, which we obtain in a computationally efficient manner -- on consumer hardware using only a few million training examples -- by 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32763;&#35793;&#21644;&#27880;&#37322;&#34701;&#21512;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#26412;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#36890;&#36807;TransFusion&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#24378;&#22823;&#30340;&#39044;&#27979;&#65292;&#19988;&#22312;&#20004;&#20010;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19968;&#33268;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.13582</link><description>&lt;p&gt;
&#36890;&#36807;&#32763;&#35793;&#21644;&#27880;&#37322;&#34701;&#21512;&#25913;&#36827;&#20302;&#36164;&#28304;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Better Low-Resource Entity Recognition Through Translation and Annotation Fusion. (arXiv:2305.13582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32763;&#35793;&#21644;&#27880;&#37322;&#34701;&#21512;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#26412;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#36890;&#36807;TransFusion&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#24378;&#22823;&#30340;&#39044;&#27979;&#65292;&#19988;&#22312;&#20004;&#20010;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19968;&#33268;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#23454;&#29616;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#36716;&#31227;&#33267;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#65292;&#36890;&#24120;&#34920;&#29616;&#20986;&#24615;&#33021;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26410;&#34987;&#20805;&#20998;&#35757;&#32451;&#25110;&#26410;&#21253;&#21547;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#12290;&#21463;&#36825;&#20123;&#27169;&#22411;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#20248;&#31168;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32763;&#35793;&#21644;&#34701;&#21512;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#26412;&#32763;&#35793;&#25104;&#39640;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#23558;&#27880;&#37322;&#34701;&#21512;&#22238;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransFusion&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#35757;&#32451;&#29992;&#20110;&#34701;&#21512;&#26469;&#33258;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#20197;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#36827;&#34892;&#24378;&#22823;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#25968;&#25454;&#38598;MasakhaNER2.0&#21644;LORELEI NER&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#36328;&#35821;&#35328;NER&#22522;&#32447;&#30340;&#19968;&#33268;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained multilingual language models have enabled significant advancements in cross-lingual transfer. However, these models often exhibit a performance disparity when transferring from high-resource languages to low-resource languages, especially for languages that are underrepresented or not in the pre-training data. Motivated by the superior performance of these models on high-resource languages compared to low-resource languages, we introduce a Translation-and-fusion framework, which translates low-resource language text into a high-resource language for annotation using fully supervised models before fusing the annotations back into the low-resource language. Based on this framework, we present TransFusion, a model trained to fuse predictions from a high-resource language to make robust predictions on low-resource languages. We evaluate our methods on two low-resource named entity recognition (NER) datasets, MasakhaNER2.0 and LORELEI NER, covering 25 languages, and show consist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#23558;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#20026;&#26368;&#32456;&#34920;&#31034;&#65292;&#32469;&#36807;&#20013;&#38388;&#30340;Transformer&#35745;&#31639;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20013;&#21487;&#8220;&#31397;&#35270;&#8221;GPT-2&#21644;BERT&#30340;&#26089;&#26399;&#23618;&#34920;&#31034;&#65292;&#26174;&#31034;&#32463;&#24120;&#22312;&#26089;&#26399;&#23618;&#20013;LMs&#24050;&#32463;&#39044;&#27979;&#26368;&#32456;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.09435</link><description>&lt;p&gt;
&#36339;&#36291;&#21040;&#32467;&#35770;&#65306;&#29992;&#32447;&#24615;&#21464;&#25442;&#31616;&#21270;Transformers
&lt;/p&gt;
&lt;p&gt;
Jump to Conclusions: Short-Cutting Transformers With Linear Transformations. (arXiv:2303.09435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#23558;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#20026;&#26368;&#32456;&#34920;&#31034;&#65292;&#32469;&#36807;&#20013;&#38388;&#30340;Transformer&#35745;&#31639;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20013;&#21487;&#8220;&#31397;&#35270;&#8221;GPT-2&#21644;BERT&#30340;&#26089;&#26399;&#23618;&#34920;&#31034;&#65292;&#26174;&#31034;&#32463;&#24120;&#22312;&#26089;&#26399;&#23618;&#20013;LMs&#24050;&#32463;&#39044;&#27979;&#26368;&#32456;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#22312;&#27599;&#20010;&#23618;&#27425;&#19978;&#37117;&#21019;&#24314;&#20854;&#36755;&#20837;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#20294;&#21482;&#20351;&#29992;&#26368;&#32456;&#23618;&#30340;&#34920;&#31034;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#30340;&#20869;&#37096;&#20915;&#31574;&#36807;&#31243;&#21644;&#20013;&#38388;&#34920;&#31034;&#30340;&#23454;&#29992;&#24615;&#21464;&#24471;&#27169;&#31946;&#19981;&#28165;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#19968;&#28857;&#65292;&#21487;&#20197;&#23558;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#20026;&#26368;&#32456;&#34920;&#31034;&#65292;&#32469;&#36807;&#20013;&#38388;&#30340;Transformer&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#26469;&#36827;&#34892;&#36825;&#31181;&#36716;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#27604;&#30446;&#21069;&#27969;&#34892;&#30340;&#22312;&#26368;&#32456;&#23618;&#31354;&#38388;&#20013;&#26816;&#26597;&#25152;&#26377;&#23618;&#30340;&#38544;&#34255;&#34920;&#31034;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#30340;&#36817;&#20284;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#8220;&#31397;&#35270;&#8221;GPT-2&#21644;BERT&#30340;&#26089;&#26399;&#23618;&#34920;&#31034;&#65292;&#26174;&#31034;&#32463;&#24120;&#22312;&#26089;&#26399;&#23618;&#20013;LMs&#24050;&#32463;&#39044;&#27979;&#26368;&#32456;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#26368;&#36817;&#30340;&#26089;&#26399;&#36864;&#20986;&#31574;&#30053;&#30340;&#23454;&#29992;&#24615;&#65292;&#34920;&#26126;&#24403;&#26088;&#22312;&#8230;&#8230;(&#21407;&#25991;&#25130;&#27490;)
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models (LMs) create hidden representations of their inputs at every layer, but only use final-layer representations for prediction. This obscures the internal decision-making process of the model and the utility of its intermediate representations. One way to elucidate this is to cast the hidden representations as final representations, bypassing the transformer computation in-between. In this work, we suggest a simple method for such casting, by using linear transformations. We show that our approach produces more accurate approximations than the prevailing practice of inspecting hidden representations from all layers in the space of the final layer. Moreover, in the context of language modeling, our method allows "peeking" into early layer representations of GPT-2 and BERT, showing that often LMs already predict the final output in early layers. We then demonstrate the practicality of our method to recent early exit strategies, showing that when aiming, for
&lt;/p&gt;</description></item></channel></rss>