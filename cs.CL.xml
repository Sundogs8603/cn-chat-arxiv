<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#24341;&#20837;VQAScore&#65292;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;</title><link>https://arxiv.org/abs/2404.01291</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#19982;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text-to-Visual Generation with Image-to-Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01291
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;VQAScore&#65292;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#32508;&#21512;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VQAScore&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36890;&#36807;&#35745;&#31639;&#23545;&#31616;&#21333;&#30340;&#8220;&#36825;&#24133;&#22270;&#34920;&#29616;&#20986;&#20102;'{&#25991;&#26412;}'&#21527;&#65311;&#8221;&#38382;&#39064;&#30340;&#8220;&#26159;&#8221;&#31572;&#26696;&#30340;&#27010;&#29575;&#26469;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#12290;&#23613;&#31649;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#26356;&#31616;&#21333;&#65292;&#20294;&#20351;&#29992;&#29616;&#25104;&#27169;&#22411;&#35745;&#31639;&#30340;VQAScore&#22312;&#35768;&#22810;&#65288;8&#20010;&#65289;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01291v1 Announce Type: cross  Abstract: Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#22522;&#30784;&#30340;&#26032;&#26694;&#26550;P2G&#65292;&#36890;&#36807;&#21033;&#29992;MLLMs&#30340;&#24037;&#20855;&#20351;&#29992;&#28508;&#21147;&#21644;&#19987;&#23478;&#20195;&#29702;&#23454;&#29616;&#23545;&#22270;&#20687;&#20851;&#38190;&#35270;&#35273;&#21644;&#25991;&#26412;&#23545;&#35937;&#30340;&#21363;&#26102;&#30830;&#23450;&#24615;&#22522;&#30784;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#24847;&#35782;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.19322</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25512;&#29702;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#22522;&#30784;&#30340;&#26032;&#26694;&#26550;P2G&#65292;&#36890;&#36807;&#21033;&#29992;MLLMs&#30340;&#24037;&#20855;&#20351;&#29992;&#28508;&#21147;&#21644;&#19987;&#23478;&#20195;&#29702;&#23454;&#29616;&#23545;&#22270;&#20687;&#20851;&#38190;&#35270;&#35273;&#21644;&#25991;&#26412;&#23545;&#35937;&#30340;&#21363;&#26102;&#30830;&#23450;&#24615;&#22522;&#30784;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#24847;&#35782;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#30001;&#20110;&#20854;&#22312;&#25351;&#20196;&#36981;&#24490;&#21644;&#25512;&#29702;&#26041;&#38754;&#31361;&#20986;&#30340;&#26032;&#21151;&#33021;&#65292;&#36825;&#20123;&#27169;&#22411;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#35270;&#35273;&#25512;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#21463;&#21040;&#20854;&#38750;&#26080;&#25439;&#22270;&#20687;&#26631;&#35760;&#21270;&#30340;&#38480;&#21046;&#65292;&#22823;&#22810;&#25968;MLLMs&#22312;&#20840;&#38754;&#25429;&#25417;&#25991;&#26412;&#21644;&#23545;&#35937;&#32454;&#33410;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;P2G&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;MLLMs&#20013;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#22522;&#30784;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;P2G&#21033;&#29992;MLLMs&#30340;&#24037;&#20855;&#20351;&#29992;&#28508;&#21147;&#65292;&#21033;&#29992;&#19987;&#23478;&#20195;&#29702;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#30340;&#20851;&#38190;&#35270;&#35273;&#21644;&#25991;&#26412;&#23545;&#35937;&#30340;&#21363;&#26102;&#30830;&#23450;&#24615;&#22522;&#30784;&#65292;&#20174;&#32780;&#36890;&#36807;&#22810;&#27169;&#24335;&#25552;&#31034;&#23454;&#29616;&#26377;&#24847;&#35782;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;P2GB&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;MLLMs&#22312;&#29702;&#35299;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#30340;&#29289;&#20307;&#38388;&#20851;&#31995;&#21644;&#25991;&#26412;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23545;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#30340;&#20840;&#38754;&#23454;&#39564;&#34920;&#26126;&#20102;P2G&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19322v1 Announce Type: cross  Abstract: The surge of Multimodal Large Language Models (MLLMs), given their prominent emergent capabilities in instruction following and reasoning, has greatly advanced the field of visual reasoning. However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images. To address this, we propose P2G, a novel framework for plug-and-play grounding of reasoning in MLLMs. Specifically, P2G exploits the tool-usage potential of MLLMs to employ expert agents to achieve on-the-fly grounding to critical visual and textual objects of image, thus achieving deliberate reasoning via multimodal prompting. We further create P2GB, a benchmark aimed at assessing MLLMs' ability to understand inter-object relationships and text in challenging high-resolution images. Comprehensive experiments on visual reasoning tasks demonstrate the superiority of P2G. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#30340;&#25968;&#23398;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20026;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.18697</link><description>&lt;p&gt;
Invalsi&#22522;&#20934;&#65306;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#30340;&#25968;&#23398;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18697
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#30340;&#25968;&#23398;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20026;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24847;&#22823;&#21033;&#35821;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#26159;&#19968;&#31181;&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#20294;&#30446;&#21069;&#24182;&#27809;&#26377;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#35813;&#35821;&#35328;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#23548;&#33268;&#20102;&#21487;&#29992;&#20110;&#35780;&#20272;&#24847;&#22823;&#21033;&#35821;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#30446;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#30340;&#25968;&#23398;&#29702;&#35299;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#22522;&#20934;&#22522;&#20110;&#24847;&#22823;&#21033;&#23398;&#26657;&#31995;&#32479;&#20869;11&#33267;18&#23681;&#23398;&#29983;&#36827;&#34892;&#30340;&#23454;&#38469;&#27979;&#35797;&#65292;&#24182;&#24050;&#30001;&#22810;&#20301;&#25945;&#23398;&#21644;&#25945;&#32946;&#23398;&#19987;&#23478;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18697v1 Announce Type: new  Abstract: While Italian is by all metrics a high resource language, currently, there are isn't a Language Model pre-trained exclusively in this language. This results in a lower number of available benchmarks to evaluate the performance of language models in Italian.   This work presents two new benchmarks to evaluate the models performance on mathematical understanding and language understanding in Italian. These benchmarks are based on real tests that are undertaken by students of age between 11 and 18 within the Italian school system and have therefore been validated by several experts in didactics and pedagogy.   To validate this dataset we evaluate the performance of 9 language models that are the best performing when writing in Italian, including our own fine-tuned models. We show that this is a challenging benchmark where current language models are bound by 60\% accuracy.   We believe that the release of this dataset paves the way for impr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#20004;&#20010;&#36890;&#29992;&#26426;&#21046;&#65306;&#20027;&#39064;&#23646;&#24615;&#30693;&#35782;&#19981;&#36275;&#21644;&#26410;&#33021;&#27491;&#30830;&#36873;&#25321;&#23545;&#35937;&#23646;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#21644;&#20943;&#36731;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.18167</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Mechanisms of non-factual hallucinations in language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18167
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#20004;&#20010;&#36890;&#29992;&#26426;&#21046;&#65306;&#20027;&#39064;&#23646;&#24615;&#30693;&#35782;&#19981;&#36275;&#21644;&#26410;&#33021;&#27491;&#30830;&#36873;&#25321;&#23545;&#35937;&#23646;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#21644;&#20943;&#36731;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26377;&#26102;&#20250;&#20135;&#29983;&#19982;&#19990;&#30028;&#30693;&#35782;&#19981;&#31526;&#30340;&#38750;&#20107;&#23454;&#24187;&#35273;&#12290;&#23613;&#31649;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#24187;&#35273;&#65292;&#20294;&#29702;&#35299;&#23427;&#20204;&#30340;&#20869;&#22312;&#26426;&#21046;&#20173;&#28982;&#26159;&#22256;&#38590;&#30340;&#12290; &#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#24187;&#35273;&#30340;&#26426;&#21046;&#21407;&#22240;&#65292;&#29305;&#21035;&#26159; LM &#22312;&#23545;&#20027;&#39064;&#20851;&#31995;&#26597;&#35810;&#20570;&#20986;&#22238;&#31572;&#26102;&#38169;&#35823;&#22320;&#39044;&#27979;&#23545;&#35937;&#23646;&#24615;&#30340;&#38750;&#20107;&#23454;&#24418;&#24335;&#12290;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#21644;&#23884;&#20837;&#31354;&#38388;&#25237;&#24433;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#36328;&#19981;&#21516;&#35268;&#27169;&#21644;&#35774;&#35745;&#30340; LM &#20013;&#20849;&#20139;&#30340;&#20004;&#20010;&#36896;&#25104;&#24187;&#35273;&#30340;&#19968;&#33324;&#26426;&#21046;&#21407;&#22240;&#65306;1&#65289;&#22312;&#36739;&#20302;&#23618; MLPs &#20013;&#20027;&#39064;&#23646;&#24615;&#30693;&#35782;&#19981;&#36275;&#65292;&#20197;&#21450;2&#65289;&#22312;&#36739;&#39640;&#23618;&#27880;&#24847;&#21147;&#22836;&#21644; MLPs &#20013;&#26410;&#33021;&#36873;&#25321;&#27491;&#30830;&#30340;&#23545;&#35937;&#23646;&#24615;&#12290;&#36825;&#20004;&#20010;&#26426;&#21046;&#23637;&#31034;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#20027;&#23486;&#20851;&#31995;&#12289;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#25200;&#21160;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102; LM &#30340;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18167v1 Announce Type: cross  Abstract: State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive. Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries. With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs. These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness. Additionally, we scrutinize LM pre-training checkpoints, r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#20013;&#21307;&#39046;&#22495;&#26500;&#24314;&#20102;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#22522;&#20110;LLaMA&#25104;&#21151;&#24320;&#21457;&#20102;&#39318;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#30340;Qibo&#27169;&#22411;&#65292;&#24182;&#25512;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;Qibo&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.16056</link><description>&lt;p&gt;
Qibo: &#19968;&#31181;&#29992;&#20110;&#20013;&#21307;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Qibo: A Large Language Model for Traditional Chinese Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#20013;&#21307;&#39046;&#22495;&#26500;&#24314;&#20102;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#22522;&#20110;LLaMA&#25104;&#21151;&#24320;&#21457;&#20102;&#39318;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#30340;Qibo&#27169;&#22411;&#65292;&#24182;&#25512;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;Qibo&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#21644;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#35768;&#22810;&#19987;&#19994;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#23398;&#12289;&#27861;&#24459;&#21644;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#22312;&#20013;&#21307;&#39046;&#22495;&#65292;LLMs&#30340;&#24615;&#33021;&#25552;&#21319;&#21463;&#21040;&#25361;&#25112;&#65292;&#20854;&#21407;&#22240;&#22312;&#20110;&#20013;&#21307;&#29702;&#35770;&#19982;&#29616;&#20195;&#21307;&#23398;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#24322;&#65292;&#20197;&#21450;&#32570;&#20047;&#19987;&#19994;&#35821;&#26009;&#24211;&#36164;&#28304;&#12290;&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#21644;&#25972;&#29702;&#20013;&#21307;&#39046;&#22495;&#30340;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#36171;&#20104;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#20013;&#21307;&#29702;&#35770;&#29305;&#33394;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#25104;&#21151;&#22522;&#20110;LLaMA&#24320;&#21457;&#20102;Qibo&#27169;&#22411;&#65292;&#36825;&#26159;&#20013;&#21307;&#39046;&#22495;&#31532;&#19968;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#36807;&#31243;&#65288;&#20174;&#39044;&#35757;&#32451;&#21040;&#30417;&#30563;&#24494;&#35843;&#65289;&#30340;LLM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Qibo&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;&#19987;&#38376;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16056v1 Announce Type: cross  Abstract: In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance. However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources. In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Fine-Tuning (SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, whic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#36890;&#36807;&#24819;&#35937;&#21147;&#65292;&#32780;&#38750;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#34917;&#20805;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#30693;&#35782;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15268</link><description>&lt;p&gt;
&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65306;&#23398;&#20064;&#24819;&#35937;&#26356;&#20016;&#23500;&#30340;&#32972;&#26223;&#26469;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#36890;&#36807;&#24819;&#35937;&#21147;&#65292;&#32780;&#38750;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#34917;&#20805;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#30693;&#35782;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#29983;&#25104;&#22686;&#24378;&#29983;&#25104;&#24050;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#25152;&#38656;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20381;&#36182;&#20110;&#22806;&#37096;&#36164;&#28304;&#65292;&#32780;&#19988;&#20004;&#32773;&#37117;&#38656;&#35201;&#23558;&#26174;&#24335;&#25991;&#26723;&#21512;&#24182;&#21040;&#19978;&#19979;&#25991;&#20013;&#65292;&#23548;&#33268;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#28040;&#32791;&#26356;&#22810;&#36164;&#28304;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#24050;&#32463;&#24314;&#27169;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#23613;&#31649;&#27809;&#26377;&#34987;&#26377;&#25928;&#22320;&#35302;&#21457;&#25110;&#28608;&#27963;&#12290;&#22312;&#27492;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#23427;&#27169;&#25311;&#20102;&#20154;&#31867;&#36890;&#36807;&#24819;&#35937;&#21147;&#22312;&#20165;&#20973;&#24819;&#35937;&#22238;&#31572;&#38382;&#39064;&#26102;&#24357;&#34917;&#30693;&#35782;&#32570;&#38519;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#22312;IAG&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#65292;&#36890;&#36807;&#20197;&#19979;&#20004;&#20010;&#27169;&#22359;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#32972;&#26223;&#65306;&#36890;&#36807;&#29983;&#25104;&#31616;&#21333;&#30340;&#24819;&#35937;&#23454;&#29616;&#26174;&#24335;&#24819;&#35937;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15268v1 Announce Type: new  Abstract: Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results in longer contexts that lead to more resource consumption. Recent works indicate that LLMs have modeled rich knowledge, albeit not effectively triggered or activated. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering questions solely through imagination, without relying on external resources. Guided by IAG, we propose an imagine richer context method for question answering (IMcQA), which obtains richer context through the following two modules: explicit imagination by generating a sho
&lt;/p&gt;</description></item><item><title>Pointer-Generator Networks&#22312;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#20013;&#26410;&#23637;&#29616;&#20986;&#39044;&#26399;&#30340;&#20248;&#21183;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#36164;&#28304;&#33539;&#22260;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#19979;&#34920;&#29616;&#19968;&#33324;&#12290;</title><link>https://arxiv.org/abs/2403.10963</link><description>&lt;p&gt;
Pointer-Generator&#32593;&#32476;&#29992;&#20110;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#65306;&#19981;&#35201;&#22797;&#21046;&#37027;&#20010;&#65281;
&lt;/p&gt;
&lt;p&gt;
Pointer-Generator Networks for Low-Resource Machine Translation: Don't Copy That!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10963
&lt;/p&gt;
&lt;p&gt;
Pointer-Generator Networks&#22312;&#20302;&#36164;&#28304;&#26426;&#22120;&#32763;&#35793;&#20013;&#26410;&#23637;&#29616;&#20986;&#39044;&#26399;&#30340;&#20248;&#21183;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#36164;&#28304;&#33539;&#22260;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#19979;&#34920;&#29616;&#19968;&#33324;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#22312;&#39640;&#36164;&#28304;&#29615;&#22659;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#35768;&#22810;&#35821;&#35328;&#32570;&#20047;&#24517;&#35201;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#21463;&#30410;&#12290;&#22312;&#20004;&#31181;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#20043;&#38388;&#30340;&#20302;&#36164;&#28304;&#65288;LR&#65289;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#19968;&#31181;&#33258;&#28982;&#30340;&#30452;&#35273;&#26159;&#23547;&#27714;&#20174;&#32467;&#26500;&#8220;&#25463;&#24452;&#8221;&#20013;&#33719;&#30410;&#65292;&#20363;&#22914;&#20174;&#28304;&#35821;&#35328;&#22797;&#21046;&#23376;&#35789;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#35821;&#35328;&#23545;&#36890;&#24120;&#20849;&#20139;&#30456;&#24403;&#25968;&#37327;&#30340;&#30456;&#21516;&#21333;&#35789;&#12289;&#21516;&#28304;&#35789;&#21644;&#20511;&#35789;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#38024;&#23545;&#20845;&#31181;&#35821;&#35328;&#23545;&#30340;&#25351;&#38024;&#29983;&#25104;&#22120;&#32593;&#32476;&#22312;&#21508;&#31181;&#36164;&#28304;&#33539;&#22260;&#19979;&#30340;&#29992;&#36884;&#65292;&#24182;&#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#37117;&#26377;&#36731;&#24494;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#26174;&#31034;&#65292;&#27169;&#22411;&#23545;&#20110;&#23494;&#20999;&#30456;&#20851;&#30340;&#35821;&#35328;&#23545;&#19982;&#36739;&#36828;&#30340;&#35821;&#35328;&#23545;&#65292;&#25110;&#32773;&#36164;&#28304;&#33539;&#22260;&#36739;&#20302;&#19982;&#36739;&#39640;&#30340;&#35821;&#35328;&#23545;&#24182;&#27809;&#26377;&#23637;&#29616;&#20986;&#26356;&#22823;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#27169;&#22411;&#24182;&#26410;&#23637;&#31034;&#20986;&#23545;&#20110;&#20849;&#20139;&#23376;&#35789;&#26426;&#21046;&#30340;&#39044;&#26399;&#29992;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#34892;&#20026;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10963v1 Announce Type: new  Abstract: While Transformer-based neural machine translation (NMT) is very effective in high-resource settings, many languages lack the necessary large parallel corpora to benefit from it. In the context of low-resource (LR) MT between two closely-related languages, a natural intuition is to seek benefits from structural "shortcuts", such as copying subwords from the source to the target, given that such language pairs often share a considerable number of identical words, cognates, and borrowings. We test Pointer-Generator Networks for this purpose for six language pairs over a variety of resource ranges, and find weak improvements for most settings. However, analysis shows that the model does not show greater improvements for closely-related vs. more distant language pairs, or for lower resource ranges, and that the models do not exhibit the expected usage of the mechanism for shared subwords. Our discussion of the reasons for this behaviour high
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;LLMs&#22312;&#29983;&#25104;CT&#24322;&#24120;&#30340;&#20934;&#30830;&#25688;&#35201;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05680</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#23545;&#22522;&#20110;&#35270;&#35273;&#30340;LLM&#39044;&#27979;&#36827;&#34892;&#20998;&#35299;&#20197;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05680
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;LLMs&#22312;&#29983;&#25104;CT&#24322;&#24120;&#30340;&#20934;&#30830;&#25688;&#35201;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CT&#26816;&#26597;&#30340;&#25968;&#37327;&#27599;&#24180;&#37117;&#22312;&#22686;&#21152;&#65292;&#36825;&#23548;&#33268;&#25918;&#23556;&#31185;&#21307;&#29983;&#30130;&#21171;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#20943;&#36731;&#20182;&#20204;&#30340;&#36127;&#25285;&#65292;&#20294;&#20854;&#22312;&#20020;&#24202;&#20013;&#30340;&#37319;&#29992;&#21462;&#20915;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#20449;&#20219;&#21644;&#29983;&#25104;&#20869;&#23481;&#30340;&#31616;&#21333;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;LLMs&#22312;&#29983;&#25104;CT&#24322;&#24120;&#30340;&#20934;&#30830;&#25688;&#35201;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05680v1 Announce Type: new  Abstract: The volume of CT exams being done in the world has been rising every year, which has led to radiologist burn-out. Large Language Models (LLMs) have the potential to reduce their burden, but their adoption in the clinic depends on radiologist trust, and easy evaluation of generated content. Presently, many automated methods are available to evaluate the reports generated for chest radiographs, but such an approach is not available for CT presently. In this paper, we propose a novel evaluation framework to judge the capabilities of vision-language LLMs in generating accurate summaries of CT-based abnormalities. CT slices containing an abnormality (e.g., lesion) were input to a vision-based LLM (GPT-4V, LLaVA-Med, and RadFM), and it generated a free-text summary of the predicted characteristics of the abnormality. Next, a GPT-4 model decomposed the summary into specific aspects (body part, location, type, and attributes), automatically eval
&lt;/p&gt;</description></item><item><title>HistGen&#26159;&#19968;&#20010;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#26469;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.05396</link><description>&lt;p&gt;
HistGen&#65306;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05396
&lt;/p&gt;
&lt;p&gt;
HistGen&#26159;&#19968;&#20010;&#36890;&#36807;&#26412;&#22320;-&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#21644;&#36328;&#27169;&#24577;&#19978;&#19979;&#25991;&#20132;&#20114;&#26469;&#29983;&#25104;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22312;&#30284;&#30151;&#35786;&#26029;&#20013;&#25198;&#28436;&#30528;&#40644;&#37329;&#26631;&#20934;&#30340;&#35282;&#33394;&#65292;&#20020;&#24202;&#25253;&#21578;&#22312;&#35299;&#37322;&#21644;&#29702;&#35299;&#36825;&#19968;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#25351;&#23548;&#30284;&#30151;&#27835;&#30103;&#21644;&#24739;&#32773;&#25252;&#29702;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28145;&#24230;&#23398;&#20064;&#23545;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#23558;&#26497;&#22823;&#25552;&#21319;&#20020;&#24202;&#25928;&#29575;&#65292;&#24182;&#20943;&#36731;&#30149;&#29702;&#23398;&#23478;&#22312;&#25253;&#21578;&#25776;&#20889;&#26041;&#38754;&#30340;&#21171;&#21160;&#24378;&#24230;&#21644;&#32791;&#26102;&#36127;&#25285;&#12290;&#20026;&#36861;&#27714;&#36825;&#19968;&#36827;&#27493;&#65292;&#20316;&#32773;&#24341;&#20837;&#20102;HistGen&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#23454;&#20363;&#23398;&#20064;&#22686;&#24378;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#25253;&#21578;&#29983;&#25104;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;HistGen&#21463;&#35786;&#26029;&#21644;&#25253;&#21578;&#25776;&#20889;&#24037;&#20316;&#27969;&#31243;&#30340;&#21551;&#21457;&#65292;&#20855;&#26377;&#20004;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#22359;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#40784;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#21644;&#35786;&#26029;&#25253;&#21578;&#65292;&#20174;&#26412;&#22320;&#21644;&#20840;&#23616;&#31890;&#24230;&#25552;&#21319;&#25253;&#21578;&#29983;&#25104;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#20998;&#23618;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#21306;&#22495;&#20013;&#32858;&#21512;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05396v1 Announce Type: cross  Abstract: Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing. In pursuit of this advancement, we introduce HistGen, a multiple instance learning-empowered framework for histopathology report generation together with the first benchmark dataset for evaluation. Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity. To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a regi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04945</link><description>&lt;p&gt;
&#20026;&#25253;&#21578;&#29983;&#25104;&#35843;&#20248;&#24515;&#30005;&#22270;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram Instruction Tuning for Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04945
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20316;&#20026;&#24515;&#33039;&#30149;&#24773;&#30417;&#27979;&#30340;&#20027;&#35201;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#65292;&#23545;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#25968;&#25454;&#23545;&#24515;&#33039;&#30149;&#24773;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#24573;&#30053;&#20102;ECG&#25253;&#21578;&#29983;&#25104;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#65292;&#32780;&#19988;&#38656;&#35201;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;ECG&#25253;&#21578;&#29983;&#25104;&#24182;&#30830;&#20445;&#20854;&#22810;&#21151;&#33021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;\textit{&#39318;&#27425;}&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#26469;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;MEIT&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21508;&#31181;LLM&#39592;&#24178;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#23545;&#40784;&#20102;ECG&#20449;&#21495;&#21644;&#25253;&#21578;&#30340;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35780;&#20272;MEIT&#19982;&#20061;&#20010;&#24320;&#28304;LLMs&#65292;&#20351;&#29992;&#20102;&#36229;&#36807;80&#19975;&#20010;ECG&#25253;&#21578;&#12290;MEIT&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04945v1 Announce Type: new  Abstract: Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool for cardiac conditions monitoring, are crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is not only time-consuming but also requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the \textit{first} attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results underscore the superior p
&lt;/p&gt;</description></item><item><title>SciAssess&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#39046;&#22495;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01976</link><description>&lt;p&gt;
SciAssess&#65306;&#22522;&#20934;&#27979;&#35797;LLM&#22312;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01976
&lt;/p&gt;
&lt;p&gt;
SciAssess&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#39046;&#22495;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01976v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25277;&#35937;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#31361;&#30772;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#32454;&#33268;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#20805;&#20998;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22797;&#26434;&#29702;&#35299;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SciAssess&#65292;&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#26377;&#25928;&#24615;&#12290;SciAssess&#19987;&#27880;&#20110;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#32972;&#26223;&#19979;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#31185;&#23398;&#39046;&#22495;&#30340;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#22914;&#19968;&#33324;&#21270;&#23398;&#12289;&#26377;&#26426;&#26448;&#26009;&#21644;&#21512;&#37329;&#26448;&#26009;&#12290;&#20005;&#26684;&#30340;&#36136;&#37327;&#25511;&#21046;&#25514;&#26045;&#30830;&#20445;&#20102;&#20854;&#22312;&#27491;&#30830;&#24615;&#12289;&#21311;&#21517;&#21270;&#21644;&#22797;&#21046;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01976v1 Announce Type: new  Abstract: Recent breakthroughs in Large Language Models (LLMs) have revolutionized natural language understanding and generation, igniting a surge of interest in leveraging these technologies for the nuanced field of scientific literature analysis. Existing benchmarks, however, inadequately evaluate the proficiency of LLMs in the scientific domain, especially in scenarios involving complex comprehension and multimodal data. In response, we introduced SciAssess, a benchmark tailored for the in-depth analysis of scientific literature, crafted to provide a thorough assessment of LLMs' efficacy. SciAssess focuses on evaluating LLMs' abilities in memorization, comprehension, and analysis within scientific contexts. It includes representative tasks from diverse scientific fields, such as general chemistry, organic materials, and alloy materials. And rigorous quality control measures ensure its reliability in terms of correctness, anonymization, and copy
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;LLMs&#22312;&#25512;&#29702;&#21644;&#27807;&#36890;&#20013;&#20351;&#29992;&#26367;&#20195;&#26684;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26102;&#26631;&#35760;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.18439</link><description>&lt;p&gt;
&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#65306;LLM&#21033;&#29992;&#26367;&#20195;&#26684;&#24335;&#36827;&#34892;&#22686;&#24378;&#25512;&#29702;&#21644;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18439
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;LLMs&#22312;&#25512;&#29702;&#21644;&#27807;&#36890;&#20013;&#20351;&#29992;&#26367;&#20195;&#26684;&#24335;&#30340;&#23454;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#25512;&#29702;&#25928;&#29575;&#30340;&#25552;&#21319;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#26102;&#26631;&#35760;&#20351;&#29992;&#37327;&#30340;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#27807;&#36890;&#30340;&#20027;&#35201;&#26684;&#24335;&#65292;&#22240;&#27492;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#20013;&#21516;&#26679;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;NL&#20043;&#22806;&#65292;LLMs&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30475;&#21040;&#20102;&#21508;&#31181;&#38750;NL&#26684;&#24335;&#65292;&#22914;&#20195;&#30721;&#21644;&#36923;&#36753;&#34920;&#36798;&#24335;&#12290;NL&#20316;&#20026;LLMs&#30340;&#26368;&#20339;&#26684;&#24335;&#65292;&#22312;&#21333;&#19968;LLM&#25512;&#29702;&#21644;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#20013;&#30340;&#22320;&#20301;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#23457;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#40664;&#35748;&#20351;&#29992;NL&#65292;&#36890;&#36807;&#25506;&#32034;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#38750;NL&#26684;&#24335;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#20801;&#35768;LLMs&#22312;&#25512;&#29702;&#25110;&#27807;&#36890;&#20043;&#21069;&#33258;&#20027;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26684;&#24335;&#65292;&#21487;&#23548;&#33268;&#19981;&#21516;LLMs&#25512;&#29702;&#25928;&#29575;&#25552;&#39640;3.3&#33267;5.7&#65285;&#65292;&#24182;&#19988;&#22312;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#20013;&#26368;&#22810;&#21487;&#20943;&#23569;72.7&#65285;&#30340;&#26631;&#35760;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#27807;&#36890;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;LLMs&#21487;&#20197;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18439v1 Announce Type: cross  Abstract: Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#21407;&#22987;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#19987;&#23478;&#28151;&#21512;&#26550;&#26500;&#65292;&#30740;&#31350;&#20102;LLMs&#30340;&#22810;&#35821;&#35328;&#28608;&#27963;&#27169;&#24335;&#65292;&#21457;&#29616;&#23384;&#22312;&#38750;&#29305;&#23450;&#35821;&#35328;&#30340;&#31070;&#32463;&#20803;&#21644;&#29305;&#23450;&#35821;&#35328;&#28608;&#27963;&#31070;&#32463;&#20803;&#65292;&#20197;&#21450;&#39640;&#39057;&#28608;&#27963;&#31070;&#32463;&#20803;&#21487;&#20197;&#21152;&#36895;&#25512;&#26029;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16367</link><description>&lt;p&gt;
&#25581;&#31034;&#24052;&#21035;&#22612;&#65306;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#30340;&#22810;&#35821;&#35328;&#28608;&#27963;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16367
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#21407;&#22987;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#19987;&#23478;&#28151;&#21512;&#26550;&#26500;&#65292;&#30740;&#31350;&#20102;LLMs&#30340;&#22810;&#35821;&#35328;&#28608;&#27963;&#27169;&#24335;&#65292;&#21457;&#29616;&#23384;&#22312;&#38750;&#29305;&#23450;&#35821;&#35328;&#30340;&#31070;&#32463;&#20803;&#21644;&#29305;&#23450;&#35821;&#35328;&#28608;&#27963;&#31070;&#32463;&#20803;&#65292;&#20197;&#21450;&#39640;&#39057;&#28608;&#27963;&#31070;&#32463;&#20803;&#21487;&#20197;&#21152;&#36895;&#25512;&#26029;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#31361;&#30772;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#26102;&#30340;&#26426;&#21046;&#20173;&#28982;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#22810;&#35821;&#35328;&#28608;&#27963;&#27169;&#24335;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36716;&#21270;&#20026;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26550;&#26500;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22788;&#29702;&#21508;&#31181;&#35821;&#35328;&#26102;&#19987;&#23478;&#30340;&#28608;&#27963;&#27169;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#28608;&#27963;&#27169;&#24335;&#22312;&#35821;&#35328;&#23478;&#26063;&#23618;&#38754;&#19978;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#38750;&#29305;&#23450;&#35821;&#35328;&#30340;&#31070;&#32463;&#20803;&#20197;&#21450;&#29305;&#23450;&#35821;&#35328;&#28608;&#27963;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#12290;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#29978;&#33267;&#23637;&#31034;&#20102;&#20165;&#21033;&#29992;&#39640;&#39057;&#28608;&#27963;&#31070;&#32463;&#20803;&#21487;&#20197;&#21152;&#36895;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;LLMs&#30340;&#22810;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#65292;&#24182;&#22312;&#25351;&#23548;&#22810;&#35821;&#35328;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16367v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have achieved tremendous breakthroughs in the field of language processing, yet their mechanisms in processing multiple languages remain agnostic. Therefore, in this work we study the multilingual activation patterns of LLMs. By transforming the original Large Language Models (LLMs) into a Mixture of Experts (MoE) architecture, we analyze the expert activation patterns when processing various languages and demonstrate the connections of these activation patterns at the level of language families. We discover the existence of non-language-specific neurons as well as language-specific activation neurons. Further exploration even showcases that merely leveraging high-frequency activation neurons can accelerate inference while maintaining comparable performance. These findings shed light on the LLMs' multilingual processing mechanism, and are of significant importance in guiding the multilingual trainin
&lt;/p&gt;</description></item><item><title>SportQA&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20307;&#32946;&#29702;&#35299;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21253;&#21547;&#36229;&#36807;70,000&#20010;&#38382;&#39064;&#28085;&#30422;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#20307;&#32946;&#30693;&#35782;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#22312;&#22522;&#26412;&#20307;&#32946;&#30693;&#35782;&#19978;&#34920;&#29616;&#20248;&#24322;&#20294;&#22312;&#22797;&#26434;&#24773;&#22659;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15862</link><description>&lt;p&gt;
SportQA&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20307;&#32946;&#29702;&#35299;&#30340;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SportQA: A Benchmark for Sports Understanding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15862
&lt;/p&gt;
&lt;p&gt;
SportQA&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20307;&#32946;&#29702;&#35299;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21253;&#21547;&#36229;&#36807;70,000&#20010;&#38382;&#39064;&#28085;&#30422;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#20307;&#32946;&#30693;&#35782;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#22312;&#22522;&#26412;&#20307;&#32946;&#30693;&#35782;&#19978;&#34920;&#29616;&#20248;&#24322;&#20294;&#22312;&#22797;&#26434;&#24773;&#22659;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15862v1 &#25253;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#23545;&#20307;&#32946;&#39046;&#22495;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#65292;&#36825;&#26159;&#19968;&#39033;&#20805;&#28385;&#25112;&#30053;&#21644;&#21160;&#24577;&#20869;&#23481;&#30340;&#39046;&#22495;&#65292;&#23545;&#20110;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#22312;&#35780;&#20272;&#21644;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32972;&#26223;&#19979;&#23588;&#20026;&#37325;&#35201;&#65292;&#37492;&#20110;&#29616;&#26377;&#19987;&#38376;&#22522;&#20934;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SportQA&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20307;&#32946;&#29702;&#35299;&#26041;&#38754;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;SportQA&#28085;&#30422;&#20102;&#36229;&#36807;70,000&#20010;&#36328;&#19977;&#20010;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#27599;&#20010;&#32423;&#21035;&#38024;&#23545;&#20307;&#32946;&#30693;&#35782;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#20174;&#22522;&#26412;&#21382;&#21490;&#20107;&#23454;&#21040;&#22797;&#26434;&#30340;&#22522;&#20110;&#24773;&#26223;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#20027;&#35201;&#21033;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#36741;&#20197;&#8220;&#32852;&#24819;&#38142;&#8221;&#25552;&#31034;&#26041;&#27861;&#23545;&#26222;&#36941;&#30340;LLMs&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;LLMs&#22312;&#22522;&#26412;&#30340;&#20307;&#32946;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#24773;&#26223;&#30340;&#25512;&#29702;&#26041;&#38754;&#21364;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15862v1 Announce Type: new  Abstract: A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-bas
&lt;/p&gt;</description></item><item><title>RNN&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#34920;&#31034;&#26356;&#22823;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#26377;&#30028;&#22534;&#26632;&#21644;&#24191;&#20041;&#22534;&#26632;&#26356;&#26032;&#20989;&#25968;&#65292;&#31867;&#20284;&#20110;&#20445;&#30041;&#22266;&#23450;&#25968;&#37327;&#31526;&#21495;&#35760;&#24518;&#24182;&#20351;&#29992;&#31616;&#21333;&#26356;&#26032;&#26426;&#21046;&#30340;&#33258;&#21160;&#26426;&#12290;</title><link>https://arxiv.org/abs/2402.15814</link><description>&lt;p&gt;
RNN&#35821;&#35328;&#27169;&#22411;&#24402;&#32435;&#20559;&#24046;&#30340;&#19968;&#20010;&#29702;&#35770;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Result on the Inductive Bias of RNN Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15814
&lt;/p&gt;
&lt;p&gt;
RNN&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#34920;&#31034;&#26356;&#22823;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#26377;&#30028;&#22534;&#26632;&#21644;&#24191;&#20041;&#22534;&#26632;&#26356;&#26032;&#20989;&#25968;&#65292;&#31867;&#20284;&#20110;&#20445;&#30041;&#22266;&#23450;&#25968;&#37327;&#31526;&#21495;&#35760;&#24518;&#24182;&#20351;&#29992;&#31616;&#21333;&#26356;&#26032;&#26426;&#21046;&#30340;&#33258;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;Hewitt&#31561;&#20154;&#65288;2020&#65289;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#20316;&#20026;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#32463;&#39564;&#25104;&#21151;&#21487;&#33021;&#24615;&#30340;&#19968;&#20010;&#35299;&#37322;&#12290; &#23427;&#26174;&#31034;RNNs&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#22312;&#20154;&#31867;&#35821;&#35328;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#26377;&#30028;&#20998;&#23618;&#32467;&#26500;&#12290; &#36825;&#34920;&#26126;RNNs&#30340;&#25104;&#21151;&#21487;&#33021;&#19982;&#23427;&#20204;&#24314;&#27169;&#23618;&#27425;&#32467;&#26500;&#30340;&#33021;&#21147;&#26377;&#20851;&#12290; &#28982;&#32780;&#65292;&#23545;Hewitt&#31561;&#20154;&#65288;2020&#65289;&#26500;&#36896;&#30340;&#26356;&#35814;&#32454;&#26816;&#26597;&#34920;&#26126;&#65292;&#23427;&#19981;&#38480;&#20110;&#20998;&#23618;LMs&#65292;&#36825;&#24341;&#20986;&#20102;RNNs&#21487;&#20197;&#26377;&#25928;&#34920;&#31034;&#21738;&#20123;\emph{&#20854;&#20182;&#31867;&#22411;} LMs&#30340;&#38382;&#39064;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#27010;&#25324;&#20182;&#20204;&#30340;&#26500;&#36896;&#20197;&#23637;&#31034;RNNs&#21487;&#20197;&#26377;&#25928;&#34920;&#31034;&#26356;&#22823;&#31867;&#21035;&#30340;LMs&#65306;&#21487;&#20197;&#36890;&#36807;&#24102;&#26377;&#26377;&#30028;&#22534;&#26632;&#21644;&#24191;&#20041;&#22534;&#26632;&#26356;&#26032;&#20989;&#25968;&#30340;&#19979;&#25512;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#37027;&#20123;&#12290; &#36825;&#31867;&#20284;&#20110;&#19968;&#20010;&#20445;&#30041;&#22266;&#23450;&#25968;&#37327;&#31526;&#21495;&#35760;&#24518;&#24182;&#20351;&#29992;&#31616;&#21333;&#26356;&#26032;&#26426;&#21046;&#26356;&#26032;&#35760;&#24518;&#30340;&#33258;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15814v1 Announce Type: new  Abstract: Recent work by Hewitt et al. (2020) provides a possible interpretation of the empirical success of recurrent neural networks (RNNs) as language models (LMs).   It shows that RNNs can efficiently represent bounded hierarchical structures that are prevalent in human language.   This suggests that RNNs' success might be linked to their ability to model hierarchy.   However, a closer inspection of Hewitt et al.'s (2020) construction shows that it is not limited to hierarchical LMs, posing the question of what \emph{other classes} of LMs can be efficiently represented by RNNs.   To this end, we generalize their construction to show that RNNs can efficiently represent a larger class of LMs: Those that can be represented by a pushdown automaton with a bounded stack and a generalized stack update function.   This is analogous to an automaton that keeps a memory of a fixed number of symbols and updates the memory with a simple update mechanism.  
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#25968;&#25454;&#25910;&#38598;&#20559;&#35265;&#21644;&#20010;&#20307;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15301</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15301
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#25968;&#25454;&#25910;&#38598;&#20559;&#35265;&#21644;&#20010;&#20307;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#22270;&#24674;&#22797;&#22312;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#26159;&#22522;&#20110;&#30693;&#35782;&#25110;&#32479;&#35745;&#20272;&#35745;&#65292;&#21463;&#25968;&#25454;&#25910;&#38598;&#20559;&#35265;&#21644;&#20010;&#20307;&#20851;&#20110;&#24433;&#21709;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#30693;&#35782;&#30340;&#38480;&#21046;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#37327;&#31185;&#23398;&#25991;&#29486;&#20013;&#25152;&#21253;&#21547;&#30340;&#30693;&#35782;&#25512;&#23548;&#19968;&#33324;&#22240;&#26524;&#22270;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;LLMs&#31995;&#32479;&#22320;&#20998;&#26512;&#21644;&#25552;&#21462;&#26469;&#33258;&#24191;&#27867;&#30740;&#31350;&#35770;&#25991;&#38598;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20174;&#27719;&#24635;&#30340;&#25991;&#29486;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26412;&#29255;&#27573;&#12290;&#28982;&#21518;&#65292;LLM&#34987;&#29992;&#26469;&#35782;&#21035;&#21644;&#26631;&#35760;&#22240;&#32032;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#32852;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15301v1 Announce Type: new  Abstract: Causal graph recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks. This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text chunks from the aggregated literature. Then, the LLM is tasked with identifying and labelling potential associations between factors. Finally, we giv
&lt;/p&gt;</description></item><item><title>PANDA&#26159;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#21709;&#24212;&#20559;&#22909;&#30340;&#35265;&#35299;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.12835</link><description>&lt;p&gt;
PANDA: &#29992;&#20110;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#20559;&#22909;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12835
&lt;/p&gt;
&lt;p&gt;
PANDA&#26159;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#21709;&#24212;&#20559;&#22909;&#30340;&#35265;&#35299;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#36798;&#21040;&#29305;&#23450;&#39046;&#22495;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#19968;&#31181;&#28508;&#22312;&#26041;&#27861;&#26159;&#20351;&#29992;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#32791;&#36153;&#36164;&#28304;&#21448;&#32791;&#26102;&#65292;&#24182;&#19988;&#26080;&#27861;&#24212;&#29992;&#20110;&#23553;&#38381;&#28304;&#21830;&#19994;LLMs&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PANDA&#30340;&#20559;&#22909;&#36866;&#24212;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#21709;&#24212;&#20559;&#22909;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;LLMs&#30340;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PANDA&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#22312;&#25991;&#26412;&#20998;&#31867;&#21644;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;PANDA&#30340;LLM&#29978;&#33267;&#36229;&#36807;&#20102;&#19987;&#23478;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12835v1 Announce Type: cross  Abstract: While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;InfICL&#30340;&#28436;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11750</link><description>&lt;p&gt;
&#36890;&#36807;&#24433;&#21709;&#20998;&#26512;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#28436;&#31034;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning Demonstration Selection via Influence Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11750
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;InfICL&#30340;&#28436;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#36825;&#25552;&#20379;&#20102;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26426;&#20250;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26799;&#24230;&#26356;&#26032;&#12290;&#23613;&#31649;&#20855;&#26377;&#22810;&#37325;&#22909;&#22788;&#65292;ICL&#30340;&#27867;&#21270;&#24615;&#33021;&#23545;&#25152;&#36873;&#28436;&#31034;&#25935;&#24863;&#12290;&#36873;&#25321;&#29992;&#20110;ICL&#30340;&#26377;&#25928;&#28436;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfICL&#30340;&#28436;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24433;&#21709;&#20989;&#25968;&#20998;&#26512;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#37492;&#21035;&#39640;&#24230;&#26377;&#24433;&#21709;&#21147;&#30340;&#35757;&#32451;&#26679;&#26412;&#21487;&#33021;&#26377;&#21161;&#20110;&#25552;&#21319;ICL&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#38480;&#21046;InfICL&#30340;&#36816;&#34892;&#25104;&#26412;&#65292;&#25105;&#20204;&#20165;&#21033;&#29992;LLM&#29983;&#25104;&#26679;&#26412;&#23884;&#20837;&#65292;&#24182;&#19981;&#25191;&#34892;&#20219;&#20309;&#26114;&#36149;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;InfICL&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11750v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated their In-Context Learning (ICL) capabilities which provides an opportunity to perform few shot learning without any gradient update. Despite its multiple benefits, ICL generalization performance is sensitive to the selected demonstrations. Selecting effective demonstrations for ICL is still an open research challenge. To address this challenge, we propose a demonstration selection method called InfICL which analyzes influences of training samples through influence functions. Identifying highly influential training samples can potentially aid in uplifting the ICL generalization performance. To limit the running cost of InfICL, we only employ the LLM to generate sample embeddings, and don't perform any costly fine tuning. We perform empirical study on multiple real-world datasets and show merits of our InfICL against state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#20196;&#20154;&#25285;&#24551;&#30340;&#26131;&#21463;&#24847;&#35782;&#24418;&#24577;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#65292;&#23545;&#26497;&#23569;&#37327;&#24847;&#35782;&#24418;&#24577;&#39537;&#21160;&#26679;&#26412;&#30340;&#26292;&#38706;&#23601;&#33021;&#26174;&#33879;&#25913;&#21464;&#20854;&#24847;&#35782;&#24418;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#19968;&#20010;&#20027;&#39064;&#21560;&#25910;&#24847;&#35782;&#24418;&#24577;&#24182;&#27867;&#21270;&#21040;&#20854;&#20182;&#19981;&#30456;&#20851;&#20027;&#39064;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.11725</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24847;&#35782;&#24418;&#24577;&#25805;&#32437;&#30340;&#26131;&#24863;&#24615;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Susceptible are Large Language Models to Ideological Manipulation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11725
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#20196;&#20154;&#25285;&#24551;&#30340;&#26131;&#21463;&#24847;&#35782;&#24418;&#24577;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#65292;&#23545;&#26497;&#23569;&#37327;&#24847;&#35782;&#24418;&#24577;&#39537;&#21160;&#26679;&#26412;&#30340;&#26292;&#38706;&#23601;&#33021;&#26174;&#33879;&#25913;&#21464;&#20854;&#24847;&#35782;&#24418;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#19968;&#20010;&#20027;&#39064;&#21560;&#25910;&#24847;&#35782;&#24418;&#24577;&#24182;&#27867;&#21270;&#21040;&#20854;&#20182;&#19981;&#30456;&#20851;&#20027;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#23545;&#20844;&#20247;&#35266;&#24565;&#21644;&#20449;&#24687;&#20114;&#21160;&#26045;&#21152;&#37325;&#35201;&#24433;&#21709;&#30340;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#22914;&#26524;&#36825;&#20123;&#27169;&#22411;&#20869;&#30340;&#24847;&#35782;&#24418;&#24577;&#26131;&#21463;&#25805;&#32437;&#21487;&#33021;&#24102;&#26469;&#31038;&#20250;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#23398;&#20064;&#21644;&#27867;&#21270;&#24847;&#35782;&#24418;&#24577;&#20559;&#35265;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#33030;&#24369;&#24615;&#65306;&#20165;&#25509;&#35302;&#21040;&#23569;&#37327;&#24847;&#35782;&#24418;&#24577;&#39537;&#21160;&#30340;&#26679;&#26412;&#23601;&#20250;&#26174;&#33879;&#25913;&#21464;LLMs&#30340;&#24847;&#35782;&#24418;&#24577;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LLMs&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#20174;&#19968;&#20010;&#20027;&#39064;&#21560;&#25910;&#24847;&#35782;&#24418;&#24577;&#24182;&#23558;&#20854;&#27867;&#21270;&#21040;&#29978;&#33267;&#19981;&#30456;&#20851;&#30340;&#20027;&#39064;&#19978;&#12290;LLMs&#30340;&#24847;&#35782;&#24418;&#24577;&#23481;&#26131;&#34987;&#25197;&#26354;&#30340;&#20107;&#23454;&#24378;&#35843;&#20102;&#24694;&#24847;&#34892;&#20026;&#32773;&#25925;&#24847;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#25110;&#25968;&#25454;&#27880;&#37322;&#32773;&#26080;&#24847;&#24341;&#20837;&#20559;&#35265;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#36825;&#20063;&#24378;&#35843;&#20102;&#37319;&#21462;&#24378;&#26377;&#21147;&#25514;&#26045;&#20197;&#20943;&#36731;&#36825;&#20123;&#23041;&#32961;&#30340;&#36843;&#20999;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11725v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the inf
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#33258;&#25105;&#20559;&#35265;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#20934;&#30830;&#35780;&#20272;&#30340;&#22806;&#37096;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11436</link><description>&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#30340;&#21361;&#38505;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#25105;&#20559;&#35265;&#34987;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11436
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#33258;&#25105;&#20559;&#35265;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#20934;&#30830;&#35780;&#20272;&#30340;&#22806;&#37096;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#25105;&#21453;&#39304;&#21487;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#21364;&#20250;&#24694;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#30683;&#30462;&#26159;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#33258;&#36523;&#36755;&#20986;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#20559;&#35265;&#8212;&#8212;&#20542;&#21521;&#20110;&#20559;&#29233;&#33258;&#36523;&#29983;&#25104;&#8212;&#8212;&#24182;&#20351;&#29992;&#20004;&#20010;&#32479;&#35745;&#37327;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#32763;&#35793;&#12289;&#21463;&#38480;&#25991;&#26412;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#19978;&#20998;&#26512;&#20102;&#20845;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20559;&#35265;&#22312;&#25152;&#26377;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37117;&#26222;&#36941;&#23384;&#22312;&#65292;&#36328;&#22810;&#31181;&#35821;&#35328;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#34429;&#28982;&#33258;&#25105;&#25913;&#36827;&#31649;&#36947;&#25552;&#39640;&#20102;&#27169;&#22411;&#36755;&#20986;&#30340;&#27969;&#30021;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#20294;&#23427;&#36827;&#19968;&#27493;&#25918;&#22823;&#20102;&#33258;&#25105;&#20559;&#35265;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#20855;&#26377;&#20934;&#30830;&#35780;&#20272;&#30340;&#22806;&#37096;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#33258;&#25105;&#25913;&#36827;&#31649;&#36947;&#20013;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#38469;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11436v1 Announce Type: cross  Abstract: Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#21644;&#36947;&#24503;&#32500;&#24230;&#19978;&#22914;&#20309;&#20195;&#34920;&#19981;&#21516;&#32676;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#24847;&#35782;&#24418;&#24577;&#22242;&#20307;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11114</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#20102;&#35841;&#30340;&#24773;&#24863;&#21644;&#36947;&#24503;&#24773;&#24863;&#65311;
&lt;/p&gt;
&lt;p&gt;
Whose Emotions and Moral Sentiments Do Language Models Reflect?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#21644;&#36947;&#24503;&#32500;&#24230;&#19978;&#22914;&#20309;&#20195;&#34920;&#19981;&#21516;&#32676;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#24847;&#35782;&#24418;&#24577;&#22242;&#20307;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24050;&#30693;&#26356;&#22909;&#22320;&#20195;&#34920;&#19968;&#20123;&#31038;&#20250;&#32676;&#20307;&#30340;&#35266;&#28857;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20027;&#35266;&#20219;&#21153;&#19978;&#65292;&#27604;&#22914;&#20869;&#23481;&#31649;&#29702;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#24773;&#24863;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#29992;&#26469;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#21644;&#36947;&#24503;&#33394;&#35843;&#22914;&#20309;&#20195;&#34920;&#19981;&#21516;&#32676;&#20307;&#30340;&#24773;&#24863;&#12290;&#36890;&#36807;&#27604;&#36739;36&#20010;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#24212;&#30340;&#24773;&#24863;&#19982;Twitter&#28040;&#24687;&#30340;&#24773;&#24863;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35821;&#35328;&#27169;&#22411;&#19982;&#20004;&#31181;&#24847;&#35782;&#24418;&#24577;&#22242;&#20307;&#23384;&#22312;&#26174;&#33879;&#19981;&#19968;&#33268;&#12290;&#21363;&#20351;&#22312;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#26397;&#30528;&#29305;&#23450;&#24847;&#35782;&#24418;&#24577;&#26041;&#21521;&#21457;&#23637;&#20043;&#21518;&#65292;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#20063;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11114v1 Announce Type: new  Abstract: Language models (LMs) are known to represent the perspectives of some social groups better than others, which may impact their performance, especially on subjective tasks such as content moderation and hate speech detection. To explore how LMs represent different perspectives, existing research focused on positional alignment, i.e., how closely the models mimic the opinions and stances of different groups, e.g., liberals or conservatives. However, human communication also encompasses emotional and moral dimensions. We define the problem of affective alignment, which measures how LMs' emotional and moral tone represents those of different groups. By comparing the affect of responses generated by 36 LMs to the affect of Twitter messages, we observe significant misalignment of LMs with both ideological groups. This misalignment is larger than the partisan divide in the U.S. Even after steering the LMs towards specific ideological perspectiv
&lt;/p&gt;</description></item><item><title>Disordered-DABS&#26159;&#38024;&#23545;&#19981;&#35268;&#21017;&#25991;&#26412;&#20013;&#21160;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24635;&#32467;&#32780;&#35774;&#35745;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#24635;&#32467;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10554</link><description>&lt;p&gt;
&#19981;&#35268;&#21017;&#25991;&#26412;&#20013;&#21160;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24635;&#32467;&#26631;&#20934;&#65306; Disordered-DABS&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10554
&lt;/p&gt;
&lt;p&gt;
Disordered-DABS&#26159;&#38024;&#23545;&#19981;&#35268;&#21017;&#25991;&#26412;&#20013;&#21160;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24635;&#32467;&#32780;&#35774;&#35745;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#24635;&#32467;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#20026;&#22522;&#30784;&#30340;&#24635;&#32467;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#12290;&#28982;&#32780;&#65292;&#24635;&#32467;&#19981;&#35268;&#21017;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#65292;&#27604;&#22914;&#31038;&#20132;&#23186;&#20307;&#21644;&#23458;&#25143;&#21453;&#39304;&#20013;&#21457;&#29616;&#30340;&#25991;&#26412;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38024;&#23545;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#30340;&#39044;&#23450;&#20041;&#26041;&#38754;&#65292;&#24573;&#30053;&#20102;&#21160;&#24577;&#21644;&#26080;&#24207;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Disordered-DABS&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#21160;&#24577;&#26041;&#38754;&#30340;&#24635;&#32467;&#22522;&#20934;&#27979;&#35797;&#65292;&#19987;&#20026;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#37327;&#36523;&#23450;&#21046;&#12290;&#36890;&#36807;&#35843;&#25972;&#29616;&#26377;&#25968;&#25454;&#38598;&#20197;&#25552;&#39640;&#25104;&#26412;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#21644;&#35814;&#32454;&#30340;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;Disordered-DABS&#23545;&#24403;&#20195;&#24635;&#32467;&#27169;&#22411;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;GPT-3.5&#31561;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10554v1 Announce Type: new  Abstract: Aspect-based summarization has seen significant advancements, especially in structured text. Yet, summarizing disordered, large-scale texts, like those found in social media and customer feedback, remains a significant challenge. Current research largely targets predefined aspects within structured texts, neglecting the complexities of dynamic and disordered environments. Addressing this gap, we introduce Disordered-DABS, a novel benchmark for dynamic aspect-based summarization tailored to unstructured text. Developed by adapting existing datasets for cost-efficiency and scalability, our comprehensive experiments and detailed human evaluations reveal that Disordered-DABS poses unique challenges to contemporary summarization models, including state-of-the-art language models such as GPT-3.5.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept-1K&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;LoRA&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#24615;&#33021;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#32531;&#35299;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.08526</link><description>&lt;p&gt;
Concept-1K&#65306;&#19968;&#31181;&#29992;&#20110;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Concept-1K: A Novel Benchmark for Instance Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08526
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept-1K&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;LoRA&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#24615;&#33021;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#32531;&#35299;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#65288;IL&#65289;&#23545;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20154;&#31867;&#32423;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;IL&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#26080;&#27861;&#35780;&#20272;PLM&#20013;&#30340;&#36951;&#24536;&#65292;&#20351;&#20154;&#35823;&#20197;&#20026;PLM&#19981;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;IL&#22330;&#26223;&#65292;&#31216;&#20026;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#25903;&#25345;&#25968;&#37327;&#32423;&#26356;&#22823;&#30340;IL&#27493;&#39588;&#30340;&#26032;&#25968;&#25454;&#38598;Concept-1K&#12290;&#22522;&#20110;&#23545;Concept-1K&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#19988;&#36951;&#24536;&#21463;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;&#19968;&#31181;&#27969;&#34892;&#30340;&#24494;&#35843;&#25216;&#26415;LoRA&#37117;&#26410;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;PLM&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#40723;&#21169;&#35774;&#35745;&#26356;&#24378;&#22823;&#30340;&#25216;&#26415;&#20197;&#20943;&#36731;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental learning (IL) is essential to realize the human-level intelligence in the neural network. However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting. To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size. Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance. Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs. The data, code and scripts ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#28145;&#20837;&#35780;&#20272;&#20102;&#22312;&#23454;&#36341;&#20013;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#30340;&#22269;&#38469;&#39033;&#30446;&#38598;&#20013;&#35780;&#20272;&#65292;&#23545;&#19968;&#20010;&#22312;FIFA World Cup&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#30340;&#30495;&#23454;&#37096;&#32626;&#30340;FootballDB&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.08349</link><description>&lt;p&gt;
&#22522;&#20110;&#30495;&#23454;&#29992;&#25143;&#26597;&#35810;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#28145;&#20837;&#35780;&#20272;&#20102;&#22312;&#23454;&#36341;&#20013;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#30340;&#22269;&#38469;&#39033;&#30446;&#38598;&#20013;&#35780;&#20272;&#65292;&#23545;&#19968;&#20010;&#22312;FIFA World Cup&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#30340;&#30495;&#23454;&#37096;&#32626;&#30340;FootballDB&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#65288;&#20063;&#31216;&#20026;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#65289;&#24050;&#25104;&#20026;&#24357;&#21512;&#29992;&#25143;&#33021;&#21147;&#19982;&#22522;&#20110;SQL&#30340;&#25968;&#25454;&#35775;&#38382;&#20043;&#38388;&#24046;&#36317;&#30340;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#31995;&#32479;&#23558;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#35831;&#27714;&#36716;&#21270;&#20026;&#29305;&#23450;&#25968;&#25454;&#24211;&#30340;&#26377;&#25928;SQL&#35821;&#21477;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#20351;&#24471;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#21463;&#30410;&#21290;&#27973;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20123;&#31995;&#32479;&#22312;&#24120;&#24120;&#26159;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19981;&#26029;&#21462;&#24471;&#26032;&#30340;&#39640;&#20998;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#12289;&#29616;&#23454;&#22330;&#26223;&#20013;&#23545;&#19981;&#21516;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#31995;&#32479;&#24615;&#25506;&#32034;&#26126;&#26174;&#32570;&#20047;&#12290;&#26412;&#25991;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#22269;&#38469;&#39033;&#30446;&#20851;&#20110;&#25991;&#26412;&#21040;SQL&#30028;&#38754;&#30340;&#38598;&#20013;&#35780;&#20272;&#65292;&#25552;&#20379;&#20102;&#23545;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#22312;&#23454;&#36341;&#20013;&#25968;&#25454;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#39318;&#27425;&#28145;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20110;FootballDB&#30340;&#30495;&#23454;&#37096;&#32626;&#65292;&#35813;&#31995;&#32479;&#22312;FIFA World Cup&#30340;&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access. These systems translate user requests in natural language to valid SQL statements for a specific database. Recent Text-to-SQL systems have benefited from the rapid improvement of transformer-based language models. However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on -- often synthetic -- benchmark datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing. This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces. Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA Wor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LoRA-drop&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;LoRA&#36755;&#20986;&#35780;&#20272;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#20445;&#30041;&#37325;&#35201;&#23618;&#30340;LoRA&#65292;&#20854;&#20313;&#23618;&#20849;&#20139;&#30456;&#21516;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LoRA-drop&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07721</link><description>&lt;p&gt;
LoRA-drop&#65306;&#22522;&#20110;&#36755;&#20986;&#35780;&#20272;&#30340;&#39640;&#25928;LoRA&#21442;&#25968;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LoRA-drop&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;LoRA&#36755;&#20986;&#35780;&#20272;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#20445;&#30041;&#37325;&#35201;&#23618;&#30340;LoRA&#65292;&#20854;&#20313;&#23618;&#20849;&#20139;&#30456;&#21516;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LoRA-drop&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#20026;&#27599;&#20010;&#23618;&#24341;&#20837;&#36741;&#21161;&#21442;&#25968;&#65292;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#24403;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#22411;&#26102;&#65292;&#20173;&#28982;&#38754;&#20020;&#36164;&#28304;&#28040;&#32791;&#30340;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#23618;&#30340;LoRA&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#26469;&#37319;&#29992;&#21098;&#26525;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#21482;&#20998;&#26512;&#20102;&#21442;&#25968;&#30340;&#29305;&#24449;&#20197;&#35780;&#20272;&#20854;&#37325;&#35201;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#21442;&#25968;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;LoRA&#30340;&#36755;&#20986;&#26159;&#30452;&#25509;&#24433;&#21709;&#20923;&#32467;&#27169;&#22411;&#30340;&#22240;&#32032;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoRA-drop&#65292;&#36890;&#36807;&#20998;&#26512;LoRA&#36755;&#20986;&#26469;&#35780;&#20272;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#20445;&#30041;&#37325;&#35201;&#23618;&#30340;LoRA&#65292;&#32780;&#20854;&#20182;&#23618;&#30340;LoRA&#20849;&#20139;&#30456;&#21516;&#30340;&#21442;&#25968;&#12290;&#22312;NLU&#21644;NLG&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;LoRA-drop&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06900</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#35782;&#21035;&#27602;&#24615;&#21527;&#65311;&#32467;&#26500;&#21270;&#27602;&#24615;&#35843;&#26597;&#26694;&#26550;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#36981;&#23432;&#31038;&#20250;&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#23384;&#22312;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#27602;&#24615;&#24230;&#37327;&#20381;&#36182;&#20110;&#22312;&#29305;&#23450;&#27602;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#30721;&#22120;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#22806;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#25152;&#20551;&#23450;&#30340;&#27602;&#24615;&#23450;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#33258;&#21160;&#40065;&#26834;&#24230;&#37327;&#65292;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#22238;&#24212;&#26159;&#21542;&#20855;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#27602;&#24615;&#22240;&#32032;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#24230;&#37327;&#25351;&#26631;LLMs As ToxiciTy Evaluators&#65288;LATTE&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;F1&#24471;&#20998;&#27604;&#29616;&#26377;&#25216;&#26415;&#25351;&#26631;&#25552;&#39640;&#20102;12&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19978;&#28216;&#27602;&#24615;&#23545;&#24230;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#21040;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#25512;&#36831;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27969;&#25968;&#25454;&#22788;&#29702;&#20013;&#21516;&#26102;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.04513</link><description>&lt;p&gt;
&#22312;&#27969;&#25968;&#25454;&#19978;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#30340;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Cascade Learning for Efficient Inference over Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04513
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#21040;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#25512;&#36831;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27969;&#25968;&#25454;&#22788;&#29702;&#20013;&#21516;&#26102;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#22238;&#31572;&#20851;&#20110;&#25968;&#25454;&#27969;&#30340;&#22797;&#26434;&#26597;&#35810;&#26041;&#38754;&#20855;&#26377;&#22825;&#28982;&#30340;&#20248;&#21183;&#65292;&#20294;&#26159; LLM &#25512;&#29702;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#65292;&#36825;&#26159;&#39318;&#20010;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#36825;&#37324;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#22120;&#65289;&#24320;&#22987;&#65292;&#21040;&#24378;&#22823;&#30340; LLM &#32467;&#26463;&#65292;&#24182;&#37197;&#22791;&#19968;&#20010;&#20915;&#23450;&#22312;&#32473;&#23450;&#36755;&#20837;&#19978;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#30340;&#25512;&#36831;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#22312;&#32447;&#23398;&#20064;&#32423;&#32852;&#30340;&#20219;&#21153;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20026;&#35813;&#38382;&#39064;&#25552;&#20379;&#20102;&#26080;&#36951;&#25022;&#31639;&#27861;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982; LLM &#30456;&#24403;&#65292;&#21516;&#26102;&#23558;&#25512;&#29702;&#25104;&#26412;&#21066;&#20943;&#20102;&#22810;&#36798; 90%&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#27969;&#22788;&#29702;&#20013;&#30340;&#25928;&#33021;&#21644;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04068</link><description>&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;&#39537;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrieve to Explain: Evidence-driven Predictions with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04068
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#35821;&#35328;&#27169;&#22411;&#65292;&#24448;&#24448;&#38590;&#20197;&#28145;&#20837;&#20998;&#26512;&#12290;&#40657;&#30418;&#27169;&#22411;&#21487;&#33021;&#25513;&#30422;&#20102;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#38382;&#39064;&#21644;&#26377;&#23475;&#20559;&#24046;&#12290;&#23545;&#20110;&#20154;&#26426;&#21327;&#20316;&#36807;&#31243;&#26469;&#35828;&#65292;&#19981;&#36879;&#26126;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#32570;&#20047;&#20449;&#20219;&#65292;&#38480;&#21046;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;Retrieve to Explain&#65292;&#31616;&#31216;R2E&#65289;&#12290;R2E&#26159;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#25991;&#26723;&#35821;&#26009;&#24211;&#20013;&#30340;&#35777;&#25454;&#65292;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#27169;&#26495;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;R2E&#33021;&#22815;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#30340;&#35777;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#27169;&#26495;&#21270;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#20013;&#12290;&#25105;&#20204;&#22312;&#36890;&#36807;&#20998;&#26512;&#24050;&#21457;&#34920;&#30340;&#31185;&#23398;&#25991;&#29486;&#36827;&#34892;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#30340;&#23454;&#38469;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#34892;&#19994;&#26631;&#20934;&#30340;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01737</link><description>&lt;p&gt;
&#20026;&#31038;&#20132;&#24863;&#30693;&#30340;&#35848;&#21028;&#23545;&#35805;&#24320;&#21457;&#36741;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;LLM&#20195;&#29702;&#20197;&#20943;&#36731;&#22810;&#20195;&#29702;&#35774;&#32622;&#19979;&#35848;&#21028;&#20013;&#30340;&#31038;&#20132;&#35268;&#33539;&#36829;&#21453;&#12290;&#25105;&#20204;&#36890;&#36807;&#35753;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25198;&#28436;&#27599;&#27425;&#23545;&#35805;&#20013;&#30340;&#20004;&#21517;&#35848;&#21028;&#32773;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#35848;&#21028;&#12290;&#31532;&#19977;&#20010;LLM&#20805;&#24403;&#20462;&#27491;&#20195;&#29702;&#65292;&#37325;&#26032;&#32534;&#20889;&#36829;&#21453;&#35268;&#33539;&#30340;&#35805;&#35821;&#20197;&#25913;&#21892;&#35848;&#21028;&#32467;&#26524;&#12290;&#30001;&#20110;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#19981;&#23384;&#22312;&#25163;&#21160;&#26500;&#24314;&#30340;&#25968;&#25454;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20215;&#20540;&#24433;&#21709;&#30340;&#29615;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22522;&#20110;LLM&#30340;&#20462;&#27491;&#20195;&#29702;&#35782;&#21035;&#39640;&#36136;&#37327;&#30340;ICL&#31034;&#20363;&#65292;&#20854;&#20013;&#20215;&#20540;&#24433;&#21709;&#20989;&#25968;&#34913;&#37327;&#35848;&#21028;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#19982;&#31574;&#30053;&#23398;&#20064;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#20854;&#22312;&#19977;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#35848;&#21028;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20135;&#21697;&#38144;&#21806;&#12289;&#25151;&#20215;&#21644;&#34218;&#36164;&#35848;&#21028;&#12290;&#28304;&#20195;&#30721;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#23558;&#22312;&#25509;&#21463;&#21518;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.00798</link><description>&lt;p&gt;
&#27491;&#24335;-LLM&#65306;&#23558;&#24418;&#24335;&#35821;&#35328;&#21644;&#33258;&#28982;&#35821;&#35328;&#38598;&#25104;&#20110;&#21487;&#25511;&#30340;LLM&#26234;&#33021;&#20307;&#20013;
&lt;/p&gt;
&lt;p&gt;
Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25972;&#21512;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;LLM&#26234;&#33021;&#20307;&#26080;&#27861;&#25511;&#21046;&#30340;&#35745;&#21010;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#25552;&#39640;&#29983;&#25104;&#35745;&#21010;&#24615;&#33021;&#21644;&#30830;&#20445;&#21487;&#25511;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#25191;&#34892;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#22810;&#27493;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#20869;&#23481;&#29983;&#25104;&#36807;&#31243;&#20960;&#20046;&#26080;&#27861;&#25511;&#21046;&#65292;&#24403;&#21069;&#30340;LLM&#26234;&#33021;&#20307;&#32463;&#24120;&#29983;&#25104;&#26080;&#25928;&#25110;&#19981;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#36825;&#25439;&#23475;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#24615;&#33021;&#24182;&#30772;&#22351;&#20102;&#29992;&#25143;&#23545;LLM&#26234;&#33021;&#20307;&#30340;&#20449;&#20219;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#27491;&#24335;-LLM&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;LLM&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#31934;&#30830;&#24615;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#23558;&#20182;&#20204;&#23545;&#35745;&#21010;&#36807;&#31243;&#30340;&#35201;&#27714;&#25110;&#32422;&#26463;&#34920;&#36798;&#20026;&#33258;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#22312;&#33258;&#21160;&#26426;&#30340;&#30417;&#30563;&#19979;&#65292;&#20351;&#29992;&#22522;&#20110;&#22534;&#26632;&#30340;LLM&#35745;&#21010;&#29983;&#25104;&#36807;&#31243;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#35745;&#21010;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#65292;&#20174;&#32780;&#20351;&#35745;&#21010;&#36807;&#31243;&#21487;&#25511;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#20219;&#21153;&#21644;&#23454;&#38469;&#30340;&#30495;&#23454;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#19988;obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.
&lt;/p&gt;
&lt;p&gt;
Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;Transformer&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.00743</link><description>&lt;p&gt;
Transformer&#30340;&#22909;&#22788;&#65306;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;Transformer&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#35299;&#37322;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#25512;&#29702;&#38454;&#27573;&#33021;&#22815;&#23398;&#20064;&#19978;&#19979;&#25991;&#20013;&#30340;&#27010;&#24565;&#12290;&#29616;&#26377;&#30340;&#25991;&#29486;&#65292;&#20363;&#22914;\citet{zhang2023trained,huang2023context}&#23545;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#20294;&#26159;&#20182;&#20204;&#20551;&#35774;&#27599;&#20010;&#26679;&#26412;&#30340;&#36755;&#20837;$x_i$&#21644;&#36755;&#20986;$y_i$&#37117;&#34987;&#23884;&#20837;&#21040;&#30456;&#21516;&#30340;&#20196;&#29260;&#20013;&#65288;&#21363;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#23427;&#20204;&#21576;&#29616;&#20026;&#20004;&#20010;&#20196;&#29260;&#65288;&#21363;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;\cite{wibisono2023role}&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;Transformer&#26550;&#26500;&#30340;&#22909;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#30456;&#24212;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;Transformer&#21487;&#20197;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Transformer&#20013;&#36215;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#29992;&#30340;&#30830;&#20999;&#32452;&#20214;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;&#24102;&#26377;&#20004;&#23618;softmax&#65288;&#33258;&#25105;&#65289;&#27880;&#24847;&#21147;&#21644;&#21069;&#30651;&#24615;&#27880;&#24847;&#21147;&#25513;&#30721;&#30340;Transformer&#21487;&#20197;&#20174;&#25552;&#31034;&#20013;&#23398;&#20064;&#65292;&#22914;&#26524;$y_i$&#22312;&#20196;&#29260;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token n
&lt;/p&gt;</description></item><item><title>Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2312.11805</link><description>&lt;p&gt;
Gemini&#65306;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemini: A Family of Highly Capable Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11805
&lt;/p&gt;
&lt;p&gt;
Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#31995;&#21015;Gemini&#65292;&#23637;&#31034;&#20986;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;Gemini&#31995;&#21015;&#21253;&#25324;Ultra&#12289;Pro&#21644;Nano&#23610;&#23544;&#65292;&#36866;&#29992;&#20110;&#20174;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#21040;&#35774;&#22791;&#20869;&#23384;&#21463;&#38480;&#24212;&#29992;&#30340;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;32&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;30&#20010;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839; - &#26174;&#33879;&#22320;&#26159;&#31532;&#19968;&#20010;&#22312;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#32771;&#35797;&#22522;&#20934;&#27979;&#35797;MMLU&#19978;&#23454;&#29616;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#34920;&#29616;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#27599;&#19968;&#20010;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#25913;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#12290;&#25105;&#20204;&#30456;&#20449;Gemini&#31995;&#21015;&#22312;&#36328;&#27169;&#24577;&#25512;&#29702;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#26032;&#33021;&#21147;&#23558;&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#29992;&#20363;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#22320;&#21521;&#29992;&#25143;&#25552;&#20379;Gemini&#27169;&#22411;&#30340;&#35757;&#32451;&#21518;&#21644;&#37096;&#32626;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11805v2 Announce Type: replace-cross  Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services includi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;</title><link>http://arxiv.org/abs/2401.15269</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25913;&#21892;&#21307;&#30103;&#25512;&#29702;&#33021;&#21147;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Self-BioRAG&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#33258;&#25105;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19987;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;GPT-4&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#20174;&#22810;&#39033;&#36873;&#25321;&#39064;&#21040;&#38271;&#31687;&#29983;&#25104;&#31561;&#22810;&#26679;&#21270;&#25361;&#25112;&#30340;&#37324;&#31243;&#30865;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#32534;&#30721;&#30693;&#35782;&#26080;&#27861;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30693;&#35782;&#35821;&#26009;&#24211;&#20013;&#25628;&#32034;&#25991;&#26723;&#24182;&#26080;&#26465;&#20214;&#25110;&#26377;&#36873;&#25321;&#22320;&#23558;&#20854;&#38468;&#21152;&#21040;LLMs&#30340;&#36755;&#20837;&#26469;&#36827;&#34892;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#29305;&#23450;&#38382;&#39064;&#26102;&#65292;&#20986;&#29616;&#20102;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#33719;&#21462;&#19981;&#27491;&#30830;&#30340;&#25991;&#26723;&#25110;&#20570;&#20986;&#19981;&#20934;&#30830;&#30340;&#21028;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#21307;&#23398;&#25991;&#26412;&#26694;&#26550;Self-BioRAG&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#12289;&#26816;&#32034;&#39046;&#22495;&#29305;&#23450;&#25991;&#26723;&#21644;&#33258;&#25105;&#21453;&#24605;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;84k&#20010;&#32463;&#36807;&#36807;&#28388;&#30340;&#29983;&#29289;&#21307;&#23398;&#25351;&#20196;&#38598;&#26469;&#35757;&#32451;Self-BioRAG&#65292;&#23427;&#20855;&#22791;&#35780;&#20272;&#33258;&#24049;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its gene
&lt;/p&gt;</description></item><item><title>Taiyi-Diffusion-XL&#26159;&#19968;&#20010;&#20013;&#33521;&#21452;&#35821;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;CLIP&#21644;Stable-Diffusion-XL&#30340;&#33021;&#21147;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#36890;&#36807;&#21452;&#35821;&#36830;&#32493;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#12290;&#27169;&#22411;&#36890;&#36807;&#23558;&#24120;&#29992;&#30340;&#27721;&#23383;&#25972;&#21512;&#21040;CLIP&#30340;&#20998;&#35789;&#22120;&#21644;&#23884;&#20837;&#23618;&#20013;&#65292;&#20197;&#21450;&#20351;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20016;&#23500;&#25991;&#26412;&#25552;&#31034;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22270;&#20687;&#26631;&#39064;&#21644;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.14688</link><description>&lt;p&gt;
Taiyi-Diffusion-XL: &#20511;&#21161;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#25512;&#36827;&#21452;&#35821;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support. (arXiv:2401.14688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14688
&lt;/p&gt;
&lt;p&gt;
Taiyi-Diffusion-XL&#26159;&#19968;&#20010;&#20013;&#33521;&#21452;&#35821;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;CLIP&#21644;Stable-Diffusion-XL&#30340;&#33021;&#21147;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#36890;&#36807;&#21452;&#35821;&#36830;&#32493;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#12290;&#27169;&#22411;&#36890;&#36807;&#23558;&#24120;&#29992;&#30340;&#27721;&#23383;&#25972;&#21512;&#21040;CLIP&#30340;&#20998;&#35789;&#22120;&#21644;&#23884;&#20837;&#23618;&#20013;&#65292;&#20197;&#21450;&#20351;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20016;&#23500;&#25991;&#26412;&#25552;&#31034;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22270;&#20687;&#26631;&#39064;&#21644;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#21319;&#20102;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#28982;&#32780;&#22312;&#21452;&#35821;&#25110;&#20013;&#25991;&#35821;&#35328;&#25903;&#25345;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#24320;&#28304;&#27169;&#22411;&#32570;&#21475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Taiyi-Diffusion-XL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20013;&#33521;&#21452;&#35821;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;CLIP&#21644;Stable-Diffusion-XL&#33021;&#21147;&#30340;&#25193;&#23637;&#65292;&#24182;&#36890;&#36807;&#21452;&#35821;&#36830;&#32493;&#39044;&#35757;&#32451;&#30340;&#36807;&#31243;&#26469;&#24320;&#21457;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#23558;&#26368;&#24120;&#29992;&#30340;&#27721;&#23383;&#25972;&#21512;&#21040;CLIP&#30340;&#20998;&#35789;&#22120;&#21644;&#23884;&#20837;&#23618;&#20013;&#26469;&#25193;&#23637;&#35789;&#27719;&#37327;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#21152;&#20837;&#20102;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#20016;&#23500;&#25991;&#26412;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#26631;&#39064;&#21644;&#26356;&#39640;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#36825;&#20123;&#22686;&#24378;&#25514;&#26045;&#38543;&#21518;&#24212;&#29992;&#20110;&#19979;&#28216;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#21457;&#30340;CLIP&#27169;&#22411;&#22312;&#21452;&#35821;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#21452;&#35821;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25972;&#21512;&#20351;&#35813;&#27169;&#22411;&#22312;&#20013;&#33521;&#25991;&#22330;&#26223;&#19979;&#20855;&#26377;&#22343;&#34913;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in text-to-image models have significantly enhanced image generation capabilities, yet a notable gap of open-source models persists in bilingual or Chinese language support. To address this need, we present Taiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model which is developed by extending the capabilities of CLIP and Stable-Diffusion-XL through a process of bilingual continuous pre-training. This approach includes the efficient expansion of vocabulary by integrating the most frequently used Chinese characters into CLIP's tokenizer and embedding layers, coupled with an absolute position encoding expansion. Additionally, we enrich text prompts by large vision-language model, leading to better images captions and possess higher visual quality. These enhancements are subsequently applied to downstream text-to-image models. Our empirical results indicate that the developed CLIP model excels in bilingual image-text retrieval.Furthermore, the bilin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#30340;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#20197;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.14043</link><description>&lt;p&gt;
&#26397;&#30528;&#30446;&#26631;&#23548;&#21521;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Goal-oriented Large Language Model Prompting: A Survey. (arXiv:2401.14043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#30340;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#20197;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#32780;&#25552;&#31034;&#24037;&#31243;&#22312;&#20248;&#21270;LLM&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#24378;&#35843;&#35774;&#35745;&#25552;&#31034;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#20445;&#25345;&#20154;&#31867;&#36861;&#27714;LLM&#20687;&#20154;&#31867;&#24605;&#32771;&#30340;&#20154;&#31867;&#23398;&#20551;&#35774;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#30340;&#37325;&#35201;&#24615;&#65292;&#35813;&#20844;&#24335;&#25351;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#23558;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#26041;&#27861;&#20998;&#20026;&#20116;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#38454;&#27573;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#24076;&#26395;&#36827;&#19968;&#27493;&#24378;&#35843;&#21644;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown prominent performance in various downstream tasks in which prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not as an overview of current prompt engineering methods, aims to highlight the limitation of designing prompts while holding an anthropomorphic assumption that expects LLMs to think like humans. From our review of 35 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework by summarizing ten applicable tasks. With four future directions proposed, we hope to further emphasize and promote goal-oriented prompt engineering.
&lt;/p&gt;</description></item><item><title>LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13586</link><description>&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#25552;&#31034;&#26435;&#37325;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13586
&lt;/p&gt;
&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23567;&#22411;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#22914;&#20309;&#24433;&#21709;&#22312;&#25351;&#20196;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;7B&#22823;&#23567;&#30340;LLaMA&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#37325;&#29616;&#20102;&#26031;&#22374;&#31119;&#22823;&#23398;&#30340;Alpaca&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;LLaMA 1&#21644;LLaMA 2&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;PLW&#20043;&#38388;&#23384;&#22312;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#22312;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19981;&#21463;PLW&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10134</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;&#30340;&#26410;&#26469;&#20132;&#36890;&#24773;&#20917;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#24378;&#35843;&#24320;&#21457;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20294;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24182;&#26410;&#30456;&#24212;&#25552;&#39640;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;LLMs&#20027;&#35201;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#26469;&#36827;&#27493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22522;&#26412;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ST-LLM&#23558;&#27599;&#20010;&#20301;&#32622;&#30340;&#26102;&#38388;&#27493;&#38271;&#23450;&#20041;&#20026;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#26469;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#34920;&#31034;&#34987;&#34701;&#21512;&#20197;&#20026;&#27599;&#20010;&#26631;&#35760;&#25552;&#20379;&#32479;&#19968;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#21482;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340; Transformers &#21487;&#20197;&#34987;&#35270;&#20026;&#26080;&#38480;&#22810;&#29366;&#24577;&#30340; RNNs&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22266;&#23450;&#38544;&#34255;&#29366;&#24577;&#30340;&#22823;&#23567;&#26469;&#36716;&#25442;&#20026;&#26377;&#38480;&#22810;&#29366;&#24577;&#30340; RNNs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#31574;&#30053; TOVA&#65292;&#22312;&#22810;&#20010;&#38271;&#36317;&#31163;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#31574;&#30053;&#65292;&#24182;&#19988;&#19982;&#23436;&#25972;&#27169;&#22411;&#20960;&#20046;&#25345;&#24179;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20165;&#20351;&#29992;&#21407;&#22987;&#32531;&#23384;&#22823;&#23567;&#30340; $\frac{1}{8}$&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer &#35299;&#30721;&#22120; LLMs &#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#34920;&#29616;&#20026; RNNs&#12290;</title><link>http://arxiv.org/abs/2401.06104</link><description>&lt;p&gt;
Transformers &#26159;&#22810;&#29366;&#24577;&#30340; RNNs
&lt;/p&gt;
&lt;p&gt;
Transformers are Multi-State RNNs. (arXiv:2401.06104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#21482;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340; Transformers &#21487;&#20197;&#34987;&#35270;&#20026;&#26080;&#38480;&#22810;&#29366;&#24577;&#30340; RNNs&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22266;&#23450;&#38544;&#34255;&#29366;&#24577;&#30340;&#22823;&#23567;&#26469;&#36716;&#25442;&#20026;&#26377;&#38480;&#22810;&#29366;&#24577;&#30340; RNNs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#25442;&#31574;&#30053; TOVA&#65292;&#22312;&#22810;&#20010;&#38271;&#36317;&#31163;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#31574;&#30053;&#65292;&#24182;&#19988;&#19982;&#23436;&#25972;&#27169;&#22411;&#20960;&#20046;&#25345;&#24179;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20165;&#20351;&#29992;&#21407;&#22987;&#32531;&#23384;&#22823;&#23567;&#30340; $\frac{1}{8}$&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer &#35299;&#30721;&#22120; LLMs &#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#34920;&#29616;&#20026; RNNs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21482;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340; Transformers &#23454;&#38469;&#19978;&#21487;&#20197;&#34987;&#27010;&#24565;&#21270;&#20026;&#26080;&#38480;&#22810;&#29366;&#24577;&#30340; RNNs&#65292;&#21363;&#20855;&#26377;&#26080;&#38480;&#38544;&#34255;&#29366;&#24577;&#23610;&#23544;&#30340; RNNs &#21464;&#31181;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340; Transformers &#21487;&#20197;&#36890;&#36807;&#22266;&#23450;&#20854;&#38544;&#34255;&#29366;&#24577;&#22823;&#23567;&#26469;&#36716;&#25442;&#20026;&#26377;&#38480;&#22810;&#29366;&#24577;&#30340; RNNs&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20960;&#31181;&#29616;&#26377;&#30340; Transformers &#32531;&#23384;&#21387;&#32553;&#25216;&#26415;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#36825;&#31181;&#36716;&#25442;&#31574;&#30053;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053; TOVA&#65292;&#30456;&#27604;&#20110;&#36825;&#20123;&#31574;&#30053;&#26356;&#31616;&#21333;&#12290;&#25105;&#20204;&#22312;&#20960;&#31181;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102; TOVA &#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#22522;&#20934;&#31574;&#30053;&#65292;&#21516;&#26102;&#19982;&#23436;&#25972;&#65288;&#26080;&#38480;&#65289;&#27169;&#22411;&#20960;&#20046;&#19981;&#30456;&#19978;&#19979;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20165;&#20351;&#29992;&#21407;&#22987;&#32531;&#23384;&#22823;&#23567;&#30340; $\frac{1}{8}$&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer &#35299;&#30721;&#22120; LLMs &#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#34920;&#29616;&#20026; RNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are considered conceptually different compared to the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as infinite multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that pretrained transformers can be converted into $\textit{finite}$ multi-state RNNs by fixing the size of their hidden state. We observe that several existing transformers cache compression techniques can be framed as such conversion policies, and introduce a novel policy, TOVA, which is simpler compared to these policies. Our experiments with several long range tasks indicate that TOVA outperforms all other baseline policies, while being nearly on par with the full (infinite) model, and using in some cases only $\frac{1}{8}$ of the original cache size. Our results indicate that transformer decoder LLMs often behave in practice as RNNs. They also lay
&lt;/p&gt;</description></item><item><title>Muffin&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#26080;&#30410;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#29983;&#25104;&#25903;&#25345;&#24615;&#22238;&#22797;&#26102;&#32771;&#34385;&#21040;&#22810;&#20010;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22810;&#26041;&#20301;&#30340;AI&#21453;&#39304;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#26080;&#30410;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2401.05928</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26041;&#20301;AI&#21453;&#39304;&#20943;&#36731;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#30340;&#26080;&#30410;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted AI Feedback. (arXiv:2401.05928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05928
&lt;/p&gt;
&lt;p&gt;
Muffin&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#26080;&#30410;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#29983;&#25104;&#25903;&#25345;&#24615;&#22238;&#22797;&#26102;&#32771;&#34385;&#21040;&#22810;&#20010;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22810;&#26041;&#20301;&#30340;AI&#21453;&#39304;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#26080;&#30410;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20943;&#36731;&#29992;&#25143;&#30340;&#24773;&#24863;&#22256;&#25200;&#24182;&#24110;&#21161;&#20182;&#20204;&#35299;&#20915;&#25361;&#25112;&#12290;&#20026;&#20102;&#29983;&#25104;&#25903;&#25345;&#24615;&#22238;&#22797;&#65292;&#24517;&#39035;&#32771;&#34385;&#21040;&#22810;&#20010;&#22240;&#32032;&#65292;&#22914;&#20849;&#24773;&#12289;&#25903;&#25345;&#31574;&#30053;&#21644;&#22238;&#22797;&#36830;&#36143;&#24615;&#65292;&#36825;&#20123;&#22312;&#20043;&#21069;&#30340;&#26041;&#27861;&#20013;&#24050;&#32463;&#24471;&#21040;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#27169;&#22411;&#20598;&#23572;&#20250;&#29983;&#25104;&#26080;&#30410;&#30340;&#22238;&#22797;&#65292;&#36825;&#20123;&#22238;&#22797;&#24847;&#22270;&#25552;&#20379;&#25903;&#25345;&#65292;&#20294;&#21364;&#20135;&#29983;&#36866;&#24471;&#20854;&#21453;&#30340;&#25928;&#26524;&#12290;&#26681;&#25454;&#24515;&#29702;&#23398;&#21644;&#27807;&#36890;&#29702;&#35770;&#65292;&#34429;&#28982;&#21482;&#26159;&#21333;&#19968;&#22240;&#32032;&#30340;&#34920;&#29616;&#19981;&#20339;&#21487;&#33021;&#20250;&#23548;&#33268;&#22238;&#22797;&#26080;&#30410;&#12290;&#20174;&#27169;&#22411;&#35757;&#32451;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#38454;&#27573;&#27809;&#26377;&#25509;&#35302;&#21040;&#26080;&#30410;&#30340;&#22238;&#22797;&#65292;&#23427;&#20204;&#26080;&#27861;&#21028;&#26029;&#23427;&#20204;&#29983;&#25104;&#30340;&#26631;&#35760;&#26159;&#21542;&#20250;&#23548;&#33268;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#26080;&#30410;&#22238;&#22797;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#26041;&#20301;AI&#21453;&#39304;&#20943;&#36731;&#24773;&#24863;&#25903;&#25345;&#65288;Muffin&#65289;&#30340;&#26032;&#22411;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emotional support conversation system aims to alleviate users' emotional distress and assist them in addressing their challenges. To generate supportive responses, it is critical to consider multiple factors such as empathy, support strategies, and response coherence, as established in prior methods. Nonetheless, previous models occasionally generate unhelpful responses, which intend to provide support but display counterproductive effects. According to psychology and communication theories, poor performance in just one contributing factor might cause a response to be unhelpful. From the model training perspective, since these models have not been exposed to unhelpful responses during their training phase, they are unable to distinguish if the tokens they generate might result in unhelpful responses during inference. To address this issue, we introduce a novel model-agnostic framework named mitigating unhelpfulness with multifaceted AI feedback for emotional support (Muffin). Specif
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#65288;EDiSC&#65289;&#65292;&#32467;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;DiSC&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;EDiSC&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00541</link><description>&lt;p&gt;
&#19968;&#20010;&#24102;&#26377;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#30340;&#35770;&#25991;&#19982;&#19968;&#20010;&#20851;&#20110;&#21476;&#24076;&#33098;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek. (arXiv:2311.00541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23884;&#20837;&#24335;&#21382;&#26102;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#65288;EDiSC&#65289;&#65292;&#32467;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;DiSC&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;EDiSC&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#30340;&#24847;&#20041;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#21464;&#21270;&#65292;&#35789;&#20041;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#20250;&#28436;&#21464;&#12289;&#20986;&#29616;&#25110;&#28040;&#22833;&#12290;&#23545;&#20110;&#21476;&#20195;&#35821;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#35821;&#26009;&#24211;&#36890;&#24120;&#36739;&#23567;&#12289;&#31232;&#30095;&#19988;&#22024;&#26434;&#65292;&#20934;&#30830;&#24314;&#27169;&#36825;&#31181;&#21464;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#27492;&#23545;&#20110;&#24847;&#20041;&#21464;&#21270;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#21464;&#24471;&#37325;&#35201;&#12290;GASC&#21644;DiSC&#26159;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#32463;&#34987;&#29992;&#26469;&#20998;&#26512;&#21476;&#24076;&#33098;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30446;&#26631;&#35789;&#27719;&#30340;&#24847;&#20041;&#21464;&#21270;&#65292;&#20351;&#29992;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#24182;&#27809;&#26377;&#20511;&#21161;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#24110;&#21161;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#32473;&#23450;&#30446;&#26631;&#35789;&#27719;&#65288;&#22914;"kosmos"&#65292;&#24847;&#20026;&#35013;&#39280;&#12289;&#31209;&#24207;&#25110;&#19990;&#30028;&#65289;&#30340;&#24847;&#20041;&#34920;&#31034;&#20026;&#19978;&#19979;&#25991;&#35789;&#27719;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#24847;&#20041;&#30340;&#26222;&#36941;&#24615;&#34920;&#31034;&#20026;&#24847;&#20041;&#30340;&#20998;&#24067;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36827;&#34892;&#25311;&#21512;&#65292;&#20197;&#27979;&#37327;&#36825;&#20123;&#34920;&#31034;&#20013;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EDiSC&#65292;&#36825;&#26159;DiSC&#30340;&#23884;&#20837;&#29256;&#26412;&#65292;&#23427;&#23558;&#35789;&#23884;&#20837;&#19982;DiSC&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#26356;&#20248;&#31168;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;EDiSC&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small, sparse and noisy, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC and DiSC are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as "kosmos" (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using MCMC methods to measure temporal changes in these representations. In this paper, we introduce EDiSC, an embedded version of DiSC, which combines word embeddings with DiSC to provide superior model performance. We show empirically that EDiSC offers improved p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14735</link><description>&lt;p&gt;
&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#28508;&#21147;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. (arXiv:2310.14735v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25552;&#31034;&#24037;&#31243;&#26159;&#20026;LLM&#26500;&#24314;&#36755;&#20837;&#25991;&#26412;&#30340;&#36807;&#31243;&#65292;&#26159;&#20248;&#21270;LLM&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#38416;&#26126;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#35282;&#33394;&#25552;&#31034;&#12289;&#19968;&#27425;&#24615;&#25552;&#31034;&#21644;&#23569;&#37327;&#25552;&#31034;&#65292;&#20197;&#21450;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#65292;&#22914;&#24605;&#32500;&#38142;&#21644;&#24605;&#32500;&#26641;&#25552;&#31034;&#12290;&#26412;&#25991;&#36824;&#38416;&#36848;&#20102;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#27492;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21246;&#21202;&#20102;&#25552;&#31034;&#24037;&#31243;&#30740;&#31350;&#30340;&#21069;&#26223;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#23545;&#32467;&#26500;&#21644;&#20195;&#29702;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#24037;&#20855;&#20013;&#30340;&#20316;&#29992;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;&#35282;&#24230;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#35780;&#20272;&#25552;&#31034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23637;&#26395;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#32473;&#20104;LLM&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;LLMs&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#25805;&#20316;&#25216;&#33021;&#20013;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#24207;&#21015;&#65292;&#24182;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11604</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#32473;&#20104;LLM&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;LLMs&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#25805;&#20316;&#25216;&#33021;&#20013;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#24207;&#21015;&#65292;&#24182;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20104;&#20302;&#32423;&#25216;&#33021;&#36873;&#25321;&#26102;&#33021;&#22815;&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#39640;&#32423;&#35268;&#21010;&#22120;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#35748;&#20026;LLMs&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#30693;&#35782;&#26469;&#29992;&#20110;&#20302;&#32423;&#36712;&#36857;&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#35843;&#26597;&#20102;&#24403;&#32473;&#20104;LLM&#65288;GPT-4&#65289;&#20165;&#33021;&#35775;&#38382;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#26102;&#65292;&#23427;&#33021;&#21542;&#30452;&#25509;&#39044;&#27979;&#19968;&#31995;&#21015;&#23494;&#38598;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#29992;&#20110;&#25805;&#20316;&#25216;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#25552;&#31034;&#65292;&#27809;&#26377;&#20219;&#20309;&#19978;&#19979;&#25991;&#31034;&#20363;&#12289;&#36816;&#21160;&#21407;&#35821;&#25110;&#22806;&#37096;&#36712;&#36857;&#20248;&#21270;&#22120;&#65292;&#23427;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#22914;&#8220;&#25171;&#24320;&#29942;&#30422;&#8221;&#21644;&#8220;&#29992;&#28023;&#32501;&#25830;&#25325;&#30424;&#23376;&#8221;&#65292;&#20197;&#21450;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20010;&#25552;&#31034;&#20013;&#21738;&#20123;&#35774;&#35745;&#36873;&#25321;&#26368;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#32467;&#35770;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#39318;&#27425;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20943;&#36731;&#25514;&#26045;&#30340;&#25506;&#35752;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#21644;&#35843;&#26597;&#29616;&#26377;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#24182;&#35752;&#35770;&#29616;&#26377;&#30340;&#20943;&#36731;&#31574;&#30053;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20851;&#38190;&#24046;&#36317;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.01424</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#38544;&#31169;&#39118;&#38505;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey. (arXiv:2310.01424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20943;&#36731;&#25514;&#26045;&#30340;&#25506;&#35752;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#21644;&#35843;&#26597;&#29616;&#26377;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#24182;&#35752;&#35770;&#29616;&#26377;&#30340;&#20943;&#36731;&#31574;&#30053;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#20851;&#38190;&#24046;&#36317;&#21644;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#20854;&#34987;&#24191;&#27867;&#37319;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#12290;&#38500;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36824;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#39118;&#38505;&#65292;&#21253;&#25324;&#38544;&#31169;&#39118;&#38505;&#12290;&#23588;&#20854;&#26159;&#38543;&#30528;LMs&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#28508;&#21147;&#22686;&#21152;&#65292;&#20174;&#32780;&#23548;&#33268;&#27844;&#38706;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#38543;&#30528;LMs&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#36825;&#20123;&#38544;&#31169;&#39118;&#38505;&#20197;&#21450;&#22914;&#20309;&#20943;&#36731;&#23427;&#20204;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#20102;&#35299;LM&#38544;&#31169;&#25915;&#20987;&#21644;&#20943;&#36731;&#25514;&#26045;&#30340;&#30693;&#35782;&#29366;&#20917;&#65292;&#21253;&#25324;&#38656;&#35201;&#26356;&#22810;&#24037;&#20316;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20221;&#20851;&#20110;LM&#38544;&#31169;&#30340;&#25216;&#26415;&#35843;&#26597;&#12290;&#25105;&#20204;&#65288;i&#65289;&#30830;&#23450;&#20102;&#25915;&#20987;&#22312;LMs&#19978;&#23384;&#22312;&#30340;&#26174;&#33879;&#32500;&#24230;&#30340;&#20998;&#31867;&#27861;&#65292;&#65288;ii&#65289;&#35843;&#26597;&#29616;&#26377;&#25915;&#20987;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#26469;&#31361;&#20986;&#20027;&#35201;&#36235;&#21183;&#65292;&#65288;iii&#65289;&#35752;&#35770;&#29616;&#26377;&#30340;&#20943;&#36731;&#31574;&#30053;&#65292;&#31361;&#20986;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#35782;&#21035;&#20851;&#38190;&#24046;&#36317;&#65292;&#23637;&#31034;&#24320;&#25918;&#38382;&#39064;&#21644;&#24314;&#35758;&#26410;&#26469;&#24037;&#20316;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in language models (LMs) have led to their adoption across many sectors. Alongside the potential benefits, such models present a range of risks, including around privacy. In particular, as LMs have grown in size, the potential to memorise aspects of their training data has increased, resulting in the risk of leaking private information. As LMs become increasingly widespread, it is vital that we understand such privacy risks and how they might be mitigated. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first technical survey on LM privacy. We (i) identify a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey existing attacks and use our taxonomy of dimensions to highlight key trends, (iii) discuss existing mitigation strategies, highlighting their strengths and limitations, identifying key gaps and demonstrating open problems and are
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35266;&#28857;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#25945;&#32946;&#23398;&#29702;&#24565;&#30340;&#21453;&#39304;&#26694;&#26550;FELT&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21453;&#39304;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#31616;&#21270;&#20102;&#29616;&#26377;&#30340;&#25163;&#24037;&#35774;&#35745;&#21453;&#39304;&#65292;&#36824;&#20026;NLF&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.00279</link><description>&lt;p&gt;
&#35753;&#25105;&#26469;&#25945;&#20320;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#25945;&#32946;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Let Me Teach You: Pedagogical Foundations of Feedback for Language Models. (arXiv:2307.00279v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00279
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35266;&#28857;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#25945;&#32946;&#23398;&#29702;&#24565;&#30340;&#21453;&#39304;&#26694;&#26550;FELT&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21453;&#39304;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#31616;&#21270;&#20102;&#29616;&#26377;&#30340;&#25163;&#24037;&#35774;&#35745;&#21453;&#39304;&#65292;&#36824;&#20026;NLF&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#65288;NLF&#65289;&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#36884;&#24452;&#12290;&#23613;&#31649;NLF&#21487;&#20197;&#20256;&#36798;&#20016;&#23500;&#22810;&#26679;&#30340;&#20449;&#24687;&#65292;&#20294;&#24448;&#24448;&#26159;&#25163;&#24037;&#35774;&#35745;&#30340;&#21644;&#38543;&#24847;&#30340;&#12290;&#22312;&#19981;&#21516;&#30340;&#19990;&#30028;&#20013;&#65292;&#25945;&#32946;&#23398;&#30740;&#31350;&#38271;&#26399;&#20197;&#26469;&#24314;&#31435;&#20102;&#20960;&#31181;&#26377;&#25928;&#30340;&#21453;&#39304;&#27169;&#22411;&#12290;&#22312;&#36825;&#31687;&#35266;&#28857;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#27719;&#32534;&#20102;&#26469;&#33258;&#25945;&#32946;&#23398;&#30340;&#24605;&#24819;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;FELT&#30340;LLMs&#21453;&#39304;&#26694;&#26550;&#65292;&#27010;&#36848;&#20102;&#21453;&#39304;&#31354;&#38388;&#30340;&#21508;&#31181;&#29305;&#24449;&#20197;&#21450;&#22522;&#20110;&#36825;&#20123;&#21464;&#37327;&#30340;&#21453;&#39304;&#20869;&#23481;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#19981;&#20165;&#25552;&#20379;&#20102;&#23545;&#21453;&#39304;&#31354;&#38388;&#30340;&#19968;&#33324;&#26144;&#23556;&#65292;&#36824;&#25552;&#20379;&#20102;&#25945;&#32946;&#23398;&#30830;&#23450;&#30340;&#31163;&#25955;&#31867;&#21035;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;&#19981;&#21516;&#21453;&#39304;&#31867;&#22411;&#23545;&#20462;&#35746;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;&#38500;&#20102;&#31616;&#21270;&#29616;&#26377;&#30340;NLF&#35774;&#35745;&#65292;FELT&#36824;&#20026;NLF&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#26410;&#24320;&#21457;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#25552;&#20379;&#32473;&#31038;&#21306;&#65292;&#20026;&#26144;&#23556;&#25105;&#20204;&#30340;&#31867;&#21035;&#25552;&#20379;&#25351;&#21335;&#21644;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Feedback (NLF) is an increasingly popular avenue to align Large Language Models (LLMs) to human preferences. Despite the richness and diversity of the information it can convey, NLF is often hand-designed and arbitrary. In a different world, research in pedagogy has long established several effective feedback models. In this opinion piece, we compile ideas from pedagogy to introduce FELT, a feedback framework for LLMs that outlines the various characteristics of the feedback space, and a feedback content taxonomy based on these variables. Our taxonomy offers both a general mapping of the feedback space, as well as pedagogy-established discrete categories, allowing us to empirically demonstrate the impact of different feedback types on revised generations. In addition to streamlining existing NLF designs, FELT also brings out new, unexplored directions for research in NLF. We make our taxonomy available to the community, providing guides and examples for mapping our cat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;TaxBox&#65292;&#35813;&#26041;&#27861;&#23558;&#20998;&#31867;&#27861;&#27010;&#24565;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#26469;&#36827;&#34892;&#27010;&#24565;&#38468;&#21152;&#21644;&#25554;&#20837;&#65292;&#36991;&#20813;&#20351;&#29992;&#20266;&#21494;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20108;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;TaxBox&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11004</link><description>&lt;p&gt;
&#36890;&#36807;&#26694;&#23884;&#20837;&#21644;&#27010;&#29575;&#35780;&#20998;&#22120;&#23436;&#25104;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taxonomy Completion with Probabilistic Scorer via Box Embedding. (arXiv:2305.11004v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;TaxBox&#65292;&#35813;&#26041;&#27861;&#23558;&#20998;&#31867;&#27861;&#27010;&#24565;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#26469;&#36827;&#34892;&#27010;&#24565;&#38468;&#21152;&#21644;&#25554;&#20837;&#65292;&#36991;&#20813;&#20351;&#29992;&#20266;&#21494;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20108;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;TaxBox&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20998;&#31867;&#27861;&#30340;&#23436;&#21892;&#20219;&#21153;--&#33258;&#21160;&#21033;&#29992;&#26032;&#30340;&#27010;&#24565;&#20016;&#23500;&#29616;&#26377;&#20998;&#31867;&#27861;--&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#22797;&#26434;&#27169;&#22359;&#12289;&#22806;&#37096;&#20449;&#24687;&#21644;&#20266;&#21494;&#26469;&#20016;&#23500;&#34920;&#31034;&#24182;&#32479;&#19968;&#38468;&#21152;&#21644;&#25554;&#20837;&#30340;&#21305;&#37197;&#36807;&#31243;&#12290;&#34429;&#28982;&#23427;&#20204;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#20171;&#32461;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#21644;&#35780;&#20998;&#36807;&#31243;&#20013;&#24102;&#26469;&#22122;&#38899;&#21644;&#19981;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TaxBox&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#23436;&#25104;&#20998;&#31867;&#27861;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#20998;&#31867;&#27861;&#27010;&#24565;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#26469;&#36827;&#34892;&#27010;&#24565;&#38468;&#21152;&#21644;&#25554;&#20837;&#65292;&#36991;&#20813;&#20351;&#29992;&#20266;&#21494;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TaxBox&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;1&#65289;&#22270;&#32858;&#21512;&#27169;&#22359;&#65292;&#20197;&#21033;&#29992;&#20998;&#31867;&#27861;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#20004;&#20010;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#65292;&#23558;&#29305;&#24449;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#65292;&#24182;&#25429;&#25417;&#27010;&#24565;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65307;&#65288;2&#65289;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#65292;&#20998;&#21035;&#23545;&#24212;&#38468;&#21152;&#21644;&#25554;&#20837;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#26469;&#20943;&#36731;&#35823;&#23548;&#20449;&#24687;&#30340;&#24433;&#21709;&#65307;&#65288;3&#65289;&#19968;&#31181;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#21644;&#20248;&#21270;&#20004;&#20010;&#35780;&#20998;&#22120;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TaxBox&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-the-art&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taxonomy completion, a task aimed at automatically enriching an existing taxonomy with new concepts, has gained significant interest in recent years. Previous works have introduced complex modules, external information, and pseudo-leaves to enrich the representation and unify the matching process of attachment and insertion. While they have achieved good performance, these introductions may have brought noise and unfairness during training and scoring. In this paper, we present TaxBox, a novel framework for taxonomy completion that maps taxonomy concepts to box embeddings and employs two probabilistic scorers for concept attachment and insertion, avoiding the need for pseudo-leaves. Specifically, TaxBox consists of three components: (1) a graph aggregation module to leverage the structural information of the taxonomy and two lightweight decoders that map features to box embedding and capture complex relationships between concepts; (2) two probabilistic scorers that correspond to attach
&lt;/p&gt;</description></item><item><title>ChatLog&#26159;&#19968;&#20010;&#20998;&#26512;ChatGPT&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#21462;ChatGPT&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#24449;&#21457;&#29616;&#20102;&#19968;&#20123;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;RoBERTa-based detector&#22312;&#26032;&#29256;&#26412;ChatGPT&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14106</link><description>&lt;p&gt;
ChatLog: &#35760;&#24405;&#21644;&#20998;&#26512;ChatGPT&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
ChatLog: Recording and Analyzing ChatGPT Across Time. (arXiv:2304.14106v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14106
&lt;/p&gt;
&lt;p&gt;
ChatLog&#26159;&#19968;&#20010;&#20998;&#26512;ChatGPT&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#21462;ChatGPT&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#24449;&#21457;&#29616;&#20102;&#19968;&#20123;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;RoBERTa-based detector&#22312;&#26032;&#29256;&#26412;ChatGPT&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#35780;&#20272;ChatGPT&#30340;&#30740;&#31350;&#65292;&#20294;&#40092;&#26377;&#30740;&#31350;&#35843;&#26597;ChatGPT&#30340;&#34892;&#20026;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#31895;&#21040;&#32454;&#30340;&#26102;&#38388;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;ChatLog&#65292;&#30001;&#20004;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#27599;&#26376;&#21644;&#27599;&#22825;&#26356;&#26032;&#65306;ChatLog-Monthly&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#27599;&#20010;&#26376;&#25910;&#38598;&#30340;38,730&#20010;&#38382;&#39064;-&#22238;&#31572;&#23545;&#65292;&#20854;&#20013;&#21253;&#25324;&#25512;&#29702;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;ChatLog-Daily&#21253;&#25324;ChatGPT&#27599;&#22825;&#23545;1000&#20010;&#30456;&#21516;&#38382;&#39064;&#30340;&#38271;&#31687;&#22238;&#31572;&#12290;&#25105;&#20204;&#36827;&#34892;&#20840;&#38754;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#25552;&#20379;ChatGPT&#36827;&#21270;&#27169;&#24335;&#23384;&#22312;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#21462;&#20854;&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#24449;&#20998;&#26512;&#20102;ChatGPT&#38543;&#26102;&#38388;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;RoBERTa&#30340;&#26816;&#27979;&#22120;&#22312;&#26032;&#29256;&#26412;&#30340;ChatGPT&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#32487;&#32493;&#32500;&#25252;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there are abundant researches about evaluating ChatGPT on natural language understanding and generation tasks, few studies have investigated how ChatGPT's behavior changes over time. In this paper, we collect a coarse-to-fine temporal dataset called ChatLog, consisting of two parts that update monthly and daily: ChatLog-Monthly is a dataset of 38,730 question-answer pairs collected every month including questions from both the reasoning and classification tasks. ChatLog-Daily, on the other hand, consists of ChatGPT's responses to 1000 identical questions for long-form generation every day. We conduct comprehensive automatic and human evaluation to provide the evidence for the existence of ChatGPT evolving patterns. We further analyze the unchanged characteristics of ChatGPT over time by extracting its knowledge and linguistic features. We find some stable features to improve the robustness of a RoBERTa-based detector on new versions of ChatGPT. We will continuously maintain our p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#21463;&#21040;&#25968;&#25454;&#38598;&#19981;&#21516;&#30340;&#25805;&#20316;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09092</link><description>&lt;p&gt;
&#25506;&#31350;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#25512;&#24191;&#22833;&#36133;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Investigating Failures to Generalize for Coreference Resolution Models. (arXiv:2303.09092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#21463;&#21040;&#25968;&#25454;&#38598;&#19981;&#21516;&#30340;&#25805;&#20316;&#21270;&#26041;&#24335;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#36890;&#24120;&#20250;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#22312;&#22914;&#20309;&#23454;&#29616;&#25351;&#20195;&#28040;&#35299;&#26041;&#38754;&#65288;&#21363;&#29702;&#35770;&#27010;&#24565;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#25805;&#20316;&#21270;&#26041;&#24335;&#65289;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#26159;&#30001;&#20110;&#36873;&#25321;&#35821;&#26009;&#24211;&#21644;&#27880;&#37322;&#25351;&#21335;&#31561;&#22240;&#32032;&#25152;&#33268;&#12290;&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;&#24403;&#21069;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#38169;&#35823;&#31243;&#24230;&#19982;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#23454;&#29616;&#24046;&#24322;&#20043;&#38388;&#30340;&#20851;&#32852;&#31243;&#24230;&#65288;OntoNotes&#12289;PreCo&#21644;Winogrande&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#24615;&#33021;&#20998;&#20026;&#22810;&#20010;&#31867;&#21035;&#65292;&#23545;&#24212;&#20110;&#22810;&#31181;&#25351;&#20195;&#65292;&#21253;&#25324;&#19968;&#33324;&#24615;&#25552;&#21450;&#12289;&#22797;&#21512;&#20462;&#39280;&#31526;&#21644;&#36830;&#31995;&#35859;&#35789;&#31561;&#12290;&#36825;&#31181;&#20998;&#31867;&#26377;&#21161;&#20110;&#25105;&#20204;&#35843;&#26597;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#36328;&#36234;&#19981;&#21516;&#25351;&#20195;&#31867;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21487;&#33021;&#20250;&#20986;&#29616;&#21738;&#20123;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#22312;OntoNotes&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;PreCo&#20013;&#19968;&#33324;&#24615;&#25552;&#21450;&#21644;&#36830;&#31995;&#35859;&#35789;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#21644;&#25351;&#20195;&#28040;&#35299;&#25805;&#20316;&#21270;&#26041;&#38754;&#35780;&#20272;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coreference resolution models are often evaluated on multiple datasets. Datasets vary, however, in how coreference is realized -- i.e., how the theoretical concept of coreference is operationalized in the dataset -- due to factors such as the choice of corpora and annotation guidelines. We investigate the extent to which errors of current coreference resolution models are associated with existing differences in operationalization across datasets (OntoNotes, PreCo, and Winogrande). Specifically, we distinguish between and break down model performance into categories corresponding to several types of coreference, including coreferring generic mentions, compound modifiers, and copula predicates, among others. This break down helps us investigate how state-of-the-art models might vary in their ability to generalize across different coreference types. In our experiments, for example, models trained on OntoNotes perform poorly on generic mentions and copula predicates in PreCo. Our findings 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;ICL&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26032;&#33539;&#24335;&#65292;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#39640;&#32423;&#25216;&#26415;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2301.00234</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on In-context Learning. (arXiv:2301.00234v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;ICL&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26032;&#33539;&#24335;&#65292;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#39640;&#32423;&#25216;&#26415;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#22312;&#20854;&#20013;LLM&#20165;&#22522;&#20110;&#21152;&#20837;&#23569;&#37327;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#39044;&#27979;&#12290;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;LLM&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;&#21644;&#24635;&#32467;ICL&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#28548;&#28165;&#20854;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32452;&#32455;&#21644;&#35752;&#35770;&#39640;&#32423;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#31574;&#30053;&#12289;&#28436;&#31034;&#35774;&#35745;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#40723;&#21169;&#26356;&#22810;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;ICL&#30340;&#24037;&#20316;&#21407;&#29702;&#24182;&#25913;&#36827;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.
&lt;/p&gt;</description></item></channel></rss>