<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#23581;&#35797;&#24635;&#32467;&#21644;&#35780;&#20272;&#30001;&#35813;&#39046;&#22495;&#36804;&#20170;&#36827;&#23637;&#32780;&#24418;&#25104;&#30340;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.10144</link><description>&lt;p&gt;
NLP&#39564;&#35777;&#65306;&#36208;&#21521;&#19968;&#31181;&#36890;&#29992;&#30340;&#29992;&#20110;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
NLP Verification: Towards a General Methodology for Certifying Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#24635;&#32467;&#21644;&#35780;&#20272;&#30001;&#35813;&#39046;&#22495;&#36804;&#20170;&#36827;&#23637;&#32780;&#24418;&#25104;&#30340;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65306;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24773;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#23545;&#21464;&#21270;&#25110;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#33021;&#23545;&#20854;&#36755;&#20986;&#32473;&#20986;&#20445;&#35777;&#12290;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#19981;&#21516;&#65292;NLP&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26041;&#27861;&#35770;&#65292;&#23613;&#31649;&#36817;&#24180;&#26469;&#25991;&#29486;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;NLP&#39564;&#35777;&#30340;&#23454;&#29992;&#38382;&#39064;&#24120;&#24120;&#28041;&#21450;&#19981;&#28145;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#25552;&#28860;&#21644;&#35780;&#20272;&#19968;&#20010;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#35813;&#27969;&#31243;&#26469;&#28304;&#20110;&#36804;&#20170;&#20026;&#27490;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#35821;&#20041;&#27867;&#21270;&#25216;&#26415;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10144v1 Announce Type: cross  Abstract: Deep neural networks have exhibited substantial success in the field of Natural Language Processing (NLP) and ensuring their safety and reliability is crucial: there are safety critical contexts where such models must be robust to variability or attack, and give guarantees over their output. Unlike Computer Vision, NLP lacks a unified verification methodology and, despite recent advancements in literature, they are often light on the pragmatical issues of NLP verification. In this paper, we make an attempt to distil and evaluate general components of an NLP verification pipeline, that emerges from the progress in the field to date. Our contributions are two-fold. Firstly, we give a general characterisation of verifiable subspaces that result from embedding sentences into continuous spaces. We identify, and give an effective method to deal with, the technical challenge of semantic generalisability of verified subspaces; and propose it a
&lt;/p&gt;</description></item><item><title>MedFLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#20998;&#26512;&#30340;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;SVD&#25439;&#22833;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#65292;&#39564;&#35777;&#20102;&#29992;&#35821;&#35328;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04626</link><description>&lt;p&gt;
MedFLIP&#65306;&#21307;&#23398;&#35270;&#35273;&#19982;&#35821;&#35328;&#33258;&#30417;&#30563;&#24555;&#36895;&#39044;&#35757;&#32451;&#19982;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04626
&lt;/p&gt;
&lt;p&gt;
MedFLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#20998;&#26512;&#30340;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;SVD&#25439;&#22833;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#65292;&#39564;&#35777;&#20102;&#29992;&#35821;&#35328;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#20998;&#26512;&#39046;&#22495;&#65292;&#24191;&#27867;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAEs&#65289;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#20114;&#30456;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;MAEs&#23545;&#36328;&#27169;&#24577;&#23398;&#20064;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MedFLIP&#65292;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#20998;&#26512;&#30340;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;MAEs&#36827;&#34892;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#22312;&#21307;&#23398;&#35786;&#26029;&#20013;&#24120;&#35265;&#30340;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#23545;&#22270;&#20687;&#36827;&#34892;&#25513;&#34109;&#19981;&#20250;&#24433;&#21709;&#36328;&#27169;&#24577;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD&#25439;&#22833;&#20197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#36825;&#31867;&#25968;&#25454;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#35821;&#35328;&#23558;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;MedFLIP&#23545;&#25513;&#34109;&#36807;&#31243;&#30340;&#25193;&#23637;&#26631;&#24535;&#30528;&#35813;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04626v1 Announce Type: cross  Abstract: Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and multimodal data. However, the impact of MAEs on intermodality remains a key challenge. We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis. We explore MAEs for zero-shot learning with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics. We verify that masking an image does not affect intermodal learning. Furthermore, we propose the SVD loss to enhance the representation learning for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data. Lastly, we validate using language will improve the zero-shot performance for the medical image analysis. MedFLIP scaling of the masking process marks an advancement in the field, offering 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;ConAgents&#26694;&#26550;&#65292;&#36890;&#36807;&#21512;&#20316;&#21644;&#20114;&#21160;&#20195;&#29702;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20351;&#29992;&#24037;&#20855;&#65292;&#24182;&#24341;&#20837;&#20102;&#36845;&#20195;&#26657;&#20934;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;&#20195;&#29702;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#33021;&#21147;&#26377;&#38480;&#21644;&#33258;&#36866;&#24212;&#32416;&#27491;&#38169;&#35823;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03031</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#21644;&#20114;&#21160;&#20195;&#29702;&#23398;&#20064;&#20351;&#29992;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Learning to Use Tools via Cooperative and Interactive Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03031
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;ConAgents&#26694;&#26550;&#65292;&#36890;&#36807;&#21512;&#20316;&#21644;&#20114;&#21160;&#20195;&#29702;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20351;&#29992;&#24037;&#20855;&#65292;&#24182;&#24341;&#20837;&#20102;&#36845;&#20195;&#26657;&#20934;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;&#20195;&#29702;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#33021;&#21147;&#26377;&#38480;&#21644;&#33258;&#36866;&#24212;&#32416;&#27491;&#38169;&#35823;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#23398;&#20064;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#20154;&#33021;&#22815;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#25193;&#23637;&#20854;&#21151;&#33021;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#21333;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24490;&#29615;&#36873;&#25321;&#21644;&#25191;&#34892;&#24037;&#20855;&#65292;&#28982;&#21518;&#23558;&#32467;&#26524;&#21512;&#24182;&#21040;&#19979;&#19968;&#20010;&#21160;&#20316;&#39044;&#27979;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#26102;&#20173;&#28982;&#23384;&#22312;&#28508;&#22312;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#65306;&#65288;1&#65289;&#21333;&#20010;LLM&#30340;&#22266;&#26377;&#33021;&#21147;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#21463;&#38480;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22312;&#20219;&#21153;&#22833;&#36133;&#26102;&#38590;&#20197;&#33258;&#36866;&#24212;&#22320;&#32416;&#27491;&#38169;&#35823;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConAgents&#65292;&#21363;&#21512;&#20316;&#21644;&#20114;&#21160;&#20195;&#29702;&#26694;&#26550;&#65292;&#23558;&#24037;&#20855;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#27169;&#22359;&#21270;&#20026;Grounding&#65288;&#22522;&#30784;&#65289;&#12289;Execution&#65288;&#25191;&#34892;&#65289;&#21644;Observing&#65288;&#35266;&#23519;&#65289;&#20195;&#29702;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#36845;&#20195;&#26657;&#20934;&#65288;IterCali&#65289;&#26041;&#27861;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#26469;&#33258;&#24037;&#20855;&#29615;&#22659;&#30340;&#21453;&#39304;&#23545;&#33258;&#24049;&#36827;&#34892;&#35843;&#25972;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36229;&#36807;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03031v1 Announce Type: new  Abstract: Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the super
&lt;/p&gt;</description></item><item><title>&#23545; NLP &#27169;&#22411;&#20013;&#24120;&#29992;&#30340;&#20998;&#35789;&#22120;&#36827;&#34892;&#20102;&#25511;&#21046;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#36138;&#23146;&#25512;&#29702;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#19978;&#19979;&#25991;&#24863;&#30693;&#20998;&#35789;&#22120; SaGe &#22312;&#24418;&#24577;&#23545;&#40784;&#26041;&#38754;&#34920;&#29616;&#26368;&#20248;&#12290;</title><link>https://arxiv.org/abs/2403.01289</link><description>&lt;p&gt;
&#36138;&#23146;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;&#65306;&#23545;&#20998;&#35789;&#25512;&#29702;&#26041;&#27861;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Greed is All You Need: An Evaluation of Tokenizer Inference Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01289
&lt;/p&gt;
&lt;p&gt;
&#23545; NLP &#27169;&#22411;&#20013;&#24120;&#29992;&#30340;&#20998;&#35789;&#22120;&#36827;&#34892;&#20102;&#25511;&#21046;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#36138;&#23146;&#25512;&#29702;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#19978;&#19979;&#25991;&#24863;&#30693;&#20998;&#35789;&#22120; SaGe &#22312;&#24418;&#24577;&#23545;&#40784;&#26041;&#38754;&#34920;&#29616;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982; BPE &#21644; WordPiece &#36825;&#26679;&#30340;&#23376;&#35789;&#20998;&#35789;&#22120;&#36890;&#24120;&#29992;&#20110;&#26500;&#24314; NLP &#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#65292;&#20294;&#26159;&#23558;&#25991;&#26412;&#35299;&#30721;&#20026;&#36825;&#20123;&#35789;&#27719;&#34920;&#20013;&#30340;&#19968;&#31995;&#21015;&#26631;&#35760;&#30340;&#26041;&#27861;&#36890;&#24120;&#26410;&#25351;&#23450;&#65292;&#25110;&#32773;&#19981;&#36866;&#21512;&#23427;&#20204;&#26500;&#24314;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#22235;&#31181;&#19981;&#21516;&#31639;&#27861;&#21644;&#19977;&#31181;&#35789;&#27719;&#37327;&#22823;&#23567;&#20043;&#38388;&#30340;&#19971;&#31181;&#20998;&#35789;&#22120;&#25512;&#29702;&#26041;&#27861;&#36827;&#34892;&#20102;&#25511;&#21046;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#22312;&#33521;&#35821;&#19978;&#20026;&#27492;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20869;&#22312;&#35780;&#20272;&#22871;&#20214;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#24418;&#24577;&#23398;&#12289;&#35748;&#30693;&#21644;&#20449;&#24687;&#29702;&#35770;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#26368;&#24120;&#29992;&#30340;&#20998;&#35789;&#22120;&#65292;&#36138;&#23146;&#25512;&#29702;&#34920;&#29616;&#20986;&#20154;&#24847;&#26009;&#22320;&#33391;&#22909;&#65307;&#26368;&#36817;&#24341;&#20837;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#20998;&#35789;&#22120; SaGe &#22312;&#24418;&#24577;&#23545;&#40784;&#26041;&#38754;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01289v1 Announce Type: new  Abstract: While subword tokenizers such as BPE and WordPiece are typically used to build vocabularies for NLP models, the method of decoding text into a sequence of tokens from these vocabularies is often left unspecified, or ill-suited to the method in which they were constructed. We provide a controlled analysis of seven tokenizer inference methods across four different algorithms and three vocabulary sizes, performed on a novel intrinsic evaluation suite we curated for English, combining measures rooted in morphology, cognition, and information theory. We show that for the most commonly used tokenizers, greedy inference performs surprisingly well; and that SaGe, a recently-introduced contextually-informed tokenizer, outperforms all others on morphological alignment.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#21487;&#33021;&#20250;&#36896;&#25104;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#24187;&#35273;&#65292;&#30740;&#31350;&#34920;&#26126;&#36890;&#36807;&#27979;&#35797;&#26102;&#38388;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.17509</link><description>&lt;p&gt;
&#26497;&#31471;&#22833;&#35843;&#19982;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Extreme Miscalibration and the Illusion of Adversarial Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17509
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#21487;&#33021;&#20250;&#36896;&#25104;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#24187;&#35273;&#65292;&#30740;&#31350;&#34920;&#26126;&#36890;&#36807;&#27979;&#35797;&#26102;&#38388;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#35823;&#20998;&#31867;&#12290;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#32463;&#24120;&#34987;&#29992;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#26377;&#24847;&#25110;&#26080;&#24847;&#22320;&#22833;&#35843;&#27169;&#22411;&#20250;&#25513;&#30422;&#26799;&#24230;&#65292;&#20174;&#32780;&#24178;&#25200;&#23545;&#25239;&#25915;&#20987;&#25628;&#32034;&#26041;&#27861;&#65292;&#23548;&#33268;&#34920;&#38754;&#19978;&#30475;&#20284;&#22686;&#21152;&#20102;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#35266;&#23519;&#21040;&#30340;&#40065;&#26834;&#24615;&#22686;&#30410;&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#24187;&#35273;&#65288;IOR&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#25163;&#22914;&#20309;&#25191;&#34892;&#21508;&#31181;&#24418;&#24335;&#30340;&#27979;&#35797;&#26102;&#38388;&#28201;&#24230;&#26657;&#20934;&#26469;&#25269;&#28040;&#19978;&#36848;&#24178;&#25200;&#65292;&#20351;&#23545;&#25239;&#25915;&#20987;&#33021;&#22815;&#25214;&#21040;&#23545;&#25239;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25958;&#20419;NLP&#31038;&#21306;&#22312;&#20854;&#40065;&#26834;&#24615;&#35780;&#20272;&#20013;&#32435;&#20837;&#27979;&#35797;&#26102;&#38388;&#28201;&#24230;&#32553;&#25918;&#65292;&#20197;&#30830;&#20445;&#35266;&#23519;&#21040;&#30340;&#20219;&#20309;&#22686;&#30410;&#37117;&#26159;&#30495;&#23454;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17509v1 Announce Type: new  Abstract: Deep learning-based Natural Language Processing (NLP) models are vulnerable to adversarial attacks, where small perturbations can cause a model to misclassify. Adversarial Training (AT) is often used to increase model robustness. However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with adversarial attack search methods, giving rise to an apparent increase in robustness. We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the adversarial attack to find adversarial examples. Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine. Finally, we show how the temperature can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23481;&#38169;&#24615;&#37327;&#23376;&#35745;&#31639;&#30340;&#35270;&#35282;&#19979;Transformer&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#26500;&#24314;Transformer&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.16714</link><description>&lt;p&gt;
&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#26159;Transformer&#26550;&#26500;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Quantum linear algebra is all you need for Transformer architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23481;&#38169;&#24615;&#37327;&#23376;&#35745;&#31639;&#30340;&#35270;&#35282;&#19979;Transformer&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#26500;&#24314;Transformer&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#21019;&#20316;&#12290;&#26412;&#25991;&#36890;&#36807;&#23481;&#38169;&#24615;&#37327;&#23376;&#35745;&#31639;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;Transformer&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20934;&#22791;self-attention&#30697;&#38453;&#30340;&#22359;&#32534;&#30721;&#65292;&#24182;&#32467;&#21512;&#37327;&#23376;&#23376;&#31243;&#24207;&#26500;&#24314;&#20102;Transformer&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16714v1 Announce Type: cross  Abstract: Generative machine learning methods such as large-language models are revolutionizing the creation of text and images. While these models are powerful they also harness a large amount of computational resources. The transformer is a key component in large language models that aims to generate a suitable completion of a given partial sequence. In this work, we investigate transformer architectures under the lens of fault-tolerant quantum computing. The input model is one where pre-trained weight matrices are given as block encodings to construct the query, key, and value matrices for the transformer. As a first step, we show how to prepare a block encoding of the self-attention matrix, with a row-wise application of the softmax function using the Hadamard product. In addition, we combine quantum subroutines to construct important building blocks in the transformer, the residual connection, layer normalization, and the feed-forward neura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15938</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#25110;&#35760;&#24518;&#65306;&#25968;&#25454;&#27745;&#26579;&#19982;&#21487;&#20449;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#30340;&#35828;&#27861;&#36890;&#24120;&#26159;&#36890;&#36807;&#22312;&#24320;&#25918;&#33719;&#21462;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#26469;&#25903;&#25345;&#30340;&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24222;&#22823;&#35268;&#27169;&#21644;&#24191;&#27867;&#26469;&#28304;&#65292;&#23427;&#21487;&#33021;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#21253;&#21547;&#27979;&#35797;&#25968;&#25454;&#65292;&#23548;&#33268;LLMs&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#20197;&#21450;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#23545;&#20110;LLMs&#26469;&#35828;&#26816;&#27979;&#21644;&#20943;&#36731;&#25968;&#25454;&#27745;&#26579;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CDD&#65292;&#21363;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;CDD&#12290;CDD&#20165;&#38656;&#35201;&#37319;&#26679;&#25991;&#26412;&#26469;&#26816;&#27979;&#25968;&#25454;&#27745;&#26579;&#65292;&#36890;&#36807;&#35782;&#21035;LLMs&#36755;&#20986;&#20998;&#24067;&#30340;&#23792;&#20540;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#35780;&#20272;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;TED&#65306;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
&lt;/p&gt;</description></item><item><title>Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12146</link><description>&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12146
&lt;/p&gt;
&lt;p&gt;
Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#20219;&#21153;&#20013;&#23637;&#29616;&#24378;&#22823;&#24615;&#33021;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#31561;&#21487;&#38752;&#24615;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;GPT-4&#36825;&#26679;&#39640;&#33021;&#21147;&#30340;LLMs&#22312;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#36890;&#24120;&#34987;&#35843;&#25972;&#26469;&#35780;&#20272;&#23545;&#30456;&#21516;&#26597;&#35810;&#30340;&#21709;&#24212;&#30340;&#30456;&#23545;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textit{Meta}$ $\textit{Ranking}$&#65288;MR&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#30452;&#25509;&#35780;&#20272;&#21709;&#24212;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#36827;&#34892;&#27604;&#36739;&#26469;&#23454;&#29616;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#21709;&#24212;&#30340;&#38169;&#35823;&#26816;&#27979;&#20013;&#65292;MR&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#20063;&#33021;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;MR&#21487;&#20197;&#34987;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
&lt;/p&gt;</description></item><item><title>II-MMR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25913;&#36827;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22810;&#27169;&#24577;&#22810;&#36339;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#31572;&#26696;&#39044;&#27979;&#24341;&#23548;&#30340;CoT&#25552;&#31034;&#21644;&#30693;&#35782;&#19977;&#20803;&#32452;&#24341;&#23548;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#23545;&#19981;&#21516;&#25512;&#29702;&#24773;&#20917;&#30340;&#20998;&#26512;&#21644;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.11058</link><description>&lt;p&gt;
II-MMR&#65306;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#35782;&#21035;&#21644;&#25913;&#36827;&#22810;&#27169;&#24577;&#22810;&#36339;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11058
&lt;/p&gt;
&lt;p&gt;
II-MMR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25913;&#36827;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22810;&#27169;&#24577;&#22810;&#36339;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#31572;&#26696;&#39044;&#27979;&#24341;&#23548;&#30340;CoT&#25552;&#31034;&#21644;&#30693;&#35782;&#19977;&#20803;&#32452;&#24341;&#23548;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#23545;&#19981;&#21516;&#25512;&#29702;&#24773;&#20917;&#30340;&#20998;&#26512;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36890;&#24120;&#28041;&#21450;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#22810;&#26679;&#25512;&#29702;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;VQA&#30740;&#31350;&#20165;&#20851;&#27880;&#35780;&#20272;&#27169;&#22411;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#65292;&#32780;&#27809;&#26377;&#22312;&#19981;&#21516;&#25512;&#29702;&#24773;&#20917;&#19979;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;"CoT"&#25552;&#31034;&#26080;&#27861;&#26377;&#25928;&#29983;&#25104;VQA&#30340;&#25512;&#29702;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#30340;&#22797;&#26434;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;II-MMR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25913;&#36827;VQA&#20013;&#30340;&#22810;&#27169;&#24577;&#22810;&#36339;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;II-MMR&#25509;&#21463;&#24102;&#26377;&#22270;&#20687;&#30340;VQA&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#25552;&#31034;&#25214;&#21040;&#25512;&#29702;&#36335;&#24452;&#20197;&#33719;&#24471;&#31572;&#26696;&#65306;(i)&#31572;&#26696;&#39044;&#27979;&#24341;&#23548;&#30340;CoT&#25552;&#31034;&#65292;&#25110;&#32773;(ii)&#30693;&#35782;&#19977;&#20803;&#32452;&#24341;&#23548;&#30340;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;II-MMR&#20998;&#26512;&#36825;&#26465;&#36335;&#24452;&#65292;&#36890;&#36807;&#20272;&#35745;&#26377;&#22810;&#23569;&#36339;&#21644;&#20160;&#20040;&#31867;&#22411;&#65288;&#21363;&#35270;&#35273;&#25110;&#36229;&#20986;&#65289;&#26469;&#35782;&#21035;&#24403;&#21069;VQA&#22522;&#20934;&#20013;&#30340;&#19981;&#21516;&#25512;&#29702;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11058v1 Announce Type: cross  Abstract: Visual Question Answering (VQA) often involves diverse reasoning scenarios across Vision and Language (V&amp;L). Most prior VQA studies, however, have merely focused on assessing the model's overall accuracy without evaluating it on different reasoning cases. Furthermore, some recent works observe that conventional Chain-of-Thought (CoT) prompting fails to generate effective reasoning for VQA, especially for complex scenarios requiring multi-hop reasoning. In this paper, we propose II-MMR, a novel idea to identify and improve multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQA question with an image and finds a reasoning path to reach its answer using two novel language promptings: (i) answer prediction-guided CoT prompt, or (ii) knowledge triplet-guided prompt. II-MMR then analyzes this path to identify different reasoning cases in current VQA benchmarks by estimating how many hops and what types (i.e., visual or beyon
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32437;&#21521;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#25104;&#24335;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#21644;&#23450;&#21046;&#21270;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29087;&#24713;&#21270;&#38454;&#27573;&#21518;&#65292;&#29992;&#25143;&#24863;&#30693;&#21040;&#30340;&#31995;&#32479;&#25928;&#29992;&#25552;&#39640;&#20102;&#12290;</title><link>https://arxiv.org/abs/2402.09894</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#26159;&#26032;&#39062;&#24615;&#65306;&#20851;&#20110;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#25928;&#29992;&#21644;&#23450;&#21046;&#21270;&#30340;&#32437;&#21521;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09894
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32437;&#21521;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#25104;&#24335;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#21644;&#23450;&#21046;&#21270;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29087;&#24713;&#21270;&#38454;&#27573;&#21518;&#65292;&#29992;&#25143;&#24863;&#30693;&#21040;&#30340;&#31995;&#32479;&#25928;&#29992;&#25552;&#39640;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#20026;&#20154;&#20204;&#22312;&#26085;&#24120;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26032;&#39062;&#32780;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#26377;&#35768;&#22810;AI&#24037;&#20316;&#27969;&#31243;&#36890;&#36807;&#23558;AI&#36755;&#20986;&#19982;&#20154;&#31867;&#20114;&#21160;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#30495;&#23454;&#32780;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;AI&#20855;&#26377;&#26080;&#21487;&#21542;&#35748;&#30340;&#21560;&#24341;&#21147;&#65292;&#20294;&#22312;&#26032;&#40092;&#24863;&#28040;&#22833;&#21518;&#65292;&#29983;&#25104;&#24335;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#22914;&#20309;&#20173;&#28982;&#19981;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#26500;&#24314;&#30340;&#24037;&#20855;&#20855;&#26377;&#20010;&#24615;&#21270;&#21644;&#24555;&#36895;&#36866;&#24212;&#30340;&#28508;&#21147;&#65292;&#20294;&#29992;&#25143;&#26159;&#21542;&#20805;&#20998;&#21033;&#29992;&#20102;&#20010;&#24615;&#21270;&#30340;&#21487;&#33021;&#24615;&#21602;&#65311;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20026;&#26399;&#19977;&#21608;&#30340;&#32437;&#21521;&#30740;&#31350;&#65292;&#20849;&#26377;12&#20010;&#29992;&#25143;&#65292;&#26088;&#22312;&#20102;&#35299;&#31185;&#23398;&#20256;&#25773;&#20013;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#30340;&#29087;&#24713;&#24230;&#21644;&#23450;&#21046;&#21270;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#29087;&#24713;&#21270;&#38454;&#27573;&#25345;&#32493;&#20102;4.3&#20010;&#20250;&#35805;&#65292;&#29992;&#25143;&#22312;&#36825;&#20010;&#38454;&#27573;&#25506;&#32034;&#24037;&#20316;&#27969;&#31243;&#30340;&#21151;&#33021;&#20197;&#21450;&#20182;&#20204;&#21457;&#29616;&#21738;&#20123;&#26041;&#38754;&#26377;&#29992;&#12290;&#22312;&#29087;&#24713;&#21270;&#21518;&#65292;&#31995;&#32479;&#30340;&#24863;&#30693;&#25928;&#29992;&#35780;&#20998;&#39640;&#20110;&#20043;&#21069;&#65292;&#34920;&#26126;&#20102;&#24863;&#30693;&#25928;&#29992;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09894v1 Announce Type: cross  Abstract: Generative AI brings novel and impressive abilities to help people in everyday tasks. There are many AI workflows that solve real and complex problems by chaining AI outputs together with human interaction. Although there is an undeniable lure of AI, it's uncertain how useful generative AI workflows are after the novelty wears off. Additionally, tools built with generative AI have the potential to be personalized and adapted quickly and easily, but do users take advantage of the potential to customize? We conducted a three-week longitudinal study with 12 users to understand the familiarization and customization of generative AI tools for science communication. Our study revealed that the familiarization phase lasts for 4.3 sessions, where users explore the capabilities of the workflow and which aspects they find useful. After familiarization, the perceived utility of the system is rated higher than before, indicating that the perceived
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09723</link><description>&lt;p&gt;
&#26377;&#38480;&#39044;&#31639;&#19979;&#30340;&#36805;&#36895;&#23398;&#20064;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification for Prompt Learning under a Limited Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09723
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#24341;&#21457;&#20102;&#23545;&#33258;&#21160;&#23398;&#20064;&#21512;&#36866;&#25552;&#31034;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#25104;&#26412;&#65288;&#20363;&#22914;&#35775;&#38382;LLM&#21644;&#35780;&#20272;&#21709;&#24212;&#65289;&#23578;&#26410;&#24471;&#21040;&#32771;&#34385;&#12290;&#20026;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#24037;&#20316;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#26126;&#30830;&#24341;&#20837;&#20102;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#22312;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI-FB&#65289;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65288;&#29992;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65289;&#65292;&#20197;&#31995;&#32479;&#22320;&#21033;&#29992;BAI-FB&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#30340;&#21147;&#37327;&#12290;&#25552;&#31034;&#23398;&#20064;&#30340;&#29420;&#29305;&#29305;&#28857;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#25552;&#20986;&#20102;TRIPLE&#30340;&#20004;&#20010;&#22522;&#20110;&#23884;&#20837;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>SemRel2024&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#21512;&#21487;&#20197;&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21512;&#28085;&#30422;&#20102;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#31561;14&#31181;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2402.08638</link><description>&lt;p&gt;
SemRel2024: 14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08638
&lt;/p&gt;
&lt;p&gt;
SemRel2024&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;14&#31181;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65292;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#21512;&#21487;&#20197;&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21512;&#28085;&#30422;&#20102;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#31561;14&#31181;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#21644;&#37327;&#21270;&#35821;&#20041;&#30456;&#20851;&#24615;&#26159;&#35821;&#35328;&#34920;&#36798;&#30340;&#26680;&#24515;&#12290;&#23427;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#21253;&#25324;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#27934;&#23519;&#12290;&#34429;&#28982;&#26089;&#26399;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#19978;&#65292;&#24448;&#24448;&#26159;&#22312;&#33521;&#35821;&#35821;&#22659;&#20013;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#24191;&#27867;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#20540;&#24471;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SemRel&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#27597;&#35821;&#20026;14&#31181;&#35821;&#35328;&#36827;&#34892;&#27880;&#37322;&#30340;&#26032;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#25968;&#25454;&#38598;&#21512;&#65306;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#12290;&#36825;&#20123;&#35821;&#35328;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#30340;&#35821;&#31995;&#65292;&#20027;&#35201;&#22312;&#38750;&#27954;&#21644;&#20122;&#27954;&#20351;&#29992;&#65292;&#36825;&#20123;&#22320;&#21306;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30456;&#23545;&#36739;&#23569;&#12290;SemRel&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#37117;&#26159;&#19982;&#19968;&#20010;&#34920;&#31034;&#30456;&#20851;&#24615;&#24471;&#20998;&#30340;&#21477;&#23376;&#23545;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring and quantifying semantic relatedness is central to representing language. It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs). While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness. In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources. Each instance in the SemRel datasets is a sentence pair associated with a score that repr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;AI&#27169;&#22411;&#22823;&#23567;&#22686;&#38271;&#21644;&#21512;&#25104;&#25968;&#25454;&#24341;&#20837;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#36935;&#24635;&#20307;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.07043</link><description>&lt;p&gt;
&#23614;&#24052;&#30340;&#25925;&#20107;&#65306;&#20316;&#20026;&#23610;&#24230;&#24459;&#21464;&#21270;&#30340;&#27169;&#22411;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
A Tale of Tails: Model Collapse as a Change of Scaling Laws
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;AI&#27169;&#22411;&#22823;&#23567;&#22686;&#38271;&#21644;&#21512;&#25104;&#25968;&#25454;&#24341;&#20837;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#36935;&#24635;&#20307;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#38271;&#65292;&#31070;&#32463;&#23610;&#24230;&#24459;&#24050;&#25104;&#20026;&#39044;&#27979;&#22823;&#27169;&#22411;&#22312;&#25193;&#23481;&#21644;&#21407;&#22987;&#65288;&#20154;&#31867;&#25110;&#33258;&#28982;&#65289;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#22686;&#21152;&#26102;&#25913;&#21892;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#27969;&#34892;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#24847;&#21619;&#30528;&#22312;&#32447;&#25968;&#25454;&#21644;&#25991;&#26412;&#30340;&#29983;&#24577;&#31995;&#32479;&#23558;&#36880;&#28176;&#21253;&#21547;&#36234;&#26469;&#36234;&#22810;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38382;&#65306;&#24403;&#21512;&#25104;&#25968;&#25454;&#36827;&#20837;&#35757;&#32451;&#35821;&#26009;&#24211;&#26102;&#65292;&#23610;&#24230;&#24459;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#26410;&#26469;&#30340;&#27169;&#22411;&#20173;&#20250;&#25913;&#21892;&#65292;&#36824;&#26159;&#27880;&#23450;&#20250;&#23436;&#20840;&#23849;&#28291;&#65288;&#27169;&#22411;&#23849;&#28291;&#65289;&#65311;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#23849;&#28291;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#24191;&#27867;&#30340;&#34928;&#20943;&#29616;&#35937;&#65292;&#20998;&#26512;&#20102;&#23610;&#24230;&#30340;&#20007;&#22833;&#12289;&#19982;&#20195;&#25968;&#30340;&#21464;&#21270;&#23610;&#24230;&#12289;&#25216;&#33021;&#30340;"&#36951;&#24536;"&#20197;&#21450;&#28151;&#21512;&#20154;&#31867;&#21644;&#21512;&#25104;&#25968;&#25454;&#26102;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36890;&#36807;&#23545;&#19968;&#20010;&#31639;&#26415;&#20219;&#21153;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#21040;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#25512;&#36831;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27969;&#25968;&#25454;&#22788;&#29702;&#20013;&#21516;&#26102;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.04513</link><description>&lt;p&gt;
&#22312;&#27969;&#25968;&#25454;&#19978;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#30340;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Cascade Learning for Efficient Inference over Streams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04513
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#21040;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;&#25512;&#36831;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27969;&#25968;&#25454;&#22788;&#29702;&#20013;&#21516;&#26102;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#22238;&#31572;&#20851;&#20110;&#25968;&#25454;&#27969;&#30340;&#22797;&#26434;&#26597;&#35810;&#26041;&#38754;&#20855;&#26377;&#22825;&#28982;&#30340;&#20248;&#21183;&#65292;&#20294;&#26159; LLM &#25512;&#29702;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#32423;&#32852;&#23398;&#20064;&#65292;&#36825;&#26159;&#39318;&#20010;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#36825;&#37324;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#8220;&#32423;&#32852;&#8221;&#27169;&#22411;&#65292;&#20174;&#23481;&#37327;&#36739;&#20302;&#30340;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#22120;&#65289;&#24320;&#22987;&#65292;&#21040;&#24378;&#22823;&#30340; LLM &#32467;&#26463;&#65292;&#24182;&#37197;&#22791;&#19968;&#20010;&#20915;&#23450;&#22312;&#32473;&#23450;&#36755;&#20837;&#19978;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#30340;&#25512;&#36831;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#22312;&#32447;&#23398;&#20064;&#32423;&#32852;&#30340;&#20219;&#21153;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20026;&#35813;&#38382;&#39064;&#25552;&#20379;&#20102;&#26080;&#36951;&#25022;&#31639;&#27861;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982; LLM &#30456;&#24403;&#65292;&#21516;&#26102;&#23558;&#25512;&#29702;&#25104;&#26412;&#21066;&#20943;&#20102;&#22810;&#36798; 90%&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#27969;&#22788;&#29702;&#20013;&#30340;&#25928;&#33021;&#21644;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have a natural role in answering complex queries about data streams, but the high computational cost of LLM inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a "cascade" of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful LLM, along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels LLMs in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#20154;&#20204;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#20154;&#29305;&#36136;&#36807;&#24230;&#24402;&#22240;&#30340;&#29616;&#35937;&#65292;&#24182;&#21628;&#21505;&#23398;&#26415;&#30028;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20869;&#23481;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;</title><link>https://arxiv.org/abs/2402.03962</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#21453;&#23545;&#34394;&#20551;&#30340;AI&#33192;&#32960;&#24615;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03962
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#20154;&#20204;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#20154;&#29305;&#36136;&#36807;&#24230;&#24402;&#22240;&#30340;&#29616;&#35937;&#65292;&#24182;&#21628;&#21505;&#23398;&#26415;&#30028;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20869;&#23481;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#19968;&#31181;&#20542;&#21521;&#65292;&#30475;&#21040;&#21608;&#22260;&#30340;&#29289;&#20307;&#20855;&#26377;&#31867;&#20284;"&#20154;&#31867;"&#30340;&#29305;&#36136;&#12290;&#25105;&#20204;&#32473;&#27773;&#36710;&#21462;&#21517;&#23383;&#65292;&#21644;&#23456;&#29289;&#29978;&#33267;&#23478;&#29992;&#30005;&#22120;&#20132;&#35848;&#65292;&#20223;&#20315;&#23427;&#20204;&#33021;&#20687;&#20854;&#20182;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#25105;&#20204;&#12290;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#25311;&#20154;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20154;&#20204;&#22768;&#31216;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#33021;&#22815;&#23519;&#35273;&#21040;&#31867;&#20284;&#20110;&#20154;&#31867;&#26234;&#33021;&#30340;&#29305;&#36136;&#12290;&#22312;&#36825;&#31687;&#20301;&#32622;&#35770;&#25991;&#20013;&#65292;&#32771;&#34385;&#21040;&#19987;&#19994;&#28608;&#21169;&#12289;&#20154;&#31867;&#20559;&#35265;&#21644;&#19968;&#33324;&#26041;&#27861;&#35770;&#35774;&#32622;&#65292;&#25105;&#20204;&#35752;&#35770;&#22914;&#20309;&#24403;&#21069;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36861;&#27714;&#20026;&#23558;&#31867;&#20154;&#29305;&#36136;&#24402;&#22240;&#20110;LLM&#25171;&#24320;&#20102;&#28389;&#35294;&#20043;&#38376;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#21457;&#29616;&#21487;&#35299;&#37322;&#20026;&#20154;&#31867;&#30340;&#27169;&#24335;&#24182;&#19981;&#24212;&#35813;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;&#32771;&#34385;&#21040;&#23186;&#20307;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#36941;&#25551;&#20889;&#65292;&#25105;&#20204;&#21628;&#21505;&#23398;&#26415;&#30028;&#29305;&#21035;&#35880;&#24910;&#65292;&#24182;&#23545;&#23398;&#26415;&#35802;&#20449;&#21407;&#21017;&#26377;&#39069;&#22806;&#30340;&#24847;&#35782;&#65292;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20449;&#24687;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI rese
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03299</link><description>&lt;p&gt;
GUARD: &#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#21335;&#30340;&#21512;&#35268;&#24615;
&lt;/p&gt;
&lt;p&gt;
GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#21644;&#26377;&#23475;&#22238;&#24212;&#30340;"&#36234;&#29425;"&#24050;&#32463;&#40723;&#21169;&#31038;&#21306;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#23433;&#20840;&#25514;&#26045;&#26159;&#22312;&#21457;&#24067;&#20043;&#21069;&#29992;&#36234;&#29425;&#20027;&#21160;&#27979;&#35797;LLM&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#27979;&#35797;&#23558;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22823;&#35268;&#27169;&#19988;&#39640;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#36861;&#38543;&#19968;&#31181;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#31574;&#30053;&#19979;&#65292;&#20197;&#20154;&#31867;&#29983;&#25104;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#36234;&#29425;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65292;&#23558;&#22235;&#31181;&#19981;&#21516;&#35282;&#33394;&#20998;&#37197;&#32473;&#29992;&#25143;LLM&#65292;&#20197;&#20415;&#21327;&#20316;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#29616;&#26377;&#30340;&#36234;&#29425;&#65292;&#24182;&#36890;&#36807;&#21477;&#23376;&#36880;&#21477;&#36827;&#34892;&#32858;&#31867;&#39057;&#29575;&#21644;&#35821;&#20041;&#27169;&#24335;&#30340;&#21010;&#20998;&#65292;&#23558;&#23427;&#20204;&#20998;&#25104;&#19981;&#21516;&#30340;&#29420;&#31435;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#35282;&#33394;&#31995;&#32479;&#23558;&#21033;&#29992;&#36825;&#20010;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
&lt;/p&gt;</description></item><item><title>&#27979;&#35797;&#20102;&#22810;&#31181;&#31616;&#21333;&#30340;&#22810;LLM&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#21457;&#29616;&#20004;&#20010;LLM&#20195;&#29702;&#25353;&#39034;&#24207;&#20351;&#29992;&#30340;&#31616;&#21333;&#32467;&#26500;&#34920;&#29616;&#26368;&#20339;&#65292;&#19968;&#20010;&#32534;&#36753;&#36890;&#29992;&#25805;&#20316;&#27969;&#31243;&#65292;&#21478;&#19968;&#20010;&#39564;&#35777;&#21487;&#25191;&#34892;&#24615;&#65292;&#22312;&#23450;&#21046;&#25805;&#20316;&#27969;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#65292;&#26377;&#26395;&#36827;&#19968;&#27493;&#25506;&#32034;&#22810;&#20195;&#29702;&#32534;&#36753;&#20307;&#31995;&#32467;&#26500;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.09510</link><description>&lt;p&gt;
&#19968;&#20992;&#20999;&#24182;&#38750;&#38134;&#24377;&#65306;&#23450;&#21046;&#24320;&#25918;&#39046;&#22495;&#30340;&#25805;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
One Size Does Not Fit All: Customizing Open-Domain Procedures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09510
&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#20102;&#22810;&#31181;&#31616;&#21333;&#30340;&#22810;LLM&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#21457;&#29616;&#20004;&#20010;LLM&#20195;&#29702;&#25353;&#39034;&#24207;&#20351;&#29992;&#30340;&#31616;&#21333;&#32467;&#26500;&#34920;&#29616;&#26368;&#20339;&#65292;&#19968;&#20010;&#32534;&#36753;&#36890;&#29992;&#25805;&#20316;&#27969;&#31243;&#65292;&#21478;&#19968;&#20010;&#39564;&#35777;&#21487;&#25191;&#34892;&#24615;&#65292;&#22312;&#23450;&#21046;&#25805;&#20316;&#27969;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#65292;&#26377;&#26395;&#36827;&#19968;&#27493;&#25506;&#32034;&#22810;&#20195;&#29702;&#32534;&#36753;&#20307;&#31995;&#32467;&#26500;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#25805;&#20316;&#30340;&#27969;&#31243;&#65292;&#27604;&#22914;&#22914;&#20309;&#31181;&#26893;&#33457;&#22253;&#65292;&#29616;&#22312;&#34987;&#25968;&#30334;&#19975;&#29992;&#25143;&#20351;&#29992;&#65292;&#20294;&#26377;&#26102;&#38656;&#35201;&#33258;&#23450;&#20041;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#29305;&#23450;&#38656;&#27714;&#65292;&#20363;&#22914;&#19981;&#20351;&#29992;&#26432;&#34411;&#21058;&#31181;&#26893;&#33457;&#22253;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#34913;&#37327;&#21644;&#25913;&#36827;&#19968;&#20010;LLM&#25191;&#34892;&#27492;&#31867;&#23450;&#21046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#27979;&#35797;&#20960;&#31181;&#31616;&#21333;&#30340;&#22810;LLM&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#26032;&#30340;&#35780;&#20272;&#38598;CustomPlans&#23545;&#20854;&#36827;&#34892;&#23450;&#21046;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;200&#22810;&#20010;WikiHow&#25805;&#20316;&#27969;&#31243;&#32452;&#25104;&#65292;&#27599;&#20010;&#27969;&#31243;&#37117;&#26377;&#29305;&#23450;&#30340;&#23450;&#21046;&#38656;&#27714;&#12290;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#20855;&#26377;&#20004;&#20010;LLM&#20195;&#29702;&#30340;&#20307;&#31995;&#32467;&#26500;&#34920;&#29616;&#26368;&#20339;&#65292;&#23427;&#20204;&#25353;&#39034;&#24207;&#20351;&#29992;&#65292;&#19968;&#20010;&#32534;&#36753;&#36890;&#29992;&#25805;&#20316;&#27969;&#31243;&#65292;&#21478;&#19968;&#20010;&#39564;&#35777;&#20854;&#21487;&#25191;&#34892;&#24615;&#65292;&#26126;&#26174;&#20248;&#20110;&#31471;&#21040;&#31471;&#25552;&#31034;&#30340;LLM&#65288;10.5&#65285;&#32477;&#23545;&#65289;&#12290;&#36825;&#34920;&#26126;LLM&#21487;&#20197;&#34987;&#21512;&#29702;&#26377;&#25928;&#22320;&#37197;&#32622;&#20197;&#36827;&#34892;&#25805;&#20316;&#27969;&#31243;&#30340;&#23450;&#21046;&#12290;&#36825;&#20063;&#34920;&#26126;&#65292;&#22810;&#20195;&#29702;&#32534;&#36753;&#20307;&#31995;&#32467;&#26500;&#21487;&#33021;&#20540;&#24471;&#36827;&#19968;&#27493;&#25506;&#35752;&#65292;&#20197;&#29992;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09510v2 Announce Type: replace  Abstract: How-to procedures, such as how to plant a garden, are now used by millions of users, but sometimes need customizing to meet a user's specific needs, e.g., planting a garden without pesticides. Our goal is to measure and improve an LLM's ability to perform such customization. Our approach is to test several simple multi-LLM-agent architectures for customization, as well as an end-to-end LLM, using a new evaluation set, called CustomPlans, of over 200 WikiHow procedures each with a customization need. We find that a simple architecture with two LLM agents used sequentially performs best, one that edits a generic how-to procedure and one that verifies its executability, significantly outperforming (10.5% absolute) an end-to-end prompted LLM. This suggests that LLMs can be configured reasonably effectively for procedure customization. This also suggests that multi-agent editing architectures may be worth exploring further for other custo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;Meta-Continual Active Learning&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#35299;&#20915;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.03732</link><description>&lt;p&gt;
&#23398;&#20064;&#23398;&#20064;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#25345;&#20037;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn for Few-shot Continual Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;Meta-Continual Active Learning&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#35299;&#20915;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#30830;&#20445;&#35299;&#20915;&#20808;&#21069;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#23637;&#31034;&#23545;&#26032;&#39046;&#22495;&#30340;&#21487;&#22609;&#24615;&#12290;&#26368;&#36817;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#23616;&#38480;&#20110;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#65288;CAL&#65289;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#65292;&#20294;&#26410;&#26631;&#35760;&#25968;&#25454;&#20805;&#36275;&#65292;&#20294;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20803;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#26469;&#35299;&#20915;&#20219;&#21153;&#20043;&#38388;&#30340;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#23569;&#26679;&#26412;CAL&#35774;&#32622;&#20013;&#19981;&#21516;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#19978;&#65292;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#26377;&#38480;&#65292;&#20294;&#22312;&#23569;&#25968;-shot&#25552;&#31034;&#19979;&#26377;&#19968;&#23450;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.11052</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#25366;&#25496;&#23454;&#39564;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Mining experimental data from Materials Science literature with Large Language Models. (arXiv:2401.11052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#19978;&#65292;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#26377;&#38480;&#65292;&#20294;&#22312;&#23569;&#25968;-shot&#25552;&#31034;&#19979;&#26377;&#19968;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35780;&#20272;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5-Turbo&#12289;GPT-4&#21644;GPT-4-Turbo&#65292;&#22312;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#31185;&#23398;&#25991;&#26723;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#20998;&#26512;&#22797;&#26434;&#30340;&#26448;&#26009;&#34920;&#36798;&#24335;&#65292;&#24378;&#35843;&#21270;&#23398;&#24335;&#30340;&#26631;&#20934;&#21270;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#35780;&#20272;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20449;&#24687;&#25552;&#21462;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#65288;i&#65289;&#30740;&#31350;&#26448;&#26009;&#21644;&#29289;&#29702;&#24615;&#36136;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#65288;ii&#65289;&#36825;&#20123;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#12290;LLMs&#22312;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#19982;&#22522;&#20110;BERT&#26550;&#26500;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23545;&#20110;NER&#65292;LLMs&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#26080;&#27861;&#36229;&#36234;&#22522;&#20934;&#32447;&#65292;&#24182;&#19988;&#20165;&#22312;&#23569;&#25968;-shot&#25552;&#31034;&#19979;&#26377;&#23569;&#37327;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;...
&lt;/p&gt;
&lt;p&gt;
This study is dedicated to evaluating the capabilities of advanced large language models (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in the extraction of structured information from scientific documents within the field of materials science. We introduce a novel methodology for the comparative analysis of intricate material expressions, emphasising the standardisation of chemical formulas to tackle the complexities inherent in materials science information assessment. To this end, we primarily focus on two critical tasks of information extraction: (i) a named entity recognition (NER) of studied materials and physical properties and (ii) a relation extraction (RE) between these entities. The performance of LLMs in executing these tasks is benchmarked against traditional models based on the BERT architecture and rule-based approaches. For NER, LLMs fail to outperform the baseline with zero-shot prompting and exhibit only limited improvement with few-shot prompting. However, for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21482;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#23569;&#37327;&#35757;&#32451;&#27493;&#39588;&#33719;&#21462;&#39640;&#36136;&#37327;&#25991;&#26412;&#23884;&#20837;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#31454;&#20105;&#28608;&#28872;&#30340;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.00368</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Improving Text Embeddings with Large Language Models. (arXiv:2401.00368v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21482;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#23569;&#37327;&#35757;&#32451;&#27493;&#39588;&#33719;&#21462;&#39640;&#36136;&#37327;&#25991;&#26412;&#23884;&#20837;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#31454;&#20105;&#28608;&#28872;&#30340;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#23569;&#20110;1k&#20010;&#35757;&#32451;&#27493;&#39588;&#21363;&#21487;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#22810;&#38454;&#27573;&#20013;&#38388;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#25968;&#21313;&#20159;&#20010;&#24369;&#30417;&#30563;&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#20877;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#26500;&#24314;&#22797;&#26434;&#30340;&#35757;&#32451;&#27969;&#31243;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#36890;&#24120;&#21463;&#20219;&#21153;&#22810;&#26679;&#24615;&#21644;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#38480;&#21046;&#30340;&#25163;&#21160;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#19987;&#26377;&#30340;LLM&#26469;&#20026;&#36817;100&#31181;&#35821;&#35328;&#30340;&#25968;&#21313;&#19975;&#20010;&#25991;&#26412;&#23884;&#20837;&#20219;&#21153;&#29983;&#25104;&#22810;&#26679;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;&#23545;&#27604;&#25439;&#22833;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#24320;&#28304;&#30340;&#21482;&#26377;&#35299;&#30721;&#22120;&#30340;LLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31454;&#20105;&#28608;&#28872;&#30340;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#24403;&#19982;&#21512;&#25104;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#28151;&#21512;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21019;&#36896;&#20102;&#26032;&#30340;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across nearly 100 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;LLMs&#23545;&#35823;&#23548;&#20449;&#24687;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#22312;&#20107;&#23454;&#30693;&#35782;&#19978;&#30340;&#27491;&#30830;&#20449;&#24565;&#24456;&#23481;&#26131;&#34987;&#21508;&#31181;&#35828;&#26381;&#31574;&#30053;&#25152;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2312.09085</link><description>&lt;p&gt;
&#22320;&#29699;&#26159;&#25153;&#24179;&#30340;&#65292;&#22240;&#20026;......&#65306;&#36890;&#36807;&#35828;&#26381;&#24615;&#23545;&#35805;&#30740;&#31350;LLMs&#23545;&#35823;&#23548;&#20449;&#24687;&#30340;&#20449;&#20208;
&lt;/p&gt;
&lt;p&gt;
The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation. (arXiv:2312.09085v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;LLMs&#23545;&#35823;&#23548;&#20449;&#24687;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#22312;&#20107;&#23454;&#30693;&#35782;&#19978;&#30340;&#27491;&#30830;&#20449;&#24565;&#24456;&#23481;&#26131;&#34987;&#21508;&#31181;&#35828;&#26381;&#31574;&#30053;&#25152;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23553;&#35013;&#20102;&#22823;&#37327;&#30693;&#35782;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#22806;&#37096;&#35823;&#23548;&#20449;&#24687;&#30340;&#25915;&#20987;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#22312;&#21333;&#36718;&#23545;&#35805;&#20013;&#30740;&#31350;&#20102;&#36825;&#31181;&#26131;&#21463;&#25915;&#20987;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#29305;&#21035;&#26159;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#65292;&#20449;&#20208;&#21487;&#20197;&#21457;&#29983;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;LLMs&#23545;&#35828;&#26381;&#24615;&#23545;&#35805;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#23427;&#20204;&#21487;&#20197;&#27491;&#30830;&#22238;&#31572;&#30340;&#20107;&#23454;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;Farm&#65288;&#21363;&#20107;&#23454;&#21040;&#35823;&#23548;&#65289;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#19982;&#31995;&#32479;&#29983;&#25104;&#30340;&#35828;&#26381;&#24615;&#35823;&#23548;&#20449;&#24687;&#30456;&#21305;&#37197;&#30340;&#20107;&#23454;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#36861;&#36394;LLMs&#22312;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#30340;&#20449;&#20208;&#21464;&#21270;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#20107;&#23454;&#30693;&#35782;&#19978;&#30340;&#27491;&#30830;&#20449;&#24565;&#24456;&#23481;&#26131;&#34987;&#21508;&#31181;&#35828;&#26381;&#31574;&#30053;&#25152;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02905</link><description>&lt;p&gt;
&#20351;&#29992;&#24744;&#30340;&#26412;&#33021;&#65306;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#19982;&#36716;&#25442;&#22120;&#36827;&#34892;&#25351;&#20196;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#32473;&#20104;&#23427;&#20204;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#25351;&#20196;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#35843;&#25972;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#31639;&#27861;&#26469;&#33258;&#21160;&#20248;&#21270;&#32473;&#20104;&#40657;&#30418;LLMs&#30340;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#39640;&#24230;&#22797;&#26434;&#65288;&#20363;&#22914;&#39640;&#32500;&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#22914;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;LLM&#24615;&#33021;&#30340;&#20989;&#25968;&#65292;BO&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;BO&#20351;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#35813;&#27169;&#22411;&#34987;&#29992;&#20316;BO&#30340;&#20195;&#29702;&#26469;&#24314;&#27169;&#30446;&#26631;&#20989;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#22810;&#27425;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#23588;&#20854;&#26159;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#24314;&#27169;&#39640;&#24230;&#22797;&#26434;&#30340;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31070;&#32463;&#25506;&#27979;&#22120;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algor
&lt;/p&gt;</description></item><item><title>TRAM&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#35780;&#20272;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;&#21313;&#20010;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;TRAM&#22522;&#20934;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20107;&#20214;&#30340;&#21508;&#31181;&#26102;&#38388;&#26041;&#38754;&#12290;&#23613;&#31649;&#20351;&#29992;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#26102;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.00835</link><description>&lt;p&gt;
TRAM&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TRAM: Benchmarking Temporal Reasoning for Large Language Models. (arXiv:2310.00835v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00835
&lt;/p&gt;
&lt;p&gt;
TRAM&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#35780;&#20272;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;&#21313;&#20010;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;TRAM&#22522;&#20934;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20107;&#20214;&#30340;&#21508;&#31181;&#26102;&#38388;&#26041;&#38754;&#12290;&#23613;&#31649;&#20351;&#29992;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#26102;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#25512;&#29702;&#23545;&#20110;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#20107;&#20214;&#30340;&#32454;&#24494;&#24046;&#21035;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#24448;&#23545;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#33539;&#22260;&#26377;&#38480;&#65292;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#36825;&#23548;&#33268;&#19981;&#21516;&#30740;&#31350;&#38388;&#30340;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;TRAM&#30340;&#26102;&#38388;&#25512;&#29702;&#22522;&#20934;&#35780;&#20272;&#65292;&#30001;&#21313;&#20010;&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#28085;&#30422;&#20102;&#20107;&#20214;&#30340;&#21508;&#31181;&#26102;&#38388;&#26041;&#38754;&#65292;&#22914;&#39034;&#24207;&#12289;&#31639;&#26415;&#12289;&#39057;&#29575;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#20351;&#29992;&#20102;&#27969;&#34892;&#30340;LLM&#65292;&#22914;GPT-4&#21644;Llama2&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26102;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#24076;&#26395;TRAM&#33021;&#22815;&#25512;&#21160;&#36827;&#19968;&#27493;&#25552;&#21319;&#26102;&#38388;&#25512;&#29702;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the temporal reasoning capabilities of large language models (LLMs). We conduct an extensive evaluation using popular LLMs, such as GPT-4 and Llama2, in both zero-shot and few-shot learning scenarios. Additionally, we employ BERT-based models to establish the baseline evaluations. Our findings indicate that these models still trail human performance in temporal reasoning tasks. It is our aspiration that TRAM will spur further progress in enhancing the temporal reason
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#21457;&#29616;&#20102;&#24403;&#20195;&#27169;&#22411;&#22312;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#27714;&#35299;&#26041;&#38754;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02691</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#34920;&#29616;&#30340;&#32852;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Joint Study of Phrase Grounding and Task Performance in Vision and Language Models. (arXiv:2309.02691v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02691
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#21457;&#29616;&#20102;&#24403;&#20195;&#27169;&#22411;&#22312;&#30701;&#35821;&#23450;&#20301;&#21644;&#20219;&#21153;&#27714;&#35299;&#26041;&#38754;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38656;&#35201;&#23545;&#35270;&#35273;&#32972;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#65292;&#20851;&#38190;&#26159;&#23558;&#21333;&#35789;&#21644;&#30701;&#35821;&#19982;&#22270;&#20687;&#21306;&#22495;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#36890;&#24120;&#39044;&#26399;&#20197;&#26377;&#21161;&#20110;&#27867;&#21270;&#30340;&#26041;&#24335;&#35299;&#20915;&#20219;&#21153;&#65292;&#35266;&#23519;&#21040;&#24403;&#20195;&#27169;&#22411;&#20013;&#30340;&#36825;&#31181;&#23450;&#20301;&#20063;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#20849;&#21516;&#30740;&#31350;&#20219;&#21153;&#25191;&#34892;&#21644;&#30701;&#35821;&#23450;&#20301;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#22522;&#20934;&#26469;&#30740;&#31350;&#20004;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20195;&#27169;&#22411;&#22312;&#23450;&#20301;&#30701;&#35821;&#21644;&#35299;&#20915;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#23450;&#20301;&#26631;&#27880;&#36827;&#34892;&#24378;&#21046;&#24615;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#25152;&#21019;&#24314;&#30340;&#21160;&#24577;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/lil-lab/phrase_grounding&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key to tasks that require reasoning about natural language in visual contexts is grounding words and phrases to image regions. However, observing this grounding in contemporary models is complex, even if it is generally expected to take place if the task is addressed in a way that is conductive to generalization. We propose a framework to jointly study task performance and phrase grounding, and propose three benchmarks to study the relation between the two. Our results show that contemporary models demonstrate inconsistency between their ability to ground phrases and solve tasks. We show how this can be addressed through brute-force training on ground phrasing annotations, and analyze the dynamics it creates. Code and at available at https://github.com/lil-lab/phrase_grounding.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#24182;&#36827;&#34892;&#34892;&#21160;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#65292;&#20195;&#29702;&#22120;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#27169;&#22411;&#22238;&#28378;&#20013;&#36827;&#34892;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.01399</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#24182;&#36827;&#34892;&#34892;&#21160;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#65292;&#20195;&#29702;&#22120;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#27169;&#22411;&#22238;&#28378;&#20013;&#36827;&#34892;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#19982;&#20154;&#31867;&#22312;&#19990;&#30028;&#20013;&#30456;&#20114;&#20316;&#29992;&#65292;&#20195;&#29702;&#22120;&#38656;&#35201;&#29702;&#35299;&#20154;&#20204;&#20351;&#29992;&#30340;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#31867;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#35270;&#35273;&#19990;&#30028;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#22522;&#20110;&#35821;&#35328;&#34892;&#21160;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#20195;&#29702;&#22120;&#21487;&#20197;&#36890;&#36807;&#20219;&#21153;&#22870;&#21169;&#23398;&#20064;&#25191;&#34892;&#31616;&#21333;&#30340;&#35821;&#35328;&#25351;&#20196;&#65292;&#20294;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#21487;&#20197;&#21033;&#29992;&#20256;&#36798;&#19968;&#33324;&#30693;&#35782;&#12289;&#25551;&#36848;&#19990;&#30028;&#29366;&#24577;&#12289;&#25552;&#20379;&#20114;&#21160;&#21453;&#39304;&#31561;&#22810;&#26679;&#21270;&#35821;&#35328;&#30340;&#20195;&#29702;&#22120;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#65306;&#23558;&#20250;&#34987;&#35266;&#23519;&#21040;&#20160;&#20040;&#12289;&#19990;&#30028;&#23558;&#22914;&#20309;&#36816;&#34892;&#20197;&#21450;&#21738;&#20123;&#24773;&#20917;&#23558;&#33719;&#24471;&#22870;&#21169;&#12290;&#36825;&#20010;&#35266;&#28857;&#23558;&#35821;&#35328;&#29702;&#35299;&#19982;&#26410;&#26469;&#39044;&#27979;&#32479;&#19968;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Dynalang&#65292;&#19968;&#31181;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#30340;&#20195;&#29702;&#22120;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#24819;&#20687;&#30340;&#27169;&#22411;&#22238;&#28378;&#20013;&#23398;&#20064;&#34892;&#21160;&#12290;&#19982;&#21482;&#20351;&#29992;&#35821;&#35328;&#39044;&#27979;&#21160;&#20316;&#30340;&#20256;&#32479;&#20195;&#29702;&#22120;&#19981;&#21516;&#65292;Dynalang&#36890;&#36807;&#36807;&#21435;&#30340;&#35821;&#35328;&#36824;&#21487;&#20197;&#33719;&#21462;&#20016;&#23500;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#34920;&#29616;&#21147;&#30340;&#21516;&#26102;&#65292;&#21160;&#24577;&#20943;&#23569;&#26080;&#25928;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15805</link><description>&lt;p&gt;
&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#29992;&#20110;&#39640;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. (arXiv:2305.15805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#34920;&#29616;&#21147;&#30340;&#21516;&#26102;&#65292;&#21160;&#24577;&#20943;&#23569;&#26080;&#25928;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#29992;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#38590;&#20197;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#20943;&#23569;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#22823;&#22810;&#25968;LLM&#20173;&#28982;&#22312;&#25152;&#26377;&#26631;&#35760;&#23545;&#20043;&#38388;&#37319;&#29992;&#27880;&#24847;&#23618;&#65292;&#20174;&#32780;&#20135;&#29983;&#20108;&#27425;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#26469;&#21160;&#24577;&#20462;&#21098;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21487;&#23398;&#20064;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30830;&#23450;&#21738;&#20123;&#26080;&#20851;&#30340;&#26631;&#35760;&#21487;&#20197;&#20174;&#19978;&#19979;&#25991;&#20013;&#21024;&#38500;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#35299;&#20915;&#20102;&#24615;&#33021;&#38382;&#39064;&#65292;&#32780;&#19988;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20026;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21098;&#26525;&#24378;&#24230;&#21487;&#20197;&#30001;&#31232;&#30095;&#24230;&#21442;&#25968;&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity para
&lt;/p&gt;</description></item><item><title>SPECTRON&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24310;&#32493;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26469;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.15255</link><description>&lt;p&gt;
&#24102;&#26377;&#35821;&#38899;&#30340;LM&#65306;&#36229;&#36234;&#35821;&#38899;&#20196;&#29260;&#30340;&#21475;&#35821;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
LMs with a Voice: Spoken Language Modeling beyond Speech Tokens. (arXiv:2305.15255v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15255
&lt;/p&gt;
&lt;p&gt;
SPECTRON&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24310;&#32493;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#26469;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SPECTRON&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#25191;&#34892;&#35821;&#38899;&#24310;&#32493;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#36755;&#20986;&#65292;&#25972;&#20010;&#31995;&#32479;&#37117;&#22312;&#39057;&#35889;&#22270;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#22312;&#39057;&#35889;&#22270;&#39046;&#22495;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#30456;&#23545;&#20110;&#20351;&#29992;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#30340;&#29616;&#26377;&#32423;&#32852;&#26041;&#27861;&#31616;&#21270;&#20102;&#25105;&#20204;&#30340;&#35821;&#38899;&#24310;&#32493;&#31995;&#32479;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#20041;&#20869;&#23481;&#21644;&#35762;&#35805;&#32773;&#20445;&#25252;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#33719;&#24471;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32593;&#31449;https://michelleramanovich.github.io/spectron/spectron&#19978;&#21487;&#20197;&#25214;&#21040;&#38899;&#39057;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SPECTRON, a novel approach to adapting pre-trained language models (LMs) to perform speech continuation. By leveraging pre-trained speech encoders, our model generates both text and speech outputs with the entire system being trained end-to-end operating directly on spectrograms. Training the entire model in the spectrogram domain simplifies our speech continuation system versus existing cascade methods which use discrete speech representations. We further show our method surpasses existing spoken language models both in semantic content and speaker preservation while also benefiting from the knowledge transferred from pre-existing models. Audio samples can be found in our website https://michelleramanovich.github.io/spectron/spectron
&lt;/p&gt;</description></item></channel></rss>