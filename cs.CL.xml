<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#31034;&#20363;&#30340;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-Vision&#22312;&#31185;&#23398;&#22270;&#20687;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#23545;&#25552;&#31034;&#12289;&#21453;&#20107;&#23454;&#25991;&#26412;&#21644;&#30456;&#23545;&#31354;&#38388;&#20851;&#31995;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#21152;&#24555;&#30740;&#31350;&#20154;&#21592;&#23545;&#26032;&#27169;&#22411;&#30340;&#30452;&#35266;&#29702;&#35299;&#65292;&#24182;&#23637;&#31034;GPT-Vision&#22914;&#20309;&#25552;&#39640;&#20449;&#24687;&#21487;&#35775;&#38382;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.02069</link><description>&lt;p&gt;
&#20351;&#29992;&#31185;&#23398;&#22270;&#20687;&#26469;&#25581;&#31034;GPT-Vision&#30340;&#33021;&#21147;&#30340;&#22522;&#20110;&#30452;&#35273;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Grounded Intuition of GPT-Vision's Abilities with Scientific Images. (arXiv:2311.02069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#31034;&#20363;&#30340;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-Vision&#22312;&#31185;&#23398;&#22270;&#20687;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#23545;&#25552;&#31034;&#12289;&#21453;&#20107;&#23454;&#25991;&#26412;&#21644;&#30456;&#23545;&#31354;&#38388;&#20851;&#31995;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#26377;&#21161;&#20110;&#21152;&#24555;&#30740;&#31350;&#20154;&#21592;&#23545;&#26032;&#27169;&#22411;&#30340;&#30452;&#35266;&#29702;&#35299;&#65292;&#24182;&#23637;&#31034;GPT-Vision&#22914;&#20309;&#25552;&#39640;&#20449;&#24687;&#21487;&#35775;&#38382;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-Vision&#22312;&#35768;&#22810;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#32473;&#25105;&#20204;&#30041;&#19979;&#20102;&#28145;&#21051;&#21360;&#35937;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#19968;&#20010;&#24120;&#35265;&#30340;&#26032;&#25361;&#25112;&#65306;&#25105;&#20204;&#23545;&#20854;&#33021;&#21147;&#21644;&#38480;&#21046;&#20960;&#20046;&#19968;&#26080;&#25152;&#30693;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#19968;&#31181;&#35768;&#22810;&#20154;&#26412;&#33021;&#22320;&#35797;&#22270;&#22521;&#20859;&#23545;&#36825;&#20010;&#26032;&#27169;&#22411;&#30340;&#8220;&#22522;&#20110;&#30452;&#35273;&#8221;&#29702;&#35299;&#30340;&#36807;&#31243;&#12290;&#21463;&#21040;&#36828;&#31163;&#22522;&#20934;&#27979;&#35797;&#32780;&#20542;&#21521;&#20110;&#22522;&#20110;&#31034;&#20363;&#30340;&#23450;&#24615;&#35780;&#20272;&#30340;&#26368;&#26032;&#36235;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#31038;&#20250;&#31185;&#23398;&#21644;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#25166;&#26681;&#29702;&#35770;&#21644;&#20027;&#39064;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23450;&#24615;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#26469;&#30740;&#31350;&#31185;&#23398;&#22270;&#20687;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#65292;&#21457;&#29616;GPT-Vision&#23545;&#25552;&#31034;&#12289;&#22270;&#20687;&#20013;&#30340;&#21453;&#20107;&#23454;&#25991;&#26412;&#21644;&#30456;&#23545;&#31354;&#38388;&#20851;&#31995;&#29305;&#21035;&#25935;&#24863;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#20998;&#26512;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21152;&#24555;&#23545;&#26032;&#27169;&#22411;&#30340;&#22522;&#20110;&#30452;&#35273;&#30340;&#29702;&#35299;&#65292;&#21516;&#26102;&#23637;&#31034;GPT-Vision&#22914;&#20309;&#24212;&#29992;&#20110;&#20351;&#20449;&#24687;&#26356;&#26131;&#20110;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-Vision has impressed us on a range of vision-language tasks, but it comes with the familiar new challenge: we have little idea of its capabilities and limitations. In our study, we formalize a process that many have instinctively been trying already to develop "grounded intuition" of this new model. Inspired by the recent movement away from benchmarking in favor of example-driven qualitative evaluation, we draw upon grounded theory and thematic analysis in social science and human-computer interaction to establish a rigorous framework for qualitative evaluation in natural language processing. We use our technique to examine alt text generation for scientific figures, finding that GPT-Vision is particularly sensitive to prompting, counterfactual text in images, and relative spatial relationships. Our method and analysis aim to help researchers ramp up their own grounded intuitions of new models while exposing how GPT-Vision can be applied to make information more accessible.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#30340;&#37051;&#36817;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#25345;&#32493;&#39044;&#35757;&#32451;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#19971;&#31181;&#19981;&#21516;&#35821;&#35328;&#21644;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;MIXAG&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.02025</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#37051;&#36817;&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection. (arXiv:2311.02025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#30340;&#37051;&#36817;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#25345;&#32493;&#39044;&#35757;&#32451;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#19971;&#31181;&#19981;&#21516;&#35821;&#35328;&#21644;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;MIXAG&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#35821;&#35328;&#20013;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#25345;&#32493;&#39044;&#35757;&#32451;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#20197;&#25913;&#21892;&#36328;&#35821;&#35328;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#12290;&#23545;&#20110;&#25968;&#25454;&#22686;&#24378;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#29616;&#26377;&#30340;&#22522;&#20110;&#37051;&#36817;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;MIXAG&#65292;&#19968;&#31181;&#26681;&#25454;&#23454;&#20363;&#34920;&#31034;&#30340;&#35282;&#24230;&#25554;&#20540;&#19968;&#23545;&#23454;&#20363;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#19971;&#31181;&#19982;&#33521;&#35821;&#22312;&#35821;&#35328;&#31867;&#22411;&#19978;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#19977;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21487;&#20197;&#25552;&#21319;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#25152;&#26377;&#30446;&#26631;&#35821;&#35328;&#20013;&#65292;MIXAG&#22312;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#26174;&#33879;&#25913;&#21892;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23637;&#31034;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#22914;&#20309;&#25913;&#36827;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance few-shot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;RNN-like&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#36951;&#24536;&#25552;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#21512;&#25104;&#26799;&#24230;&#25945;&#23548;&#27169;&#22411;&#35760;&#20303;&#25552;&#31034;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2311.01981</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26799;&#24230;&#32531;&#35299;RNN-like&#35821;&#35328;&#27169;&#22411;&#30340;&#36951;&#24536;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models. (arXiv:2311.01981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;RNN-like&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#36951;&#24536;&#25552;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#21512;&#25104;&#26799;&#24230;&#25945;&#23548;&#27169;&#22411;&#35760;&#20303;&#25552;&#31034;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;NLP&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#20851;&#27880;RNN-like&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#26377;&#20960;&#20010;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#23637;&#31034;&#20986;&#19982;&#20256;&#32479;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;RNN&#30340;&#24490;&#29615;&#24615;&#36136;&#65292;&#36825;&#31181;&#35821;&#35328;&#27169;&#22411;&#21482;&#33021;&#22312;&#19968;&#32452;&#22266;&#23450;&#38271;&#24230;&#30340;&#29366;&#24577;&#21521;&#37327;&#20013;&#23384;&#20648;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#32463;&#36807;&#20102;&#35768;&#22810;&#25913;&#36827;&#21644;&#20248;&#21270;&#65292;&#24403;&#32473;&#20986;&#22797;&#26434;&#30340;&#25351;&#20196;&#25110;&#25552;&#31034;&#26102;&#65292;&#23427;&#20204;&#20173;&#28982;&#20250;&#36951;&#24536;&#12290;&#20316;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#35201;&#21644;&#26368;&#20851;&#27880;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#32531;&#35299;&#25552;&#31034;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21512;&#25104;&#26799;&#24230;&#25945;&#23548;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#35760;&#20303;&#25552;&#31034;&#30340;&#26550;&#26500;&#12290;&#20026;&#20102;&#24378;&#21046;&#27169;&#22411;&#35760;&#20303;&#25552;&#31034;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#32534;&#30721;&#25552;&#31034;&#30340;&#29366;&#24577;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#21270;&#20026;&#27169;&#22411;&#21442;&#25968;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
RNN-like language models are getting renewed attention from NLP researchers in recent years and several models have made significant progress, which demonstrates performance comparable to traditional transformers. However, due to the recurrent nature of RNNs, this kind of language model can only store information in a set of fixed-length state vectors. As a consequence, they still suffer from forgetfulness though after a lot of improvements and optimizations, when given complex instructions or prompts. As the prompted generation is the main and most concerned function of LMs, solving the problem of forgetting in the process of generation is no wonder of vital importance. In this paper, focusing on easing the prompt forgetting during generation, we proposed an architecture to teach the model memorizing prompt during generation by synthetic gradient. To force the model to memorize the prompt, we derive the states that encode the prompt, then transform it into model parameter modification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#32467;&#26500;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#23545;&#25552;&#31034;&#30340;&#35821;&#35328;&#23646;&#24615;&#26377;&#36739;&#39640;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01967</link><description>&lt;p&gt;
&#25552;&#31034;&#30340;&#35821;&#35328;&#65306;&#20160;&#20040;&#35821;&#35328;&#23646;&#24615;&#20351;&#24471;&#25552;&#31034;&#25104;&#21151;&#65311;
&lt;/p&gt;
&lt;p&gt;
The language of prompting: What linguistic properties make a prompt successful?. (arXiv:2311.01967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#32467;&#26500;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#23545;&#25552;&#31034;&#30340;&#35821;&#35328;&#23646;&#24615;&#26377;&#36739;&#39640;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#19968;&#20195;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;&#26469;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24615;&#33021;&#23545;&#25552;&#31034;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65292;&#20154;&#20204;&#20184;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#36827;&#34892;&#20247;&#21253;&#25552;&#31034;&#25110;&#35774;&#35745;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#23545;&#25552;&#31034;&#30340;&#35821;&#35328;&#23646;&#24615;&#19982;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#20041;&#19978;&#31561;&#25928;&#20294;&#22312;&#35821;&#35328;&#32467;&#26500;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#35821;&#27861;&#23646;&#24615;&#65288;&#22914;&#24773;&#24577;&#12289;&#26102;&#24577;&#12289;&#35821;&#24577;&#21644;&#35821;&#27668;&#65289;&#20197;&#21450;&#36890;&#36807;&#20351;&#29992;&#21516;&#20041;&#35789;&#24341;&#20837;&#35789;&#27719;-&#35821;&#20041;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#24120;&#35265;&#20551;&#35774;&#30456;&#30683;&#30462;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#22256;&#24785;&#24230;&#30340;&#25552;&#31034;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#36825;&#20123;&#25552;&#31034;&#21453;&#26144;&#20102;&#39044;&#35757;&#32451;&#25110;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#20351;&#29992;&#12290;&#25552;&#31034;&#22312;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#20043;&#38388;&#36716;&#31227;&#25928;&#26524;&#19981;&#20339;&#65292;&#24615;&#33021;&#26377;&#25152;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on lower perplexity prompts that reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19981;&#24688;&#24403;&#20351;&#29992;&#35780;&#20272;&#22522;&#20934;&#21644;&#35823;&#23548;&#24615;&#35299;&#37322;&#35780;&#20272;&#32467;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#22522;&#20934;&#27844;&#28431;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2311.01964</link><description>&lt;p&gt;
&#19981;&#35201;&#35753;&#20320;&#30340;LLM&#25104;&#20026;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#27450;&#39575;&#32773;
&lt;/p&gt;
&lt;p&gt;
Don't Make Your LLM an Evaluation Benchmark Cheater. (arXiv:2311.01964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19981;&#24688;&#24403;&#20351;&#29992;&#35780;&#20272;&#22522;&#20934;&#21644;&#35823;&#23548;&#24615;&#35299;&#37322;&#35780;&#20272;&#32467;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#22522;&#20934;&#27844;&#28431;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#21069;&#27839;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#33021;&#21147;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#26500;&#24314;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#27979;&#37327;LLMs&#22312;&#19981;&#21516;&#26041;&#38754;&#30340;&#33021;&#21147;&#27700;&#24179;&#12290;&#23613;&#31649;&#24050;&#32463;&#21457;&#24067;&#20102;&#35768;&#22810;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#22522;&#20934;&#30340;&#21512;&#29702;&#20351;&#29992;&#21644;&#19981;&#21516;&#27169;&#22411;&#30340;&#20844;&#24179;&#27604;&#36739;&#30340;&#20851;&#27880;&#36234;&#26469;&#36234;&#22810;&#12290;&#37492;&#20110;&#36825;&#20123;&#20851;&#27880;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#19981;&#24688;&#24403;&#20351;&#29992;&#35780;&#20272;&#22522;&#20934;&#21644;&#35823;&#23548;&#24615;&#35299;&#37322;&#35780;&#20272;&#32467;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#24433;&#21709;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#19968;&#20010;&#29305;&#27530;&#38382;&#39064;&#65292;&#21363;&#23548;&#33268;&#19981;&#24688;&#24403;&#35780;&#20272;&#30340;\emph{&#22522;&#20934;&#27844;&#28431;}&#65292;&#21363;&#35780;&#20272;&#38598;&#30456;&#20851;&#30340;&#25968;&#25454;&#20598;&#23572;&#34987;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26159;&#22312;&#27169;&#22411;&#27979;&#35797;&#20043;&#21069;&#20934;&#22791;&#30340;&#65292;&#22240;&#27492;&#36825;&#31181;&#29616;&#35937;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models~(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, \ie \emph{benchmark leakage}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26684;&#32599;&#23425;&#26681;&#22823;&#23398;&#22312;BabyLM&#25361;&#25112;&#36187;&#20013;&#30340;&#24037;&#20316;&#65292;&#25506;&#35752;&#20102;&#23558;&#35757;&#32451;&#36807;&#31243;&#31616;&#21333;&#21270;&#30340;&#31574;&#30053;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#20165;&#25913;&#21464;&#19978;&#19979;&#25991;&#22823;&#23567;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#21508;&#39033;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2311.01955</link><description>&lt;p&gt;
&#22826;&#22810;&#20449;&#24687;&#65306;&#20445;&#25345;BabyLM&#35757;&#32451;&#31616;&#21333;
&lt;/p&gt;
&lt;p&gt;
Too Much Information: Keeping Training Simple for BabyLMs. (arXiv:2311.01955v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26684;&#32599;&#23425;&#26681;&#22823;&#23398;&#22312;BabyLM&#25361;&#25112;&#36187;&#20013;&#30340;&#24037;&#20316;&#65292;&#25506;&#35752;&#20102;&#23558;&#35757;&#32451;&#36807;&#31243;&#31616;&#21333;&#21270;&#30340;&#31574;&#30053;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#20165;&#25913;&#21464;&#19978;&#19979;&#25991;&#22823;&#23567;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#21508;&#39033;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#26684;&#32599;&#23425;&#26681;&#22823;&#23398;&#22312;BabyLM&#25361;&#25112;&#36187;&#20013;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#36981;&#24490;&#23156;&#20799;&#19968;&#26679;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#20808;&#20171;&#20110;&#36739;&#31616;&#21333;&#30340;&#27010;&#24565;&#65292;&#28982;&#21518;&#24314;&#31435;&#22312;&#27492;&#22522;&#30784;&#19978;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#27010;&#24565;&#30340;&#24605;&#24819;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#35282;&#24230;&#65292;&#21363;&#19978;&#19979;&#25991;&#22823;&#23567;&#12289;&#35789;&#27719;&#37327;&#21644;&#25968;&#25454;&#30340;&#25972;&#20307;&#35821;&#35328;&#22797;&#26434;&#24615;&#65292;&#26469;&#30740;&#31350;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#26377;&#19978;&#19979;&#25991;&#22823;&#23567;&#36825;&#20010;&#31616;&#21333;&#30340;&#25913;&#21464;&#23545;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30495;&#27491;&#26377;&#30410;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#31616;&#21333;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#30340;&#21464;&#21270;&#20351;&#25105;&#20204;&#22312;(Super)GLUE&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2&#20010;&#28857;&#65292;&#22312;MSGS&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;1&#20010;&#28857;&#65292;&#22312;BLiMP&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;12%&#12290;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#21463;&#38480;&#27169;&#22411;&#32988;&#36807;&#20102;&#35757;&#32451;&#20102;10&#20493;&#25968;&#25454;&#37327;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper details the work of the University of Groningen for the BabyLM Challenge. We follow the idea that, like babies, language models should be introduced to simpler concepts first and build off of that knowledge to understand more complex concepts. We examine this strategy of simple-then-complex through a variety of lenses, namely context size, vocabulary, and overall linguistic complexity of the data. We find that only one, context size, is truly beneficial to training a language model. However this simple change to context size gives us improvements of 2 points on average on (Super)GLUE tasks, 1 point on MSGS tasks, and 12\% on average on BLiMP tasks. Our context-limited model outperforms the baseline that was trained on 10$\times$ the amount of data.
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;HICL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;HICL&#21033;&#29992;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26356;&#26126;&#30830;&#30340;&#25552;&#31034;&#26041;&#24335;&#26469;&#22686;&#24378;LLM&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#31034;&#20363;&#26816;&#32034;&#22120;&#65288;HER&#65289;&#65292;&#25105;&#20204;&#36824;&#33021;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#31034;&#20363;&#26469;&#22686;&#24378;&#31034;&#33539;&#12290;&#23545;&#20110;&#24320;&#25918;&#22495;&#38382;&#31572;&#65292;HICL&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.01949</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks. (arXiv:2311.01949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01949
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;HICL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;HICL&#21033;&#29992;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26356;&#26126;&#30830;&#30340;&#25552;&#31034;&#26041;&#24335;&#26469;&#22686;&#24378;LLM&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#31034;&#20363;&#26816;&#32034;&#22120;&#65288;HER&#65289;&#65292;&#25105;&#20204;&#36824;&#33021;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#31034;&#20363;&#26469;&#22686;&#24378;&#31034;&#33539;&#12290;&#23545;&#20110;&#24320;&#25918;&#22495;&#38382;&#31572;&#65292;HICL&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#24050;&#32463;&#20986;&#29616;&#65292;&#20351;&#24471;&#23427;&#20204;&#33021;&#22815;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#36755;&#20837;-&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#26631;&#20934;ICL&#35774;&#32622;&#19979;&#65292;LLM&#26377;&#26102;&#20250;&#24573;&#30053;&#31034;&#33539;&#20013;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#31216;&#20026;&#25552;&#31034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;HICL&#65289;&#65292;&#26469;&#25506;&#32034;ICL&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#12290;HICL&#21033;&#29992;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#30693;&#35782;&#19982;LLM&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36319;&#36394;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#20197;&#35782;&#21035;&#29305;&#23450;&#30340;&#31034;&#20363;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#31034;&#20363;&#26816;&#32034;&#22120;&#65288;HER&#65289;&#26469;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#31034;&#20363;&#36827;&#34892;&#22686;&#24378;&#31034;&#33539;&#12290;&#25105;&#20204;&#20351;&#29992;HER&#35780;&#20272;&#20102;HICL&#22312;3&#20010;&#24320;&#25918;&#22495;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) ability has emerged with the increasing scale of large language models (LLMs), enabling them to learn input-label mappings from demonstrations and perform well on downstream tasks. However, under the standard ICL setting, LLMs may sometimes neglect query-related information in demonstrations, leading to incorrect predictions. To address this limitation, we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to explore the power of ICL in open-domain question answering, an important form in knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract query-related knowledge from demonstrations, then concatenates the knowledge to prompt LLMs in a more explicit way. Furthermore, we track the source of this knowledge to identify specific examples, and introduce a Hint-related Example Retriever (HER) to select informative examples for enhanced demonstrations. We evaluate HICL with HER on 3 open-domain QA benchmarks, and observe av
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;TDGU&#65292;&#23558;&#21160;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#24102;&#26377;&#26102;&#38388;&#25139;&#30340;&#22270;&#20107;&#20214;&#65292;&#24182;&#20351;&#29992;&#26102;&#24577;&#22522;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;TDGU&#22312;&#25991;&#26412;&#28216;&#25103;TextWorld&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;DGU&#12290;</title><link>http://arxiv.org/abs/2311.01928</link><description>&lt;p&gt;
&#20174;&#20132;&#20114;&#24335;&#25991;&#26412;&#28216;&#25103;&#26500;&#24314;&#26102;&#38388;&#21160;&#24577;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Constructing Temporal Dynamic Knowledge Graphs from Interactive Text-based Games. (arXiv:2311.01928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;TDGU&#65292;&#23558;&#21160;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#24102;&#26377;&#26102;&#38388;&#25139;&#30340;&#22270;&#20107;&#20214;&#65292;&#24182;&#20351;&#29992;&#26102;&#24577;&#22522;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;TDGU&#22312;&#25991;&#26412;&#28216;&#25103;TextWorld&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;DGU&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20132;&#20114;&#24335;&#25991;&#26412;&#28216;&#25103;&#34987;&#29992;&#20316;&#27979;&#35797;&#20132;&#20114;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35797;&#39564;&#22330;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#31163;&#25955;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#28216;&#25103;&#29366;&#24577;&#30340;&#26041;&#24335;&#26469;&#29609;&#25991;&#26412;&#28216;&#25103;&#65292;&#20854;&#20013;&#30693;&#35782;&#22270;&#35889;&#30001;Discrete Graph Updater (DGU) &#26500;&#24314;&#12290;&#34429;&#28982;DGU&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26102;&#38388;&#24615;&#21644;&#23545;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#22797;&#26434;&#29615;&#22659;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#65292;&#20854;&#30693;&#35782;&#22270;&#35889;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;DGU&#30340;&#24369;&#28857;&#24182;&#20445;&#25345;&#20854;&#39640;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#31163;&#25955;&#22270;&#26356;&#26032;&#22120; (TDGU)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#21160;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#24102;&#26377;&#26102;&#38388;&#25139;&#30340;&#22270;&#20107;&#20214;&#65292;&#24182;&#20351;&#29992;&#26102;&#24577;&#22522;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#20174;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;TextWorld&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;TDGU&#20248;&#20110;&#22522;&#32447;DGU&#12290;
&lt;/p&gt;
&lt;p&gt;
In natural language processing, interactive text-based games serve as a test bed for interactive AI systems. Prior work has proposed to play text-based games by acting based on discrete knowledge graphs constructed by the Discrete Graph Updater (DGU) to represent the game state from the natural language description. While DGU has shown promising results with high interpretability, it suffers from lower knowledge graph accuracy due to its lack of temporality and limited generalizability to complex environments with objects with the same label. In order to address DGU's weaknesses while preserving its high interpretability, we propose the Temporal Discrete Graph Updater (TDGU), a novel neural network model that represents dynamic knowledge graphs as a sequence of timestamped graph events and models them using a temporal point based graph neural network. Through experiments on the dataset collected from a text-based game TextWorld, we show that TDGU outperforms the baseline DGU. We furthe
&lt;/p&gt;</description></item><item><title>GateLoop&#26159;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#24207;&#21015;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#32473;Attention&#12290;</title><link>http://arxiv.org/abs/2311.01927</link><description>&lt;p&gt;
GateLoop: &#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01927
&lt;/p&gt;
&lt;p&gt;
GateLoop&#26159;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#24207;&#21015;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#32473;Attention&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#36882;&#24402;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#38271;&#24207;&#21015;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#29616;&#26377;&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20854;&#28508;&#21147;&#12290;&#22312;&#36825;&#19968;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GateLoop&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#30784;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#25511;&#21046;&#30340;&#29366;&#24577;&#36716;&#25442;&#26469;&#25512;&#24191;&#32447;&#24615;&#36882;&#24402;&#27169;&#22411;&#65292;&#22914;S4&#12289;S5&#12289;LRU&#21644;RetNet&#12290;&#21033;&#29992;&#36825;&#19968;&#29702;&#35770;&#36827;&#27493;&#65292;GateLoop&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#22312;&#23454;&#35777;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20302;&#25104;&#26412;&#30340;$O(l)$&#36882;&#24402;&#27169;&#24335;&#21644;&#39640;&#24230;&#20248;&#21270;&#30340;&#20851;&#32852;&#25195;&#25551;&#23454;&#29616;&#30340;&#39640;&#25928;$O(l \log_{2} l)$&#24182;&#34892;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;$O(l^2)$&#30340;&#20195;&#29702;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23545;Transformer&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#26550;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#21521;Attention&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;&#32780;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#26080;&#20851;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on da
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25991;&#31456;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#23398;&#20013;&#30340;&#24212;&#29992;&#21644;&#24847;&#20041;&#12290;LLMs&#22312;&#30693;&#35782;&#26816;&#32034;&#12289;&#30740;&#31350;&#25903;&#25345;&#12289;&#20020;&#24202;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#35786;&#26029;&#36741;&#21161;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22810;&#27169;&#24577;LLMs&#21487;&#20197;&#22788;&#29702;&#21307;&#23398;&#24433;&#20687;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31561;&#22810;&#26679;&#21270;&#25968;&#25454;&#31867;&#22411;&#20197;&#22686;&#24378;&#35786;&#26029;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01918</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38416;&#26126;&#20102;&#20154;&#24037;&#21307;&#30103;&#21161;&#25163;&#30340;&#36827;&#23637;&#36335;&#24452;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review. (arXiv:2311.01918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25991;&#31456;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#23398;&#20013;&#30340;&#24212;&#29992;&#21644;&#24847;&#20041;&#12290;LLMs&#22312;&#30693;&#35782;&#26816;&#32034;&#12289;&#30740;&#31350;&#25903;&#25345;&#12289;&#20020;&#24202;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#35786;&#26029;&#36741;&#21161;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22810;&#27169;&#24577;LLMs&#21487;&#20197;&#22788;&#29702;&#21307;&#23398;&#24433;&#20687;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31561;&#22810;&#26679;&#21270;&#25968;&#25454;&#31867;&#22411;&#20197;&#22686;&#24378;&#35786;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#27169;&#25311;&#20154;&#31867;&#32423;&#21035;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#22686;&#24378;&#21307;&#30103;&#21508;&#20010;&#26041;&#38754;&#30340;&#37325;&#35201;&#20852;&#36259;&#65292;&#33539;&#22260;&#20174;&#21307;&#23398;&#25945;&#32946;&#21040;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#28041;&#21450;&#22810;&#26041;&#38754;&#30340;&#25968;&#25454;&#27169;&#24577;&#21644;&#24494;&#22937;&#30340;&#25512;&#29702;&#25216;&#33021;&#65292;&#36825;&#32473;LLMs&#30340;&#25972;&#21512;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#22312;&#21307;&#23398;&#20013;&#30340;&#24212;&#29992;&#21644;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#32771;&#23519;&#20102;&#36890;&#29992;&#22411;&#21644;&#19987;&#38376;&#21270;LLMs&#30340;&#22522;&#26412;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#30693;&#35782;&#26816;&#32034;&#12289;&#30740;&#31350;&#25903;&#25345;&#12289;&#20020;&#24202;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#35786;&#26029;&#36741;&#21161;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#37492;&#20110;&#21307;&#23398;&#30340;&#22266;&#26377;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#35813;&#32508;&#36848;&#36827;&#19968;&#27493;&#20851;&#27880;&#22810;&#27169;&#24577;LLMs&#65292;&#30740;&#31350;&#20854;&#22788;&#29702;&#21307;&#23398;&#24433;&#20687;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31561;&#22810;&#26679;&#21270;&#25968;&#25454;&#31867;&#22411;&#20197;&#22686;&#24378;&#35786;&#26029;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of artificial intelligence, large language models (LLMs) have shown promising capabilities in mimicking human-level language comprehension and reasoning. This has sparked significant interest in applying LLMs to enhance various aspects of healthcare, ranging from medical education to clinical decision support. However, medicine involves multifaceted data modalities and nuanced reasoning skills, presenting challenges for integrating LLMs. This paper provides a comprehensive review on the applications and implications of LLMs in medicine. It begins by examining the fundamental applications of general-purpose and specialized LLMs, demonstrating their utilities in knowledge retrieval, research support, clinical workflow automation, and diagnostic assistance. Recognizing the inherent multimodality of medicine, the review then focuses on multimodal LLMs, investigating their ability to process diverse data types like medical imaging and EHRs to augment diagnostic ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#22522;&#20110;Llama 2&#30340;&#31995;&#32479;&#65292;&#22312;&#22788;&#29702;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#31616;&#21270;&#30340;PLABA&#20849;&#20139;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;&#36890;&#36807;&#24341;&#20837;&#21477;&#23376;&#32423;&#21644;&#26631;&#35760;&#32423;&#30340;&#25439;&#22833;&#26435;&#37325;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20135;&#29983;&#19982;&#20154;&#24037;&#27880;&#37322;&#32773;&#30456;&#20284;&#30340;&#31616;&#21270;&#32467;&#26524;&#65292;&#35821;&#35328;&#26356;&#31616;&#21333;&#65292;&#24182;&#19988;&#36827;&#34892;&#26356;&#22810;&#30340;&#32534;&#36753;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2311.01907</link><description>&lt;p&gt;
BoschAI @ PLABA 2023: &#21033;&#29992;&#32534;&#36753;&#25805;&#20316;&#22312;&#31471;&#21040;&#31471;&#31070;&#32463;&#21477;&#23376;&#31616;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
BoschAI @ PLABA 2023: Leveraging Edit Operations in End-to-End Neural Sentence Simplification. (arXiv:2311.01907v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#22522;&#20110;Llama 2&#30340;&#31995;&#32479;&#65292;&#22312;&#22788;&#29702;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#31616;&#21270;&#30340;PLABA&#20849;&#20139;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;&#36890;&#36807;&#24341;&#20837;&#21477;&#23376;&#32423;&#21644;&#26631;&#35760;&#32423;&#30340;&#25439;&#22833;&#26435;&#37325;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20135;&#29983;&#19982;&#20154;&#24037;&#27880;&#37322;&#32773;&#30456;&#20284;&#30340;&#31616;&#21270;&#32467;&#26524;&#65292;&#35821;&#35328;&#26356;&#31616;&#21333;&#65292;&#24182;&#19988;&#36827;&#34892;&#26356;&#22810;&#30340;&#32534;&#36753;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31616;&#21270;&#21487;&#20197;&#24110;&#21161;&#26222;&#36890;&#20154;&#29702;&#35299;&#22797;&#26434;&#30340;&#31185;&#23398;&#25991;&#26412;&#12290;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#29992;&#20110;&#23558;&#22797;&#26434;&#35821;&#35328;&#36716;&#25442;&#20026;&#31616;&#21333;&#35821;&#35328;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22522;&#20110;Llama 2&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#22788;&#29702;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#31616;&#21270;&#30340;PLABA&#20849;&#20139;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;&#25105;&#20204;&#21457;&#29616;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#20849;&#20139;&#30340;&#26631;&#35760;&#30340;&#25968;&#37327;&#24456;&#22810;&#65292;&#23548;&#33268;&#35757;&#32451;&#20449;&#21495;&#36739;&#24369;&#21644;&#20445;&#23432;&#30340;&#32534;&#36753;&#27169;&#22411;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21477;&#23376;&#32423;&#21644;&#26631;&#35760;&#32423;&#30340;&#25439;&#22833;&#26435;&#37325;&#12290;&#23427;&#20204;&#32473;&#20104;&#20462;&#25913;&#30340;&#26631;&#35760;&#26356;&#39640;&#30340;&#26435;&#37325;&#65292;&#20462;&#25913;&#36890;&#36807;&#32534;&#36753;&#36317;&#31163;&#21644;&#32534;&#36753;&#25805;&#20316;&#36827;&#34892;&#25351;&#31034;&#12290;&#25105;&#20204;&#22312;PLABA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#31616;&#21270;&#32467;&#26524;&#26356;&#25509;&#36817;&#20154;&#24037;&#27880;&#37322;&#32773;&#21019;&#24314;&#30340;&#32467;&#26524;&#65288;+1.8% / +3.5% SARI&#65289;&#65292;&#35821;&#35328;&#26356;&#31616;&#21333;&#65288;-1 / -1.1 FKGL&#65289;&#65292;&#24182;&#19988;&#32534;&#36753;&#26356;&#22810;&#65288;1.6x / 1.8x&#32534;&#36753;&#36317;&#31163;&#65289;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#26631;&#20934;&#20132;&#21449;&#29109;&#36827;&#34892;&#24494;&#35843;&#30340;&#30456;&#21516;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic simplification can help laypeople to comprehend complex scientific text. Language models are frequently applied to this task by translating from complex to simple language. In this paper, we describe our system based on Llama 2, which ranked first in the PLABA shared task addressing the simplification of biomedical text. We find that the large portion of shared tokens between input and output leads to weak training signals and conservatively editing models. To mitigate these issues, we propose sentence-level and token-level loss weights. They give higher weight to modified tokens, indicated by edit distance and edit operations, respectively. We conduct an empirical evaluation on the PLABA dataset and find that both approaches lead to simplifications closer to those created by human annotators (+1.8% / +3.5% SARI), simpler language (-1 / -1.1 FKGL) and more edits (1.6x / 1.8x edit distance) compared to the same model fine-tuned with standard cross entropy. We furthermore show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25351;&#31034;&#24615;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#27010;&#35272;&#38271;&#31687;&#35752;&#35770;&#65292;&#24182;&#26681;&#25454;&#35770;&#36848;&#21477;&#36827;&#34892;&#32858;&#31867;&#21644;&#20998;&#31867;&#29983;&#25104;&#25688;&#35201;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#35770;&#22363;&#35752;&#35770;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01882</link><description>&lt;p&gt;
&#38271;&#31687;&#35752;&#35770;&#30340;&#25351;&#31034;&#24615;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Indicative Summarization of Long Discussions. (arXiv:2311.01882v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25351;&#31034;&#24615;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#27010;&#35272;&#38271;&#31687;&#35752;&#35770;&#65292;&#24182;&#26681;&#25454;&#35770;&#36848;&#21477;&#36827;&#34892;&#32858;&#31867;&#21644;&#20998;&#31867;&#29983;&#25104;&#25688;&#35201;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#35770;&#22363;&#35752;&#35770;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35770;&#22363;&#40723;&#21169;&#20154;&#20204;&#22312;&#35768;&#22810;&#20027;&#39064;&#19978;&#20132;&#27969;&#21644;&#35752;&#35770;&#19981;&#21516;&#30340;&#31435;&#22330;&#12290;&#23427;&#20204;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#20010;&#23637;&#31034;&#33258;&#24049;&#35266;&#28857;&#30340;&#26426;&#20250;&#65292;&#36824;&#21487;&#20197;&#27719;&#38598;&#24191;&#27867;&#30340;&#20854;&#20182;&#35266;&#28857;&#12290;&#28982;&#32780;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#38271;&#31687;&#35752;&#35770;&#24456;&#38590;&#27010;&#35272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#38271;&#31687;&#35752;&#35770;&#25351;&#31034;&#24615;&#25688;&#35201;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#22522;&#26412;&#19978;&#20316;&#20026;&#30446;&#24405;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23545;&#35770;&#36848;&#21477;&#36827;&#34892;&#32858;&#31867;&#65292;&#29983;&#25104;&#32858;&#31867;&#26631;&#31614;&#20316;&#20026;&#25277;&#35937;&#25688;&#35201;&#65292;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;&#32858;&#31867;&#26631;&#31614;&#20998;&#31867;&#20026;&#35770;&#35777;&#26694;&#26550;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#20004;&#32423;&#25688;&#35201;&#12290;&#22522;&#20110;&#32463;&#36807;&#24191;&#27867;&#20248;&#21270;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;19&#20010;LLM&#29992;&#20110;&#29983;&#25104;&#32858;&#31867;&#26631;&#31614;&#21644;&#26694;&#26550;&#20998;&#31867;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25351;&#31034;&#24615;&#25688;&#35201;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21517;&#20026;Discussion Explorer&#30340;&#26032;&#30340;&#21487;&#35270;&#21270;&#30028;&#38754;&#36827;&#34892;&#20102;&#30446;&#26631;&#39537;&#21160;&#30340;&#29992;&#25143;&#30740;&#31350;&#65306;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26377;&#29992;&#30340;&#25351;&#31034;&#24615;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online forums encourage the exchange and discussion of different stances on many topics. Not only do they provide an opportunity to present one's own arguments, but may also gather a broad cross-section of others' arguments. However, the resulting long discussions are difficult to overview. This paper presents a novel unsupervised approach using large language models (LLMs) to generating indicative summaries for long discussions that basically serve as tables of contents. Our approach first clusters argument sentences, generates cluster labels as abstractive summaries, and classifies the generated cluster labels into argumentation frames resulting in a two-level summary. Based on an extensively optimized prompt engineering approach, we evaluate 19~LLMs for generative cluster labeling and frame classification. To evaluate the usefulness of our indicative summaries, we conduct a purpose-driven user study via a new visual interface called Discussion Explorer: It shows that our proposed in
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24773;&#24863;&#20998;&#26512;&#30340;&#22810;LLM&#35848;&#21028;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#36807;&#31243;&#36798;&#21040;&#20849;&#35782;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;LLM&#26080;&#27861;&#20570;&#20986;&#23436;&#32654;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01876</link><description>&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#36890;&#36807;LLM&#35848;&#21028;
&lt;/p&gt;
&lt;p&gt;
Sentiment Analysis through LLM Negotiations. (arXiv:2311.01876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01876
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24773;&#24863;&#20998;&#26512;&#30340;&#22810;LLM&#35848;&#21028;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#36807;&#31243;&#36798;&#21040;&#20849;&#35782;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;LLM&#26080;&#27861;&#20570;&#20986;&#23436;&#32654;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#30340;&#26631;&#20934;&#33539;&#24335;&#26159;&#20381;&#36182;&#20110;&#19968;&#20010;&#21333;&#19968;&#30340;LLM&#65292;&#24182;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26694;&#26550;&#19979;&#22312;&#19968;&#36718;&#20013;&#20570;&#20986;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26694;&#26550;&#30340;&#20851;&#38190;&#21155;&#21183;&#22312;&#20110;&#21333;&#20010;LLM&#29983;&#25104;&#30340;&#21333;&#27425;&#36755;&#20986;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#23436;&#32654;&#30340;&#20915;&#31574;&#65292;&#23601;&#20687;&#20154;&#31867;&#26377;&#26102;&#38656;&#35201;&#22810;&#27425;&#23581;&#35797;&#25165;&#33021;&#20570;&#23545;&#19968;&#26679;&#12290;&#36825;&#22312;&#24773;&#24863;&#20998;&#26512;&#30340;&#20219;&#21153;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#22240;&#20026;&#38656;&#35201;&#28145;&#20837;&#25512;&#29702;&#26469;&#35299;&#20915;&#36755;&#20837;&#20013;&#30340;&#22797;&#26434;&#35821;&#35328;&#29616;&#35937;&#65288;&#22914;&#20174;&#21477;&#32452;&#21512;&#12289;&#35773;&#21050;&#31561;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;LLM&#35848;&#21028;&#26694;&#26550;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#24102;&#26377;&#25512;&#29702;&#30340;&#29983;&#25104;&#22120;&#26469;&#25552;&#20379;&#20915;&#31574;&#21644;&#35299;&#37322;&#65292;&#24182;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#22120;&#21487;&#20449;&#24230;&#30340;&#25512;&#23548;&#35299;&#37322;&#30340;&#21028;&#21035;&#22120;&#12290;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#36845;&#20195;&#30452;&#33267;&#36798;&#25104;&#20849;&#35782;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33258;&#28982;&#22320;&#35299;&#20915;&#20102;&#21069;&#36848;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25105;&#20204;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
A standard paradigm for sentiment analysis is to rely on a singular LLM and makes the decision in a single round under the framework of in-context learning. This framework suffers the key disadvantage that the single-turn output generated by a single LLM might not deliver the perfect decision, just as humans sometimes need multiple attempts to get things right. This is especially true for the task of sentiment analysis where deep reasoning is required to address the complex linguistic phenomenon (e.g., clause composition, irony, etc) in the input.  To address this issue, this paper introduces a multi-LLM negotiation framework for sentiment analysis. The framework consists of a reasoning-infused generator to provide decision along with rationale, a explanation-deriving discriminator to evaluate the credibility of the generator. The generator and the discriminator iterate until a consensus is reached. The proposed framework naturally addressed the aforementioned challenge, as we are able
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#25928;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#31070;&#32463;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21442;&#25968;&#35843;&#25972;&#21644;&#23383;&#31526;&#32423;&#21464;&#24322;&#31561;&#31574;&#30053;&#65292;&#21487;&#20197;&#20462;&#25913;GPT-3.5&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#20351;&#20854;&#23545;&#20154;&#31867;&#19981;&#21487;&#30097;&#20294;&#33021;&#23548;&#33268;&#31070;&#32463;&#25991;&#26412;&#26816;&#27979;&#22120;&#35823;&#21028;&#12290;</title><link>http://arxiv.org/abs/2311.01873</link><description>&lt;p&gt;
&#39640;&#25928;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#31070;&#32463;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Black-Box Adversarial Attacks on Neural Text Detectors. (arXiv:2311.01873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#25928;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#31070;&#32463;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21442;&#25968;&#35843;&#25972;&#21644;&#23383;&#31526;&#32423;&#21464;&#24322;&#31561;&#31574;&#30053;&#65292;&#21487;&#20197;&#20462;&#25913;GPT-3.5&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#20351;&#20854;&#23545;&#20154;&#31867;&#19981;&#21487;&#30097;&#20294;&#33021;&#23548;&#33268;&#31070;&#32463;&#25991;&#26412;&#26816;&#27979;&#22120;&#35823;&#21028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25991;&#26412;&#26816;&#27979;&#22120;&#26159;&#35757;&#32451;&#29992;&#20110;&#26816;&#27979;&#32473;&#23450;&#25991;&#26412;&#26159;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36824;&#26159;&#30001;&#20154;&#31867;&#32534;&#20889;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#31616;&#21333;&#32780;&#36164;&#28304;&#39640;&#25928;&#30340;&#31574;&#30053;&#65288;&#21442;&#25968;&#35843;&#25972;&#65292;&#25552;&#31034;&#24037;&#31243;&#21644;&#23383;&#31526;&#32423;&#21464;&#24322;&#65289;&#65292;&#29992;&#20110;&#20462;&#25913;&#30001;GPT-3.5&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#36825;&#20123;&#20462;&#25913;&#23545;&#20154;&#31867;&#26469;&#35828;&#19981;&#21487;&#30097;&#25110;&#19981;&#26131;&#23519;&#35273;&#65292;&#20294;&#20250;&#23548;&#33268;&#31070;&#32463;&#25991;&#26412;&#26816;&#27979;&#22120;&#35823;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29305;&#21035;&#26159;&#21442;&#25968;&#35843;&#25972;&#21644;&#23383;&#31526;&#32423;&#21464;&#24322;&#26159;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural text detectors are models trained to detect whether a given text was generated by a language model or written by a human. In this paper, we investigate three simple and resource-efficient strategies (parameter tweaking, prompt engineering, and character-level mutations) to alter texts generated by GPT-3.5 that are unsuspicious or unnoticeable for humans but cause misclassification by neural text detectors. The results show that especially parameter tweaking and character-level mutations are effective strategies.
&lt;/p&gt;</description></item><item><title>Multi-EuP&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27431;&#27954;&#35758;&#20250;&#30340;22K&#20010;&#22810;&#35821;&#35328;&#25991;&#26723;&#65292;&#26088;&#22312;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#20154;&#21475;&#20559;&#35265;&#12290;&#23427;&#25552;&#20379;&#20102;&#30495;&#23454;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#21644;&#36328;&#35821;&#35328;&#30456;&#20851;&#24615;&#35780;&#21028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#20016;&#23500;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21333;&#35821;&#21644;&#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2311.01870</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#27431;&#27954;&#35758;&#20250;&#25968;&#25454;&#38598;&#29992;&#20110;&#20998;&#26512;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval. (arXiv:2311.01870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01870
&lt;/p&gt;
&lt;p&gt;
Multi-EuP&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27431;&#27954;&#35758;&#20250;&#30340;22K&#20010;&#22810;&#35821;&#35328;&#25991;&#26723;&#65292;&#26088;&#22312;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#20154;&#21475;&#20559;&#35265;&#12290;&#23427;&#25552;&#20379;&#20102;&#30495;&#23454;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#21644;&#36328;&#35821;&#35328;&#30456;&#20851;&#24615;&#35780;&#21028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#20016;&#23500;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21333;&#35821;&#21644;&#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Multi-EuP&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#27431;&#27954;&#35758;&#20250;&#30340;22K&#20010;&#22810;&#35821;&#35328;&#25991;&#26723;&#65292;&#28085;&#30422;&#20102;24&#31181;&#35821;&#35328;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#30740;&#31350;&#22810;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#29615;&#22659;&#19979;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#20998;&#26512;&#22312;&#25490;&#21517;&#19978;&#30340;&#35821;&#35328;&#21644;&#20154;&#21475;&#20559;&#35265;&#12290;&#23427;&#25317;&#26377;&#19968;&#20010;&#30495;&#23454;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#30340;&#20027;&#39064;&#34987;&#32763;&#35793;&#25104;&#20102;&#25152;&#26377;24&#31181;&#35821;&#35328;&#65292;&#24182;&#25552;&#20379;&#36328;&#35821;&#35328;&#30456;&#20851;&#24615;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25552;&#20379;&#20102;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#20016;&#23500;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#20415;&#20110;&#30740;&#31350;&#20154;&#21475;&#20559;&#35265;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;Multi-EuP&#22312;&#21333;&#35821;&#21644;&#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;&#26631;&#35760;&#21270;&#31574;&#30053;&#36873;&#25321;&#24341;&#36215;&#30340;&#35821;&#35328;&#20559;&#35265;&#30340;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K multi-lingual documents collected from the European Parliament, spanning 24 languages. This dataset is designed to investigate fairness in a multilingual information retrieval (IR) context to analyze both language and demographic bias in a ranking context. It boasts an authentic multilingual corpus, featuring topics translated into all 24 languages, as well as cross-lingual relevance judgments. Furthermore, it offers rich demographic information associated with its documents, facilitating the study of demographic bias. We report the effectiveness of Multi-EuP for benchmarking both monolingual and multilingual IR. We also conduct a preliminary experiment on language bias caused by the choice of tokenization strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#24320;&#21457;&#27010;&#24565;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;LLMs&#25110;&#20351;&#29992;&#29616;&#26377;LLMs&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#26356;&#22909;&#22320;&#31526;&#21512;&#20154;&#31867;&#30452;&#35273;&#24182;&#25913;&#21892;&#20102;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01866</link><description>&lt;p&gt;
&#26397;&#30528;&#27010;&#24565;&#24863;&#30693;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Concept-Aware Large Language Models. (arXiv:2311.01866v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#24320;&#21457;&#27010;&#24565;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;LLMs&#25110;&#20351;&#29992;&#29616;&#26377;LLMs&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#26356;&#22909;&#22320;&#31526;&#21512;&#20154;&#31867;&#30452;&#35273;&#24182;&#25913;&#21892;&#20102;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#22312;&#21508;&#31181;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#23398;&#20064;&#12289;&#25512;&#29702;&#21644;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#36171;&#20104;&#26426;&#22120;&#24418;&#25104;&#21644;&#25512;&#29702;&#27010;&#24565;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#23588;&#20854;&#26159;&#65292;&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20027;&#35201;&#22312;&#35789;&#20803;&#32423;&#21035;&#19978;&#25805;&#20316;&#65292;&#32780;&#19981;&#26159;&#27010;&#24565;&#32423;&#21035;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24403;&#20195;LLMs&#23545;&#20154;&#31867;&#27010;&#24565;&#21450;&#20854;&#32467;&#26500;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#19981;&#21516;&#38454;&#27573;&#20013;&#24320;&#21457;&#27010;&#24565;&#24863;&#30693;LLMs&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#24565;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;LLMs&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#20351;&#29992;&#29616;&#26377;LLMs&#36755;&#20986;&#30340;&#26356;&#31616;&#21333;&#26041;&#27861;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#25105;&#20204;&#30340;&#27010;&#24565;&#39564;&#35777;&#35777;&#26126;&#20102;&#26356;&#22909;&#22320;&#21305;&#37197;&#20154;&#31867;&#30452;&#35273;&#65292;&#24182;&#25552;&#21319;&#20102;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#20102;&#27010;&#24565;&#24863;&#30693;LLMs&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts play a pivotal role in various human cognitive functions, including learning, reasoning and communication. However, there is very little work on endowing machines with the ability to form and reason with concepts. In particular, state-of-the-art large language models (LLMs) work at the level of tokens, not concepts.  In this work, we analyze how well contemporary LLMs capture human concepts and their structure. We then discuss ways to develop concept-aware LLMs, taking place at different stages of the pipeline. We sketch a method for pretraining LLMs using concepts, and also explore the simpler approach that uses the output of existing LLMs. Despite its simplicity, our proof-of-concept is shown to better match human intuition, as well as improve the robustness of predictions. These preliminary results underscore the promise of concept-aware LLMs.
&lt;/p&gt;</description></item><item><title>SortNet&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27604;&#36739;&#22120;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#25490;&#24207;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#26500;&#24314;&#35757;&#32451;&#38598;&#65292;&#26681;&#25454;&#25104;&#23545;&#39033;&#30446;&#20043;&#38388;&#30340;&#25490;&#24207;&#31034;&#20363;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2311.01864</link><description>&lt;p&gt;
SortNet: &#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25490;&#24207;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
SortNet: Learning To Rank By a Neural-Based Sorting Algorithm. (arXiv:2311.01864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01864
&lt;/p&gt;
&lt;p&gt;
SortNet&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27604;&#36739;&#22120;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#25490;&#24207;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#26500;&#24314;&#35757;&#32451;&#38598;&#65292;&#26681;&#25454;&#25104;&#23545;&#39033;&#30446;&#20043;&#38388;&#30340;&#25490;&#24207;&#31034;&#20363;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#30456;&#20851;&#24615;&#25490;&#21517;&#30340;&#38382;&#39064;&#65292;&#21363;&#26681;&#25454;&#32473;&#23450;&#30340;&#26631;&#20934;&#23545;&#19968;&#32452;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#12290;&#30001;&#20110;&#29992;&#25143;&#21487;&#33021;&#20559;&#22909;&#19981;&#21516;&#30340;&#30456;&#20851;&#24615;&#26631;&#20934;&#65292;&#22240;&#27492;&#25490;&#24207;&#31639;&#27861;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#36827;&#34892;&#35843;&#25972;&#12290;&#23398;&#20064;&#25490;&#24207;&#30340;&#20219;&#21153;&#22312;&#25991;&#29486;&#20013;&#23384;&#22312;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;1&#65289;&#36890;&#36807;&#31034;&#20363;&#23398;&#20064;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#35780;&#20272;&#27599;&#20010;&#23545;&#35937;&#30340;&#23646;&#24615;&#65292;&#29983;&#25104;&#21487;&#29992;&#20110;&#23545;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#30340;&#32477;&#23545;&#30456;&#20851;&#24615;&#20540;&#65307;2&#65289;&#19968;&#31181;&#25104;&#23545;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#35937;&#23545;&#26469;&#23398;&#20064;&#8220;&#20559;&#22909;&#20989;&#25968;&#8221;&#65292;&#23450;&#20041;&#21738;&#19968;&#20010;&#23545;&#35937;&#24212;&#35813;&#39318;&#20808;&#25490;&#21517;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SortNet&#65292;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27604;&#36739;&#22120;&#26469;&#23545;&#23545;&#35937;&#36827;&#34892;&#33258;&#36866;&#24212;&#25490;&#24207;&#30340;&#31639;&#27861;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38598;&#25552;&#20379;&#20102;&#23545;&#20110;&#25104;&#23545;&#39033;&#30446;&#20043;&#38388;&#25152;&#38656;&#25490;&#24207;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#26500;&#24314;&#65292;&#27599;&#27425;&#36845;&#20195;&#37117;&#20250;&#28155;&#21152;&#26368;&#20855;&#20449;&#24687;&#24615;&#30340;&#35757;&#32451;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#27604;&#36739;&#22120;&#37319;&#29992;&#20102;&#36830;&#25509;&#20027;&#20041;&#20307;&#31995;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of relevance ranking consists of sorting a set of objects with respect to a given criterion. Since users may prefer different relevance criteria, the ranking algorithms should be adaptable to the user needs. Two main approaches exist in literature for the task of learning to rank: 1) a score function, learned by examples, which evaluates the properties of each object yielding an absolute relevance value that can be used to order the objects or 2) a pairwise approach, where a "preference function" is learned using pairs of objects to define which one has to be ranked first. In this paper, we present SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. The neural network training set provides examples of the desired ordering between pairs of items and it is constructed by an iterative procedure which, at each iteration, adds the most informative training examples. Moreover, the comparator adopts a connectionist architecture that 
&lt;/p&gt;</description></item><item><title>$R^3$-NL2GQL&#26159;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#21644;&#36739;&#22823;&#30340;Foundation Models&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#12289;&#37325;&#20889;&#21644;&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#20943;&#36731;&#24187;&#35273;&#65292;&#35299;&#20915;&#20102;NL2GQL&#20219;&#21153;&#20013;GQL&#29983;&#25104;&#33021;&#21147;&#21644;&#36328;&#27169;&#24335;&#36890;&#29992;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01862</link><description>&lt;p&gt;
$R^3$-NL2GQL:&#19968;&#31181;&#29992;&#20110;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#20943;&#36731;&#24187;&#35273;&#30340;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
$R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation. (arXiv:2311.01862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01862
&lt;/p&gt;
&lt;p&gt;
$R^3$-NL2GQL&#26159;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#21644;&#36739;&#22823;&#30340;Foundation Models&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#12289;&#37325;&#20889;&#21644;&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#20943;&#36731;&#24187;&#35273;&#65292;&#35299;&#20915;&#20102;NL2GQL&#20219;&#21153;&#20013;GQL&#29983;&#25104;&#33021;&#21147;&#21644;&#36328;&#27169;&#24335;&#36890;&#29992;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20351;&#29992;Foundation Models&#26500;&#24314;&#30340;NL2SQL&#20219;&#21153;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#32467;&#26524;&#65292;&#28982;&#32780;&#30452;&#25509;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#21040;&#22270;&#26597;&#35810;&#35821;&#35328;&#65288;NL2GQL&#65289;&#20219;&#21153;&#38754;&#20020;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;GQL&#21644;SQL&#34920;&#36798;&#24335;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#19988;GQL&#23384;&#22312;&#22810;&#31181;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;NL2GQL&#20219;&#21153;&#20013;&#65292;&#26356;&#22823;&#30340;Foundation Models&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#36328;&#27169;&#24335;&#36890;&#29992;&#33021;&#21147;&#65292;&#32780;&#36739;&#23567;&#30340;Foundation Models&#21017;&#36890;&#36807;&#24494;&#35843;&#38590;&#20197;&#25552;&#39640;&#20854;GQL&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24847;&#22270;&#29702;&#35299;&#21644;&#26356;&#39640;&#30340;&#35821;&#27861;&#20934;&#30830;&#24615;&#12290;&#19982;&#22522;&#20110;&#35268;&#21017;&#21644;&#27133;&#22635;&#20805;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;R3-NL2GQL&#65292;&#35813;&#26041;&#27861;&#23558;&#36739;&#23567;&#21644;&#36739;&#22823;&#30340;Foundation Models&#29992;&#20316;&#37325;&#26032;&#25490;&#21517;&#12289;&#37325;&#20889;&#21644;&#32454;&#21270;&#22120;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36739;&#23567;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#20449;&#24687;&#30340;&#37325;&#26032;&#25490;&#21517;&#21644;&#37325;&#20889;&#65292;&#24182;&#21033;&#29992;&#21331;&#36234;&#30340;&#36890;&#29992;&#21270;&#21644;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While current NL2SQL tasks constructed using Foundation Models have achieved commendable results, their direct application to Natural Language to Graph Query Language (NL2GQL) tasks poses challenges due to the significant differences between GQL and SQL expressions, as well as the numerous types of GQL. Our extensive experiments reveal that in NL2GQL tasks, larger Foundation Models demonstrate superior cross-schema generalization abilities, while smaller Foundation Models struggle to improve their GQL generation capabilities through fine-tuning. However, after fine-tuning, smaller models exhibit better intent comprehension and higher grammatical accuracy. Diverging from rule-based and slot-filling techniques, we introduce R3-NL2GQL, which employs both smaller and larger Foundation Models as reranker, rewriter and refiner. The approach harnesses the comprehension ability of smaller models for information reranker and rewriter, and the exceptional generalization and generation capabiliti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#21487;&#25193;&#23637;&#30340;&#31867;&#27604;&#26144;&#23556;&#24341;&#25806;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#24120;&#35782;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#26469;&#30830;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#24341;&#25806;&#21487;&#20197;&#22788;&#29702;&#37096;&#20998;&#31867;&#27604;&#65292;&#24182;&#25552;&#20379;&#26032;&#23454;&#20307;&#30340;&#24314;&#35758;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#32463;&#20856;2x2&#31867;&#27604;&#38382;&#39064;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;81.2&#65285;&#65292;&#22312;&#26356;&#22823;&#30340;&#38382;&#39064;&#19978;&#20026;77.8&#65285;&#65292;&#27492;&#22806;&#65292;&#35813;&#24341;&#25806;&#36824;&#20248;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.01860</link><description>&lt;p&gt;
FAME&#65306;&#28789;&#27963;&#21487;&#25193;&#23637;&#30340;&#31867;&#27604;&#26144;&#23556;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
FAME: Flexible, Scalable Analogy Mappings Engine. (arXiv:2311.01860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01860
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#21487;&#25193;&#23637;&#30340;&#31867;&#27604;&#26144;&#23556;&#24341;&#25806;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#24120;&#35782;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#26469;&#30830;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#24341;&#25806;&#21487;&#20197;&#22788;&#29702;&#37096;&#20998;&#31867;&#27604;&#65292;&#24182;&#25552;&#20379;&#26032;&#23454;&#20307;&#30340;&#24314;&#35758;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#32463;&#20856;2x2&#31867;&#27604;&#38382;&#39064;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;81.2&#65285;&#65292;&#22312;&#26356;&#22823;&#30340;&#38382;&#39064;&#19978;&#20026;77.8&#65285;&#65292;&#27492;&#22806;&#65292;&#35813;&#24341;&#25806;&#36824;&#20248;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#26680;&#24515;&#33021;&#21147;&#20043;&#19968;&#65307;&#22312;&#38754;&#23545;&#26032;&#24773;&#22659;&#26102;&#65292;&#25105;&#20204;&#32463;&#24120;&#20174;&#20854;&#20182;&#39046;&#22495;&#20013;&#36716;&#31227;&#20808;&#21069;&#30340;&#32463;&#39564;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#35745;&#31639;&#31867;&#27604;&#30340;&#24037;&#20316;&#37117;&#20005;&#37325;&#20381;&#36182;&#22797;&#26434;&#30340;&#25163;&#24037;&#21046;&#20316;&#36755;&#20837;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25918;&#26494;&#20102;&#36755;&#20837;&#35201;&#27714;&#65292;&#21482;&#38656;&#23545;&#23454;&#20307;&#36827;&#34892;&#26144;&#23556;&#12290;&#25105;&#20204;&#33258;&#21160;&#25552;&#21462;&#24120;&#35782;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#30830;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#37096;&#20998;&#31867;&#27604;&#65292;&#24182;&#24314;&#35758;&#28155;&#21152;&#26032;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36755;&#20986;&#26131;&#20110;&#35299;&#37322;&#65292;&#29992;&#25143;&#21487;&#20197;&#29702;&#35299;&#20026;&#20160;&#20040;&#36873;&#25321;&#20102;&#29305;&#23450;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27491;&#30830;&#22320;&#26144;&#23556;&#20102;81.2&#65285;&#30340;&#32463;&#20856;2x2&#31867;&#27604;&#38382;&#39064;&#65288;&#29468;&#27979;&#27700;&#24179;=50&#65285;&#65289;&#12290;&#22312;&#26356;&#22823;&#30340;&#38382;&#39064;&#19978;&#65292;&#23427;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;77.8&#65285;&#65288;&#24179;&#22343;&#29468;&#27979;&#27700;&#24179;=13.1&#65285;&#65289;&#12290;&#22312;&#21478;&#19968;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#32988;&#36807;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#33258;&#21160;&#24314;&#35758;&#30340;&#26032;&#23454;&#20307;&#31867;&#20284;&#20110;&#20154;&#31867;&#24314;&#35758;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogy is one of the core capacities of human cognition; when faced with new situations, we often transfer prior experience from other domains. Most work on computational analogy relies heavily on complex, manually crafted input. In this work, we relax the input requirements, requiring only names of entities to be mapped. We automatically extract commonsense representations and use them to identify a mapping between the entities. Unlike previous works, our framework can handle partial analogies and suggest new entities to be added. Moreover, our method's output is easily interpretable, allowing for users to understand why a specific mapping was chosen.  Experiments show that our model correctly maps 81.2% of classical 2x2 analogy problems (guess level=50%). On larger problems, it achieves 77.8% accuracy (mean guess level=13.1%). In another experiment, we show our algorithm outperforms human performance, and the automatic suggestions of new entities resemble those suggested by humans. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#25903;&#25345;&#31185;&#23398;&#24037;&#20316;&#27969;&#26102;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#31185;&#23398;&#39046;&#22495;&#36827;&#34892;&#19977;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#39640;&#25928;&#35299;&#37322;&#24037;&#20316;&#27969;&#65292;&#20294;&#22312;&#32452;&#20214;&#20132;&#25442;&#21644;&#26377;&#30446;&#30340;&#24037;&#20316;&#26041;&#38754;&#30340;&#24615;&#33021;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2311.01825</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25327;&#25937;&#34892;&#21160;&#65306;&#20351;&#29992;ChatGPT&#20943;&#23569;&#31185;&#23398;&#24037;&#20316;&#27969;&#24320;&#21457;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT. (arXiv:2311.01825v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#25903;&#25345;&#31185;&#23398;&#24037;&#20316;&#27969;&#26102;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#31185;&#23398;&#39046;&#22495;&#36827;&#34892;&#19977;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#39640;&#25928;&#35299;&#37322;&#24037;&#20316;&#27969;&#65292;&#20294;&#22312;&#32452;&#20214;&#20132;&#25442;&#21644;&#26377;&#30446;&#30340;&#24037;&#20316;&#26041;&#38754;&#30340;&#24615;&#33021;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#24037;&#20316;&#27969;&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#34920;&#36798;&#21644;&#25191;&#34892;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#26512;&#27969;&#31243;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#22312;&#22823;&#22411;&#35745;&#31639;&#38598;&#32676;&#19978;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#25552;&#20379;&#20102;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#35768;&#22810;&#40657;&#30418;&#24037;&#20855;&#21644;&#24517;&#35201;&#30340;&#28145;&#23618;&#22522;&#30784;&#35774;&#26045;&#26632;&#65292;&#23454;&#29616;&#24037;&#20316;&#27969;&#38750;&#24120;&#22256;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29992;&#25143;&#25903;&#25345;&#24037;&#20855;&#24456;&#23569;&#65292;&#24182;&#19988;&#21487;&#29992;&#31034;&#20363;&#30340;&#25968;&#37327;&#36828;&#36828;&#20302;&#20110;&#20256;&#32479;&#32534;&#31243;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;ChatGPT&#65292;&#22312;&#22788;&#29702;&#31185;&#23398;&#24037;&#20316;&#27969;&#26102;&#25903;&#25345;&#29992;&#25143;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31185;&#23398;&#39046;&#22495;&#36827;&#34892;&#20102;&#19977;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;ChatGPT&#30340;&#29702;&#35299;&#12289;&#36866;&#24212;&#21644;&#25193;&#23637;&#24037;&#20316;&#27969;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#24037;&#20316;&#27969;&#65292;&#20294;&#22312;&#20132;&#25442;&#32452;&#20214;&#25110;&#26377;&#30446;&#30340;&#30340;&#24037;&#20316;&#26041;&#38754;&#24615;&#33021;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific workflow systems are increasingly popular for expressing and executing complex data analysis pipelines over large datasets, as they offer reproducibility, dependability, and scalability of analyses by automatic parallelization on large compute clusters. However, implementing workflows is difficult due to the involvement of many black-box tools and the deep infrastructure stack necessary for their execution. Simultaneously, user-supporting tools are rare, and the number of available examples is much lower than in classical programming languages. To address these challenges, we investigate the efficiency of Large Language Models (LLMs), specifically ChatGPT, to support users when dealing with scientific workflows. We performed three user studies in two scientific domains to evaluate ChatGPT for comprehending, adapting, and extending workflows. Our results indicate that LLMs efficiently interpret workflows but achieve lower performance for exchanging components or purposeful wo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#32534;&#20889;&#26497;&#31616;&#35821;&#27861;&#65292;&#24182;&#21033;&#29992;&#35768;&#21487;&#32773;/-&#34987;&#35768;&#21487;&#32773;&#26469;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#36991;&#20813;&#36807;&#24230;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2311.01820</link><description>&lt;p&gt;
&#26497;&#31616;&#35821;&#27861;&#65306;&#26080;&#36807;&#24230;&#29983;&#25104;&#30340;&#26500;&#36896;
&lt;/p&gt;
&lt;p&gt;
Minimalist Grammar: Construction without Overgeneration. (arXiv:2311.01820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#32534;&#20889;&#26497;&#31616;&#35821;&#27861;&#65292;&#24182;&#21033;&#29992;&#35768;&#21487;&#32773;/-&#34987;&#35768;&#21487;&#32773;&#26469;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#36991;&#20813;&#36807;&#24230;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#32534;&#20889;&#26497;&#31616;&#35821;&#27861;&#65288;MG&#65289;&#30340;&#25351;&#21335;&#12290;&#20026;&#20102;&#23558;&#25351;&#21335;&#21576;&#29616;&#20026;&#19968;&#31181;&#31639;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#30340;&#19968;&#31181;&#21464;&#20307;&#20316;&#20026;&#36755;&#20837;&#26684;&#24335;&#12290;&#22914;&#26524;CFG&#27809;&#26377;&#36882;&#24402;&#65292;&#21363;&#27809;&#26377;&#38750;&#32456;&#32467;&#31526;&#21487;&#20197;&#65288;&#38388;&#25509;&#65289;&#23548;&#20986;&#21253;&#21547;&#33258;&#36523;&#30340;&#21491;&#25163;&#36793;&#65292;&#21017;&#21487;&#20197;&#25490;&#38500;&#36807;&#24230;&#29983;&#25104;&#12290;&#26500;&#24314;&#30340;MG&#21033;&#29992;&#35768;&#21487;&#32773;/-&#34987;&#35768;&#21487;&#32773;&#20316;&#20026;&#29305;&#27530;&#30340;&#24322;&#24120;&#22788;&#29702;&#26041;&#24335;&#12290;&#22312;&#19968;&#20010;&#25512;&#23548;$A\_eats\_B\mapsto^* peter\_eats\_apples$&#30340;CFG&#26684;&#24335;&#20013;&#65292;&#20854;&#20013;$A$&#21644;$B$&#29983;&#25104;&#21517;&#35789;&#30701;&#35821;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#36807;&#24230;&#29983;&#25104;&#65292;&#20363;&#22914;$i\_eats\_apples$&#12290;&#20026;&#20102;&#36991;&#20813;&#36807;&#24230;&#29983;&#25104;&#65292;CFG&#38656;&#35201;&#35768;&#22810;&#38750;&#32456;&#32467;&#31526;&#21644;&#35268;&#21017;&#65292;&#20027;&#35201;&#20135;&#29983;&#30456;&#21516;&#30340;&#21333;&#35789;&#65292;&#20197;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#30340;MG&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20135;&#29983;&#30456;&#21516;&#21333;&#35789;&#30340;CFG&#35268;&#21017;&#24635;&#32467;&#20026;&#19968;&#20010;&#39033;&#30446;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#20998;&#37197;&#35768;&#21487;&#32773;/-&#34987;&#35768;&#21487;&#32773;&#26469;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#12290;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#30340;&#22256;&#38590;&#22312;&#20110;&#65292;&#22312;&#22823;&#22810;&#25968;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35768;&#21487;&#32773;/-&#34987;&#35768;&#21487;&#32773;&#26080;&#25928;
&lt;/p&gt;
&lt;p&gt;
In this paper we give instructions on how to write a minimalist grammar (MG). In order to present the instructions as an algorithm, we use a variant of context free grammars (CFG) as an input format. We can exclude overgeneration, if the CFG has no recursion, i.e. no non-terminal can (indirectly) derive to a right-hand side containing itself. The constructed MGs utilize licensors/-ees as a special way of exception handling. A CFG format for a derivation $A\_eats\_B\mapsto^* peter\_eats\_apples$, where $A$ and $B$ generate noun phrases, normally leads to overgeneration, e.\,g., $i\_eats\_apples$. In order to avoid overgeneration, a CFG would need many non-terminal symbols and rules, that mainly produce the same word, just to handle exceptions. In our MGs however, we can summarize CFG rules that produce the same word in one item and handle exceptions by a proper distribution of licensees/-ors. The difficulty with this technique is that in most generations the majority of licensees/-ors i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#36755;&#20837;&#30340;&#26497;&#21270;&#25991;&#31456;&#20043;&#38388;&#20943;&#23567;&#26497;&#24615;&#24046;&#24322;&#65292;&#20197;&#20943;&#36731;&#26694;&#26550;&#20559;&#35265;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#26694;&#26550;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#22312;&#35757;&#32451;&#27169;&#22411;&#20197;&#26368;&#23567;&#21270;&#20449;&#24687;&#32534;&#26694;&#20559;&#35265;&#30340;&#26497;&#24615;&#25439;&#22833;&#26102;&#12290;</title><link>http://arxiv.org/abs/2311.01817</link><description>&lt;p&gt;
&#20351;&#29992;&#26497;&#24615;&#26368;&#23567;&#21270;&#25439;&#22833;&#20943;&#36731;&#26694;&#26550;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Framing Bias with Polarity Minimization Loss. (arXiv:2311.01817v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01817
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#36755;&#20837;&#30340;&#26497;&#21270;&#25991;&#31456;&#20043;&#38388;&#20943;&#23567;&#26497;&#24615;&#24046;&#24322;&#65292;&#20197;&#20943;&#36731;&#26694;&#26550;&#20559;&#35265;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#26694;&#26550;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#22312;&#35757;&#32451;&#27169;&#22411;&#20197;&#26368;&#23567;&#21270;&#20449;&#24687;&#32534;&#26694;&#20559;&#35265;&#30340;&#26497;&#24615;&#25439;&#22833;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26694;&#26550;&#20559;&#35265;&#36890;&#36807;&#25197;&#26354;&#23454;&#38469;&#20107;&#20214;&#30340;&#24863;&#30693;&#65292;&#21152;&#21095;&#20102;&#25919;&#27835;&#26497;&#21270;&#12290;&#25345;&#26377;&#19981;&#21516;&#25919;&#27835;&#31435;&#22330;&#30340;&#23186;&#20307;&#26426;&#26500;&#24448;&#24448;&#22312;&#25253;&#36947;&#30456;&#21516;&#20107;&#20214;&#26102;&#20351;&#29992;&#26497;&#21270;&#35821;&#35328;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#40723;&#21169;&#27169;&#22411;&#22312;&#36755;&#20837;&#30340;&#26497;&#21270;&#25991;&#31456;&#20043;&#38388;&#20943;&#23567;&#26497;&#24615;&#24046;&#24322;&#65292;&#20197;&#20943;&#36731;&#26694;&#26550;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#26088;&#22312;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#20197;&#21452;&#21521;&#26144;&#23556;&#26497;&#24615;&#30340;&#20004;&#20010;&#26497;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24341;&#20837;&#25552;&#20986;&#30340;&#26497;&#24615;&#26368;&#23567;&#21270;&#25439;&#22833;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#26694;&#26550;&#20559;&#35265;&#65292;&#19982;&#22522;&#20110;BART&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#27169;&#22411;&#30456;&#27604;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27169;&#22411;&#34987;&#35757;&#32451;&#20197;&#26368;&#23567;&#21270;&#19982;&#20449;&#24687;&#32534;&#26694;&#20559;&#35265;&#65288;&#21363;&#36873;&#25321;&#24615;&#25253;&#36947;&#20449;&#24687;&#65289;&#30456;&#20851;&#30340;&#26497;&#24615;&#25439;&#22833;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#25928;&#26524;&#26368;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Framing bias plays a significant role in exacerbating political polarization by distorting the perception of actual events. Media outlets with divergent political stances often use polarized language in their reporting of the same event. We propose a new loss function that encourages the model to minimize the polarity difference between the polarized input articles to reduce framing bias. Specifically, our loss is designed to jointly optimize the model to map polarity ends bidirectionally. Our experimental results demonstrate that incorporating the proposed polarity minimization loss leads to a substantial reduction in framing bias when compared to a BART-based multi-document summarization model. Notably, we find that the effectiveness of this approach is most pronounced when the model is trained to minimize the polarity loss associated with informational framing bias (i.e., skewed selection of information to report).
&lt;/p&gt;</description></item><item><title>AFPQ&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#38750;&#23545;&#31216;&#28014;&#28857;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27491;&#20540;&#21644;&#36127;&#20540;&#35774;&#32622;&#19981;&#21516;&#30340;&#27604;&#20363;&#23610;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20854;&#20182;&#37327;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#39069;&#22806;&#23384;&#20648;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2311.01792</link><description>&lt;p&gt;
AFPQ&#65306;&#38754;&#21521;LLMs&#30340;&#38750;&#23545;&#31216;&#28014;&#28857;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
AFPQ: Asymmetric Floating Point Quantization for LLMs. (arXiv:2311.01792v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01792
&lt;/p&gt;
&lt;p&gt;
AFPQ&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#38750;&#23545;&#31216;&#28014;&#28857;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27491;&#20540;&#21644;&#36127;&#20540;&#35774;&#32622;&#19981;&#21516;&#30340;&#27604;&#20363;&#23610;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20854;&#20182;&#37327;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#39069;&#22806;&#23384;&#20648;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38754;&#20020;&#26377;&#38480;&#30340;&#20869;&#23384;&#23481;&#37327;&#21644;&#24102;&#23485;&#30340;&#37096;&#32626;&#25361;&#25112;&#12290;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#21487;&#20197;&#33410;&#30465;&#20869;&#23384;&#24182;&#21152;&#36895;&#25512;&#26029;&#12290;&#23613;&#31649;&#28014;&#28857;&#65288;FP&#65289;&#26684;&#24335;&#22312;LLM&#37327;&#21270;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#23567;&#32452;&#22823;&#23567;&#25110;&#23376;4&#20301;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20043;&#21069;&#30340;FP&#37327;&#21270;&#32570;&#20047;&#19981;&#23545;&#31216;&#24615;&#65292;&#19981;&#36866;&#21512;&#22788;&#29702;LLM&#26435;&#37325;&#24352;&#37327;&#30340;&#19981;&#23545;&#31216;&#20540;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#23545;&#31216;FP&#37327;&#21270;&#65288;AFPQ&#65289;&#65292;&#20026;&#27491;&#20540;&#21644;&#36127;&#20540;&#35774;&#32622;&#20102;&#20998;&#21035;&#30340;&#27604;&#20363;&#23610;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#22320;&#25554;&#20837;&#20854;&#20182;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;GPTQ&#21644;AWQ&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;&#38750;&#23545;&#31216;&#25972;&#25968;&#65288;INT&#65289;&#37327;&#21270;&#30456;&#27604;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23384;&#20648;&#31354;&#38388;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/zhangsichengsjtu/AFPQ&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show great performance in various tasks, but face deployment challenges from limited memory capacity and bandwidth. Low-bit weight quantization can save memory and accelerate inference. Although floating-point (FP) formats show good performance in LLM quantization, they tend to perform poorly with small group sizes or sub-4 bits. We find the reason is that the absence of asymmetry in previous FP quantization makes it unsuitable for handling asymmetric value distribution of LLM weight tensors. In this work, we propose asymmetric FP quantization (AFPQ), which sets separate scales for positive and negative values. Our method leads to large accuracy improvements and can be easily plugged into other quantization methods, including GPTQ and AWQ, for better performance. Besides, no additional storage is needed compared with asymmetric integer (INT) quantization. The code is available at https://github.com/zhangsichengsjtu/AFPQ.
&lt;/p&gt;</description></item><item><title>TCM-GPT&#26159;&#19968;&#31181;&#29992;&#20110;&#20256;&#32479;&#20013;&#21307;&#39046;&#22495;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20256;&#32479;&#20013;&#21307;&#19987;&#29992;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01786</link><description>&lt;p&gt;
TCM-GPT:&#29992;&#20110;&#20256;&#32479;&#20013;&#21307;&#39046;&#22495;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine. (arXiv:2311.01786v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01786
&lt;/p&gt;
&lt;p&gt;
TCM-GPT&#26159;&#19968;&#31181;&#29992;&#20110;&#20256;&#32479;&#20013;&#21307;&#39046;&#22495;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20256;&#32479;&#20013;&#21307;&#19987;&#29992;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24050;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#12290;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24182;&#20855;&#26377;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#20256;&#32479;&#20013;&#21307;&#39046;&#22495;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#36890;&#29992;&#27169;&#22411;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#24448;&#24448;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#12289;&#29420;&#29305;&#30446;&#26631;&#21644;&#35745;&#31639;&#25928;&#29575;&#31561;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#20256;&#32479;&#20013;&#21307;&#65289;&#30340;&#26377;&#25928;&#24615;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;TCMDA&#65288;&#20256;&#32479;&#20013;&#21307;&#39046;&#22495;&#36866;&#24212;&#65289;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35782;&#21035;&#39046;&#22495;&#20851;&#38190;&#35789;&#24182;&#20174;&#36890;&#29992;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#20256;&#32479;&#20013;&#21307;&#29305;&#23450;&#35821;&#26009;&#24211;TCM-Corpus-1B&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;TCMDA&#26041;&#27861;&#20351;&#29992;&#36825;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training and fine-tuning have emerged as a promising paradigm across various natural language processing (NLP) tasks. The effectiveness of pretrained large language models (LLM) has witnessed further enhancement, holding potential for applications in the field of medicine, particularly in the context of Traditional Chinese Medicine (TCM). However, the application of these general models to specific domains often yields suboptimal results, primarily due to challenges like lack of domain knowledge, unique objectives, and computational efficiency. Furthermore, their effectiveness in specialized domains, such as Traditional Chinese Medicine, requires comprehensive evaluation. To address the above issues, we propose a novel domain specific TCMDA (TCM Domain Adaptation) approach, efficient pre-training with domain-specific corpus. Specifically, we first construct a large TCM-specific corpus, TCM-Corpus-1B, by identifying domain keywords and retreving from general corpus. Then, our TCMDA 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UP4LS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#23646;&#24615;&#26500;&#24314;&#29992;&#25143;&#20449;&#24687;&#20197;&#22686;&#24378;&#35821;&#35328;&#38544;&#20889;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#38544;&#20889;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01775</link><description>&lt;p&gt;
UP4LS: &#30001;&#22810;&#20010;&#23646;&#24615;&#26500;&#24314;&#30340;&#29992;&#25143;&#20449;&#24687;&#29992;&#20110;&#22686;&#24378;&#35821;&#35328;&#38544;&#20889;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis. (arXiv:2311.01775v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UP4LS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#23646;&#24615;&#26500;&#24314;&#29992;&#25143;&#20449;&#24687;&#20197;&#22686;&#24378;&#35821;&#35328;&#38544;&#20889;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#38544;&#20889;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#38544;&#20889;&#20998;&#26512;&#20219;&#21153;&#26088;&#22312;&#26377;&#25928;&#26816;&#27979;&#36890;&#36807;&#35821;&#35328;&#38544;&#20889;&#26415;&#29983;&#25104;&#30340;&#38544;&#20889;&#29289;&#12290;&#29616;&#26377;&#30340;&#38544;&#20889;&#20998;&#26512;&#26041;&#27861;&#24573;&#35270;&#20102;&#29992;&#25143;&#20010;&#24615;&#21270;&#29305;&#24449;&#65292;&#23548;&#33268;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#38544;&#20889;&#29289;&#30340;&#26377;&#38480;&#20986;&#29616;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#26816;&#27979;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550; UP4LS&#65292;&#29992;&#20110;&#22686;&#24378;&#38544;&#20889;&#20998;&#26512;&#24615;&#33021;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#29992;&#25143;&#20449;&#24687;&#20026;&#26680;&#24515;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#24086;&#23376;&#20869;&#23481;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29992;&#25143;&#23646;&#24615;&#65292;&#22914;&#20889;&#20316;&#20064;&#24815;&#12289;&#24515;&#29702;&#29366;&#24577;&#21644;&#20851;&#27880;&#39046;&#22495;&#65292;&#20174;&#32780;&#20026;&#38544;&#20889;&#20998;&#26512;&#26500;&#24314;&#20102;&#29992;&#25143;&#20449;&#24687;&#12290;&#23545;&#20110;&#27599;&#20010;&#23646;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#12290;&#36890;&#36807;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#23558;&#25552;&#21462;&#21040;&#30340;&#29305;&#24449;&#26144;&#23556;&#21040;&#39640;&#32500;&#29992;&#25143;&#29305;&#24449;&#19978;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21462;&#20869;&#23481;&#29305;&#24449;&#12290;&#23558;&#29992;&#25143;&#21644;&#20869;&#23481;&#29305;&#24449;&#36827;&#34892;&#38598;&#25104;&#20197;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20248;&#20808;&#32771;&#34385;&#38544;&#20889;&#29289;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;UP4LS &#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#38544;&#20889;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linguistic steganalysis (LS) tasks aim to effectively detect stegos generated by linguistic steganography. Existing LS methods overlook the distinctive user characteristics, leading to weak performance in social networks. The limited occurrence of stegos further complicates detection. In this paper, we propose the UP4LS, a novel framework with the User Profile for enhancing LS performance. Specifically, by delving into post content, we explore user attributes like writing habits, psychological states, and focal areas, thereby building the user profile for LS. For each attribute, we design the identified feature extraction module. The extracted features are mapped to high-dimensional user features via deep-learning networks from existing methods. Then the language model is employed to extract content features. The user and content features are integrated to optimize feature representation. During the training phase, we prioritize the distribution of stegos. Experiments demonstrate that 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;PPTC&#22522;&#20934;&#65292;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26681;&#25454;&#29992;&#25143;&#25351;&#20196;&#21019;&#24314;&#21644;&#32534;&#36753;PPT&#25991;&#20214;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#27979;&#35797;&#65292;&#21457;&#29616;GPT-4&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#20026;75.1%&#12290;</title><link>http://arxiv.org/abs/2311.01767</link><description>&lt;p&gt;
PPTC&#22522;&#20934;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;PowerPoint&#20219;&#21153;&#23436;&#25104;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion. (arXiv:2311.01767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01767
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;PPTC&#22522;&#20934;&#65292;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26681;&#25454;&#29992;&#25143;&#25351;&#20196;&#21019;&#24314;&#21644;&#32534;&#36753;PPT&#25991;&#20214;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#27979;&#35797;&#65292;&#21457;&#29616;GPT-4&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#20026;75.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#27979;&#35797;&#23427;&#20204;&#23545;&#22522;&#26412;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#38646;&#27425;/&#23569;&#27425;&#23581;&#35797;&#33021;&#21147;&#20197;&#21450;&#23558;&#25351;&#20196;&#32763;&#35793;&#25104;&#24037;&#20855;API&#30340;&#33021;&#21147;&#19978;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21033;&#29992;&#22797;&#26434;&#24037;&#20855;&#23436;&#25104;&#22797;&#26434;&#22810;&#36718;&#12289;&#22810;&#27169;&#24577;&#25351;&#20196;&#30340;LLM&#30340;&#35780;&#20272;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PowerPoint&#20219;&#21153;&#23436;&#25104;&#65288;PPTC&#65289;&#22522;&#20934;&#65292;&#35780;&#20272;LLM&#26681;&#25454;&#29992;&#25143;&#25351;&#20196;&#21019;&#24314;&#21644;&#32534;&#36753;PPT&#25991;&#20214;&#30340;&#33021;&#21147;&#12290;&#23427;&#21253;&#21547;279&#20010;&#28085;&#30422;&#19981;&#21516;&#20027;&#39064;&#30340;&#22810;&#36718;&#23545;&#35805;&#65292;&#28041;&#21450;&#22810;&#27169;&#24577;&#25805;&#20316;&#30340;&#25968;&#30334;&#20010;&#25351;&#20196;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;PPTX-Match&#35780;&#20272;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#26681;&#25454;&#39044;&#27979;&#25991;&#20214;&#32780;&#19981;&#26159;&#26631;&#31614;API&#24207;&#21015;&#26469;&#35780;&#20272;LLM&#26159;&#21542;&#23436;&#25104;&#20102;&#25351;&#20196;&#65292;&#22240;&#27492;&#25903;&#25345;&#21508;&#31181;LLM&#29983;&#25104;&#30340;API&#24207;&#21015;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;3&#20010;&#38381;&#21512;&#22411;LLM&#21644;6&#20010;&#24320;&#28304;LLM&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;LLM&#65292;&#36798;&#21040;&#20102;75.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs' ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1\% accuracy i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#26469;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#35777;&#25454;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#29616;&#20851;&#31995;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01766</link><description>&lt;p&gt;
&#25903;&#25345;&#36824;&#26159;&#21453;&#39539;&#65306;&#20998;&#26512;&#35777;&#25454;&#31435;&#22330;&#20197;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#26469;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#35777;&#25454;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#29616;&#20851;&#31995;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35823;&#23548;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#22269;&#23478;&#32423;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#26159;&#21508;&#31181;&#22312;&#32447;&#20260;&#23475;&#30340;&#20027;&#35201;&#26469;&#28304;&#20043;&#19968;&#12290;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#35823;&#23548;&#20449;&#24687;&#24418;&#24335;&#26159;&#19978;&#19979;&#25991;&#38169;&#35823;&#65288;OOC&#65289;&#20449;&#24687;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#20449;&#24687;&#34987;&#38169;&#35823;&#22320;&#20851;&#32852;&#36215;&#26469;&#65292;&#20363;&#22914;&#30495;&#23454;&#22270;&#20687;&#19982;&#34394;&#20551;&#30340;&#25991;&#26412;&#26631;&#39064;&#25110;&#35823;&#23548;&#24615;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#22806;&#37096;&#35777;&#25454;&#26469;&#25269;&#24481;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#31435;&#22330;&#30340;&#19981;&#21516;&#35777;&#25454;&#30340;&#20316;&#29992;&#12290;&#21463;&#21040;&#35777;&#25454;&#31435;&#22330;&#20195;&#34920;&#19981;&#21516;&#26816;&#27979;&#32467;&#26524;&#30340;&#20559;&#35265;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#25552;&#21462;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#30340;&#20849;&#29616;&#20851;&#31995;&#35745;&#31639;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#21040;&#25991;&#26412;SEN&#20013;&#12290;&#23545;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the role of different pieces of evidence with different stances. Motivated by the intuition that the stance of evidence represents a bias towards different detection results, we propose a stance extraction network (SEN) that can extract the stances of different pieces of multi-modal evidence in a unified framework. Moreover, we introduce a support-refutation score calculated based on the co-occurrence relations of named entities into the textual SEN. Extensive experiments on a public large-scale data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#38024;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22810;&#20219;&#21153;&#29983;&#25104;&#24335;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;Indo LEGO-ABSA&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2311.01757</link><description>&lt;p&gt;
Indo LEGO-ABSA&#65306;&#19968;&#31181;&#38024;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#30340;&#22810;&#20219;&#21153;&#29983;&#25104;&#24335;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language. (arXiv:2311.01757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#38024;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22810;&#20219;&#21153;&#29983;&#25104;&#24335;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;Indo LEGO-ABSA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#26159;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35782;&#21035;&#21644;&#29702;&#35299;&#19982;&#23454;&#20307;&#30340;&#29305;&#23450;&#26041;&#38754;&#30456;&#20851;&#30340;&#24773;&#24863;&#12290;&#21069;&#26399;&#30740;&#31350;&#24050;&#32463;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#12290;LEGO-ABSA&#26159;&#19968;&#20010;&#25104;&#21151;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#22312;&#33521;&#25991;&#20013;&#12290;LEGO-ABSA&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#29615;&#22659;&#20013;&#23578;&#26410;&#24212;&#29992;&#35813;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#20013;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#25552;&#31034;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#24320;&#21457;&#20102;Indo LEGO-ABSA&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#19968;&#20010;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis is a method in natural language processing aimed at identifying and understanding sentiments related to specific aspects of an entity. Aspects are words or phrases that represent an aspect or attribute of a particular entity. Previous research has utilized generative pre-trained language models to perform aspect-based sentiment analysis. LEGO-ABSA is one framework that has successfully employed generative pre-trained language models in aspect-based sentiment analysis, particularly in English. LEGO-ABSA uses a multitask learning and prompting approach to enhance model performance. However, the application of this approach has not been done in the context of Bahasa Indonesia. Therefore, this research aims to implement the multitask learning and prompting approach in aspect-based sentiment analysis for Bahasa Indonesia using generative pre-trained language models. In this study, the Indo LEGO-ABSA model is developed, which is an aspect-based sentiment analy
&lt;/p&gt;</description></item><item><title>EmojiLM&#26159;&#19968;&#20010;&#19987;&#38376;&#22788;&#29702;&#25991;&#26412;-&#34920;&#24773;&#31526;&#21495;&#21452;&#21521;&#32763;&#35793;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;-&#34920;&#24773;&#31526;&#21495;&#24182;&#34892;&#35821;&#26009;&#24211;Text2Emoji&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#19982;&#34920;&#24773;&#31526;&#21495;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2311.01751</link><description>&lt;p&gt;
EmojiLM: &#23545;&#26032;&#30340;&#34920;&#24773;&#31526;&#21495;&#35821;&#35328;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
EmojiLM: Modeling the New Emoji Language. (arXiv:2311.01751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01751
&lt;/p&gt;
&lt;p&gt;
EmojiLM&#26159;&#19968;&#20010;&#19987;&#38376;&#22788;&#29702;&#25991;&#26412;-&#34920;&#24773;&#31526;&#21495;&#21452;&#21521;&#32763;&#35793;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;-&#34920;&#24773;&#31526;&#21495;&#24182;&#34892;&#35821;&#26009;&#24211;Text2Emoji&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#19982;&#34920;&#24773;&#31526;&#21495;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#36890;&#36807;&#20854;&#22810;&#26679;&#30340;&#20869;&#23481;&#36814;&#26469;&#20102;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#30340;&#20154;&#20204;&#12290;&#34920;&#24773;&#31526;&#21495;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#24471;&#30410;&#20110;&#34920;&#24773;&#31526;&#21495;&#22312;&#25991;&#21270;&#25110;&#35821;&#35328;&#36793;&#30028;&#20043;&#22806;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#34920;&#24773;&#31526;&#21495;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#21333;&#20010;&#34920;&#24773;&#31526;&#21495;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#26377;&#38480;&#30340;&#25968;&#25454;&#36164;&#28304;&#21487;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#26377;&#36259;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21512;&#25104;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;-&#34920;&#24773;&#31526;&#21495;&#24182;&#34892;&#35821;&#26009;&#24211;Text2Emoji&#12290;&#22522;&#20110;&#36825;&#20010;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;EmojiLM&#65292;&#19987;&#38376;&#29992;&#20110;&#25991;&#26412;-&#34920;&#24773;&#31526;&#21495;&#30340;&#21452;&#21521;&#32763;&#35793;&#12290;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#21644;&#20154;&#24037;&#35780;&#20272;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#24378;&#22522;&#20934;&#32447;&#65292;&#24182;&#19988;&#24182;&#34892;&#35821;&#26009;&#24211;&#23545;&#20110;&#19982;&#34920;&#24773;&#31526;&#21495;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of the internet, online social media welcomes people with different backgrounds through its diverse content. The increasing usage of emoji becomes a noticeable trend thanks to emoji's rich information beyond cultural or linguistic borders. However, the current study on emojis is limited to single emoji prediction and there are limited data resources available for further study of the interesting linguistic phenomenon. To this end, we synthesize a large text-emoji parallel corpus, Text2Emoji, from a large language model. Based on the parallel corpus, we distill a sequence-to-sequence model, EmojiLM, which is specialized in the text-emoji bidirectional translation. Extensive experiments on public benchmarks and human evaluation demonstrate that our proposed model outperforms strong baselines and the parallel corpus benefits emoji-related downstream tasks.
&lt;/p&gt;</description></item><item><title>SAC^3&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#24863;&#30693;&#20132;&#21449;&#26816;&#26597;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21152;&#20837;&#35821;&#20041;&#31561;&#25928;&#38382;&#39064;&#25200;&#21160;&#21644;&#36328;&#27169;&#22411;&#21709;&#24212;&#19968;&#33268;&#24615;&#26816;&#26597;&#31561;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#38382;&#39064;&#32423;&#21035;&#21644;&#27169;&#22411;&#32423;&#21035;&#30340;&#24187;&#35273;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;SAC^3&#22312;&#26816;&#27979;&#38750;&#20107;&#23454;&#21644;&#20107;&#23454;&#38472;&#36848;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26368;&#26032;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2311.01740</link><description>&lt;p&gt;
SAC^3: &#22522;&#20110;&#35821;&#20041;&#24863;&#30693;&#20132;&#21449;&#26816;&#26597;&#19968;&#33268;&#24615;&#30340;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#21487;&#38752;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SAC$^3$: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency. (arXiv:2311.01740v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01740
&lt;/p&gt;
&lt;p&gt;
SAC^3&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#24863;&#30693;&#20132;&#21449;&#26816;&#26597;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21152;&#20837;&#35821;&#20041;&#31561;&#25928;&#38382;&#39064;&#25200;&#21160;&#21644;&#36328;&#27169;&#22411;&#21709;&#24212;&#19968;&#33268;&#24615;&#26816;&#26597;&#31561;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#38382;&#39064;&#32423;&#21035;&#21644;&#27169;&#22411;&#32423;&#21035;&#30340;&#24187;&#35273;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;SAC^3&#22312;&#26816;&#27979;&#38750;&#20107;&#23454;&#21644;&#20107;&#23454;&#38472;&#36848;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#26816;&#27979;&#26159;&#20102;&#35299;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33258;&#19968;&#33268;&#24615;&#30340;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#20004;&#31181;&#24187;&#35273;&#31867;&#22411;&#65292;&#21363;&#22522;&#20110;&#38382;&#39064;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24187;&#35273;&#65292;&#23427;&#20204;&#20165;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#26816;&#26597;&#26080;&#27861;&#26377;&#25928;&#35782;&#21035;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21363;&#35821;&#20041;&#24863;&#30693;&#20132;&#21449;&#26816;&#26597;&#19968;&#33268;&#24615;&#65288;SAC^3&#65289;&#65292;&#23427;&#22312;&#33258;&#19968;&#33268;&#24615;&#26816;&#26597;&#21407;&#21017;&#30340;&#22522;&#30784;&#19978;&#21152;&#20837;&#20102;&#39069;&#22806;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#31561;&#25928;&#38382;&#39064;&#25200;&#21160;&#21644;&#36328;&#27169;&#22411;&#21709;&#24212;&#19968;&#33268;&#24615;&#26816;&#26597;&#26469;&#26816;&#27979;&#38382;&#39064;&#32423;&#21035;&#21644;&#27169;&#22411;&#32423;&#21035;&#30340;&#24187;&#35273;&#12290;&#36890;&#36807;&#24191;&#27867;&#32780;&#31995;&#32479;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;SAC^3&#22312;&#26816;&#27979;&#38750;&#20107;&#23454;&#21644;&#20107;&#23454;&#38472;&#36848;&#26041;&#38754;&#32988;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC$^3$) that expands on the principle of self-consistency checking. Our SAC$^3$ approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC$^3$ outperforms the state of the art in detecting both non-factual and factual statements across multip
&lt;/p&gt;</description></item><item><title>Proto-lm&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20869;&#32622;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#38454;&#27573;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#23884;&#20837;&#26469;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20026;&#21019;&#24314;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01732</link><description>&lt;p&gt;
Proto-lm&#65306;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#32622;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models. (arXiv:2311.01732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01732
&lt;/p&gt;
&lt;p&gt;
Proto-lm&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20869;&#32622;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#38454;&#27573;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#23884;&#20837;&#26469;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20026;&#21019;&#24314;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#26377;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20854;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#30446;&#21069;&#29992;&#20110;&#35299;&#37322;LLMs&#30340;&#26041;&#27861;&#26159;&#20107;&#21518;&#30340;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#20043;&#21518;&#24212;&#29992;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#27604;&#22914;&#23427;&#20204;&#20851;&#27880;&#20302;&#32423;&#29305;&#24449;&#24182;&#19988;&#22312;&#26356;&#39640;&#32423;&#25991;&#26412;&#21333;&#20301;&#19978;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;proto-lm&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#30333;&#30418;&#23376;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#22312;&#24494;&#35843;&#38454;&#27573;&#23398;&#20064;&#21363;&#26102;&#21487;&#35299;&#37322;&#30340;&#23884;&#20837;&#65292;&#21516;&#26102;&#20445;&#25345;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;&#36825;&#31181;&#22312;LLMs&#20013;&#30340;&#26032;&#39062;&#35299;&#37322;&#24615;&#26041;&#27861;&#21487;&#20197;&#20026;&#26080;&#38656;&#29306;&#29298;&#24615;&#33021;&#30340;&#26356;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higher level text units. In this work, we introduce proto-lm, a prototypical network-based white-box framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance. Our method's applicability and interpretability are demonstrated through experiments on a wide range of NLP tasks, and our results indicate a new possibility of creating interpretable models without sacrificing performance. This novel approach to interpretability in LLMs can pave the way for more interpretable models without the need to sacrifice performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#20013;&#22269;ASQP&#25968;&#25454;&#38598;&#65292;&#23545;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#27169;&#22411;&#22312;ASQP&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25913;&#36827;ASQP&#25216;&#26415;&#21644;&#25552;&#39640;GPT&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01713</link><description>&lt;p&gt;
&#23545;&#20013;&#22269;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction. (arXiv:2311.01713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#20013;&#22269;ASQP&#25968;&#25454;&#38598;&#65292;&#23545;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#27169;&#22411;&#22312;ASQP&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25913;&#36827;ASQP&#25216;&#26415;&#21644;&#25552;&#39640;GPT&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;ASQP&#65289;&#26159;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#30340;&#19968;&#20010;&#20851;&#38190;&#23376;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;ASQP&#25968;&#25454;&#38598;&#29305;&#28857;&#26159;&#35268;&#27169;&#23567;&#19988;&#22235;&#20803;&#32452;&#23494;&#24230;&#20302;&#65292;&#36825;&#38459;&#30861;&#20102;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#25193;&#22823;&#23481;&#37327;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#20013;&#22269;ASQP&#25968;&#25454;&#38598;&#65292;&#20174;&#22810;&#20010;&#22312;&#32447;&#24179;&#21488;&#25910;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#20960;&#20010;&#26174;&#33879;&#30340;&#29305;&#28857;&#65306;&#26356;&#22823;&#30340;&#35268;&#27169;&#65288;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#26377;10,000+&#20010;&#26679;&#26412;&#65289;&#65292;&#20016;&#23500;&#30340;&#26041;&#38754;&#31867;&#21035;&#65292;&#27599;&#20010;&#21477;&#23376;&#26356;&#22810;&#30340;&#35789;&#25968;&#20197;&#21450;&#27604;&#29616;&#26377;ASQP&#25968;&#25454;&#38598;&#26356;&#39640;&#30340;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#27425;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#27169;&#22411;&#22312;ASQP&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;ASQP&#22522;&#20934;&#32447;&#23454;&#39564;&#24378;&#35843;&#20102;&#38656;&#35201;&#25506;&#32034;&#39069;&#22806;&#25216;&#26415;&#26469;&#35299;&#20915;ASQP&#30340;&#38656;&#27714;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#25913;&#36827;GPT&#24615;&#33021;&#30340;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect sentiment quad prediction (ASQP) is a critical subtask of aspect-level sentiment analysis. Current ASQP datasets are characterized by their small size and low quadruple density, which hinders technical development. To expand capacity, we construct two large Chinese ASQP datasets crawled from multiple online platforms. The datasets hold several significant characteristics: larger size (each with 10,000+ samples) and rich aspect categories, more words per sentence, and higher density than existing ASQP datasets. Moreover, we are the first to evaluate the performance of Generative Pre-trained Transformer (GPT) series models on ASQP and exhibit potential issues. The experiments with state-of-the-art ASQP baselines underscore the need to explore additional techniques to address ASQP, as well as the importance of further investigation into methods to improve the performance of GPTs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#38889;&#22269;&#22312;&#32447;&#25253;&#32440;&#20013;&#25919;&#27835;&#24847;&#22270;&#30340;&#26032;&#30340;&#38889;&#25991;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;12000&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#35268;&#27169;&#30340;&#38889;&#22269;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#22810;&#20219;&#21153;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01712</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22312;&#32447;&#26032;&#38395;&#20013;&#25919;&#27835;&#24847;&#22270;&#30340;&#26032;&#38889;&#25991;&#25991;&#26412;&#20998;&#31867;&#22522;&#20934;&#65288;arXiv:2311.01712v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
A New Korean Text Classification Benchmark for Recognizing the Political Intents in Online Newspapers. (arXiv:2311.01712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01712
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#38889;&#22269;&#22312;&#32447;&#25253;&#32440;&#20013;&#25919;&#27835;&#24847;&#22270;&#30340;&#26032;&#30340;&#38889;&#25991;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;12000&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#35268;&#27169;&#30340;&#38889;&#22269;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#22810;&#20219;&#21153;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29992;&#25143;&#22312;&#21508;&#31181;&#26434;&#24535;&#19978;&#38405;&#35835;&#22312;&#32447;&#25991;&#31456;&#26102;&#65292;&#21487;&#33021;&#20250;&#22312;&#21306;&#20998;&#25991;&#26412;&#20013;&#30340;&#38544;&#21547;&#24847;&#22270;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35299;&#25991;&#26412;&#30340;&#35821;&#22659;&#65292;&#19987;&#27880;&#20110;&#33258;&#21160;&#35782;&#21035;&#32473;&#23450;&#22312;&#32447;&#25253;&#32440;&#30340;&#25919;&#27835;&#24847;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#25991;&#31456;&#30340;&#26032;&#39062;&#30340;&#38889;&#25991;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#20998;&#31867;&#22522;&#20934;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;&#38889;&#22269;&#20845;&#23478;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25253;&#32440;&#32452;&#32455;&#25919;&#27835;&#26639;&#30446;&#30340;12000&#31687;&#26032;&#38395;&#25991;&#31456;&#65292;&#36825;&#20123;&#25991;&#31456;&#21487;&#33021;&#21253;&#21547;&#26377;&#25919;&#27835;&#24847;&#22270;&#12290;&#25152;&#26377;&#30340;&#25991;&#26412;&#26679;&#26412;&#21516;&#26102;&#20197;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#26631;&#35760;&#65288;1&#65289;&#25919;&#27835;&#20542;&#21521;&#30340;&#27700;&#24179;&#21644;&#65288;2&#65289;&#20146;&#25919;&#24220;&#30340;&#27700;&#24179;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26159;&#21253;&#21547;&#38271;&#25991;&#26412;&#24182;&#35299;&#20915;&#22810;&#20219;&#21153;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#22823;&#35268;&#27169;&#30340;&#38889;&#22269;&#26032;&#38395;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#26368;&#36817;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many users reading online articles in various magazines may suffer considerable difficulty in distinguishing the implicit intents in texts. In this work, we focus on automatically recognizing the political intents of a given online newspaper by understanding the context of the text. To solve this task, we present a novel Korean text classification dataset that contains various articles. We also provide deep-learning-based text classification baseline models trained on the proposed dataset. Our dataset contains 12,000 news articles that may contain political intentions, from the politics section of six of the most representative newspaper organizations in South Korea. All the text samples are labeled simultaneously in two aspects (1) the level of political orientation and (2) the level of pro-government. To the best of our knowledge, our paper is the most large-scale Korean news dataset that contains long text and addresses multi-task classification problems. We also train recent state-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#65288;DFKD&#65289;&#26694;&#26550;&#65292;&#21363;DFKD-T$^{3}$&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23558;&#36890;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#21387;&#32553;&#21451;&#22909;&#30340;&#20219;&#21153;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#33976;&#39311;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01689</link><description>&lt;p&gt;
&#26080;&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#65306;&#36890;&#36807;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Data-Free Distillation of Language Model by Text-to-Text Transfer. (arXiv:2311.01689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#65288;DFKD&#65289;&#26694;&#26550;&#65292;&#21363;DFKD-T$^{3}$&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23558;&#36890;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#21387;&#32553;&#21451;&#22909;&#30340;&#20219;&#21153;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#33976;&#39311;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#65288;DFKD&#65289;&#22312;&#21387;&#32553;&#27169;&#22411;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20808;&#21069;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23545;DFKD&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#31867;&#21035;&#20219;&#21153;&#36827;&#34892;&#33976;&#39311;&#30340;&#20165;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#24573;&#35270;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DFKD&#26694;&#26550;&#65292;&#21517;&#20026;DFKD-T$^{3}$&#65292;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#25511;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;&#12290;&#36825;&#20010;&#26032;&#39062;&#30340;DFKD-T$^{3}$&#26694;&#26550;&#23548;&#33268;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#26694;&#26550;&#65292;&#23558;&#36890;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#21387;&#32553;&#21451;&#22909;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#29305;&#24322;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21319;&#22312;&#24773;&#24863;&#20998;&#26512;&#12289;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#33976;&#39311;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the model when original training data is unavailable. Previous works for DFKD in NLP mainly focus on distilling encoder-only structures like BERT on classification tasks, which overlook the notable progress of generative language modeling. In this work, we propose a novel DFKD framework, namely DFKD-T$^{3}$, where the pretrained generative language model can also serve as a controllable data generator for model compression. This novel framework DFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to transform the general domain corpus to compression-friendly task data, targeting to improve both the \textit{specificity} and \textit{diversity}. Extensive experiments show that our method can boost the distillation performance in various downstream tasks such as sentiment analysis, linguistic acceptability, and information extraction. Furthermore, we show that the generated texts can be directly used 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#25193;&#23637;&#31572;&#26696;&#31354;&#38388;&#21644;&#24120;&#35782;&#22686;&#24378;&#35780;&#20998;&#26426;&#21046;&#65288;CASE&#65289;&#65292;&#23427;&#36890;&#36807;&#26681;&#25454;&#36755;&#20837;&#20013;&#21333;&#35789;&#30340;&#35821;&#20041;&#20851;&#31995;&#20998;&#37197;&#37325;&#35201;&#24615;&#26435;&#37325;&#26469;&#25913;&#21892;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#35780;&#20998;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#29983;&#25104;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110;&#36873;&#39033;&#30340;&#35789;&#27719;&#19981;&#21516;&#30340;&#31572;&#26696;&#26469;&#36827;&#19968;&#27493;&#25193;&#23637;&#31572;&#26696;&#31354;&#38388;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#38477;&#20302;&#22122;&#22768;&#21644;&#25552;&#20379;&#38544;&#21547;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01684</link><description>&lt;p&gt;
CASE: &#24102;&#26377;&#25193;&#23637;&#31572;&#26696;&#31354;&#38388;&#30340;&#24120;&#35782;&#22686;&#24378;&#35780;&#20998;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
CASE: Commonsense-Augmented Score with an Expanded Answer Space. (arXiv:2311.01684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01684
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#25193;&#23637;&#31572;&#26696;&#31354;&#38388;&#21644;&#24120;&#35782;&#22686;&#24378;&#35780;&#20998;&#26426;&#21046;&#65288;CASE&#65289;&#65292;&#23427;&#36890;&#36807;&#26681;&#25454;&#36755;&#20837;&#20013;&#21333;&#35789;&#30340;&#35821;&#20041;&#20851;&#31995;&#20998;&#37197;&#37325;&#35201;&#24615;&#26435;&#37325;&#26469;&#25913;&#21892;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#35780;&#20998;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#29983;&#25104;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110;&#36873;&#39033;&#30340;&#35789;&#27719;&#19981;&#21516;&#30340;&#31572;&#26696;&#26469;&#36827;&#19968;&#27493;&#25193;&#23637;&#31572;&#26696;&#31354;&#38388;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#38477;&#20302;&#22122;&#22768;&#21644;&#25552;&#20379;&#38544;&#21547;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#35757;&#32451;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;LM&#27010;&#29575;&#34987;&#29992;&#20316;&#27599;&#20010;&#31572;&#26696;&#36873;&#25321;&#30340;&#21512;&#29702;&#24615;&#30340;&#19981;&#23436;&#32654;&#24230;&#37327;&#12290;&#22522;&#26412;&#35780;&#20998;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#23558;&#25152;&#26377;&#21333;&#35789;&#37117;&#35270;&#20026;&#21516;&#31561;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#25193;&#23637;&#31572;&#26696;&#31354;&#38388;&#30340;&#24120;&#35782;&#22686;&#24378;&#35780;&#20998;&#26426;&#21046;&#65288;CASE&#65289;&#65292;&#36890;&#36807;&#26681;&#25454;&#36755;&#20837;&#20013;&#21333;&#35789;&#19982;&#20854;&#20182;&#21333;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#20998;&#37197;&#37325;&#35201;&#24615;&#26435;&#37325;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#21160;&#24577;&#21152;&#26435;&#26041;&#27861;&#19981;&#20165;&#38477;&#20302;&#20102;&#19981;&#37325;&#35201;&#21333;&#35789;&#30340;&#22122;&#22768;&#65292;&#36824;&#21521;&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#33021;&#23545;&#22238;&#31572;&#38382;&#39064;&#26377;&#29992;&#30340;&#38544;&#21547;&#24120;&#35782;&#30693;&#35782;&#12290;&#25105;&#20204;&#36824;&#22312;&#25193;&#23637;&#31572;&#26696;&#31354;&#38388;&#26041;&#38754;&#36981;&#24490;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#29983;&#25104;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110;&#36873;&#39033;&#30340;&#35789;&#27719;&#19981;&#21516;&#30340;&#31572;&#26696;&#12290;&#24403;&#19982;&#31572;&#26696;&#31354;&#38388;&#25193;&#23637;&#30456;&#32467;&#21512;&#26102;&#65292;&#33021;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have demonstrated impressive zero-shot performance on NLP tasks thanks to the knowledge they acquired in their training. In multiple-choice QA tasks, the LM probabilities are used as an imperfect measure of the plausibility of each answer choice. One of the major limitations of the basic score is that it treats all words as equally important. We propose CASE, a Commonsense-Augmented Score with an Expanded Answer Space. CASE addresses this limitation by assigning importance weights for individual words based on their semantic relations to other words in the input. The dynamic weighting approach outperforms basic LM scores, not only because it reduces noise from unimportant words, but also because it informs the model of implicit commonsense knowledge that may be useful for answering the question. We then also follow prior work in expanding the answer space by generating lexically-divergent answers that are conceptually-similar to the choices. When combined with answer space expansi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DialogBench&#65292;&#19968;&#20010;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;28&#20010;LLMs&#30340;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#23545;&#25552;&#21319;&#24615;&#33021;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2311.01677</link><description>&lt;p&gt;
DialogBench: &#23558;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DialogBench: Evaluating LLMs as Human-like Dialogue Systems. (arXiv:2311.01677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DialogBench&#65292;&#19968;&#20010;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;28&#20010;LLMs&#30340;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#23545;&#25552;&#21319;&#24615;&#33021;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26032;&#30340;&#23545;&#35805;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#21047;&#26032;&#20102;&#20154;&#20204;&#23545;&#23545;&#35805;&#31995;&#32479;&#30340;&#21360;&#35937;&#12290;&#23545;&#35805;&#31995;&#32479;&#38271;&#26399;&#20197;&#26469;&#30340;&#30446;&#26631;&#26159;&#36275;&#22815;&#20687;&#20154;&#31867;&#65292;&#20197;&#20415;&#36890;&#36807;&#28385;&#36275;&#20132;&#27969;&#12289;&#24773;&#24863;&#21644;&#31038;&#20132;&#24402;&#23646;&#30340;&#38656;&#35201;&#19982;&#29992;&#25143;&#24314;&#31435;&#38271;&#26399;&#32852;&#31995;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DialogBench&#65292;&#19968;&#20010;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#65292;&#30446;&#21069;&#21253;&#21547;12&#20010;&#23545;&#35805;&#20219;&#21153;&#65292;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#24212;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#29983;&#25104;&#27599;&#20010;&#20219;&#21153;&#30340;&#35780;&#20272;&#23454;&#20363;&#12290;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#24191;&#27867;&#20351;&#29992;&#30340;&#35774;&#35745;&#21407;&#21017;&#35774;&#35745;&#22522;&#26412;&#25552;&#31034;&#65292;&#24182;&#36827;&#19968;&#27493;&#20943;&#36731;&#29616;&#26377;&#30340;&#20559;&#35265;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#35780;&#20272;&#23454;&#20363;&#12290;&#25105;&#20204;&#23545;28&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65288;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#25351;&#23548;&#35843;&#20248;&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#25351;&#23548;&#24494;&#35843;&#25928;&#30410;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities, refreshing human's impressions on dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users by satisfying the need for communication, affection and social belonging. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that currently contains $12$ dialogue tasks to assess the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely-used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive test over $28$ LLMs (including pre-trained and supervised instruction-tuning) shows that instruction fine-tuning benefits 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#24773;&#33410;&#26816;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#26597;&#35810;&#21644;&#20505;&#36873;&#24773;&#33410;&#20043;&#38388;&#30340;&#25277;&#35937;&#35821;&#20041;&#20851;&#32852;&#24230;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20381;&#36182;&#20110;&#35789;&#27719;&#25110;&#35821;&#20041;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2311.01666</link><description>&lt;p&gt;
&#23558;&#24773;&#33410;&#26816;&#32034;&#20316;&#20026;&#25277;&#35937;&#35821;&#20041;&#20851;&#32852;&#24230;&#35780;&#20272;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Plot Retrieval as an Assessment of Abstract Semantic Association. (arXiv:2311.01666v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#24773;&#33410;&#26816;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#26597;&#35810;&#21644;&#20505;&#36873;&#24773;&#33410;&#20043;&#38388;&#30340;&#25277;&#35937;&#35821;&#20041;&#20851;&#32852;&#24230;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20381;&#36182;&#20110;&#35789;&#27719;&#25110;&#35821;&#20041;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20070;&#31821;&#20013;&#26816;&#32034;&#30456;&#20851;&#24773;&#33410;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#21487;&#20197;&#25552;&#39640;&#35835;&#32773;&#30340;&#38405;&#35835;&#20307;&#39564;&#21644;&#25928;&#29575;&#12290;&#35835;&#32773;&#36890;&#24120;&#21482;&#25552;&#20379;&#19968;&#20010;&#22522;&#20110;&#33258;&#24049;&#29702;&#35299;&#12289;&#25688;&#35201;&#25110;&#29468;&#27979;&#30340;&#25277;&#35937;&#21644;&#27169;&#31946;&#30340;&#25551;&#36848;&#20316;&#20026;&#26597;&#35810;&#65292;&#36825;&#35201;&#27714;&#26816;&#32034;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#26469;&#20272;&#35745;&#26597;&#35810;&#21644;&#20505;&#36873;&#24773;&#33410;&#20043;&#38388;&#30340;&#25277;&#35937;&#35821;&#20041;&#20851;&#32852;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20449;&#24687;&#26816;&#32034;&#25968;&#25454;&#38598;&#19981;&#33021;&#24456;&#22909;&#22320;&#21453;&#26144;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24773;&#33410;&#26816;&#32034;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;IR&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#24773;&#33410;&#26816;&#32034;&#19978;&#30340;&#24615;&#33021;&#12290;&#24773;&#33410;&#26816;&#32034;&#20013;&#30340;&#25991;&#26412;&#23545;&#20855;&#26377;&#36739;&#23569;&#30340;&#35789;&#37325;&#21472;&#21644;&#26356;&#22810;&#30340;&#25277;&#35937;&#35821;&#20041;&#20851;&#32852;&#24230;&#65292;&#21487;&#20197;&#21453;&#26144;IR&#27169;&#22411;&#20272;&#35745;&#25277;&#35937;&#35821;&#20041;&#20851;&#32852;&#24230;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20256;&#32479;&#30340;&#35789;&#27719;&#25110;&#35821;&#20041;&#21305;&#37197;&#12290;&#36890;&#36807;&#21508;&#31181;&#35789;&#27719;&#26816;&#32034;&#12289;&#31232;&#30095;&#26816;&#32034;&#21644;&#23494;&#38598;&#26816;&#32034;&#30340;&#22823;&#37327;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Retrieving relevant plots from the book for a query is a critical task, which can improve the reading experience and efficiency of readers. Readers usually only give an abstract and vague description as the query based on their own understanding, summaries, or speculations of the plot, which requires the retrieval model to have a strong ability to estimate the abstract semantic associations between the query and candidate plots. However, existing information retrieval (IR) datasets cannot reflect this ability well. In this paper, we propose Plot Retrieval, a labeled dataset to train and evaluate the performance of IR models on the novel task Plot Retrieval. Text pairs in Plot Retrieval have less word overlap and more abstract semantic association, which can reflect the ability of the IR models to estimate the abstract semantic association, rather than just traditional lexical or semantic matching. Extensive experiments across various lexical retrieval, sparse retrieval, dense retrieval
&lt;/p&gt;</description></item><item><title>MARRS&#26159;&#19968;&#20010;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#22810;&#27169;&#24577;&#21442;&#32771;&#35299;&#26512;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#23545;&#35805;&#24335;&#12289;&#35270;&#35273;&#21644;&#32972;&#26223;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#19978;&#19979;&#25991;&#26597;&#35810;&#30340;&#22788;&#29702;&#12290;&#36825;&#20010;&#31995;&#32479;&#33021;&#22815;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#29702;&#35299;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2311.01650</link><description>&lt;p&gt;
MARRS: &#22810;&#27169;&#24577;&#21442;&#32771;&#35299;&#26512;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MARRS: Multimodal Reference Resolution System. (arXiv:2311.01650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01650
&lt;/p&gt;
&lt;p&gt;
MARRS&#26159;&#19968;&#20010;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#22810;&#27169;&#24577;&#21442;&#32771;&#35299;&#26512;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#23545;&#35805;&#24335;&#12289;&#35270;&#35273;&#21644;&#32972;&#26223;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#19978;&#19979;&#25991;&#26597;&#35810;&#30340;&#22788;&#29702;&#12290;&#36825;&#20010;&#31995;&#32479;&#33021;&#22815;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#29702;&#35299;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#22788;&#29702;&#19978;&#19979;&#25991;&#23545;&#20110;&#20219;&#20309;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#20010;&#19978;&#19979;&#25991;&#21487;&#33021;&#26159;&#23545;&#35805;&#24335;&#30340;&#65288;&#20381;&#36182;&#20110;&#20043;&#21069;&#30340;&#29992;&#25143;&#26597;&#35810;&#25110;&#31995;&#32479;&#22238;&#31572;&#65289;&#65292;&#20063;&#21487;&#33021;&#26159;&#35270;&#35273;&#30340;&#65288;&#20381;&#36182;&#20110;&#29992;&#25143;&#30475;&#21040;&#30340;&#19996;&#35199;&#65292;&#20363;&#22914;&#20182;&#20204;&#30340;&#23631;&#24149;&#19978;&#65289;&#65292;&#25110;&#32773;&#26159;&#32972;&#26223;&#30340;&#65288;&#22522;&#20110;&#19968;&#20123;&#20449;&#21495;&#65292;&#27604;&#22914;&#21709;&#36215;&#30340;&#38393;&#38047;&#25110;&#32773;&#27491;&#22312;&#25773;&#25918;&#30340;&#38899;&#20048;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MARRS&#65288;&#22810;&#27169;&#24577;&#21442;&#32771;&#35299;&#26512;&#31995;&#32479;&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#22312;&#22788;&#29702;&#23545;&#35805;&#24335;&#12289;&#35270;&#35273;&#21644;&#32972;&#26223;&#19978;&#19979;&#25991;&#26041;&#38754;&#36127;&#36131;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23454;&#29616;&#19978;&#19979;&#25991;&#26597;&#35810;&#30340;&#22788;&#29702;&#65307;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#32771;&#35299;&#26512;&#65292;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#26597;&#35810;&#37325;&#20889;&#22788;&#29702;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#30456;&#20114;&#34917;&#20805;&#65292;&#24418;&#25104;&#19968;&#20010;&#32479;&#19968;&#12289;&#36830;&#36143;&#12289;&#36731;&#37327;&#32423;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#29702;&#35299;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successfully handling context is essential for any dialog understanding task. This context maybe be conversational (relying on previous user queries or system responses), visual (relying on what the user sees, for example, on their screen), or background (based on signals such as a ringing alarm or playing music). In this work, we present an overview of MARRS, or Multimodal Reference Resolution System, an on-device framework within a Natural Language Understanding system, responsible for handling conversational, visual and background context. In particular, we present different machine learning models to enable handing contextual queries; specifically, one to enable reference resolution, and one to handle context via query rewriting. We also describe how these models complement each other to form a unified, coherent, lightweight system that can understand context while preserving user privacy.
&lt;/p&gt;</description></item><item><title>VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2311.01623</link><description>&lt;p&gt;
VQPy&#65306;&#19968;&#31181;&#38754;&#21521;&#29616;&#20195;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01623
&lt;/p&gt;
&lt;p&gt;
VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#20170;&#31995;&#32479;&#21644;&#26381;&#21153;&#20013;&#12290;&#22312;&#35270;&#39057;&#20998;&#26512;&#30340;&#21069;&#27839;&#26159;&#29992;&#25143;&#24320;&#21457;&#30340;&#35270;&#39057;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#65288;&#20363;&#22914;&#20154;&#65292;&#21160;&#29289;&#65292;&#27773;&#36710;&#31561;&#65289;&#19982;&#20256;&#32479;&#38754;&#21521;&#23545;&#35937;&#35821;&#35328;&#24314;&#27169;&#30340;&#23545;&#35937;&#30456;&#20284;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21517;&#20026;VQPy&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#31471;&#65288;&#19968;&#31181;Python&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#21487;&#20197;&#34920;&#36798;&#35270;&#39057;&#23545;&#35937;&#21450;&#20854;&#20132;&#20114;&#30340;&#32467;&#26500;&#65289;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#33258;&#21160;&#29983;&#25104;&#21644;&#20248;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#26045;&#21644;&#24320;&#28304;&#20102;VQPy&#65292;&#23427;&#24050;&#32463;&#20316;&#20026;Cisco DeepVision&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#20135;&#21697;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
&lt;/p&gt;</description></item><item><title>ACQUIRED&#26159;&#19968;&#20010;&#29992;&#20110;&#22238;&#31572;&#30495;&#23454;&#29983;&#27963;&#35270;&#39057;&#20013;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#22810;&#27169;&#24577;&#27169;&#22411;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#39046;&#22495;&#24357;&#34917;&#20102;&#30446;&#21069;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#26679;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#21644;&#25512;&#29702;&#32500;&#24230;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01620</link><description>&lt;p&gt;
ACQUIRED: &#29992;&#20110;&#22238;&#31572;&#30495;&#23454;&#29983;&#27963;&#35270;&#39057;&#20013;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos. (arXiv:2311.01620v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01620
&lt;/p&gt;
&lt;p&gt;
ACQUIRED&#26159;&#19968;&#20010;&#29992;&#20110;&#22238;&#31572;&#30495;&#23454;&#29983;&#27963;&#35270;&#39057;&#20013;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#22810;&#27169;&#24577;&#27169;&#22411;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#39046;&#22495;&#24357;&#34917;&#20102;&#30446;&#21069;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#26679;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#21644;&#25512;&#29702;&#32500;&#24230;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21453;&#20107;&#23454;&#25512;&#29702;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33021;&#21147;&#12290;&#23427;&#28041;&#21450;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#36755;&#20837;&#39044;&#27979;&#20551;&#35774;&#24773;&#22659;&#19979;&#30340;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;AI&#27169;&#22411;&#33021;&#22815;&#20174;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#24182;&#25506;&#32034;&#20551;&#35774;&#22330;&#26223;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#37327;&#38024;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#20013;&#65292;&#23427;&#20204;&#21482;&#35206;&#30422;&#21512;&#25104;&#29615;&#22659;&#25110;&#29305;&#23450;&#31867;&#22411;&#30340;&#20107;&#20214;&#65288;&#20363;&#22914;&#20132;&#36890;&#30896;&#25758;&#65289;&#65292;&#20351;&#24471;&#22312;&#22810;&#26679;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#21644;&#25512;&#29702;&#32500;&#24230;&#20013;&#21487;&#38752;&#22320;&#35780;&#20272;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;ACQUIRED&#65306;&#23427;&#30001;3.9K&#20010;&#24102;&#27880;&#37322;&#30340;&#35270;&#39057;&#32452;&#25104;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#20107;&#20214;&#31867;&#22411;&#65292;&#21516;&#26102;&#21253;&#21547;&#20102;&#31532;&#19968;&#20154;&#31216;&#21644;&#31532;&#19977;&#20154;&#31216;&#35270;&#35282;&#65292;&#20197;&#30830;&#20445;&#20851;&#27880;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#35270;&#39057;&#37117;&#26631;&#27880;&#26377;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal counterfactual reasoning is a vital yet challenging ability for AI systems. It involves predicting the outcomes of hypothetical circumstances based on vision and language inputs, which enables AI models to learn from failures and explore hypothetical scenarios. Despite its importance, there are only a few datasets targeting the counterfactual reasoning abilities of multimodal models. Among them, they only cover reasoning over synthetic environments or specific types of events (e.g. traffic collisions), making them hard to reliably benchmark the model generalization ability in diverse real-world scenarios and reasoning dimensions. To overcome these limitations, we develop a video question answering dataset, ACQUIRED: it consists of 3.9K annotated videos, encompassing a wide range of event types and incorporating both first and third-person viewpoints, which ensures a focus on real-world diversity. In addition, each video is annotated with questions that span three distinct di
&lt;/p&gt;</description></item><item><title>FLAP&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;&#35821;&#35328;&#38899;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25513;&#30422;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#37325;&#26500;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#23545;&#40784;&#30340;&#38899;&#39057;&#21644;&#35821;&#35328;&#34920;&#31034;&#65292;&#20197;&#21450;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#25991;&#26412;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#38899;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#24615;&#33021;&#65288;SoTA&#65289;&#12290;</title><link>http://arxiv.org/abs/2311.01615</link><description>&lt;p&gt;
FLAP: &#24555;&#36895;&#35821;&#35328;&#38899;&#39057;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FLAP: Fast Language-Audio Pre-training. (arXiv:2311.01615v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01615
&lt;/p&gt;
&lt;p&gt;
FLAP&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;&#35821;&#35328;&#38899;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25513;&#30422;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#37325;&#26500;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#23545;&#40784;&#30340;&#38899;&#39057;&#21644;&#35821;&#35328;&#34920;&#31034;&#65292;&#20197;&#21450;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#25991;&#26412;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#22312;&#38899;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#24615;&#33021;&#65288;SoTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLAP&#30340;&#24555;&#36895;&#35821;&#35328;&#38899;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#25513;&#30422;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#37325;&#26500;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#23545;&#40784;&#30340;&#38899;&#39057;&#21644;&#35821;&#35328;&#34920;&#31034;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;FLAP&#38543;&#26426;&#20002;&#25481;&#38899;&#39057;&#35889;&#22270;&#30340;&#26631;&#35760;&#65292;&#20165;&#19987;&#27880;&#20110;&#21097;&#20313;&#26631;&#35760;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#65292;FLAP&#23398;&#20064;&#23558;&#37197;&#23545;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#22312;&#20849;&#20139;&#28508;&#22312;&#31354;&#38388;&#20013;&#23545;&#40784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;FLAP&#21033;&#29992;&#22810;&#20010;&#25513;&#27169;&#35270;&#22270;&#36827;&#34892;&#36328;&#27169;&#24577;&#23545;&#27604;&#65292;&#24182;&#23398;&#20064;&#37325;&#26500;&#38899;&#39057;&#26631;&#35760;&#30340;&#25513;&#27169;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;FLAP&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#25991;&#26412;&#36755;&#20837;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#26356;&#24378;&#22823;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#38899;&#39057;-&#25991;&#26412;&#34920;&#31034;&#65292;&#20351;&#24471;FLAP&#22312;AudioCaps&#65288;&#36798;&#21040;53.0&#65285; R@1&#65289;&#21644;Clotho&#65288;&#36798;&#21040;25.5&#65285; R@1&#65289;&#30340;&#38899;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#24615;&#33021;&#65288;SoTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Fast Language-Audio Pre-training (FLAP), a self-supervised approach that efficiently and effectively learns aligned audio and language representations through masking, contrastive learning and reconstruction. For efficiency, FLAP randomly drops audio spectrogram tokens, focusing solely on the remaining ones for self-supervision. Through inter-modal contrastive learning, FLAP learns to align paired audio and text representations in a shared latent space. Notably, FLAP leverages multiple augmented views via masking for inter-modal contrast and learns to reconstruct the masked portion of audio tokens. Moreover, FLAP leverages large language models (LLMs) to augment the text inputs, contributing to improved performance. These approaches lead to more robust and informative audio-text representations, enabling FLAP to achieve state-of-the-art (SoTA) performance on audio-text retrieval tasks on AudioCaps (achieving 53.0% R@1) and Clotho (achieving 25.5% R@1).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;KG-FRUS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20197;&#30693;&#35782;&#22270;&#35889;&#30340;&#24418;&#24335;&#32534;&#30721;&#20102;&#36229;&#36807;30&#19975;&#20221;&#32654;&#22269;&#25919;&#24220;&#22806;&#20132;&#25991;&#20214;&#65292;&#24182;&#20351;&#29992;&#25552;&#21462;&#30340;&#23454;&#20307;&#12289;&#20803;&#25968;&#25454;&#21450;&#26469;&#33258;Wikidata&#30340;&#39069;&#22806;&#23454;&#20307;&#21644;&#20851;&#31995;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#20851;&#31995;&#25429;&#25417;&#20102;&#22806;&#20132;&#12289;&#22269;&#38469;&#20851;&#31995;&#21644;&#25919;&#27835;&#31561;&#22797;&#26434;&#39046;&#22495;&#30340;&#21327;&#21516;&#21644;&#21160;&#21147;&#12290;&#25991;&#31456;&#28436;&#31034;&#20102;&#19981;&#21516;&#30340;&#25506;&#32034;&#25968;&#25454;&#38598;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01606</link><description>&lt;p&gt;
KG-FRUS&#65306;&#19968;&#20010;&#30001;127&#24180;&#32654;&#22269;&#22806;&#20132;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#32534;&#30721;&#30340;&#26032;&#22411;&#22270;&#24418;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KG-FRUS: a Novel Graph-based Dataset of 127 Years of US Diplomatic Relations. (arXiv:2311.01606v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;KG-FRUS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20197;&#30693;&#35782;&#22270;&#35889;&#30340;&#24418;&#24335;&#32534;&#30721;&#20102;&#36229;&#36807;30&#19975;&#20221;&#32654;&#22269;&#25919;&#24220;&#22806;&#20132;&#25991;&#20214;&#65292;&#24182;&#20351;&#29992;&#25552;&#21462;&#30340;&#23454;&#20307;&#12289;&#20803;&#25968;&#25454;&#21450;&#26469;&#33258;Wikidata&#30340;&#39069;&#22806;&#23454;&#20307;&#21644;&#20851;&#31995;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#20851;&#31995;&#25429;&#25417;&#20102;&#22806;&#20132;&#12289;&#22269;&#38469;&#20851;&#31995;&#21644;&#25919;&#27835;&#31561;&#22797;&#26434;&#39046;&#22495;&#30340;&#21327;&#21516;&#21644;&#21160;&#21147;&#12290;&#25991;&#31456;&#28436;&#31034;&#20102;&#19981;&#21516;&#30340;&#25506;&#32034;&#25968;&#25454;&#38598;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;KG-FRUS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;30&#19975;&#20221;&#32654;&#22269;&#25919;&#24220;&#22806;&#20132;&#25991;&#20214;&#65292;&#24182;&#20197;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30340;&#24418;&#24335;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#21033;&#29992;&#12298;&#32654;&#22269;&#23545;&#22806;&#20851;&#31995;&#12299;&#65288;FRUS&#65289;&#30340;&#25968;&#25454;&#65288;&#20197;XML&#25991;&#20214;&#24418;&#24335;&#25552;&#20379;&#65289;&#65292;&#25552;&#21462;&#20854;&#20013;&#20851;&#20110;&#25991;&#20214;&#12289;&#20010;&#20154;&#21644;&#22269;&#23478;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;&#25552;&#21462;&#30340;&#23454;&#20307;&#21450;&#20854;&#30456;&#20851;&#20803;&#25968;&#25454;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;Wikidata&#20013;&#34917;&#20805;&#20102;&#39069;&#22806;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;KG&#20013;&#30340;&#20851;&#31995;&#25429;&#25417;&#20102;&#30740;&#31350;&#21644;&#29702;&#35299;&#22806;&#20132;&#12289;&#22269;&#38469;&#20851;&#31995;&#21644;&#25919;&#27835;&#31561;&#22797;&#26434;&#39046;&#22495;&#25152;&#38656;&#30340;&#21327;&#21516;&#21644;&#21160;&#21147;&#12290;&#36825;&#36828;&#36828;&#36229;&#20986;&#20102;&#20165;&#20165;&#25910;&#38598;&#25991;&#20214;&#32780;&#24573;&#30053;&#25991;&#26412;&#20013;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#31616;&#21333;&#38598;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#28436;&#31034;&#19981;&#21516;&#30340;&#25506;&#32034;KG&#26041;&#27861;&#23637;&#31034;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26597;&#35810;&#35821;&#35328;&#26469;&#22238;&#31572;&#31616;&#21333;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current paper, we present the KG-FRUS dataset, comprised of more than 300,000 US government diplomatic documents encoded in a Knowledge Graph (KG). We leverage the data of the Foreign Relations of the United States (FRUS) (available as XML files) to extract information about the documents and the individuals and countries mentioned within them. We use the extracted entities, and associated metadata, to create a graph-based dataset. Further, we supplement the created KG with additional entities and relations from Wikidata. The relations in the KG capture the synergies and dynamics required to study and understand the complex fields of diplomacy, foreign relations, and politics. This goes well beyond a simple collection of documents which neglects the relations between entities in the text. We showcase a range of possibilities of the current dataset by illustrating different approaches to probe the KG. In the paper, we exemplify how to use a query language to answer simple researc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRED&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01605</link><description>&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#39044;&#27979;&#30340;&#24544;&#23454;&#21644;&#31283;&#20581;&#30340;&#26412;&#22320;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful and Robust Local Interpretability for Textual Predictions. (arXiv:2311.01605v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRED&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#24471;&#21040;&#20449;&#20219;&#21644;&#37096;&#32626;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#22797;&#26434;&#65292;&#24182;&#19988;&#32570;&#20047;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#19981;&#33021;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;FRED&#65288;Faithful and Robust Explainer for textual Documents&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24403;&#36825;&#20123;&#35789;&#34987;&#31227;&#38500;&#26102;&#23545;&#39044;&#27979;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#27491;&#24335;&#30340;&#23450;&#20041;&#21644;&#23545;&#21487;&#35299;&#37322;&#20998;&#31867;&#22120;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#30830;&#31435;&#20102;FRED&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FRED&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is essential for machine learning models to be trusted and deployed in critical domains. However, existing methods for interpreting text models are often complex, lack solid mathematical foundations, and their performance is not guaranteed. In this paper, we propose FRED (Faithful and Robust Explainer for textual Documents), a novel method for interpreting predictions over text. FRED identifies key words in a document that significantly impact the prediction when removed. We establish the reliability of FRED through formal definitions and theoretical analyses on interpretable classifiers. Additionally, our empirical evaluation against state-of-the-art methods demonstrates the effectiveness of FRED in providing insights into text models.
&lt;/p&gt;</description></item><item><title>MetaReVision&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21040;&#30340;&#22522;&#26412;&#27010;&#24565;&#20316;&#20026;&#25903;&#25345;&#38598;&#21512;&#26469;&#24555;&#36895;&#23398;&#20064;&#21644;&#35782;&#21035;&#26032;&#30340;&#22270;&#20687;&#22522;&#30784;&#32452;&#21512;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2311.01580</link><description>&lt;p&gt;
MetaReVision: &#20351;&#29992;&#26816;&#32034;&#36827;&#34892;&#20803;&#23398;&#20064;&#30340;&#35270;&#35273;&#22522;&#30784;&#32452;&#21512;&#27010;&#24565;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition. (arXiv:2311.01580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01580
&lt;/p&gt;
&lt;p&gt;
MetaReVision&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21040;&#30340;&#22522;&#26412;&#27010;&#24565;&#20316;&#20026;&#25903;&#25345;&#38598;&#21512;&#26469;&#24555;&#36895;&#23398;&#20064;&#21644;&#35782;&#21035;&#26032;&#30340;&#22270;&#20687;&#22522;&#30784;&#32452;&#21512;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#22238;&#24518;&#21644;&#25512;&#24191;&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#33719;&#21462;&#30340;&#22522;&#26412;&#27010;&#24565;&#26469;&#23398;&#20064;&#26032;&#30340;&#32452;&#21512;&#27010;&#24565;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411; - MetaReVision&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;&#22522;&#30784;&#32452;&#21512;&#27010;&#24565;&#23398;&#20064;&#38382;&#39064;&#12290;MetaReVision&#30001;&#19968;&#20010;&#26816;&#32034;&#27169;&#22359;&#21644;&#19968;&#20010;&#20803;&#23398;&#20064;&#27169;&#22359;&#32452;&#25104;&#65292;&#26088;&#22312;&#23558;&#26816;&#32034;&#21040;&#30340;&#22522;&#26412;&#27010;&#24565;&#20316;&#20026;&#25903;&#25345;&#38598;&#21512;&#24182;&#29992;&#20110;&#20803;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26469;&#35782;&#21035;&#22522;&#20110;&#22270;&#20687;&#30340;&#32452;&#21512;&#27010;&#24565;&#12290;&#36890;&#36807;&#20174;&#26816;&#32034;&#22120;&#26500;&#24314;&#30340;episode&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;MetaReVision&#23398;&#20064;&#21040;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#32452;&#21512;&#34920;&#31034;&#65292;&#21487;&#20197;&#24555;&#36895;&#26356;&#26032;&#20197;&#35782;&#21035;&#26032;&#30340;&#32452;&#21512;&#27010;&#24565;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;CompCOCO&#21644;CompFlickr&#26469;&#35780;&#20272;&#22522;&#20110;&#22270;&#20687;&#30340;&#32452;&#21512;&#27010;&#24565;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MetaReVision&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#65292;&#24182;&#19988;&#26816;&#32034;&#27169;&#22359;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have the ability to learn novel compositional concepts by recalling and generalizing primitive concepts acquired from past experiences. Inspired by this observation, in this paper, we propose MetaReVision, a retrieval-enhanced meta-learning model to address the visually grounded compositional concept learning problem. The proposed MetaReVision consists of a retrieval module and a meta-learning module which are designed to incorporate retrieved primitive concepts as a supporting set to meta-train vision-anguage models for grounded compositional concept recognition. Through meta-learning from episodes constructed by the retriever, MetaReVision learns a generic compositional representation that can be fast updated to recognize novel compositional concepts. We create CompCOCO and CompFlickr to benchmark the grounded compositional concept learning. Our experimental results show that MetaReVision outperforms other competitive baselines and the retrieval module plays an important role 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#23398;&#20064;&#19982;&#25991;&#26412;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#20004;&#20010;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#19978;&#35757;&#32451;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#26102;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01571</link><description>&lt;p&gt;
&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#27169;&#22411;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Preserving the knowledge of long clinical texts using aggregated ensembles of large language models. (arXiv:2311.01571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#23398;&#20064;&#19982;&#25991;&#26412;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#20004;&#20010;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#19978;&#35757;&#32451;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#26102;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#65292;&#22914;&#20837;&#38498;&#35760;&#24405;&#12289;&#20986;&#38498;&#23567;&#32467;&#21644;&#36827;&#23637;&#35760;&#24405;&#65292;&#21253;&#21547;&#20016;&#23500;&#32780;&#23453;&#36149;&#30340;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#20020;&#24202;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#20110;BERT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20020;&#24202;&#25991;&#26412;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#36755;&#20837;&#38271;&#24230;&#30340;&#38480;&#21046;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#21333;&#29420;&#20351;&#29992;&#27169;&#22411;&#38598;&#25104;&#25110;&#25991;&#26412;&#32858;&#21512;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#23398;&#20064;&#19982;&#25991;&#26412;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#22312;&#20004;&#20010;&#20020;&#24202;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#65288;&#27515;&#20129;&#39044;&#27979;&#21644;&#20303;&#38498;&#22825;&#25968;&#39044;&#27979;&#65289;&#19978;&#35757;&#32451;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#22522;&#32447;&#12289;&#29420;&#31435;&#30340;&#38598;&#25104;&#21644;&#32858;&#21512;&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#26102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical texts, such as admission notes, discharge summaries, and progress notes, contain rich and valuable information that can be used for various clinical outcome prediction tasks. However, applying large language models, such as BERT-based models, to clinical texts poses two major challenges: the limitation of input length and the diversity of data sources. This paper proposes a novel method to preserve the knowledge of long clinical texts using aggregated ensembles of large language models. Unlike previous studies which use model ensembling or text aggregation methods separately, we combine ensemble learning with text aggregation and train multiple large language models on two clinical outcome tasks: mortality prediction and length of stay prediction. We show that our method can achieve better results than baselines, ensembling, and aggregation individually, and can improve the performance of large language models while handling long inputs and diverse datasets. We conduct extensi
&lt;/p&gt;</description></item><item><title>&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#20196;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#23545;&#19968;&#25490;&#24207;&#33021;&#21147;&#33976;&#39311;&#20026;&#26356;&#39640;&#25928;&#30340;&#21333;&#28857;&#25490;&#24207;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#25490;&#24207;&#22120;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01555</link><description>&lt;p&gt;
&#32763;&#35793;&#21518;&#30340;&#35770;&#25991;&#26631;&#39064;&#65306;&#25351;&#20196;&#33976;&#39311;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#39640;&#25928;&#30340;&#38646;-shot&#25490;&#24207;&#22120;
&lt;/p&gt;
&lt;p&gt;
Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers. (arXiv:2311.01555v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01555
&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#20196;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#23545;&#19968;&#25490;&#24207;&#33021;&#21147;&#33976;&#39311;&#20026;&#26356;&#39640;&#25928;&#30340;&#21333;&#28857;&#25490;&#24207;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#25490;&#24207;&#22120;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#30456;&#20851;&#24615;&#25490;&#24207;&#22120;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#25991;&#26723;&#36827;&#34892;&#19968;&#23545;&#19968;&#25110;&#19968;&#23545;&#22810;&#30340;&#27604;&#36739;&#12290;&#23613;&#31649;&#36825;&#20123;&#19968;&#23545;&#22810;&#21644;&#19968;&#23545;&#19968;&#30340;&#26041;&#27861;&#26377;&#25928;&#65292;&#20294;&#25928;&#29575;&#19981;&#39640;&#65292;&#19988;&#20005;&#37325;&#20381;&#36182;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25351;&#20196;&#33976;&#39311;&#26041;&#27861;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#24320;&#28304;LLMs&#30340;&#19968;&#23545;&#19968;&#25490;&#24207;&#33021;&#21147;&#33976;&#39311;&#20026;&#26356;&#31616;&#21333;&#20294;&#26356;&#39640;&#25928;&#30340;&#21333;&#28857;&#25490;&#24207;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#30456;&#21516;&#30340;LLMs&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22797;&#26434;&#30340;&#25351;&#20196;&#37319;&#29992;&#26377;&#25928;&#30340;&#19968;&#23545;&#19968;&#26041;&#27861;&#23545;&#25991;&#26723;&#36827;&#34892;&#25490;&#24207;&#65292;&#28982;&#21518;&#23558;&#25945;&#24072;&#30340;&#39044;&#27979;&#32467;&#26524;&#36716;&#21270;&#20026;&#37319;&#29992;&#26356;&#31616;&#21333;&#30340;&#25351;&#20196;&#30340;&#21333;&#28857;&#25490;&#24207;&#26041;&#27861;&#12290;&#22312;BEIR&#12289;TREC&#21644;ReDial&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#33976;&#39311;&#21487;&#20197;&#23558;&#25928;&#29575;&#25552;&#39640;10&#21040;100&#20493;&#65292;&#21516;&#26102;&#25552;&#39640;LLMs&#30340;&#25490;&#24207;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the great potential of Large Language Models (LLMs) serving as zero-shot relevance rankers. The typical approach involves making comparisons between pairs or lists of documents. Although effective, these listwise and pairwise methods are not efficient and also heavily rely on intricate prompt engineering. To tackle this problem, we introduce a novel instruction distillation method. The key idea is to distill the pairwise ranking ability of open-sourced LLMs to a simpler but more efficient pointwise ranking. Specifically, given the same LLM, we first rank documents using the effective pairwise approach with complex instructions, and then distill the teacher predictions to the pointwise approach with simpler instructions. Evaluation results on the BEIR, TREC, and ReDial datasets demonstrate that instruction distillation can improve efficiency by 10 to 100x and also enhance the ranking performance of LLMs. Furthermore, our approach surpasses the performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21387;&#32553;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;DTM&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#19988;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DTM&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#20462;&#21098;&#25481;&#36229;&#36807;90%&#30340;LLM&#32452;&#20214;&#21644;&#37327;&#21270;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2311.01544</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65306;&#36890;&#36807;&#27979;&#37327;&#34928;&#20943;&#26469;&#20462;&#21098;LLM&#32452;&#20214;&#24182;&#20248;&#21270;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21387;&#32553;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;DTM&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#19988;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DTM&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#20462;&#21098;&#25481;&#36229;&#36807;90%&#30340;LLM&#32452;&#20214;&#21644;&#37327;&#21270;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#22823;&#23567;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#30340;&#26377;&#25928;&#37096;&#32626;&#21644;LLM&#21387;&#32553;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#21387;&#32553;LLM&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25351;&#26631;&#22914;&#22256;&#24785;&#24230;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#23616;&#38480;&#24615;&#12290;DTM&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;DTM&#36824;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#35780;&#20272;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#31532;&#19968;&#20010;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;FDTM&#65289;&#22312;&#27169;&#22411;&#31232;&#30095;&#21270;&#20013;&#26174;&#31034;&#65292;&#36229;&#36807;90%&#30340;&#25152;&#26377;&#32452;&#20214;&#21487;&#20197;&#20462;&#21098;&#25481;&#12290;&#23545;&#20110;&#37327;&#21270;&#65292;FDTM&#34920;&#26126;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#21487;&#20197;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. Their ever-increasing size, however, raised concerns about their effective deployment and the need for LLM compressions. This study introduces the Divergent Token metrics (DTMs), a novel approach for assessing compressed LLMs, addressing the limitations of traditional measures like perplexity that fail to accurately reflect text generation quality. DTMs focus on token divergence, providing deeper insights into the subtleties of model compression. Our results indicate that significant levels of precision and sparsity can be achieved without compromising text generation quality. Moreover, DTMs offers a more precise evaluation of each component's impact individually. Utilizing the First Divergent Token metric (FDTM) in model sparsification reveals that nearly 20% of all components can be pruned over 90%. In terms of quantization, the FDTM suggests that over 80% of parameters can be s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32508;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21019;&#24314;&#39640;&#36136;&#37327;&#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#25351;&#20196;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01487</link><description>&lt;p&gt;
&#20248;&#31168;&#30340;&#35270;&#35273;&#25351;&#23548;&#26377;&#20160;&#20040;&#29305;&#28857;&#65311;&#32508;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#25351;&#20196;&#29992;&#20110;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning. (arXiv:2311.01487v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01487
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21019;&#24314;&#39640;&#36136;&#37327;&#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#25351;&#20196;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26159;&#25552;&#39640;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#30528;&#30524;&#20110;&#19981;&#21516;&#28966;&#28857;&#21644;&#29305;&#24449;&#30340;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;MLLMs&#22312;&#35780;&#20272;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;MLLMs&#65292;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#19968;&#20010;&#26356;&#22522;&#26412;&#30340;&#38382;&#39064;&#65306;&#8220;&#20160;&#20040;&#26679;&#30340;&#35270;&#35273;&#25351;&#23548;&#25165;&#26159;&#22909;&#30340;&#65311;&#8221;&#36890;&#36807;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#20391;&#37325;&#20110;&#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#30340;&#25351;&#23548;&#23545;&#20110;&#25913;&#21892;MLLMs&#22312;&#35780;&#20272;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#29305;&#21035;&#26377;&#25928;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#21512;&#25104;-&#22797;&#26434;&#21270;-&#37325;&#26500;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#22810;&#20010;&#38454;&#27573;&#36880;&#28176;&#22686;&#21152;&#25351;&#20196;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual instruction tuning is an essential approach to improving the zero-shot generalization capability of Multi-modal Large Language Models (MLLMs). A surge of visual instruction datasets with various focuses and characteristics have been proposed recently, enabling MLLMs to achieve surprising results on evaluation benchmarks. To develop more capable MLLMs, in this paper, we aim to investigate a more fundamental question: ``what makes for good visual instructions?''. By conducting a comprehensive empirical study, we find that instructions focused on complex visual reasoning tasks are particularly effective in improving the performance of MLLMs on evaluation benchmarks. Building upon this finding, we design a systematic approach to automatically creating high-quality complex visual reasoning instructions. Our approach employs a synthesis-complication-reformulation paradigm, leveraging multiple stages to gradually increase the complexity of the instructions while guaranteeing quality. B
&lt;/p&gt;</description></item><item><title>RENA&#26159;&#19968;&#31181;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#20851;&#31995;&#25552;&#21462;&#24037;&#20855;&#65292;&#29992;&#20110;&#20174;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#20013;&#25552;&#21462;&#19982;&#20256;&#26579;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#23454;&#20307;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#20026;&#27969;&#34892;&#30149;&#30417;&#27979;&#25552;&#20379;&#23454;&#26102;&#35299;&#26512;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01472</link><description>&lt;p&gt;
&#20174;&#26032;&#38395;&#25991;&#31456;&#20013;&#25552;&#21462;&#20851;&#31995;&#65288;RENA&#65289;&#65306;&#29992;&#20110;&#27969;&#34892;&#30149;&#30417;&#27979;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction from News Articles (RENA): A Tool for Epidemic Surveillance. (arXiv:2311.01472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01472
&lt;/p&gt;
&lt;p&gt;
RENA&#26159;&#19968;&#31181;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#20851;&#31995;&#25552;&#21462;&#24037;&#20855;&#65292;&#29992;&#20110;&#20174;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#20013;&#25552;&#21462;&#19982;&#20256;&#26579;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#23454;&#20307;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#20026;&#27969;&#34892;&#30149;&#30417;&#27979;&#25552;&#20379;&#23454;&#26102;&#35299;&#26512;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#20174;&#26032;&#38395;&#25991;&#31456;&#65288;RENA&#65289;&#26159;&#19968;&#20010;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#21462;&#19982;&#20256;&#26579;&#30149;&#30456;&#20851;&#30340;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20851;&#38190;&#23454;&#20307;&#21450;&#20854;&#35821;&#20041;&#20851;&#31995;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;React&#26694;&#26550;&#26500;&#24314;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#20248;&#38597;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#36755;&#20837;&#26032;&#38395;&#25991;&#31456;&#24182;&#20174;&#20004;&#20010;&#27169;&#22411;&#20013;&#36873;&#25321;&#65292;&#20197;&#29983;&#25104;&#25152;&#25552;&#20379;&#25991;&#26412;&#20013;&#30340;&#20851;&#31995;&#30340;&#20840;&#38754;&#21015;&#34920;&#12290;&#22240;&#27492;&#65292;RENA&#20801;&#35768;&#23454;&#26102;&#35299;&#26512;&#26032;&#38395;&#25991;&#31456;&#25552;&#21462;&#27969;&#34892;&#30149;&#30417;&#27979;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20026;&#24320;&#28304;&#24773;&#25253;&#39537;&#21160;&#30340;&#27969;&#34892;&#30149;&#39044;&#35686;&#31995;&#32479;EPIWATCH&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction from News Articles (RENA) is a browser-based tool designed to extract key entities and their semantic relationships in English language news articles related to infectious diseases. Constructed using the React framework, this system presents users with an elegant and user-friendly interface. It enables users to input a news article and select from a choice of two models to generate a comprehensive list of relations within the provided text. As a result, RENA allows real-time parsing of news articles to extract key information for epidemic surveillance, contributing to EPIWATCH, an open-source intelligence-based epidemic warning system.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#21270;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#30340;&#25968;&#23398;&#24418;&#24335;&#65292;&#24314;&#31435;&#20102;&#20248;&#21270;&#30340;ClimateBERT&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#32467;&#26524;&#27604;&#36739;&#20998;&#26512;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#33391;&#22909;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2311.01469</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#29615;&#20445;&#34394;&#20551;&#23459;&#20256;
&lt;/p&gt;
&lt;p&gt;
Leveraging Language Models to Detect Greenwashing. (arXiv:2311.01469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#21270;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#30340;&#25968;&#23398;&#24418;&#24335;&#65292;&#24314;&#31435;&#20102;&#20248;&#21270;&#30340;ClimateBERT&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#32467;&#26524;&#27604;&#36739;&#20998;&#26512;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#33391;&#22909;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#27668;&#20505;&#21464;&#21270;&#30340;&#21518;&#26524;&#36234;&#26469;&#36234;&#24341;&#36215;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#20225;&#19994;&#22312;&#21487;&#25345;&#32493;&#21457;&#23637;&#25253;&#21578;&#20013;&#24378;&#35843;&#20854;&#29615;&#20445;&#21162;&#21147;&#20197;&#22686;&#24378;&#20844;&#20247;&#24418;&#35937;&#12290;&#28982;&#32780;&#65292;&#23545;&#27492;&#31867;&#25253;&#21578;&#30340;&#23457;&#26680;&#32570;&#20047;&#20005;&#26684;&#30340;&#30417;&#31649;&#65292;&#21487;&#33021;&#23548;&#33268;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#36827;&#34892;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#26469;&#37327;&#21270;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#35813;&#38382;&#39064;&#30340;&#20248;&#21270;ClimateBERT&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#32467;&#26524;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#21487;&#25345;&#32493;&#21457;&#23637;&#25253;&#21578;&#30340;&#27979;&#35797;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#23454;&#29616;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;86.34%&#21644;F1&#20540;0.67&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#25506;&#32034;&#30340;&#33391;&#22909;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, climate change repercussions have increasingly captured public interest. Consequently, corporations are emphasizing their environmental efforts in sustainability reports to bolster their public image. Yet, the absence of stringent regulations in review of such reports allows potential greenwashing. In this study, we introduce a novel methodology to train a language model on generated labels for greenwashing risk. Our primary contributions encompass: developing a mathematical formulation to quantify greenwashing risk, a fine-tuned ClimateBERT model for this problem, and a comparative analysis of results. On a test set comprising of sustainability reports, our best model achieved an average accuracy score of 86.34% and F1 score of 0.67, demonstrating that our methods show a promising direction of exploration for this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#27169;&#25311;&#26426;&#22120;&#20154;&#21046;&#23450;&#35745;&#21010;&#65292;&#22312;ScienceWorld&#20013;&#23454;&#29616;30&#31867;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#22312;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;1.4&#20493;&#65292;&#24403;&#22635;&#20805;&#23613;&#21487;&#33021;&#22810;&#30340;&#20808;&#21069;&#27493;&#39588;&#26102;&#25552;&#39640;&#21040;3.5&#20493;&#65292;&#21363;&#20351;&#21482;&#35757;&#32451;&#20102;6.5%&#30340;&#25968;&#25454;&#65292;&#20063;&#27604;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;2.2&#20493;&#12290;&#19981;&#21516;&#31867;&#21035;&#30340;&#21160;&#20316;&#34920;&#29616;&#24046;&#24322;&#24456;&#22823;&#65292;&#35828;&#26126;&#24179;&#22343;&#20219;&#21153;&#21487;&#33021;&#20250;&#38544;&#34255;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01468</link><description>&lt;p&gt;
&#35760;&#20303;&#20320;&#25152;&#20570;&#30340;&#65292;&#36825;&#26679;&#20320;&#23601;&#30693;&#36947;&#25509;&#19979;&#26469;&#35813;&#20570;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remember what you did so you know what to do next. (arXiv:2311.01468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#27169;&#25311;&#26426;&#22120;&#20154;&#21046;&#23450;&#35745;&#21010;&#65292;&#22312;ScienceWorld&#20013;&#23454;&#29616;30&#31867;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#22312;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;1.4&#20493;&#65292;&#24403;&#22635;&#20805;&#23613;&#21487;&#33021;&#22810;&#30340;&#20808;&#21069;&#27493;&#39588;&#26102;&#25552;&#39640;&#21040;3.5&#20493;&#65292;&#21363;&#20351;&#21482;&#35757;&#32451;&#20102;6.5%&#30340;&#25968;&#25454;&#65292;&#20063;&#27604;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;2.2&#20493;&#12290;&#19981;&#21516;&#31867;&#21035;&#30340;&#21160;&#20316;&#34920;&#29616;&#24046;&#24322;&#24456;&#22823;&#65292;&#35828;&#26126;&#24179;&#22343;&#20219;&#21153;&#21487;&#33021;&#20250;&#38544;&#34255;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#19968;&#20010;&#20013;&#31561;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-J 6B&#21442;&#25968;&#65289;&#65292;&#20026;&#27169;&#25311;&#26426;&#22120;&#20154;&#22312;ScienceWorld&#20013;&#23454;&#29616;30&#31867;&#30446;&#26631;&#65288;&#19968;&#20010;&#29992;&#20110;&#23567;&#23398;&#31185;&#23398;&#23454;&#39564;&#30340;&#25991;&#26412;&#28216;&#25103;&#27169;&#25311;&#22120;&#65289;&#21046;&#23450;&#35745;&#21010;&#12290;&#20808;&#21069;&#21457;&#34920;&#30340;&#23454;&#35777;&#30740;&#31350;&#22768;&#31216;&#65292;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36866;&#29992;&#24615;&#36739;&#24046;&#65288;Wang&#31561;&#65292;2022&#65289;&#12290;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#65288;&#21333;&#20010;&#21069;&#19968;&#20010;&#27493;&#39588;&#65289;&#65292;LLM&#30340;&#24615;&#33021;&#36229;&#36807;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;1.4&#20493;&#12290;&#24403;&#25105;&#20204;&#23613;&#21487;&#33021;&#22810;&#22320;&#22635;&#20805;LLM&#30340;&#36755;&#20837;&#32531;&#20914;&#21306;&#26102;&#65292;&#25913;&#36827;&#25928;&#26524;&#25552;&#39640;&#21040;3.5&#20493;&#12290;&#21363;&#20351;&#21482;&#23545;6.5%&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;2.2&#20493;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#23545;&#20110;30&#31867;&#21160;&#20316;&#65292;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#65292;&#34920;&#26126;&#23545;&#20219;&#21153;&#36827;&#34892;&#24179;&#22343;&#21487;&#33021;&#20250;&#38544;&#34255;&#26174;&#33879;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#19982;&#25105;&#20204;&#21516;&#26102;&#36827;&#34892;&#30340;Lin&#31561;&#20154;&#65288;2023&#65289;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#19968;&#31181;&#20004;&#37096;&#20998;&#26041;&#27861;&#65288;SwiftSa&#65289;
&lt;/p&gt;
&lt;p&gt;
We explore using a moderately sized large language model (GPT-J 6B parameters) to create a plan for a simulated robot to achieve 30 classes of goals in ScienceWorld, a text game simulator for elementary science experiments. Previously published empirical work claimed that large language models (LLMs) are a poor fit (Wang et al., 2022) compared to reinforcement learning. Using the Markov assumption (a single previous step), the LLM outperforms the reinforcement learning-based approach by a factor of 1.4. When we fill the LLM's input buffer with as many prior steps as possible, improvement rises to 3.5x. Even when training on only 6.5% of the training data, we observe a 2.2x improvement over the reinforcement-learning-based approach. Our experiments show that performance varies widely across the 30 classes of actions, indicating that averaging over tasks can hide significant performance issues. In work contemporaneous with ours, Lin et al. (2023) demonstrated a two-part approach (SwiftSa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#21019;&#24314;&#21487;&#38752;&#12289;&#21487;&#20449;&#21644;&#26080;&#20559;&#32622;&#30340;LLM&#27169;&#22411;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#30528;&#37325;&#20110;&#37327;&#21270;&#12289;&#39564;&#35777;&#21644;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;LLM&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2311.01463</link><description>&lt;p&gt;
&#25903;&#25345;&#21487;&#20449;&#24230;&#30340;LLM&#21019;&#24314;&#36807;&#31243;&#65306;&#22788;&#29702;&#21307;&#30103;AI&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI. (arXiv:2311.01463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01463
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#21019;&#24314;&#21487;&#38752;&#12289;&#21487;&#20449;&#21644;&#26080;&#20559;&#32622;&#30340;LLM&#27169;&#22411;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#30528;&#37325;&#20110;&#37327;&#21270;&#12289;&#39564;&#35777;&#21644;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;LLM&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30701;&#26102;&#38388;&#20869;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36805;&#36895;&#22686;&#22810;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20934;&#30830;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#24187;&#35273;&#31561;&#38382;&#39064;&#65292;&#21307;&#30103;&#39046;&#22495;&#23545;&#20854;&#37319;&#29992;&#23384;&#22312;&#29369;&#35947;&#12290;&#37492;&#20110;&#21307;&#30103;&#20107;&#20851;&#37325;&#22823;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#29978;&#33267;&#25552;&#20986;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#20043;&#21069;&#19981;&#24212;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21019;&#24314;&#21487;&#38752;&#12289;&#21487;&#20449;&#21644;&#26080;&#20559;&#32622;&#27169;&#22411;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#36825;&#26159;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#24212;&#29992;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#22312;&#21307;&#30103;&#32972;&#26223;&#19979;&#23545;&#24187;&#35273;&#36827;&#34892;&#37327;&#21270;&#12289;&#39564;&#35777;&#21644;&#32531;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLM&#22312;&#21307;&#30103;&#39046;&#22495;&#26410;&#26469;&#30340;&#21487;&#33021;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have proliferated across multiple domains in as short period of time. There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations. Give the high stakes nature of healthcare, many researchers have even cautioned against its usage until these issues are resolved. The key to the implementation and deployment of LLMs in healthcare is to make these models trustworthy, transparent (as much possible) and explainable. In this paper we describe the key elements in creating reliable, trustworthy, and unbiased models as a necessary condition for their adoption in healthcare. Specifically we focus on the quantification, validation, and mitigation of hallucinations in the context in healthcare. Lastly, we discuss how the future of LLMs in healthcare may look like.
&lt;/p&gt;</description></item><item><title>FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2311.01282</link><description>&lt;p&gt;
FlashDecoding++: &#22312;GPU&#19978;&#21152;&#36895;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26356;&#24555;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01282
&lt;/p&gt;
&lt;p&gt;
FlashDecoding++&#26159;&#19968;&#31181;&#24555;&#36895;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12289;&#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#21644;&#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#26410;&#35299;&#20915;&#65306;(1) &#21516;&#27493;&#37096;&#20998;softmax&#26356;&#26032;&#12290;softmax&#25805;&#20316;&#38656;&#35201;&#21516;&#27493;&#26356;&#26032;&#27599;&#20010;&#37096;&#20998;softmax&#32467;&#26524;&#65292;&#23548;&#33268;LLM&#20013;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#24320;&#38144;&#22686;&#21152;&#32422;20%&#12290;(2) &#26410;&#20805;&#20998;&#21033;&#29992;&#25153;&#24179;GEMM&#35745;&#31639;&#12290;&#22312;LLM&#25512;&#29702;&#20013;&#25191;&#34892;GEMM&#30340;&#30697;&#38453;&#24418;&#29366;&#26159;&#25153;&#24179;&#30340;&#65292;&#23548;&#33268;&#22312;&#20808;&#21069;&#30340;&#35774;&#35745;&#20013;&#22635;&#20805;&#38646;&#21518;&#35745;&#31639;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#24615;&#33021;&#25439;&#22833;&#36229;&#36807;50%&#12290;(3) &#38745;&#24577;&#25968;&#25454;&#27969;&#23548;&#33268;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;LLM&#20013;&#30340;&#20869;&#26680;&#24615;&#33021;&#21462;&#20915;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#25968;&#25454;&#29305;&#24449;&#12289;&#30828;&#20214;&#37197;&#32622;&#31561;&#12290;&#21333;&#19968;&#21644;&#38745;&#24577;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#23548;&#33268;LLM&#25512;&#29702;&#20013;&#19981;&#21516;&#24418;&#29366;&#30340;GEMM&#30340;&#24615;&#33021;&#25439;&#22833;&#36798;&#21040;50.25%&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FlashDecoding++&#65292;&#19968;&#31181;&#24555;&#36895;&#25903;&#25345;&#20027;&#27969;LLM&#21644;&#30828;&#20214;&#21518;&#31471;&#30340;LLM&#25512;&#29702;&#24341;&#25806;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;FlashDecoding++&#23454;&#29616;&#20102;&#20197;&#19979;&#30446;&#26631;&#65306;
&lt;/p&gt;
&lt;p&gt;
As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and &gt;50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#12289;&#29983;&#25104;&#27807;&#36890;&#20505;&#36873;&#20197;&#21450;&#27169;&#25311;&#21463;&#20247;&#21453;&#24212;&#65292;&#26469;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;&#12290;&#36890;&#36807;&#35780;&#20272;&#20843;&#20010;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#22522;&#26412;&#36807;&#31243;&#30340;&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00687</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#21463;&#20247;&#32676;&#20307;&#65292;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#12289;&#29983;&#25104;&#27807;&#36890;&#20505;&#36873;&#20197;&#21450;&#27169;&#25311;&#21463;&#20247;&#21453;&#24212;&#65292;&#26469;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;&#12290;&#36890;&#36807;&#35780;&#20272;&#20843;&#20010;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#22522;&#26412;&#36807;&#31243;&#30340;&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#19982;&#20182;&#20154;&#36827;&#34892;&#27807;&#36890;&#20197;&#23454;&#29616;&#33258;&#24049;&#30340;&#30446;&#26631;&#65311;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#32463;&#39564;&#25110;&#20182;&#20154;&#30340;&#24314;&#35758;&#65292;&#25110;&#32773;&#36890;&#36807;&#39044;&#27979;&#23545;&#26041;&#30340;&#21453;&#24212;&#26469;&#26500;&#36896;&#20505;&#36873;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#26159;&#26377;&#38480;&#21644;&#26377;&#20559;&#35265;&#30340;&#65292;&#32780;&#19988;&#23545;&#28508;&#22312;&#32467;&#26524;&#36827;&#34892;&#25512;&#29702;&#21487;&#33021;&#26159;&#22256;&#38590;&#19988;&#35748;&#30693;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#26469;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#27807;&#36890;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25506;&#32034;-&#29983;&#25104;-&#27169;&#25311;&#65288;EGS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25509;&#21463;&#20219;&#20309;&#19968;&#20010;&#20010;&#20307;&#19982;&#19968;&#20010;&#30446;&#26631;&#21463;&#20247;&#36827;&#34892;&#27807;&#36890;&#30340;&#22330;&#26223;&#20316;&#20026;&#36755;&#20837;&#12290;EGS&#65288;1&#65289;&#36890;&#36807;&#29983;&#25104;&#19982;&#22330;&#26223;&#30456;&#20851;&#30340;&#22810;&#26679;&#21270;&#24314;&#35758;&#26469;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#65292;&#65288;2&#65289;&#29983;&#25104;&#20197;&#37096;&#20998;&#24314;&#35758;&#20026;&#26465;&#20214;&#30340;&#27807;&#36890;&#20505;&#36873;&#65292;&#65288;3&#65289;&#27169;&#25311;&#19981;&#21516;&#21463;&#20247;&#30340;&#21453;&#24212;&#65292;&#20197;&#30830;&#23450;&#26368;&#20339;&#20505;&#36873;&#21644;&#24314;&#35758;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#21313;&#20010;&#22522;&#26412;&#36807;&#31243;&#30340;&#20843;&#20010;&#22330;&#26223;&#19978;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal com
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#35821;&#27861;&#20064;&#24471;&#20027;&#35201;&#21463;&#21040;&#23545;&#35821;&#38899;&#25968;&#25454;&#30340;&#26292;&#38706;&#39537;&#21160;&#65292;&#24182;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00128</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#21457;&#23637;&#24615;&#25968;&#25454;&#36827;&#34892;&#35821;&#27861;&#20064;&#24471;&#30340;&#35838;&#31243;&#23398;&#20064;&#25928;&#26524;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the effect of curriculum learning with developmental data for grammar acquisition. (arXiv:2311.00128v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00128
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#35821;&#27861;&#20064;&#24471;&#20027;&#35201;&#21463;&#21040;&#23545;&#35821;&#38899;&#25968;&#25454;&#30340;&#26292;&#38706;&#39537;&#21160;&#65292;&#24182;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#27861;&#20064;&#24471;&#22312;&#35821;&#35328;&#8220;&#31616;&#21333;&#24615;&#8221;&#21644;&#25968;&#25454;&#30340;&#26469;&#28304;&#27169;&#24577;&#65288;&#35821;&#38899; vs &#25991;&#26412;&#65289;&#26041;&#38754;&#30340;&#24433;&#21709;&#31243;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;BabyBERTa&#20316;&#20026;&#25506;&#38024;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#27861;&#20064;&#24471;&#20027;&#35201;&#21463;&#21040;&#23545;&#35821;&#38899;&#25968;&#25454;&#30340;&#26292;&#38706;&#30340;&#39537;&#21160;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#23545;&#20004;&#20010;BabyLM&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;AO-Childes&#21644;Open Subtitles&#65289;&#30340;&#26292;&#38706;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#23558;&#36755;&#20837;&#25968;&#25454;&#20197;&#19981;&#21516;&#26041;&#24335;&#21576;&#29616;&#32473;&#27169;&#22411;&#30340;&#26041;&#27861;&#24471;&#20986;&#20102;&#36825;&#19968;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#24207;&#21015;&#32423;&#22797;&#26434;&#24615;&#30340;&#23398;&#20064;&#35745;&#21010;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#8220;&#22359;&#8221;&#30340;&#24433;&#21709;&#8212;&#8212;&#36825;&#20123;&#22359;&#35206;&#30422;&#20102;&#28304;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#35821;&#26009;&#24211;&#20013;&#27599;&#20010;&#26631;&#35760;&#25968;&#37327;&#24179;&#34913;&#30340;&#25991;&#26412;&#33539;&#22260;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#31243;&#24230;&#22320;&#35753;&#27169;&#22411;&#25509;&#35302;&#19981;&#21516;&#35821;&#26009;&#24211;&#30340;&#23398;&#20064;&#35745;&#21010;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#36807;&#24230;&#25509;&#35302;AO-Childes&#21644;Open Subtitles&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21487;&#27604;&#36739;&#30340;&#25511;&#21046;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#20123;&#21457;&#29616;&#65292;&#35813;&#25968;&#25454;&#38598;&#20013;&#26333;&#20809;&#31243;&#24230;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work explores the degree to which grammar acquisition is driven by language `simplicity' and the source modality (speech vs. text) of data. Using BabyBERTa as a probe, we find that grammar acquisition is largely driven by exposure to speech data, and in particular through exposure to two of the BabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this finding by examining various ways of presenting input data to our model. First, we assess the impact of various sequence-level complexity based curricula. We then examine the impact of learning over `blocks' -- covering spans of text that are balanced for the number of tokens in each of the source corpora (rather than number of lines). Finally, we explore curricula that vary the degree to which the model is exposed to different corpora. In all cases, we find that over-exposure to AO-Childes and Open Subtitles significantly drives performance. We verify these findings through a comparable control dataset in which expos
&lt;/p&gt;</description></item><item><title>CapsFusion&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.20550</link><description>&lt;p&gt;
CapsFusion: &#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20550
&lt;/p&gt;
&lt;p&gt;
CapsFusion&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#25191;&#34892;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26174;&#33879;&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#35268;&#27169;&#22522;&#20110;&#32593;&#32476;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#22312;&#36825;&#19968;&#25104;&#21151;&#20013;&#36215;&#30528;&#26681;&#26412;&#24615;&#30340;&#36129;&#29486;&#65292;&#20294;&#23384;&#22312;&#30528;&#36807;&#22810;&#30340;&#22122;&#22768;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#30001;&#29983;&#25104;&#24335;&#23383;&#24149;&#27169;&#22411;&#21512;&#25104;&#30340;&#26367;&#20195;&#23383;&#24149;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22522;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#21512;&#25104;&#23383;&#24149;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#21487;&#25193;&#23637;&#24615;&#19981;&#36275;&#21644;&#19990;&#30028;&#30693;&#35782;&#20007;&#22833;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#20854;&#21021;&#22987;&#22522;&#20934;&#25104;&#21151;&#20013;&#22823;&#37096;&#20998;&#34987;&#25513;&#30422;&#20102;&#12290;&#32463;&#36807;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#26159;&#29616;&#26377;&#21512;&#25104;&#23383;&#24149;&#20013;&#36807;&#20110;&#31616;&#21270;&#30340;&#35821;&#35328;&#32467;&#26500;&#21644;&#32570;&#20047;&#30693;&#35782;&#32454;&#33410;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CapsFusion&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#25968;&#25454;&#20197;&#25913;&#21892;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20033</link><description>&lt;p&gt;
&#29992;&#20110;&#20020;&#24202;&#24635;&#32467;&#20013;&#20107;&#23454;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#20223;&#32534;&#36753;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#25968;&#25454;&#20197;&#25913;&#21892;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#21644;LLaMA&#31995;&#21015;&#22312;&#25429;&#25417;&#21644;&#27987;&#32553;&#20851;&#38190;&#19978;&#19979;&#25991;&#20449;&#24687;&#21450;&#22312;&#24635;&#32467;&#20219;&#21153;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24322;&#24120;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#31038;&#21306;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#34394;&#26500;&#38382;&#39064;&#30340;&#25285;&#24551;&#20173;&#22312;&#19981;&#26029;&#19978;&#21319;&#12290;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#34394;&#26500;&#30340;&#25688;&#35201;&#65292;&#36825;&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;NLP&#20219;&#21153;&#65288;&#20363;&#22914;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#65289;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#38169;&#35823;&#30340;&#35786;&#26029;&#12290;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23454;&#29616;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#25215;&#35834;&#65292;&#20294;&#36825;&#31181;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#65292;&#32780;&#22312;&#20020;&#24202;&#39046;&#22495;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31649;&#36947;&#65292;&#20351;&#29992;ChatGPT&#20195;&#26367;&#20154;&#31867;&#19987;&#23478;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#20197;&#25913;&#21892;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summari
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;</title><link>http://arxiv.org/abs/2310.17688</link><description>&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#26102;&#20195;&#31649;&#29702;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17688
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#20849;&#35782;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#22823;&#35268;&#27169;&#30340;&#31038;&#20250;&#21361;&#23475;&#21644;&#24694;&#24847;&#20351;&#29992;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22833;&#21435;&#25511;&#21046;&#30340;&#19981;&#21487;&#36870;&#36716;&#30340;&#25439;&#22833;&#12290;&#37492;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21644;&#25345;&#32493;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#21457;&#21644;&#27835;&#29702;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&amp;D and governance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;&#21644;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#21644;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16789</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Detecting Pretraining Data from Large Language Models. (arXiv:2310.16789v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;&#21644;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#21644;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#29992;&#20110;&#35757;&#32451;&#23427;&#20204;&#30340;&#25968;&#25454;&#24456;&#23569;&#34987;&#20844;&#24320;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#25968;&#25454;&#30340;&#35268;&#27169;&#20043;&#22823;&#65292;&#21487;&#33021;&#21253;&#21547;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#26448;&#26009;&#12289;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#20197;&#21450;&#29992;&#20110;&#24191;&#27867;&#25253;&#36947;&#30340;&#21442;&#32771;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#25105;&#20204;&#20960;&#20046;&#21487;&#20197;&#32943;&#23450;&#23427;&#20204;&#21253;&#21547;&#20102;&#28508;&#22312;&#30340;&#38382;&#39064;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30446;&#21069;&#26080;&#27861;&#30693;&#36947;&#36825;&#20123;&#25991;&#26412;&#20013;&#21253;&#21547;&#20102;&#21738;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#20197;&#21450;&#27604;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#26816;&#27979;&#38382;&#39064;&#65306;&#22312;&#19981;&#30693;&#36947;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#27573;&#25991;&#26412;&#21644;&#23545;LLM&#30340;&#40657;&#30418;&#35775;&#38382;&#65292;&#25105;&#20204;&#33021;&#21542;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#26159;&#22312;&#25552;&#20379;&#30340;&#25991;&#26412;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65311;&#20026;&#20102;&#26041;&#20415;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;WIKIMIA&#65292;&#20351;&#29992;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#21644;&#20043;&#21518;&#21019;&#24314;&#30340;&#25968;&#25454;&#26469;&#25903;&#25345;&#37329;&#26631;&#20934;&#26816;&#27979;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;Min-K% Prob&#65292;&#22522;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20551;&#35774;&#65306;&#19968;&#20010;&#26410;&#35265;&#36807;&#30340;&#20363;&#23376;&#21487;&#33021;&#21253;&#21547;&#20960;&#20010;&#20855;&#26377;&#36739;&#20302;&#27010;&#29575;&#30340;&#31163;&#32676;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low pro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#20687;&#24459;&#24072;&#19968;&#26679;&#20351;&#29992;IRAC&#26041;&#27861;&#20998;&#26512;&#27861;&#24459;&#24773;&#26223;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#39532;&#26469;&#35199;&#20122;&#21512;&#21516;&#27861;&#21644;&#28595;&#22823;&#21033;&#20122;&#31038;&#20250;&#27861;&#24773;&#26223;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;IRAC&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#20998;&#26512;&#65292;&#20316;&#32773;&#21457;&#29616;ChatGPT&#22312;&#27861;&#24459;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.14880</link><description>&lt;p&gt;
ChatGPT&#33021;&#20687;&#24459;&#24072;&#19968;&#26679;&#20351;&#29992;IRAC&#26041;&#27861;&#20998;&#26512;&#27861;&#24459;&#24773;&#26223;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?. (arXiv:2310.14880v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#20687;&#24459;&#24072;&#19968;&#26679;&#20351;&#29992;IRAC&#26041;&#27861;&#20998;&#26512;&#27861;&#24459;&#24773;&#26223;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#39532;&#26469;&#35199;&#20122;&#21512;&#21516;&#27861;&#21644;&#28595;&#22823;&#21033;&#20122;&#31038;&#20250;&#27861;&#24773;&#26223;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;IRAC&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#20998;&#26512;&#65292;&#20316;&#32773;&#21457;&#29616;ChatGPT&#22312;&#27861;&#24459;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;(&#22914;ChatGPT)&#22312;&#27861;&#24459;&#39046;&#22495;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#20854;&#31361;&#20986;&#30340;&#33021;&#21147;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;LLMs&#26159;&#21542;&#33021;&#22815;&#20687;&#24459;&#24072;&#37027;&#26679;&#20998;&#26512;&#27861;&#24459;&#26696;&#20363;&#24182;&#36827;&#34892;&#25512;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#39532;&#26469;&#35199;&#20122;&#21512;&#21516;&#27861;&#21644;&#28595;&#22823;&#21033;&#20122;&#31038;&#20250;&#27861;&#20851;&#20110;&#34987;&#36193;&#20859;&#20799;&#31461;&#30340;&#24773;&#26223;&#12290;&#25105;&#20204;&#20351;&#29992;IRAC&#26041;&#27861;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#65292;IRAC&#26041;&#27861;&#26159;&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#24191;&#27867;&#20351;&#29992;&#30340;&#32452;&#32455;&#27861;&#24459;&#20998;&#26512;&#30340;&#26694;&#26550;&#12290;&#35821;&#26009;&#24211;&#20013;&#30340;&#27599;&#20010;&#24773;&#26223;&#37117;&#20197;&#21322;&#32467;&#26500;&#21270;&#26684;&#24335;&#27880;&#37322;&#20102;&#23436;&#25972;&#30340;IRAC&#20998;&#26512;&#65292;&#20197;&#20415;&#26426;&#22120;&#21644;&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#33021;&#22815;&#35299;&#37322;&#21644;&#29702;&#35299;&#36825;&#20123;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;ChatGPT&#36827;&#34892;IRAC&#20998;&#26512;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#20197;&#20102;&#35299;&#20854;&#19982;&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#20998;&#26512;&#30340;&#21563;&#21512;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, have drawn a lot of attentions recently in the legal domain due to its emergent ability to tackle a variety of legal tasks. However, it is still unknown if LLMs are able to analyze a legal case and perform reasoning in the same manner as lawyers. Therefore, we constructed a novel corpus consisting of scenarios pertain to Contract Acts Malaysia and Australian Social Act for Dependent Child. ChatGPT is applied to perform analysis on the corpus using the IRAC method, which is a framework widely used by legal professionals for organizing legal analysis. Each scenario in the corpus is annotated with a complete IRAC analysis in a semi-structured format so that both machines and legal professionals are able to interpret and understand the annotations. In addition, we conducted the first empirical assessment of ChatGPT for IRAC analysis in order to understand how well it aligns with the analysis of legal professionals. Our experimental results she
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.09219</link><description>&lt;p&gt;
"&#20975;&#21033;&#26159;&#19968;&#20010;&#28201;&#26262;&#30340;&#20154;&#65292;&#32422;&#29791;&#22827;&#26159;&#19968;&#20010;&#27036;&#26679;": LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#29992;&#25143;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21327;&#21161;&#25776;&#20889;&#21508;&#31181;&#31867;&#22411;&#30340;&#20869;&#23481;&#65292;&#21253;&#25324;&#25512;&#33616;&#20449;&#31561;&#32844;&#19994;&#25991;&#20214;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#26041;&#20415;&#24615;&#65292;&#20294;&#36825;&#20123;&#24212;&#29992;&#24341;&#20837;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#30001;&#20110;&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#21487;&#33021;&#34987;&#29992;&#25143;&#30452;&#25509;&#22312;&#32844;&#19994;&#25110;&#23398;&#26415;&#22330;&#26223;&#20013;&#20351;&#29992;&#65292;&#23427;&#20204;&#26377;&#21487;&#33021;&#36896;&#25104;&#30452;&#25509;&#30340;&#31038;&#20250;&#20260;&#23475;&#65292;&#22914;&#38477;&#20302;&#22899;&#24615;&#30003;&#35831;&#32773;&#30340;&#25104;&#21151;&#29575;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#26469;&#30340;&#32531;&#35299;&#21644;&#30417;&#25511;&#65292;&#20840;&#38754;&#30740;&#31350;&#27492;&#31867;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#21644;&#30456;&#20851;&#20260;&#23475;&#21183;&#22312;&#24517;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#30740;&#31350;&#12290;&#21463;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#32500;&#24230;&#26469;&#23637;&#29616;LLM&#29983;&#25104;&#30340;&#20449;&#20214;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#35821;&#35328;&#39118;&#26684;&#30340;&#20559;&#35265;&#21644;&#35789;&#27719;&#20869;&#23481;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25512;&#33616;&#20449;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative language models advance, users have started to utilize Large Language Models (LLMs) to assist in writing various types of content, including professional documents such as recommendation letters. Despite their convenience, these applications introduce unprecedented fairness concerns. As generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. Therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. In this paper, we critically examine gender bias in LLM-generated reference letters. Inspired by findings in social science, we design evaluation methods to manifest gender biases in LLM-generated letters through 2 dimensions: biases in language style and biases in lexical content. Furthermore, we investigate the ext
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#21644;&#23545;&#40784;&#20004;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#19982;&#25552;&#20379;&#30340;&#30693;&#35782;&#28304;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#22238;&#22797;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08372</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#21644;&#23545;&#40784;&#26469;&#25552;&#21319;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment. (arXiv:2310.08372v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#21644;&#23545;&#40784;&#20004;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#19982;&#25552;&#20379;&#30340;&#30693;&#35782;&#28304;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#22238;&#22797;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#23481;&#26131;&#29983;&#25104;&#19982;&#25552;&#20379;&#30340;&#30693;&#35782;&#28304;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#22238;&#22797;&#12290;&#22312;&#36825;&#31181;&#19981;&#19968;&#33268;&#30340;&#22238;&#22797;&#20013;&#65292;&#23545;&#35805;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#34920;&#36798;&#20854;&#20381;&#36182;&#30340;&#22806;&#37096;&#30693;&#35782;&#12290;&#21463;&#20808;&#21069;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#35813;&#24037;&#20316;&#21457;&#29616;&#21464;&#21387;&#22120;&#20013;&#30340;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;&#36127;&#36131;&#20107;&#23454;&#30693;&#35782;&#30340;&#34920;&#36798;&#65292;&#22240;&#27492;&#25105;&#20204;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#21644;&#23545;&#40784;&#20004;&#31181;&#26041;&#27861;&#65292;&#23545;FFNs&#30340;&#20107;&#23454;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#39640;&#25928;&#25913;&#36827;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;K-Dial&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#21387;&#22120;&#20013;&#30340;&#25193;&#23637;FFNs&#20197;&#22686;&#24378;&#29305;&#23450;&#27169;&#24335;&#30340;&#30693;&#35782;&#23545;&#35805;&#36755;&#20837;&#30340;&#20107;&#23454;&#30693;&#35782;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23545;FFNs&#22312;&#22238;&#22797;&#20013;&#30340;&#34920;&#36798;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20197;&#20351;&#20854;&#19982;&#20107;&#23454;&#19968;&#33268;&#30340;&#26368;&#20248;&#30693;&#35782;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source. In such inconsistent responses, the dialogue models fail to accurately express the external knowledge they rely upon. Inspired by previous work which identified that feed-forward networks (FFNs) within Transformers are responsible for factual knowledge expressions, we investigate two methods to efficiently improve the factual expression capability {of FFNs} by knowledge enhancement and alignment respectively. We first propose \textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers to enhance factual knowledge expressions} given the specific patterns of knowledge-grounded dialogue inputs. Additionally, we apply the reinforcement learning for factual consistency (RLFC) method to implicitly adjust FFNs' expressions in responses by aligning with gold knowledge for the factual consistency prefere
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#25277;&#21462;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#21462;&#20102;&#20844;&#21496;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#30340;&#32467;&#26500;&#21270;ESG&#30456;&#20851;&#20449;&#24687;&#65292;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#31616;&#27905;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#21487;&#34892;&#21160;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.05628</link><description>&lt;p&gt;
&#20174;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#23548;&#20986;&#32467;&#26500;&#21270;&#35265;&#35299;&#65306;&#38378;&#20809;&#36824;&#26159;&#40644;&#37329;&#65311;
&lt;/p&gt;
&lt;p&gt;
Glitter or Gold? Deriving Structured Insights from Sustainability Reports via Large Language Models. (arXiv:2310.05628v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05628
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#25277;&#21462;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#21462;&#20102;&#20844;&#21496;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#30340;&#32467;&#26500;&#21270;ESG&#30456;&#20851;&#20449;&#24687;&#65292;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#31616;&#27905;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#21487;&#34892;&#21160;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#37492;&#20110;&#25237;&#36164;&#32773;&#23545;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#38382;&#39064;&#36234;&#26469;&#36234;&#20851;&#27880;&#65292;&#19968;&#20123;&#30417;&#31649;&#26426;&#26500;&#24320;&#22987;&#35201;&#27714;&#19978;&#24066;&#20844;&#21496;&#25259;&#38706;&#38750;&#36130;&#21153;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#20197;&#21508;&#31181;&#38750;&#32467;&#26500;&#21270;&#30340;&#22810;&#27169;&#24577;&#25991;&#26723;&#24418;&#24335;&#20844;&#24320;&#21457;&#24067;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#25968;&#25454;&#32858;&#21512;&#21644;&#25972;&#21512;&#21040;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#23548;&#20986;&#36328;&#20844;&#21496;&#21644;&#24066;&#22330;&#30340;&#21487;&#25345;&#32493;&#24615;&#23454;&#36341;&#35265;&#35299;&#24182;&#19981;&#30452;&#35266;&#12290;&#37492;&#20110;&#36825;&#20123;&#21069;&#25552;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#37319;&#29992;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#25216;&#26415;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#31616;&#27905;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#21487;&#34892;&#21160;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31361;&#30772;&#20102;&#20256;&#32479;&#30340;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#32467;&#21512;&#31361;&#20986;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#65292;&#20174;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#25552;&#21462;&#20855;&#26377;&#35821;&#20041;&#32467;&#26500;&#30340;&#19982;ESG&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, several regulatory bodies have started requiring the disclosure of non-financial information from publicly listed companies, in light of the investors' increasing attention to Environmental, Social, and Governance (ESG) issues. Such information is publicly released in a variety of non-structured and multi-modal documentation. Hence, it is not straightforward to aggregate and consolidate such data in a cohesive framework to further derive insights about sustainability practices across companies and markets. Given these premises, it is natural to resort to Information Extraction (IE) techniques to provide concise, informative, and actionable data to the stakeholders. Moving beyond traditional text processing techniques, in this work we leverage Large Language Models (LLMs), along with the prominent in-context learning technique and the Retrieved Augmented Generation (RAG) paradigm, to extract semantically structured ESG-related information from companies' sustainabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#24402;&#22240;&#20998;&#25968;&#30340;&#36830;&#32493;&#23646;&#24615;&#26469;&#30830;&#23450;&#24212;&#26174;&#31034;&#30340;&#26368;&#20339; k &#20010;&#26631;&#35760;&#30340;&#21160;&#24577; Top-k &#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25972;&#21512;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21160;&#24577; k &#20027;&#35201;&#25913;&#36827;&#20102;&#38598;&#25104;&#26799;&#24230;&#21644; GradientXInput &#30340;&#34920;&#29616;&#65292;&#20026;&#20154;&#31867;&#35299;&#37322;&#25552;&#20379;&#20102;&#20855;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#24402;&#22240;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2310.05619</link><description>&lt;p&gt;
&#21160;&#24577; Top-k &#20272;&#35745;&#26041;&#27861;&#29992;&#20110;&#25972;&#21512;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20043;&#38388;&#30340;&#20998;&#27495;
&lt;/p&gt;
&lt;p&gt;
Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods. (arXiv:2310.05619v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#24402;&#22240;&#20998;&#25968;&#30340;&#36830;&#32493;&#23646;&#24615;&#26469;&#30830;&#23450;&#24212;&#26174;&#31034;&#30340;&#26368;&#20339; k &#20010;&#26631;&#35760;&#30340;&#21160;&#24577; Top-k &#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25972;&#21512;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21160;&#24577; k &#20027;&#35201;&#25913;&#36827;&#20102;&#38598;&#25104;&#26799;&#24230;&#21644; GradientXInput &#30340;&#34920;&#29616;&#65292;&#20026;&#20154;&#31867;&#35299;&#37322;&#25552;&#20379;&#20102;&#20855;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#24402;&#22240;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#20998;&#25968;&#29992;&#20110;&#36890;&#36807;&#31361;&#20986;&#26174;&#31034; k &#20010;&#26631;&#35760;&#26469;&#21521;&#29992;&#25143;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#24402;&#22240;&#20998;&#25968;&#30340;&#36830;&#32493;&#23646;&#24615;&#26469;&#30830;&#23450;&#24212;&#26174;&#31034;&#30340;&#26368;&#20339; k &#20010;&#26631;&#35760;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21477;&#23376;&#20043;&#38388;&#26159;&#21160;&#24577;&#30340;&#65292;&#19981;&#20381;&#36182;&#20110;&#20855;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#21477;&#23376;&#38271;&#24230;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312; NLI &#20219;&#21153;&#20013;&#27604;&#36739;&#22810;&#31181;&#26041;&#27861;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20351;&#29992;&#22266;&#23450;&#30340; k &#21644;&#21160;&#24577;&#30340; k&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;&#38745;&#24577;&#30340; k &#26102;&#65292;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#21644; Vanilla Gradient &#22312;&#22823;&#22810;&#25968;&#26041;&#27861;&#20043;&#38388;&#21644;&#26041;&#27861;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#25351;&#26631;&#19978;&#34920;&#29616;&#24471;&#26368;&#22909;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20351;&#29992;&#21160;&#24577; k &#26102;&#30340;&#20248;&#21183;&#28040;&#22833;&#20102;&#65292;&#32780;&#21160;&#24577; k &#20027;&#35201;&#25913;&#36827;&#20102;&#38598;&#25104;&#26799;&#24230;&#21644; GradientXInput &#30340;&#34920;&#29616;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#35777;&#26126;&#36890;&#36807;&#20998;&#26512;&#24402;&#22240;&#20998;&#25968;&#30340;&#36830;&#32493;&#23646;&#24615;&#23545;&#20110;&#25972;&#21512;&#20154;&#31867;&#35299;&#37322;&#30340;&#24402;&#22240;&#20449;&#21495;&#26159;&#20855;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution scores are used for explaining the prediction of a text classifier to users by highlighting a k number of tokens. In this work, we propose a way to determine the number of optimal k tokens that should be displayed from sequential properties of the attribution scores. Our approach is dynamic across sentences, method-agnostic, and deals with sentence length bias. We compare agreement between multiple methods and humans on an NLI task, using fixed k and dynamic k. We find that perturbation-based methods and Vanilla Gradient exhibit highest agreement on most method--method and method--human agreement metrics with a static k. Their advantage over other methods disappears with dynamic ks which mainly improve Integrated Gradient and GradientXInput. To our knowledge, this is the first evidence that sequential properties of attribution scores are informative for consolidating attribution signals for human interpretation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.05280</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#38543;&#26426;&#40550;&#40521;&#26356;&#21361;&#38505;&#21527;&#65311;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#20854;&#33021;&#22815;&#25353;&#29031;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#65292;&#21253;&#25324;&#22312;&#23545;&#35805;&#20013;&#27169;&#20223;&#36890;&#29992;&#25110;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#20154;&#26684;&#12290;&#36890;&#29992;&#20154;&#26684;&#25351;&#30340;&#26159;&#26469;&#33258;&#26576;&#19968;&#20154;&#21475;&#32676;&#20307;&#30340;&#20010;&#20307;&#65288;&#20363;&#22914;&#20122;&#27954;&#20154;&#65289;&#65292;&#32780;&#29305;&#23450;&#20154;&#26684;&#21487;&#20197;&#26159;&#21382;&#21490;&#20154;&#29289;&#30340;&#23454;&#38469;&#22995;&#21517;&#12290;&#34429;&#28982;&#37319;&#29992;&#20154;&#26684;&#20351;&#23545;&#35805;&#31995;&#32479;&#26356;&#20855;&#21560;&#24341;&#21147;&#21644;&#20146;&#21644;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#65292;&#21487;&#33021;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#32780;&#21152;&#21095;&#31038;&#20250;&#20559;&#35265;&#65292;&#36827;&#19968;&#27493;&#36896;&#25104;&#31038;&#20250;&#20260;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#8220;&#20154;&#26684;&#20559;&#35265;&#8221;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26377;&#23475;&#23545;&#35805;&#27169;&#22411;&#34892;&#20026;&#23545;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23558;&#20154;&#26684;&#20559;&#35265;&#20998;&#20026;&#26377;&#23475;&#34920;&#36798;&#21644;&#26377;&#23475;&#35748;&#21516;&#20004;&#31867;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#34913;&#37327;&#20116;&#20010;&#26041;&#38754;&#30340;&#20154;&#26684;&#20559;&#35265;&#65306;&#20882;&#29359;&#24615;&#12289;&#26377;&#27602;&#24310;&#32493;&#12289;&#20851;&#24576;&#12289;&#21051;&#26495;&#21360;&#35937;&#30340;&#35748;&#21516;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#27979;&#35797;&#32467;&#26524;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#23384;&#22312;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#21487;&#35266;&#23519;&#21040;&#19968;&#33268;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.02553</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#32763;&#35793;&#30340;&#34892;&#20026;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#27979;&#35797;&#32467;&#26524;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#23384;&#22312;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#21487;&#35266;&#23519;&#21040;&#19968;&#33268;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#20013;&#30340;&#34892;&#20026;&#27979;&#35797;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#34892;&#20026;&#26469;&#32454;&#31890;&#24230;&#35780;&#20272;&#31995;&#32479;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#26426;&#22120;&#32763;&#35793;&#20013;&#34892;&#20026;&#27979;&#35797;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#27979;&#35797;&#33539;&#22260;&#26377;&#38480;&#12289;&#28085;&#30422;&#30340;&#35821;&#35328;&#31181;&#31867;&#20063;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#28304;&#21477;&#23376;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22791;&#36873;&#38598;&#65292;&#20197;&#39564;&#35777;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#26159;&#21542;&#34920;&#29616;&#20986;&#39044;&#26399;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20351;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#34892;&#20026;&#27979;&#35797;&#23454;&#38469;&#21487;&#34892;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#21147;&#25237;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#35780;&#20272;&#26694;&#26550;&#24212;&#29992;&#20110;&#22810;&#20010;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#24635;&#20307;&#19978;&#36890;&#36807;&#29575;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#21487;&#35266;&#23519;&#21040;&#30340;&#36235;&#21183;&#30456;&#31526;&#65292;&#20294;&#20173;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02618</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;GTFS: &#20174;&#25991;&#23383;&#21040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20132;&#36890;&#34892;&#25968;&#25454;&#21457;&#24067;&#26631;&#20934;General Transit Feed Specification&#65288;GTFS&#65289;&#26159;&#34920;&#26684;&#25968;&#25454;&#65292;&#20449;&#24687;&#20998;&#25955;&#22312;&#19981;&#21516;&#30340;&#25991;&#20214;&#20013;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#24037;&#20855;&#25110;&#21253;&#26469;&#26816;&#32034;&#20449;&#24687;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#36235;&#21183;&#20063;&#22312;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#30340;&#24819;&#27861;&#26159;&#30475;&#30475;&#24403;&#21069;&#24191;&#27867;&#37319;&#29992;&#30340;LLMs&#65288;ChatGPT&#65289;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20174;GTFS&#20013;&#26816;&#32034;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#27979;&#35797;ChatGPT&#65288;GPT-3.5&#65289;&#26159;&#21542;&#29702;&#35299;GTFS&#35268;&#33539;&#12290;GPT-3.5&#22312;&#25105;&#20204;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQ&#65289;&#20013;&#27491;&#30830;&#22238;&#31572;&#20102;77%&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#36807;&#28388;&#30340;GTFS&#25968;&#25454;&#38598;&#23545;LLM&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#12290;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#38646;-shot&#21644;&#31243;&#24207;&#21512;&#25104;&#12290;&#31243;&#24207;&#21512;&#25104;&#30340;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#31616;&#21333;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;90%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;40%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#35270;&#22120;&#24341;&#23548;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#31867;&#22411;&#12289;&#21151;&#33021;&#25110;API&#31561;&#20840;&#23616;&#19978;&#19979;&#25991;&#26102;&#65292;&#33021;&#22815;&#25552;&#39640;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10763</link><description>&lt;p&gt;
&#20351;&#29992;&#30417;&#35270;&#22120;&#24341;&#23548;&#20840;&#23616;&#19978;&#19979;&#25991;&#25351;&#23548;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Guiding Language Models of Code with Global Context using Monitors. (arXiv:2306.10763v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#35270;&#22120;&#24341;&#23548;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#31867;&#22411;&#12289;&#21151;&#33021;&#25110;API&#31561;&#20840;&#23616;&#19978;&#19979;&#25991;&#26102;&#65292;&#33021;&#22815;&#25552;&#39640;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21608;&#22260;&#20195;&#30721;&#25552;&#20379;&#36275;&#22815;&#19978;&#19979;&#25991;&#26102;&#25928;&#26524;&#24456;&#22909;&#12290;&#20294;&#24403;&#38656;&#35201;&#22312;&#23384;&#20648;&#24211;&#25110;&#38142;&#25509;&#24211;&#20013;&#20351;&#29992;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#22411;&#12289;&#21151;&#33021;&#25110;API&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#23601;&#19981;&#20877;&#25104;&#31435;&#12290;LMs&#22312;&#23545;&#36825;&#31181;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#24847;&#35782;&#26377;&#38480;&#26102;&#20250;&#20986;&#29616;&#38169;&#35823;&#39044;&#27979;&#30340;&#24773;&#20917;&#12290;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDEs&#65289;&#36890;&#36807;&#38745;&#24577;&#20998;&#26512;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#20102;&#35299;&#23384;&#20648;&#24211;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#23558;&#24320;&#21457;&#20154;&#21592;&#20139;&#21463;&#21040;&#30340;&#36825;&#31181;&#24110;&#21161;&#25193;&#23637;&#21040;&#20102;LMs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#35270;&#22120;&#24341;&#23548;&#35299;&#30721;&#65288;MGD&#65289;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#30417;&#35270;&#22120;&#20351;&#29992;&#38745;&#24577;&#20998;&#26512;&#26469;&#24341;&#23548;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;Java&#26041;&#27861;&#34917;&#20840;&#30340;&#23384;&#20648;&#24211;&#32423;&#25968;&#25454;&#38598;PragmaticCode&#65292;&#24182;&#22312;&#20854;&#19978;&#35780;&#20272;&#20102;MGD&#12290;&#22312;&#19981;&#21516;&#21442;&#25968;&#35268;&#27169;&#30340;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#30417;&#35270;&#31867;&#22411;&#19968;&#33268;&#30340;&#23545;&#35937;&#35299;&#24341;&#29992;&#65292;MGD&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#32534;&#35793;&#29575;&#24182;&#19982;&#30495;&#23454;&#32467;&#26524;&#36798;&#25104;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#26356;&#23569;&#21442;&#25968;&#30340;LMs&#65292;&#22312;&#19982;MGD&#30456;&#32467;&#21512;&#26102;&#33021;&#22815;&#36229;&#36234;&#26356;&#22823;&#30340;LMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.  Integrated development environments (IDEs) assist developers in understanding repository context using static analysis. We extend this assistance, enjoyed by developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LM
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;APA&#65292;&#20854;&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65288;PPO&#65289;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#30340;&#23849;&#28291;&#19982;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02231</link><description>&lt;p&gt;
&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#30340;Fine-Tuning&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;APA&#65292;&#20854;&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65288;PPO&#65289;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#30340;&#23849;&#28291;&#19982;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#21487;&#38752;&#26041;&#27861;&#12290;&#22312;&#20247;&#22810;RLHF&#25216;&#26415;&#20013;&#65292;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;PPO&#24456;&#27969;&#34892;&#65292;&#20294;&#23427;&#21487;&#33021;&#20250;&#36973;&#21463;&#27169;&#24335;&#23849;&#28291;&#12289;&#19981;&#31283;&#23450;&#21644;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;--&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#65288;APA&#65289;&#65292;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#26102;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#22987;&#32456;&#27604;PPO&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;PPO&#30456;&#27604;&#65292;APA&#21487;&#20197;&#26356;&#31283;&#23450;&#22320;&#25511;&#21046;&#27169;&#22411;&#19982;&#21021;&#22987;&#31574;&#30053;&#30340;&#20559;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#20250;&#23849;&#28291;&#20026;&#30830;&#23450;&#24615;&#36755;&#20986;&#12290;&#38500;&#20102;&#32463;&#39564;&#32467;&#26524;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;APA&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19987;&#29992;&#30340;&#35821;&#27861;&#26469;&#22686;&#24378;&#31034;&#20363;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#21644;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.19234</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#35821;&#27861;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Grammar Prompting for Domain-Specific Language Generation with Large Language Models. (arXiv:2305.19234v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19987;&#29992;&#30340;&#35821;&#27861;&#26469;&#22686;&#24378;&#31034;&#20363;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#21644;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#20174;&#20165;&#26377;&#20960;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20174;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#35821;&#35328;&#65288;&#20363;&#22914;&#65292;&#20174;&#35821;&#20041;&#35299;&#26512;&#21040;&#22797;&#26434;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65289;&#29983;&#25104;&#23383;&#31526;&#20018;&#65292;LLM&#21482;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#36827;&#34892;&#27867;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;$\textbf{&#35821;&#27861;&#25552;&#31034;}$&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32972;&#31185;&#26031;-&#35834;&#23572;&#33539;&#24335;&#65288;BNF&#65289;&#20013;&#34920;&#36798;&#30340;&#35821;&#27861;&#26469;&#21551;&#29992;LLM&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#21644;&#29305;&#23450;&#39046;&#22495;&#30340;&#32422;&#26463;&#26465;&#20214;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35821;&#27861;&#25552;&#31034;&#20351;&#29992;&#19968;&#20010;&#19987;&#38376;&#30340;&#35821;&#27861;&#26469;&#22686;&#24378;&#27599;&#20010;&#28436;&#31034;&#31034;&#20363;&#65292;&#35813;&#35821;&#27861;&#36275;&#20197;&#29983;&#25104;&#29305;&#23450;&#30340;&#36755;&#20986;&#31034;&#20363;&#65292;&#20854;&#20013;&#35813;&#19987;&#38376;&#30340;&#35821;&#27861;&#26159;&#20840;DSL&#35821;&#27861;&#30340;&#23376;&#38598;&#12290;&#23545;&#20110;&#25512;&#29702;&#65292;LLM&#39318;&#20808;&#39044;&#27979;&#19968;&#20010;&#32473;&#23450;&#27979;&#35797;&#36755;&#20837;&#30340;BNF&#35821;&#27861;&#65292;&#28982;&#21518;&#26681;&#25454;&#35821;&#27861;&#35268;&#21017;&#29983;&#25104;&#36755;&#20986;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35821;&#27861;&#25552;&#31034;&#21487;&#20197;&#20351;LLM&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We explore $\textbf{grammar prompting}$ as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25193;&#25955;-&#35821;&#35328;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#22522;&#20110;7&#20010;&#35270;&#35273;&#35821;&#35328;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;CLIP&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16397</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#26159;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25193;&#25955;-&#35821;&#35328;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#22522;&#20110;7&#20010;&#35270;&#35273;&#35821;&#35328;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;CLIP&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23450;&#24615;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19982;&#37492;&#21035;&#24335;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#19981;&#21516;&#65292;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#32622;&#20110;&#33258;&#21160;&#32454;&#31890;&#24230;&#23450;&#37327;&#35780;&#20272;&#39640;&#32423;&#29616;&#35937;&#65288;&#22914;&#32452;&#21512;&#24615;&#65289;&#30340;&#20219;&#21153;&#20013;&#26159;&#19968;&#39033;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#20004;&#39033;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;DiffusionITM&#30340;&#26032;&#26041;&#27861;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#31283;&#23450;&#25193;&#25955;&#65289;&#36716;&#25442;&#20026;&#20219;&#20309;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;(ITM)&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;7&#20010;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12289;&#20559;&#24046;&#35780;&#20272;&#21644;&#35814;&#32454;&#20998;&#26512;&#30340;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Stable Diffusion + DiffusionITM&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#65288;&#22914;CLEVR&#21644;Winoground&#31561;&#65289;&#19978;&#20248;&#20110;CLIP&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MS-COCO&#19978;&#24494;&#35843;&#20445;&#25345;&#22270;&#20687;&#29305;&#24449;&#30340;&#36716;&#31227;&#35774;&#32622;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM. Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining ge
&lt;/p&gt;</description></item><item><title>LLMDet&#26159;&#19968;&#20010;&#31532;&#19977;&#26041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#65292;&#33021;&#22815;&#20174;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#26469;&#28304;&#65292;&#24182;&#28385;&#36275;&#31934;&#32454;&#36861;&#36394;&#12289;&#20013;&#38388;&#21028;&#26029;&#21644;&#24555;&#36895;&#26816;&#27979;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.15004</link><description>&lt;p&gt;
LLMDet:&#19968;&#31181;&#31532;&#19977;&#26041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
LLMDet: A Third Party Large Language Models Generated Text Detection Tool. (arXiv:2305.15004v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15004
&lt;/p&gt;
&lt;p&gt;
LLMDet&#26159;&#19968;&#20010;&#31532;&#19977;&#26041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#65292;&#33021;&#22815;&#20174;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#26469;&#28304;&#65292;&#24182;&#28385;&#36275;&#31934;&#32454;&#36861;&#36394;&#12289;&#20013;&#38388;&#21028;&#26029;&#21644;&#24555;&#36895;&#26816;&#27979;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#25776;&#20889;&#25991;&#26412;&#38750;&#24120;&#30456;&#20284;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;&#22312;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#21644;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#20013;&#30340;&#28508;&#22312;&#28389;&#29992;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#39640;&#24230;&#23454;&#29992;&#30340;&#26816;&#27979;&#24037;&#20855;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#32473;&#23450;&#25991;&#26412;&#30340;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;LLM&#30340;&#35775;&#38382;&#65292;&#24182;&#19988;&#21482;&#33021;&#21306;&#20998;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#24037;&#25776;&#20889;&#30340;&#25991;&#26412;&#65292;&#26410;&#33021;&#28385;&#36275;&#31934;&#32454;&#36861;&#36394;&#12289;&#20013;&#38388;&#21028;&#26029;&#21644;&#24555;&#36895;&#26816;&#27979;&#30340;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMDet&#65292;&#19968;&#31181;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#23433;&#20840;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#26816;&#27979;&#24037;&#20855;&#65292;&#21487;&#20197;&#20174;&#29305;&#23450;&#30340;LLM&#65288;&#22914;GPT-2&#12289;OPT&#12289;LLaMA&#31561;&#65289;&#20013;&#33719;&#21462;&#25991;&#26412;&#12290;&#22312;LLMDet&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#26174;&#33879;n-gram&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#27010;&#29575;&#20316;&#20026;&#29305;&#24449;&#65292;&#29992;&#20110;&#35745;&#31639;&#27599;&#20010;LLM&#30340;&#20195;&#29702;&#22256;&#24785;&#24230;&#12290;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;LLM&#30340;&#20195;&#29702;&#22256;&#24785;&#24230;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generated texts from large language models (LLMs) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct. Consequently, there is an urgent need for a highly practical detection tool capable of accurately identifying the source of a given text. However, existing detection tools typically rely on access to LLMs and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection. Therefore, we propose LLMDet, a model-specific, secure, efficient, and extendable detection tool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and others. In LLMDet, we record the next-token probabilities of salient n-grams as features to calculate proxy perplexity for each LLM. By jointly analyzing the proxy perplexities of LLMs, we can determine the source of the generated text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#35780;&#20272;&#24230;&#37327;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#23545;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#26696;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14711</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#22522;&#20110;&#22270;&#20687;&#23383;&#24149;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning. (arXiv:2305.14711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#35780;&#20272;&#24230;&#37327;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#23545;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#26696;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#35780;&#20272;&#24230;&#37327;&#22312;&#22270;&#20687;&#23383;&#24149;&#31561;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#28145;&#20837;&#25506;&#35752;&#8212;&#8212;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#32534;&#30721;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35780;&#20272;&#30446;&#30340;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#34920;&#29616;&#24182;&#28508;&#22312;&#22320;&#25918;&#22823;&#20559;&#35265;&#12290;&#26412;&#25991;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#23545;&#27169;&#22411;&#35780;&#20272;&#24230;&#37327;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#21644;&#37327;&#21270;&#20102;&#19981;&#21516;&#35780;&#20272;&#24230;&#37327;&#20013;&#20851;&#20110;&#32844;&#19994;&#12289;&#27963;&#21160;&#21644;&#29289;&#20307;&#27010;&#24565;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#26377;&#20559;&#35265;&#30340;&#24230;&#37327;&#24102;&#26469;&#30340;&#36127;&#38754;&#21518;&#26524;&#65292;&#27604;&#22914;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#20559;&#21521;&#26377;&#20559;&#35265;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21521;&#29983;&#25104;&#27169;&#22411;&#20256;&#25773;&#20559;&#35265;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained model-based evaluation metrics have demonstrated strong performance with high correlations with human judgments in various natural language generation tasks such as image captioning. Despite the impressive results, their impact on fairness is under-explored -- it is widely acknowledged that pretrained models can encode societal biases, and utilizing them for evaluation purposes may inadvertently manifest and potentially amplify biases. In this paper, we conduct a systematic study in gender biases of model-based evaluation metrics with a focus on image captioning tasks. Specifically, we first identify and quantify gender biases in different evaluation metrics regarding profession, activity, and object concepts. Then, we demonstrate the negative consequences of using these biased metrics, such as favoring biased generation models in deployment and propagating the biases to generation models through reinforcement learning. We also present a simple but effective alternative to r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MixAlign&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#65292;&#23454;&#29616;&#33258;&#21160;&#30340;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22240;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#38382;&#39064;&#21644;&#30693;&#35782;&#32780;&#23548;&#33268;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13669</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24335;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment. (arXiv:2305.13669v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MixAlign&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#65292;&#23454;&#29616;&#33258;&#21160;&#30340;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22240;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#38382;&#39064;&#21644;&#30693;&#35782;&#32780;&#23548;&#33268;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#36817;&#26399;&#36827;&#23637;&#26174;&#33879;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#29983;&#25104;&#35823;&#23548;&#24615;&#21644;&#19981;&#25903;&#25345;&#30340;&#22238;&#31572;&#12290;&#19968;&#31181;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20174;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#21644;&#25972;&#21512;&#25903;&#25345;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#38382;&#39064;&#36890;&#24120;&#19982;&#23384;&#20648;&#30340;&#30693;&#35782;&#19981;&#22826;&#23545;&#40784;&#65292;&#22240;&#20026;&#20182;&#20204;&#22312;&#25552;&#38382;&#21069;&#19981;&#30693;&#36947;&#21487;&#29992;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#19981;&#23545;&#40784;&#21487;&#33021;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#23450;&#20301;&#21644;&#21033;&#29992;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#36843;&#20351;&#20854;&#36890;&#36807;&#24573;&#30053;&#25110;&#35206;&#30422;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#32780;&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MixAlign&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#20197;&#33719;&#24471;&#24182;&#25972;&#21512;&#20851;&#20110;&#29992;&#25143;&#38382;&#39064;&#19982;&#23384;&#20648;&#20449;&#24687;&#30456;&#20851;&#24615;&#30340;&#28548;&#28165;&#20449;&#24687;&#12290; MixAlign &#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#24182;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#20154;&#24037;&#29992;&#25143;&#28548;&#28165;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable recent advances in language models, they still struggle with the hallucination problem and can generate misleading and unsupported responses. A common approach to mitigate the hallucination issue is retrieving and incorporating supporting evidence from a knowledge base. However, user questions usually do not align well with the stored knowledge, as they are unaware of the information available before asking questions. This misalignment can limit the language model's ability to locate and utilize the knowledge, potentially forcing it to hallucinate by ignoring or overriding the retrieved evidence. To address this issue, we introduce MixAlign, a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic question-knowledge alignment and, if necessary, further enhances this alignment through human user clari
&lt;/p&gt;</description></item><item><title>Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13484</link><description>&lt;p&gt;
Flover&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13484
&lt;/p&gt;
&lt;p&gt;
Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#27169;&#22411;&#25512;&#26029;&#24615;&#33021;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23588;&#20854;&#26159;&#22312;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#24182;&#34987;&#37096;&#32626;&#22312;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24773;&#20917;&#19979;&#12290;&#33258;&#22238;&#24402;&#27169;&#22411;&#30001;&#20110;&#22312;&#20247;&#22810;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22240;&#27492;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#19978;&#37319;&#29992;&#20102;&#19968;&#31181;&#26102;&#38388;&#20381;&#36182;&#32467;&#26500;&#65292;&#20854;&#20013;&#24403;&#21069;token&#30340;&#27010;&#29575;&#20998;&#24067;&#21463;&#21040;&#21069;&#38754;token&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26412;&#36136;&#19978;&#30340;&#24207;&#21015;&#29305;&#24615;&#36981;&#24490;&#39532;&#23572;&#21487;&#22827;&#38142;&#20551;&#35774;&#65292;&#32570;&#20047;&#26102;&#38388;&#24182;&#34892;&#24615;&#65292;&#22240;&#27492;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#32972;&#26223;&#19979;&#65292;&#25512;&#26029;&#35831;&#27714;&#36981;&#24490;&#27850;&#26494;&#26102;&#38388;&#20998;&#24067;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#21709;&#24212;&#38271;&#24230;&#65292;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#32570;&#22833;&#26356;&#21152;&#26126;&#26174;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22914;&#21160;&#24577;&#25209;&#22788;&#29702;&#21644;&#24182;&#21457;&#27169;&#22411;&#23454;&#20363;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#31895;&#31890;&#24230;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#30340;&#24320;&#38144;&#21644;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#23545;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#35780;&#20272;&#26041;&#27861;iEvaLM&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#29616;&#26377;&#35780;&#20272;&#21327;&#35758;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.13112</link><description>&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#23545;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models. (arXiv:2305.13112v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#23545;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#35780;&#20272;&#26041;&#27861;iEvaLM&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#29616;&#26377;&#35780;&#20272;&#21327;&#35758;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#34920;&#26126;&#20854;&#22312;&#21457;&#23637;&#26356;&#24378;&#22823;&#30340;&#23545;&#35805;&#22411;&#25512;&#33616;&#31995;&#32479;&#65288;CRSs&#65289;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#36825;&#20123;&#31995;&#32479;&#20381;&#36182;&#20110;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;ChatGPT&#36827;&#34892;&#23545;&#35805;&#22411;&#25512;&#33616;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;&#35780;&#20272;&#21327;&#35758;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#29616;&#26377;&#35780;&#20272;&#21327;&#35758;&#21487;&#33021;&#36807;&#20998;&#24378;&#35843;&#19982;&#30001;&#20154;&#31867;&#26631;&#27880;&#32773;&#29983;&#25104;&#30340;&#22320;&#38754;&#30495;&#23454;&#29289;&#21697;&#25110;&#35805;&#35821;&#30340;&#21305;&#37197;&#65292;&#32780;&#24573;&#35270;&#20102;&#20316;&#20026;&#19968;&#31181;&#26377;&#33021;&#21147;&#30340;CRS&#30340;&#20132;&#20114;&#24615;&#36136;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#20132;&#20114;&#24335;&#35780;&#20272;&#26041;&#27861;&#65292;&#21517;&#20026;iEvaLM&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;LLMs&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#21508;&#31181;&#20132;&#20114;&#22330;&#26223;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;CRS&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#27969;&#34892;&#30340;&#35780;&#20272;&#21327;&#35758;&#30456;&#27604;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22806;&#37096;&#30693;&#35782;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for conversational recommendation, revealing the inadequacy of the existing evaluation protocol. It might over-emphasize the matching with the ground-truth items or utterances generated by human annotators, while neglecting the interactive nature of being a capable CRS. To overcome the limitation, we further propose an interactive Evaluation approach based on LLMs named iEvaLM that harnesses LLM-based user simulators. Our evaluation approach can simulate various interaction scenarios between users and systems. Through the experiments on two publicly available CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of ex
&lt;/p&gt;</description></item><item><title>ACCENT&#26159;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#20107;&#20214;&#24120;&#35782;&#35780;&#20215;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20107;&#20214;-&#20851;&#31995;&#20803;&#32452;&#19982;CSKB&#30340;&#20860;&#23481;&#24615;&#35780;&#20272;&#21709;&#24212;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35780;&#20215;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07797</link><description>&lt;p&gt;
ACCENT:&#19968;&#31181;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#21160;&#20107;&#20214;&#24120;&#35782;&#35780;&#20215;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems. (arXiv:2305.07797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07797
&lt;/p&gt;
&lt;p&gt;
ACCENT&#26159;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#20107;&#20214;&#24120;&#35782;&#35780;&#20215;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20107;&#20214;-&#20851;&#31995;&#20803;&#32452;&#19982;CSKB&#30340;&#20860;&#23481;&#24615;&#35780;&#20272;&#21709;&#24212;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35780;&#20215;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#22312;&#20154;&#31867;&#20132;&#27969;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#22240;&#27492;&#23545;&#20110;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#12290;&#20294;&#26159;&#65292;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24120;&#35782;&#25512;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#20851;&#27880;&#20107;&#20214;&#24120;&#35782;&#65292;&#23427;&#32771;&#34385;&#20107;&#20214;&#21450;&#20854;&#20851;&#31995;&#65292;&#22312;&#23545;&#35805;&#21644;&#19968;&#33324;&#24120;&#35782;&#25512;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;ACCENT&#65292;&#19968;&#31181;&#21463;&#24120;&#35782;&#30693;&#35782;&#24211; (CSKBs) &#25480;&#26435;&#30340;&#20107;&#20214;&#24120;&#35782;&#35780;&#20215;&#25351;&#26631;&#12290;ACCENT&#39318;&#20808;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#20107;&#20214;-&#20851;&#31995;&#20803;&#32452;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#23427;&#20204;&#19982;CSKB&#30340;&#20860;&#23481;&#24615;&#26469;&#35780;&#20272;&#21709;&#24212;&#12290;&#20026;&#20102;&#35780;&#20272;ACCENT&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#20107;&#20214;&#24120;&#35782;&#35780;&#20215;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ACCENT&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20107;&#20214;&#24120;&#35782;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#26356;&#33021;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning is omnipresent in human communications and thus is an important feature for open-domain dialogue systems. However, evaluating commonsense in dialogue systems is still an open challenge. We take the first step by focusing on event commonsense that considers events and their relations, and is crucial in both dialogues and general commonsense reasoning. We propose ACCENT, an event commonsense evaluation metric empowered by commonsense knowledge bases (CSKBs). ACCENT first extracts event-relation tuples from a dialogue, and then evaluates the response by scoring the tuples in terms of their compatibility with the CSKB. To evaluate ACCENT, we construct the first public event commonsense evaluation dataset for open-domain dialogues. Our experiments show that ACCENT is an efficient metric for event commonsense evaluation, which achieves higher correlations with human judgments than existing baselines.
&lt;/p&gt;</description></item><item><title>DIN-SQL&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20351;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.11015</link><description>&lt;p&gt;
DIN-SQL: &#33258;&#32416;&#27491;&#30340;&#25991;&#26412;&#21040;SQL&#20998;&#35299;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11015
&lt;/p&gt;
&lt;p&gt;
DIN-SQL&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20351;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#20998;&#35299;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23613;&#31649;SQL&#26597;&#35810;&#20855;&#26377;&#22768;&#26126;&#24335;&#32467;&#26500;&#65292;&#20294;&#21487;&#20197;&#23558;&#20854;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#22823;&#32422;&#25552;&#39640;&#20102;10&#65285;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25512;&#21521;&#26368;&#26032;&#27700;&#24179;&#65292;&#24182;&#22312;Holdout Spider&#25968;&#25454;&#38598;&#19978;&#29978;&#33267;&#36229;&#36807;&#20102;&#32463;&#36807;&#31934;&#35843;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of decomposing a complex text-to-sql task into smaller sub-tasks and how such a decomposition can significantly improve the performance of Large Language Models (LLMs) in the reasoning process. There is currently a significant gap between the performance of fine-tuned models and prompting approaches using LLMs on challenging text-to-sql datasets such as Spider. We show that SQL queries, despite their declarative structure, can be broken down into sub-problems and the solutions of those sub-problems can be fed into LLMs to significantly improve their performance. Our experiments with three LLMs show that this approach consistently improves their performance by roughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even beating large fine-tuned models on the holdout Spider dataset.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.03843</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35201;&#36880;&#27493;&#24605;&#32771;&#65311;&#25512;&#29702;&#28304;&#20110;&#32463;&#39564;&#30340;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#30528;&#24378;&#22823;&#32780;&#31070;&#31192;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#32431;&#31929;&#30340;&#24605;&#32500;&#27493;&#39588;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#29702;&#20986;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#24471;&#20986;&#30340;&#25512;&#35770; - &#23613;&#31649;&#25105;&#20204;&#20174;&#19990;&#30028;&#19978;&#27809;&#26377;&#24471;&#21040;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#12290;&#21516;&#26679;&#22320;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#29983;&#25104;&#20013;&#38388;&#27493;&#39588;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36825;&#20123;&#35757;&#32451;&#26465;&#20214;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#23450;&#20041;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#21697;&#23545;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#27599;&#20010;&#26679;&#21697;&#21482;&#21253;&#25324;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#21464;&#37327;&#12290;&#25105;&#20204;&#27604;&#36739;&#20351;&#29992;&#25512;&#29702;&#29983;&#25104;&#30340;&#21464;&#37327;&#23376;&#38598;&#19982;&#20351;&#29992;&#23436;&#25972;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#26816;&#32034;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#25512;&#21368;&#36131;&#20219;&#30340;&#38382;&#39064;&#65292;&#19988;&#26816;&#32034;&#22120;&#36873;&#25321;&#30340;&#21477;&#23376;&#21644;&#35821;&#35328;&#27169;&#22411;&#19981;&#32771;&#34385;&#21477;&#23376;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#37117;&#20250;&#24433;&#21709;&#25512;&#29702;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;ReForMask&#65292;&#37319;&#29992;&#25513;&#30721;&#26816;&#32034;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#21477;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.09146</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#25512;&#29702;&#33021;&#21147;&#65311;&#26816;&#32034;&#27169;&#22359;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#20105;
&lt;/p&gt;
&lt;p&gt;
Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model. (arXiv:2212.09146v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#26816;&#32034;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#25512;&#21368;&#36131;&#20219;&#30340;&#38382;&#39064;&#65292;&#19988;&#26816;&#32034;&#22120;&#36873;&#25321;&#30340;&#21477;&#23376;&#21644;&#35821;&#35328;&#27169;&#22411;&#19981;&#32771;&#34385;&#21477;&#23376;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#37117;&#20250;&#24433;&#21709;&#25512;&#29702;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;ReForMask&#65292;&#37319;&#29992;&#25513;&#30721;&#26816;&#32034;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#21477;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#37319;&#29992;&#26816;&#32034;&#22120;&#26469;&#36873;&#25321;&#25903;&#25345;&#25991;&#26723;&#65292;&#22312;&#35299;&#20915;&#24120;&#35265;&#30340;NLP&#38382;&#39064;&#65288;&#21253;&#25324;&#35821;&#35328;&#24314;&#27169;&#21644;&#38382;&#31572;&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;REALM&#65292;kNN-LM&#65292;FiD&#21644;DPR&#65292;ATLAS&#21644;Flan-T5&#21644;Contriever&#32806;&#21512;&#65289;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#25512;&#29702;&#26816;&#32034;&#35821;&#21477;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#32034;-&#38405;&#35835;&#27169;&#22411;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#26082;&#26469;&#33258;&#26816;&#32034;&#27169;&#22359;&#65292;&#20063;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26816;&#32034;&#22120;&#20351;&#29992;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#36890;&#24120;&#19981;&#36275;&#20197;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#32771;&#34385;&#35821;&#21477;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23548;&#33268;&#21363;&#20351;&#20351;&#29992;&#36739;&#22823;&#30340;&#27169;&#22411;&#65292;&#25512;&#29702;&#24615;&#33021;&#20063;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#26816;&#32034;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20013;&#38754;&#20020;&#30528;&#8220;&#36131;&#24618;&#28216;&#25103;&#8221;&#30340;&#38382;&#39064;&#65306;&#24403;&#26816;&#32034;&#22120;&#36873;&#25321;&#27491;&#30830;&#30340;&#35821;&#21477;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#25512;&#29702;&#65307;&#24403;&#26816;&#32034;&#22120;&#36873;&#25321;&#38169;&#35823;&#30340;&#35821;&#21477;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#33391;&#22909;&#30340;&#25512;&#29702;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;ReForMask&#65292;&#37319;&#29992;&#25513;&#30721;&#26816;&#32034;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#21477;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReForMask&#22312;&#22810;&#31181;&#24120;&#35265;&#30340;NLP&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting pretrained language models with retrievers to select the supporting documents has shown promise in effectively solving common NLP problems, including language modeling and question answering, in an interpretable way. In this paper, we first study the strengths and weaknesses of different retriever-augmented language models (REALM, $k$NN-LM, FiD coupled with DPR, and ATLAS and Flan-T5 coupled with Contriever) in reasoning over the retrieved statements in different tasks. We show how the retrieve-then-read models' limitations in reasoning are rooted both in the retriever module as well as the language model. Our experimental results demonstrate that the similarity metric used by the retrievers is generally insufficient for reasoning tasks. Additionally, we show that the language models in retriever-augmented models do not take the complicated relations between the statements into account, which leads to poor reasoning performance even when using the larger models. Moreover, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25277;&#35937;&#25688;&#35201;&#30340;&#22522;&#30784;&#19978;&#20248;&#21270;&#26102;&#38388;&#32447;&#25688;&#35201;&#30340;&#36136;&#37327;&#21644;&#21487;&#35835;&#24615;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.07596</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#25277;&#35937;&#26102;&#38388;&#32447;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Towards Abstractive Timeline Summarisation using Preference-based Reinforcement Learning. (arXiv:2211.07596v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25277;&#35937;&#25688;&#35201;&#30340;&#22522;&#30784;&#19978;&#20248;&#21270;&#26102;&#38388;&#32447;&#25688;&#35201;&#30340;&#36136;&#37327;&#21644;&#21487;&#35835;&#24615;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#24635;&#32467;&#22810;&#20010;&#26032;&#38395;&#26469;&#28304;&#25253;&#36947;&#30340;&#26102;&#38388;&#32447;&#12290;&#22522;&#20110;Transformer&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#19988;&#31616;&#27905;&#30340;&#38271;&#25991;&#26723;&#25688;&#35201;&#65292;&#20294;&#22312;&#26102;&#38388;&#32447;&#25688;&#35201;&#65288;TLS&#65289;&#31561;&#29305;&#23450;&#20219;&#21153;&#19978;&#21487;&#33021;&#26080;&#27861;&#36229;&#36234;&#24050;&#24314;&#31435;&#30340;&#25277;&#21462;&#26041;&#27861;&#12290;&#34429;&#28982;&#25277;&#21462;&#25688;&#35201;&#26356;&#24544;&#23454;&#20110;&#26469;&#28304;&#65292;&#20294;&#21487;&#33021;&#19981;&#22826;&#26131;&#35835;&#19988;&#21253;&#21547;&#20887;&#20313;&#25110;&#19981;&#24517;&#35201;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PBRL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#25688;&#35201;&#22120;&#36866;&#24212;&#20110;TLS&#65292;&#21487;&#20197;&#20811;&#26381;&#25277;&#21462;&#26102;&#38388;&#32447;&#25688;&#35201;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#22797;&#21512;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#20851;&#38190;&#35789;&#21644;&#20559;&#22909;&#26631;&#31614;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36890;&#36807;&#32447;&#19979;&#24378;&#21270;&#23398;&#20064;&#23545;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#25688;&#35201;&#22120;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#21487;&#27604;&#36739;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel pipeline for summarising timelines of events reported by multiple news sources. Transformer-based models for abstractive summarisation generate coherent and concise summaries of long documents but can fail to outperform established extractive methods on specialised tasks such as timeline summarisation (TLS). While extractive summaries are more faithful to their sources, they may be less readable and contain redundant or unnecessary information. This paper proposes a preference-based reinforcement learning (PBRL) method for adapting pretrained abstractive summarisers to TLS, which can overcome the drawbacks of extractive timeline summaries. We define a compound reward function that learns from keywords of interest and pairwise preference labels, which we use to fine-tune a pretrained abstractive summariser via offline reinforcement learning. We carry out both automated and human evaluation on three datasets, finding that our method outperforms a comparable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#38544;&#21947;&#29992;&#25143;&#27169;&#25311;&#22120;&#12290;&#35813;&#27169;&#25311;&#22120;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#27169;&#25311;&#35780;&#20272;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#32773;&#26694;&#26550;&#26469;&#29983;&#25104;&#19981;&#21516;&#33021;&#21147;&#30340;&#23545;&#35805;&#31995;&#32479;&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2204.00763</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#38544;&#21947;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#38544;&#21947;&#29992;&#25143;&#27169;&#25311;&#22120;&#12290;&#35813;&#27169;&#25311;&#22120;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#27169;&#25311;&#35780;&#20272;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#32773;&#26694;&#26550;&#26469;&#29983;&#25104;&#19981;&#21516;&#33021;&#21147;&#30340;&#23545;&#35805;&#31995;&#32479;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#65288;TDS&#65289;&#20027;&#35201;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#25110;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#36827;&#34892;&#35780;&#20272;&#12290;&#35780;&#20272;&#36890;&#24120;&#20165;&#38480;&#20110;&#21333;&#36718;&#25110;&#38750;&#24120;&#32791;&#26102;&#12290;&#20316;&#20026;&#19968;&#20010;&#26367;&#20195;&#26041;&#26696;&#65292;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#21487;&#20197;&#35753;&#25105;&#20204;&#32771;&#34385;&#19968;&#31995;&#21015;&#29992;&#25143;&#30446;&#26631;&#65292;&#20197;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#27169;&#25311;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#29616;&#26377;&#29992;&#25143;&#27169;&#25311;&#22120;&#35780;&#20272;TDS&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#20248;&#21270;TDS&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31471;&#21040;&#31471;TDS&#35780;&#20272;&#30340;&#38544;&#21947;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#27169;&#25311;&#22120;&#23450;&#20041;&#20026;&#38544;&#21947;&#24615;&#30340;&#65292;&#22914;&#26524;&#23427;&#22312;&#19982;&#31995;&#32479;&#30340;&#20132;&#20114;&#20013;&#27169;&#25311;&#29992;&#25143;&#30340;&#31867;&#27604;&#24605;&#32500;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27979;&#35797;&#32773;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#30340;&#23545;&#35805;&#31995;&#32479;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26500;&#24314;&#20102;&#19968;&#20010;&#38544;&#21947;&#24615;&#29992;&#25143;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue systems (TDSs) are assessed mainly in an offline setting or through human evaluation. The evaluation is often limited to single-turn or is very time-intensive. As an alternative, user simulators that mimic user behavior allow us to consider a broad set of user goals to generate human-like conversations for simulated evaluation. Employing existing user simulators to evaluate TDSs is challenging as user simulators are primarily designed to optimize dialogue policies for TDSs and have limited evaluation capabilities. Moreover, the evaluation of user simulators is an open challenge.  In this work, we propose a metaphorical user simulator for end-to-end TDS evaluation, where we define a simulator to be metaphorical if it simulates user's analogical thinking in interactions with systems. We also propose a tester-based evaluation framework to generate variants, i.e., dialogue systems with different capabilities. Our user simulator constructs a metaphorical user model th
&lt;/p&gt;</description></item><item><title>CoPaSul&#24037;&#20855;&#21253;&#25552;&#20379;&#20102;&#33258;&#21160;&#30340;&#38901;&#24459;&#26631;&#27880;&#21644;&#29305;&#24449;&#25552;&#21462;&#21151;&#33021;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#36718;&#24275;&#30340;&#21442;&#25968;&#21270;&#21644;&#21472;&#21152;&#38901;&#24459;&#39118;&#26684;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35813;&#24037;&#20855;&#21253;&#21487;&#20197;&#24471;&#21040;&#19982;&#38901;&#24459;&#36793;&#30028;&#21644;&#31361;&#20986;&#24615;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#31995;&#25968;&#32858;&#31867;&#24471;&#21040;&#38901;&#24459;&#36718;&#24275;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/1612.04765</link><description>&lt;p&gt;
CoPaSul&#25163;&#20876;--&#22522;&#20110;&#36718;&#24275;&#30340;&#21442;&#25968;&#21270;&#21644;&#21472;&#21152;&#38901;&#24459;&#39118;&#26684;&#21270;
&lt;/p&gt;
&lt;p&gt;
CoPaSul Manual -- Contour-based parametric and superpositional intonation stylization. (arXiv:1612.04765v11 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1612.04765
&lt;/p&gt;
&lt;p&gt;
CoPaSul&#24037;&#20855;&#21253;&#25552;&#20379;&#20102;&#33258;&#21160;&#30340;&#38901;&#24459;&#26631;&#27880;&#21644;&#29305;&#24449;&#25552;&#21462;&#21151;&#33021;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#36718;&#24275;&#30340;&#21442;&#25968;&#21270;&#21644;&#21472;&#21152;&#38901;&#24459;&#39118;&#26684;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35813;&#24037;&#20855;&#21253;&#21487;&#20197;&#24471;&#21040;&#19982;&#38901;&#24459;&#36793;&#30028;&#21644;&#31361;&#20986;&#24615;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#31995;&#25968;&#32858;&#31867;&#24471;&#21040;&#38901;&#24459;&#36718;&#24275;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CoPaSul&#24037;&#20855;&#21253;&#30340;&#30446;&#30340;&#26159;&#33258;&#21160;&#30340;&#38901;&#24459;&#26631;&#27880;&#21644;&#20174;&#38899;&#33410;&#21040;&#35821;&#21477;&#32423;&#21035;&#30340;&#38901;&#24459;&#29305;&#24449;&#25552;&#21462;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#38901;&#24459;&#34987;&#34920;&#31034;&#20026;&#20840;&#23616;&#21644;&#23616;&#37096;&#36718;&#24275;&#30340;&#21472;&#21152;&#65292;&#36825;&#20123;&#36718;&#24275;&#22312;&#22810;&#39033;&#24335;&#31995;&#25968;&#30340;&#21442;&#25968;&#21270;&#25551;&#36848;&#19979;&#12290;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#65288;&#36890;&#24120;&#19982;&#20294;&#19981;&#19968;&#23450;&#38480;&#20110;&#35821;&#35843;&#30701;&#35821;&#30456;&#20851;&#65289;&#65292;&#39118;&#26684;&#21270;&#29992;&#20110;&#20197;&#26102;&#38388;&#21464;&#21270;&#30340;F0&#27700;&#24179;&#21644;&#33539;&#22260;&#26469;&#34920;&#31034;&#38899;&#35843;&#12290;&#22312;&#23616;&#37096;&#23618;&#38754;&#19978;&#65288;&#20363;&#22914;&#65292;&#37325;&#38899;&#32452;&#65289;&#65292;&#25551;&#36848;&#23616;&#37096;&#36718;&#24275;&#24418;&#29366;&#12290;&#36890;&#36807;&#36825;&#31181;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#20960;&#20010;&#19982;&#38901;&#24459;&#36793;&#30028;&#21644;&#31361;&#20986;&#24615;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31995;&#25968;&#32858;&#31867;&#65292;&#21487;&#20197;&#20197;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#24335;&#33719;&#24471;&#38901;&#24459;&#36718;&#24275;&#31867;&#21035;&#12290;&#38500;&#20102;&#22522;&#20110;&#39118;&#26684;&#21270;&#30340;&#29305;&#24449;&#25552;&#21462;&#22806;&#65292;&#36824;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#30340;F0&#21644;&#33021;&#37327;&#27979;&#37327;&#65288;&#20363;&#22914;&#65292;&#24179;&#22343;&#20540;&#21644;&#26041;&#24046;&#65289;&#20197;&#21450;&#38901;&#24459;&#26041;&#38754;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purposes of the CoPaSul toolkit are (1) automatic prosodic annotation and (2) prosodic feature extraction from syllable to utterance level. CoPaSul stands for contour-based, parametric, superpositional intonation stylization. In this framework intonation is represented as a superposition of global and local contours that are described parametrically in terms of polynomial coefficients. On the global level (usually associated but not necessarily restricted to intonation phrases) the stylization serves to represent register in terms of time-varying F0 level and range. On the local level (e.g. accent groups), local contour shapes are described. From this parameterization several features related to prosodic boundaries and prominence can be derived. Furthermore, by coefficient clustering prosodic contour classes can be obtained in a bottom-up way. Next to the stylization-based feature extraction also standard F0 and energy measures (e.g. mean and variance) as well as rhythmic aspects c
&lt;/p&gt;</description></item></channel></rss>