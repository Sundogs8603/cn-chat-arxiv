<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>VideoXum&#26159;&#19968;&#20010;&#26032;&#30340;&#32852;&#21512;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#23427;&#30340;&#30446;&#26631;&#26159;&#20174;&#38271;&#35270;&#39057;&#20013;&#29983;&#25104;&#23545;&#24212;&#30340;&#31616;&#21270;&#35270;&#39057;&#21098;&#36753;&#21644;&#25991;&#26412;&#25688;&#35201;&#65292;&#21033;&#29992;&#20102;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#27169;&#22411;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.12060</link><description>&lt;p&gt;
VideoXum: &#35270;&#39057;&#30340;&#36328;&#27169;&#24577;&#35270;&#35273;&#21644;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
VideoXum: Cross-modal Visual and Textural Summarization of Videos. (arXiv:2303.12060v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12060
&lt;/p&gt;
&lt;p&gt;
VideoXum&#26159;&#19968;&#20010;&#26032;&#30340;&#32852;&#21512;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#23427;&#30340;&#30446;&#26631;&#26159;&#20174;&#38271;&#35270;&#39057;&#20013;&#29983;&#25104;&#23545;&#24212;&#30340;&#31616;&#21270;&#35270;&#39057;&#21098;&#36753;&#21644;&#25991;&#26412;&#25688;&#35201;&#65292;&#21033;&#29992;&#20102;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#12290;&#35813;&#27169;&#22411;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25688;&#35201;&#26088;&#22312;&#20174;&#28304;&#35270;&#39057;&#20013;&#25552;&#28860;&#20986;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#31616;&#30701;&#30340;&#35270;&#39057;&#21098;&#36753;&#25110;&#25991;&#26412;&#21465;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#20154;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598; -- VideoXum&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#21033;&#29992;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#26469;&#23545;&#40784;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35270;&#39057;&#21644;&#25991;&#26412;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video summarization aims to distill the most important information from a source video to produce either an abridged clip or a textual narrative. Traditionally, different methods have been proposed depending on whether the output is a video or text, thus ignoring the correlation between the two semantically related tasks of visual summarization and textual summarization. We propose a new joint video and text summarization task. The goal is to generate both a shortened video clip along with the corresponding textual summary from a long video, collectively referred to as a cross-modal summary. The generated shortened video clip and text narratives should be semantically well aligned. To this end, we first build a large-scale human-annotated dataset -- VideoXum (X refers to different modalities). The dataset is reannotated based on ActivityNet. After we filter out the videos that do not meet the length requirements, 14,001 long videos remain in our new dataset. Each video in our reannotat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#25919;&#27835;&#23478;&#30340;&#24847;&#35782;&#24418;&#24577;&#65292;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#25919;&#27835;&#21151;&#33021;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.12057</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#23398;&#20064;&#29615;&#22659;&#19979;&#29992;&#20110;&#35780;&#20272;&#25919;&#27835;&#23478;&#30340;&#24847;&#35782;&#24418;&#24577;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting. (arXiv:2303.12057v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#25919;&#27835;&#23478;&#30340;&#24847;&#35782;&#24418;&#24577;&#65292;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#25919;&#27835;&#21151;&#33021;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#30693;&#35782;&#21487;&#20197;&#20026;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#21487;&#35266;&#27979;&#24615;&#21644;&#27979;&#37327;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20854;&#20013;&#19968;&#31181;&#27169;&#22411;&#22312;&#34913;&#37327;&#31435;&#27861;&#32773;&#30340;&#28508;&#22312;&#24847;&#35782;&#24418;&#24577;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#22609;&#36896;&#25919;&#31574;&#30340;&#25919;&#27835;&#21151;&#33021;&#65292;&#20197;&#21450;&#25919;&#27835;&#34892;&#20026;&#32773;&#20195;&#34920;&#20854;&#36873;&#27665;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;ChatGPT&#22312;&#20004;&#20004;&#27604;&#36739;&#20013;&#36873;&#25321;&#26356;&#33258;&#30001;&#27966;&#65288;&#25110;&#20445;&#23432;&#27966;&#65289;&#30340;&#21442;&#35758;&#21592;&#65292;&#23558;&#31532;116&#23626;&#32654;&#22269;&#22269;&#20250;&#30340;&#21442;&#35758;&#21592;&#25353;&#29031;&#33258;&#30001;&#27966;-&#20445;&#23432;&#27966;&#30340;&#20809;&#35889;&#36827;&#34892;&#32553;&#25918;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#22312;&#37325;&#22797;&#36845;&#20195;&#20013;&#20135;&#29983;&#20102;&#31283;&#23450;&#30340;&#31572;&#26696;&#65292;&#27809;&#26377;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#19988;&#19981;&#20165;&#20165;&#26159;&#20174;&#21333;&#19968;&#26469;&#28304;&#20013;&#22797;&#21046;&#20449;&#24687;&#12290;&#36825;&#20010;&#26032;&#23610;&#24230;&#19982;&#29616;&#26377;&#30340;&#33258;&#30001;&#27966;-&#20445;&#23432;&#27966;&#23610;&#24230;&#65288;&#22914;NOMINATE&#65289;&#24378;&#30456;&#20851;&#65292;&#20294;&#20063;&#22312;&#20960;&#20010;&#37325;&#35201;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#27604;&#22914;&#27491;&#30830;&#23450;&#20301;&#19968;&#20123;&#36335;&#24452;&#20381;&#36182;&#21644;&#33258;&#30001;&#27966;&#27966;&#21035;&#30340;&#35758;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mass aggregation of knowledge embedded in large language models (LLMs) holds the promise of new solutions to problems of observability and measurement in the social sciences. We examine the utility of one such model for a particularly difficult measurement task: measuring the latent ideology of lawmakers, which allows us to better understand functions that are core to democracy, such as how politics shape policy and how political actors represent their constituents. We scale the senators of the 116th United States Congress along the liberal-conservative spectrum by prompting ChatGPT to select the more liberal (or conservative) senator in pairwise comparisons. We show that the LLM produced stable answers across repeated iterations, did not hallucinate, and was not simply regurgitating information from a single source. This new scale strongly correlates with pre-existing liberal-conservative scales such as NOMINATE, but also differs in several important ways, such as correctly placin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#38382;&#39064;&#30340;&#22238;&#31572;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#30340;&#26426;&#22120;&#32763;&#35793;&#35780;&#27979;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#30340;&#34920;&#29616;&#20173;&#28982;&#33853;&#21518;&#20110;&#20856;&#22411;&#20154;&#31867;&#30340;&#21453;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.12038</link><description>&lt;p&gt;
&#35780;&#20272;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#23545;&#35805;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
Grading Conversational Responses Of Chatbots. (arXiv:2303.12038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#38382;&#39064;&#30340;&#22238;&#31572;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#30340;&#26426;&#22120;&#32763;&#35793;&#35780;&#27979;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#30340;&#34920;&#29616;&#20173;&#28982;&#33853;&#21518;&#20110;&#20856;&#22411;&#20154;&#31867;&#30340;&#21453;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#22238;&#31572;&#22522;&#26412;&#38382;&#39064;&#65292;&#29978;&#33267;&#22238;&#24212;&#22855;&#24618;&#30340;&#25552;&#31034;&#65292;&#20294;&#26368;&#36817;&#23427;&#20204;&#30340;&#25913;&#36827;&#26174;&#33879;&#25552;&#39640;&#12290;&#20687;OpenAI ChatGPT3&#36825;&#26679;&#30340;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#19981;&#20165;&#33021;&#22815;&#22238;&#31572;&#22522;&#26412;&#38382;&#39064;&#65292;&#36824;&#33021;&#32534;&#20889;&#20195;&#30721;&#12289;&#30005;&#24433;&#21095;&#26412;&#24182;&#27169;&#20223;&#30693;&#21517;&#20154;&#29289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ChatGPT&#23545;&#20174;&#27969;&#34892;Quora&#35770;&#22363;&#30340;&#25968;&#25454;&#38598;&#20013;&#25552;&#20986;&#30340;&#21508;&#31181;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#21521;ChatGPT&#25552;&#20132;&#20102;60&#20010;&#38382;&#39064;&#65292;&#24182;&#26681;&#25454;&#19977;&#20010;&#34892;&#19994;&#26631;&#20934;&#35780;&#20998;&#26426;&#22120;&#32763;&#35793;&#30340;&#24230;&#37327;&#26631;&#20934;(BLEU&#12289;METEOR&#21644;ROUGE)&#23545;&#31572;&#26696;&#36827;&#34892;&#35780;&#20998;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#20801;&#35768;&#25105;&#20204;&#23558;&#26426;&#22120;&#30340;&#22238;&#22797;&#19982;&#21516;&#19968;&#38382;&#39064;&#30340;&#26368;&#22810;&#36190;&#21516;&#30340;&#20154;&#31867;&#31572;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35780;&#20272;ChatGPT&#25552;&#20132;&#20154;&#24615;&#21270;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;ChatGPT&#30340;&#21709;&#24212;&#21644;&#32763;&#35793;&#33021;&#21147;&#38750;&#24120;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#36798;&#19981;&#21040;&#20856;&#22411;&#20154;&#31867;&#21453;&#24212;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots have long been capable of answering basic questions and even responding to obscure prompts, but recently their improvements have been far more significant. Modern chatbots like Open AIs ChatGPT3 not only have the ability to answer basic questions but can write code and movie scripts and imitate well-known people. In this paper, we analyze ChatGPTs' responses to various questions from a dataset of queries from the popular Quora forum. We submitted sixty questions to ChatGPT and scored the answers based on three industry-standard metrics for grading machine translation: BLEU, METEOR, and ROUGE. These metrics allow us to compare the machine responses with the most upvoted human answer to the same question to assess ChatGPT's ability to submit a humanistic reply. The results showed that while the responses and translation abilities of ChatGPT are remarkable, they still fall short of what a typical human reaction would be.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#39044;&#27979;&#19968;&#20010;&#20154;&#22312;&#26410;&#26469;&#20107;&#20214;&#20013;&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#20204;&#22312;&#21475;&#32617;&#12289;&#31181;&#26063;&#24179;&#31561;&#21644;&#29305;&#26391;&#26222;&#31561;&#35805;&#39064;&#19978;&#30340;&#31435;&#22330;&#65292;&#24182;&#25581;&#31034;&#20102;&#36807;&#21435;&#34892;&#20026;&#22312;&#39044;&#27979;&#26410;&#26469;&#34892;&#20026;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.12029</link><description>&lt;p&gt;
&#25140;&#21475;&#32617;&#26159;&#21542;&#24847;&#21619;&#30528;&#21453;&#23545;&#29305;&#26391;&#26222;&#65311;&#65306;&#36328;&#26032;&#20896;&#30123;&#24773;&#21644;2020&#24180;&#32654;&#22269;&#22823;&#36873;&#30340;&#30446;&#26631;&#29305;&#23450;&#29992;&#25143;&#31435;&#22330;&#39044;&#27979;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Wearing Masks Implies Refuting Trump?: Towards Target-specific User Stance Prediction across Events in COVID-19 and US Election 2020. (arXiv:2303.12029v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12029
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#39044;&#27979;&#19968;&#20010;&#20154;&#22312;&#26410;&#26469;&#20107;&#20214;&#20013;&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#20204;&#22312;&#21475;&#32617;&#12289;&#31181;&#26063;&#24179;&#31561;&#21644;&#29305;&#26391;&#26222;&#31561;&#35805;&#39064;&#19978;&#30340;&#31435;&#22330;&#65292;&#24182;&#25581;&#31034;&#20102;&#36807;&#21435;&#34892;&#20026;&#22312;&#39044;&#27979;&#26410;&#26469;&#34892;&#20026;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#20105;&#35758;&#30340;&#35805;&#39064;&#20013;&#65292;&#25345;&#30456;&#20284;&#35266;&#28857;&#30340;&#20154;&#21487;&#33021;&#20250;&#24418;&#25104;&#22238;&#38899;&#23460;&#65292;&#24182;&#21487;&#33021;&#23545;&#20854;&#20182;&#35805;&#39064;&#25345;&#26377;&#31867;&#20284;&#30340;&#25919;&#27835;&#35266;&#28857;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#32852;&#31995;&#23384;&#22312;&#30340;&#36830;&#25509;&#34892;&#20026;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#39044;&#27979;&#19968;&#20010;&#20154;&#22312;&#26410;&#26469;&#20107;&#20214;&#20013;&#20250;&#22914;&#20309;&#34892;&#21160;&#30340;&#29420;&#29305;&#26426;&#20250;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#36827;&#34892;&#36830;&#25509;&#34892;&#20026;&#20998;&#26512;&#12290;&#25105;&#20204;&#21033;&#29992;&#22312; Twitter &#19978;&#25910;&#38598;&#30340;&#20851;&#20110;&#19977;&#20010;&#30475;&#20284;&#29420;&#31435;&#30340;&#35805;&#39064;&#65288;&#21475;&#32617;&#12289;&#31181;&#26063;&#24179;&#31561;&#21644;&#29305;&#26391;&#26222;&#65289;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#31070;&#32463;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#20154;&#20204;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#20182;&#20204;&#22312;&#27599;&#20010;&#19982;&#35805;&#39064;&#26377;&#20851;&#30340;&#20107;&#20214;&#20013;&#30340;&#22312;&#32447;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#19977;&#20010;&#20027;&#39064;&#20107;&#20214;&#30340;&#35266;&#28857;&#20043;&#38388;&#30340;&#24378;&#36830;&#25509;&#65292;&#24182;&#23637;&#31034;&#20102;&#36807;&#21435;&#34892;&#20026;&#22312;&#39044;&#27979;&#19968;&#20010;&#20154;&#26410;&#26469;&#34892;&#20026;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
People who share similar opinions towards controversial topics could form an echo chamber and may share similar political views toward other topics as well. The existence of such connections, which we call connected behavior, gives researchers a unique opportunity to predict how one would behave for a future event given their past behaviors. In this work, we propose a framework to conduct connected behavior analysis. Neural stance detection models are trained on Twitter data collected on three seemingly independent topics, i.e., wearing a mask, racial equality, and Trump, to detect people's stance, which we consider as their online behavior in each topic-related event. Our results reveal a strong connection between the stances toward the three topical events and demonstrate the power of past behaviors in predicting one's future behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;cTBL&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34920;&#26684;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26816;&#32034;&#20449;&#24687;&#25903;&#25745;&#30340;&#23545;&#35805;&#21709;&#24212;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12024</link><description>&lt;p&gt;
cTBL&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23545;&#35805;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;cTBL&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34920;&#26684;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26816;&#32034;&#20449;&#24687;&#25903;&#25745;&#30340;&#23545;&#35805;&#21709;&#24212;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#26469;&#28304;&#20013;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;Conversation Table (cTBL)&#65292;&#36825;&#26159;&#19968;&#31181;&#19977;&#27493;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#34920;&#26684;&#20449;&#24687;&#24182;&#29983;&#25104;&#22522;&#20110;&#26816;&#32034;&#20449;&#24687;&#30340;&#23545;&#35805;&#21709;&#24212;&#12290;cTBL&#20351;&#29992;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#24182;&#22312;HyrbiDialogue&#25968;&#25454;&#38598;Top-1&#21644;Top-3&#20934;&#30830;&#24615;&#19978;&#30456;&#23545;&#20110;&#31232;&#30095;&#26816;&#32034;&#25552;&#39640;&#20102;&#26368;&#22810;5%&#12290;&#27492;&#22806;&#65292;cTBL&#20351;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#30693;&#35782;&#26816;&#32034;&#65292;&#22312;HyrbiDialogue&#19978;&#20135;&#29983;&#20102;&#26368;&#39640;46%&#30340;ROUGE&#20998;&#25968;&#30456;&#23545;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20154;&#24037;&#35780;&#20272;&#21709;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
An open challenge in multimodal conversational AI requires augmenting large language models with information from textual and non-textual sources for multi-turn dialogue. To address this problem, this paper introduces Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve tabular information and generate dialogue responses grounded on the retrieved information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs tabular knowledge retrieval using both encoder and decoder models, resulting in up to 46% relative improvement in ROUGE scores and better human evaluation for response generation on HyrbiDialogue.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65292;&#20855;&#26377;&#19981;&#21516;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26032;&#27169;&#24335;&#22312;&#26410;&#26469;&#26377;&#30528;&#24456;&#39640;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.12023</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#30340;&#36923;&#36753;&#25512;&#29702;&#30740;&#31350;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Logical Reasoning over Natural Language as Knowledge Representation: A Survey. (arXiv:2303.12023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65292;&#20855;&#26377;&#19981;&#21516;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36825;&#31181;&#26032;&#27169;&#24335;&#22312;&#26410;&#26469;&#26377;&#30528;&#24456;&#39640;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#25512;&#29702;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#26234;&#33021;&#30340;&#26680;&#24515;&#12290;&#20197;&#24448;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#36923;&#36753;&#25512;&#29702;&#30740;&#31350;&#20351;&#29992;&#24418;&#24335;&#21270;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65288;&#21644;&#31526;&#21495;&#25512;&#29702;&#22120;&#65289;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24418;&#24335;&#21270;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#35777;&#26126;&#20855;&#26377;&#22256;&#38590;&#65288;&#20363;&#22914;&#33030;&#24369;&#24615;&#21644;&#30693;&#35782;&#33719;&#21462;&#29942;&#39048;&#65289;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#30693;&#35782;&#34920;&#31034;&#65288;&#20197;&#21450;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25512;&#29702;&#22120;&#65289;&#65292;&#21253;&#25324;&#36923;&#36753;&#25512;&#29702;&#30340;&#21746;&#23398;&#23450;&#20041;&#21644;&#20998;&#31867;&#65292;&#26032;&#27169;&#24335;&#30340;&#20248;&#21183;&#12289;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#26410;&#26469;&#38656;&#35201;&#30340;&#20219;&#21153;&#21644;&#26041;&#27861;&#20197;&#21450;&#19982;&#30456;&#20851; NLP &#39046;&#22495;&#30340;&#20851;&#31995;&#12290;&#36825;&#31181;&#26032;&#27169;&#24335;&#26159;&#24456;&#26377;&#21069;&#36884;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#24418;&#24335;&#21270;&#34920;&#31034;&#30340;&#35768;&#22810;&#25361;&#25112;&#65292;&#32780;&#19988;&#20063;&#20855;&#26377;&#20248;&#20110;&#31471;&#21040;&#31471;&#31070;&#32463;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical reasoning is central to human cognition and intelligence. Past research of logical reasoning within AI uses formal language as knowledge representation~(and symbolic reasoners). However, reasoning with formal language has proved challenging~(e.g., brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation~(and pretrained language models as reasoners), including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, desirable tasks &amp; methods in the future, and relation to related NLP fields. This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#12289;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#36827;&#34892;&#36328;&#39046;&#22495;&#29305;&#24449;&#23545;&#40784;&#65292;&#36824;&#21487;&#20197;&#24378;&#21046;&#30446;&#26631;&#26679;&#26412;&#19982;&#28304;&#22495;&#20013;&#30340;&#30456;&#24212;&#21407;&#22411;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#20135;&#29983;&#20266;&#26631;&#31614;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11945</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#65306;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Cross-Domain Rumor Detection with Contrastive Learning and Cross-Attention. (arXiv:2303.11945v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#12289;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#36827;&#34892;&#36328;&#39046;&#22495;&#29305;&#24449;&#23545;&#40784;&#65292;&#36824;&#21487;&#20197;&#24378;&#21046;&#30446;&#26631;&#26679;&#26412;&#19982;&#28304;&#22495;&#20013;&#30340;&#30456;&#24212;&#21407;&#22411;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#20135;&#29983;&#20266;&#26631;&#31614;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35875;&#35328;&#36890;&#24120;&#20276;&#38543;&#30528;&#31361;&#21457;&#26032;&#38395;&#25110;&#28909;&#38376;&#35805;&#39064;&#32780;&#20986;&#29616;&#65292;&#20005;&#37325;&#38459;&#30861;&#30495;&#30456;&#30340;&#26597;&#35777;&#12290;&#29616;&#26377;&#30340;&#35875;&#35328;&#26816;&#27979;&#26041;&#27861;&#22823;&#22810;&#19987;&#27880;&#20110;&#30456;&#21516;&#39046;&#22495;&#65292;&#22240;&#27492;&#22312;&#36328;&#39046;&#22495;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#23454;&#20363;&#21644;&#21407;&#22411;&#30340;&#65292;&#24102;&#26377;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#36827;&#34892;&#36328;&#39046;&#22495;&#29305;&#24449;&#23545;&#40784;&#65292;&#36824;&#21487;&#20197;&#24378;&#21046;&#30446;&#26631;&#26679;&#26412;&#19982;&#32473;&#23450;&#28304;&#22495;&#30340;&#30456;&#24212;&#21407;&#22411;&#23545;&#40784;&#12290;&#30001;&#20110;&#30446;&#26631;&#22495;&#20013;&#30340;&#30446;&#26631;&#26631;&#31614;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#25209;&#28304;&#22495;&#26679;&#26412;&#30340;&#20180;&#32454;&#21021;&#22987;&#21270;&#20013;&#24515;&#26469;&#20135;&#29983;&#20266;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#22788;&#29702;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#19968;&#23545;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#65292;&#20197;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#12290;&#30001;&#20110;&#39046;&#22495;&#23545;&#20013;&#30340;&#26679;&#26412;&#20542;&#21521;&#20110;&#34920;&#36798;&#30456;&#20284;&#30340;&#35821;&#20041;&#27169;&#24335;&#65292;&#22240;&#27492;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive rumors usually appear along with breaking news or trending topics, seriously hindering the truth. Existing rumor detection methods are mostly focused on the same domain, and thus have poor performance in cross-domain scenarios due to domain shift. In this work, we propose an end-to-end instance-wise and prototype-wise contrastive learning model with a cross-attention mechanism for cross-domain rumor detection. The model not only performs cross-domain feature alignment but also enforces target samples to align with the corresponding prototypes of a given source domain. Since target labels in a target domain are unavailable, we use a clustering-based approach with carefully initialized centers by a batch of source domain samples to produce pseudo labels. Moreover, we use a cross-attention mechanism on a pair of source data and target data with the same labels to learn domain-invariant representations. Because the samples in a domain pair tend to express similar semantic patterns,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20013;&#22269;&#20013;&#32423;&#33521;&#35821;&#23398;&#20064;&#32773;&#22312;&#21465;&#20107;&#20027;&#39064;&#19978;&#30340;&#20889;&#20316;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT&#22312;&#21465;&#20107;&#24615;&#12289;&#35789;&#27719;&#20855;&#20307;&#24615;&#21644;&#25351;&#20195;&#36830;&#36143;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#20316;&#23478;&#65292;&#20294;&#22312;&#21477;&#27861;&#31616;&#21333;&#24615;&#21644;&#28145;&#23618;&#36830;&#36143;&#24615;&#26041;&#38754;&#30053;&#36874;&#19968;&#31609;&#12290;</title><link>http://arxiv.org/abs/2303.11812</link><description>&lt;p&gt;
&#20013;&#22269;&#20013;&#32423;&#33521;&#35821;&#23398;&#20064;&#32773;&#22312;&#28145;&#23618;&#36830;&#36143;&#24615;&#26041;&#38754;&#32988;&#36807;ChatGPT&#65306;&#22522;&#20110;&#33521;&#35821;&#21465;&#20107;&#20889;&#20316;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Chinese Intermediate English Learners outdid ChatGPT in deep cohesion: Evidence from English narrative writing. (arXiv:2303.11812v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20013;&#22269;&#20013;&#32423;&#33521;&#35821;&#23398;&#20064;&#32773;&#22312;&#21465;&#20107;&#20027;&#39064;&#19978;&#30340;&#20889;&#20316;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT&#22312;&#21465;&#20107;&#24615;&#12289;&#35789;&#27719;&#20855;&#20307;&#24615;&#21644;&#25351;&#20195;&#36830;&#36143;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#20316;&#23478;&#65292;&#20294;&#22312;&#21477;&#27861;&#31616;&#21333;&#24615;&#21644;&#28145;&#23618;&#36830;&#36143;&#24615;&#26041;&#38754;&#30053;&#36874;&#19968;&#31609;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#21487;&#20197;&#24555;&#36895;&#29983;&#25104;&#25351;&#23450;&#20027;&#39064;&#30340;&#25991;&#26412;&#65292;&#20294;&#26159;&#19981;&#30693;&#36947;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#21542;&#22312;&#20889;&#20316;&#30340;&#25152;&#26377;&#26041;&#38754;&#19978;&#37117;&#27604;&#20154;&#31867;&#20316;&#23478;&#20248;&#31168;&#65292;&#24182;&#19988;&#19981;&#30693;&#36947;&#22312;&#26356;&#26032;&#21629;&#20196;&#30340;&#22522;&#30784;&#19978;&#20854;&#20889;&#20316;&#36136;&#37327;&#26159;&#21542;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20013;&#22269;&#20013;&#32423;&#33521;&#35821;&#65288;CIE&#65289;&#23398;&#20064;&#32773;&#22312;&#21465;&#20107;&#20027;&#39064;&#19978;&#30340;&#20889;&#20316;&#34920;&#29616;&#65292;&#20197;&#25581;&#31034;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20889;&#20316;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25968;&#25454;&#20351;&#29992;Coh-Metrix&#65288;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#35821;&#35328;&#31687;&#31456;&#30340;&#29305;&#27530;&#24037;&#20855;&#65289;&#25353;&#29031;&#20116;&#20010;&#31687;&#31456;&#25104;&#20998;&#36827;&#34892;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#21465;&#20107;&#24615;&#12289;&#35789;&#27719;&#20855;&#20307;&#24615;&#21644;&#25351;&#20195;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#20316;&#23478;&#65292;&#20294;&#22312;&#21477;&#27861;&#31616;&#21333;&#24615;&#21644;&#28145;&#23618;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#12290;&#22312;&#26356;&#26032;&#20102;&#26356;&#22810;&#30340;&#20462;&#35746;&#21629;&#20196;&#21518;&#65292;&#32467;&#26524;&#29256;&#26412;&#30340;&#21477;&#27861;&#31616;&#21333;&#24615;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#20294;&#20173;&#36828;&#36828;&#33853;&#21518;&#20110;CIE&#23398;&#20064;&#32773;&#30340;&#20889;&#20316;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a publicly available chatbot that can quickly generate texts on given topics, but it is unknown whether the chatbot is really superior to human writers in all aspects of writing and whether its writing quality can be prominently improved on the basis of updating commands. Consequently, this study compared the writing performance on a narrative topic by ChatGPT and Chinese intermediate English (CIE) learners so as to reveal the chatbot's advantage and disadvantage in writing. The data were analyzed in terms of five discourse components using Coh-Metrix (a special instrument for analyzing language discourses), and the results revealed that ChatGPT performed better than human writers in narrativity, word concreteness, and referential cohesion, but worse in syntactic simplicity and deep cohesion in its initial version. After more revision commands were updated, while the resulting version was facilitated in syntactic simplicity, yet it is still lagged far behind CIE learners' wr
&lt;/p&gt;</description></item><item><title>LEAPT&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#21516;&#22768;&#32763;&#35793;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#22343;&#20855;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.11750</link><description>&lt;p&gt;
LEAPT: &#23398;&#20064;&#36866;&#24212;&#24615;&#21069;&#32512;&#21040;&#21069;&#32512;&#32763;&#35793;&#20197;&#36827;&#34892;&#21516;&#22768;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
LEAPT: Learning Adaptive Prefix-to-prefix Translation For Simultaneous Machine Translation. (arXiv:2303.11750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11750
&lt;/p&gt;
&lt;p&gt;
LEAPT&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#21516;&#22768;&#32763;&#35793;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#22343;&#20855;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#22768;&#26426;&#22120;&#32763;&#35793;&#26088;&#22312;&#23454;&#29616;&#23454;&#26102;&#32763;&#35793;&#65292;&#22312;&#35768;&#22810;&#29616;&#22330;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#30001;&#20110;&#20934;&#30830;&#24615;&#19982;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#32780;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#22312;&#31934;&#24230;&#19982;&#24310;&#36831;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#27169;&#22411;&#38656;&#35201;&#31561;&#24453;&#36866;&#24403;&#30340;&#27969;&#24335;&#25991;&#26412;&#65288;&#35835;&#21462;&#31574;&#30053;&#65289;&#65292;&#28982;&#21518;&#29983;&#25104;&#32763;&#35793;&#65288;&#20889;&#20837;&#31574;&#30053;&#65289;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#24037;&#20316;&#30340;&#20889;&#20837;&#31574;&#30053;&#35201;&#20040;&#30001;&#20110;&#31471;&#21040;&#31471;&#35757;&#32451;&#32780;&#23545;&#35813;&#26041;&#27861;&#26412;&#36523;&#20855;&#26377;&#29305;&#24322;&#24615;&#65292;&#35201;&#20040;&#30001;&#20110;&#38750;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#32780;&#21463;&#21040;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#21516;&#22768;&#26426;&#22120;&#32763;&#35793;&#30340;&#36890;&#29992;&#19988;&#26356;&#22909;&#30340;&#20889;&#20837;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21463;&#20154;&#31867;&#21475;&#35793;&#32773;&#21450;&#8220;&#31561;&#24453;&#8221;&#31574;&#30053;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEAPT&#30340;&#26032;&#30340;&#33258;&#36866;&#24212;&#21069;&#32512;&#21040;&#21069;&#32512;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#25105;&#20204;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#22914;&#20309;&#32763;&#35793;&#28304;&#21477;&#23376;&#21069;&#32512;&#24182;&#21033;&#29992;&#26410;&#26469;&#30340;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#27700;&#24179;&#22122;&#22768;&#21644;&#19981;&#21516;&#27969;&#24335;&#25991;&#26412;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#31454;&#20105;&#27169;&#22411;&#65292;&#31934;&#24230;&#19982;&#24310;&#36831;&#22343;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous machine translation, which aims at a real-time translation, is useful in many live scenarios but very challenging due to the trade-off between accuracy and latency. To achieve the balance for both, the model needs to wait for appropriate streaming text (READ policy) and then generates its translation (WRITE policy). However, WRITE policies of previous work either are specific to the method itself due to the end-to-end training or suffer from the input mismatch between training and decoding for the non-end-to-end training. Therefore, it is essential to learn a generic and better WRITE policy for simultaneous machine translation. Inspired by strategies utilized by human interpreters and "wait" policies, we propose a novel adaptive prefix-to-prefix training policy called LEAPT, which allows our machine translation model to learn how to translate source sentence prefixes and make use of the future context. Experiments show that our proposed methods greatly outperform competiti
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#36890;&#36807;&#25552;&#20379;&#26368;&#23569;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#22823;&#21270;&#30340;&#8220;&#24320;&#25918;&#24615;&#8221;&#65292;&#22312;&#23454;&#36341;&#20013;&#23548;&#33268;&#20102;&#8220;&#24320;&#25918;&#39046;&#22495;&#24726;&#35770;&#8221;&#8212;&#8212;-&#35201;&#27714;&#29992;&#25143;&#8220;&#38386;&#32842;&#20219;&#20309;&#20107;&#24773;&#8221;&#20250;&#23548;&#33268;&#38750;&#24120;&#29421;&#31364;&#30340;&#23545;&#35805;&#24418;&#24335;&#12290;&#27492;&#30740;&#31350;&#36827;&#19968;&#27493;&#35299;&#37322;&#20102;&#20849;&#21516;&#22522;&#30784;&#29702;&#35770;&#19982;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29616;&#20849;&#21516;&#22522;&#30784;&#30340;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2303.11708</link><description>&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24726;&#35770;&#65306;&#20849;&#21516;&#22522;&#30784;&#26159;&#23454;&#29616;&#20154;&#31867;&#23545;&#35805;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
The Open-domain Paradox for Chatbots: Common Ground as the Basis for Human-like Dialogue. (arXiv:2303.11708v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11708
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#36890;&#36807;&#25552;&#20379;&#26368;&#23569;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#22823;&#21270;&#30340;&#8220;&#24320;&#25918;&#24615;&#8221;&#65292;&#22312;&#23454;&#36341;&#20013;&#23548;&#33268;&#20102;&#8220;&#24320;&#25918;&#39046;&#22495;&#24726;&#35770;&#8221;&#8212;&#8212;-&#35201;&#27714;&#29992;&#25143;&#8220;&#38386;&#32842;&#20219;&#20309;&#20107;&#24773;&#8221;&#20250;&#23548;&#33268;&#38750;&#24120;&#29421;&#31364;&#30340;&#23545;&#35805;&#24418;&#24335;&#12290;&#27492;&#30740;&#31350;&#36827;&#19968;&#27493;&#35299;&#37322;&#20102;&#20849;&#21516;&#22522;&#30784;&#29702;&#35770;&#19982;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29616;&#20849;&#21516;&#22522;&#30784;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#25512;&#21160;&#20102;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#65292;&#20854;&#8220;&#24320;&#25918;&#24615;&#8221;&#39044;&#35745;&#36890;&#36807;&#21521;&#29992;&#25143;&#25552;&#20379;&#26368;&#23569;&#20449;&#24687;&#65292;&#21253;&#25324;&#20551;&#23450;&#30340;&#20849;&#21516;&#27963;&#21160;&#65292;&#26469;&#23454;&#29616;&#26368;&#22823;&#21270;&#12290;&#28982;&#32780;&#65292;&#35777;&#25454;&#34920;&#26126;&#25928;&#26524;&#30456;&#21453;&#12290;&#35201;&#27714;&#29992;&#25143;&#8220;&#38386;&#32842;&#20219;&#20309;&#20107;&#24773;&#8221;&#20250;&#23548;&#33268;&#38750;&#24120;&#29421;&#31364;&#30340;&#23545;&#35805;&#24418;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#24320;&#25918;&#39046;&#22495;&#24726;&#35770;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20849;&#21516;&#22522;&#30784;&#29702;&#35770;&#35299;&#37322;&#20102;&#36825;&#20010;&#24726;&#35770;&#20316;&#20026;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36136;&#30097;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#32972;&#21518;&#30340;&#20551;&#35774;&#65292;&#24182;&#30830;&#23450;&#23454;&#29616;&#20154;&#26426;&#23545;&#35805;&#20013;&#20849;&#21516;&#22522;&#30784;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a surge in interest in the development of open-domain chatbots, driven by the recent advancements of large language models. The "openness" of the dialogue is expected to be maximized by providing minimal information to the users about the common ground they can expect, including the presumed joint activity. However, evidence suggests that the effect is the opposite. Asking users to "just chat about anything" results in a very narrow form of dialogue, which we refer to as the "open-domain paradox". In this paper, we explain this paradox through the theory of common ground as the basis for human-like communication. Furthermore, we question the assumptions behind open-domain chatbots and identify paths forward for enabling common ground in human-computer dialogue.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#26500;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#29305;&#23450;&#26041;&#38754;&#21644;&#19968;&#33324;&#24847;&#35265;&#25688;&#35201;&#12290;SW-LOO&#26041;&#27861;&#36890;&#36807;&#31181;&#23376;&#35789;&#21305;&#37197;&#30830;&#23450;&#35780;&#35770;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#22312;SPACE&#21644;OPOSUM+&#25968;&#25454;&#38598;&#19978;&#37117;&#26377;&#24456;&#22909;&#25928;&#26524;&#65307;NLI-LOO&#26041;&#27861;&#36890;&#36807;NLI&#27169;&#22411;&#35782;&#21035;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#21477;&#23376;&#65292;&#22312;SPACE&#25968;&#25454;&#38598;&#19978;&#26377;&#24456;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11660</link><description>&lt;p&gt;
&#26500;&#24314;&#31616;&#21333;&#26377;&#25928;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#29992;&#20110;&#26080;&#30417;&#30563;&#24773;&#24863;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Simple Yet Effective Synthetic Dataset Construction for Unsupervised Opinion Summarization. (arXiv:2303.11660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#26500;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#29983;&#25104;&#29305;&#23450;&#26041;&#38754;&#21644;&#19968;&#33324;&#24847;&#35265;&#25688;&#35201;&#12290;SW-LOO&#26041;&#27861;&#36890;&#36807;&#31181;&#23376;&#35789;&#21305;&#37197;&#30830;&#23450;&#35780;&#35770;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#22312;SPACE&#21644;OPOSUM+&#25968;&#25454;&#38598;&#19978;&#37117;&#26377;&#24456;&#22909;&#25928;&#26524;&#65307;NLI-LOO&#26041;&#27861;&#36890;&#36807;NLI&#27169;&#22411;&#35782;&#21035;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#21477;&#23376;&#65292;&#22312;SPACE&#25968;&#25454;&#38598;&#19978;&#26377;&#24456;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#35265;&#25688;&#35201;&#25552;&#20379;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23545;&#22823;&#37327;&#35780;&#35770;&#20013;&#34920;&#36798;&#30340;&#24847;&#35265;&#36827;&#34892;&#25688;&#35201;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#65292;&#29983;&#25104;&#29305;&#23450;&#26041;&#38754;&#21644;&#19968;&#33324;&#25688;&#35201;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#31616;&#21333;&#26377;&#25928;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21253;&#21547;&#29305;&#23450;&#26041;&#38754;&#30340;&#35780;&#35770;&#20869;&#23481;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#29983;&#25104;&#29305;&#23450;&#26041;&#38754;&#21644;&#19968;&#33324;&#24847;&#35265;&#25688;&#35201;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#31181;&#23376;&#35789;&#30340;&#30041;&#19968;&#27861;&#65288;SW-LOO&#65289;&#65292;&#20165;&#36890;&#36807;&#31934;&#30830;&#21305;&#37197;&#26041;&#38754;&#30340;&#31181;&#23376;&#35789;&#26469;&#30830;&#23450;&#35780;&#35770;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#22312;SPACE&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;3.4 ROUGE-L&#28857;&#65292;&#22312;OPOSUM+&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;0.5 ROUGE-1&#28857;&#65292;&#29992;&#20110;&#29305;&#23450;&#26041;&#38754;&#30340;&#24847;&#35265;&#25688;&#35201;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#30041;&#19968;&#27861;&#65288;NLI-LOO&#65289;&#65292;&#22312;&#26356;&#26222;&#36941;&#30340;&#35774;&#32622;&#20013;&#65292;&#21033;&#29992;NLI&#27169;&#22411;&#35782;&#21035;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#21477;&#23376;&#65292;&#32780;&#19981;&#20351;&#29992;&#31181;&#23376;&#35789;&#65292;&#24182;&#22312;SPACE&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;1.2 ROUGE-L&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinion summarization provides an important solution for summarizing opinions expressed among a large number of reviews. However, generating aspect-specific and general summaries is challenging due to the lack of annotated data. In this work, we propose two simple yet effective unsupervised approaches to generate both aspect-specific and general opinion summaries by training on synthetic datasets constructed with aspect-related review contents. Our first approach, Seed Words Based Leave-One-Out (SW-LOO), identifies aspect-related portions of reviews simply by exact-matching aspect seed words and outperforms existing methods by 3.4 ROUGE-L points on SPACE and 0.5 ROUGE-1 point on OPOSUM+ for aspect-specific opinion summarization. Our second approach, Natural Language Inference Based Leave-One-Out (NLI-LOO) identifies aspect-related sentences utilizing an NLI model in a more general setting without using seed words and outperforms existing approaches by 1.2 ROUGE-L points on SPACE for as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#20998;&#25903;&#21327;&#20316;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#35805;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#32780;&#38750;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#32593;&#32476;&#20998;&#25903;&#30340;&#35757;&#32451;&#20013;&#32771;&#34385;&#21040;&#23545;&#35805;&#23646;&#24615;&#65292;&#20351;&#19981;&#21516;&#20998;&#25903;&#30340;&#29305;&#24449;&#22810;&#26679;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.11621</link><description>&lt;p&gt;
&#24322;&#26500;&#20998;&#25903;&#21327;&#20316;&#23398;&#20064;&#29992;&#20110;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous-Branch Collaborative Learning for Dialogue Generation. (arXiv:2303.11621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#20998;&#25903;&#21327;&#20316;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#35805;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#32780;&#38750;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#32593;&#32476;&#20998;&#25903;&#30340;&#35757;&#32451;&#20013;&#32771;&#34385;&#21040;&#23545;&#35805;&#23646;&#24615;&#65292;&#20351;&#19981;&#21516;&#20998;&#25903;&#30340;&#29305;&#24449;&#22810;&#26679;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#39640;&#32423;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#33719;&#24471;&#39640;&#24615;&#33021;&#21644;&#36731;&#37327;&#32423;&#27169;&#22411;&#30340;&#26041;&#27861;&#26159;&#30693;&#35782;&#33976;&#39311;&#65292;&#20854;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#24378;&#22823;&#25945;&#24072;&#27169;&#22411;&#12290;&#21327;&#20316;&#23398;&#20064;&#65292;&#20063;&#31216;&#20026;&#22312;&#32447;&#30693;&#35782;&#33976;&#39311;&#65292;&#22312;&#32570;&#20047;&#35757;&#32451;&#33391;&#22909;&#30340;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#36827;&#34892;&#21333;&#38454;&#27573;&#32676;&#20307;&#33976;&#39311;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#30001;&#20110;&#30456;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#29420;&#31435;&#30456;&#21516;&#30340;&#35757;&#32451;&#38598;&#23384;&#22312;&#20005;&#37325;&#30340;&#20998;&#25903;&#21516;&#36136;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#32593;&#32476;&#20998;&#25903;&#30340;&#35757;&#32451;&#20013;&#32771;&#34385;&#23545;&#35805;&#23646;&#24615;&#12290;&#27599;&#20010;&#20998;&#25903;&#22522;&#20110;&#25152;&#36873;&#23376;&#38598;&#23398;&#20064;&#19982;&#23646;&#24615;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#22522;&#20110;&#32676;&#20307;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21253;&#25324;&#31215;&#26497;&#33976;&#39311;&#21644;&#28040;&#26497;&#33976;&#39311;&#65292;&#36827;&#19968;&#27493;&#20351;&#19981;&#21516;&#20998;&#25903;&#30340;&#29305;&#24449;&#22810;&#26679;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of deep learning, advanced dialogue generation methods usually require a greater amount of computational resources. One promising approach to obtaining a high-performance and lightweight model is knowledge distillation, which relies heavily on the pre-trained powerful teacher. Collaborative learning, also known as online knowledge distillation, is an effective way to conduct one-stage group distillation in the absence of a well-trained large teacher model. However, previous work has a severe branch homogeneity problem due to the same training objective and the independent identical training sets. To alleviate this problem, we consider the dialogue attributes in the training of network branches. Each branch learns the attribute-related features based on the selected subset. Furthermore, we propose a dual group-based knowledge distillation method, consisting of positive distillation and negative distillation, to further diversify the features of different branches in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;Transformer&#22312;&#35821;&#38899;&#30456;&#20851;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#21516;&#26102;&#65292;&#25351;&#20986;&#20102;&#22312;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#20013;Transformer&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.11607</link><description>&lt;p&gt;
&#35770;&#25991;&#32763;&#35793;&#65306;&#35821;&#38899;&#22788;&#29702;&#20013;&#30340;Transformer&#65306;&#32508;&#36848;&#65288;arXiv:2303.11607v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Transformers in Speech Processing: A Survey. (arXiv:2303.11607v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;Transformer&#22312;&#35821;&#38899;&#30456;&#20851;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#21516;&#26102;&#65292;&#25351;&#20986;&#20102;&#22312;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#20013;Transformer&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#25104;&#21151;&#24341;&#36215;&#20102;&#35821;&#38899;&#22788;&#29702;&#31038;&#21306;&#30340;&#20852;&#36259;&#65292;&#36827;&#32780;&#25506;&#32034;&#20102;&#20854;&#27169;&#25311;&#35821;&#38899;&#24207;&#21015;&#20013;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#26368;&#36817;&#65292;Transformer &#22312;&#21508;&#31181;&#28041;&#21450;&#35821;&#38899;&#30340;&#39046;&#22495;&#20013;&#21517;&#22768;&#40522;&#36215;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21512;&#25104;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#35821;&#38899;&#22768;&#35843;&#23398;&#12289;&#35821;&#38899;&#22686;&#24378;&#12289;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#65292;&#20197;&#21450;&#35768;&#22810;&#22810;&#27169;&#24577;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20379;&#19968;&#20221;&#32508;&#21512;&#24615;&#35843;&#26597;&#25253;&#21578;&#65292;&#26088;&#22312;&#26725;&#25509;&#35821;&#38899;&#25216;&#26415;&#21508;&#23376;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#35821;&#38899;&#25216;&#26415;&#39046;&#22495;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#20026;&#24076;&#26395;&#21033;&#29992;Transformer&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#25351;&#20986;&#20102;Transformer&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#22312;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable success of transformers in the field of natural language processing has sparked the interest of the speech-processing community, leading to an exploration of their potential for modeling long-range dependencies within speech sequences. Recently, transformers have gained prominence across various speech-related domains, including automatic speech recognition, speech synthesis, speech translation, speech para-linguistics, speech enhancement, spoken dialogue systems, and numerous multimodal applications. In this paper, we present a comprehensive survey that aims to bridge research studies from diverse subfields within speech technology. By consolidating findings from across the speech technology landscape, we provide a valuable resource for researchers interested in harnessing the power of transformers to advance the field. We identify the challenges encountered by transformers in speech processing while also offering insights into potential solutions to address these issue
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.11593</link><description>&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#25163;&#24615;&#26102;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11593
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23545;&#26497;&#20854;&#22810;&#26679;&#30340;&#20998;&#23376;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#25551;&#36848;&#31526;&#29983;&#25104;&#24050;&#32463;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;SMILES&#65292;&#21363;&#20998;&#23376;&#32467;&#26500;&#30340;&#25991;&#23383;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21270;&#23398;&#32467;&#26500;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;NLP&#27169;&#22411;&#8212;&#8212;Transformer&#65292;&#22312;&#23398;&#20064;SMILES&#21644;&#21270;&#23398;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;Transformer&#24555;&#36895;&#23398;&#20064;&#20998;&#23376;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#25165;&#33021;&#29702;&#35299;&#25972;&#20307;&#32467;&#26500;&#12290;&#19982;&#20043;&#19968;&#33268;&#30340;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#23398;&#20064;&#27493;&#39588;&#20013;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;Transformer&#38656;&#35201;&#29305;&#21035;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#25165;&#33021;&#23398;&#20064;&#25163;&#24615;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#20986;&#29616;&#20302;&#32763;&#35793;&#20934;&#30830;&#29575;&#30340;&#20572;&#28382;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen development of descriptor generation based on representation learning of extremely diverse molecules, especially those that apply natural language processing (NLP) models to SMILES, a literal representation of molecular structure. However, little research has been done on how these models understand chemical structure. To address this, we investigated the relationship between the learning progress of SMILES and chemical structure using a representative NLP model, the Transformer. The results suggest that while the Transformer learns partial structures of molecules quickly, it requires extended training to understand overall structures. Consistently, the accuracy of molecular property predictions using descriptors generated from models at different learning steps was similar from the beginning to the end of training. Furthermore, we found that the Transformer requires particularly long training to learn chirality and sometimes stagnates with low translation accura
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.11525</link><description>&lt;p&gt;
SIFT: &#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#20197;&#26368;&#22823;&#38480;&#24230;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#25928;&#29575;&#65288;&#19982;&#35757;&#32451;FLOPS&#30456;&#20851;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#65289;&#12290; &#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;FLOP&#65292;&#20294;&#20351;&#29992;&#31232;&#30095;&#26435;&#37325;&#36827;&#34892;&#35757;&#32451;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25439;&#22833;&#25110;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#21608;&#26399;&#65292;&#20351;&#24471;&#32467;&#26524;&#30340;&#35757;&#32451;&#25928;&#29575;&#19981;&#22815;&#28165;&#26224;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#31232;&#30095;&#24615;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#21516;&#30340;FLOPS&#65292;&#24182;&#36890;&#36807;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#23637;&#31034;&#35757;&#32451;&#25928;&#29575;&#25552;&#39640;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SIFT&#65292;&#19968;&#32452;&#29992;&#20316;&#23494;&#38598;&#23618;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#26469;&#25552;&#39640;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;FLOP&#25928;&#29575;&#30340;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#12290; &#27599;&#20010;&#36716;&#25442;&#37117;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#65288;&#31232;&#30095;&#32423;&#21035;&#65289;&#21442;&#25968;&#21270;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31232;&#30095;&#25513;&#33180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;250&#22810;&#20010;&#20851;&#20110;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26368;&#36817;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#22522;&#26412;&#30340;&#21477;&#27861;&#12289;&#35821;&#20041;&#12289;&#35821;&#29992;&#12289;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23481;&#26131;&#20986;&#29616;&#19981;&#23454;&#22238;&#31572;&#12289;&#24120;&#35782;&#38169;&#35823;&#12289;&#35760;&#24518;&#21270;&#25991;&#26412;&#21644;&#31038;&#20250;&#20559;&#35265;&#31561;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.11504</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Language Model Behavior: A Comprehensive Survey. (arXiv:2303.11504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11504
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;250&#22810;&#20010;&#20851;&#20110;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26368;&#36817;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#22522;&#26412;&#30340;&#21477;&#27861;&#12289;&#35821;&#20041;&#12289;&#35821;&#29992;&#12289;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23481;&#26131;&#20986;&#29616;&#19981;&#23454;&#22238;&#31572;&#12289;&#24120;&#35782;&#38169;&#35823;&#12289;&#35760;&#24518;&#21270;&#25991;&#26412;&#21644;&#31038;&#20250;&#20559;&#35265;&#31561;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#28982;&#32780;&#23427;&#20204;&#29983;&#25104;&#30340;&#25991;&#26412;&#21363;&#20351;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20063;&#24120;&#24120;&#20196;&#20154;&#24778;&#35766;&#12290;&#22312;&#26412;&#27425;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;250&#22810;&#20010;&#20851;&#20110;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26368;&#36817;&#30740;&#31350;&#65292;&#36825;&#20123;&#30740;&#31350;&#22312;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#20043;&#21069;&#36827;&#34892;&#12290;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#22522;&#26412;&#30340;&#21477;&#27861;&#12289;&#35821;&#20041;&#12289;&#35821;&#29992;&#12289;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#23545;&#29305;&#23450;&#30340;&#36755;&#20837;&#21644;&#34920;&#38754;&#29305;&#24449;&#24456;&#25935;&#24863;&#12290;&#23613;&#31649;&#27169;&#22411;&#38543;&#30528;&#21442;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#20986;&#29616;&#19981;&#23454;&#22238;&#31572;&#12289;&#24120;&#35782;&#38169;&#35823;&#12289;&#35760;&#24518;&#21270;&#25991;&#26412;&#21644;&#31038;&#20250;&#20559;&#35265;&#12290;&#20854;&#20013;&#35768;&#22810;&#24369;&#28857;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#23545;&#25991;&#26412;&#20013;&#25152;&#23398;&#27169;&#24335;&#30340;&#36807;&#24230;&#25512;&#24191;&#25110;&#36807;&#24230;&#27867;&#21270;&#12290;&#25105;&#20204;&#32508;&#21512;&#20102;&#26368;&#36817;&#30340;&#32467;&#26524;&#65292;&#31361;&#20986;&#20102;&#30446;&#21069;&#24050;&#30693;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20570;&#20160;&#20040;&#21644;&#19981;&#33021;&#20570;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about what large language models can and cannot do.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;Codex&#34429;&#28982;&#26377;&#21161;&#20110;&#36991;&#20813;&#19968;&#20123;&#31616;&#21333;Bug&#65292;&#20294;&#32463;&#24120;&#20250;&#29983;&#25104;&#24050;&#30693;&#30340;&#65292;&#36880;&#23383;&#30340;Bug&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#21462;&#36991;&#20813;&#31574;&#30053;&#26469;&#20943;&#23569;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#22686;&#21152;&#26377;&#21019;&#24847;&#30340;Bug&#30340;&#29983;&#20135;&#12290;</title><link>http://arxiv.org/abs/2303.11455</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31616;&#21333;&#24858;&#34850; Bug
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Simple, Stupid Bugs. (arXiv:2303.11455v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;Codex&#34429;&#28982;&#26377;&#21161;&#20110;&#36991;&#20813;&#19968;&#20123;&#31616;&#21333;Bug&#65292;&#20294;&#32463;&#24120;&#20250;&#29983;&#25104;&#24050;&#30693;&#30340;&#65292;&#36880;&#23383;&#30340;Bug&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#21462;&#36991;&#20813;&#31574;&#30053;&#26469;&#20943;&#23569;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#22686;&#21152;&#26377;&#21019;&#24847;&#30340;Bug&#30340;&#29983;&#20135;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#22823;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29992;&#20110;&#36741;&#21161;&#24320;&#21457;&#32773;&#36827;&#34892;&#32534;&#30721;&#20219;&#21153;&#30340;&#22522;&#20110;AI&#30340;&#31995;&#32479;&#21464;&#24471;&#26222;&#36941;&#21487;&#29992;&#65307;Copilot&#20415;&#26159;&#36825;&#26679;&#30340;&#31995;&#32479;&#12290;Copilot&#20351;&#29992;Codex&#36825;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#23436;&#25104;&#19968;&#20010;&#30456;&#24212;&#30340;&#8220;&#25552;&#31034;&#8221;&#21518;&#30340;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;Codex&#26159;&#22312;&#20844;&#20849;GitHub&#23384;&#20648;&#24211;&#19978;&#35757;&#32451;&#30340;&#65292;&#21363;&#21487;&#33021;&#21253;&#21547;&#38169;&#35823;&#21644;&#28431;&#27934;&#30340;&#20195;&#30721;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;Codex&#20250;&#22797;&#21046;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#28431;&#27934;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Codex&#29983;&#25104;&#30340;&#19968;&#20010;&#26377;&#36259;&#30340; Bug &#31867;&#22411;&#30340;&#26131;&#21457;&#24615;&#65292;&#21363;&#21333;&#35821;&#21477; Bug&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#31616;&#21333;&#24858;&#34850; Bug &#25110; SStuBs&#12290;&#25105;&#20204;&#21457;&#29616;Codex&#21644;&#31867;&#20284;&#30340;LLMs&#30830;&#23454;&#26377;&#21161;&#20110;&#36991;&#20813;&#19968;&#20123;SStuBs&#65292;&#20294;&#30830;&#23454;&#20250;&#29983;&#25104;&#24050;&#30693;&#30340;&#65292;&#36880;&#23383;&#30340;SStuBs&#65292;&#20854;&#20986;&#29616;&#30340;&#21487;&#33021;&#24615;&#26159;&#24050;&#30693;&#27491;&#30830;&#20195;&#30721;&#30340;2&#20493;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;Codex&#29983;&#25104;&#30340;SStuBs&#30340;&#21518;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#36991;&#20813;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#26377;&#21161;&#20110;&#20943;&#23569;&#24050;&#30693;&#30340;&#65292;&#36880;&#23383;&#30340;SStubs&#30340;&#29983;&#20135;&#65292;&#24182;&#22686;&#21152;&#26377;&#21019;&#24847;&#30340;Bug&#30340;&#29983;&#20135;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of powerful neural language models, AI-based systems to assist developers in coding tasks are becoming widely available; Copilot is one such system. Copilot uses Codex, a large language model (LLM), to complete code conditioned on a preceding "prompt". Codex, however, is trained on public GitHub repositories, viz., on code that may include bugs and vulnerabilities. Previous studies [1], [2] show Codex reproduces vulnerabilities seen in training. In this study, we examine how prone Codex is to generate an interesting bug category, single statement bugs, commonly referred to as simple, stupid bugs or SStuBs in the MSR community. We find that Codex and similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs as much as 2x as likely than known, verbatim correct code. We explore the consequences of the Codex generated SStuBs and propose avoidance strategies that suggest the possibility of reducing the production of known, verbatim SStubs, and increase th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CommonsenseQA&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#22871;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#65292;GPT-4&#30340;&#34920;&#29616;&#21450;&#20854;&#23545;&#24120;&#35782;&#30693;&#35782;&#30340;&#22788;&#29702;&#21644;&#25972;&#21512;&#36807;&#31243;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#25105;&#20204;&#20063;&#21457;&#29616;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11436</link><description>&lt;p&gt;
&#24515;&#28789;&#19982;&#26426;&#22120;: &#35299;&#24320;GPT-4&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
Mind meets machine: Unravelling GPT-4's cognitive psychology. (arXiv:2303.11436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CommonsenseQA&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#22871;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#65292;GPT-4&#30340;&#34920;&#29616;&#21450;&#20854;&#23545;&#24120;&#35782;&#30693;&#35782;&#30340;&#22788;&#29702;&#21644;&#25972;&#21512;&#36807;&#31243;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#25105;&#20204;&#20063;&#21457;&#29616;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#25104;&#20998;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#29615;&#22659;&#35266;&#23519;&#25512;&#26029;&#32467;&#35770;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#25104;&#20026;&#36234;&#26469;&#36234;&#33021;&#22815;&#25191;&#34892;&#20154;&#31867;&#32423;&#20219;&#21153;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;GPT-4&#21450;&#20854;&#22312;&#21307;&#23398;&#32771;&#35797;&#12289;&#24459;&#24072;&#32771;&#35797;&#31561;&#20154;&#31867;&#38590;&#20197;&#23436;&#25104;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#30340;&#25104;&#21151;&#65292;&#22686;&#21152;&#20102;LLMs&#25104;&#20026;&#23436;&#32654;&#26234;&#33021;&#24037;&#20855;&#30340;&#20449;&#24515;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;GPT-4&#35770;&#25991;&#21521;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#26576;&#20123;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20294;&#23545;GPT-4&#22312;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#29305;&#21035;&#26159;&#29616;&#26377;&#30340;&#24050;&#32463;&#30830;&#31435;&#22909;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#36824;&#26159;&#32570;&#22833;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20851;&#27880;GPT-4&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;CommonsenseQA&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#22871;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#21450;&#20854;&#35748;&#30693;&#24515;&#29702;&#23398;&#24037;&#20855;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#33021;&#22815;&#29702;&#35299;GPT-4&#22914;&#20309;&#22312;&#20854;&#35821;&#35328;&#29983;&#25104;&#36807;&#31243;&#20013;&#22788;&#29702;&#21644;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#65292;&#20197;&#21450;&#20854;&#22312;&#36825;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning is a basic ingredient of intelligence in humans, empowering the ability to deduce conclusions based on the observations of surroundings. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans such as medical exam, bar exam and others has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Though, the GPT-4 paper has shown performance on some common sense reasoning tasks, a comprehensive assessment of GPT-4 on common sense reasoning tasks, particularly on the existing well-established datasets is missing. In this study, we focus on the evaluation of GPT-4's performance on a set of common sense reasoning questions from the widely used CommonsenseQA dataset along with tools from cognitive psychology. In doing so, we understand how GPT-4 processes and integrates common sense k
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11403</link><description>&lt;p&gt;
eP-ALM:&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#24863;&#30693;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36804;&#20170;&#20026;&#27490;&#32473;&#19990;&#30028;&#30041;&#19979;&#20102;&#28145;&#21051;&#21360;&#35937;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#27169;&#22411;&#25152;&#20855;&#26377;&#30340;&#38750;&#21516;&#23547;&#24120;&#30340;&#33021;&#21147;&#12290;&#22312;&#35270;&#35273;&#26041;&#38754;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;&#21363;ViT&#65289;&#20063;&#22312;&#36861;&#38543;&#21516;&#19968;&#36235;&#21183;&#65292;&#21462;&#24471;&#20102;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#38543;&#30528;&#36825;&#31181;&#21333;&#27169;&#22411;&#30340;&#20016;&#23500;&#22810;&#26679;&#65292;&#33258;&#28982;&#20250;&#24341;&#21457;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#38656;&#35201;&#36319;&#38543;&#36825;&#20010;&#36235;&#21183;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#21162;&#21147;&#38598;&#20013;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#24182;&#25552;&#20986;&#29992;&#24863;&#30693;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#20182;&#20204;&#20173;&#28982;&#35757;&#32451;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#20381;&#36182;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#22312;&#24040;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;CLIP&#65289;&#65292;&#24182;&#28155;&#21152;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#22823;&#22810;&#25968;&#20851;&#27880;Zero-Shot&#21644;In Context Learning&#65292;&#35266;&#23519;&#21040;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;eP-ALM&#65292;&#19968;&#31181;&#23558;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#20855;&#26377;&#26497;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26032;&#30340;&#39044;&#35757;&#32451;&#65292;&#20173;&#28982;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with l
&lt;/p&gt;</description></item><item><title>MM-REACT&#26159;&#19968;&#31181;&#34701;&#21512;&#35270;&#35273;&#19987;&#23478;&#21644;ChatGPT&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#21644;&#34892;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#25991;&#23383;&#25552;&#31034;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;ChatGPT&#21644;&#21508;&#31181;&#35270;&#35273;&#19987;&#23478;&#30340;&#21327;&#21516;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#39640;&#32423;&#35270;&#35273;&#29702;&#35299;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.11381</link><description>&lt;p&gt;
MM-REACT&#65306;&#20419;&#36827;ChatGPT&#36827;&#34892;&#22810;&#27169;&#24577;&#25512;&#29702;&#21644;&#34892;&#21160;&#30340;&#31995;&#32479;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action. (arXiv:2303.11381v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11381
&lt;/p&gt;
&lt;p&gt;
MM-REACT&#26159;&#19968;&#31181;&#34701;&#21512;&#35270;&#35273;&#19987;&#23478;&#21644;ChatGPT&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#21644;&#34892;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#25991;&#23383;&#25552;&#31034;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;ChatGPT&#21644;&#21508;&#31181;&#35270;&#35273;&#19987;&#23478;&#30340;&#21327;&#21516;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#39640;&#32423;&#35270;&#35273;&#29702;&#35299;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MM-REACT&#65292;&#19968;&#31181;&#23558;ChatGPT&#19982;&#35270;&#35273;&#19987;&#23478;&#27744;&#38598;&#25104;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#25512;&#29702;&#21644;&#34892;&#21160;&#30340;&#31995;&#32479;&#33539;&#24335;&#12290;&#25105;&#20204;&#23450;&#20041;&#24182;&#25506;&#32034;&#20102;&#19968;&#31995;&#21015;&#20808;&#36827;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#24456;&#26377;&#36259;&#65292;&#20294;&#21487;&#33021;&#36229;&#20986;&#20102;&#29616;&#26377;&#35270;&#35273;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#33539;&#22260;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#39640;&#32423;&#35270;&#35273;&#26234;&#33021;&#65292;MM-REACT&#24341;&#20837;&#20102;&#19968;&#31181;&#25991;&#26412;&#25552;&#31034;&#35774;&#35745;&#65292;&#21487;&#20197;&#34920;&#31034;&#25991;&#26412;&#25551;&#36848;&#12289;&#25991;&#26412;&#21270;&#30340;&#31354;&#38388;&#22352;&#26631;&#21644;&#23545;&#40784;&#30340;&#25991;&#20214;&#21517;&#65292;&#29992;&#20110;&#22788;&#29702;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#23494;&#38598;&#30340;&#35270;&#35273;&#20449;&#21495;&#12290;MM-REACT&#30340;&#25552;&#31034;&#35774;&#35745;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#25509;&#21463;&#12289;&#20851;&#32852;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;ChatGPT&#21644;&#21508;&#31181;&#35270;&#35273;&#19987;&#23478;&#30340;&#21327;&#21516;&#32452;&#21512;&#12290;&#38646;&#26679;&#26412;&#23454;&#39564;&#35777;&#26126;&#20102;MM-REACT&#35299;&#20915;&#24863;&#20852;&#36259;&#30340;&#29305;&#23450;&#33021;&#21147;&#20197;&#21450;&#22312;&#38656;&#35201;&#39640;&#32423;&#35270;&#35273;&#29702;&#35299;&#30340;&#19981;&#21516;&#22330;&#26223;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;M
&lt;/p&gt;
&lt;p&gt;
We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos. MM-REACT's prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore, we discuss and compare M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11366</link><description>&lt;p&gt;
Reflexion&#65306;&#20855;&#26377;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30340;&#21457;&#23637;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20869;&#37096;&#27169;&#22411;&#24494;&#35843;&#12289;&#22806;&#37096;&#27169;&#22411;&#24494;&#35843;&#25110;&#22312;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#25110;&#32570;&#20047;&#33391;&#22909;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20195;&#29702;&#27809;&#26377;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#22266;&#26377;&#30340;&#26576;&#20123;&#21697;&#36136;&#65292;&#29305;&#21035;&#26159;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21453;&#24605;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#36807;&#31243;&#39640;&#25928;&#22320;&#35299;&#20915;&#26032;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986; Reflexion&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#36171;&#20104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20854;&#29616;&#26377;&#30340;&#25512;&#29702;&#36712;&#36857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#25991;&#26723;&#32423;&#32454;&#31890;&#24230;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#25968;&#25454;&#38598;DocRED-FE&#65292;&#37325;&#26032;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23618;&#30340;&#23454;&#20307;&#31867;&#22411;&#27169;&#24335;&#65292;&#24182;&#26681;&#25454;&#35813;&#27169;&#24335;&#25163;&#21160;&#37325;&#26032;&#27880;&#37322;&#20102;DocRED&#12290;&#22312;&#20840;&#38754;&#30340;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;DocRED-FE&#30340;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#21487;&#20197;&#25552;&#39640;&#20851;&#31995;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11141</link><description>&lt;p&gt;
DocRED-FE&#65306;&#19968;&#20221;&#25991;&#26723;&#32423;&#32454;&#31890;&#24230;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DocRED-FE: A Document-Level Fine-Grained Entity And Relation Extraction Dataset. (arXiv:2303.11141v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#25991;&#26723;&#32423;&#32454;&#31890;&#24230;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#25968;&#25454;&#38598;DocRED-FE&#65292;&#37325;&#26032;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23618;&#30340;&#23454;&#20307;&#31867;&#22411;&#27169;&#24335;&#65292;&#24182;&#26681;&#25454;&#35813;&#27169;&#24335;&#25163;&#21160;&#37325;&#26032;&#27880;&#37322;&#20102;DocRED&#12290;&#22312;&#20840;&#38754;&#30340;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;DocRED-FE&#30340;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#21487;&#20197;&#25552;&#39640;&#20851;&#31995;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#65288;JERE&#65289;&#26159;&#20449;&#24687;&#25277;&#21462;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;&#31895;&#31890;&#24230;JERE&#19978;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25991;&#26723;&#32423;&#32454;&#31890;&#24230;JERE&#25968;&#25454;&#38598;DocRED-FE&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#26469;&#25913;&#36827;DocRED&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23618;&#30340;&#23454;&#20307;&#31867;&#22411;&#27169;&#24335;&#65292;&#21253;&#25324;11&#20010;&#31895;&#31890;&#24230;&#31867;&#22411;&#21644;119&#20010;&#32454;&#31890;&#24230;&#31867;&#22411;&#65292;&#28982;&#21518;&#26681;&#25454;&#35813;&#27169;&#24335;&#25163;&#21160;&#37325;&#26032;&#27880;&#37322;&#20102;DocRED&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#23545;&#29616;&#26377;&#30340;JERE&#27169;&#22411;&#32780;&#35328;&#65292;DocRED-FE&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65307;&#65288;2&#65289;&#25105;&#20204;&#30340;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#21487;&#20197;&#25552;&#39640;&#20851;&#31995;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;https://github.com/PKU-TANGENT/DOCRED-FE&#19978;&#25552;&#20379;&#20102;DocRED-FE&#30340;&#25351;&#20196;&#21644;&#25105;&#20204;&#22522;&#32447;&#27169;&#22411;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint entity and relation extraction (JERE) is one of the most important tasks in information extraction. However, most existing works focus on sentence-level coarse-grained JERE, which have limitations in real-world scenarios. In this paper, we construct a large-scale document-level fine-grained JERE dataset DocRED-FE, which improves DocRED with Fine-Grained Entity Type. Specifically, we redesign a hierarchical entity type schema including 11 coarse-grained types and 119 fine-grained types, and then re-annotate DocRED manually according to this schema. Through comprehensive experiments we find that: (1) DocRED-FE is challenging to existing JERE models; (2) Our fine-grained entity types promote relation classification. We make DocRED-FE with instruction and the code for our baselines publicly available at https://github.com/PKU-TANGENT/DOCRED-FE.
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2303.10475</link><description>&lt;p&gt;
&#20165;&#20165;&#25552;&#31034;&#36275;&#22815;&#20102;&#21527;&#65311;&#19981;&#26159;&#30340;&#12290;&#25351;&#23548;&#23398;&#20064;&#30340;&#20840;&#38754;&#21644;&#26356;&#24191;&#38420;&#35270;&#35282;&#65288;arXiv&#65306;2303.10475v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10475
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#35821;&#20041;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#25110;&#19968;&#26465;&#25991;&#26412;&#25351;&#20196;&#26469;&#34920;&#36798;&#12290;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#24341;&#36215;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#26631;&#35760;&#31034;&#20363;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#65292;&#25110;&#32773;&#31995;&#32479;&#38656;&#35201;&#31435;&#21363;&#22788;&#29702;&#26032;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#36825;&#19981;&#26159;&#29992;&#25143;&#21451;&#22909;&#30340;&#65292;&#22240;&#20026;&#26368;&#32456;&#29992;&#25143;&#21487;&#33021;&#26356;&#24895;&#24847;&#22312;&#20351;&#29992;&#31995;&#32479;&#20043;&#21069;&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#32780;&#19981;&#26159;&#19968;&#32452;&#31034;&#20363;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#31038;&#21306;&#20173;&#28982;&#38754;&#20020;&#30528;&#19968;&#20123;&#20849;&#21516;&#30340;&#38382;&#39064;&#12290;&#26412;&#27425;&#35843;&#26597;&#26088;&#22312;&#24635;&#32467;&#25351;&#23548;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Task semantics can be expressed by a set of input-to-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning from task instructions. Despite its impressive progress, there are some common issues that the community struggles with. This survey paper tries to summarize the current research on instruction learning, particularly, by answering the following questions:
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;GLEN&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;3,465&#31181;&#19981;&#21516;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#26631;&#27880;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#38454;&#27573;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#22823;&#26412;&#20307;&#22823;&#23567;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09093</link><description>&lt;p&gt;
GLEN&#65306;&#38754;&#21521;&#25968;&#21315;&#31181;&#31867;&#22411;&#30340;&#36890;&#29992;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GLEN: General-Purpose Event Detection for Thousands of Types. (arXiv:2303.09093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09093
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;GLEN&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;3,465&#31181;&#19981;&#21516;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#26631;&#27880;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#38454;&#27573;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#22823;&#26412;&#20307;&#22823;&#23567;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25277;&#21462;&#31995;&#32479;&#30340;&#21457;&#23637;&#19968;&#30452;&#21463;&#38480;&#20110;&#32570;&#20047;&#24191;&#27867;&#35206;&#30422;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20351;&#20107;&#20214;&#25277;&#21462;&#31995;&#32479;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;GLEN&#65292;&#28085;&#30422;&#20102;3,465&#31181;&#19981;&#21516;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#26412;&#20307;&#27604;&#20219;&#20309;&#24403;&#21069;&#25968;&#25454;&#38598;&#37117;&#22823;20&#20493;&#20197;&#19978;&#12290;GLEN&#21033;&#29992;DWD&#21472;&#21152;&#25216;&#26415;&#21019;&#24314;&#65292;&#36890;&#36807;&#25552;&#20379;&#32500;&#22522;&#30334;&#31185;Qnode&#21644;PropBank&#35282;&#33394;&#38598;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20351;&#29992;PropBank&#30340;&#29616;&#26377;&#26631;&#27880;&#20316;&#20026;&#38388;&#25509;&#30417;&#30563;&#26469;&#23436;&#25104;&#21019;&#24314;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#38454;&#27573;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;GLEN&#30340;&#22823;&#26412;&#20307;&#22823;&#23567;&#21644;&#37096;&#20998;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65288;F1&#20998;&#25968;&#25552;&#39640;&#20102;&#32422;10%&#65289;&#65292;&#19982;&#20256;&#32479;&#30340;&#20998;&#31867;&#22522;&#32447;&#21644;&#36739;&#26032;&#30340;&#22522;&#20110;&#23450;&#20041;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#26174;&#31034;&#26631;&#31614;&#22122;&#22768;&#20173;&#28982;&#26159;&#25552;&#39640;&#24615;&#33021;&#30340;&#26368;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of event extraction systems has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 3,465 different event types, making it over 20x larger in ontology than any current dataset. GLEN is created by utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables us to use the abundant existing annotation for PropBank as distant supervision. In addition, we also propose a new multi-stage event detection model specifically designed to handle the large ontology size and partial labels in GLEN. We show that our model exhibits superior performance (~10% F1 gain) compared to both conventional classification baselines and newer definition-based models. Finally, we perform error analysis and show that label noise is still the largest challenge for improving performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31639;&#27861;&#29983;&#25104; LTL &#20844;&#24335;&#65292;&#36716;&#25442;&#25104;&#32467;&#26500;&#21270;&#33521;&#35821;&#26469;&#32508;&#21512;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#30340; LTL &#35268;&#26684;&#32763;&#35793;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.08006</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#26684;&#33258;&#28982;&#35821;&#35328;&#21040;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#32763;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification. (arXiv:2303.08006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31639;&#27861;&#29983;&#25104; LTL &#20844;&#24335;&#65292;&#36716;&#25442;&#25104;&#32467;&#26500;&#21270;&#33521;&#35821;&#26469;&#32508;&#21512;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#30340; LTL &#35268;&#26684;&#32763;&#35793;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26381;&#21153;&#20110;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#65292;&#36171;&#20104;&#20854;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#24182;&#29992;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#31561;&#24418;&#24335;&#35821;&#35328;&#23450;&#20041;&#20855;&#20307;&#20219;&#21153;&#35268;&#26684;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#32763;&#35793;&#25104; LTL &#35268;&#26684;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#38750;&#24120;&#26377;&#38480;&#30340;&#21463;&#35797;&#32773;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#12290;&#19982;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;LTL&#32763;&#35793;&#22120;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#32780;&#26159;&#36890;&#36807;&#31639;&#27861;&#29983;&#25104;LTL&#20844;&#24335;&#65292;&#36716;&#25442;&#25104;&#32467;&#26500;&#21270;&#33521;&#35821;&#65292;&#28982;&#21518;&#21033;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25913;&#20889;&#33021;&#21147;&#26469;&#32508;&#21512;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make robots accessible to a broad audience, it is critical to endow them with the ability to take universal modes of communication, like commands given in natural language, and extract a concrete desired task specification, defined using a formal language like linear temporal logic (LTL). In this paper, we present a learning-based approach for translating from natural language commands to LTL specifications with very limited human-labeled training data. This is in stark contrast to existing natural-language to LTL translators, which require large human-labeled datasets, often in the form of labeled pairs of LTL formulas and natural language commands, to train the translator. To reduce reliance on human data, our approach generates a large synthetic training dataset through algorithmic generation of LTL formulas, conversion to structured English, and then exploiting the paraphrasing capabilities of modern large language models (LLMs) to synthesize a diverse corpus of natural language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35299;&#20915;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#23637;&#31034;&#20102;&#36825;&#31181;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05382</link><description>&lt;p&gt;
ChatGPT&#24050;&#22312;&#22320;&#24179;&#32447;&#19978;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23601;&#26159;&#25105;&#20204;&#38656;&#35201;&#30340;&#26234;&#33021;&#20132;&#36890;&#35299;&#20915;&#26041;&#26696;&#65311;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?. (arXiv:2303.05382v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35299;&#20915;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#23637;&#31034;&#20102;&#36825;&#31181;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;&#20855;&#26377;60&#20159;&#21442;&#25968;&#30340;&#37325;&#35201;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#12290;ChatGPT&#23637;&#31034;&#20102;LLM&#30340;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#23545;&#35805;&#21709;&#24212;&#26041;&#38754;&#12290;&#38543;&#30528;LLM&#22312;&#21508;&#31181;&#30740;&#31350;&#25110;&#24037;&#31243;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#29616;&#22312;&#26159;&#26102;&#20505;&#35774;&#24819;LLM&#22914;&#20309;&#38761;&#26032;&#25105;&#20204;&#22788;&#29702;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#26041;&#24335;&#20102;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#20915;&#20851;&#38190;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#26234;&#33021;&#31995;&#32479;&#36824;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#36890;&#36807;LLM&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;LLM&#35013;&#22791;&#30340;&#36825;&#20123;&#28508;&#22312;&#30340;&#20132;&#36890;&#24212;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35777;&#26126;&#36825;&#31181;&#28508;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#12290;&#23613;&#31649;&#23384;&#22312;&#28508;&#22312;&#30340;&#30410;&#22788;&#65292;&#20294;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, developed by OpenAI, is one of the milestone large language models (LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive language understanding capability of LLM, particularly in generating conversational response. As LLMs start to gain more attention in various research or engineering domains, it is time to envision how LLM may revolutionize the way we approach intelligent transportation systems. This paper explores the future applications of LLM in addressing key transportation problems. By leveraging LLM with cross-modal encoder, an intelligent system can also process traffic data from different modalities and execute transportation operations through an LLM. We present and validate these potential transportation applications equipped by LLM. To further demonstrate this potential, we also provide a concrete smartphone-based crash report auto-generation and analysis framework as a use case. Despite the potential benefits, challenges related to data privac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#30340;Few-shot&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24847;&#22270;&#26816;&#27979;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20219;&#21153;&#26469;&#35299;&#20915;&#35821;&#20041;&#30456;&#20284;&#30340;&#32454;&#31890;&#24230;&#24847;&#22270;&#38382;&#39064;&#65292;&#32467;&#26524;&#22312;&#19977;&#20010;few-shot&#24847;&#22270;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.01593</link><description>&lt;p&gt;
QAID&#65306;&#21551;&#21457;&#24335;&#30340;Few-shot&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QAID: Question Answering Inspired Few-shot Intent Detection. (arXiv:2303.01593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#30340;Few-shot&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24847;&#22270;&#26816;&#27979;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20219;&#21153;&#26469;&#35299;&#20915;&#35821;&#20041;&#30456;&#20284;&#30340;&#32454;&#31890;&#24230;&#24847;&#22270;&#38382;&#39064;&#65292;&#32467;&#26524;&#22312;&#19977;&#20010;few-shot&#24847;&#22270;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#26816;&#27979;&#28041;&#21450;&#21040;&#19968;&#20123;&#35821;&#20041;&#30456;&#20284;&#30340;&#32454;&#31890;&#24230;&#24847;&#22270;&#65292;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#24847;&#22270;&#26816;&#27979;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20219;&#21153;&#65292;&#23558;&#35805;&#35821;&#21644;&#24847;&#22270;&#21517;&#20316;&#20026;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#22521;&#35757;&#27169;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#25209;&#37327;&#23545;&#27604;&#25439;&#22833;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#22521;&#35757;&#26469;&#25913;&#21892;&#26597;&#35810;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#26597;&#35810;&#21644;&#21516;&#19968;&#24847;&#22270;&#31572;&#26696;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#21270;&#20196;&#29260;&#32423;&#30456;&#20284;&#24230;&#20998;&#25968;&#12290;&#25105;&#20497;&#22312;&#19977;&#20010;few-shot&#24847;&#22270;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intent detection with semantically similar fine-grained intents is a challenging task. To address it, we reformulate intent detection as a question-answering retrieval task by treating utterances and intent names as questions and answers. To that end, we utilize a question-answering retrieval architecture and adopt a two stages training schema with batch contrastive loss. In the pre-training stage, we improve query representations through self-supervised training. Then, in the fine-tuning stage, we increase contextualized token-level similarity scores between queries and answers from the same intent. Our results on three few-shot intent detection benchmarks achieve state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; Vid2Seq&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#23494;&#38598;&#20107;&#20214;&#23383;&#24149;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#36716;&#24405;&#35821;&#38899;&#30340;&#21477;&#23376;&#36793;&#30028;&#36716;&#21270;&#20026;&#20266;&#20107;&#20214;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#36716;&#24405;&#35821;&#38899;&#21477;&#23376;&#20316;&#20026;&#20266;&#20107;&#20214;&#23383;&#24149;&#65292;&#25105;&#20204;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#27880; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#30340;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26159;&#30446;&#21069;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2302.14115</link><description>&lt;p&gt;
Vid2Seq: &#29992;&#20110;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning. (arXiv:2302.14115v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; Vid2Seq&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#23494;&#38598;&#20107;&#20214;&#23383;&#24149;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#36716;&#24405;&#35821;&#38899;&#30340;&#21477;&#23376;&#36793;&#30028;&#36716;&#21270;&#20026;&#20266;&#20107;&#20214;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#36716;&#24405;&#35821;&#38899;&#21477;&#23376;&#20316;&#20026;&#20266;&#20107;&#20214;&#23383;&#24149;&#65292;&#25105;&#20204;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#27880; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#30340;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26159;&#30446;&#21069;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; Vid2Seq&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#21333;&#32423;&#23494;&#38598;&#20107;&#20214;&#23383;&#24149;&#27169;&#22411;&#65292;&#23427;&#26159;&#22312;&#22823;&#35268;&#27169; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#12290; Vid2Seq &#26550;&#26500;&#36890;&#36807;&#29305;&#27530;&#30340;&#26102;&#38388;&#26631;&#35760;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#26080;&#32541;&#22320;&#39044;&#27979;&#20107;&#20214;&#36793;&#30028;&#21644;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26410;&#26631;&#27880; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#36716;&#24405;&#35821;&#38899;&#30340;&#21477;&#23376;&#36793;&#30028;&#36716;&#21270;&#20026;&#20266;&#20107;&#20214;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#36716;&#24405;&#35821;&#38899;&#21477;&#23376;&#20316;&#20026;&#20266;&#20107;&#20214;&#23383;&#24149;&#12290;&#20351;&#29992; YT-Temporal-1B &#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340; Vid2Seq &#27169;&#22411;&#22312;&#21508;&#31181;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324; YouCook2&#12289;ViTT &#21644; ActivityNet Captions&#12290; Vid2Seq &#36824;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#35270;&#39057;&#27573;&#33853;&#23383;&#24149;&#21644;&#35270;&#39057;&#29255;&#27573;&#23383;&#24149;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce Vid2Seq, a multi-modal single-stage dense event captioning model pretrained on narrated videos which are readily-available at scale. The Vid2Seq architecture augments a language model with special time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output sequence. Such a unified model requires large-scale training data, which is not available in current annotated datasets. We show that it is possible to leverage unlabeled narrated videos for dense video captioning, by reformulating sentence boundaries of transcribed speech as pseudo event boundaries, and using the transcribed speech sentences as pseudo event captions. The resulting Vid2Seq model pretrained on the YT-Temporal-1B dataset improves the state of the art on a variety of dense video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions. Vid2Seq also generalizes well to the tasks of video paragraph captioning and video clip captionin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20363;&#27979;&#35797;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#24615;&#21035;&#20559;&#35265;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#22312;&#20351;&#29992;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#26102;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#65292;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#27973;&#23618;&#30340;&#35821;&#35328;&#32447;&#32034;&#26469;&#25913;&#21464;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>http://arxiv.org/abs/2301.04347</link><description>&lt;p&gt;
Counteracts&#65306;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#35797;&#21051;&#26495;&#21360;&#35937;&#30340;&#23545;&#25239;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counteracts: Testing Stereotypical Representation in Pre-trained Language Models. (arXiv:2301.04347v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20363;&#27979;&#35797;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#24615;&#21035;&#20559;&#35265;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#22312;&#20351;&#29992;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#26102;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#65292;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#27973;&#23618;&#30340;&#35821;&#35328;&#32447;&#32034;&#26469;&#25913;&#21464;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#19982;&#20154;&#31867;&#19968;&#26679;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#33021;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#33258;&#24049;&#30340;&#20559;&#35265;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#19979;&#28216;&#20219;&#21153;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31649;&#36947;&#30340;&#19968;&#37096;&#20998;&#38598;&#25104;&#65292;&#26377;&#24517;&#35201;&#20102;&#35299;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#21644;&#20943;&#36731;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#20363;&#26469;&#27979;&#35797;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35813;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;9&#20010;&#19981;&#21516;&#30340;&#22635;&#31354;&#24335;&#25552;&#31034;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#30693;&#35782;&#21644;&#22522;&#30784;&#25552;&#31034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#26102;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#27973;&#23618;&#30340;&#35821;&#35328;&#32447;&#32034;&#65292;&#22914;&#21333;&#35789;&#20301;&#32622;&#21644;&#21477;&#27861;&#32467;&#26500;&#26469;&#25913;&#21464;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have demonstrated strong performance on various natural language understanding tasks. Similar to humans, language models could also have their own bias that is learned from the training data. As more and more downstream tasks integrate language models as part of the pipeline, it is necessary to understand the internal stereotypical representation and the methods to mitigate the negative effects. In this paper, we proposed a simple method to test the internal stereotypical representation in pre-trained language models using counterexamples. We mainly focused on gender bias, but the method can be extended to other types of bias. We evaluated models on 9 different cloze-style prompts consisting of knowledge and base prompts. Our results indicate that pre-trained language models show a certain amount of robustness when using unrelated knowledge, and prefer shallow linguistic cues, such as word position and syntactic structure, to alter the internal stereotypical representat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#20165;&#20351;&#29992;&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#27809;&#26377;&#23545;&#35270;&#35273;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22235;&#39033;&#20195;&#34920;&#24615;&#35270;&#35273;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09778</link><description>&lt;p&gt;
&#25105;&#31455;&#28982;&#27809;&#26377;&#22270;&#29255;&#20102;&#65281;&#20165;&#20351;&#29992;&#35821;&#35328;&#25968;&#25454;&#23398;&#20064;&#35270;&#35273;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
I Can't Believe There's No Images! Learning Visual Tasks Using only Language Data. (arXiv:2211.09778v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21033;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#20165;&#20351;&#29992;&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#27809;&#26377;&#23545;&#35270;&#35273;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22235;&#39033;&#20195;&#34920;&#24615;&#35270;&#35273;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#25152;&#38656;&#30340;&#35768;&#22810;&#39640;&#32423;&#25216;&#33021;&#65292;&#20363;&#22914;&#35299;&#26512;&#38382;&#39064;&#12289;&#27604;&#36739;&#21644;&#23545;&#27604;&#35821;&#20041;&#20197;&#21450;&#32534;&#20889;&#25551;&#36848;&#65292;&#20063;&#21516;&#26679;&#36866;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#65292;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#33021;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#23398;&#20064;&#36825;&#20123;&#25216;&#33021;&#65292;&#28982;&#21518;&#22312;&#27809;&#26377;&#23545;&#35270;&#35273;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#23427;&#20204;&#36716;&#31227;&#21040;&#35270;&#35273;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#21033;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#36341;&#20013;&#65292;&#23545;&#27604;&#27169;&#22411;&#20013;&#19981;&#21516;&#27169;&#24577;&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#31995;&#32479;&#24615;&#24046;&#24322;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#24046;&#24322;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#32531;&#35299;&#27492;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#20165; &#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#22312;&#22235;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#19978;&#29983;&#25104;&#27169;&#22411;&#65306;&#22270;&#20687;&#23383;&#24149;&#12289;&#35270;&#35273;&#34164;&#21547;&#12289;&#35270;&#35273;&#38382;&#31572;&#21644;&#35270;&#35273;&#26032;&#38395;&#65292;&#24182;&#20351;&#29992;&#22270;&#20687;&#23545;&#26631;&#20934;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many high-level skills that are required for computer vision tasks, such as parsing questions, comparing and contrasting semantics, and writing descriptions, are also required in other domains such as natural language processing. In this paper, we ask whether it is possible to learn those skills from textual data and then transfer them to vision tasks without ever training on visual training data. Key to our approach is exploiting the joint embedding space of contrastively trained vision and language encoders. In practice, there can be systematic differences between embedding spaces for different modalities in contrastive models, and we analyze how these differences affect our approach and study strategies to mitigate this concern. We produce models using only text training data on four representative tasks: image captioning, visual entailment, visual question answering and visual news, and evaluate them on standard benchmarks using images. We find these models generally perform close 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptCap&#65292;&#19968;&#31181;&#20351;&#29992;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#20013;&#36890;&#29992;&#22270;&#20687;&#23383;&#24149;&#26080;&#27861;&#20934;&#30830;&#25551;&#36848;&#35270;&#35273;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.09699</link><description>&lt;p&gt;
PromptCap&#65306;&#20351;&#29992;GPT-3&#30340;&#25552;&#31034;&#24341;&#23548;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#36827;&#34892;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3. (arXiv:2211.09699v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptCap&#65292;&#19968;&#31181;&#20351;&#29992;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#20013;&#36890;&#29992;&#22270;&#20687;&#23383;&#24149;&#26080;&#27861;&#20934;&#30830;&#25551;&#36848;&#35270;&#35273;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#28041;&#21450;&#38656;&#35201;&#36229;&#36234;&#22270;&#29255;&#20197;&#20135;&#29983;&#27491;&#30830;&#31572;&#26696;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29305;&#21035;&#36866;&#29992;&#20110;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#30693;&#35782;&#26816;&#32034;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#20351;LM&#29702;&#35299;&#22270;&#20687;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#23383;&#24149;&#27169;&#22411;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#21333;&#20010;&#23383;&#24149;&#21477;&#23376;&#20013;&#24635;&#32467;&#22270;&#20687;&#26102;&#65292;&#35201;&#25551;&#36848;&#21738;&#20123;&#35270;&#35273;&#23454;&#20307;&#32463;&#24120;&#19981;&#26126;&#30830;&#12290;&#36890;&#29992;&#22270;&#20687;&#23383;&#24149;&#32463;&#24120;&#38169;&#36807;LM&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#25152;&#24517;&#38656;&#30340;&#35270;&#35273;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptCap&#65288;Prompt-guided image Captioning&#65289;&#65292;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#26088;&#22312;&#25104;&#20026;&#22270;&#20687;&#21644;&#40657;&#30418;LM&#20043;&#38388;&#26356;&#22909;&#30340;&#36830;&#25509;&#22120;&#12290;&#19982;&#36890;&#29992;&#23383;&#24149;&#19981;&#21516;&#65292;PromptCap&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#25511;&#21046;&#29983;&#25104;&#30340;&#23383;&#24149;&#20013;&#35201;&#25551;&#36848;&#30340;&#35270;&#35273;&#23454;&#20307;&#12290;&#25552;&#31034;&#21253;&#21547;&#23383;&#24149;&#24212;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based visual question answering (VQA) involves questions that require world knowledge beyond the image to yield the correct answer. Large language models (LMs) like GPT-3 are particularly helpful for this task because of their strong knowledge retrieval and reasoning capabilities. To enable LM to understand images, prior work uses a captioning model to convert images into text. However, when summarizing an image in a single caption sentence, which visual entities to describe are often underspecified. Generic image captions often miss visual details essential for the LM to answer visual questions correctly. To address this challenge, we propose PromptCap (Prompt-guided image Captioning), a captioning model designed to serve as a better connector between images and black-box LMs. Different from generic captions, PromptCap takes a natural-language prompt to control the visual entities to describe in the generated caption. The prompt contains a question that the caption should ai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19982;&#21327;&#20316;&#20307;&#39564;&#26234;&#33021;&#20307;&#20132;&#20114;&#25910;&#38598;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#25910;&#38598;&#20102;&#39318;&#20010;&#20132;&#20114;&#22522;&#30784;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2211.06552</link><description>&lt;p&gt;
&#25910;&#38598;&#20132;&#20114;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#22522;&#30784;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Collecting Interactive Multi-modal Datasets for Grounded Language Understanding. (arXiv:2211.06552v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19982;&#21327;&#20316;&#20307;&#39564;&#26234;&#33021;&#20307;&#20132;&#20114;&#25910;&#38598;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#25910;&#38598;&#20102;&#39318;&#20010;&#20132;&#20114;&#22522;&#30784;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#24935;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#21644;&#29615;&#22659;&#12290;&#20174;&#24456;&#23567;&#30340;&#26102;&#20505;&#24320;&#22987;&#65292;&#20154;&#31867;&#36890;&#36807;&#27169;&#20223;&#20182;&#20154;&#30340;&#34892;&#20026;&#25110;&#25353;&#29031;&#25552;&#20379;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23398;&#20064;&#26032;&#25216;&#33021;&#24182;&#23398;&#20250;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;&#30740;&#31350;&#33021;&#22815;&#22312;&#26426;&#22120;&#20013;&#23454;&#29616;&#31867;&#20284;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#26412;&#25991;&#20316;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;&#65288;1&#65289;&#24418;&#24335;&#21270;&#21327;&#20316;&#20307;&#39564;&#26234;&#33021;&#20307;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65307;&#65288;2&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#36827;&#34892;&#24191;&#27867;&#21644;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#25910;&#38598;&#24037;&#20855;&#65307;&#65288;3&#65289;&#25910;&#38598;&#20102;&#39318;&#20010;&#20132;&#20114;&#22522;&#30784;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence can remarkably adapt quickly to new tasks and environments. Starting from a very young age, humans acquire new skills and learn how to solve new tasks either by imitating the behavior of others or by following provided natural language instructions. To facilitate research which can enable similar capabilities in machines, we made the following contributions (1) formalized the collaborative embodied agent using natural language task; (2) developed a tool for extensive and scalable data collection; and (3) collected the first dataset for interactive grounded language understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26234;&#33021;&#25216;&#26415;&#20013;&#26085;&#30410;&#26222;&#36941;&#30340;&#25991;&#26412;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#31867;&#21035;&#65306;&#38544;&#34109;&#19981;&#23433;&#20840;&#25991;&#26412;&#12290;&#35813;&#25991;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#26234;&#33021;&#31995;&#32479;&#20869;&#37096;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.09306</link><description>&lt;p&gt;
&#32531;&#35299;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#20013;&#38544;&#34109;&#19981;&#23433;&#20840;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Mitigating Covertly Unsafe Text within Natural Language Systems. (arXiv:2210.09306v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26234;&#33021;&#25216;&#26415;&#20013;&#26085;&#30410;&#26222;&#36941;&#30340;&#25991;&#26412;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#31867;&#21035;&#65306;&#38544;&#34109;&#19981;&#23433;&#20840;&#25991;&#26412;&#12290;&#35813;&#25991;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#26234;&#33021;&#31995;&#32479;&#20869;&#37096;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25216;&#26415;&#20013;&#19968;&#20010;&#26085;&#30410;&#26222;&#36941;&#30340;&#38382;&#39064;&#26159;&#25991;&#26412;&#23433;&#20840;&#24615;&#65292;&#22240;&#20026;&#19981;&#21463;&#25511;&#21046;&#30340;&#31995;&#32479;&#21487;&#33021;&#20250;&#21521;&#29992;&#25143;&#29983;&#25104;&#23548;&#33268;&#20260;&#23475;&#25110;&#23041;&#32961;&#29983;&#21629;&#30340;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#23548;&#33268;&#36523;&#20307;&#20260;&#23475;&#30340;&#29983;&#25104;&#35821;&#21477;&#30340;&#26126;&#30830;&#31243;&#24230;&#19981;&#21516;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21306;&#20998;&#20102;&#21487;&#33021;&#23548;&#33268;&#36523;&#20307;&#20260;&#23475;&#30340;&#25991;&#26412;&#31867;&#22411;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#23588;&#20854;&#26410;&#34987;&#25506;&#32034;&#30340;&#31867;&#21035;&#65306;&#38544;&#34109;&#19981;&#23433;&#20840;&#25991;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#35299;&#20102;&#36825;&#20010;&#31867;&#21035;&#24182;&#20998;&#26512;&#20102;&#27599;&#20010;&#23567;&#31867;&#21035;&#20013;&#25991;&#26412;&#30340;&#29983;&#25104;&#26041;&#24335;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23450;&#20041;&#20102;&#23548;&#33268;&#29289;&#29702;&#20260;&#23475;&#30340;&#38544;&#34109;&#19981;&#23433;&#20840;&#35821;&#35328;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#36825;&#20010;&#24494;&#22937;&#20294;&#21361;&#38505;&#30340;&#38382;&#39064;&#38656;&#35201;&#25104;&#20026;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#30417;&#31649;&#26426;&#26500;&#30340;&#20248;&#20808;&#32771;&#34385;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#21551;&#21457;&#26410;&#26469;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#24110;&#21161;&#25552;&#39640;&#26234;&#33021;&#31995;&#32479;&#20869;&#37096;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasingly prevalent problem for intelligent technologies is text safety, as uncontrolled systems may generate recommendations to their users that lead to injury or life-threatening consequences. However, the degree of explicitness of a generated statement that can cause physical harm varies. In this paper, we distinguish types of text that can lead to physical harm and establish one particularly underexplored category: covertly unsafe text. Then, we further break down this category with respect to the system's information and discuss solutions to mitigate the generation of text in each of these subcategories. Ultimately, our work defines the problem of covertly unsafe language that causes physical harm and argues that this subtle yet dangerous issue needs to be prioritized by stakeholders and regulators. We highlight mitigation strategies to inspire future researchers to tackle this challenging problem and help improve safety within smart systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22312;&#24179;&#38754;&#24615;&#21644;&#25237;&#24433;&#24615;&#23450;&#20041;&#19979;&#26641;&#30340;&#26368;&#22823;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#25237;&#24433;&#21644;&#24179;&#38754;&#25490;&#21015;&#30340;&#22810;&#20010;&#24615;&#36136;&#65292;&#21457;&#29616;&#27611;&#27611;&#34411;&#26641;&#26368;&#20248;&#65292;&#25512;&#24191;&#20102;&#20043;&#21069;&#30340;&#26497;&#20540;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.06924</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#38754;&#24615;&#19982;&#25237;&#24433;&#24615;&#23450;&#20041;&#19979;&#26641;&#30340;&#26368;&#22823;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;&#65288;arXiv:2206.06924v3[cs.DS] &#26356;&#26032;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
The Maximum Linear Arrangement Problem for trees under projectivity and planarity. (arXiv:2206.06924v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06924
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22312;&#24179;&#38754;&#24615;&#21644;&#25237;&#24433;&#24615;&#23450;&#20041;&#19979;&#26641;&#30340;&#26368;&#22823;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#26368;&#22823;&#25237;&#24433;&#21644;&#24179;&#38754;&#25490;&#21015;&#30340;&#22810;&#20010;&#24615;&#36136;&#65292;&#21457;&#29616;&#27611;&#27611;&#34411;&#26641;&#26368;&#20248;&#65292;&#25512;&#24191;&#20102;&#20043;&#21069;&#30340;&#26497;&#20540;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#32447;&#24615;&#25490;&#21015;&#38382;&#39064;(MaxLA)&#26159;&#25351;&#25214;&#21040;&#20174;&#22270;G&#30340;n&#20010;&#39030;&#28857;&#21040;&#19981;&#21516;&#36830;&#32493;&#25972;&#25968;&#30340;&#26144;&#23556;$ \pi $&#65292;&#20351;&#24471;$ D(G)=\sum_{uv\in E(G)}|\pi(u)-\pi(v)| $&#26368;&#22823;&#21270;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39030;&#28857;&#34987;&#35748;&#20026;&#22312;&#19968;&#26465;&#27700;&#24179;&#32447;&#19978;&#65292;&#24182;&#19988;&#36793;&#26159;&#20316;&#20026;&#21322;&#22278;&#24359;&#30011;&#22312;&#32447;&#19978;&#26041;&#30340;&#12290;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#25490;&#21015;&#30340;MaxLA&#21464;&#20307;&#12290;&#22312;&#24179;&#38754;&#21464;&#20307;&#20013;&#65292;&#31105;&#27490;&#36793;&#20132;&#21449;&#12290;&#22312;&#38024;&#23545;&#26681;&#26641;&#30340;&#25237;&#24433;&#21464;&#20307;&#20013;&#65292;&#25490;&#21015;&#26159;&#24179;&#38754;&#30340;&#65292;&#32780;&#26681;&#19981;&#33021;&#34987;&#20219;&#20309;&#36793;&#35206;&#30422;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#35299;&#20915;&#26641;&#30340;&#24179;&#38754;&#21644;&#25237;&#24433;MaxLA&#30340;O(n)-&#26102;&#38388;&#21644;O(n&#65289;-&#31354;&#38388;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#26368;&#22823;&#25237;&#24433;&#21644;&#24179;&#38754;&#25490;&#21015;&#30340;&#22810;&#20010;&#24615;&#36136;&#65292;&#24182;&#19988;&#34920;&#26126;&#27611;&#27611;&#34411;&#26641;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#25152;&#26377;&#26641;&#20013;&#26368;&#22823;&#21270;&#24179;&#38754;MaxLA&#65292;&#22240;&#27492;&#25512;&#24191;&#20102;&#20808;&#21069;&#20851;&#20110;&#26641;&#30340;&#26497;&#20540;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Maximum Linear Arrangement problem (MaxLA) consists of finding a mapping $\pi$ from the $n$ vertices of a graph $G$ to distinct consecutive integers that maximizes $D(G)=\sum_{uv\in E(G)}|\pi(u) - \pi(v)|$. In this setting, vertices are considered to lie on a horizontal line and edges are drawn as semicircles above the line. There exist variants of MaxLA in which the arrangements are constrained. In the planar variant, edge crossings are forbidden. In the projective variant for rooted trees, arrangements are planar and the root cannot be covered by any edge. Here we present $O(n)$-time and $O(n)$-space algorithms that solve planar and projective MaxLA for trees. We also prove several properties of maximum projective and planar arrangements, and show that caterpillar trees maximize planar MaxLA over all trees of a fixed size thereby generalizing a previous extremal result on trees.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25163;&#35821;&#32763;&#35793;&#26694;&#26550;ConSLT&#65292;&#36890;&#36807;&#23558;&#26631;&#35760;&#32423;&#23545;&#27604;&#23398;&#20064;&#32435;&#20837;SLT&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#26631;&#35760;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#20844;&#24320;&#21487;&#29992;&#30340;&#25163;&#35821;&#32763;&#35793;&#35821;&#26009;&#24211;&#38750;&#24120;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.04916</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25163;&#35821;&#32763;&#35793;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Token-level Contrastive Framework for Sign Language Translation. (arXiv:2204.04916v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04916
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25163;&#35821;&#32763;&#35793;&#26694;&#26550;ConSLT&#65292;&#36890;&#36807;&#23558;&#26631;&#35760;&#32423;&#23545;&#27604;&#23398;&#20064;&#32435;&#20837;SLT&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#26631;&#35760;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#20844;&#24320;&#21487;&#29992;&#30340;&#25163;&#35821;&#32763;&#35793;&#35821;&#26009;&#24211;&#38750;&#24120;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proposed a token-level contrastive framework for sign language translation, ConSLT, which incorporates token-level contrastive learning into the SLT decoding process to learn effective token representations and alleviate the issue of limited publicly available SLT corpus.
&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#32763;&#35793;&#26159;&#19968;&#39033;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#24357;&#21512;&#32843;&#20154;&#21644;&#21548;&#21147;&#20154;&#20043;&#38388;&#30340;&#27807;&#36890;&#38548;&#38402;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#26041;&#27861;&#26469;&#23454;&#29616;&#25163;&#35821;&#32763;&#35793;&#65292;&#20294;&#20844;&#24320;&#21487;&#29992;&#30340;&#25163;&#35821;&#32763;&#35793;&#35821;&#26009;&#24211;&#38750;&#24120;&#26377;&#38480;&#65292;&#36825;&#23548;&#33268;&#20102;&#26631;&#35760;&#34920;&#31034;&#30340;&#23849;&#28291;&#21644;&#29983;&#25104;&#26631;&#35760;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConSLT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25163;&#35821;&#32763;&#35793;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#26631;&#35760;&#32423;&#23545;&#27604;&#23398;&#20064;&#32435;&#20837;SLT&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#26631;&#35760;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ConSLT&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#23558;&#27599;&#20010;&#26631;&#35760;&#21450;&#20854;&#30001;&#19981;&#21516;&#20002;&#22833;&#25513;&#30721;&#29983;&#25104;&#30340;&#23545;&#24212;&#26631;&#35760;&#35270;&#20026;&#27491;&#23545;&#65292;&#28982;&#21518;&#38543;&#26426;&#20174;&#24403;&#21069;&#21477;&#23376;&#20013;&#19981;&#22312;&#35789;&#27719;&#34920;&#20013;&#30340;$K$&#20010;&#26631;&#35760;&#20013;&#25277;&#26679;&#26500;&#24314;&#36127;&#20363;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#26469;&#39564;&#35777;ConSLT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign Language Translation (SLT) is a promising technology to bridge the communication gap between the deaf and the hearing people. Recently, researchers have adopted Neural Machine Translation (NMT) methods, which usually require large-scale corpus for training, to achieve SLT. However, the publicly available SLT corpus is very limited, which causes the collapse of the token representations and the inaccuracy of the generated tokens. To alleviate this issue, we propose ConSLT, a novel token-level \textbf{Con}trastive learning framework for \textbf{S}ign \textbf{L}anguage \textbf{T}ranslation , which learns effective token representations by incorporating token-level contrastive learning into the SLT decoding process. Concretely, ConSLT treats each token and its counterpart generated by different dropout masks as positive pairs during decoding, and then randomly samples $K$ tokens in the vocabulary that are not in the current sentence to construct negative examples. We conduct comprehen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;token-replaced&#26816;&#27979;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;token-replaced&#26816;&#27979;&#38382;&#39064;&#65292;&#33021;&#22815;&#20248;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;</title><link>http://arxiv.org/abs/2203.03235</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;token-replaced&#26816;&#27979;&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Token-replaced Detection Model as Few-shot Learner. (arXiv:2203.03235v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;token-replaced&#26816;&#27979;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;token-replaced&#26816;&#27979;&#38382;&#39064;&#65292;&#33021;&#22815;&#20248;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;token-replaced&#26816;&#27979;&#27169;&#22411;&#65288;&#27604;&#22914;ELECTRA&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;&#35813;&#26041;&#27861;&#23558;&#20998;&#31867;&#25110;&#22238;&#24402;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;token-replaced&#26816;&#27979;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#27599;&#20010;&#20219;&#21153;&#23450;&#20041;&#19968;&#20010;&#27169;&#26495;&#21644;&#26631;&#31614;&#25551;&#36848;&#35789;&#65292;&#24182;&#23558;&#23427;&#20204;&#25918;&#20837;&#36755;&#20837;&#20013;&#24418;&#25104;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;token-replaced&#26816;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;&#21738;&#20010;&#26631;&#31614;&#25551;&#36848;&#35789;&#22312;&#25552;&#31034;&#20013;&#26159;&#26368;&#21407;&#22987;&#30340;&#65288;&#21363;&#26368;&#23569;&#26356;&#25913;&#30340;&#65289;&#12290;&#23545;16&#20010;&#25968;&#25454;&#38598;&#30340;&#31995;&#32479;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#21477;&#35805;&#21644;&#20004;&#21477;&#35805;&#30340;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#37117;&#20248;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained masked language models have demonstrated remarkable ability as few-shot learners. In this paper, as an alternative, we propose a novel approach to few-shot learning with pre-trained token-replaced detection models like ELECTRA. In this approach, we reformulate a classification or a regression task as a token-replaced detection problem. Specifically, we first define a template and label description words for each task and put them into the input to form a natural language prompt. Then, we employ the pre-trained token-replaced detection model to predict which label description word is the most original (i.e., least replaced) among all label description words in the prompt. A systematic evaluation on 16 datasets demonstrates that our approach outperforms few-shot learners with pre-trained masked language models in both one-sentence and two-sentence learning tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;DeBERTaV3&#65292;&#20351;&#29992;&#26356;&#21152;&#26679;&#26412;&#26377;&#25928;&#30340;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#65288;RTD&#65289;&#21462;&#20195;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;DeBERTaV3&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.09543</link><description>&lt;p&gt;
DeBERTaV3&#65306;&#20351;&#29992;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#30340;ELECTRA&#39118;&#26684;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;DeBERTa&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. (arXiv:2111.09543v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;DeBERTaV3&#65292;&#20351;&#29992;&#26356;&#21152;&#26679;&#26412;&#26377;&#25928;&#30340;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#65288;RTD&#65289;&#21462;&#20195;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;DeBERTaV3&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;DeBERTaV3&#65292;&#23427;&#36890;&#36807;&#23558;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#26367;&#25442;&#20026;&#26356;&#21152;&#26679;&#26412;&#26377;&#25928;&#30340;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#65288;RTD&#65289;&#26469;&#25913;&#36827;&#21407;&#22987;&#30340;DeBERTa&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ELECTRA&#20013;&#30340;&#39321;&#33609;&#23884;&#20837;&#20849;&#20139;&#20250;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#22240;&#20026;&#21028;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#35757;&#32451;&#25439;&#22833;&#23558;&#20196;&#29260;&#23884;&#20837;&#25289;&#21521;&#19981;&#21516;&#30340;&#26041;&#21521;&#65292;&#20250;&#36896;&#25104;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;DeBERTa&#30456;&#21516;&#30340;&#35774;&#32622;&#39044;&#35757;&#32451;&#20102;DeBERTaV3&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;&#20197;&#20843;&#39033;&#20219;&#21153;&#20026;&#20363;&#30340;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;DeBERTaV3 Large&#27169;&#22411;&#24179;&#22343;&#24471;&#20998;&#20026;91.37&#65285;&#65292;&#27604;D&#39640;1.37&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% over D
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19977;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#36164;&#28304;&#65292;&#29992;&#20110;&#25429;&#25417;&#21644;&#35828;&#26126;&#20896;&#29366;&#30149;&#27602;&#22823;&#27969;&#34892;&#23545;&#24503;&#35821;&#22312;&#32447;&#26032;&#38395;&#30340;&#24433;&#21709;&#65306;&#24503;&#35821;&#26032;&#38395;&#28304;&#30340;RSS&#35821;&#26009;&#24211;&#12289;&#19968;&#20010;&#38745;&#24577;&#20294;&#19981;&#26029;&#26356;&#26032;&#30340;HTML&#39029;&#38754;&#21644;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#12290;&#20854;&#20013;&#65292;Web&#24212;&#29992;&#31243;&#24207;&#21487;&#20351;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#21644;&#26356;&#24191;&#27867;&#30340;&#20844;&#20247;&#33021;&#22815;&#25506;&#32034;&#36825;&#20123;&#24433;&#21709;&#32780;&#26080;&#38656;&#25110;&#21482;&#38656;&#31245;&#26377;&#35821;&#26009;&#24211;&#34920;&#31034;&#21644;&#25506;&#32034;&#25110;&#32479;&#35745;&#20998;&#26512;&#26041;&#38754;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2005.13316</link><description>&lt;p&gt;
&#38754;&#23545;&#20896;&#29366;&#30149;&#27602;&#21361;&#26426;&#65292;&#36861;&#36394;&#12289;&#25506;&#32034;&#21644;&#20998;&#26512;&#24503;&#35821;&#22312;&#32447;&#26032;&#38395;&#30340;&#26368;&#26032;&#21457;&#23637;&#65306;cOWIDplus&#20998;&#26512;&#21644;cOWIDplus&#27983;&#35272;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tracking, exploring and analyzing recent developments in German-language online press in the face of the coronavirus crisis: cOWIDplus Analysis and cOWIDplus Viewer. (arXiv:2005.13316v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.13316
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19977;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#36164;&#28304;&#65292;&#29992;&#20110;&#25429;&#25417;&#21644;&#35828;&#26126;&#20896;&#29366;&#30149;&#27602;&#22823;&#27969;&#34892;&#23545;&#24503;&#35821;&#22312;&#32447;&#26032;&#38395;&#30340;&#24433;&#21709;&#65306;&#24503;&#35821;&#26032;&#38395;&#28304;&#30340;RSS&#35821;&#26009;&#24211;&#12289;&#19968;&#20010;&#38745;&#24577;&#20294;&#19981;&#26029;&#26356;&#26032;&#30340;HTML&#39029;&#38754;&#21644;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#12290;&#20854;&#20013;&#65292;Web&#24212;&#29992;&#31243;&#24207;&#21487;&#20351;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#21644;&#26356;&#24191;&#27867;&#30340;&#20844;&#20247;&#33021;&#22815;&#25506;&#32034;&#36825;&#20123;&#24433;&#21709;&#32780;&#26080;&#38656;&#25110;&#21482;&#38656;&#31245;&#26377;&#35821;&#26009;&#24211;&#34920;&#31034;&#21644;&#25506;&#32034;&#25110;&#32479;&#35745;&#20998;&#26512;&#26041;&#38754;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#30149;&#27602;&#22823;&#27969;&#34892;&#21487;&#33021;&#26159;&#33258;&#20108;&#25112;&#20197;&#26469;&#19990;&#30028;&#38754;&#20020;&#30340;&#26368;&#22823;&#21361;&#26426;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#23427;&#20063;&#23545;&#20316;&#20026;&#25105;&#20204;&#20027;&#35201;&#27807;&#36890;&#24037;&#20855;&#30340;&#35821;&#35328;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#36164;&#28304;&#65292;&#26088;&#22312;&#25429;&#25417;&#21644;&#35828;&#26126;&#36825;&#20123;&#24433;&#21709;&#23545;&#24503;&#35821;&#30340;&#23376;&#38598;&#36896;&#25104;&#30340;&#24433;&#21709;&#65306;&#24503;&#35821;&#26032;&#38395;&#28304;&#30340;RSS&#35821;&#26009;&#24211;&#65288;&#25552;&#20379;&#33258;&#30001;&#21487;&#29992;&#30340;&#26410;&#25130;&#26029;&#30340;&#19968;&#20803;&#39057;&#29575;&#21015;&#34920;&#65289;&#65292;&#19968;&#20010;&#38745;&#24577;&#20294;&#19981;&#26029;&#26356;&#26032;&#30340;HTML&#39029;&#38754;&#65292;&#29992;&#20110;&#36319;&#36394;&#25152;&#29992;&#35789;&#27719;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#20351;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#21644;&#26356;&#24191;&#27867;&#30340;&#20844;&#20247;&#33021;&#22815;&#25506;&#32034;&#36825;&#20123;&#24433;&#21709;&#65292;&#32780;&#26080;&#38656;&#25110;&#21482;&#38656;&#31245;&#26377;&#35821;&#26009;&#24211;&#34920;&#31034;&#21644;&#25506;&#32034;&#25110;&#32479;&#35745;&#20998;&#26512;&#26041;&#38754;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The coronavirus pandemic may be the largest crisis the world has had to face since World War II. It does not come as a surprise that it is also having an impact on language as our primary communication tool. We present three inter-connected resources that are designed to capture and illustrate these effects on a subset of the German language: An RSS corpus of German-language newsfeeds (with freely available untruncated unigram frequency lists), a static but continuously updated HTML page tracking the diversity of the used vocabulary and a web application that enables other researchers and the broader public to explore these effects without any or with little knowledge of corpus representation/exploration or statistical analyses.
&lt;/p&gt;</description></item></channel></rss>